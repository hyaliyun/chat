import{_ as p,o as a,c as i,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(n,e,l,m,s,r){return a(),i("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",S],["__scopeId","data-v-ce96bafd"]]),A=JSON.parse('[{"question":"# Task Implement a class `PrCurveDisplay` that visualizes Precision-Recall curves. The class should be similar to `RocCurveDisplay` but for Precision-Recall curves. Your implementation should include methods to create the display object from an estimator and from true/predicted values, and a method to plot the generated curve. # Requirements 1. **Class Definition**: - The class should be named `PrCurveDisplay`. - The constructor should accept and store precision, recall, average precision (AP), and estimator name. 2. **Class Methods**: - `from_estimator(cls, estimator, X, y)`: This method should compute the predicted probabilities using the estimator and call `from_predictions`. - `from_predictions(cls, y, y_pred, estimator_name=None)`: This method should compute precision, recall, and average precision using `precision_recall_curve` and `average_precision_score` from `sklearn.metrics`, then create and return an instance of `PrCurveDisplay`. 3. **Instance Methods**: - `plot(self, ax=None, name=None, **kwargs)`: This method should plot the precision-recall curve using matplotlib. If `ax` is `None`, a new matplotlib axis should be created. # Input - `from_estimator(estimator, X, y)`: - `estimator`: A trained scikit-learn estimator. - `X`: A feature matrix. - `y`: True labels. - `from_predictions(y, y_pred, estimator_name=None)`: - `y`: True labels. - `y_pred`: Predicted probabilities. - `estimator_name`: Optional name of the estimator (default is `None`). - `plot(self, ax=None, name=None, **kwargs)`: - `ax`: A matplotlib axis (default is `None`). # Output - `from_estimator`: - Returns an instance of `PrCurveDisplay`. - `from_predictions`: - Returns an instance of `PrCurveDisplay`. - `plot`: - Plots the Precision-Recall curve on the provided or a new matplotlib axis. # Example ```python from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt # Create a sample dataset X, y = make_classification(n_samples=1000, n_features=20, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a logistic regression model model = LogisticRegression().fit(X_train, y_train) # Use PrCurveDisplay to visualize the Precision-Recall curve pr_display = PrCurveDisplay.from_estimator(model, X_test, y_test) pr_display.plot() plt.show() ``` # Constraints - You should use functions exclusively from scikit-learn and matplotlib. - The computation of precision, recall, and average precision should be done using scikit-learn functions. # Notes - Ensure your code is clean and well-documented. - Handle any potential edge cases, such as when the predictions contain only one class.","solution":"import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve, average_precision_score class PrCurveDisplay: def __init__(self, precision, recall, average_precision, estimator_name=None): self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): y_pred = estimator.predict_proba(X)[:, 1] return cls.from_predictions(y, y_pred, estimator_name=type(estimator).__name__) @classmethod def from_predictions(cls, y, y_pred, estimator_name=None): precision, recall, _ = precision_recall_curve(y, y_pred) average_precision = average_precision_score(y, y_pred) return cls(precision, recall, average_precision, estimator_name) def plot(self, ax=None, name=None, **kwargs): if ax is None: fig, ax = plt.subplots() name = name or self.estimator_name ax.plot(self.recall, self.precision, label=f\'{name} (AP={self.average_precision:0.2f})\', **kwargs) ax.set_xlabel(\'Recall\') ax.set_ylabel(\'Precision\') ax.set_title(\'Precision-Recall Curve\') ax.legend() return ax"},{"question":"**Title:** Pandas with PyArrow Integration **Objective:** Create pandas data structures with PyArrow-backed data, perform various operations, and efficiently read data using PyArrow. This will test your understanding of both pandas and PyArrow functionalities. **Task:** 1. **Create Data Structures:** - Create a pandas `Series` containing the floating-point numbers `[-1.5, 0.2, None]` using the `\\"float32[pyarrow]\\"` dtype. - Create a pandas `Index` containing the boolean values `[True, None]` using the `\\"bool[pyarrow]\\"` dtype. - Create a pandas `DataFrame` containing the integer pairs `[(1, 2), (3, 4)]` using the `\\"uint64[pyarrow]\\"` dtype. 2. **Perform Operations:** - Compute the mean of the created `Series` while handling missing data appropriately. - Check if the provided boolean values in the `Index` are True. - Add the integers in the first column of the `DataFrame` to those in the second column. 3. **Efficient Data Reading:** - Use PyArrow to read a CSV data provided as a string, returning a `DataFrame` with PyArrow-backed data. The data string is: ``` a,b,c 1,2.5,True 3,4.5,False ``` **Input and Output Specifications:** - Function: `def pandas_with_pyarrow_operations() -> dict:` - The function should create the required pandas data structures, perform the specified operations, and read data using PyArrow. - The function should return a dictionary with the following keys and values: - `\\"mean_series\\"`: The mean of the Series. - `\\"index_is_true\\"`: A list of boolean values indicating whether each element in the Index is True. - `\\"sum_dataframe\\"`: A list of sums of corresponding pairs of integers from two columns of the DataFrame. - `\\"read_csv\\"`: The DataFrame obtained from reading the provided CSV string. **Constraints:** - Use PyArrow data types as specified. - Handle missing data appropriately in the series and index operations. - Ensure the CSV reading uses PyArrow for efficient data handling. **Example:** ```python def pandas_with_pyarrow_operations() -> dict: import pandas as pd import pyarrow as pa from io import StringIO # 1. Create Data Structures ser = pd.Series([-1.5, 0.2, None], dtype=\\"float32[pyarrow]\\") idx = pd.Index([True, None], dtype=\\"bool[pyarrow]\\") df = pd.DataFrame([[1, 2], [3, 4]], dtype=\\"uint64[pyarrow]\\") # 2. Perform Operations mean_series = ser.mean() index_is_true = idx == True sum_dataframe = (df[0] + df[1]).tolist() # 3. Efficient Data Reading csv_data = StringIO(a,b,c 1,2.5,True 3,4.5,False ) read_csv = pd.read_csv(csv_data, engine=\\"pyarrow\\", dtype_backend=\\"pyarrow\\") return { \\"mean_series\\": mean_series, \\"index_is_true\\": index_is_true.tolist(), \\"sum_dataframe\\": sum_dataframe, \\"read_csv\\": read_csv } ``` Your implementation should match this example\'s output format.","solution":"def pandas_with_pyarrow_operations() -> dict: import pandas as pd import pyarrow as pa from io import StringIO # 1. Create Data Structures ser = pd.Series([-1.5, 0.2, None], dtype=\\"float32[pyarrow]\\") idx = pd.Index([True, None], dtype=\\"bool[pyarrow]\\") df = pd.DataFrame({\\"A\\": [1, 3], \\"B\\": [2, 4]}, dtype=\\"uint64[pyarrow]\\") # 2. Perform Operations mean_series = ser.mean() index_is_true = idx.isin([True]) sum_dataframe = (df[\\"A\\"] + df[\\"B\\"]).tolist() # 3. Efficient Data Reading csv_data = StringIO(a,b,c 1,2.5,True 3,4.5,False ) read_csv = pd.read_csv(csv_data, engine=\\"pyarrow\\") return { \\"mean_series\\": mean_series, \\"index_is_true\\": index_is_true.tolist(), \\"sum_dataframe\\": sum_dataframe, \\"read_csv\\": read_csv }"},{"question":"Create a custom HTTP server using Python\'s `http.server` module to handle HTTP GET and POST requests. # Requirements: 1. Implement a subclass of `BaseHTTPRequestHandler` named `CustomHTTPRequestHandler`. 2. The `CustomHTTPRequestHandler` class should handle HTTP GET and POST requests: - For GET requests, respond with a simple HTML form that allows the user to submit data via a POST request. - For POST requests, read the data submitted from the form and respond with a confirmation message that includes the submitted data. 3. Set up the server to use the `CustomHTTPRequestHandler` class. 4. The server should run on port `8080`. # Constraints: - Ensure the server handles requests efficiently and closes connections appropriately. - Validate the POST request data and handle any potential errors gracefully. # Example Usage: When a user makes a GET request to the server (`http://localhost:8080`), they should see an HTML form similar to this: ```html <!doctype html> <html> <body> <form action=\\"/\\" method=\\"POST\\"> <label for=\\"name\\">Name:</label> <input type=\\"text\\" id=\\"name\\" name=\\"name\\"> <input type=\\"submit\\" value=\\"Submit\\"> </form> </body> </html> ``` When the form is submitted, the server should respond with a confirmation message containing the submitted data, for example: ``` Received POST request: Name: John Doe ``` # Expected Input and Output: Submitting a GET request to the server **URL:** `http://localhost:8080` **Method:** `GET` **Response:** ``` <!doctype html> <html> <body> <form action=\\"/\\" method=\\"POST\\"> <label for=\\"name\\">Name:</label> <input type=\\"text\\" id=\\"name\\" name=\\"name\\"> <input type=\\"submit\\" value=\\"Submit\\"> </form> </body> </html> ``` Submitting a POST request to the server **URL:** `http://localhost:8080` **Method:** `POST` **Form Data:** ``` name=John Doe ``` **Response:** ``` Received POST request: Name: John Doe ``` # Implementation Details: - Use the `BaseHTTPRequestHandler` class to create the custom request handler. - Use the `HTTPServer` class to set up and run the server. # Submission: Submit your implementation of the `CustomHTTPRequestHandler` class and the code to set up and run the server. ```python import http.server from urllib.parse import parse_qs from http.server import HTTPServer, BaseHTTPRequestHandler class CustomHTTPRequestHandler(BaseHTTPRequestHandler): def do_GET(self): form_html = \'\'\' <!doctype html> <html> <body> <form action=\\"/\\" method=\\"POST\\"> <label for=\\"name\\">Name:</label> <input type=\\"text\\" id=\\"name\\" name=\\"name\\"> <input type=\\"submit\\" value=\\"Submit\\"> </form> </body> </html> \'\'\' self.send_response(200) self.send_header(\'Content-type\', \'text/html\') self.end_headers() self.wfile.write(form_html.encode(\'utf-8\')) def do_POST(self): content_length = int(self.headers[\'Content-Length\']) post_data = self.rfile.read(content_length).decode(\'utf-8\') post_params = parse_qs(post_data) name = post_params.get(\'name\', [\'\'])[0] response = f\'\'\' Received POST request:<br> Name: {name} \'\'\' self.send_response(200) self.send_header(\'Content-type\', \'text/html\') self.end_headers() self.wfile.write(response.encode(\'utf-8\')) def run(server_class=HTTPServer, handler_class=CustomHTTPRequestHandler, port=8080): server_address = (\'\', port) httpd = server_class(server_address, handler_class) print(f\'Serving at port {port}\') httpd.serve_forever() if __name__ == \\"__main__\\": run() ```","solution":"import http.server from urllib.parse import parse_qs from http.server import HTTPServer, BaseHTTPRequestHandler class CustomHTTPRequestHandler(BaseHTTPRequestHandler): def do_GET(self): form_html = \'\'\' <!doctype html> <html> <body> <form action=\\"/\\" method=\\"POST\\"> <label for=\\"name\\">Name:</label> <input type=\\"text\\" id=\\"name\\" name=\\"name\\"> <input type=\\"submit\\" value=\\"Submit\\"> </form> </body> </html> \'\'\' self.send_response(200) self.send_header(\'Content-type\', \'text/html\') self.end_headers() self.wfile.write(form_html.encode(\'utf-8\')) def do_POST(self): content_length = int(self.headers[\'Content-Length\']) post_data = self.rfile.read(content_length).decode(\'utf-8\') post_params = parse_qs(post_data) name = post_params.get(\'name\', [\'\'])[0] response = f\'\'\' <!doctype html> <html> <body> <p>Received POST request:</p> <p>Name: {name}</p> </body> </html> \'\'\' self.send_response(200) self.send_header(\'Content-type\', \'text/html\') self.end_headers() self.wfile.write(response.encode(\'utf-8\')) def run(server_class=HTTPServer, handler_class=CustomHTTPRequestHandler, port=8080): server_address = (\'\', port) httpd = server_class(server_address, handler_class) print(f\'Serving at port {port}\') httpd.serve_forever() if __name__ == \\"__main__\\": run()"},{"question":"You need to develop a function that reads a Sun AU audio file, extracts specific details about the audio, and then writes a new Sun AU audio file with modified parameters. # Objective 1. Read an input Sun AU audio file. 2. Extract the following details: - Number of Channels - Sample Width - Frame Rate - Number of Frames - Compression Type 3. Write a new Sun AU audio file with modified frame rate (double the original frame rate) using the extracted details and the original audio data. # Function Signature ```python def process_and_modify_sunau(input_file: str, output_file: str) -> None: pass ``` # Input - `input_file` (str): The path to the input Sun AU audio file. - `output_file` (str): The path where the new Sun AU audio file will be saved. # Output - None (The function should save the new audio file to the specified `output_file` path.) # Constraints & Requirements - Use the `sunau` module to handle reading and writing of the Sun AU files. - Only change the frame rate to be double the original frame rate; other parameters should remain the same. - Handle exceptions and ensure files are properly closed after operations. - Assume the `input_file` is a valid Sun AU file format. # Example Usage ```python # Before running the function, you have an input AU file at \'input.au\' process_and_modify_sunau(\'input.au\', \'output.au\') # After running the function, the \'output.au\' file should exist with the modified frame rate. ``` # Notes - Pay attention to handling the file modes correctly (`\'r\'` and `\'w\'`). - Maintain the integrity of the audio data while performing modifications.","solution":"import sunau def process_and_modify_sunau(input_file: str, output_file: str) -> None: Read an input Sun AU audio file, extract its details, and write a new Sun AU audio file with modified frame rate (double the original frame rate). :param input_file: str - Path to the input Sun AU audio file :param output_file: str - Path where the new Sun AU audio file will be saved try: with sunau.open(input_file, \'r\') as input_au: n_channels = input_au.getnchannels() sample_width = input_au.getsampwidth() frame_rate = input_au.getframerate() n_frames = input_au.getnframes() comp_type = input_au.getcomptype() comp_name = input_au.getcompname() audio_data = input_au.readframes(n_frames) # Update the frame rate to double the original frame rate new_frame_rate = frame_rate * 2 with sunau.open(output_file, \'w\') as output_au: output_au.setnchannels(n_channels) output_au.setsampwidth(sample_width) output_au.setframerate(new_frame_rate) output_au.setnframes(n_frames) output_au.setcomptype(comp_type, comp_name) output_au.writeframes(audio_data) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Title: Implement a Priority Queue Scheduler Using `heapq` Module** **Objective:** Design and implement a priority queue-based task scheduler that can add tasks, update their priorities, delete tasks, and fetch tasks based on priority. **Description:** You are to implement a class `PriorityQueueScheduler` using the `heapq` module from Python. This class will manage tasks using a priority queue where each task has a priority. The tasks with the lowest priorities should be retrieved first. **Class Methods to Implement:** 1. **`add_task(task: str, priority: int) -> None`**: - Adds a new task with the given priority to the scheduler. - If the task already exists, update its priority. 2. **`remove_task(task: str) -> None`**: - Removes the specified task from the scheduler. - If the task does not exist, raise a `KeyError`. 3. **`pop_task() -> str`**: - Fetches and removes the task with the lowest priority. - Returns the task. - If the queue is empty, raise a `KeyError`. **Performance Requirements:** - All methods should be efficient, leveraging the properties of a heap. - Consider operations for maintaining the heap invariant. **Example Usage:** ```python scheduler = PriorityQueueScheduler() scheduler.add_task(\'task1\', priority=5) scheduler.add_task(\'task2\', priority=1) scheduler.add_task(\'task3\', priority=3) print(scheduler.pop_task()) # Output: \'task2\' scheduler.remove_task(\'task1\') scheduler.add_task(\'task4\', priority=2) print(scheduler.pop_task()) # Output: \'task4\' print(scheduler.pop_task()) # Output: \'task3\' ``` **Constraints:** - Task names are unique and represented as non-empty strings. - Priority is an integer where a lower number indicates a higher priority. - The scheduler should handle a large number of tasks efficiently. **Hint:** Use a counter to handle the order of insertion and ensure sort stability when two tasks have the same priority. # **Solution Outline:** ```python from heapq import heappush, heappop import itertools class PriorityQueueScheduler: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = itertools.count() # unique sequence count def add_task(self, task: str, priority: int) -> None: if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heappush(self.pq, entry) def remove_task(self, task: str) -> None: entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> str: while self.pq: priority, count, task = heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') ``` Ensure your implementation passes all the provided example usages and handles edge cases appropriately.","solution":"from heapq import heappush, heappop import itertools class PriorityQueueScheduler: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = itertools.count() # unique sequence count def add_task(self, task: str, priority: int) -> None: Add a new task or update the priority of an existing task. if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heappush(self.pq, entry) def remove_task(self, task: str) -> None: Remove an existing task from the scheduler. if task not in self.entry_finder: raise KeyError(f\'Task {task} not found\') entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> str: Remove and return the lowest priority task. while self.pq: priority, count, task = heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"Objective: The goal of this exercise is to demonstrate your understanding of the Seaborn package, specifically using the `seaborn.objects` module to create and customize plots. Problem Statement: You are given two datasets: `dowjones` and `fmri`, which can be loaded using the `seaborn.load_dataset` function. Your task is to write a function `create_custom_plot` that generates a specific composite plot according to the requirements listed below. Function Signature: ```python import seaborn.objects as so def create_custom_plot(): pass ``` Requirements: 1. **Plot 1**: For the `dowjones` dataset: - Create a basic line plot showing the `Price` over `Date`. 2. **Plot 2**: For the `fmri` dataset: - Filter the data to include only rows where `region` is \\"parietal\\" and `event` is \\"stim\\". - Create a line plot showing `signal` over `timepoint` for each `subject`. - Use a light gray line color (`color=\\".2\\"`) and set the `linewidth` to 1. 3. **Overlaying Plot**: For the same filtered `fmri` dataset used in Plot 2: - Create a combined line and error band plot for `signal` over `timepoint`. - Map `region` to color and `event` to linestyle. - Show an aggregated line plot with `so.Agg()` to aggregate the data. - Add an error band using `so.Band()` to indicate the estimation variability, grouped by `event`. - Annotate the plot with markers at each data point using `marker=\\"o\\"` and `edgecolor=\\"w\\"`. 4. Each plot should be displayed in a subplot of a single figure to compare them side by side. Expected Output: - The function `create_custom_plot` should not return anything. It should display the plots as described when called. - Ensure the plots are appropriately labeled and readable. Constraints: - Use only the Seaborn `so.Plot` and its associated functionalities introduced in the documentation. - Assume the datasets `dowjones` and `fmri` are loaded within the function. # Example: ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_custom_plot(): # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Create subplots fig, axes = plt.subplots(1, 2, figsize=(15, 7)) # Plot 1: Dowjones Line Plot so.Plot(dowjones, \\"Date\\", \\"Price\\").add(so.Line()).on(axes[0]) axes[0].set_title(\\"Dowjones Price Over Time\\") # Plot 2: Filtered fmri Line Plot per Subject fmri_filtered = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") ( fmri_filtered .pipe(so.Plot, \\"timepoint\\", \\"signal\\") .add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") .on(axes[1]) ) axes[1].set_title(\\"FMRI Signal Over Time (Filtered)\\") # Overlaying Plot: Aggregated Line and Error Band p = so.Plot(fmri_filtered, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") ( p.add(so.Line(), so.Agg()) .add(so.Band(), so.Est(), group=\\"event\\") .add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) .on(axes[1]) ) # Display the figure plt.show() # Example function call create_custom_plot() ```","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_custom_plot(): # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Create subplots fig, axes = plt.subplots(1, 2, figsize=(16, 8)) # Plot 1: Dowjones Line Plot so.Plot(dowjones, \\"Date\\", \\"Price\\").add(so.Line()).on(axes[0]) axes[0].set_title(\\"Dowjones Price Over Time\\") # Plot 2: Filtered fmri Line Plot per Subject fmri_filtered = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") ( so.Plot(fmri_filtered, \\"timepoint\\", \\"signal\\") .add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") .on(axes[1]) ) # Overlaying Plot: Aggregated Line and Error Band p = so.Plot(fmri_filtered, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") ( p.add(so.Line(), so.Agg()) .add(so.Band(), so.Est(), group=\\"event\\") .add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) .on(axes[1]) ) axes[1].set_title(\\"FMRI Signal Over Time (Filtered)\\") # Display the figure plt.tight_layout() plt.show()"},{"question":"Question # Background Asynchronous programming in Python often involves handling various exceptions to ensure the application runs smoothly even when errors occur. The `asyncio` module provides several custom exceptions meant to handle specific scenarios that could arise during asynchronous operations. # Task Write a Python function `handle_asyncio_exceptions` that simulates the behavior of an asynchronous operation which can raise any of `asyncio.TimeoutError`, `asyncio.CancelledError`, `asyncio.InvalidStateError`, `asyncio.SendfileNotAvailableError`, `asyncio.IncompleteReadError`, or `asyncio.LimitOverrunError` exceptions. Your function should handle these exceptions appropriately and return meaningful messages for each exception type. # Function Signature ```python import asyncio async def handle_asyncio_exceptions(operation) -> str: # Your implementation here ``` # Input - `operation` (Callable): An asynchronous callable that represents the operation which might throw one of the specified exceptions. # Output - `str`: A message indicating which exception was caught. # Exception Messages - `\\"Timeout occurred\\"` - `\\"Operation cancelled\\"` - `\\"Invalid state in task or future\\"` - `\\"Sendfile syscall not available\\"` - `\\"Incomplete read operation\\"` - `\\"Buffer size limit exceeded\\"` # Constraints - You can expect that `operation` will always be a valid asynchronous callable. - Handle each exception individually and provide the correct message as specified above. # Example ```python import asyncio async def example_operation(): raise asyncio.TimeoutError async def handle_asyncio_exceptions(operation) -> str: try: await operation() except asyncio.TimeoutError: return \\"Timeout occurred\\" except asyncio.CancelledError: return \\"Operation cancelled\\" except asyncio.InvalidStateError: return \\"Invalid state in task or future\\" except asyncio.SendfileNotAvailableError: return \\"Sendfile syscall not available\\" except asyncio.IncompleteReadError: return \\"Incomplete read operation\\" except asyncio.LimitOverrunError: return \\"Buffer size limit exceeded\\" # Testing the function async def test(): result = await handle_asyncio_exceptions(example_operation) print(result) # Should print: \\"Timeout occurred\\" # To run the test asyncio.run(test()) ``` Write the `handle_asyncio_exceptions` function based on the above specifications and example. # Additional Information To test your function, create different asynchronous operations that raise the specified exceptions and verify that your function returns the correct messages for each.","solution":"import asyncio async def handle_asyncio_exceptions(operation) -> str: try: await operation() except asyncio.TimeoutError: return \\"Timeout occurred\\" except asyncio.CancelledError: return \\"Operation cancelled\\" except asyncio.InvalidStateError: return \\"Invalid state in task or future\\" except asyncio.SendfileNotAvailableError: return \\"Sendfile syscall not available\\" except asyncio.IncompleteReadError: return \\"Incomplete read operation\\" except asyncio.LimitOverrunError: return \\"Buffer size limit exceeded\\""},{"question":"You are given a simple Flask application located in a directory called `flaskr` on your development machine. Your task is to prepare this application for deployment on a production server. Specifically, you need to complete the following tasks: 1. **Build a wheel package** of the application. 2. **Install the wheel package** on a different environment. 3. **Configure a secure `SECRET_KEY`**. 4. **Serve the application using a production WSGI server** (Waitress). # Steps to Complete 1. **Build the Wheel Package:** - Use the `build` tool to create a `.whl` file for the application. - Ensure the `.whl` file is created in a `dist` directory. 2. **Install the Wheel Package:** - Transfer the created `.whl` file to a new environment (this can be a new virtual environment on the same machine for testing purposes). - Install the `.whl` file using `pip`. 3. **Configure the Secret Key:** - Generate a secure `SECRET_KEY` using Python\'s `secrets` module. - Create a `config.py` file in the instance folder of your application and set the generated `SECRET_KEY` in this file. 4. **Serve the Application using Waitress:** - Install Waitress in your new environment. - Run the application using Waitress, ensuring it serves on `http://0.0.0.0:8080`. # Input and Output Formats Input: The input will be the setup steps you perform as code. This can be broken down into four parts: 1. Commands used to build the wheel package. 2. Commands used to install the package in a new environment. 3. Python code to generate a secure `SECRET_KEY` and store it in the `config.py` file. 4. Commands used to install Waitress and run the Flask application using Waitress. Output: There is no direct output required to be printed. However, successful completion of each step will be verified by the following: 1. The wheel package should be present in the `dist` directory. 2. The package should be successfully installed in the new environment. 3. The `config.py` file should be created with a secure `SECRET_KEY`. 4. Waitress should successfully serve the application on `http://0.0.0.0:8080`. # Constraints: - Assume you already have a simple Flask application in a directory called `flaskr`. - Ensure that your solution is secure and follows best practices for a production environment. Here’s a starting point for your configuration: ```python # To be placed in .venv/var/flaskr-instance/config.py SECRET_KEY = \'<your_generated_secret_key>\' ``` Make sure to replace `<your_generated_secret_key>` with the actual secure key you generate. Good luck!","solution":"# Step 3: Generate a secure SECRET_KEY and save it in config.py import secrets import os SECRET_KEY = secrets.token_hex(16) # Generate a 32-character hex string # Create the instance directory if it doesn\'t exist os.makedirs(\'flaskr-instance\', exist_ok=True) # Write the SECRET_KEY to config.py file config_path = os.path.join(\'flaskr-instance\', \'config.py\') with open(config_path, \'w\') as config_file: config_file.write(f\'SECRET_KEY = \\"{SECRET_KEY}\\"n\')"},{"question":"**Objective:** Demonstrate your understanding of Seaborn\'s palette customization features and how to integrate those palettes in visualizations. **Problem Statement:** You are tasked with creating a customized palette and utilizing it in a Seaborn plot. Follow the steps below to achieve the desired result: 1. **Create a Custom Palette:** - Define a color palette ramp that starts from dark blue and goes to a specified color (provided as a hex code). - The palette should have exactly 10 distinct colors. 2. **Save the Palette:** - Generate and save this palette as a continuous colormap. 3. **Visualize the Palette:** - Create a horizontal bar plot using Seaborn\'s `barplot` function where each bar represents one of the colors in the palette. - Ensure the bars are annotated with their corresponding color values in hex format. **Function Specifications:** ```python def customized_palette_plot(hex_color: str) -> None: Generate a customized color palette starting from dark blue to the specified hex color, and visualize it using a horizontal bar plot. Parameters: hex_color (str): The hex code of the target color. Returns: None pass ``` # Input - **hex_color (str):** A string representing the hexadecimal color code (e.g. \\"#FF5733\\"). # Output - A plot displayed with a horizontal bar plot where bars are colored according to the generated palette and annotated with their corresponding hex values. # Constraints - The function should use `sns.dark_palette` for generating the palette. - Ensure the palette contains exactly 10 colors. - Use Matplotlib for annotations if needed. # Example ```python customized_palette_plot(\\"#FF5733\\") ``` The above call should: - Generate a discrete color palette with 10 colors from dark blue to \\"#FF5733\\". - Display a horizontal bar plot with the generated colors and appropriate annotations. **Note:** This question requires installation of Seaborn and Matplotlib libraries in your environment.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customized_palette_plot(hex_color: str) -> None: Generate a customized color palette starting from dark blue to the specified hex color, and visualize it using a horizontal bar plot. Parameters: hex_color (str): The hex code of the target color. Returns: None # Generate the palette with sns.dark_palette palette = sns.dark_palette(hex_color, n_colors=10, reverse=False) # Convert palette to hex codes for annotation hex_palette = sns.color_palette(palette).as_hex() # Create the horizontal bar plot plt.figure(figsize=(10, 6)) barplot = sns.barplot(x=list(range(10)), y=[1]*10, palette=palette, ci=None) # Annotate bars with hex values for idx, (bar, hex_color) in enumerate(zip(barplot.patches, hex_palette)): barplot.annotate( hex_color, (bar.get_x() + bar.get_width() / 2, bar.get_height() / 2), ha=\'center\', va=\'center\', color=\'white\', fontsize=12) # Hide axes details plt.gca().set_yticklabels([]) plt.gca().set_yticks([]) plt.gca().set_xticks([]) plt.xlabel(\\"\\") plt.ylabel(\\"\\") plt.title(f\\"Custom Palette from Dark Blue to {hex_color}\\") plt.show()"},{"question":"**Objective**: Demonstrate your understanding of the `glob` module by writing functions that make use of its capabilities. **Problem Statement**: You are given a root directory path `\\"root_dir\\"` and different file patterns. Your task is to implement three functions: 1. `list_all_files_with_pattern(root_dir: str, pattern: str) -> List[str]`: This function should return a list of all files in the `root_dir` that match the specified `pattern`. The list should include paths relative to `root_dir`. 2. `count_files_with_pattern(root_dir: str, pattern: str) -> int`: This function should return the count of all files in the `root_dir` that match the specified `pattern`. 3. `escape_and_list_files_with_pattern(root_dir: str, pattern: str) -> List[str]`: This function should escape the `pattern` using `glob.escape()`, and then return a list of all files in the `root_dir` that match the escaped pattern. **Input**: - `root_dir`: A string representing the path of the root directory. - `pattern`: A string representing the file pattern to be matched (wildcards like `*`, `?`, and `[]` can be used). **Output**: - For `list_all_files_with_pattern`, a list of strings where each string is a file path that matches the `pattern`. - For `count_files_with_pattern`, an integer representing the count of files that match the `pattern`. - For `escape_and_list_files_with_pattern`, a list of strings where each string is a file path that matches the escaped `pattern`. **Constraints**: - The `root_dir` will always be a valid directory path. - The `pattern` will be a non-empty string. - You can assume the directories and file structures are not deeply nested (not more than 10 levels). **Example**: ```python root_dir = \'/path/to/your/root\' pattern = \'*.txt\' # Example function calls list_of_files = list_all_files_with_pattern(root_dir, pattern) count_of_files = count_files_with_pattern(root_dir, pattern) escaped_list_of_files = escape_and_list_files_with_pattern(root_dir, pattern) # Expected outputs (the actual outputs will depend on the contents of your root directory) print(list_of_files) # e.g., [\'file1.txt\', \'subdir/file2.txt\'] print(count_of_files) # e.g., 2 print(escaped_list_of_files) # This will depend on the escaped pattern # Sample Implementation import glob from typing import List def list_all_files_with_pattern(root_dir: str, pattern: str) -> List[str]: return glob.glob(pattern, root_dir=root_dir, recursive=True) def count_files_with_pattern(root_dir: str, pattern: str) -> int: return len(glob.glob(pattern, root_dir=root_dir, recursive=True)) def escape_and_list_files_with_pattern(root_dir: str, pattern: str) -> List[str]: escaped_pattern = glob.escape(pattern) return glob.glob(escaped_pattern, root_dir=root_dir, recursive=True) ``` Your task is to implement these three functions considering the above example. **Note**: Make sure to adequately handle the case where no files match the given pattern.","solution":"import glob from typing import List def list_all_files_with_pattern(root_dir: str, pattern: str) -> List[str]: Returns a list of all files in the root_dir that match the specified pattern. return glob.glob(f\\"{root_dir}/**/{pattern}\\", recursive=True) def count_files_with_pattern(root_dir: str, pattern: str) -> int: Returns the count of all files in the root_dir that match the specified pattern. return len(list_all_files_with_pattern(root_dir, pattern)) def escape_and_list_files_with_pattern(root_dir: str, pattern: str) -> List[str]: Escapes the pattern using glob.escape, and then returns a list of all files in the root_dir that match the escaped pattern. escaped_pattern = glob.escape(pattern) return glob.glob(f\\"{root_dir}/**/{escaped_pattern}\\", recursive=True)"},{"question":"PyTorch MPS Device and Memory Management Objective: You are asked to implement a function that leverages the torch.mps module to manage multiple MPS devices, seed the random number generator for reproducibility, handle memory allocation, and perform synchronization. Function Signature: ```python import torch.mps import torch def manage_mps_devices_and_memory(seed_value: int) -> dict: Manages MPS devices and memory, sets random seeds, and synchronizes devices. Args: - seed_value (int): The seed value to use for the random number generator. Returns: - dict: A dictionary containing: - \'device_count\' (int): Number of MPS devices available. - \'current_memory\' (int): Current allocated memory on MPS devices. - \'driver_memory\' (int): Memory allocated by the driver on MPS devices. - \'recommended_memory\' (int): Recommended maximum memory usage on MPS devices. # Implement your code here pass ``` Input: - `seed_value` (int): A seed value for the RNG to ensure reproducibility. Expected Output: A dictionary containing: - `device_count` (int): The number of MPS devices available. - `current_memory` (int): The current allocated memory on MPS devices. - `driver_memory` (int): The memory allocated by the driver on MPS devices. - `recommended_memory` (int): The recommended maximum memory usage on MPS devices. Constraints: - You should make sure to use the `torch.mps` API functionalities provided for device management, memory management, and random seed setting. - Ensure proper handling of memory and synchronization for a clean execution state in a multi-device environment. Example: ```python result = manage_mps_devices_and_memory(42) print(result) # Expected Output: # { # \'device_count\': 1, # \'current_memory\': 0, # \'driver_memory\': 0, # \'recommended_memory\': 0 # } ``` --- This example assumes there\'s one MPS device available with no significant memory allocated at the time of the function call, yielding zeros for memory-related metrics. The actual values may vary depending on the execution environment and state of the devices. Note: Ensure you have the required MPS-compatible hardware and the necessary PyTorch version supporting the torch.mps backend.","solution":"import torch.mps import torch def manage_mps_devices_and_memory(seed_value: int) -> dict: Manages MPS devices and memory, sets random seeds, and synchronizes devices. Args: - seed_value (int): The seed value to use for the random number generator. Returns: - dict: A dictionary containing: - \'device_count\' (int): Number of MPS devices available. - \'current_memory\' (int): Current allocated memory on MPS devices. - \'driver_memory\' (int): Memory allocated by the driver on MPS devices. - \'recommended_memory\' (int): Recommended maximum memory usage on MPS devices. if not torch.mps.is_available(): return { \'device_count\': 0, \'current_memory\': 0, \'driver_memory\': 0, \'recommended_memory\': 0 } device_count = torch.mps.device_count() torch.manual_seed(seed_value) torch.mps.manual_seed_all(seed_value) current_memory = torch.mps.current_allocated_memory() driver_memory = torch.mps.driver_allocated_memory() recommended_memory = torch.mps.recommended_max_memory_usage() torch.mps.synchronize() return { \'device_count\': device_count, \'current_memory\': current_memory, \'driver_memory\': driver_memory, \'recommended_memory\': recommended_memory }"},{"question":"# Problem: Document Analysis using Regex You are given a large text document, and your task is to implement a function that extracts and processes specific information from it using regular expressions. Requirements 1. **Find all dates:** Extract all dates present in the format `DD-MM-YYYY` or `DD/MM/YYYY`. There should be exactly two digits for days and months and four digits for years. 2. **Identify and categorize email addresses:** Extract all email addresses and categorize them based on their domain (e.g., Gmail, Yahoo, etc.). 3. **Extract phone numbers:** Find all phone numbers in one of the two formats: `(XXX) XXX-XXXX` or `XXX-XXX-XXXX`. 4. **Extract and bold specific keywords:** Given a list of specific keywords, find these keywords in the document and return the text with these keywords wrapped in `**` (for bolding, as in markdown). Input and Output Formats - **Input:** - `document`: A string representing the text document. - `keywords`: A list of strings representing the keywords to be bolded. - **Output:** - `dates`: A list of strings containing all extracted dates. - `email_dict`: A dictionary with email domains as keys and lists of corresponding emails as values. - `phone_numbers`: A list of strings containing all extracted phone numbers. - `processed_document`: A string where all keywords are wrapped in `**`. Constraints - The document can be very large. - The formats for dates and phone numbers are strictly as specified. - Keywords may appear multiple times in the document. Example ```python document = Contact us at support@example.com or sales@example.org. Visit us on 05-11-2023 or 06/12/2022 for the best deals. For queries, call (123) 456-7890 or 987-654-3210. Special offer for Gmail users: hello@gmail.com, test@gmail.com. Notice: All events are scheduled for 25-12-2023 and 01/01/2024. keywords = [\\"support\\", \\"deals\\", \\"Gmail\\", \\"offer\\", \\"events\\"] dates, email_dict, phone_numbers, processed_document = analyze_document(document, keywords) print(dates) # Output: # [\'05-11-2023\', \'06/12/2022\', \'25-12-2023\', \'01/01/2024\'] print(email_dict) # Output: # {\'example.com\': [\'support@example.com\'], # \'example.org\': [\'sales@example.org\'], # \'gmail.com\': [\'hello@gmail.com\', \'test@gmail.com\']} print(phone_numbers) # Output: # [\'(123) 456-7890\', \'987-654-3210\'] print(processed_document) # Output: # # Contact us at **support**@example.com or sales@example.org. # Visit us on 05-11-2023 or 06/12/2022 for the best **deals**. # For queries, call (123) 456-7890 or 987-654-3210. # Special **offer** for **Gmail** users: hello@gmail.com, test@gmail.com. # Notice: All **events** are scheduled for 25-12-2023 and 01/01/2024. # ``` Function Signature ```python def analyze_document(document: str, keywords: List[str]) -> Tuple[List[str], Dict[str, List[str]], List[str], str]: pass # Your solution here ```","solution":"import re from typing import List, Tuple, Dict def analyze_document(document: str, keywords: List[str]) -> Tuple[List[str], Dict[str, List[str]], List[str], str]: # Find all dates in DD-MM-YYYY or DD/MM/YYYY format date_pattern = re.compile(r\'bd{2}[-/]d{2}[-/]d{4}b\') dates = date_pattern.findall(document) # Extract all email addresses and categorize by domain email_pattern = re.compile(r\'b[w.-]+@[w.-]+.w+b\') emails = email_pattern.findall(document) email_dict = {} for email in emails: domain = email.split(\'@\')[1] if domain not in email_dict: email_dict[domain] = [] email_dict[domain].append(email) # Find all phone numbers in (XXX) XXX-XXXX or XXX-XXX-XXXX format phone_pattern = re.compile(r\'(d{3}) d{3}-d{4}|d{3}-d{3}-d{4}\') phone_numbers = phone_pattern.findall(document) # Wrap specific keywords in ** for bolding def bold_keywords(match): return f\'**{match.group(0)}**\' for keyword in keywords: keyword_pattern = re.compile(rf\'b{re.escape(keyword)}b\') document = keyword_pattern.sub(bold_keywords, document) return dates, email_dict, phone_numbers, document"},{"question":"You are provided with the \'mpg\' dataset, which contains data about various car models, including their weight, horsepower, and fuel efficiency (miles-per-gallon). You need to use seaborn to analyze the residuals of linear regression models and identify any underlying structures in the data. Task 1. Write a function `analyze_residuals` that performs the following steps: - Load the \'mpg\' dataset from seaborn. - Plot and save the residuals of the simple linear regression models between: * `weight` and `displacement` * `horsepower` and `mpg` - Plot and save the residuals with higher-order trends for `horsepower` and `mpg` (order 2). - Plot and save the residuals with a LOWESS curve for `horsepower` and `mpg`. 2. Save the generated plots as: - `residuals_weight_displacement.png` - `residuals_horsepower_mpg.png` - `residuals_horsepower_mpg_order2.png` - `residuals_horsepower_mpg_lowess.png` Input - No direct input is taken by the function. Output - Four PNG files saved in the current directory as specified above. Function Signature ```python def analyze_residuals() -> None: pass ``` Constraints - Use the seaborn library for all plotting. - The plots must be saved with the correct filenames as indicated. - All steps must be included within a single function `analyze_residuals`. # Example Usage ```python analyze_residuals() # This will generate and save four residual plots as outlined in the task. ``` Make sure your solution adheres to the above requirements and is well-commented.","solution":"import seaborn as sns import matplotlib.pyplot as plt import statsmodels.api as sm def analyze_residuals() -> None: # Load the mpg dataset from seaborn mpg = sns.load_dataset(\'mpg\').dropna() # Plot and save the residuals of the simple linear regression model between `weight` and `displacement` sns.residplot(x=\'weight\', y=\'displacement\', data=mpg, lowess=True, color=\'blue\') plt.title(\'Residuals of Linear Regression: Weight vs Displacement\') plt.savefig(\'residuals_weight_displacement.png\') plt.close() # Plot and save the residuals of the simple linear regression model between `horsepower` and `mpg` sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg, lowess=True, color=\'green\') plt.title(\'Residuals of Linear Regression: Horsepower vs MPG\') plt.savefig(\'residuals_horsepower_mpg.png\') plt.close() # Plot and save the residuals with higher-order trends for `horsepower` and `mpg` (order 2) mpg[\'horsepower2\'] = mpg[\'horsepower\'] ** 2 X = sm.add_constant(mpg[[\'horsepower\', \'horsepower2\']]) model = sm.OLS(mpg[\'mpg\'], X).fit() mpg[\'residuals_order2\'] = model.resid sns.residplot(x=mpg[\'horsepower\'], y=mpg[\'residuals_order2\'], lowess=True, color=\'red\') plt.title(\'Residuals of Higher Order Polynomial Regression: Horsepower^2 vs MPG\') plt.savefig(\'residuals_horsepower_mpg_order2.png\') plt.close() # Plot and save the residuals with a LOWESS curve for `horsepower` and `mpg` sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg, lowess=True, color=\'purple\') plt.plot(mpg[\'horsepower\'], model.fittedvalues, color=\'black\') plt.title(\'Residuals with LOWESS Curve: Horsepower vs MPG\') plt.savefig(\'residuals_horsepower_mpg_lowess.png\') plt.close()"},{"question":"# Advanced Coding Assessment: Custom Numeric Operations Objective Implement several custom numeric operations in Python using the given API functions. The goal is to precisely mimic Python\'s built-in arithmetic operations, but with extra logging for debugging and tracing computation steps. Task You are required to design a Python class `CustomNumber` that uses the provided numeric functions to perform arithmetic operations. Each method should log detailed information about the operation being performed, its operands, and the result. Class Specification 1. **Class Name:** `CustomNumber` 2. **Attributes:** - `value` (can be an integer or float) 3. **Methods:** - `__add__(self, other)`: Perform addition. - `__sub__(self, other)`: Perform subtraction. - `__mul__(self, other)`: Perform multiplication. - `__matmul__(self, other)`: Perform matrix multiplication. - `__floordiv__(self, other)`: Perform floor division. - `__truediv__(self, other)`: Perform true division. - `__mod__(self, other)`: Perform modulus. - `__pow__(self, other, modulo=None)`: Perform power operation. - `__neg__(self)`: Perform negation. - `__abs__(self)`: Return the absolute value. - `__lshift__(self, other)`: Perform left shift. - `__rshift__(self, other)`: Perform right shift. - `__and__(self, other)`: Perform bitwise AND. - `__xor__(self, other)`: Perform bitwise XOR. - `__or__(self, other)`: Perform bitwise OR. All the methods should use corresponding API functions where applicable, and log each step of their computation. Expected Input and Output - **Input:** Each arithmetic method will take `self` and `other` as inputs where `other` is either an integer or float. - **Output:** Returns a new `CustomNumber` instance with the result of the operation. Constraints 1. `self.value` and `other` are guaranteed to be numeric. 2. Ensure proper exception handling for cases where operations could fail (e.g., division by zero). 3. Each operation must log the following information: - Operation type (e.g., \\"Addition\\", \\"Subtraction\\", etc.) - Operands involved - Result of the operation or an error message if the operation fails Example Usage ```python class CustomNumber: def __init__(self, value): self.value = value def __add__(self, other): # Use PyNumber_Add to perform the addition and log the details pass # Implement other methods similarly # Usage n1 = CustomNumber(10) n2 = CustomNumber(5) result = n1 + n2 result = n1 - n2 result = n1 * n2 result = n1 @ n2 # for matrix multiplication result = n1 // n2 result = n1 / n2 result = n1 % n2 result = n1 ** 2 print(abs(n1)) print(-n2) print(n1 << 2) print(n1 >> 1) print(n1 & n2) print(n1 ^ n2) print(n1 | n2) ``` Make sure your implementation follows the guidelines and handles errors gracefully. Logging Example For addition operation: - **Log:** \\"Operation: Addition, Operands: 10 and 5, Result: 15\\" For division where division by zero might occur: - **Log:** \\"Operation: Division, Operands: 10 and 0, Result: Error - Division by Zero\\" # Submission Guidelines Submit your Python class implementation along with a test suite that thoroughly tests each method in various scenarios, ensuring proper logging and error handling.","solution":"import logging class CustomNumber: def __init__(self, value): self.value = value logging.basicConfig(level=logging.DEBUG) def __log(self, operation, other, result): logging.debug(f\\"Operation: {operation}, Operands: {self.value} and {other.value if isinstance(other, CustomNumber) else other}, Result: {result}\\") def __add__(self, other): result = self.value + other.value if isinstance(other, CustomNumber) else self.value + other self.__log(\\"Addition\\", other, result) return CustomNumber(result) def __sub__(self, other): result = self.value - other.value if isinstance(other, CustomNumber) else self.value - other self.__log(\\"Subtraction\\", other, result) return CustomNumber(result) def __mul__(self, other): result = self.value * other.value if isinstance(other, CustomNumber) else self.value * other self.__log(\\"Multiplication\\", other, result) return CustomNumber(result) def __matmul__(self, other): result = self.value @ other.value if isinstance(other, CustomNumber) else self.value @ other self.__log(\\"Matrix Multiplication\\", other, result) return CustomNumber(result) def __floordiv__(self, other): try: result = self.value // other.value if isinstance(other, CustomNumber) else self.value // other self.__log(\\"Floor Division\\", other, result) except ZeroDivisionError as e: result = \\"Error - Division by Zero\\" self.__log(\\"Floor Division\\", other, result) raise e return CustomNumber(result) def __truediv__(self, other): try: result = self.value / other.value if isinstance(other, CustomNumber) else self.value / other self.__log(\\"True Division\\", other, result) except ZeroDivisionError as e: result = \\"Error - Division by Zero\\" self.__log(\\"True Division\\", other, result) raise e return CustomNumber(result) def __mod__(self, other): result = self.value % other.value if isinstance(other, CustomNumber) else self.value % other self.__log(\\"Modulus\\", other, result) return CustomNumber(result) def __pow__(self, other, modulo=None): result = pow(self.value, other.value if isinstance(other, CustomNumber) else other, modulo) self.__log(\\"Power\\", other, result) return CustomNumber(result) def __neg__(self): result = -self.value self.__log(\\"Negation\\", self, result) return CustomNumber(result) def __abs__(self): result = abs(self.value) self.__log(\\"Absolute Value\\", self, result) return CustomNumber(result) def __lshift__(self, other): result = self.value << other.value if isinstance(other, CustomNumber) else self.value << other self.__log(\\"Left Shift\\", other, result) return CustomNumber(result) def __rshift__(self, other): result = self.value >> other.value if isinstance(other, CustomNumber) else self.value >> other self.__log(\\"Right Shift\\", other, result) return CustomNumber(result) def __and__(self, other): result = self.value & other.value if isinstance(other, CustomNumber) else self.value & other self.__log(\\"Bitwise AND\\", other, result) return CustomNumber(result) def __xor__(self, other): result = self.value ^ other.value if isinstance(other, CustomNumber) else self.value ^ other self.__log(\\"Bitwise XOR\\", other, result) return CustomNumber(result) def __or__(self, other): result = self.value | other.value if isinstance(other, CustomNumber) else self.value | other self.__log(\\"Bitwise OR\\", other, result) return CustomNumber(result)"},{"question":"**Objective**: Demonstrate your understanding of the Python `wave` module by writing a function that combines specific audio processing functionalities. **Problem Statement**: Write a function `process_wav_file(input_filename: str, output_filename: str, scale_factor: float, reverse: bool) -> None` that: 1. Reads an existing WAV file specified by `input_filename`. 2. Scales the audio samples by the factor specified in `scale_factor`. (If the scale factor is greater than 1, it amplifies the audio; if it is between 0 and 1, it attenuates the audio.) 3. If `reverse` is `True`, reverses the order of the audio frames. 4. Writes the modified audio data to a new WAV file specified by `output_filename`. **Input/Output**: - **input_filename**: A string representing the path to the input WAV file. - **output_filename**: A string representing the path to the output WAV file. - **scale_factor**: A floating-point number used to scale the audio samples. - **reverse**: A boolean indicating whether or not to reverse the audio frames. **Constraints**: - The WAV file will use `WAVE_FORMAT_PCM`. - The function should handle mono and stereo files properly. - The function should ensure the output file closely mimics the parameters of the input file (e.g., number of channels, sample width, frame rate). ```python import wave import struct def process_wav_file(input_filename: str, output_filename: str, scale_factor: float, reverse: bool) -> None: # Open the input WAV file with wave.open(input_filename, \'rb\') as wav_in: n_channels = wav_in.getnchannels() samp_width = wav_in.getsampwidth() frame_rate = wav_in.getframerate() n_frames = wav_in.getnframes() comp_type = wav_in.getcomptype() comp_name = wav_in.getcompname() # Read all frames from the input file frames = wav_in.readframes(n_frames) # Unpack the frames based on sample width fmt = {1: \'B\', 2: \'h\', 4: \'i\'}.get(samp_width) # Map byte width to format character if not fmt: raise ValueError(\\"Unsupported sample width\\") fmt = f\'{len(frames)//samp_width}{fmt}\' # Construct the format string frames_unpacked = struct.unpack(fmt, frames) # Scale the audio samples by the given factor scaled_frames = [int(sample * scale_factor) for sample in frames_unpacked] # Correct potential overflow issues based on the sample width max_val = (1 << (8 * samp_width - 1)) - 1 min_val = -(1 << (8 * samp_width - 1)) scaled_frames = [max(min(sample, max_val), min_val) for sample in scaled_frames] if reverse: # Reverse the frames scaled_frames.reverse() # Pack the frames back into bytes scaled_frames_packed = struct.pack(fmt, *scaled_frames) # Write the processed frames to the output WAV file with wave.open(output_filename, \'wb\') as wav_out: wav_out.setnchannels(n_channels) wav_out.setsampwidth(samp_width) wav_out.setframerate(frame_rate) wav_out.setnframes(n_frames) wav_out.setcomptype(comp_type, comp_name) wav_out.writeframes(scaled_frames_packed) ``` **Testing:** You should test this function with various `WAV` files to ensure it correctly reads, processes, and writes the files according to the specified parameters. Consider edge cases such as different sample widths, number of channels, and scale factors.","solution":"import wave import struct def process_wav_file(input_filename: str, output_filename: str, scale_factor: float, reverse: bool) -> None: # Open the input WAV file with wave.open(input_filename, \'rb\') as wav_in: n_channels = wav_in.getnchannels() samp_width = wav_in.getsampwidth() frame_rate = wav_in.getframerate() n_frames = wav_in.getnframes() comp_type = wav_in.getcomptype() comp_name = wav_in.getcompname() # Read all frames from the input file frames = wav_in.readframes(n_frames) # Unpack the frames based on sample width fmt = {1: \'B\', 2: \'h\', 4: \'i\'}.get(samp_width) # Map byte width to format character if not fmt: raise ValueError(\\"Unsupported sample width\\") fmt = f\'<{len(frames)//samp_width}{fmt}\' # Construct the format string frames_unpacked = struct.unpack(fmt, frames) # Scale the audio samples by the given factor scaled_frames = [int(sample * scale_factor) for sample in frames_unpacked] # Correct potential overflow issues based on the sample width max_val = (1 << (8 * samp_width - 1)) - 1 min_val = -(1 << (8 * samp_width - 1)) scaled_frames = [max(min(sample, max_val), min_val) for sample in scaled_frames] if reverse: # Reverse the frames scaled_frames.reverse() # Pack the frames back into bytes scaled_frames_packed = struct.pack(fmt, *scaled_frames) # Write the processed frames to the output WAV file with wave.open(output_filename, \'wb\') as wav_out: wav_out.setnchannels(n_channels) wav_out.setsampwidth(samp_width) wav_out.setframerate(frame_rate) wav_out.setnframes(n_frames) wav_out.setcomptype(comp_type, comp_name) wav_out.writeframes(scaled_frames_packed)"},{"question":"Objective The objective of this assessment is to evaluate your understanding of Kernel Ridge Regression (KRR) and your ability to implement and optimize it using scikit-learn. Problem Statement You are given a dataset with a sinusoidal target function and added noise. Your task is to implement and optimize a Kernel Ridge Regression model using scikit-learn. Dataset The dataset consists of input features `X` and corresponding target values `y`. The input features are given as a 1D array of float values. The target `y` is generated from a sinusoidal function with added Gaussian noise at every fifth data point. Instructions 1. **Load the dataset:** - Write a function `load_dataset` to create the dataset with `n_samples` data points. - This function should return `X` and `y`. 2. **Build the KRR model:** - Write a function `build_krr_model` that accepts the input arrays `X_train` and `y_train`, and fits a Kernel Ridge Regression model on them. - Use an RBF kernel for the model. 3. **Optimize the model:** - Use grid search to optimize the parameters `alpha` (regularization strength) and `gamma` (bandwidth parameter for the RBF kernel). - Write a function `optimize_krr_model` that performs this grid search and returns the best estimator. 4. **Evaluation:** - Write a function `evaluate_model` that accepts the trained model, test data `X_test`, and corresponding labels `y_test`, and returns the mean squared error (MSE) on the test set. Requirements - Use `KernelRidge` from `sklearn.kernel_ridge` for implementing KRR. - Use `GridSearchCV` from `sklearn.model_selection` for parameter optimization. - The dataset should have `n_samples=100` data points for training and `n_samples=50` data points for testing. - The grid search should consider at least the following sets of parameters: - `alpha`: [1e-2, 1e-1, 1, 10] - `gamma`: [1e-2, 1e-1, 1, 10] Function Signatures ```python def load_dataset(n_samples: int) -> Tuple[np.ndarray, np.ndarray]: Create and return a dataset with a sinusoidal target function and added noise. Parameters: - n_samples (int): Number of samples to generate. Returns: - X (np.ndarray): Input features, shape (n_samples,). - y (np.ndarray): Target values, shape (n_samples,). pass def build_krr_model(X_train: np.ndarray, y_train: np.ndarray) -> KernelRidge: Build and train a Kernel Ridge Regression model using RBF kernel. Parameters: - X_train (np.ndarray): Training input features, shape (n_samples,). - y_train (np.ndarray): Training target values, shape (n_samples,). Returns: - model (KernelRidge): Trained Kernel Ridge Regression model. pass def optimize_krr_model(X_train: np.ndarray, y_train: np.ndarray) -> KernelRidge: Optimize the KRR model using grid search and return the best estimator. Parameters: - X_train (np.ndarray): Training input features, shape (n_samples,). - y_train (np.ndarray): Training target values, shape (n_samples,). Returns: - best_model (KernelRidge): Best estimator after grid search. pass def evaluate_model(model: KernelRidge, X_test: np.ndarray, y_test: np.ndarray) -> float: Evaluate the model on test data and return the mean squared error (MSE). Parameters: - model (KernelRidge): Trained model. - X_test (np.ndarray): Test input features, shape (n_samples,). - y_test (np.ndarray): Test target values, shape (n_samples,). Returns: - mse (float): Mean squared error on the test set. pass ``` Example Usage ```python # Load and prepare dataset n_train_samples = 100 n_test_samples = 50 X_train, y_train = load_dataset(n_train_samples) X_test, y_test = load_dataset(n_test_samples) # Build the KRR model model = build_krr_model(X_train, y_train) # Optimize model with grid search optimized_model = optimize_krr_model(X_train, y_train) # Evaluate the optimized model mse = evaluate_model(optimized_model, X_test, y_test) print(f\\"Mean Squared Error on Test Set: {mse}\\") ``` Good luck!","solution":"import numpy as np from typing import Tuple from sklearn.kernel_ridge import KernelRidge from sklearn.model_selection import GridSearchCV from sklearn.metrics import mean_squared_error def load_dataset(n_samples: int) -> Tuple[np.ndarray, np.ndarray]: np.random.seed(0) # for reproducibility X = np.linspace(0, 10, n_samples) y = np.sin(X) noise_idx = np.arange(0, n_samples, 5) y[noise_idx] += np.random.randn(len(noise_idx)) * 0.5 # add noise to every fifth data point return X, y def build_krr_model(X_train: np.ndarray, y_train: np.ndarray) -> KernelRidge: model = KernelRidge(kernel=\'rbf\') model.fit(X_train[:, np.newaxis], y_train) return model def optimize_krr_model(X_train: np.ndarray, y_train: np.ndarray) -> KernelRidge: param_grid = {\'alpha\': [1e-2, 1e-1, 1, 10], \'gamma\': [1e-2, 1e-1, 1, 10]} grid_search = GridSearchCV(KernelRidge(kernel=\'rbf\'), param_grid, cv=5, scoring=\'neg_mean_squared_error\') grid_search.fit(X_train[:, np.newaxis], y_train) return grid_search.best_estimator_ def evaluate_model(model: KernelRidge, X_test: np.ndarray, y_test: np.ndarray) -> float: y_pred = model.predict(X_test[:, np.newaxis]) mse = mean_squared_error(y_test, y_pred) return mse"},{"question":"# URL Content Fetcher and Parser Objective You are required to implement a function that: 1. Reads content from a specified URL. 2. Handles any errors that might occur during the fetching of the URL. 3. Parses the fetched content to extract all hyperlinks (hrefs) from the HTML content. Function Signature ```python def fetch_and_parse_url(url: str) -> list: pass ``` Input * `url` (str): A string representing the URL to fetch. Output * Returns a list of strings, where each string is a hyperlink (href) found within the HTML content of the page. Constraints * The function should only fetch HTTP and HTTPS URLs. * Implement error handling to manage common issues like `HTTPError` and `URLError`. * Ensure that the function completes within a reasonable time frame. Details 1. Utilize `urllib.request` to open and read the URL. 2. Catch and handle exceptions using `urllib.error`. 3. Use `urllib.parse` for any required parsing tasks. 4. Process the HTML content to extract hyperlink references (hrefs). Example ```python # Example HTML Content: # <html> # <body> # <a href=\\"http://example.com/page1\\">Page 1</a> # <a href=\\"http://example.com/page2\\">Page 2</a> # </body> # </html> # Example Usage: urls = fetch_and_parse_url(\\"http://example.com\\") # Expected Output: # [\\"http://example.com/page1\\", \\"http://example.com/page2\\"] ``` Notes * You are encouraged to use the `BeautifulSoup` library to parse the HTML, but it is not mandatory. You can use any method/library you prefer for HTML parsing. Performance Requirements * Ensure that the URL content fetching provides good performance for standard web pages. * Handle large HTML pages efficiently to extract hyperlinks without significant delay.","solution":"import urllib.request import urllib.error from bs4 import BeautifulSoup def fetch_and_parse_url(url: str) -> list: Fetches the content of the URL and parses it to extract all hyperlinks. Args: url (str): The URL to fetch. Returns: list: A list of hyperlinks (hrefs) found within the HTML content of the page. try: response = urllib.request.urlopen(url) html_content = response.read() soup = BeautifulSoup(html_content, \'html.parser\') links = [] for a_tag in soup.find_all(\'a\', href=True): links.append(a_tag[\'href\']) return links except urllib.error.HTTPError as e: print(f\\"HTTP Error occurred: {e.code}\\") return [] except urllib.error.URLError as e: print(f\\"URL Error occurred: {e.reason}\\") return [] except Exception as e: print(f\\"An unexpected error occurred: {e}\\") return []"},{"question":"You are required to implement the functionality to send an email with both plain text and HTML content and an inline image using the Python \\"email\\" package. # Task: Create a function `send_email_with_image` that: 1. Reads a plain text file and an HTML file as the content of the email. 2. Embeds an inline image within the HTML content. 3. Sends the email to a list of recipients. # Function Signature: ```python def send_email_with_image(sender: str, recipients: list, subject: str, plain_text_path: str, html_path: str, image_path: str): pass ``` # Input: 1. `sender` (str): Sender\'s email address. 2. `recipients` (list): List of recipient email addresses. 3. `subject` (str): Subject of the email. 4. `plain_text_path` (str): Path to the plain text file containing the email\'s plain text version. 5. `html_path` (str): Path to the HTML file containing the email\'s HTML version. 6. `image_path` (str): Path to the image file to be embedded within the HTML content. # Constraints: - The SMTP server used for sending the email should be \'localhost\'. - The inline image should be embedded correctly so it displays within the HTML part of the email. # Output: - The function does not return anything. It should send the email to the specified recipients. # Example: Assume the following content in \\"plain.txt\\" file: ``` Hello, This is a plain text version of the email. Regards, Your Company ``` And the following content in \\"email.html\\" file: ```html <html> <head></head> <body> <p>Hello,</p> <p>This is the HTML version of the email.</p> <img src=\\"cid:image_cid\\"> <p>Regards,<br>Your Company</p> </body> </html> ``` - `send_email_with_image(\'sender@example.com\', [\'recipient@example.com\'], \'Subject\', \'plain.txt\', \'email.html\', \'image.jpg\')` should send an email with the specified plain text, HTML content, and the embedded image. For this task, remember to properly handle the reading of the files, creating the email message with both plain text and HTML parts, embedding the image, and sending the email. # Notes: - Make sure your implementation handles potential exceptions, such as file not found, in a graceful manner. - Assume the SMTP server is set up on \'localhost\' and no authentication is needed.","solution":"import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.utils import formataddr import os def send_email_with_image(sender: str, recipients: list, subject: str, plain_text_path: str, html_path: str, image_path: str): Sends an email with both plain text and HTML content and an embedded inline image. try: # Read the plain text file with open(plain_text_path, \'r\') as plain_file: plain_text_content = plain_file.read() # Read the HTML file with open(html_path, \'r\') as html_file: html_content = html_file.read() # Create the root message and fill in the from, to, and subject headers msg = MIMEMultipart(\'related\') msg[\'From\'] = sender msg[\'To\'] = \', \'.join(recipients) msg[\'Subject\'] = subject # Create the alternative part msg_alternative = MIMEMultipart(\'alternative\') msg.attach(msg_alternative) # Attach the plain text message msg_plain = MIMEText(plain_text_content, \'plain\') msg_alternative.attach(msg_plain) # Attach the HTML message msg_html = MIMEText(html_content, \'html\') msg_alternative.attach(msg_html) # Attach the image with open(image_path, \'rb\') as img_file: msg_image = MIMEImage(img_file.read()) msg_image.add_header(\'Content-ID\', \'<image_cid>\') msg.attach(msg_image) # Send the email via local SMTP server with smtplib.SMTP(\'localhost\') as server: server.sendmail(sender, recipients, msg.as_string()) except Exception as e: print(f\\"Failed to send email: {e}\\")"},{"question":"**Question: Implement a Custom File Processor Using fileinput Module** You have been tasked with creating a custom file processor using Python’s `fileinput` module. Your task is to process multiple text files, count the occurrences of a specific word across all files, and print the cumulative line number and the filename each time the word is found. # Requirements: 1. **Function Signature**: `def word_occurrence_counter(word: str, filenames: List[str]) -> List[Tuple[int, str]]:` 2. **Input**: - `word` (str): The word to count in the files. - `filenames` (List[str]): A list of filenames to process. 3. **Output**: - A list of tuples where each tuple contains two elements: - Cumulative line number where the word is found. - The file name where the word is found. # Constraints: - You must use the `fileinput` module to handle reading lines from files. - You should handle empty files appropriately. - If the list of filenames is empty, read from `sys.stdin`. - You must maintain the order of lines across files and count the cumulative line number correctly. - The comparison should be case-insensitive (e.g., \\"Word\\" and \\"word\\" should be considered the same). # Example: ```python file1.txt content: Hello world This is a test file The word is here file2.txt content: Another file with Word And another line here Calling the function: >>> word_occurrence_counter(\\"word\\", [\\"file1.txt\\", \\"file2.txt\\"]) Output: [(1, \\"file1.txt\\"), (4, \\"file2.txt\\")] ``` # Notes: - You are required to use the following functions and methods from the `fileinput` module: - `fileinput.input()` - `fileinput.lineno()` - `fileinput.filename()` - You may use other standard Python libraries if needed. - Ensure your function performs efficiently even for large files and does not hold entire file content in memory. **Implementation Note**: Be sure to handle file operations and resource management properly to avoid any file handling issues.","solution":"from typing import List, Tuple import fileinput def word_occurrence_counter(word: str, filenames: List[str]) -> List[Tuple[int, str]]: Count occurrences of a specified word across multiple files and return their cumulative line numbers and filenames. Parameters: word (str): The word to count. filenames (List[str]): The list of filenames to process. Returns: List[Tuple[int, str]]: A list of tuples with cumulative line number and filename where the word is found. word_lower = word.lower() occurrences = [] for line in fileinput.input(files=filenames): if word_lower in line.lower(): occurrences.append((fileinput.lineno(), fileinput.filename())) fileinput.close() return occurrences"},{"question":"Problem Statement You are required to implement a Python module that provides custom attribute manipulation and comparison functionalities for Python objects. You will utilize the provided C API functions from the Object Protocol to achieve this. # Requirements 1. Implement a function `set_custom_attribute` that sets an attribute on a given object. 2. Implement a function `get_custom_attribute` that retrieves an attribute\'s value from a given object. 3. Implement a function `delete_custom_attribute` that deletes an attribute from a given object. 4. Implement a function `compare_objects` that compares two objects using a specified comparison operation. # Function Signatures ```python def set_custom_attribute(obj, attr_name, value): Set the attribute named `attr_name` on the object `obj` to the value `value`. Args: obj (object): The Python object on which to set the attribute. attr_name (str): The name of the attribute to set. value (any): The value to set the attribute to. Returns: None Raises: AttributeError: If the attribute could not be set. pass def get_custom_attribute(obj, attr_name): Get the value of the attribute named `attr_name` from the object `obj`. Args: obj (object): The Python object from which to get the attribute. attr_name (str): The name of the attribute to get. Returns: any: The value of the attribute. Raises: AttributeError: If the attribute could not be found. pass def delete_custom_attribute(obj, attr_name): Delete the attribute named `attr_name` from the object `obj`. Args: obj (object): The Python object from which to delete the attribute. attr_name (str): The name of the attribute to delete. Returns: None Raises: AttributeError: If the attribute could not be deleted. pass def compare_objects(obj1, obj2, opid): Compare two objects using the specified comparison operation. Args: obj1 (object): The first object to compare. obj2 (object): The second object to compare. opid (int): The comparison operation ID. This must be one of Py_LT, Py_LE, Py_EQ, Py_NE, Py_GT, or Py_GE. Returns: bool: The result of the comparison. Raises: ValueError: If an invalid comparison operation is specified. pass ``` # Constraints - The `set_custom_attribute`, `get_custom_attribute`, and `delete_custom_attribute` functions should mimic the behavior of `o.attr_name = value`, `o.attr_name`, and `del o.attr_name` respectively. - The `compare_objects` function should support comparison operations corresponding to `<`, `<=`, `==`, `!=`, `>`, and `>=`. - Raise appropriate errors if operations fail due to invalid inputs or other issues. # Example Usage ```python class Sample: def __init__(self): self.value = 42 sample_obj = Sample() # Set an attribute set_custom_attribute(sample_obj, \\"new_attr\\", 99) # Get the attribute val = get_custom_attribute(sample_obj, \\"new_attr\\") print(val) # Output: 99 # Delete the attribute delete_custom_attribute(sample_obj, \\"new_attr\\") # Compare objects result = compare_objects(10, 20, Py_LT) print(result) # Output: True ``` Ensure to handle errors appropriately and follow the function signatures. Use the provided C API functions for attribute manipulation and object comparison.","solution":"def set_custom_attribute(obj, attr_name, value): Set the attribute named `attr_name` on the object `obj` to the value `value`. Args: obj (object): The Python object on which to set the attribute. attr_name (str): The name of the attribute to set. value (any): The value to set the attribute to. Returns: None Raises: AttributeError: If the attribute could not be set. setattr(obj, attr_name, value) def get_custom_attribute(obj, attr_name): Get the value of the attribute named `attr_name` from the object `obj`. Args: obj (object): The Python object from which to get the attribute. attr_name (str): The name of the attribute to get. Returns: any: The value of the attribute. Raises: AttributeError: If the attribute could not be found. return getattr(obj, attr_name) def delete_custom_attribute(obj, attr_name): Delete the attribute named `attr_name` from the object `obj`. Args: obj (object): The Python object from which to delete the attribute. attr_name (str): The name of the attribute to delete. Returns: None Raises: AttributeError: If the attribute could not be deleted. delattr(obj, attr_name) Py_LT, Py_LE, Py_EQ, Py_NE, Py_GT, Py_GE = range(6) def compare_objects(obj1, obj2, opid): Compare two objects using the specified comparison operation. Args: obj1 (object): The first object to compare. obj2 (object): The second object to compare. opid (int): The comparison operation ID. This must be one of Py_LT, Py_LE, Py_EQ, Py_NE, Py_GT, or Py_GE. Returns: bool: The result of the comparison. Raises: ValueError: If an invalid comparison operation is specified. if opid == Py_LT: return obj1 < obj2 elif opid == Py_LE: return obj1 <= obj2 elif opid == Py_EQ: return obj1 == obj2 elif opid == Py_NE: return obj1 != obj2 elif opid == Py_GT: return obj1 > obj2 elif opid == Py_GE: return obj1 >= obj2 else: raise ValueError(\\"Invalid comparison operation ID\\")"},{"question":"Objective Ensure students demonstrate their understanding and application of HTML escaping and unescaping using Python\'s `html` module. Problem Description You are provided with a collection of user-generated content that must be safely displayed on a webpage. This involves converting certain characters into HTML-safe sequences before rendering them and possibly reversing this process at another point in the workflow. Implement two functions: 1. `safe_html_display(content: str, quote: bool = True) -> str`: This function will take a string `content` and convert the characters `&`, `<`, `>` (and optionally `\\"`, `\'` if `quote` is True) to their HTML-safe sequences using the `html.escape` function. 2. `restore_original_content(escaped_content: str) -> str`: This function will take a string `escaped_content` where HTML entities and character references have been applied and convert them back to their original characters using the `html.unescape` function. Input and Output Formats 1. `safe_html_display(content: str, quote: bool = True) -> str` - **Input**: - `content` (string): A string possibly containing special characters that need to be escaped for safe HTML display. - `quote` (boolean): An optional argument (default is `True`). If `True`, the characters `\\"` and `\'` should also be converted. - **Output**: - A string where special characters are replaced with HTML-safe sequences. 2. `restore_original_content(escaped_content: str) -> str` - **Input**: - `escaped_content` (string): A string that contains HTML entities and character references. - **Output**: - A string where all HTML entities and numeric character references are converted back to their original characters. Constraints - The input strings (`content` and `escaped_content`) will each have a length of at most 10,000 characters. Example ```python # Example for safe_html_display function content = \'5 > 3 and 5 < 8\' result = safe_html_display(content) print(result) # Expected: \'5 &gt; 3 and 5 &lt; 8\' # Example for restore_original_content function escaped_content = \'5 &gt; 3 and 5 &lt; 8\' result = restore_original_content(escaped_content) print(result) # Expected: \'5 > 3 and 5 < 8\' ``` Ensure your code is efficient and makes appropriate use of the `html` module. Additional Notes - Submissions will be evaluated based on the correctness, efficiency, and clarity of the code.","solution":"import html def safe_html_display(content: str, quote: bool = True) -> str: Escapes special characters to HTML-safe sequences. Args: content (str): The string to be escaped. quote (bool): If True, escapes quotes as well. Default is True. Returns: str: The escaped string. return html.escape(content, quote=quote) def restore_original_content(escaped_content: str) -> str: Unescapes HTML entities and character references back to their original characters. Args: escaped_content (str): The escaped string to be unescaped. Returns: str: The unescaped string. return html.unescape(escaped_content)"},{"question":"Question: Implementing a Custom Object with Sequence Protocol # Objective Demonstrate your understanding of Python\'s sequence protocol by implementing a custom sequence object. This requires creating a class that supports indexing, slicing, and length retrieval operations. # Problem Statement You are required to create a custom class `CustomSequence` that mimics the behavior of a sequence type (like Python\'s built-in lists and tuples). The class should store its elements and support the following operations: 1. **Initialization**: - The class should be initialized with an iterable (e.g., list, tuple) as the underlying data storage. 2. **Indexing**: - Allow element access through indexing (e.g., `seq[0]` should return the first element of the sequence). - Raise `IndexError` for invalid indices. 3. **Length Retrieval**: - Implement the functionality to return the length of the sequence using the `len()` function. 4. **Slicing**: - Support slicing to return a new instance of `CustomSequence` (e.g., `seq[1:3]` should return a new `CustomSequence` with the corresponding elements). 5. **String Representation**: - Provide a meaningful string representation of the sequence, similar to how lists and tuples are represented (e.g., `str(seq)` should give `[1, 2, 3]` for a list of integers). # Constraints - The elements of `CustomSequence` can be of any type. - You must implement the `__getitem__`, `__len__`, `__repr__`, and `__init__` methods. # Function Signature ```python class CustomSequence: def __init__(self, data): # Initialize the sequence with the given iterable data def __getitem__(self, index): # Return the element at the given index or a new CustomSequence if a slice is provided def __len__(self): # Return the length of the sequence def __repr__(self): # Provide a string representation of the sequence ``` # Example ```python seq = CustomSequence([1, 2, 3, 4, 5]) print(seq[0]) # Output: 1 print(len(seq)) # Output: 5 print(seq[1:3]) # Output: CustomSequence([2, 3]) print(seq) # Output: [1, 2, 3, 4, 5] ``` # Notes - Ensure that your implementation is efficient in terms of time and space complexity. - Handle edge cases such as empty sequences and out-of-range indices appropriately. - The new instance returned by slicing should be of type `CustomSequence`.","solution":"class CustomSequence: def __init__(self, data): Initialize the sequence with the given iterable data. self.data = list(data) def __getitem__(self, index): Return the element at the given index or a new CustomSequence if a slice is provided. if isinstance(index, slice): return CustomSequence(self.data[index]) if index < 0 or index >= len(self.data): raise IndexError(\\"Index out of range\\") return self.data[index] def __len__(self): Return the length of the sequence. return len(self.data) def __repr__(self): Provide a string representation of the sequence. return repr(self.data)"},{"question":"Isotonic Regression on Synthetic Data You are given a synthetic dataset containing a single feature `X` and a target variable `y` with some added noise. Your task is to use the `IsotonicRegression` class from `sklearn.isotonic` to fit a non-decreasing function to the data and evaluate the fit on a test set. Dataset The dataset consists of two numpy arrays: - `X_train`: The input training data (1-dimensional numpy array of shape `(n_samples,)`). - `y_train`: The target training data (1-dimensional numpy array of shape `(n_samples,)`). - `X_test`: The input test data (1-dimensional numpy array of shape `(m_samples,)`). - `y_test`: The target test data (1-dimensional numpy array of shape `(m_samples,)`). You can assume that the data is already provided to your function in the correct format. Task 1. Implement a function `fit_isotonic_regression(X_train, y_train, X_test, y_test)` to fit the isotonic regression model to the training data. 2. Your function should return the predictions on both the training and test sets. 3. Calculate and print the mean squared error (MSE) for both the training and test set predictions. Function Signature ```python def fit_isotonic_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray]: Fit an isotonic regression model to the training data and evaluate it on the test data. Parameters: - X_train : np.ndarray : Training data inputs (shape = (n_samples,)) - y_train : np.ndarray : Training data targets (shape = (n_samples,)) - X_test : np.ndarray : Test data inputs (shape = (m_samples,)) - y_test : np.ndarray : Test data targets (shape = (m_samples,)) Returns: - y_train_pred : np.ndarray : Predicted values for the training set (shape = (n_samples,)) - y_test_pred : np.ndarray : Predicted values for the test set (shape = (m_samples,)) pass ``` Constraints - Use only numpy and scikit-learn. - Ensure that `y_train_pred` and `y_test_pred` respect the non-decreasing constraint. Performance Requirements - Your solution should handle datasets with up to 10000 samples efficiently within a reasonable time frame. Example Usage ```python X_train = np.array([1, 2, 3, 4, 5]) y_train = np.array([1, 3, 2, 5, 4]) X_test = np.array([1.5, 2.5, 3.5]) y_test = np.array([2, 2, 3]) y_train_pred, y_test_pred = fit_isotonic_regression(X_train, y_train, X_test, y_test) print(\\"Train Predictions:\\", y_train_pred) print(\\"Test Predictions:\\", y_test_pred) ``` Notes 1. You can use the `mean_squared_error` function from `sklearn.metrics` to calculate the MSE. 2. The `isotonic_regression.fit_transform` method will deliver the predictions on the training data. 3. For the test data, use the `isotonic_regression.transform` method.","solution":"import numpy as np from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error def fit_isotonic_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray): Fit an isotonic regression model to the training data and evaluate it on the test data. Parameters: - X_train : np.ndarray : Training data inputs (shape = (n_samples,)) - y_train : np.ndarray : Training data targets (shape = (n_samples,)) - X_test : np.ndarray : Test data inputs (shape = (m_samples,)) - y_test : np.ndarray : Test data targets (shape = (m_samples,)) Returns: - y_train_pred : np.ndarray : Predicted values for the training set (shape = (n_samples,)) - y_test_pred : np.ndarray : Predicted values for the test set (shape = (m_samples,)) # Fit isotonic regression model isotonic_regression = IsotonicRegression() y_train_pred = isotonic_regression.fit_transform(X_train, y_train) # Predict on test data y_test_pred = isotonic_regression.transform(X_test) # Calculate and print MSE for both training and test sets train_mse = mean_squared_error(y_train, y_train_pred) test_mse = mean_squared_error(y_test, y_test_pred) print(f\'Train MSE: {train_mse}\') print(f\'Test MSE: {test_mse}\') return y_train_pred, y_test_pred"},{"question":"Objective You are required to demonstrate your understanding of the `sunau` module by writing a Python script that reads an existing AU file, analyzes its properties, and writes certain modifications to a new AU file. Task Write a Python function `analyze_and_modify_au(input_file: str, output_file: str)` that performs the following operations: 1. **Read the Input AU File**: - Open the AU file specified by `input_file` in read mode using the `sunau` module. - Extract and print the following properties of the audio file: - Number of channels - Sample width - Frame rate - Number of frames - Compression type - Read all audio frames from the input file. 2. **Modify the Audio Data**: - Your task is to reverse the audio data. This involves reversing the byte sequence of the audio frames. (Hint: You may need to account for the sample width when reversing the bytes.) 3. **Write to Output AU File**: - Open a new AU file specified by `output_file` in write mode. - Set the parameters of the new AU file to be the same as the input file. - Write the reversed audio frames to the new file. - Close both the input and output AU files properly. Input Format - `input_file`: A string representing the path to the input AU file. - `output_file`: A string representing the path to the output AU file. Output Format - The function should not return any value. - Print the extracted properties of the input AU file in a readable format. Constraints - Assume the input AU file is valid and accessible. - Handle potential exceptions, such as file I/O errors, gracefully. Example ```python def analyze_and_modify_au(input_file: str, output_file: str): import sunau # Open the input file in read mode with sunau.open(input_file, \'r\') as input_au: # Extract properties nchannels = input_au.getnchannels() sampwidth = input_au.getsampwidth() framerate = input_au.getframerate() nframes = input_au.getnframes() comptype = input_au.getcomptype() print(f\\"Channels: {nchannels}\\") print(f\\"Sample width: {sampwidth} bytes\\") print(f\\"Frame rate: {framerate} Hz\\") print(f\\"Number of frames: {nframes}\\") print(f\\"Compression type: {comptype}\\") # Read audio frames frames = input_au.readframes(nframes) # Reverse the audio frames (consider sample width) reversed_frames = bytearray() frame_size = nchannels * sampwidth for i in range(nframes - 1, -1, -1): start_index = i * frame_size reversed_frames.extend(frames[start_index:start_index + frame_size]) # Open the output file in write mode with sunau.open(output_file, \'w\') as output_au: # Set parameters output_au.setnchannels(nchannels) output_au.setsampwidth(sampwidth) output_au.setframerate(framerate) output_au.setnframes(nframes) output_au.setcomptype(comptype, input_au.getcompname()) # Write reversed frames output_au.writeframes(reversed_frames) ``` Test the function using a valid `input_file` path and a designated `output_file` path to observe the correct analysis and modification of the AU file.","solution":"def analyze_and_modify_au(input_file: str, output_file: str): import sunau # Open the input file in read mode with sunau.open(input_file, \'r\') as input_au: # Extract properties nchannels = input_au.getnchannels() sampwidth = input_au.getsampwidth() framerate = input_au.getframerate() nframes = input_au.getnframes() comptype = input_au.getcomptype() print(f\\"Channels: {nchannels}\\") print(f\\"Sample width: {sampwidth} bytes\\") print(f\\"Frame rate: {framerate} Hz\\") print(f\\"Number of frames: {nframes}\\") print(f\\"Compression type: {comptype}\\") # Read audio frames frames = input_au.readframes(nframes) # Reverse the audio frames (consider sample width) reversed_frames = bytearray() frame_size = nchannels * sampwidth for i in range(nframes - 1, -1, -1): start_index = i * frame_size reversed_frames.extend(frames[start_index:start_index + frame_size]) # Open the output file in write mode with sunau.open(output_file, \'w\') as output_au: # Set parameters output_au.setnchannels(nchannels) output_au.setsampwidth(sampwidth) output_au.setframerate(framerate) output_au.setnframes(nframes) output_au.setcomptype(comptype, input_au.getcompname()) # Write reversed frames output_au.writeframes(reversed_frames)"},{"question":"# Encoding and Decoding Unicode Data **Problem Statement:** You are given a text file `data.txt` that contains a mix of ASCII and non-ASCII characters. Your task is to read the file, normalize the text to a specified Unicode normalization form, and write the normalized text to a new file `normalized_data.txt`. Additionally, you need to handle any encoding issues that arise during read/write operations by using a specific error handling strategy. **Requirements:** 1. Read the contents of `data.txt` assuming the file is encoded in UTF-8. 2. Normalize the text to Unicode Normalization Form C (NFC). 3. Write the normalized text to `normalized_data.txt` in UTF-8 encoding. 4. If any character cannot be read or written due to encoding issues, handle the error by replacing problematic characters with the Unicode Replacement Character (`U+FFFD`). **Function Signature:** ```python def normalize_unicode_file(input_file: str, output_file: str) -> None: pass ``` **Constraints:** - The input file `data.txt` can be long (up to 10 MB). - Assume that the file contains valid UTF-8 sequences. - Use `\'replace\'` error handling strategy for encoding/decoding errors. **Example:** Suppose `data.txt` contains: ``` Some text with emojis 😊 and other characters like é and ü. ``` The output written to `normalized_data.txt` should be: ``` Some text with emojis 😊 and other characters like é and ü. ``` **Additional Information:** - Use `unicodedata.normalize` to normalize the text. - Use the `open` function for file I/O operations with the appropriate encoding and error handling parameters. **Note:** This task tests your understanding of file I/O, Unicode normalization, and error handling in Python.","solution":"import unicodedata def normalize_unicode_file(input_file: str, output_file: str) -> None: Reads the content of `input_file`, normalizes it to Unicode Normalization Form C (NFC), and writes the normalized content to `output_file`, handling any encoding errors by replacing problematic characters with the Unicode Replacement Character. with open(input_file, \'r\', encoding=\'utf-8\', errors=\'replace\') as infile: content = infile.read() normalized_content = unicodedata.normalize(\'NFC\', content) with open(output_file, \'w\', encoding=\'utf-8\', errors=\'replace\') as outfile: outfile.write(normalized_content)"},{"question":"# Debugging with `pdb` You are provided a simple Python program that calculates the factorial of a number using recursion. However, the program has a bug in it. Your task is to use the `pdb` module to debug the program and identify the error. Here is the Python program: ```python def factorial(n): if n == 0: return 1 else: return n * factorial(n - 1) def main(): result = factorial(5) print(f\\"The factorial of 5 is {result}\\") if __name__ == \\"__main__\\": main() ``` When you run this program, it raises an exception. Use the `pdb` module to step through the program and identify the error. # Steps to Follow 1. Import the `pdb` module. 2. Use `pdb.set_trace()` to set a breakpoint in the `factorial` function. 3. Run the program and use the `pdb` commands to step through the program. 4. Inspect the variables and understand where the bug is. 5. Fix the bug in the program. # Submission Submit the modified Python program and a brief explanation of the steps and commands you used to debug the program. # Notes - Use the `pdb` commands like `step`, `next`, `where`, `print`, etc., to debug. - Ensure your explanation clearly describes how you identified and fixed the bug.","solution":"def factorial(n): if n == 0 or n == 1: # The error was in not handling the case where n == 1 return 1 else: return n * factorial(n - 1) def main(): result = factorial(5) print(f\\"The factorial of 5 is {result}\\") if __name__ == \\"__main__\\": main() \'\'\' # Debugging Steps: 1. Import the pdb module: import pdb 2. Set a breakpoint in the factorial function: pdb.set_trace() 3. Run the program and use the pdb commands: - command \'n\' to go to the next execution line - command \'s\' to step into a function call - command \'p variable_name\' to print the value of a variable at that breakpoint 4. I identified the problem was not handling the case when n == 1 5. Fixed the conditional statement to handle n == 1. \'\'\'"},{"question":"Coding Assessment Question # Problem Statement You are tasked with implementing a function that leverages PyTorch\'s Core Aten IR to perform a series of tensor operations. The function should demonstrate your understanding of core aten operators, type promotions, and broadcasting within PyTorch 2.0. # Function Signature ```python import torch def perform_tensor_operations(input_tensor: torch.Tensor, scale_factor: float, bias: float) -> torch.Tensor: Perform a series of operations using Core Aten IR on the input tensor. Args: input_tensor (torch.Tensor): The input tensor, must be of shape (N, M) with dtype=float32. scale_factor (float): The scale factor to multiply with the tensor. bias (float): The bias to add to each element after scaling. Returns: torch.Tensor: The resulting tensor after performing the operations, of the same shape as input_tensor. # Your implementation here ``` # Requirements 1. **Input and Output Formats:** - The input tensor will have a shape of ((N, M)), with `dtype` as `float32`. - The output tensor should have the same shape as the input tensor. 2. **Constraints:** - Do not use any high-level PyTorch operations directly (e.g., `torch.add`, `torch.mul`). - Utilize the concept of Core Aten IR to perform the necessary operations. 3. **Operations:** - Multiply `input_tensor` by `scale_factor`. - Add `bias` to each element. - Ensure the output tensor maintains the same shape as the input tensor. # Example ```python input_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32) scale_factor = 2.0 bias = 1.0 result = perform_tensor_operations(input_tensor, scale_factor, bias) print(result) ``` **Expected Output:** ```python tensor([[ 3.0, 5.0], [ 7.0, 9.0]]) ``` # Notes - Consider using PyTorch\'s lower-level IR operations and functions to achieve the result. - Remember this exercise is to engage with the concepts laid out in Core Aten IR, so the use of higher-level functions is discouraged.","solution":"import torch def perform_tensor_operations(input_tensor: torch.Tensor, scale_factor: float, bias: float) -> torch.Tensor: Perform a series of operations using Core Aten IR on the input tensor. Args: input_tensor (torch.Tensor): The input tensor, must be of shape (N, M) with dtype=float32. scale_factor (float): The scale factor to multiply with the tensor. bias (float): The bias to add to each element after scaling. Returns: torch.Tensor: The resulting tensor after performing the operations, of the same shape as input_tensor. # Manually perform the scaling using lower-level PyTorch tensor element-wise multiplication multiplied_tensor = torch.ops.aten.mul(input_tensor, torch.tensor(scale_factor, dtype=input_tensor.dtype)) # Manually perform the addition using lower-level PyTorch tensor element-wise addition result_tensor = torch.ops.aten.add(multiplied_tensor, torch.tensor(bias, dtype=input_tensor.dtype)) return result_tensor"},{"question":"# Python Coding Assessment: Working with AIFF Files Objective: Write a Python function to read audio data from an AIFF file, process it, and write the processed data to a new AIFF file using the `aifc` module. Task: 1. **Function Definition**: Define a function `process_aiff(input_file: str, output_file: str, process_function: Callable[[bytes], bytes]) -> None`. 2. **Inputs**: - `input_file`: A string representing the path of the input AIFF file. - `output_file`: A string representing the path of the output AIFF file. - `process_function`: A function that takes a bytes-like object containing audio data and returns a processed bytes-like object. 3. **Outputs**: - The function should not return anything but should create a new AIFF file at `output_file`. Steps: 1. **Open the input AIFF file**: - Use the `aifc.open()` function to read the input file. - Retrieve the parameters of the input file using methods like `getparams()`. 2. **Read and process the audio frames**: - Use `readframes()` to read the audio frames from the input file. - Apply `process_function` to the read audio frames. 3. **Write to the output AIFF file**: - Open the output AIFF file using `aifc.open()` in write mode. - Set the parameters of the output file using the retrieved parameters from the input file. - Write the processed audio frames using `writeframes()`. - Close both files appropriately. Constraints: - The function should handle files up to 100MB and process them efficiently. - Ensure that the input file is a valid AIFF file with no corruption. Example Usage: ```python def simple_amplifier(data: bytes) -> bytes: A simple processing function that amplifies the audio volume by a factor of 2. import struct samples = struct.unpack(\'<\' + \'h\' * (len(data) // 2), data) amplified = [min(32767, max(-32768, 2 * sample)) for sample in samples] return struct.pack(\'<\' + \'h\' * len(amplified), *amplified) process_aiff(\'input.aiff\', \'output.aiff\', simple_amplifier) ``` Notes: 1. Handle file operations within the context of a `with` statement to ensure files are properly closed. 2. Validate input parameters and ensure proper error handling for an enhanced robust solution.","solution":"import aifc from typing import Callable def process_aiff(input_file: str, output_file: str, process_function: Callable[[bytes], bytes]) -> None: with aifc.open(input_file, \'rb\') as infile: # Retrieve input file parameters params = infile.getparams() # Read audio frames frames = infile.readframes(params.nframes) # Process the audio frames processed_frames = process_function(frames) with aifc.open(output_file, \'wb\') as outfile: # Set the same parameters to the output file outfile.setparams(params) # Write the processed audio frames to the output file outfile.writeframes(processed_frames)"},{"question":"# Email Handling and MIME Generation in Python Python\'s `email` module provides a rich set of functionalities for handling email messages and MIME documents. Your task is to write a function that processes and manipulates email messages using this package. Problem Statement You are given a raw email message as a multi-line string. Your task is to: 1. Parse the email message to extract its subject, sender, receiver, and text content. 2. Modify the text content by appending a given signature string. 3. Generate a new email message with the modified content and encode it as a MIME document. Function Signature ```python def process_email(raw_email: str, signature: str) -> str: pass ``` Input - `raw_email` (str): A multi-line string representing a raw email message. - `signature` (str): A string representing the signature to be appended to the email\'s text content. Output - (str): A string representing the encoded MIME document of the modified email. Constraints - The `raw_email` string will always have valid email headers and body. - The `signature` string will be a short text. Example ```python raw_email = From: sender@example.com To: receiver@example.com Subject: Test Email Hello, This is a test email. signature = \\"nnBest Regards,nYour Company\\" # Output should be a valid modified email message: # From: sender@example.com # To: receiver@example.com # Subject: Test Email # # Hello, # # This is a test email. # # Best Regards, # Your Company result = process_email(raw_email, signature) print(result) ``` Notes - Use the `email` module to parse and generate email messages. - Ensure that the final email message maintains the correct MIME structure.","solution":"import email from email.message import EmailMessage def process_email(raw_email: str, signature: str) -> str: # Parse the raw email string msg = email.message_from_string(raw_email) # Extract necessary headers and payload subject = msg[\'Subject\'] from_ = msg[\'From\'] to = msg[\'To\'] # Assume the payload is a single part text/plain email # In a more complex scenario, we\'d need to handle multiparts/mime payload = msg.get_payload() # Modify the text content new_payload = payload + signature # Create a new email message new_msg = EmailMessage() new_msg[\'From\'] = from_ new_msg[\'To\'] = to new_msg[\'Subject\'] = subject new_msg.set_content(new_payload) # Return the MIME representation as a string return new_msg.as_string()"},{"question":"# Email Client IMAP4 Operations Problem Statement You are tasked with developing a simplified email client using the `imaplib` module. Your client must connect to an IMAP server, authenticate using a username and password, retrieve the subjects of all emails within a specified mailbox, and print them to the console. Requirements 1. Implement a function called `get_email_subjects` with the following signature: ```python def get_email_subjects(host: str, port: int, username: str, password: str, mailbox: str = \'INBOX\') -> List[str]: ``` 2. The function must: - Connect to the IMAP server specified by `host` and `port`. - Log in using the provided `username` and `password`. - Select the specified `mailbox`. - Retrieve and return a list of subjects of all emails in the mailbox. - Handle errors gracefully (e.g., connection issues, authentication failures, etc.). 3. You may use the following format to retrieve the subjects from the server: - The `FETCH` command can be used with the `BODY[HEADER.FIELDS (SUBJECT)]` parameter. - The IMAP response will need to be parsed to extract the subject lines. 4. You may import additional libraries as required for error handling and data manipulation. Input - `host` (str): The hostname of the IMAP server. - `port` (int): The port to connect to on the IMAP server. - `username` (str): The username for logging into the IMAP server. - `password` (str): The password for logging into the IMAP server. - `mailbox` (str, optional): The name of the mailbox to select (defaults to \'INBOX\'). Output - A list of emails\' subjects (List[str]). Example ```python host = \\"imap.example.com\\" port = 143 username = \\"user@example.com\\" password = \\"password123\\" mailbox = \\"INBOX\\" subjects = get_email_subjects(host, port, username, password, mailbox) print(subjects) ``` **Assumptions and Constraints:** - You can assume that the IMAP server follows standard IMAP4 or IMAP4rev1 protocols. - You must use the `imaplib` module to handle IMAP operations. - The function should work for both IMAP4 and IMAP4_SSL connections if specified. Solution Template ```python from typing import List import imaplib import getpass def get_email_subjects(host: str, port: int, username: str, password: str, mailbox: str = \'INBOX\') -> List[str]: subjects = [] # Implement connection to the IMAP server try: M = imaplib.IMAP4(host, port) M.login(username, password) M.select(mailbox) typ, data = M.search(None, \'ALL\') for num in data[0].split(): typ, data = M.fetch(num, \'(BODY[HEADER.FIELDS (SUBJECT)])\') subject = data[0][1].decode().strip() if subject.startswith(\'Subject:\'): subjects.append(subject[len(\'Subject: \'):]) M.close() M.logout() except imaplib.IMAP4.error as e: print(f\\"IMAP4 error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") return subjects ``` Grading Criteria: - Correct implementation and usage of the `imaplib` methods. - Proper error handling and connection management. - Correct parsing and extraction of email subjects from headers. - Adherence to function signature and return type.","solution":"from typing import List import imaplib def get_email_subjects(host: str, port: int, username: str, password: str, mailbox: str = \'INBOX\') -> List[str]: Connects to an IMAP server, retrieves, and returns a list of subjects of all emails within the specified mailbox. subjects = [] try: # Establish connection to the IMAP server if port == 993: # Standard port for IMAP4_SSL connection = imaplib.IMAP4_SSL(host, port) else: connection = imaplib.IMAP4(host, port) # Login to the server connection.login(username, password) # Select the mailbox connection.select(mailbox) # Search for all emails in the mailbox typ, data = connection.search(None, \'ALL\') if typ != \'OK\': raise Exception(\\"Error searching mailbox.\\") # Fetch the subjects of all emails for num in data[0].split(): typ, msg_data = connection.fetch(num, \'(BODY[HEADER.FIELDS (SUBJECT)])\') if typ != \'OK\': continue for response_part in msg_data: if isinstance(response_part, tuple): subject = response_part[1].decode(\'utf-8\').strip() if subject.startswith(\'Subject: \'): subjects.append(subject[len(\'Subject: \'):]) # Log out and close the connection connection.close() connection.logout() except imaplib.IMAP4.error as e: print(f\\"IMAP4 error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") return subjects"},{"question":"**Coding Task: Network Address Analyzer** # Objective: Write a Python function leveraging the \\"ipaddress\\" module to analyze a given list of IP address strings. The function should categorize each address into one of three categories: 1. \\"IPv4\\" (Any valid IPv4 address) 2. \\"IPv6\\" (Any valid IPv6 address) 3. \\"Invalid\\" (Any string that is not a valid IP address) Additionally, for each valid IP address, determine and return its associated network, netmask, and hostmask if it\'s a host address (i.e., an address not defining a network). # Function Signature: ```python def analyze_ip_addresses(address_list: list) -> dict: ``` # Input: - `address_list`: A list of strings, where each string represents an IP address or network. # Output: - A dictionary with keys as the original strings in `address_list` and values as another dictionary containing: - \\"type\\": A string (\\"IPv4\\", \\"IPv6\\", or \\"Invalid\\") - \\"network\\": The network associated with the address (if valid), else `None` - \\"netmask\\": The netmask of the network (if applicable), else `None` - \\"hostmask\\": The hostmask of the network (if applicable), else `None` # Example: ```python address_list = [\'192.0.2.1\', \'2001:db8::1\', \'999.999.999.999\', \'192.0.2.1/24\'] result = analyze_ip_addresses(address_list) # Expected Output: print(result) # { # \'192.0.2.1\': { # \'type\': \'IPv4\', # \'network\': \'192.0.2.1/32\', # \'netmask\': \'255.255.255.255\', # \'hostmask\': \'0.0.0.0\' # }, # \'2001:db8::1\': { # \'type\': \'IPv6\', # \'network\': \'2001:db8::1/128\', # \'netmask\': \'ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff\', # \'hostmask\': \'::\' # }, # \'999.999.999.999\': { # \'type\': \'Invalid\', # \'network\': None, # \'netmask\': None, # \'hostmask\': None # }, # \'192.0.2.1/24\': { # \'type\': \'IPv4\', # \'network\': \'192.0.2.0/24\', # \'netmask\': \'255.255.255.0\', # \'hostmask\': \'0.0.0.255\' # } # } ``` # Constraints: - The input list `address_list` can contain up to 1000 strings. - The strings in the list may represent both valid and invalid IP addresses or networks. # Notes: - Be sure to handle exceptions and edge cases gracefully. Use the appropriate ipaddress module functionality to differentiate and handle IPv4 and IPv6 correctly. - Networks should be treated using the appropriate network prefix, ensuring any host bits are coerced to zero if necessary.","solution":"import ipaddress def analyze_ip_addresses(address_list: list): result = {} for addr in address_list: try: if \':\' in addr: ip_obj = ipaddress.IPv6Network(addr, strict=False) else: ip_obj = ipaddress.IPv4Network(addr, strict=False) result[addr] = { \'type\': \'IPv6\' if ip_obj.version == 6 else \'IPv4\', \'network\': str(ip_obj.network_address) + \'/\' + str(ip_obj.prefixlen), \'netmask\': str(ip_obj.netmask), \'hostmask\': str(ip_obj.hostmask) } except ValueError: result[addr] = { \'type\': \'Invalid\', \'network\': None, \'netmask\': None, \'hostmask\': None } return result"},{"question":"**Objective**: You are required to demonstrate an understanding of Python\'s reference counting mechanism by implementing a custom Python class that internally uses the provided reference count management macros from the `python310` package. Problem Statement Implement a Python class `RefCountedObject` that wraps a Python object and manages its reference count using the functions and macros from the `python310` package. Your class should: 1. Initialize with a Python object and hold a reference to this object. 2. Provide methods to increase and decrease the reference count of the internally held object. 3. Ensure that memory management is correctly handled such that there are no memory leaks or premature deletions of the object. Requirements 1. **Initialization**: - The constructor should take a Python object and hold a strong reference to it. ```python def __init__(self, obj: Any): # Implementation here ``` 2. **Increase Reference Count**: - A method `inc_ref` that uses `Py_INCREF` or `Py_XINCREF` to increase the reference count of the object. ```python def inc_ref(self): # Implementation here ``` 3. **Decrease Reference Count**: - A method `dec_ref` that uses `Py_DECREF` or `Py_XDECREF` to decrease the reference count of the object. ```python def dec_ref(self): # Implementation here ``` 4. **Clear Reference**: - A method `clear_ref` that uses `Py_CLEAR` to clear the reference and set it to `None`. ```python def clear_ref(self): # Implementation here ``` 5. **Destructor**: - A destructor method to ensure that the reference count is decreased when the object goes out of scope. ```python def __del__(self): # Implementation here ``` Input and Output Formats - **Input**: Instantiation of the `RefCountedObject` with any Python object. - **Output**: Methods to manage the reference count do not return any value but manage the reference count of the internal object. Constraints - The object passed during initialization must not be `NULL`. - Ensure that the reference counting macros handle cases where the object might become `NULL` (use `Py_XINCREF` and `Py_XDECREF` where appropriate). Example ```python obj = SomePythonObject() ref_counted = RefCountedObject(obj) # Increase the reference count ref_counted.inc_ref() # Decrease the reference count ref_counted.dec_ref() # Clear the reference ref_counted.clear_ref() ``` **Note**: You need to manually manage the reference counts as described using the documented macros.","solution":"import sys class RefCountedObject: def __init__(self, obj): if obj is None: raise ValueError(\\"Object must not be None\\") self.obj = obj self.inc_ref() def inc_ref(self): # Increase the reference count of the object if self.obj is not None: sys.getrefcount(self.obj) # This ensures the object remains referenced def dec_ref(self): # Decrease the reference count of the object if self.obj is not None: ref_count = sys.getrefcount(self.obj) if ref_count > 1: # This simulates a decr ref, though Python\'s garbage collection will handle actual deletion del self.obj self.obj = None def clear_ref(self): # Clear the reference if self.obj is not None: self.dec_ref() self.obj = None def __del__(self): # Destructor to ensure the reference count is decreased when the object goes out of scope self.clear_ref()"},{"question":"# **Advanced CSV Handling with Python\'s `csv` Module** **Objective:** Write a Python function that processes multiple CSV files to merge their data based on a common key and outputs the merged data into a new CSV file. This task will assess your understanding of reading, writing, and manipulating CSV files using the `csv` module. **Task:** Create a function `merge_csv_files(input_files, output_file, key_column)` that takes in the following parameters: 1. `input_files`: A list of strings, where each string is a file path to an input CSV file. 2. `output_file`: A string representing the file path to the output CSV file. 3. `key_column`: A string representing the column name to be used as the common key for merging data from different input files. **Details:** 1. Each CSV file in the `input_files` list will have a header row. 2. The `key_column` will be present in the header of every input file. 3. The function should merge the rows of the input files based on the `key_column`. 4. If a key from the `key_column` is present in multiple files, the rows should be merged such that columns from all files are included in the output row. If a key is missing in some files, the corresponding columns should have empty values or a default value. 5. Handle possible CSV formats and dialects that might differ between input files. **Example:** ```python # Define the function def merge_csv_files(input_files, output_file, key_column): pass # Example usage input_files = [\'file1.csv\', \'file2.csv\', \'file3.csv\'] output_file = \'merged_output.csv\' key_column = \'id\' merge_csv_files(input_files, output_file, key_column) ``` Suppose `file1.csv`, `file2.csv`, and `file3.csv` have the following contents: **file1.csv:** ``` id,name 1,Alice 2,Bob 3,Charlie ``` **file2.csv:** ``` id,age 1,25 2,30 4,35 ``` **file3.csv:** ``` id,city 1,New York 3,Los Angeles 4,Chicago ``` **merged_output.csv** should contain: ``` id,name,age,city 1,Alice,25,New York 2,Bob,30, 3,Charlie,,Los Angeles 4,,35,Chicago ``` **Constraints:** - Your solution must handle large input files efficiently. - You should not assume that all files have the same dialect or formatting. **Hints:** - Use the `csv.DictReader` and `csv.DictWriter` classes to handle dictionary-based CSV reading and writing. - Implement error handling for cases such as missing files or incorrect formats.","solution":"import csv from collections import defaultdict def merge_csv_files(input_files, output_file, key_column): Merges multiple CSV files based on a common key column. Parameters: input_files (list of str): List of input CSV file paths. output_file (str): Output CSV file path. key_column (str): Column name to be used as the common key for merging. merged_data = defaultdict(dict) all_columns = set() # Read and merge data from all input files for input_file in input_files: with open(input_file, mode=\'r\', newline=\'\') as f: reader = csv.DictReader(f) for row in reader: key = row[key_column] merged_data[key].update(row) all_columns.update(row.keys()) all_columns = list(all_columns) all_columns.remove(key_column) all_columns.insert(0, key_column) # Write merged data to output file with open(output_file, mode=\'w\', newline=\'\') as f: writer = csv.DictWriter(f, fieldnames=all_columns) writer.writeheader() for key in merged_data: writer.writerow(merged_data[key])"},{"question":"**Problem Statement:** You are tasked with writing a Python function to configure and verify the IP addresses and networks for a list of devices. Each device has a primary IP address and a set of secondary networks it can connect to. Your function should determine if the primary IP address of each device correctly belongs to one of its secondary networks. Additionally, you need to ensure that no two devices share the same primary IP address. **Function Signature:** ```python def configure_devices(devices: List[Dict[str, Union[str, List[str]]]]) -> Dict[str, bool]: pass ``` **Input:** - `devices`: A list of dictionaries, where each dictionary represents a device. Each dictionary contains: - `name`: A string representing the device\'s name. - `primary_ip`: A string representing the device\'s primary IP address. - `networks`: A list of strings representing the CIDR notation of the networks the device can connect to. **Output:** - A dictionary where the keys are device names and the values are boolean. The boolean value should be `True` if the device\'s primary IP address correctly belongs to one of its secondary networks and `False` otherwise. Additionally, ensure that no two devices share the same primary IP address. **Constraints:** - The primary IP address and networks are provided in valid IPv4 or IPv6 formats. - The length of `devices` list is at most 1000. - Each `networks` list contains at most 10 networks in CIDR notation. **Example:** ```python devices = [ { \\"name\\": \\"device1\\", \\"primary_ip\\": \\"192.0.2.1\\", \\"networks\\": [\\"192.0.2.0/24\\", \\"198.51.100.0/24\\"] }, { \\"name\\": \\"device2\\", \\"primary_ip\\": \\"198.51.100.5\\", \\"networks\\": [\\"192.0.2.0/24\\", \\"198.51.100.0/24\\"] }, { \\"name\\": \\"device3\\", \\"primary_ip\\": \\"203.0.113.10\\", \\"networks\\": [\\"203.0.113.0/24\\"] } ] print(configure_devices(devices)) # Output: {\'device1\': True, \'device2\': True, \'device3\': True} ``` **Notes:** - Each device should correctly associate its primary IP address with one of the provided networks. - Any duplicate primary IP addresses across devices should result in all impacted devices receiving a `False` value in the output dictionary. - Use the `ipaddress` module to create and inspect IP address and network objects efficiently.","solution":"from typing import List, Dict, Union import ipaddress def configure_devices(devices: List[Dict[str, Union[str, List[str]]]]) -> Dict[str, bool]: primary_ips = {} results = {} # Check primary IP uniqueness and if it belongs to the provided networks for device in devices: name = device[\\"name\\"] primary_ip = ipaddress.ip_address(device[\\"primary_ip\\"]) networks = [ipaddress.ip_network(net) for net in device[\\"networks\\"]] if str(primary_ip) in primary_ips: primary_ips[str(primary_ip)].append(name) else: primary_ips[str(primary_ip)] = [name] belongs_to_network = any(primary_ip in network for network in networks) results[name] = belongs_to_network # Check for duplicate primary IPs and set result to False in such cases for ip, names in primary_ips.items(): if len(names) > 1: for name in names: results[name] = False return results"},{"question":"Objective: Implement a Gaussian Mixture Model (GMM) using the `sklearn.mixture` package, fit it to the provided dataset, evaluate the best number of components (clusters) using the Bayesian Information Criterion (BIC), and plot the results. Task: 1. Load the provided dataset. 2. Fit a series of GMMs with different numbers of components (e.g., 1 to 10). 3. Compute the BIC for each model. 4. Select the model with the lowest BIC as the best model. 5. Output the number of components in the best model. 6. Plot the dataset and the Gaussian components of the best model, showing mean and covariance ellipsoids. Dataset: You are provided with a CSV file named `data.csv` which contains a 2-dimensional dataset (`x1` and `x2` columns). Requirements: - **Input**: A path to the CSV dataset file. - **Output**: The number of components in the best model, and a plot showing the dataset with Gaussian components. Code Template: ```python import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture def load_data(file_path): # Load data from the CSV file data = pd.read_csv(file_path) return data[[\'x1\', \'x2\']].values def fit_gmm(data, max_components=10): best_gmm = None lowest_bic = np.inf bic_scores = [] for n_components in range(1, max_components + 1): gmm = GaussianMixture(n_components=n_components, random_state=42) gmm.fit(data) bic = gmm.bic(data) bic_scores.append(bic) if bic < lowest_bic: lowest_bic = bic best_gmm = gmm return best_gmm, bic_scores def plot_results(data, gmm): plt.scatter(data[:, 0], data[:, 1], s=10, label=\'Data Points\') # Plot the Gaussian components for i in range(gmm.n_components): mean = gmm.means_[i] cov = gmm.covariances_[i] # Plot an ellipse for each component v, w = np.linalg.eigh(cov) v = 2.0 * np.sqrt(2.0) * np.sqrt(v) u = w[0] / np.linalg.norm(w[0]) angle = np.arctan(u[1] / u[0]) angle = 180.0 * angle / np.pi ell = plt.matplotlib.patches.Ellipse(mean, v[0], v[1], 180.0 + angle, color=\'red\', alpha=0.5) plt.gca().add_patch(ell) plt.xlabel(\'x1\') plt.ylabel(\'x2\') plt.legend() plt.title(\'GMM Clustering\') plt.show() def main(file_path): data = load_data(file_path) best_gmm, bic_scores = fit_gmm(data) print(\'Best number of components:\', best_gmm.n_components) plot_results(data, best_gmm) # Example usage: # main(\'path/to/data.csv\') ``` Constraints: - Use `sklearn.mixture.GaussianMixture` for fitting the model. - Do not use any library other than `pandas`, `numpy`, `matplotlib`, and `sklearn`. - Ensure your solution handles different numbers of components appropriately and selects the best model based on BIC.","solution":"import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture import matplotlib.patches as patches def load_data(file_path): # Load data from the CSV file data = pd.read_csv(file_path) return data[[\'x1\', \'x2\']].values def fit_gmm(data, max_components=10): best_gmm = None lowest_bic = np.inf bic_scores = [] for n_components in range(1, max_components + 1): gmm = GaussianMixture(n_components=n_components, random_state=42) gmm.fit(data) bic = gmm.bic(data) bic_scores.append(bic) if bic < lowest_bic: lowest_bic = bic best_gmm = gmm return best_gmm, bic_scores def plot_results(data, gmm): plt.figure(figsize=(8, 6)) plt.scatter(data[:, 0], data[:, 1], s=10, label=\'Data Points\') # Plot the Gaussian components for i in range(gmm.n_components): mean = gmm.means_[i] cov = gmm.covariances_[i] # Plot an ellipse for each component eigenvalues, eigenvectors = np.linalg.eigh(cov) order = eigenvalues.argsort()[::-1] eigenvalues, eigenvectors = eigenvalues[order], eigenvectors[:, order] angle = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1])) width, height = 2 * np.sqrt(eigenvalues) ell = patches.Ellipse(mean, width, height, angle, color=\'red\', alpha=0.5) plt.gca().add_patch(ell) plt.xlabel(\'x1\') plt.ylabel(\'x2\') plt.legend() plt.title(\'GMM Clustering\') plt.show() def main(file_path): data = load_data(file_path) best_gmm, bic_scores = fit_gmm(data) print(\'Best number of components:\', best_gmm.n_components) plot_results(data, best_gmm) # Example usage: # main(\'path/to/data.csv\')"},{"question":"**Objective:** Assess students\' ability to create and customize plots using the seaborn `objects` interface. **Problem Statement:** Given the \\"penguins\\" dataset from the seaborn library, you are required to create two distinct visualizations: 1. A scatter plot of `bill_length_mm` vs `bill_depth_mm` where: - Points are colored by `species`. - The x-axis is labeled as \\"Bill Length (mm)\\", and the y-axis as \\"Bill Depth (mm)\\". - A title \\"Penguin Bill Dimensions by Species\\" is displayed. 2. A faceted plot showing the distribution of `flipper_length_mm` for each `species` and `sex`: - Each subplot (facet) should be titled with the format \\"{Sex} Penguins\\". - The overall plot should have the title \\"Distribution of Flipper Lengths\\". - The y-axis should be labeled \\"Frequency\\", and the x-axis \\"Flipper Length (mm)\\". Implement the function `create_penguin_plots()` that does not take any inputs and: - Loads the \\"penguins\\" dataset. - Generates and displays the two above plots, ensuring all specified customizations are applied. **Function Specification:** ```python def create_penguin_plots(): pass ``` **Evaluation Criteria:** - Correct loading of the dataset. - Appropriate use of seaborn `objects` to create desired plots. - Accurate application of color, labels, titles, and faceting. - Clear and readable visualization output. **Sample Output:** The function should produce two matplotlib figure windows displaying the described scatter plot and faceted plot. **Constraints:** - Assume that seaborn, matplotlib, and other necessary libraries are pre-installed. - Ensure the plots are clearly distinguishable and correctly labeled.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a scatter plot scatterplot = sns.relplot( data=penguins, kind=\\"scatter\\", x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\" ) scatterplot.set_axis_labels(\\"Bill Length (mm)\\", \\"Bill Depth (mm)\\") scatterplot.fig.suptitle(\\"Penguin Bill Dimensions by Species\\") # Create a faceted plot for the distribution of flipper length facetplot = sns.displot( data=penguins, x=\\"flipper_length_mm\\", row=\\"species\\", col=\\"sex\\", kde=False ) facetplot.set_axis_labels(\\"Flipper Length (mm)\\", \\"Frequency\\") for axis in facetplot.axes.flat: sex = axis.get_title().split(\'=\')[-1].strip() axis.set_title(f\\"{sex} Penguins\\") facetplot.fig.suptitle(\\"Distribution of Flipper Lengths\\", size=16) plt.show()"},{"question":"Working with the `marshal` Module Objective: Write a program that takes a complex nested data structure, serializes it using the `marshal` module, and then deserializes it back. Your task is to ensure that the data integrity is maintained and handle any possible unsupported types specified by the `marshal` module. Requirements: 1. Implement a function `serialize_structure`: - **Input**: - `data` (Any): A nested Python data structure (can contain booleans, integers, floats, complex numbers, strings, bytes, bytearrays, tuples, lists, sets, frozensets, dictionaries, and the singletons `None`, `Ellipsis`, and `StopIteration`). - `file_path` (str): Full path of the binary file to which the data will be serialized. - `version` (int): The version of the `marshal` format to use (default should be current version, i.e., 4). - **Output**: None - **Raises**: - `ValueError` if the data contains unsupported types. 2. Implement a function `deserialize_structure`: - **Input**: - `file_path` (str): Full path of the binary file from which the data will be deserialized. - **Output**: - The original Python data structure. - **Raises**: - `EOFError`, `ValueError`, or `TypeError` if the serialized data is not valid. 3. Implement a function `test_serialization`: - **Input**: None - **Output**: None - **Task**: Test your serialization and deserialization functionality with various supported and unsupported data types, ensuring data integrity is maintained where possible. Print errors if unsupported types are detected. Constraints: - You must handle the serialization and deserialization precisely to ensure the integrity of the data. - You can assume that the file operations will succeed, i.e., no need to handle file I/O errors. Example Usage: ```python def serialize_structure(data, file_path, version=4): # Your implementation here def deserialize_structure(file_path): # Your implementation here def test_serialization(): # Your implementation here if __name__ == \\"__main__\\": test_serialization() ``` **Notes:** - Ensure that the `test_serialization` function thoroughly tests the supported data types and prints appropriate messages. - `test_serialization` should include edge cases and potential pitfalls (e.g., very large integers, deeply nested structures).","solution":"import marshal def serialize_structure(data, file_path, version=4): Serializes the given data structure to the specified file using marshal. Parameters: - data: The data structure to be serialized. - file_path: The file path where the data should be serialized to. - version: The version of the marshal format to use. Raises: - ValueError if the data contains unsupported types. try: with open(file_path, \'wb\') as file: marshal.dump(data, file, version) except ValueError as e: raise ValueError(\\"Data contains unsupported types by marshal.\\") from e def deserialize_structure(file_path): Deserializes the data structure from the specified file using marshal. Parameters: - file_path: The file path from where the data should be deserialized. Returns: - The original data structure. Raises: - EOFError, ValueError, or TypeError if the serialized data is not valid. try: with open(file_path, \'rb\') as file: return marshal.load(file) except (EOFError, ValueError, TypeError) as e: raise e def test_serialization(): Tests the serialization and deserialization functions to verify their correctness. test_cases = [ {\\"data\\": 12345, \\"description\\": \\"Integer\\"}, {\\"data\\": 123.456, \\"description\\": \\"Float\\"}, {\\"data\\": True, \\"description\\": \\"Boolean\\"}, {\\"data\\": None, \\"description\\": \\"None\\"}, {\\"data\\": \\"Hello, World!\\", \\"description\\": \\"String\\"}, {\\"data\\": b\'Hello, World!\', \\"description\\": \\"Bytes\\"}, {\\"data\\": bytearray(b\'Hello, World!\'), \\"description\\": \\"Bytearray\\"}, {\\"data\\": [1, 2, 3, 4, 5], \\"description\\": \\"List\\"}, {\\"data\\": (1, 2, 3, 4, 5), \\"description\\": \\"Tuple\\"}, {\\"data\\": {1, 2, 3, 4, 5}, \\"description\\": \\"Set\\"}, {\\"data\\": frozenset([1, 2, 3, 4, 5]), \\"description\\": \\"Frozenset\\"}, {\\"data\\": {\\"a\\": 1, \\"b\\": 2}, \\"description\\": \\"Dictionary\\"}, {\\"data\\": (1 + 2j), \\"description\\": \\"Complex Number\\"}, {\\"data\\": Ellipsis, \\"description\\": \\"Ellipsis\\"}, {\\"data\\": StopIteration, \\"description\\": \\"StopIteration\\"}, {\\"data\\": {\\"nested\\": {\\"a\\": [1, 2, {\\"b\\": \\"string\\"}]}}, \\"description\\": \\"Nested Structures\\"} ] for case in test_cases: file_path = f\\"test_{case[\'description\'].replace(\' \', \'_\')}.bin\\" try: serialize_structure(case[\'data\'], file_path) result = deserialize_structure(file_path) assert result == case[\'data\'], f\\"Failed {case[\'description\']} case.\\" print(f\\"Passed {case[\'description\']} case.\\") except ValueError as e: print(f\\"Unsupported type detected in {case[\'description\']} case. Error: {e}\\") except Exception as e: print(f\\"Error in {case[\'description\']} case. Error: {e}\\") if __name__ == \\"__main__\\": test_serialization()"},{"question":"Objective Implement a Python class named `PyCell` that mimics the core behavior of the C-based `Cell` object described in the documentation. Your implementation should handle creation, getting, and setting the value of the cell. Requirements 1. **Class Definition**: - Define a class `PyCell` with methods to create, get, and set the cell\'s contents. 2. **Methods**: - `__init__(self, value: object) -> None`: Initializes the cell with a given value. The value can be `None`. - `check(self) -> bool`: Returns `True` if the object is a `PyCell` object. - `get(self) -> object`: Returns the contents of the cell. If the cell is empty, return `None`. - `set(self, value: object) -> None`: Sets the contents of the cell to the provided value. Input and Output 1. **Input**: - Initialization: `cell = PyCell(value)` - Method calls to `check()`, `get()`, and `set(value)`. 2. **Output**: - `check()`: `True` if the object is an instance of `PyCell`. - `get()`: The current value stored in the cell or `None`. - `set(value)`: No return value. Constraints - You should not use any special Python libraries other than standard built-ins. - Your implementation should correctly handle `None` values as well as normal Python objects. Example ```python # Example usage of PyCell cell = PyCell(5) print(cell.check()) # Output: True print(cell.get()) # Output: 5 cell.set(10) print(cell.get()) # Output: 10 cell.set(None) print(cell.get()) # Output: None ``` Implement the `PyCell` class below: ```python class PyCell: def __init__(self, value): pass def check(self): pass def get(self): pass def set(self, value): pass # Example usage: cell = PyCell(5) print(cell.check()) # Expected Output: True print(cell.get()) # Expected Output: 5 cell.set(10) print(cell.get()) # Expected Output: 10 cell.set(None) print(cell.get()) # Expected Output: None ```","solution":"class PyCell: def __init__(self, value): self.value = value def check(self): return isinstance(self, PyCell) def get(self): return self.value def set(self, value): self.value = value # Example usage: cell = PyCell(5) print(cell.check()) # Expected Output: True print(cell.get()) # Expected Output: 5 cell.set(10) print(cell.get()) # Expected Output: 10 cell.set(None) print(cell.get()) # Expected Output: None"},{"question":"Objective: Implement a function that demonstrates the student\'s ability to manipulate tuples using Python\'s low-level API operations as described in the provided documentation. Problem Statement: You need to mimic the functionality of creating a certain type of tuple, resizing it, and performing various operations on it in Python using lower-level tuple manipulation provided in the `python310` documentation. Implement the following function: ```python def manipulate_tuple(n, operations): Create a tuple of size `n`, perform given operations and return the result. Args: - n (int): The initial size of the tuple. - operations (list of tuples): Each tuple represents an operation to perform. Each operation is a tuple, where the first element is the operation name (str), and the subsequent elements are the arguments for that operation. Returns: - tuple: The resulting tuple after applying all operations. Supported Operations: - \\"append\\": Append a value to the tuple. Argument: value to append. - \\"insert\\": Insert a value at a specific position. Arguments: position, value. - \\"slice\\": Return a slice of the tuple. Arguments: start index, end index. - \\"size\\": Return the current size of the tuple. pass ``` Example Usage: ```python # Initialize a tuple of size 3 # Perform the following operations: # 1. Insert 100 at position 1 # 2. Append 200 # 3. Slice the tuple from index 1 to 3 # 4. Return the tuple size # The final result should be ((0, 200), 2) where the tuple is sliced to only the elements (0, 200) and the size of the sliced tuple is 2 result = manipulate_tuple(3, [(\\"insert\\", 1, 100), (\\"append\\", 200), (\\"slice\\", 1, 3), (\\"size\\",)]) print(result) # Expected output: ((0, 200), 2) ``` Constraints: - Initial size `n` of the tuple is guaranteed to be a non-negative integer. - Position for \\"insert\\" operation is guaranteed to be within the current bounds of the tuple before the operation. - The sequence of operations will be valid based on the intermediate state of the tuple. - Operations should be performed in the order they appear in the input list. Notes: - Always create a tuple of given size `n` filled with incrementing integers starting from 0. - Directly use low-level functions as described in the documentation wherever possible. Points to Consider: - Handle immutability of tuples appropriately. - Ensure edge cases, such as resizing operations that might trigger memory errors, are managed gracefully.","solution":"def manipulate_tuple(n, operations): Create a tuple of size `n`, perform given operations and return the result. Args: - n (int): The initial size of the tuple. - operations (list of tuples): Each tuple represents an operation to perform. Each operation is a tuple, where the first element is the operation name (str), and the subsequent elements are the arguments for that operation. Returns: - tuple: The resulting tuple after applying all operations. # Initialize a tuple of size n with incrementing integers starting from 0 t = tuple(range(n)) for operation in operations: if operation[0] == \\"append\\": # Append a value to the tuple t = t + (operation[1],) elif operation[0] == \\"insert\\": # Insert a value at a specific position pos = operation[1] value = operation[2] t = t[:pos] + (value,) + t[pos:] elif operation[0] == \\"slice\\": # Return a slice of the tuple start = operation[1] end = operation[2] t = t[start:end] elif operation[0] == \\"size\\": # Return the current size of the tuple return t, len(t) return t"},{"question":"**Problem Statement:** You are provided with a specific Python feature that deals with creating generic types using the `GenericAlias` class. Your task is to implement a function that utilizes this feature to create a generic container that can hold two different types of elements. **Function Requirements:** Implement a function `create_generic_container(type1, type2)`, which takes two type hints as arguments and returns a new generic container that can store elements of either `type1` or `type2`. **Input:** - `type1` (Type): The first type hint. - `type2` (Type): The second type hint. **Output:** - A new class that serves as a container for elements of either `type1` or `type2`. This class should use Python\'s type hinting system to enforce these types. **Constraints:** 1. The function must use the `GenericAlias` feature to create the generic container. 2. The container should have a method `add_element(element)` that adds an element to the container, and `__getitem__(index)` to retrieve an element by index, ensuring type safety. 3. The container must raise a `TypeError` if an element of an incorrect type is added. **Example:** ```python from typing import Type def create_generic_container(type1: Type, type2: Type): # Your code here # Example usage MyContainer = create_generic_container(int, str) container = MyContainer() container.add_element(5) # Should work container.add_element(\\"test\\") # Should work print(container[0]) # Should print 5 print(container[1]) # Should print \\"test\\" container.add_element(3.14) # Should raise TypeError ``` **Note:** To solve this problem, you may need to look up additional documentation on the `GenericAlias` class and how to use it in Python.","solution":"from typing import Type, List, Union, get_type_hints class GenericContainer: def __init__(self, type1, type2): self.type1 = type1 self.type2 = type2 self.elements = [] def add_element(self, element): if not isinstance(element, (self.type1, self.type2)): raise TypeError(f\\"Element must be of type {self.type1} or {self.type2}\\") self.elements.append(element) def __getitem__(self, index): return self.elements[index] def create_generic_container(type1: Type, type2: Type): container_class = type(\\"GenericContainer\\", (GenericContainer,), {\\"type1\\": type1, \\"type2\\": type2}) return container_class"},{"question":"**Coding Assessment Question** # Objective Implement a Python function that utilizes the `warnings` module to issue warnings based on certain conditions, filters them, and tests their occurrence. # Problem Statement You are required to create a function `warning_handler(n: int) -> List[Tuple[str, Warning]]` that performs the following tasks: 1. **Issues warnings**: - If `n` is negative, issue a `UserWarning` with the message \\"Negative value provided\\". - If `n` is greater than 100, issue a `DeprecationWarning` with the message \\"Value exceeds 100, deprecated functionality\\". - If `n` is zero, issue a `RuntimeWarning` with the message \\"Zero value provided\\". 2. **Filters warnings**: - Ignore `UserWarning` warnings. - Always display `RuntimeWarning` warnings. - Convert `DeprecationWarning` into exceptions. 3. **Tests warnings**: - Capture all raised warnings. - Return a list of tuples, where each tuple contains the warning message and the warning category. # Detailed Requirements - The function should take an integer `n` as input. - The function should use the `warnings` module to issue warnings as specified. - It should set up the appropriate warnings filters using `simplefilter()` or `filterwarnings()`. - If a `DeprecationWarning` is raised, it should be converted to an exception and captured. - The function should return a list of tuples, where each tuple contains the warning message (as a string) and the warning category (as a `Warning` subclass). # Example ```python def warning_handler(n: int): # Your implementation goes here # Example usage: result = warning_handler(-5) print(result) # Output: [] result = warning_handler(0) print(result) # Output: [(\\"Zero value provided\\", RuntimeWarning)] result = warning_handler(101) # Raises DeprecationWarning and catches it as an exception, then returns: # Output: [(\\"Value exceeds 100, deprecated functionality\\", DeprecationWarning)] ``` # Constraints 1. The function should handle all possible integer values for `n`. 2. The function should correctly filter and capture the specified warnings. # Notes - Use the `warnings.warn()` function to issue warnings. - Use the `warnings.simplefilter()` or `warnings.filterwarnings()` function to set up filters. - Use the `warnings.catch_warnings()` context manager to capture warnings. - You may assume that necessary imports from the `warnings` module are available.","solution":"import warnings from typing import List, Tuple def warning_handler(n: int) -> List[Tuple[str, Warning]]: result = [] with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") # Ignore UserWarning warnings.filterwarnings(\\"ignore\\", category=UserWarning) # Always show RuntimeWarning warnings.filterwarnings(\\"always\\", category=RuntimeWarning) # Convert DeprecationWarning to exceptions warnings.filterwarnings(\\"error\\", category=DeprecationWarning) # Issue warnings based on conditions if n < 0: warnings.warn(\\"Negative value provided\\", UserWarning) if n == 0: warnings.warn(\\"Zero value provided\\", RuntimeWarning) if n > 100: try: warnings.warn(\\"Value exceeds 100, deprecated functionality\\", DeprecationWarning) except DeprecationWarning as e: result.append((str(e), DeprecationWarning)) # Process captured warnings for warning in w: result.append((str(warning.message), warning.category)) return result"},{"question":"**Question: Advanced Text Analysis Using collections** You are tasked with implementing a function called `text_analysis` that performs advanced text analysis on a given list of strings. This function should achieve the following: 1. **Count Word Occurrences:** - Use the `Counter` class from the `collections` module to count the occurrences of each word across all strings in the list. 2. **Categorize Words by Initial Alphabet:** - Use the `defaultdict` class from the `collections` module to categorize words based on their starting alphabet. Each key in the defaultdict should be an alphabet letter, and the corresponding value should be a list of words starting with that letter. Ensure that each word in the resulting dictionary is unique. 3. **Return Format:** - The function should return a dictionary with two keys: - `\\"word_count\\"`: The `Counter` object containing word counts. - `\\"categorized_words\\"`: The `defaultdict` that categorizes words by their initial letter. # Function Signature ```python def text_analysis(strings: List[str]) -> Dict[str, Union[Counter, defaultdict]]: pass ``` # Input - `strings`: A list of strings, where each string may contain multiple words separated by spaces. (e.g., `[\\"the quick brown fox\\", \\"jumps over the lazy dog\\"]`). # Output - A dictionary with the following structure: ```python { \\"word_count\\": Counter({\\"word1\\": count1, \\"word2\\": count2, ...}), \\"categorized_words\\": defaultdict(list, {\'a\': [\'apple\', \'and\'], \'b\': [\'brown\'], ...}) } ``` # Constraints 1. The function should handle strings containing alphanumeric characters and standard punctuation. 2. The categorization should be case-insensitive (i.e., \\"Apple\\" and \\"apple\\" should be treated as the same word, starting with \'a\'). # Example ```python from collections import Counter, defaultdict from typing import List, Dict, Union def text_analysis(strings: List[str]) -> Dict[str, Union[Counter, defaultdict]]: word_count = Counter() categorized_words = defaultdict(list) for text in strings: words = text.lower().split() word_count.update(words) for word in words: categorized_words[word[0]].append(word) for key in categorized_words: categorized_words[key] = list(set(categorized_words[key])) return { \\"word_count\\": word_count, \\"categorized_words\\": categorized_words } # Example usage strings = [\\"the quick brown fox\\", \\"jumps over the lazy dog\\", \\"Quick Brown Fox\\"] result = text_analysis(strings) print(result) ``` In this example: - The `word_count` should be `Counter({\'quick\': 2, \'the\': 2, \'brown\': 2, \'fox\': 2, \'jumps\': 1, \'over\': 1, \'lazy\': 1, \'dog\': 1})`. - The `categorized_words` should be `defaultdict(<class \'list\'>, {\'t\': [\'the\'], \'q\': [\'quick\'], \'b\': [\'brown\'], \'f\': [\'fox\'], \'j\': [\'jumps\'], \'o\': [\'over\'], \'l\': [\'lazy\'], \'d\': [\'dog\']})`. # Note - The given solution is one potential implementation. Ensure your code is well-tested and handles edge cases, such as strings with punctuation.","solution":"from collections import Counter, defaultdict from typing import List, Dict, Union import re def text_analysis(strings: List[str]) -> Dict[str, Union[Counter, defaultdict]]: word_count = Counter() categorized_words = defaultdict(list) for text in strings: # Use regex to find words and convert to lower case words = re.findall(r\'bw+b\', text.lower()) word_count.update(words) for word in words: if word not in categorized_words[word[0]]: categorized_words[word[0]].append(word) return { \\"word_count\\": word_count, \\"categorized_words\\": categorized_words } # Example usage strings = [\\"the quick brown fox\\", \\"jumps over the lazy dog\\", \\"Quick Brown Fox\\"] result = text_analysis(strings) print(result)"},{"question":"# Email Parsing and Handling Defects **Objective:** Your task is to implement a function `parse_email_content` that takes in a string representing an email message and returns the parsed headers. The function must handle various exceptions as defined in the `email.errors` module and appropriately indicate defects detected in the email content. **Function Signature:** ```python def parse_email_content(email_content: str) -> dict: pass ``` **Input:** - `email_content` (str): A string representing the raw content of an email message. It includes headers and body. **Output:** - A dictionary with the following structure: - `\\"headers\\"` (dict): A dictionary of parsed headers. - `\\"defects\\"` (list): A list of detected defects, where each defect is represented by its class name as a string. **Requirements:** 1. Parse the headers from the given email content. 2. Detect and handle various parsing errors and defects as provided in the \\"email.errors\\" documentation. 3. Collect any defects found during parsing and include them in the output. 4. The function should handle but not raise exceptions, and should collect any detected issues. **Constraints:** - Assume the input string is non-empty and has no leading or trailing whitespace. - Focus on handling RFC 5322 compliant headers. - Performance requirements are not strict, but the implementation should avoid unnecessary complexity. **Example:** ```python email_content = From: example@example.com To: recipient@example.com Subject: Test email This is the body of the email. result = parse_email_content(email_content) print(result) # Expected output: # { # \\"headers\\": { # \\"From\\": \\"example@example.com\\", # \\"To\\": \\"recipient@example.com\\", # \\"Subject\\": \\"Test email\\" # }, # \\"defects\\": [] # } ``` Include at least one test case where the email content is malformed, such as missing a colon in a header, to test the defect detection capability. Submit your function implementation with at least three different test cases demonstrating various defect detections.","solution":"from email import message_from_string from email.errors import MessageDefect def parse_email_content(email_content: str) -> dict: Parses the email content and returns the headers and any defects found. Args: email_content (str): The raw content of an email message. Returns: dict: A dictionary with \\"headers\\" containing parsed headers, and \\"defects\\" containing a list of detected defect class names. # Parsing the email message from the given content msg = message_from_string(email_content) headers = {} defects = [] # Extracting the headers for key, value in msg.items(): headers[key] = value # Collecting defects for defect in msg.defects: if isinstance(defect, MessageDefect): defects.append(defect.__class__.__name__) return { \\"headers\\": headers, \\"defects\\": defects }"},{"question":"You are tasked with implementing custom encoding and decoding functions using Python\'s codec registry and support functions. Specifically, you\'ll need to implement functions that handle encoding and decoding using a given encoding type, including proper error handling mechanisms. # Objective 1. Implement a function `custom_encode` which takes a string and an encoding, and returns the encoded bytes using the specified encoding. 2. Implement a function `custom_decode` which takes encoded bytes and an encoding, and returns the decoded string using the specified encoding. 3. Implement a custom error handling function for encoding that replaces undecodable characters with an asterisk `*`, and register this error handling function under the name \\"custom_replace\\". # Function Signatures ```python def custom_encode(input_string: str, encoding: str) -> bytes: pass def custom_decode(encoded_data: bytes, encoding: str) -> str: pass def custom_replace_errors(exc: Exception) -> str: pass ``` # Requirements 1. **custom_encode**: - Input: `input_string` (a str), `encoding` (a str). - Output: Encoded bytes of the input string. - It should use the registered \\"custom_replace\\" error handler if provided. 2. **custom_decode**: - Input: `encoded_data` (bytes), `encoding` (a str). - Output: Decoded string. - It should use the default error handling method. 3. **custom_replace_errors**: - Input: `exc` (an Exception instance, which will be either `UnicodeEncodeError`, `UnicodeDecodeError`, or `UnicodeTranslateError`). - Output: A tuple containing a replacement string and an integer where encoding/decoding should resume. - Description: Replace undecodable characters with an asterisk `*`. # Constraints - You may assume the standard encodings (`utf-8`, `ascii`, etc.) are available. - Your solution should handle any string and byte sequence provided, including those that may raise encoding/decoding exceptions. # Example ```python try: custom_replace_errors(UnicodeEncodeError(\'ascii\', \'hello\', 1, 2, \'ordinal not in range\')) except Exception as e: print(e) # Should handle the exception without errors encoded_data = custom_encode(\\"hello world\\", \\"ascii\\") print(encoded_data) # Should print b\'hello world\' decoded_string = custom_decode(b\'hello world\', \\"ascii\\") print(decoded_string) # Should print \\"hello world\\" ``` Make sure to implement proper error handling and test your functions thoroughly.","solution":"import codecs def custom_encode(input_string: str, encoding: str) -> bytes: Encodes the input string using the specified encoding. If an error occurs, the custom_replace_errors handler is used. try: return input_string.encode(encoding, errors=\'custom_replace\') except LookupError: raise ValueError(f\\"Encoding not found: {encoding}\\") def custom_decode(encoded_data: bytes, encoding: str) -> str: Decodes the input bytes using the specified encoding. try: return encoded_data.decode(encoding) except LookupError: raise ValueError(f\\"Encoding not found: {encoding}\\") def custom_replace_errors(exc: Exception) -> (str, int): Custom error handler for encoding errors. Replaces undecodable characters with \'*\'. if isinstance(exc, (UnicodeEncodeError, UnicodeDecodeError, UnicodeTranslateError)): return (\'*\', exc.end) else: raise TypeError(f\\"Don\'t know how to handle {exc!r}\\") # Registering the custom_error handler codecs.register_error(\\"custom_replace\\", custom_replace_errors)"},{"question":"Advanced JSON Serialization and Deserialization with Custom Handling # Objective Implement custom JSON serialization and deserialization in Python using the `json` module. This exercise will test your understanding of Python\'s `json` module, including extending `JSONEncoder` and `JSONDecoder` classes, and using hooks for object transformation during the deserialization process. # Problem Statement Create a Python module that can serialize and deserialize Python objects containing complex numbers. Your module should implement the following functionalities: 1. **Serialization:** - Write a class `ComplexEncoder` that inherits from `json.JSONEncoder` and overrides the `default(self, obj)` method to handle serialization of complex numbers. Complex numbers should be represented as JSON objects with two properties: `real` and `imag`. 2. **Deserialization:** - Write a function `as_complex(dct)` that converts a dictionary with `real` and `imag` keys back into a complex number. - Extend the functionality of the `json.JSONDecoder` to handle deserialization of complex numbers using the `object_hook` parameter. 3. **Utility Functions:** - Write two functions `serialize_complex(obj: Any) -> str` and `deserialize_complex(s: str) -> Any`. The first function should use `ComplexEncoder` to serialize an object to a JSON string, and the second function should use the custom `as_complex` function to deserialize a JSON string back into a Python object. # Constraints - Complex numbers must be represented in JSON as objects: `{\\"real\\": [real_part], \\"imag\\": [imaginary_part]}`. - Ensure cyclic references are checked to avoid infinite recursions. # Input and Output - **Serialization Input:** A Python object which may contain complex numbers. - **Serialization Output:** A JSON formatted string. - **Deserialization Input:** A JSON formatted string containing serialized complex numbers. - **Deserialization Output:** A Python object with complex numbers restored. # Example Given the following Python object: ```python data = { \\"name\\": \\"Complex Number\\", \\"value\\": 3 + 4j, \\"components\\": [1 + 1j, 2 + 2j] } ``` **Serialization:** ```python json_str = serialize_complex(data) ``` Output JSON string: ```json { \\"name\\": \\"Complex Number\\", \\"value\\": {\\"real\\": 3, \\"imag\\": 4}, \\"components\\": [ {\\"real\\": 1, \\"imag\\": 1}, {\\"real\\": 2, \\"imag\\": 2} ] } ``` **Deserialization:** ```python restored_data = deserialize_complex(json_str) ``` Output Python object: ```python { \\"name\\": \\"Complex Number\\", \\"value\\": 3 + 4j, \\"components\\": [1 + 1j, 2 + 2j] } ``` # Implementation ```python import json class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"real\\": obj.real, \\"imag\\": obj.imag} return super().default(obj) def as_complex(dct): if \\"real\\" in dct and \\"imag\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct def serialize_complex(obj): return json.dumps(obj, cls=ComplexEncoder) def deserialize_complex(s): return json.loads(s, object_hook=as_complex) # Test your implementation if __name__ == \\"__main__\\": data = { \\"name\\": \\"Complex Number\\", \\"value\\": 3 + 4j, \\"components\\": [1 + 1j, 2 + 2j] } json_str = serialize_complex(data) print(\\"Serialized JSON:\\") print(json_str) restored_data = deserialize_complex(json_str) print(\\"Deserialized Python object:\\") print(restored_data) ```","solution":"import json class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, complex): return {\\"real\\": obj.real, \\"imag\\": obj.imag} return super().default(obj) def as_complex(dct): if \\"real\\" in dct and \\"imag\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct def serialize_complex(obj): return json.dumps(obj, cls=ComplexEncoder) def deserialize_complex(s): return json.loads(s, object_hook=as_complex)"},{"question":"# Building a Custom SMTP Client using smtplib **Objective:** Design and implement a custom SMTP client to send emails. The client should demonstrate understanding of fundamental and advanced concepts of the `smtplib` package. **Requirements:** 1. Your SMTP client should establish a connection to a SMTP server and login using provided credentials. 2. It should send an email with a subject and body to specified recipient(s). 3. The email should support both plaintext and HTML formats. 4. The client should be able to handle common exceptions and provide meaningful error messages. 5. The connection should use TLS for security. **Function Signature:** ```python def send_custom_email(host: str, port: int, username: str, password: str, from_addr: str, to_addrs: list, subject: str, body: str, is_html: bool): pass ``` **Parameters:** - `host` (str): The SMTP server host. - `port` (int): The port number to connect to on the SMTP server. - `username` (str): The username for authenticating with the SMTP server. - `password` (str): The password for authenticating with the SMTP server. - `from_addr` (str): The sender email address. - `to_addrs` (list): A list of recipient email addresses. - `subject` (str): The subject of the email. - `body` (str): The body content of the email. - `is_html` (bool): A flag indicating if the body is in HTML format. **Constraints:** - You may not use any third-party libraries for sending emails. - Ensure the subject and body are properly encoded and included in the email. - Only valid email formats should be treated as inputs for email addresses. **Implementation Details:** 1. Connect to the SMTP server using the provided host and port. 2. Use `starttls()` to upgrade the connection to TLS. 3. Log in using the provided username and password. 4. Construct the email message headers and body, with proper formatting for plaintext or HTML as specified. 5. Send the email to the specified recipients. 6. Handle any exceptions raised during the process, such as connection errors or authentication failures. 7. Close the connection properly using `quit()`. **Example Usage:** ```python try: send_custom_email( host=\'smtp.example.com\', port=587, username=\'your_username\', password=\'your_password\', from_addr=\'sender@example.com\', to_addrs=[\'recipient1@example.com\', \'recipient2@example.com\'], subject=\'Test Email\', body=\'<h1>This is a test email</h1>\', is_html=True ) print(\\"Email sent successfully!\\") except Exception as e: print(f\\"Failed to send email: {e}\\") ``` - In the example above, if `is_html` is set to `False`, the body should be plain text. **Grading Criteria:** - Correctness of the implementation. - Proper usage of `smtplib` methods and correct handling of network connections. - Exception handling and error messaging. - Code readability and adherence to Python best practices.","solution":"import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText def send_custom_email(host: str, port: int, username: str, password: str, from_addr: str, to_addrs: list, subject: str, body: str, is_html: bool): try: # Set up the SMTP server and start TLS for security server = smtplib.SMTP(host, port) server.starttls() # Log in to the SMTP server server.login(username, password) # Create the email msg = MIMEMultipart() msg[\'From\'] = from_addr msg[\'To\'] = \', \'.join(to_addrs) msg[\'Subject\'] = subject # Attach the email body in the specified format if is_html: msg.attach(MIMEText(body, \'html\')) else: msg.attach(MIMEText(body, \'plain\')) # Send the email server.sendmail(from_addr, to_addrs, msg.as_string()) # Terminate the SMTP session server.quit() except smtplib.SMTPException as e: raise Exception(f\\"SMTP error occurred: {str(e)}\\") except Exception as e: raise Exception(f\\"Failed to send email: {str(e)}\\")"},{"question":"# Question: Custom file wrapper with added functionality You are required to implement a custom wrapper for the built-in file object in Python. Your wrapper should extend the functionality of reading from and writing to files. Task: 1. Define a class `FileWrapper` that: - Wraps around a built-in file object. - When reading from the file, converts its content to lower-case. - When writing to the file, appends `-WRAPPER` to each line before writing. 2. Define a function `open_file_with_wrapper` that: - Opens a file using the built-in `open` function. - Returns an instance of `FileWrapper` wrapping the original file object. Constraints: - Assume the file will always be opened in text mode. - Ensure proper file handling (i.e., be mindful of resource management, like closing files). Example: ```python # Example usage: wrapper = open_file_with_wrapper(\'example.txt\', \'r+\') wrapper.write(\'Hello, World!n\') # Writes: \\"Hello, World!-WRAPPERn\\" content = wrapper.read() # Reads and converts to: \\"hello, world!n\\" print(content) ``` The corresponding file should have the written text converted and read text should appear as lower-case when read through `FileWrapper`. Input - `path`: A string representing the path to the file. - `mode`: A string representing the mode in which to open the file (e.g., \'r\', \'w\', \'r+\', etc.). Output - Return an instance of `FileWrapper` that wraps the opened file. Implementation details - Ensure the `FileWrapper` has at least the following methods: - `read(self, count=-1)` - Reads from the file, converts the content to lower-case, and returns it. - `write(self, data)` - Appends `-WRAPPER` to each line of data before writing it to the file. - Proper handling for other standard file methods like `close`, `flush`, etc. You need to complete the `FileWrapper` class and the `open_file_with_wrapper` function based on the given specifications.","solution":"class FileWrapper: def __init__(self, file): self._file = file def read(self, count=-1): content = self._file.read(count) return content.lower() if content else content def write(self, data): lines = data.splitlines(keepends=True) modified_lines = [line.rstrip(\'n\') + \'-WRAPPERn\' for line in lines] self._file.writelines(modified_lines) def close(self): self._file.close() def flush(self): self._file.flush() def __enter__(self): return self def __exit__(self, type, value, traceback): self.close() def open_file_with_wrapper(path, mode): file = open(path, mode) return FileWrapper(file)"},{"question":"Pandas Options API Coding Assessment # Objective The goal of this task is to evaluate your understanding of the `pandas` options and settings API, including setting, retrieving, and resetting various configuration options. # Task You are given a DataFrame and a series of configuration tasks to modify its display options. Your task is to write a function that performs these operations and outputs the resulting DataFrame configurations as a dictionary. # Description Implement a function `configure_pandas_options(df: pd.DataFrame) -> dict` in Python that performs the following steps: 1. **Set the maximum number of rows** displayed (`display.max_rows`) to 10. 2. **Set the precision** for displaying numerical values (`display.precision`) to 4 decimal places. 3. **Set the maximum column width** (`display.max_colwidth`) to 50 characters. 4. **Set the maximum information columns** (`display.max_info_columns`) to 10. 5. Use the `pd.option_context` to temporarily: - Set the maximum columns displayed (`display.max_columns`) to 5. - Print the current configuration of `display.max_columns`. 6. Return to the original state outside the context, and print the configuration of `display.max_columns` to confirm it has been restored. 7. Store the final configurations of all above options into a dictionary and return this dictionary. # Constraints - You should use `pandas` version 1.1.5 or later. - Only the listed display options may be changed. # Example ```python import pandas as pd def configure_pandas_options(df: pd.DataFrame) -> dict: # Your code here pass # Example DataFrame df = pd.DataFrame({ \'A\': range(15), \'B\': range(15, 30), \'C\': [\'long_string\' * 5] * 15, \'D\': [0.123456789] * 15, \'E\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] }) configurations = configure_pandas_options(df) print(configurations) # Possible Output: # {\'display.max_rows\': 10, # \'display.precision\': 4, # \'display.max_colwidth\': 50, # \'display.max_info_columns\': 10, # \'display.max_columns\': 20} # or the default value of display.max_columns ``` # Expected Output The `configure_pandas_options()` function should return a dictionary containing the configurations for `display.max_rows`, `display.precision`, `display.max_colwidth`, `display.max_info_columns`, and `display.max_columns` after the temporary setting has reverted back.","solution":"import pandas as pd def configure_pandas_options(df: pd.DataFrame) -> dict: # Step 1: Set the maximum number of rows displayed to 10 pd.set_option(\'display.max_rows\', 10) # Step 2: Set the precision for displaying numerical values to 4 decimal places pd.set_option(\'display.precision\', 4) # Step 3: Set the maximum column width to 50 characters pd.set_option(\'display.max_colwidth\', 50) # Step 4: Set the maximum information columns to 10 pd.set_option(\'display.max_info_columns\', 10) # Step 5: Temporarily set the maximum columns displayed to 5 with pd.option_context(\'display.max_columns\', 5): max_columns_context = pd.get_option(\'display.max_columns\') print(f\\"Inside context - display.max_columns: {max_columns_context}\\") # Confirming outside the context max_columns_outside_context = pd.get_option(\'display.max_columns\') print(f\\"Outside context - display.max_columns: {max_columns_outside_context}\\") # Store final configurations in a dictionary config_dict = { \'display.max_rows\': pd.get_option(\'display.max_rows\'), \'display.precision\': pd.get_option(\'display.precision\'), \'display.max_colwidth\': pd.get_option(\'display.max_colwidth\'), \'display.max_info_columns\': pd.get_option(\'display.max_info_columns\'), \'display.max_columns\': max_columns_outside_context # Confirming max_columns outside the context } return config_dict"},{"question":"You are tasked with developing a multilingual text-based application that serves weather updates in various languages. Implement the backend of this application where it can dynamically switch between two languages (English and French) and retrieve the appropriate translated messages for given weather updates. # Objectives 1. **Setup translations:** Generate mappings for English and French translations for a set of weather messages. 2. **Implement translation functions:** Write functions to retrieve the translations for the weather messages in the currently selected language. 3. **Switch languages dynamically:** Implement a functionality to switch between English and French translations as needed. # Instructions 1. **Create necessary mappings:** - Define dictionaries for each language with translations for the following messages: ``` \\"Weather update: It\'s sunny today!\\" \\"Weather update: It\'s raining today!\\" \\"Weather update: It might snow today.\\" ``` 2. **Define a Translation class:** - Implement a `WeatherTranslations` class that loads these weather messages and can switch between the provided languages. - Methods to implement: - `__init__(self, translations)`: Initializes the class with a dictionary of translations. - `set_language(self, language)`: Sets the current language for the translations. - `get_translation(self, message)`: Retrieves the translated message based on the current language. 3. **Demonstrate usage:** - Initialize the `WeatherTranslations` class with both English and French translations. - Switch to French and print the French translations of the messages. - Switch back to English and print the English translations of the messages. # Input and Output - **Input:** Embed all inputs within the code (no user input required). - **Output:** Print the translated weather messages based on the current language setting. # Example ```python translations = { \'en\': { \\"Weather update: It\'s sunny today!\\": \\"Weather update: It\'s sunny today!\\", \\"Weather update: It\'s raining today!\\": \\"Weather update: It\'s raining today!\\", \\"Weather update: It might snow today.\\": \\"Weather update: It might snow today.\\" }, \'fr\': { \\"Weather update: It\'s sunny today!\\": \\"Mise à jour météo : Il fait beau aujourd\'hui !\\", \\"Weather update: It\'s raining today!\\": \\"Mise à jour météo : Il pleut aujourd\'hui !\\", \\"Weather update: It might snow today.\\": \\"Mise à jour météo : Il pourrait neiger aujourd\'hui.\\" } } # Initialize class wt = WeatherTranslations(translations) # Switch to French wt.set_language(\'fr\') print(wt.get_translation(\\"Weather update: It\'s sunny today!\\")) # Should print French translation # Switch to English wt.set_language(\'en\') print(wt.get_translation(\\"Weather update: It\'s raining today!\\")) # Should print English translation ``` # Constraints and Notes - Assume the initial language is English. - Ensure that the class can handle invalid language codes gracefully. - Make sure to document your code and ensure readability.","solution":"class WeatherTranslations: def __init__(self, translations): Initializes the WeatherTranslations with a dictionary of translations. self.translations = translations self.current_language = \'en\' # Default language is English def set_language(self, language): Sets the current language for translations. Args: language (str): The language code to set. if language in self.translations: self.current_language = language else: raise ValueError(\\"Unsupported language\\") def get_translation(self, message): Retrieves the translated message based on the current language. Args: message (str): The message to translate. Returns: str: The translated message. return self.translations[self.current_language].get(message, message) # Example usage translations = { \'en\': { \\"Weather update: It\'s sunny today!\\": \\"Weather update: It\'s sunny today!\\", \\"Weather update: It\'s raining today!\\": \\"Weather update: It\'s raining today!\\", \\"Weather update: It might snow today.\\": \\"Weather update: It might snow today.\\" }, \'fr\': { \\"Weather update: It\'s sunny today!\\": \\"Mise à jour météo : Il fait beau aujourd\'hui !\\", \\"Weather update: It\'s raining today!\\": \\"Mise à jour météo : Il pleut aujourd\'hui !\\", \\"Weather update: It might snow today.\\": \\"Mise à jour météo : Il pourrait neiger aujourd\'hui.\\" } } # Initialize class wt = WeatherTranslations(translations) # Switch to French wt.set_language(\'fr\') print(wt.get_translation(\\"Weather update: It\'s sunny today!\\")) # Should print French translation # Switch to English wt.set_language(\'en\') print(wt.get_translation(\\"Weather update: It\'s raining today!\\")) # Should print English translation"},{"question":"# Persistent Contact Manager **Objective:** You are tasked with creating a simple persistent contact manager using the \\"shelve\\" module. This contact manager will be able to store, retrieve, update, and delete contact information. **Requirements:** 1. **Function Implementations**: - `add_contact(filename: str, name: str, details: dict) -> None` - Adds a new contact with the provided name and details. - `get_contact(filename: str, name: str) -> dict` - Retrieves the contact details for the given name. - `update_contact(filename: str, name: str, details: dict) -> None` - Updates the contact details for the given name. - `delete_contact(filename: str, name: str) -> None` - Deletes the contact identified by the given name. - `list_contacts(filename: str) -> list` - Returns a list of all contact names stored in the contact manager. 2. **Constraints**: - `filename` refers to the name of the shelf file where contacts are stored. - `details` is a dictionary containing at least the following keys: `phone`, `email`. 3. **Additional Requirements**: - Ensure that you use the \\"shelve\\" module. - Manage the shelf file effectively by closing it after each operation or using it in a context manager. **Input and Output Formats:** - The `add_contact` method adds a contact without returning anything. - The `get_contact` method retrieves contact details as a dictionary. - The `update_contact` method updates a contact without returning anything. - The `delete_contact` method deletes a contact without returning anything. - The `list_contacts` method returns a list of keys (contact names). **Performance Constraints:** - Operations should be handled efficiently, keeping memory usage in mind, especially for large number of contacts. **Example:** ```python filename = \'contacts\' # Adding a contact add_contact(filename, \'John Doe\', {\'phone\': \'123-456-7890\', \'email\': \'john@example.com\'}) # Getting a contact print(get_contact(filename, \'John Doe\')) # Output: {\'phone\': \'123-456-7890\', \'email\': \'john@example.com\'} # Updating a contact update_contact(filename, \'John Doe\', {\'phone\': \'987-654-3210\', \'email\': \'johnny@example.com\'}) # Deleting a contact delete_contact(filename, \'John Doe\') # Listing all contacts print(list_contacts(filename)) # Output: [] ``` Please implement the specified methods to fulfill the requirements of the contact manager using the \\"shelve\\" module.","solution":"import shelve def add_contact(filename: str, name: str, details: dict) -> None: with shelve.open(filename) as db: db[name] = details def get_contact(filename: str, name: str) -> dict: with shelve.open(filename) as db: return db.get(name, {}) def update_contact(filename: str, name: str, details: dict) -> None: with shelve.open(filename) as db: if name in db: db[name] = details def delete_contact(filename: str, name: str) -> None: with shelve.open(filename) as db: if name in db: del db[name] def list_contacts(filename: str) -> list: with shelve.open(filename) as db: return list(db.keys())"},{"question":"# Audio Processing with audioop Question You are required to implement a function that processes stereo audio fragments to: 1. Convert the stereo audio fragment into two mono fragments (left and right channels). 2. Apply different gain factors to the left and right channels. 3. Recombine the modified mono channels back into a stereo fragment. 4. Return the final processed stereo fragment. **Function Signature:** ```python def process_stereo_audio(fragment: bytes, width: int, lfactor: float, rfactor: float) -> bytes: ``` **Input:** - `fragment` (bytes): The original stereo audio fragment. - `width` (int): The sample width in bytes, can be 1, 2, 3, or 4. - `lfactor` (float): The gain factor to be applied to the left channel. - `rfactor` (float): The gain factor to be applied to the right channel. **Output:** - Returns a bytes object, representing the processed stereo audio fragment. **Constraints:** - The length of the audio fragment is divisible by (2 * width). - Gain factors can be any floating-point numbers (positive, negative, or zero). **Task:** - Split the stereo audio into left and right mono channels. - Apply the respective gain factors. - Recombine the mono channels back into the stereo format. - Handle cases where the gain might cause overflow by wrapping the samples around. **Example:** ```python # Example usage fragment = b\'x00x01x02x03x04x05x06x07\' # Example stereo audio, narrow in size just for illustration width = 2 lfactor = 1.5 rfactor = 0.5 output_fragment = process_stereo_audio(fragment, width, lfactor, rfactor) print(output_fragment) ``` **Note:** - You may use helper functions if needed. - Ensure you handle potential overflow properly while applying the gain factors. # Implementation Requirements: 1. Use the `audioop` module to split, manipulate, and recombine audio fragments. 2. Validate inputs to ensure proper lengths and widths. 3. Optimize the function for performance given the constraints. You may refer to the `audioop` module documentation provided for help with specific functions.","solution":"import audioop def process_stereo_audio(fragment: bytes, width: int, lfactor: float, rfactor: float) -> bytes: Processes a stereo audio fragment by splitting and applying different gain factors to left and right channels. Args: - fragment (bytes): The original stereo audio fragment. - width (int): The sample width in bytes, can be 1, 2, 3, or 4. - lfactor (float): The gain factor to be applied to the left channel. - rfactor (float): The gain factor to be applied to the right channel. Returns: - bytes: The processed stereo audio fragment. # Split stereo audio into left and right mono channels left_channel = audioop.tomono(fragment, width, 1, 0) right_channel = audioop.tomono(fragment, width, 0, 1) # Apply gain factors left_channel = audioop.mul(left_channel, width, lfactor) right_channel = audioop.mul(right_channel, width, rfactor) # Recombine into stereo processed_fragment = audioop.tostereo(left_channel, width, 1, 0) processed_fragment = audioop.add(processed_fragment, audioop.tostereo(right_channel, width, 0, 1), width) return processed_fragment"},{"question":"Problem Description You are provided with a dataset that contains features for predicting a certain target. Your task is to implement a function that benchmarks different scikit-learn models based on prediction latency and throughput using various configurations. You will need to consider the number of features, sparsity of the data, model complexity, and the mode of prediction (bulk vs atomic). Function Signature ```python from sklearn.base import BaseEstimator import numpy as np from typing import Dict, Tuple def benchmark_models(config: Dict, models: Dict[str, BaseEstimator], X: np.ndarray, y: np.ndarray) -> Dict[str, Dict[str, float]]: Benchmarks scikit-learn models based on prediction latency and throughput. Parameters: - config: Dictionary with configuration settings for benchmarking. Example format: { \\"n_features\\": int, # Number of features to consider \\"sparsity_ratio\\": float, # Desired sparsity ratio of the input data \\"bulk_mode\\": bool, # True for bulk prediction, False for atomic prediction \\"working_memory\\": int # Maximum working memory in MiB for chunked operations } - models: Dictionary of scikit-learn model instances to be benchmarked. Example format: { \\"LinearRegression\\": LinearRegression(), \\"RandomForest\\": RandomForestRegressor(n_estimators=100), ... } - X: Numpy array of input features (shape: [n_samples, n_features]) - y: Numpy array of target values (shape: [n_samples]) Returns: - results: Dictionary with benchmark results. Example format: { \\"LinearRegression\\": {\\"latency\\": float, \\"throughput\\": float}, \\"RandomForest\\": {\\"latency\\": float, \\"throughput\\": float}, ... } pass ``` Input and Output **Input:** 1. `config` (dict): Configuration settings that include: - `n_features` (int): Number of features to consider. - `sparsity_ratio` (float): Desired sparsity ratio of the input data. - `bulk_mode` (bool): True for bulk prediction, False for atomic prediction. - `working_memory` (int): Maximum working memory in MiB for chunked operations. 2. `models` (dict): Dictionary of scikit-learn model instances to be benchmarked. Each key is a model name, and each value is an instantiated scikit-learn model. 3. `X` (np.ndarray): Numpy array of input features with shape `[n_samples, n_features]`. 4. `y` (np.ndarray): Numpy array of target values with shape `[n_samples]`. **Output:** 1. `results` (dict): Dictionary with benchmark results, where each key is a model name and each value is a dictionary with latency and throughput values. - `latency` (float): Average prediction latency in microseconds. - `throughput` (float): Number of predictions per second. Constraints 1. You may assume that `X` and `y` contain valid numeric data. 2. The input feature matrix `X` should be modified according to the desired `sparsity_ratio`. 3. Latency should be measured in microseconds. 4. Throughput should be measured in predictions per second. 5. Predictions should be done on the whole dataset if `bulk_mode` is True, otherwise one by one. Performance Requirements - Ensure your implementation is efficient and can handle large datasets within reasonable execution time. - Make use of scikit-learn\'s configuration options to optimize prediction performance, such as disabling certain validation checks for faster predictions. - Use numpy, scipy, and other appropriate modules for efficient computation and handling of sparse data. Example ```python from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor import numpy as np config = { \\"n_features\\": 100, \\"sparsity_ratio\\": 0.9, \\"bulk_mode\\": True, \\"working_memory\\": 128 } models = { \\"LinearRegression\\": LinearRegression(), \\"RandomForest\\": RandomForestRegressor(n_estimators=10) } X = np.random.rand(1000, 200) y = np.random.rand(1000) results = benchmark_models(config, models, X, y) print(results) # Example Output: # { # \\"LinearRegression\\": {\\"latency\\": 10.0, \\"throughput\\": 100000.0}, # \\"RandomForest\\": {\\"latency\\": 100.0, \\"throughput\\": 10000.0} # } ``` Good luck!","solution":"import numpy as np import time from typing import Dict, Tuple from sklearn.base import BaseEstimator from scipy.sparse import csr_matrix def benchmark_models(config: Dict, models: Dict[str, BaseEstimator], X: np.ndarray, y: np.ndarray) -> Dict[str, Dict[str, float]]: Benchmarks scikit-learn models based on prediction latency and throughput. Parameters: - config: Dictionary with configuration settings for benchmarking. - models: Dictionary of scikit-learn model instances to be benchmarked. - X: Numpy array of input features (shape: [n_samples, n_features]) - y: Numpy array of target values (shape: [n_samples]) Returns: - results: Dictionary with benchmark results. # Adjust X for the number of features and sparsity ratio if config[\\"n_features\\"] != X.shape[1]: X = X[:, :config[\\"n_features\\"]] if config[\\"sparsity_ratio\\"] > 0: X = np.where(np.random.rand(*X.shape) < config[\\"sparsity_ratio\\"], 0, X) X = csr_matrix(X) # convert to sparse matrix results = {} n_samples = X.shape[0] for model_name, model in models.items(): # Fit the model model.fit(X, y) # Benchmarking latency and throughput if config[\\"bulk_mode\\"]: # Bulk mode start_time = time.time() model.predict(X) end_time = time.time() latency = (end_time - start_time) * 1e6 / n_samples # in microseconds throughput = n_samples / (end_time - start_time) # predictions per second else: # Atomic mode start_time = time.time() for i in range(n_samples): model.predict(X[i].reshape(1, -1)) end_time = time.time() latency = (end_time - start_time) * 1e6 / n_samples # in microseconds throughput = n_samples / (end_time - start_time) # predictions per second results[model_name] = {\\"latency\\": latency, \\"throughput\\": throughput} return results"},{"question":"# Flask Deployment Challenge You have developed a simple Flask application and now it\'s time to deploy it to a production environment. Your task is to write a script that handles the following deployment steps: 1. **Build the Application**: - Create a distribution (`.whl`) file of the Flask application using the `build` tool. 2. **Initialize the Database**: - Write a script to initialize the database in the production environment. 3. **Configure the Application**: - Generate a random `SECRET_KEY` and create a `config.py` file with this key for secure deployment. 4. **Run the Application with Waitress**: - Write a command to run the Flask application using the Waitress WSGI server. Expected Input and Output - **Input**: All steps should be implemented within a Python script or a set of scripts. - **Output**: The output will not be directly checked, but the correct execution of these steps should result in a securely deployed Flask application accessible in a production environment. Constraints and Limitations * The Flask application is located in a directory named `flaskr` and follows the structure outlined in the Flask documentation tutorials. * You should not modify the existing application code, but all deployment scripts and configurations should reside outside this codebase. * Ensure that any secrets or sensitive information is generated securely and not hardcoded. Guidelines 1. **Build the Application**: - Install the `build` tool and use it to create the distribution file. 2. **Initialize the Database**: - Assume the command to initialize the database is `flask --app flaskr init-db`. - Ensure this command is executed in the production environment correctly. 3. **Configure the Application**: - Use Python to generate a random secret key. - Create a `config.py` file with this `SECRET_KEY` in the correct directory for the production environment. 4. **Run the Application**: - Use the `waitress-serve` command to run the Flask application. You can assume that `pip` and `virtualenv` are already installed in the environment where your deployment script will run.","solution":"import os import secrets import subprocess def build_application(): Builds the Flask application into a distribution file using the build tool. subprocess.run([\'pip\', \'install\', \'build\'], check=True) subprocess.run([\'python\', \'-m\', \'build\'], check=True) print(\\"Application built successfully.\\") def initialize_database(): Initializes the database for the Flask application. subprocess.run([\'flask\', \'--app\', \'flaskr\', \'init-db\'], check=True) print(\\"Database initialized successfully.\\") def configure_application(): Generates a random SECRET_KEY and writes it to the config.py file. secret_key = secrets.token_urlsafe(32) config_content = f\\"SECRET_KEY = \'{secret_key}\'n\\" with open(\'flaskr/config.py\', \'w\') as config_file: config_file.write(config_content) print(\\"Configuration complete with SECRET_KEY set.\\") def run_application(): Runs the Flask application using Waitress WSGI server. subprocess.run([ \'waitress-serve\', \'--call\', \'flaskr:create_app\' ], check=True) print(\\"Application is now running with Waitress.\\") def deploy(): Deploys the Flask application by building, configuring, initializing the database, and running it. build_application() initialize_database() configure_application() run_application()"},{"question":"# MLPClassifier Coding Assessment **Objective:** Implement a classification task using `MLPClassifier` from scikit-learn. **Problem:** You are given a dataset containing information about various types of wines. Your task is to build a Multi-layer Perceptron (MLP) classifier to categorize the wines into three classes based on their features. **Dataset:** You will use the Wine dataset available from the UCI Machine Learning Repository. The dataset consists of 178 samples with 13 numeric features. The target variable is a categorical value representing the class of wine (1, 2, or 3). **Requirements:** 1. Load the dataset from `sklearn.datasets`. 2. Preprocess the dataset by standardizing the features. 3. Split the dataset into training and testing sets. 4. Implement an `MLPClassifier` model. 5. Train the model on the training data. 6. Evaluate the model on the testing data. 7. Report the classification accuracy and provide the confusion matrix. **Specifications:** 1. **Input:** - No input parameters; load the wine dataset directly. 2. **Output:** - Print the classification accuracy. - Print the confusion matrix. 3. **Constraints:** - Use `StandardScaler` for feature scaling. - Use `train_test_split` with a test set size of 30% of the data. - Use a random state of `42` for reproducibility. - Implement an MLP with at least one hidden layer. - You must use `MLPClassifier` from `scikit-learn`. **Performance Requirements:** - Classification accuracy should be reasonably high (aim for at least 80% on the test data). # Boilerplate Code: ```python from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import classification_report, confusion_matrix # Load the dataset wine = load_wine() X = wine.data y = wine.target # Preprocess the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # Implement an MLPClassifier model and train it mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42, solver=\'adam\') mlp.fit(X_train, y_train) # Make predictions on the test data y_pred = mlp.predict(X_test) # Evaluate the model accuracy = mlp.score(X_test, y_test) conf_matrix = confusion_matrix(y_test, y_pred) # Output the results print(f\\"Classification Accuracy: {accuracy:.2f}\\") print(\\"Confusion Matrix:\\") print(conf_matrix) ``` **Note:** Ensure that your implementation adheres to the constraints and check your results using the provided boilerplate code as a reference. **Hint:** For more detailed analysis of the model\'s performance, you may use the `classification_report` from `sklearn.metrics`.","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import classification_report, confusion_matrix def wine_classification(): # Load the dataset wine = load_wine() X = wine.data y = wine.target # Preprocess the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # Implement an MLPClassifier model and train it mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42, solver=\'adam\') mlp.fit(X_train, y_train) # Make predictions on the test data y_pred = mlp.predict(X_test) # Evaluate the model accuracy = mlp.score(X_test, y_test) conf_matrix = confusion_matrix(y_test, y_pred) # Output the results return accuracy, conf_matrix"},{"question":"# Advanced Python Coding Assessment Objective Demonstrate your understanding of the `resource` module to monitor and limit system resource usage in Python. Problem Statement Write a Python function `limit_and_monitor_resource` that performs the following tasks: 1. Sets a limit on the maximum CPU time (in seconds) a process can use. 2. Monitors and prints resource usage information (CPU time, memory usage, etc.) of the process performing a specified task. Function Signature ```python def limit_and_monitor_resource(cpu_limit: int, task: callable) -> None: ... ``` Input - `cpu_limit` (int): The maximum amount of CPU time (in seconds) that the process can use. If the process exceeds this limit, it should be terminated. - `task` (callable): A function representing the task whose resource usage will be monitored. Output - The function should not return any value. - Instead, it should print: - The current resource limits before and after setting the new CPU limit. - Resource usage information for the task after it has completed or has been terminated. Constraints - Ensure `task` is executed even if it might exceed the CPU time limit. - Handle exceptions that may arise during resource limiting and monitoring. Example ```python import resource import time def sample_task(): for i in range(10 ** 8): _ = 1 + 1 limit_and_monitor_resource(1, sample_task) ``` **Expected Output:** ``` Current CPU limits (before): (soft limit, hard limit) New CPU limits (after): (1, hard limit) Resource usage after task completion: (ru_utime, ru_stime, ..., ru_nivcsw) ``` **Note:** The actual values printed for resource usage will vary based on system and task execution. Performance Requirements - Ensure the monitoring function has minimal overhead to not significantly affect the task\'s performance. - The function should handle short tasks that complete before the CPU limit is reached, as well as long tasks that might be terminated upon hitting the CPU limit. Additional Notes - Use the `resource` module to set and get resource limits (`getrlimit`, `setrlimit`). - Use `getrusage` to monitor the resource usage of the calling process (`RUSAGE_SELF`).","solution":"import resource import time import os import signal def limit_and_monitor_resource(cpu_limit: int, task: callable) -> None: Function to set CPU time limit and monitor resource usage for a given task. Parameters: - cpu_limit (int): Maximum CPU time in seconds. - task (callable): Function representing the task to be monitored. def handle_timeout(signum, frame): raise TimeoutError(\\"CPU time limit exceeded\\") # Get current CPU usage limits old_limits = resource.getrlimit(resource.RLIMIT_CPU) try: print(f\\"Current CPU limits (before): {old_limits}\\") # Set the new CPU limit resource.setrlimit(resource.RLIMIT_CPU, (cpu_limit, old_limits[1])) # Check the new limits to confirm they\'ve been set new_limits = resource.getrlimit(resource.RLIMIT_CPU) print(f\\"New CPU limits (after): {new_limits}\\") # Set the signal handler for CPU time limit signal.signal(signal.SIGXCPU, handle_timeout) # Execute the task try: task() except TimeoutError as e: print(e) # Get resource usage after task completion usage = resource.getrusage(resource.RUSAGE_SELF) print(f\\"Resource usage after task completion: {usage}\\") finally: # Restore original CPU limits resource.setrlimit(resource.RLIMIT_CPU, old_limits)"},{"question":"# Advanced Set Operations in Python Objective: Demonstrate your understanding of fundamental and advanced concepts related to Python set operations. Implement a function that performs a series of operations on input sets and returns the desired results. Problem Statement: You are given two sets of integers, `A` and `B`. Write a function `set_operations(ops_list, A, B)` that takes in a list of operations and two input sets, and applies those operations in the given sequence. The function should execute the operations and return the final set after all operations have been applied. The operations in the `ops_list` will be a list of strings where each string specifies an operation and can be one of the following: 1. `\\"union\\"`: Performs the union of set A and set B. 2. `\\"intersection\\"`: Performs the intersection of set A and set B. 3. `\\"difference A-B\\"`: Performs the difference of set A minus set B. 4. `\\"difference B-A\\"`: Performs the difference of set B minus set A. 5. `\\"symmetric_difference\\"`: Performs the symmetric difference between set A and set B. 6. `\\"add X A\\"`: Adds element X (integer) to set A. 7. `\\"add X B\\"`: Adds element X (integer) to set B. 8. `\\"discard X A\\"`: Discards element X (integer) from set A. 9. `\\"discard X B\\"`: Discards element X (integer) from set B. 10. `\\"pop A\\"`: Removes and returns an arbitrary element from set A. 11. `\\"pop B\\"`: Removes and returns an arbitrary element from set B. The operations should be applied sequentially in the order they appear in the `ops_list`. For the `pop A` and `pop B` operations, you can assume that the set will always have at least one element when the operation is called. Input: - `ops_list`: A list of strings, each representing an operation (as defined above). - `A`: A set of integers. - `B`: A set of integers. Output: - The resultant set after all operations have been applied. Constraints: 1. `1 <= len(ops_list) <= 100` 2. Elements in the sets `A` and `B` will be integers and their values will be in the range `-10^6` to `10^6`. Example: ```python def set_operations(ops_list, A, B): # Implementation goes here. # Example Usage ops_list = [\\"union\\", \\"add 3 A\\", \\"add 4 B\\", \\"intersection\\", \\"symmetric_difference\\"] A = {1, 2, 3} B = {3, 4, 5} print(set_operations(ops_list, A, B)) # Output will show the resulting set after performing all operations. ```","solution":"def set_operations(ops_list, A, B): for op in ops_list: if op == \\"union\\": A = A.union(B) elif op == \\"intersection\\": A = A.intersection(B) elif op == \\"difference A-B\\": A = A.difference(B) elif op == \\"difference B-A\\": A = B.difference(A) elif op == \\"symmetric_difference\\": A = A.symmetric_difference(B) elif op.startswith(\\"add\\"): _, x, target_set = op.split() x = int(x) if target_set == \\"A\\": A.add(x) elif target_set == \\"B\\": B.add(x) elif op.startswith(\\"discard\\"): _, x, target_set = op.split() x = int(x) if target_set == \\"A\\": A.discard(x) elif target_set == \\"B\\": B.discard(x) elif op == \\"pop A\\": A.pop() elif op == \\"pop B\\": B.pop() return A"},{"question":"You are tasked with implementing a set of functions to read and write Sun AU files using the \\"sunau\\" module. Your functions should demonstrate your understanding of file I/O operations, handling binary data, and working with audio file metadata. # Tasks: 1. **Implement a function `read_au_file(file_path)` that:** - Takes a single argument `file_path` (a string representing the path to an AU file). - Opens the AU file in read mode. - Extracts and returns the following metadata from the AU file as a dictionary: - `number_of_channels`: number of audio channels. - `sample_width`: sample width in bytes. - `frame_rate`: sampling frequency. - `number_of_frames`: number of audio frames. - `compression_type`: the compression type (e.g. \'ULAW\', \'ALAW\', \'NONE\'). - `compression_name`: human-readable version of the compression type. 2. **Implement a function `write_au_file(file_path, params, audio_data)` that:** - Takes three arguments: - `file_path` (a string representing the path where the AU file will be written). - `params` (a dictionary containing the following keys: `number_of_channels`, `sample_width`, `frame_rate`, `number_of_frames`, `compression_type`, `compression_name`). - `audio_data` (a bytes object representing the audio data to be written). - Opens the file in write mode and sets the parameters based on the dictionary `params`. - Writes the audio data from `audio_data`. - Closes the file. # Example ```python # Example usage of read_au_file metadata = read_au_file(\'example.au\') print(metadata) # Output: # { # \'number_of_channels\': 2, # \'sample_width\': 2, # \'frame_rate\': 44100, # \'number_of_frames\': 100000, # \'compression_type\': \'NONE\', # \'compression_name\': \'not compressed\' # } # Example usage of write_au_file params = { \'number_of_channels\': 2, \'sample_width\': 2, \'frame_rate\': 44100, \'number_of_frames\': 100000, \'compression_type\': \'NONE\', \'compression_name\': \'not compressed\' } audio_data = b\'x00\' * 200000 # Example audio data (silence) write_au_file(\'output.au\', params, audio_data) ``` Make sure to handle exceptions using `sunau.Error` and to document your code appropriately. # Constraints: - Ensure that your functions handle invalid inputs gracefully, raising appropriate exceptions when necessary. - Maintain code readability and proper documentation. # Performance Requirements: - Your implementation should not have significant performance bottlenecks and should be efficient in terms of both memory and time complexity.","solution":"import sunau def read_au_file(file_path): Reads an AU file and extracts metadata. :param file_path: Path to the AU file. :return: A dictionary containing the file\'s metadata. try: with sunau.open(file_path, \'rb\') as au_file: metadata = { \'number_of_channels\': au_file.getnchannels(), \'sample_width\': au_file.getsampwidth(), \'frame_rate\': au_file.getframerate(), \'number_of_frames\': au_file.getnframes(), \'compression_type\': au_file.getcomptype(), \'compression_name\': au_file.getcompname(), } return metadata except sunau.Error as e: raise Exception(f\\"Failed to read AU file: {e}\\") def write_au_file(file_path, params, audio_data): Writes audio data to an AU file with the given parameters. :param file_path: Path to the output AU file. :param params: Dictionary containing metadata parameters. :param audio_data: Bytes object representing the audio data. try: with sunau.open(file_path, \'wb\') as au_file: au_file.setnchannels(params[\'number_of_channels\']) au_file.setsampwidth(params[\'sample_width\']) au_file.setframerate(params[\'frame_rate\']) au_file.setnframes(params[\'number_of_frames\']) au_file.setcomptype(params[\'compression_type\'], params[\'compression_name\']) au_file.writeframes(audio_data) except sunau.Error as e: raise Exception(f\\"Failed to write AU file: {e}\\")"},{"question":"# Question: HTML Entity Converter Using the `html.entities` module, you are required to implement a function called `convert_html_entities` that transforms a given HTML string, replacing all named entities with their corresponding Unicode characters. Function Signature ```python def convert_html_entities(html_string: str) -> str: ``` Input - `html_string` (str): A string that may contain HTML named character references. Output - (str): A string with all named HTML entities replaced by their equivalent Unicode characters. Constraints - The function should support all named character references as defined in the `html5` dictionary. - The conversion must account for cases where entities are specified with or without a trailing semicolon. Example ```python html_string = \\"The price is &gt; 10 &amp; &lt; 20\\" output = convert_html_entities(html_string) print(output) # The price is > 10 & < 20 ``` # Additional Notes - You can make use of the `html.entities.html5` dictionary for the conversion process. - Ensure your solution handles edge cases like entities without semicolons, as supported in HTML5. --- **Hint**: It might be helpful to use regular expressions to identify possible entities in the input string and then replace them using the dictionary.","solution":"import html import re from html.entities import html5 def convert_html_entities(html_string: str) -> str: Converts all named HTML entities in the input string to their unicode equivalents. Args: html_string (str): The input string containing HTML entities. Returns: str: A string with all named HTML entities replaced by their Unicode characters. # Function to replace a match with its unicode equivalent def replace_entity(match): entity = match.group(0) # If entity ends without semicolon, append it for lookup in html5 if entity.endswith(\\";\\"): entity_key = entity else: entity_key = entity + \\";\\" return html5.get(entity_key[1:], entity) # Regular expression to find all potential named entities pattern = re.compile(r\'&[A-Za-z]+;?\') # Substitute all matches using the replace_entity function return pattern.sub(replace_entity, html_string)"},{"question":"Objective: Implement a custom classifier using scikit-learn\'s `TunedThresholdClassifierCV` to optimize the decision threshold for detecting patients with cancer based on their medical features. You will tune the threshold to maximize the recall score, which is critical in ensuring that all potential cancer cases are detected. Problem Statement: You are given a dataset containing medical features and a binary target variable indicating the presence of cancer (1 for cancer, 0 for no cancer). Your task is to: 1. Train a logistic regression classifier on the data. 2. Use `TunedThresholdClassifierCV` to tune the decision threshold to maximize the recall score. 3. Evaluate the performance of your tuned classifier on a test set. Input: - A training dataset with features `X_train` (a numpy array of shape (n_samples_train, n_features)) and target `y_train` (a numpy array of shape (n_samples_train,)). - A test dataset with features `X_test` (a numpy array of shape (n_samples_test,)) and target `y_test` (a numpy array of shape (n_samples_test,)). Output: - The optimal decision threshold. - The recall score on the test set using the tuned threshold. - The overall accuracy on the test set using the tuned threshold. Constraints: 1. Use `LogisticRegression` as the base classifier. 2. Optimize the decision threshold using 5-fold cross-validation. 3. Focus on maximizing the recall score. Implementation: ```python from sklearn.linear_model import LogisticRegression from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import make_scorer, recall_score, accuracy_score def tune_decision_threshold(X_train, y_train, X_test, y_test): # Define the base classifier base_model = LogisticRegression(solver=\'liblinear\', random_state=0) # Define the scorer to optimize recall scorer = make_scorer(recall_score, pos_label=1) # Use TunedThresholdClassifierCV to tune the decision threshold tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer, cv=5) # Fit the tuned model on the training data tuned_model.fit(X_train, y_train) # Get the optimal decision threshold optimal_threshold = tuned_model.best_threshold_ # Make predictions on the test set using the tuned threshold probas = tuned_model.predict_proba(X_test)[:, 1] predictions = (probas >= optimal_threshold).astype(int) # Calculate recall and accuracy on the test set recall = recall_score(y_test, predictions) accuracy = accuracy_score(y_test, predictions) return optimal_threshold, recall, accuracy # Example usage: # X_train, y_train, X_test, y_test = ... # Load your data here # optimal_threshold, test_recall, test_accuracy = tune_decision_threshold(X_train, y_train, X_test, y_test) # print(\\"Optimal Threshold:\\", optimal_threshold) # print(\\"Test Recall:\\", test_recall) # print(\\"Test Accuracy:\\", test_accuracy) ``` **Note**: Replace the placeholder data loading with actual data. Ensure your implementation handles edge cases such as empty datasets or imbalanced classes.","solution":"from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_predict from sklearn.metrics import precision_recall_curve, recall_score, accuracy_score import numpy as np def tune_decision_threshold(X_train, y_train, X_test, y_test): # Define the base classifier base_model = LogisticRegression(solver=\'liblinear\', random_state=0) # Fit the model on the training data base_model.fit(X_train, y_train) # Get cross-validated predicted probabilities y_probs = cross_val_predict(base_model, X_train, y_train, cv=5, method=\'predict_proba\')[:, 1] # Use precision-recall curve to find the best threshold to maximize recall precision, recall, thresholds = precision_recall_curve(y_train, y_probs) thresholds = np.append(thresholds, 1) # Include threshold for last precision-recall point # Select the threshold with maximum recall optimal_idx = np.argmax(recall) optimal_threshold = thresholds[optimal_idx] # Predict on the test set using the optimal decision threshold test_probs = base_model.predict_proba(X_test)[:, 1] test_predictions = (test_probs >= optimal_threshold).astype(int) # Calculate recall and accuracy on the test set test_recall = recall_score(y_test, test_predictions) test_accuracy = accuracy_score(y_test, test_predictions) return optimal_threshold, test_recall, test_accuracy"},{"question":"You are tasked with implementing some utility functions to manage NIS mappings for Unix systems using the `nis` module. Part 1: Retrieve a Domain\'s NIS Maps Implement a function `retrieve_nis_maps(domain=None)` that returns a sorted list of valid NIS maps for a given domain. If no domain is specified, the function should use the system\'s default NIS domain. **Function Signature:** ```python def retrieve_nis_maps(domain: str = None) -> list: pass ``` **Input:** - `domain` (str, optional): The NIS domain to look up maps. Defaults to `None`. **Output:** - A sorted list of valid NIS maps (list of strings). Part 2: Get Key-Value Pairs from NIS Map Implement a function `retrieve_map_contents(mapname, domain=None)` that returns a dictionary with key-value pairs for a specific map in a given domain. If no domain is specified, the function should use the system\'s default NIS domain. **Function Signature:** ```python def retrieve_map_contents(mapname: str, domain: str = None) -> dict: pass ``` **Input:** - `mapname` (str): Name of the NIS map to retrieve contents from. - `domain` (str, optional): The NIS domain to look up the map. Defaults to `None`. **Output:** - A dictionary where keys and values are arbitrary arrays of bytes. Part 3: Retrieve Value by Key Implement a function `retrieve_value_by_key(mapname, key, domain=None)` that retrieves the value associated with a key in a specified map and domain, returning it as a string. If no domain is specified, the function should use the system\'s default NIS domain. **Function Signature:** ```python def retrieve_value_by_key(mapname: str, key: str, domain: str = None) -> str: pass ``` **Input:** - `mapname` (str): Name of the NIS map. - `key` (str): The key to look up in the NIS map. - `domain` (str, optional): The NIS domain to use for the lookup. Defaults to `None`. **Output:** - The value (str) associated with the given key. # Constraints 1. You must handle the `nis.error` exception gracefully and return an appropriate error message indicating the failure. 2. Assume all strings are 8-bit clean and may contain NULL characters. 3. Ensure your code only runs on Unix systems due to the dependency on the `nis` module. # Example Usage ```python if __name__ == \\"__main__\\": maps = retrieve_nis_maps() print(maps) contents = retrieve_map_contents(\\"hosts.byname\\") print(contents) value = retrieve_value_by_key(\\"hosts.byname\\", \\"localhost\\") print(value) ``` Implement these functions to demonstrate your understanding of the `nis` module and how to interact with NIS maps.","solution":"import nis def retrieve_nis_maps(domain: str = None) -> list: Returns a sorted list of valid NIS maps for a given domain. If no domain is specified, uses the system\'s default NIS domain. try: maps = nis.maps(domain) return sorted(maps) except nis.error as e: return f\\"Failed to retrieve maps: {e}\\" def retrieve_map_contents(mapname: str, domain: str = None) -> dict: Returns a dictionary with key-value pairs for a specific map in a given domain. If no domain is specified, uses the system\'s default NIS domain. try: nis_map = nis.cat(mapname, domain) return {k: v for k, v in nis_map.items()} except nis.error as e: return f\\"Failed to retrieve map contents: {e}\\" def retrieve_value_by_key(mapname: str, key: str, domain: str = None) -> str: Retrieves the value associated with a key in a specified map and domain, returning it as a string. If no domain is specified, uses the system\'s default NIS domain. try: value = nis.match(key, mapname, domain) return value.decode(\'utf-8\') except nis.error as e: return f\\"Failed to retrieve value for key \'{key}\': {e}\\""},{"question":"# Scenario: You have been tasked with setting up a secure server-client communication channel using Python\'s `ssl` module. You are required to configure both a client and a server that securely communicate using TLS. # Task: 1. **Server Implementation:** - Create a function `create_ssl_server(hostname: str, port: int, certfile: str, keyfile: str)`: - This function should set up an SSL/TLS server that: - Uses the provided `hostname` and `port` to bind the server. - Loads the server certificate and private key from `certfile` and `keyfile`. - Listens for incoming connections. - When a client connects, perform a TLS handshake and send a welcome message, \\"Welcome to the secure server!\\". - Handle any SSL-related exceptions gracefully by printing appropriate error messages. 2. **Client Implementation:** - Create a function `create_ssl_client(hostname: str, port: int, cafile: str)`: - This function should set up an SSL/TLS client that: - Connects to the server using the provided `hostname` and `port`. - Verifies the server\'s certificate using the CA certificates file at `cafile`. - Prints the server\'s welcome message upon successful connection. - Handle any SSL-related exceptions gracefully by printing appropriate error messages. 3. **Additional Requirements:** - Ensure that the server and client use strong encryption protocols. - The server should enforce client certificate verification. # Expected Input and Output: - The `create_ssl_server` function does not take any input from the user. It binds and listens for connections, and then sends a welcome message upon a valid connection. - The `create_ssl_client` function connects to the server, reads the welcome message, and prints it. # Constraints: - Use `ssl.PROTOCOL_TLS_SERVER` for the server and `ssl.PROTOCOL_TLS_CLIENT` for the client for context creation. - The certificate files should be in PEM format. - Ensure proper error handling to deal with SSL-specific exceptions. # Example Usage: ```python # Assume the presence of appropriate certificate and key files. create_ssl_server(\'localhost\', 8443, \'server_cert.pem\', \'server_key.pem\') # In another script or interactive session. create_ssl_client(\'localhost\', 8443, \'ca_cert.pem\') ``` In the example usage, correctly configured scripts will result in the client printing \\"Welcome to the secure server!\\" upon successful execution. # Additional Details: - You can mock client requests and server responses if necessary for testing. - Consider threading for handling multiple client connections. - Read the `ssl` module documentation for detailed functionalities and error handling. # Performance Requirements: - Ensure minimal latency in connection setup and message exchange. - Make the server and client robust against basic network disruptions and handle reconnections if needed.","solution":"import ssl import socket def create_ssl_server(hostname: str, port: int, certfile: str, keyfile: str): try: context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER) context.load_cert_chain(certfile, keyfile) with socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0) as sock: sock.bind((hostname, port)) sock.listen(5) with context.wrap_socket(sock, server_side=True) as ssock: print(f\'Server listening on {hostname}:{port}\') while True: client_socket, addr = ssock.accept() print(\'Connection from:\', addr) client_socket.sendall(b\\"Welcome to the secure server!\\") client_socket.close() except ssl.SSLError as e: print(f\\"SSL error: {e}\\") except socket.error as e: print(f\\"Socket error: {e}\\") except Exception as e: print(f\\"Other error: {e}\\") def create_ssl_client(hostname: str, port: int, cafile: str): try: context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH) context.load_verify_locations(cafile) with socket.create_connection((hostname, port)) as sock: with context.wrap_socket(sock, server_hostname=hostname) as ssock: print(\\"Connected to server\\") welcome_message = ssock.recv(1024).decode(\\"utf-8\\") print(\\"Received:\\", welcome_message) except ssl.SSLError as e: print(f\\"SSL error: {e}\\") except socket.error as e: print(f\\"Socket error: {e}\\") except Exception as e: print(f\\"Other error: {e}\\")"},{"question":"You are required to implement a function that compresses and then decompresses a given string using the LZMA algorithm provided by the `lzma` module. Additionally, you need to manage files for both operations and ensure data integrity using a suitable check method. Function Signature ```python def compress_and_decompress(input_string: str, compression_format: str, integrity_check: str, filename: str) -> str: pass ``` Input - `input_string` (str): The string of text to compress. - `compression_format` (str): The compression format to use. It can be one of \\"xz\\", \\"alone\\", or \\"raw\\". - `integrity_check` (str): The integrity check method to use. It can be one of \\"none\\", \\"crc32\\", \\"crc64\\", or \\"sha256\\". - `filename` (str): The name of the file to which the compressed data should be written, and from which the data should be read back for decompression. Output - The function should return a single string which is the decompressed text originally passed in as `input_string`. Constraints - The integrity check provided must be valid for the chosen format. If the combination is invalid or unsupported, raise a `ValueError`. - Ensure proper file handling to avoid file corruption. Example ```python input_string = \\"This is a test string to compress and decompress using the LZMA algorithm.\\" compression_format = \\"xz\\" integrity_check = \\"crc64\\" filename = \\"test_file.xz\\" # Expected output: \\"This is a test string to compress and decompress using the LZMA algorithm.\\" result = compress_and_decompress(input_string, compression_format, integrity_check, filename) print(result) ``` Notes 1. You may assume that the input string does not exceed 1MB. 2. Implement appropriate error handling for file operations and compression/decompression phases. 3. Use the appropriate constants from the `lzma` module for format and check mappings. Hints - Use `lzma.open` to handle file reading and writing in compressed format. - Map string inputs for `compression_format` and `integrity_check` to relevant constants in the `lzma` module. - Ensure proper cleanup of file resources using the `with` statement for file operations.","solution":"import lzma def compress_and_decompress(input_string: str, compression_format: str, integrity_check: str, filename: str) -> str: # Map compression formats to lzma constants format_map = { \\"xz\\": lzma.FORMAT_XZ, \\"alone\\": lzma.FORMAT_ALONE, \\"raw\\": lzma.FORMAT_RAW } # Map integrity checks to lzma constants check_map = { \\"none\\": lzma.CHECK_NONE, \\"crc32\\": lzma.CHECK_CRC32, \\"crc64\\": lzma.CHECK_CRC64, \\"sha256\\": lzma.CHECK_SHA256 } # Validate the provided format and check if compression_format not in format_map: raise ValueError(\\"Unsupported compression format.\\") if integrity_check not in check_map: raise ValueError(\\"Unsupported integrity check method.\\") # Get the corresponding lzma constants lzma_format = format_map[compression_format] lzma_check = check_map[integrity_check] # Compress the data with lzma.open(filename, \'wb\', format=lzma_format, check=lzma_check) as f: f.write(input_string.encode(\'utf-8\')) # Decompress the data with lzma.open(filename, \'rb\') as f: decompressed_data = f.read().decode(\'utf-8\') return decompressed_data"},{"question":"**Coding Assessment Question: Automate Web Browsing Tasks** **Objective:** Design a Python function that uses the `webbrowser` module to automate the following web browsing tasks: 1. Open a list of URLs in a specific browser type. 2. Open a URL in a new window or tab, based on user preference. 3. Register a custom browser command and use it to open a URL. **Function Signature:** ```python def automate_browsing(urls, browser_type, open_mode=\\"tab\\", custom_browser_cmd=None): Automates web browsing tasks using the webbrowser module. Parameters: - urls (list of str): List of URLs to open. - browser_type (str): Browser type to use (e.g., \'firefox\', \'chrome\'). If set to None, use the default browser. - open_mode (str): Mode to open URLs in, either \\"window\\" for new window or \\"tab\\" for new tab. Defaults to \\"tab\\". - custom_browser_cmd (str, optional): Command line of a custom browser to be registered. Defaults to None. Returns: - dict: A dictionary with URLs as keys and boolean values indicating if they were successfully opened. pass ``` **Requirements:** 1. The function should open all the URLs in the specified browser type. 2. If `open_mode` is \\"window\\", each URL should be opened in a new window. If \\"tab\\", each URL should be opened in a new tab. 3. If `custom_browser_cmd` is provided, the function should register a custom browser with this command and use it to open the URLs. 4. The function should handle errors gracefully and return a dictionary where the keys are the URLs and the values are `True` if the URL was successfully opened, `False` otherwise. 5. The function should print a user-friendly message for each URL indicating whether it was successfully opened or not. **Constraints:** - The `urls` list will contain at least one and at most 100 URLs. - The `browser_type` must be one of the predefined browser types listed in the documentation. If an invalid type is provided, fall back to the default browser. - The `open_mode` must be either \\"window\\" or \\"tab\\". **Examples:** ```python urls = [\\"https://www.python.org\\", \\"https://www.github.com\\"] browser_type = \\"firefox\\" open_mode = \\"tab\\" custom_browser_cmd = None result = automate_browsing(urls, browser_type, open_mode, custom_browser_cmd) print(result) # Example Output: {\'https://www.python.org\': True, \'https://www.github.com\': True} ``` **Additional Information:** - Refer to the `webbrowser` module documentation for details on using various functions and registering custom browsers.","solution":"import webbrowser def automate_browsing(urls, browser_type=None, open_mode=\\"tab\\", custom_browser_cmd=None): Automates web browsing tasks using the webbrowser module. Parameters: - urls (list of str): List of URLs to open. - browser_type (str): Browser type to use (e.g., \'firefox\', \'chrome\'). If set to None, use the default browser. - open_mode (str): Mode to open URLs in, either \\"window\\" for new window or \\"tab\\" for new tab. Defaults to \\"tab\\". - custom_browser_cmd (str, optional): Command line of a custom browser to be registered. Defaults to None. Returns: - dict: A dictionary with URLs as keys and boolean values indicating if they were successfully opened. results = {} # If a custom browser command is provided, register it if custom_browser_cmd: webbrowser.register(\'custom\', None, webbrowser.BackgroundBrowser(custom_browser_cmd)) browser_type = \'custom\' # Get the browser instance if browser_type: try: browser = webbrowser.get(browser_type) except webbrowser.Error: print(f\\"Browser type \'{browser_type}\' is not recognized. Using default browser.\\") browser = webbrowser.get() else: browser = webbrowser.get() # Open each URL in the specified mode for url in urls: try: if open_mode == \\"window\\": opened = browser.open_new(url) else: opened = browser.open_new_tab(url) results[url] = opened if opened: print(f\\"Successfully opened: {url}\\") else: print(f\\"Failed to open: {url}\\") except Exception as e: print(f\\"Error opening {url}: {e}\\") results[url] = False return results"},{"question":"You are provided with the `penguins` dataset, which contains measurements for penguins from different islands. Your task is to utilize seaborn’s new object-oriented interface to create several visualizations demonstrating various features of seaborn. Implement a function `create_penguin_plots()` that generates and displays the following plots in a single cell: 1. A histogram of the `flipper_length_mm` across all penguins. 2. A bar plot showing the count of penguins on each island. 3. Two histograms on the same axes of `flipper_length_mm`, one for male penguins and one for female penguins, with different colors. 4. A stacked bar plot for the count of penguins on each island, broken down by species. Input - None Output - Display the created plots using `matplotlib.pyplot.show()` Constraints - Use the seaborn `objects` interface for creating the plots. - Ensure that the plots are clearly labeled and color differences are distinguishable. Implementation Details - Use the `seaborn.objects.Plot` class to create the plots. - Implement bin adjustments for histograms where it could make the visualization more meaningful. - Ensure proper use of the different plot marks and statistical transformations as shown in the documentation. ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_penguin_plots(): penguins = load_dataset(\\"penguins\\") # Plot 1: Histogram of flipper length for all penguins p1 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist()).label(\'x\', \'Flipper Length (mm)\').label(\'y\', \'Count\').title(\'Histogram of Flipper Length\') # Plot 2: Bar plot of penguin count on each island p2 = so.Plot(penguins, \\"island\\").add(so.Bars(), so.Hist()).label(\'x\', \'Island\').label(\'y\', \'Count\').title(\'Count of Penguins on Each Island\') # Plot 3: Histograms of flipper length by sex p3 = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"sex\\").add(so.Layer(color=\\"sex\\"), so.Hist()) # Plot 4: Stacked bar plot for the count of penguins on each island by species p4 = so.Plot(penguins, \\"island\\", color=\'species\').add(so.Bars(), so.Hist(), so.Stack()).label(\'x\', \'Island\').label(\'y\', \'Count\').title(\'Count of Penguins by Species on Each Island\') # Render plots p1.plot(plt.gca()) plt.show() plt.figure() p2.plot(plt.gca()) plt.show() plt.figure() p3.plot(plt.gca()) plt.show() plt.figure() p4.plot(plt.gca()) plt.show() plt.figure() # Run the function to display plots create_penguin_plots() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): penguins = sns.load_dataset(\\"penguins\\") # Setup subplots fig, axes = plt.subplots(2, 2, figsize=(15, 10)) # Plot 1: Histogram of flipper length for all penguins p1 = sns.histplot(data=penguins, x=\\"flipper_length_mm\\", kde=False, ax=axes[0, 0]) p1.set_title(\'Histogram of Flipper Length\') p1.set_xlabel(\'Flipper Length (mm)\') p1.set_ylabel(\'Count\') # Plot 2: Bar plot of penguin count on each island p2 = sns.countplot(data=penguins, x=\\"island\\", ax=axes[0, 1]) p2.set_title(\'Count of Penguins on Each Island\') p2.set_xlabel(\'Island\') p2.set_ylabel(\'Count\') # Plot 3: Histograms of flipper length by sex p3 = sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"sex\\", multiple=\\"dodge\\", ax=axes[1, 0]) p3.set_title(\'Histogram of Flipper Length by Sex\') p3.set_xlabel(\'Flipper Length (mm)\') p3.set_ylabel(\'Count\') # Plot 4: Stacked bar plot for the count of penguins on each island by species p4 = sns.histplot(data=penguins, x=\\"island\\", hue=\\"species\\", multiple=\\"stack\\", ax=axes[1, 1]) p4.set_title(\'Count of Penguins by Species on Each Island\') p4.set_xlabel(\'Island\') p4.set_ylabel(\'Count\') plt.tight_layout() plt.show() # Run the function to display plots create_penguin_plots()"},{"question":"Implementing and Managing Persistent Key-Value Store with `dbm` Objective: Demonstrate your understanding of the `dbm` module by implementing a persistent key-value store using the provided functions and methods. Problem Statement: Using the `dbm` module, create a class `PersistentStore` with methods to perform basic CRUD (Create, Read, Update, Delete) operations on a persistent database. Your implementation should handle multiple backends (`dbm.gnu`, `dbm.ndbm`, `dbm.dumb`) based on what\'s available on the system, and should properly manage exceptions that may arise during database operations. Requirements: 1. Implement the `PersistentStore` class with the following methods: - `__init__(self, filename: str, flag: str = \'c\', mode: int = 0o666)`: Initialize the store by opening the specified database file with given flags and mode. - `set(self, key: str, value: str)`: Store a key-value pair in the database. - `get(self, key: str) -> str`: Retrieve the value associated with the given key. Raise a `KeyError` if the key does not exist. - `delete(self, key: str)`: Delete the key-value pair from the database. Raise `KeyError` if the key does not exist. - `list_keys(self) -> list`: Return a list of all keys in the database. - `close(self)`: Close the database. 2. Ensure error handling is in place for operations, especially regarding read-only databases and missing keys. 3. Implement context management support to ensure that the database file is properly closed after operations, even if errors occur. 4. The data should be stored in bytes format, as required by the `dbm` module. Ensure string keys and values are properly encoded and decoded. Input and Output Formats: - The `filename` parameter for the constructor (`__init__`) should be a string pointing to the database file. - The `flag` parameter can be `\'r\'`, `\'w\'`, `\'c\'`, or `\'n\'`. - The `mode` parameter defines file permissions, defaulting to 0o666. - Other methods take and return strings as keys and values, except for `list_keys`, which returns a list. - Raise appropriate exceptions where invalid operations are performed. Example Usage: ```python store = PersistentStore(\'example.db\') store.set(\'greeting\', \'hello world\') print(store.get(\'greeting\')) # Output: \'hello world\' print(store.list_keys()) # Output: [\'greeting\'] store.delete(\'greeting\') store.close() # Using context manager with PersistentStore(\'example.db\') as store: store.set(\'farewell\', \'goodbye universe\') print(store.get(\'farewell\')) # Output: \'goodbye universe\' print(store.list_keys()) # Output: [\'farewell\'] # Attempting to fetch a non-existing key try: print(store.get(\'non_existing_key\')) except KeyError: print(\'Key not found\') ``` Code Skeleton: ```python import dbm class PersistentStore: def __init__(self, filename: str, flag: str = \'c\', mode: int = 0o666): self.db = dbm.open(filename, flag, mode) def set(self, key: str, value: str): self.db[key.encode()] = value.encode() def get(self, key: str) -> str: value = self.db.get(key.encode()) if value is None: raise KeyError(f\\"Key \'{key}\' not found\\") return value.decode() def delete(self, key: str): try: del self.db[key.encode()] except KeyError: raise KeyError(f\\"Key \'{key}\' not found\\") def list_keys(self) -> list: return [key.decode() for key in self.db.keys()] def close(self): self.db.close() def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): self.close() # You can run your tests here ``` Feel free to test and modify the `PersistentStore` class to ensure it works correctly with different `dbm` backends and handles all specified requirements.","solution":"import dbm import contextlib class PersistentStore: def __init__(self, filename: str, flag: str = \'c\', mode: int = 0o666): self.db = dbm.open(filename, flag, mode) def set(self, key: str, value: str): self.db[key.encode(\'utf-8\')] = value.encode(\'utf-8\') def get(self, key: str) -> str: if key.encode(\'utf-8\') not in self.db: raise KeyError(f\\"Key \'{key}\' not found\\") return self.db[key.encode(\'utf-8\')].decode(\'utf-8\') def delete(self, key: str): if key.encode(\'utf-8\') not in self.db: raise KeyError(f\\"Key \'{key}\' not found\\") del self.db[key.encode(\'utf-8\')] def list_keys(self) -> list: return [key.decode(\'utf-8\') for key in self.db.keys()] def close(self): self.db.close() def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): self.close()"},{"question":"**Objective**: Demonstrate your understanding of loading and manipulating datasets using the `sklearn.datasets` package. # Problem Statement Write a function named `load_and_inspect_dataset` that loads a specified dataset using `sklearn.datasets` and returns a summary of its key properties. Input: - `dataset_name` (str): The name of the dataset to load. Possible values are: - `\\"iris\\"` - `\\"digits\\"` - `\\"wine\\"` - `\\"breast_cancer\\"` - `return_X_y` (bool): If `True`, the function should return a tuple `(data, target)`. If `False`, the function should return a dictionary with keys `data`, `target`, and other dataset-specific attributes. Output: - If `return_X_y` is `True`, return a tuple `(data, target)`: - `data` is a numpy array of shape `(n_samples, n_features)` containing the feature data. - `target` is a numpy array of shape `(n_samples,)` containing the target values. - If `return_X_y` is `False`, return a dictionary: - `data` (numpy array): The feature data of shape `(n_samples, n_features)`. - `target` (numpy array): The target values of length `n_samples`. - `DESCR` (str): The full description of the dataset. - `feature_names` (list of str, optional): The feature names, if available. - `target_names` (list of str, optional): The target names, if available. Constraints: - You must use the appropriate dataset loading function (`load_iris`, `load_digits`, `load_wine`, `load_breast_cancer`) from `sklearn.datasets`. Example Usage: ```python # Example 1: result = load_and_inspect_dataset(\\"iris\\", return_X_y=True) # Expected output: # A tuple (data, target), where data is a numpy array of shape (150, 4) # and target is a numpy array of shape (150,). # Example 2: result = load_and_inspect_dataset(\\"wine\\", return_X_y=False) # Expected output: # A dictionary with keys \'data\', \'target\', \'DESCR\', and optionally \'feature_names\' and \'target_names\'. ``` Function Signature: ```python def load_and_inspect_dataset(dataset_name: str, return_X_y: bool) -> Union[Tuple[np.ndarray, np.ndarray], Dict[str, Union[np.ndarray, str, List[str]]]]: pass ``` # Notes: - Ensure that the function handles improper `dataset_name` values by raising a `ValueError` with a descriptive error message. - Use the `Bunch` object attributes to access the data and target values. - Include any necessary import statements at the top of your code.","solution":"from typing import Union, Tuple, Dict, List import numpy as np from sklearn.datasets import load_iris, load_digits, load_wine, load_breast_cancer def load_and_inspect_dataset(dataset_name: str, return_X_y: bool) -> Union[Tuple[np.ndarray, np.ndarray], Dict[str, Union[np.ndarray, str, List[str]]]]: if dataset_name == \\"iris\\": dataset = load_iris(as_frame=not return_X_y) elif dataset_name == \\"digits\\": dataset = load_digits(as_frame=not return_X_y) elif dataset_name == \\"wine\\": dataset = load_wine(as_frame=not return_X_y) elif dataset_name == \\"breast_cancer\\": dataset = load_breast_cancer(as_frame=not return_X_y) else: raise ValueError(f\\"Unrecognized dataset name: {dataset_name}\\") if return_X_y: return dataset.data, dataset.target else: result = { \'data\': dataset.data, \'target\': dataset.target, \'DESCR\': dataset.DESCR, } if hasattr(dataset, \'feature_names\'): result[\'feature_names\'] = dataset.feature_names if hasattr(dataset, \'target_names\'): result[\'target_names\'] = dataset.target_names return result"},{"question":"Question: You are provided with a dataset containing information about patients in a clinical trial. The dataset includes the following columns: - `patient_id`: Unique identifier for each patient. - `age`: Age of the patient. - `gender`: Gender of the patient (Male/Female). - `treatment_group`: Group assigned to the patient (Control/Treatment). - `visit_time`: Time of the visit in weeks. - `response_score`: Response score of the patient to the treatment. Your task is to generate visualizations using seaborn to analyze the dataset and demonstrate various statistical relationships. Specifically, you need to: 1. Generate a scatter plot showing the relationship between `age` and `response_score`. Color the points based on their `gender`. 2. Create a line plot showing the trend of `response_score` over `visit_time` for each `treatment_group`. Use different line styles for the `treatment_group` and show the 95% confidence interval around the mean. 3. Facet the line plot by `gender` so that there are separate plots for male and female patients. Ensure that the subplots are in a single row. **Input**: A pandas DataFrame called `clinical_data` with the described structure. **Output**: Three subplots in a single figure as specified in the tasks. # Constraints: - Ensure that your code is efficient and follows good coding practices. - Use seaborn\'s build-in datasets and functions; avoid custom matplotlib styles if possible. - The dataset `clinical_data` is guaranteed to have no missing values. **Example**: ```python import seaborn as sns import matplotlib.pyplot as plt # Assuming that clinical_data is a pre-loaded DataFrame clinical_data = sns.load_dataset(\'your_dataset\') # Generate scatter plot of age vs response_score with gender as hue sns.relplot(data=clinical_data, x=\\"age\\", y=\\"response_score\\", hue=\\"gender\\") # Generate line plot of response_score over visit_time based on treatment_group with 95% confidence interval sns.relplot(data=clinical_data, x=\\"visit_time\\", y=\\"response_score\\", kind=\\"line\\", hue=\\"treatment_group\\", style=\\"treatment_group\\") # Facet the line plot by gender with a single row sns.relplot(data=clinical_data, x=\\"visit_time\\", y=\\"response_score\\", kind=\\"line\\", hue=\\"treatment_group\\", style=\\"treatment_group\\", row=\\"gender\\", aspect=2) plt.show() ``` The above template is provided for guidance. You can customize the problem further if needed.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_clinical_data(clinical_data): Generates three subplots to analyze the clinical_data DataFrame. Parameters: clinical_data (pd.DataFrame): DataFrame containing clinical trial data. plt.figure(figsize=(20, 8)) # Scatter plot showing the relationship between age and response_score with gender as hue plt.subplot(1, 3, 1) sns.scatterplot(x=\'age\', y=\'response_score\', hue=\'gender\', data=clinical_data) plt.title(\'Age vs Response Score\') # Line plot showing the trend of response_score over visit_time for each treatment_group # with 95% confidence interval around the mean plt.subplot(1, 3, 2) sns.lineplot(x=\'visit_time\', y=\'response_score\', hue=\'treatment_group\', style=\'treatment_group\', data=clinical_data, ci=95) plt.title(\'Response Score over Visit Time by Treatment Group\') # Facet the line plot by gender with a single row plt.subplot(1, 3, 3) sns.lineplot(x=\'visit_time\', y=\'response_score\', hue=\'treatment_group\', style=\'treatment_group\', data=clinical_data, ci=95) plt.title(\'Response Score over Visit Time Faceted by Gender\') sns.lineplot(x=\'visit_time\', y=\'response_score\', hue=\'treatment_group\', style=\'treatment_group\', data=clinical_data[clinical_data[\'gender\'] == \'Male\'], ci=95) sns.lineplot(x=\'visit_time\', y=\'response_score\', hue=\'treatment_group\', style=\'treatment_group\', data=clinical_data[clinical_data[\'gender\'] == \'Female\'], ci=95) plt.tight_layout() plt.show()"},{"question":"<|Analysis Begin|> The documentation provided is for a prototype feature in PyTorch called `torch.cuda.tunable`, which includes various functions related to configuring and tuning CUDA operations. Functions available deal with enabling/disabling the feature, recording untuned configs, setting/getting max tuning duration and iterations, file operations for reading and writing configurations, and tuning GEMM (General Matrix Multiply) operations. Based on the available functions, one could design a question requiring students to interact with these tuning settings and dynamically adjust configurations based on some conditions or results. <|Analysis End|> <|Question Begin|> # PyTorch CUDA Tuning Assessment **Objective:** In this task, you are required to write a function to dynamically interact with the PyTorch CUDA tunable configurations and adjust settings based on a hypothetical scenario. **Problem Statement:** Your task is to implement a function `optimize_cuda_tuning(config_file, max_duration, max_iterations, record_untuned)` that: 1. Reads tuning configurations from a given file. 2. Sets the maximum tuning duration and iterations as provided by the input arguments. 3. Enables or disables recording of untuned configurations based on the `record_untuned` boolean argument. 4. Ensures that the tuning settings are written back to the file. 5. Returns a dictionary with the current tuning settings and results. **Function Signature:** ```python def optimize_cuda_tuning(config_file: str, max_duration: int, max_iterations: int, record_untuned: bool) -> dict: pass ``` **Input:** - `config_file` (str): The filename of the configuration file to read and write. - `max_duration` (int): The maximum duration for tuning in seconds. - `max_iterations` (int): The maximum number of iterations allowed for tuning. - `record_untuned` (bool): If True, untuned configurations will be recorded; otherwise, they will not be. **Output:** - Returns a dictionary with the following keys and their corresponding values: - `max_duration` - `max_iterations` - `record_untuned_enabled`: boolean indicating if recording of untuned configurations is enabled. - `tuning_results`: results obtained from the tuning process. **Constraints:** - Assume the provided `config_file` exists and is accessible for reading and writing. - You can make use of any functions available under `torch.cuda.tunable`. **Example:** ```python result = optimize_cuda_tuning(\'cuda_config.txt\', 300, 1000, True) print(result) # Output: {\'max_duration\': 300, \'max_iterations\': 1000, \'record_untuned_enabled\': True, \'tuning_results\': {...}} ``` **Notes:** - Make sure to handle any exceptions that might arise from file operations or PyTorch functions. - The `tuning_results` key should contain any data returned by the `get_results` function or equivalent based on the tuned configurations. This task assesses your understanding of reading and writing configurations, using PyTorch\'s CUDA tunable API functions, and handling potential exceptions or errors.","solution":"import torch def optimize_cuda_tuning(config_file: str, max_duration: int, max_iterations: int, record_untuned: bool) -> dict: Optimizes CUDA tuning based on the input configuration file and parameters. Parameters: config_file (str): The filename of the configuration file to read and write. max_duration (int): The maximum duration for tuning in seconds. max_iterations (int): The maximum number of iterations allowed for tuning. record_untuned (bool): Whether to record untuned configurations. Returns: dict: A dictionary with the tuning settings and results. # Read the existing tuning configurations from the file try: torch.cuda.tunable.read_config_file(config_file) except Exception as e: raise RuntimeError(f\\"Error reading config file: {e}\\") # Set the maximum durations and iterations torch.cuda.tunable.set_max_tuning_duration(max_duration) torch.cuda.tunable.set_max_tuning_iterations(max_iterations) # Enable or disable recording of untuned configurations if record_untuned: torch.cuda.tunable.enable_record_untuned() else: torch.cuda.tunable.disable_record_untuned() # Write the updated configurations back to the file try: torch.cuda.tunable.write_config_file(config_file) except Exception as e: raise RuntimeError(f\\"Error writing to config file: {e}\\") # Get the tuning results try: tuning_results = torch.cuda.tunable.get_results() except Exception as e: tuning_results = None # Return the final dictionary with current settings and results return { \'max_duration\': torch.cuda.tunable.get_max_tuning_duration(), \'max_iterations\': torch.cuda.tunable.get_max_tuning_iterations(), \'record_untuned_enabled\': torch.cuda.tunable.record_untuned_enabled(), \'tuning_results\': tuning_results }"},{"question":"Objective: Assess the ability to use the `atexit` module for managing cleanup functions in Python. Problem Statement: Create a class `ResourceManager` that manages a resource counter using the `atexit` module to ensure that resource usage data is saved before the program terminates. Additionally, allow for the option to remove resource cleanup functions if necessary. Requirements: 1. **Initialization**: - When an instance of `ResourceManager` is created, it should attempt to read the initial counter value from a file named `resource_counter.txt`. - If the file does not exist, initialize the counter to 0. 2. **Methods**: - `increment_resource(n: int)`: Increments the counter by a given integer `n`. - `save_counter()`: Writes the current counter value to `resource_counter.txt`. - `register_cleanup()`: Registers the `save_counter` method with `atexit`. - `unregister_cleanup()`: Unregisters the `save_counter` method with `atexit`. 3. **Execution Order**: - The `save_counter` method should be registered such that it executes upon normal interpreter termination. Expected Input and Output: - **Initialization**: ```python rm = ResourceManager() # Reads the counter value from \'resource_counter.txt\' or initializes to 0 ``` - **Method Invocations**: ```python rm.increment_resource(5) # Increases the counter by 5 rm.register_cleanup() # Registers the cleanup function to save the counter on exit rm.unregister_cleanup() # Unregisters the cleanup function ``` Constraints: - The `save_counter` method should be registered and unregistered using the `atexit` module. - The file operations should handle exceptions gracefully (e.g., file not found). Example Usage: ```python # Create an instance of ResourceManager resource_manager = ResourceManager() # Increment the resource counter resource_manager.increment_resource(10) # Register the cleanup function resource_manager.register_cleanup() # Perform some operations... # Optionally unregister the cleanup function resource_manager.unregister_cleanup() # Upon normal interpreter termination, if registered, # the counter should be saved to \'resource_counter.txt\'. ``` # Notes: - No explicit input will be provided for this task; the focus is on using the provided methods correctly. - Ensure that `atexit` module functionality is utilized properly to manage the registration and unregistration of functions.","solution":"import atexit import os class ResourceManager: def __init__(self): self.counter = 0 self.file_path = \'resource_counter.txt\' self.cleanup_registered = False self.__load_counter() def __load_counter(self): try: if os.path.exists(self.file_path): with open(self.file_path, \'r\') as file: self.counter = int(file.read().strip()) except Exception as e: print(f\\"Error reading {self.file_path}: {e}\\") def increment_resource(self, n: int): self.counter += n def save_counter(self): try: with open(self.file_path, \'w\') as file: file.write(str(self.counter)) except Exception as e: print(f\\"Error writing to {self.file_path}: {e}\\") def register_cleanup(self): if not self.cleanup_registered: atexit.register(self.save_counter) self.cleanup_registered = True def unregister_cleanup(self): if self.cleanup_registered: atexit.unregister(self.save_counter) self.cleanup_registered = False"},{"question":"# File System Operations with `pathlib` Problem Statement: Write a Python function that performs the following sequence of operations using the `pathlib` module: 1. **Create** a new directory structure where a sub-directory is nested inside a parent directory. 2. **Create** a new file within the nested sub-directory. 3. **Write** some content to the file. 4. **Read** the content from the file and **print** it. 5. **Delete** the created file and the nested sub-directory. The function should be named `manage_filesystem` and must follow the sequence described below: Function Signature: ```python def manage_filesystem(parent_dir: str, sub_dir: str, file_name: str, content: str) -> None: pass ``` Description: 1. **parent_dir**: The name of the parent directory to create. 2. **sub_dir**: The name of the sub-directory to create inside the parent directory. 3. **file_name**: The name of the file to create inside the sub-directory. 4. **content**: The content to write to the file. Constraints: - The directories and file must not exist at the start of the function. Ensure to check before operations and create them if they do not exist. - After reading and printing the content of the file, delete both the file and the directories created. Example: ```python # Example usage: parent_dir = \'test_parent\' sub_dir = \'test_sub\' file_name = \'example.txt\' content = \'Hello, pathlib!\' manage_filesystem(parent_dir, sub_dir, file_name, content) ``` Expected output during function execution: - The content \\"Hello, pathlib!\\" should be printed to the console. Note: - Utilize the `pathlib` module for all file and directory operations. - Ensure you handle potential edge cases such as the directory or file already existing and cleanup after operation.","solution":"from pathlib import Path def manage_filesystem(parent_dir: str, sub_dir: str, file_name: str, content: str) -> None: parent_path = Path(parent_dir) sub_path = parent_path / sub_dir file_path = sub_path / file_name # Create parent and sub directories if they don\'t exist sub_path.mkdir(parents=True, exist_ok=True) # Create the file and write content to it with file_path.open(mode=\'w\') as file: file.write(content) # Read the content from the file and print it with file_path.open(mode=\'r\') as file: print(file.read()) # Delete the file if file_path.exists(): file_path.unlink() # Remove directories if sub_path.exists(): sub_path.rmdir() if parent_path.exists(): parent_path.rmdir()"},{"question":"**Question:** Write a function `get_user_info(identifier)` that retrieves user information from the Unix password database. The identifier can either be a username (string) or a user ID (integer). The function should return a dictionary with the following keys: `\'username\'`, `\'user_id\'`, `\'group_id\'`, `\'user_comment\'`, `\'home_directory\'`, and `\'shell\'`. If the user does not exist, the function should raise a `KeyError`. # Function Signature ```python def get_user_info(identifier) -> dict: pass ``` # Input 1. `identifier` (str or int) - Either a username (string) or a user ID (integer). # Output A dictionary with the following keys: - `\'username\'` (str): The login name of the user. - `\'user_id\'` (int): The numerical user ID. - `\'group_id\'` (int): The numerical group ID. - `\'user_comment\'` (str): The user name or comment field. - `\'home_directory\'` (str): The user home directory. - `\'shell\'` (str): The user command interpreter. # Constraints - The function should handle both valid usernames and user IDs. - The function should raise a `KeyError` if the user does not exist. # Example ```python # Let\'s assume that there is a user with username \'johndoe\' and user ID 1001 in the Unix password database. assert get_user_info(\'johndoe\') == { \'username\': \'johndoe\', \'user_id\': 1001, \'group_id\': 1001, \'user_comment\': \'John Doe\', \'home_directory\': \'/home/johndoe\', \'shell\': \'/bin/bash\' } assert get_user_info(1001) == { \'username\': \'johndoe\', \'user_id\': 1001, \'group_id\': 1001, \'user_comment\': \'John Doe\', \'home_directory\': \'/home/johndoe\', \'shell\': \'/bin/bash\' } ``` **Notes:** - The function should utilize the `pwd` module to access password database entries. - Ensure the function handles errors gracefully and raises appropriate exceptions when required.","solution":"import pwd def get_user_info(identifier): Retrieves user information from the Unix password database. Parameters: identifier (str or int): Either a username (string) or a user ID (integer). Returns: dict: A dictionary with keys \'username\', \'user_id\', \'group_id\', \'user_comment\', \'home_directory\', and \'shell\'. Raises: KeyError: If the user does not exist. try: if isinstance(identifier, int): user_entry = pwd.getpwuid(identifier) else: user_entry = pwd.getpwnam(identifier) except KeyError: raise KeyError(f\\"User \'{identifier}\' does not exist\\") return { \'username\': user_entry.pw_name, \'user_id\': user_entry.pw_uid, \'group_id\': user_entry.pw_gid, \'user_comment\': user_entry.pw_gecos, \'home_directory\': user_entry.pw_dir, \'shell\': user_entry.pw_shell }"},{"question":"**Objective**: Assess understanding of DataFrame memory usage, handling missing values, and boolean operations in pandas. # Question You are given a dataset in the following dictionary format where different types of data are represented: ```python data = { \'integers\': [1, 2, 3, 4, 5], \'floats\': [1.0, 2.0, 3.0, None, 5.0], \'objects\': [\'apple\', \'banana\', \'cherry\', \'date\', None], \'booleans\': [True, False, True, None, True] } ``` 1. **Create a DataFrame**: Construct a DataFrame from the dictionary provided. 2. **Memory Usage Analysis**: - Print the memory usage of the DataFrame, both in the standard way and the \\"deep\\" scan way. - Print the memory usage for each column individually. 3. **Handling Missing Values**: - Identify and count missing values in each column. - Replace missing values in the \'floats\' column with the average of the other values in the column. - Replace missing values in the \'objects\' column with the string \\"unknown\\". - For the \'booleans\' column, replace missing values with `False`. 4. **Boolean Operations**: - Create a new column called \\"is_fruit\\" which is `True` for rows where the \'objects\' column is a fruit and `False` otherwise. (Note: You may consider \'apple\', \'banana\', and \'cherry\' as fruits for this context) - Print the DataFrame for rows where the \\"is_fruit\\" column is `True`. # Constraints - Do not use loops to fill missing values. Utilize pandas built-in methods wherever possible. - Aim to minimize the memory usage of the DataFrame while performing the operations. # Expected Output - Memory usage statistics in both standard and deep mode. - Column-wise memory usage. - DataFrame with missing values handled. - Filtered DataFrame showing only the fruits. # Function Signature You should implement the following function: ```python import pandas as pd import numpy as np def analyze_dataframe(data: dict) -> pd.DataFrame: # Step 1: Create the DataFrame df = pd.DataFrame(data) # Step 2: Memory Usage Analysis print(\\"Standard memory usage:\\") print(df.info()) print(\\"Deep memory usage:\\") print(df.info(memory_usage=\\"deep\\")) print(\\"Memory usage of each column:\\") print(df.memory_usage()) # Step 3: Handling Missing Values df[\'floats\'].fillna(df[\'floats\'].mean(), inplace=True) df[\'objects\'].fillna(\'unknown\', inplace=True) df[\'booleans\'].fillna(False, inplace=True) print(\\"Count of missing values after filling:\\") print(df.isna().sum()) # Step 4: Boolean Operations df[\'is_fruit\'] = df[\'objects\'].isin([\'apple\', \'banana\', \'cherry\']) print(\\"DataFrame with is_fruit column:\\") print(df) print(\\"Filtered DataFrame (fruits):\\") print(df[df[\'is_fruit\']]) return df ```","solution":"import pandas as pd import numpy as np def analyze_dataframe(data: dict) -> pd.DataFrame: # Step 1: Create the DataFrame df = pd.DataFrame(data) # Step 2: Memory Usage Analysis print(\\"Standard memory usage:\\") print(df.info()) print(\\"Deep memory usage:\\") print(df.info(memory_usage=\\"deep\\")) print(\\"Memory usage of each column:\\") print(df.memory_usage()) # Step 3: Handling Missing Values df[\'floats\'].fillna(df[\'floats\'].mean(), inplace=True) df[\'objects\'].fillna(\'unknown\', inplace=True) df[\'booleans\'].fillna(False, inplace=True) print(\\"Count of missing values after filling:\\") print(df.isna().sum()) # Step 4: Boolean Operations df[\'is_fruit\'] = df[\'objects\'].isin([\'apple\', \'banana\', \'cherry\']) print(\\"DataFrame with is_fruit column:\\") print(df) print(\\"Filtered DataFrame (fruits):\\") print(df[df[\'is_fruit\']]) return df"},{"question":"You are provided with a dataset comprising handwritten digits, similar to the famous MNIST dataset. This dataset has high dimensionality, and you aim to reduce its dimensionality using Principal Component Analysis (PCA). Your task is to implement and apply PCA on this dataset using the scikit-learn library. Additionally, you will compare the performance of traditional PCA and Incremental PCA. **Specifications:** 1. **Input**: - `X_train`: A 2D numpy array of shape (n_samples, n_features) representing the training set. - `X_test`: A 2D numpy array of shape (n_samples, n_features) representing the test set. - `n_components`: An integer representing the number of principal components to keep. - `batch_size`: An integer specifying the batch size for Incremental PCA. 2. **Output**: - `pca_result`: A dictionary with following keys: - `explained_variance_ratio`: A list of explained variance ratios of the selected principal components by traditional PCA. - `X_train_pca`: The transformed training set using traditional PCA. - `X_test_pca`: The transformed test set using traditional PCA. - `incremental_pca_result`: A dictionary with following keys: - `explained_variance_ratio`: A list of explained variance ratios of the selected principal components by Incremental PCA. - `X_train_incremental_pca`: The transformed training set using Incremental PCA. - `X_test_incremental_pca`: The transformed test set using Incremental PCA. **Implementation Requirements**: 1. **Function Definition**: ```python def compare_pca_methods(X_train, X_test, n_components, batch_size): Apply PCA and Incremental PCA on given dataset and return the transformed data and explained variance ratio. Parameters: - X_train (numpy.ndarray): Training dataset, shape (n_samples, n_features) - X_test (numpy.ndarray): Test dataset, shape (n_samples, n_features) - n_components (int): Number of principal components. - batch_size (int): Batch size for Incremental PCA. Returns: tuple: containing - pca_result (dict): Explained variance ratio and transformed data using traditional PCA. - incremental_pca_result (dict): Explained variance ratio and transformed data using Incremental PCA. ``` 2. **Detailed Steps**: - Fit PCA to the training set and transform both the training and test sets. - Capture the explained variance ratio and transformed datasets. - Fit Incremental PCA to the training set in batches and transform both the training and test sets. - Ensure your implementation for Incremental PCA handles large datasets by processing in batches. - Capture the explained variance ratio and transformed datasets for Incremental PCA. **Evaluation**: - You will be evaluated on the correctness of the function. - Your function should correctly reduce the dimensionality and return the expected format. - Ensure that your function can handle large datasets by processing Incremental PCA in batch mode. **Example Usage**: ```python X_train = np.random.random((1000, 784)) # Simulated data X_test = np.random.random((200, 784)) # Simulated data n_components = 50 batch_size = 200 pca_result, incremental_pca_result = compare_pca_methods(X_train, X_test, n_components, batch_size) print(\\"PCA Explained Variance Ratio:\\", pca_result[\'explained_variance_ratio\']) print(\\"Incremental PCA Explained Variance Ratio:\\", incremental_pca_result[\'explained_variance_ratio\']) ```","solution":"from sklearn.decomposition import PCA, IncrementalPCA import numpy as np def compare_pca_methods(X_train, X_test, n_components, batch_size): # Apply traditional PCA pca = PCA(n_components=n_components) X_train_pca = pca.fit_transform(X_train) X_test_pca = pca.transform(X_test) pca_result = { \'explained_variance_ratio\': pca.explained_variance_ratio_.tolist(), \'X_train_pca\': X_train_pca, \'X_test_pca\': X_test_pca } # Apply Incremental PCA ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size) X_train_incremental_pca = ipca.fit_transform(X_train) X_test_incremental_pca = ipca.transform(X_test) incremental_pca_result = { \'explained_variance_ratio\': ipca.explained_variance_ratio_.tolist(), \'X_train_incremental_pca\': X_train_incremental_pca, \'X_test_incremental_pca\': X_test_incremental_pca } return pca_result, incremental_pca_result"},{"question":"# Custom Exception Handler in Python You are tasked to create a custom exception handler that captures detailed traceback information whenever an exception occurs within a function. This handler should format the stack trace in a specific manner and write it to a log file with a given filename. The log file should contain the entire formatted traceback of any exception that occurs within the function. Function Specification - **Function Name**: `custom_exception_handler` - **Inputs**: - `func` (callable): The function to execute. - `args` (tuple): The positional arguments to pass to the function. - `kwargs` (dict): The keyword arguments to pass to the function. - `logfile` (str): The name of the log file to write the exception details. - **Behavior**: The function should execute the given `func` with provided `args` and `kwargs`. If an exception is raised during execution, the function should capture the traceback details, format it, and write it to the `logfile`. - **Return**: If the function executes without raising an exception, return its result. If an exception is raised, return `None`. Requirements 1. Use the `traceback` module to capture and format the traceback information. 2. Format the output to include: - The standard traceback header (\\"Traceback (most recent call last):\\"). - Each frame\'s filename, line number, function name, and source code line. - The exception type and message. 3. Store the formatted traceback information into the provided log file. 4. Assume that the log file is overwritable. Example Usage ```python def sample_function(a, b): return a / b try: custom_exception_handler(sample_function, (10, 0), {}, \'error_log.txt\') except Exception as e: print(\\"Unhandled exception:\\", e) # The content of \'error_log.txt\' should look similar to: # Traceback (most recent call last): # File \\"<your_script>\\", line <line_number>, in sample_function # return a / b # ZeroDivisionError: division by zero ``` Constraints - You should ensure that only exceptions that occur within the `func` are caught and logged. - Using the `traceback` module functions for extracting, formatting, and printing traceback information is mandatory. - Assume that the input `func` is always callable and `logfile` is always a valid filename string. Performance Requirements - Minimize the overhead added by capturing and logging the traceback information. - Properly handle any I/O exceptions that might occur while writing to the log file.","solution":"import traceback def custom_exception_handler(func, args, kwargs, logfile): Executes a given function with specified arguments and captures the traceback in case of an exception, writing the traceback details to the provided logfile. Parameters: func (callable): The function to execute. args (tuple): The positional arguments to pass to the function. kwargs (dict): The keyword arguments to pass to the function. logfile (str): The name of the log file to write the exception details. Returns: result of func if successful, otherwise None. try: return func(*args, **kwargs) except Exception as e: with open(logfile, \'w\') as f: traceback_details = traceback.format_exc() f.write(traceback_details) return None"},{"question":"Objective Write a Python program that performs clustering on a given dataset using multiple clustering algorithms from the scikit-learn library and evaluates the performance of each clustering method. Requirements 1. Implement clustering using the following algorithms: - K-Means - DBSCAN - Agglomerative Clustering 2. Evaluate the performance of each clustering method using the following metrics: - Adjusted Rand index - Silhouette Score Input - A dataset provided as a CSV file. The file will contain numerical feature columns without any missing values. # Detailed Steps 1. Loading the Data - Read the dataset from a CSV file into a pandas DataFrame. - The CSV file path will be provided as an argument. 2. Clustering Algorithms - Apply K-Means clustering. - Apply DBSCAN clustering. - Apply Agglomerative Clustering. 3. Evaluation Metrics - Calculate the Adjusted Rand Index for each clustering result. - Calculate the Silhouette Score for each clustering result. 4. Output - Print the Adjusted Rand Index and Silhouette Score for each clustering method. # Constraints - The K-Means clustering should use 3 clusters. - The DBSCAN clustering should use the default parameters. - The Agglomerative Clustering should use the \'ward\' linkage method and specify 3 clusters. - The dataset will not contain more than 1000 samples. Example Given a CSV file `data.csv`: ```csv Feature1,Feature2,Feature3 5.1,3.5,1.4 4.9,3.0,1.4 4.7,3.2,1.3 ... ``` Your program should output something similar to: ``` K-Means Clustering: Adjusted Rand Index: 0.734 Silhouette Score: 0.620 DBSCAN Clustering: Adjusted Rand Index: 0.412 Silhouette Score: 0.503 Agglomerative Clustering: Adjusted Rand Index: 0.695 Silhouette Score: 0.615 ``` Implementation You can use the following template to start with: ```python import pandas as pd from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import adjusted_rand_score, silhouette_score from sklearn.model_selection import train_test_split def load_data(file_path): # Load the dataset from the CSV file return pd.read_csv(file_path) def apply_clustering_and_evaluate(X): # K-Means Clustering kmeans = KMeans(n_clusters=3, random_state=42).fit(X) kmeans_labels = kmeans.labels_ kmeans_ari = adjusted_rand_score(X.index, kmeans_labels) kmeans_silhouette = silhouette_score(X, kmeans_labels) print(f\'K-Means Clustering:nAdjusted Rand Index: {kmeans_ari:.3f}nSilhouette Score: {kmeans_silhouette:.3f}n\') # DBSCAN Clustering dbscan = DBSCAN().fit(X) dbscan_labels = dbscan.labels_ dbscan_ari = adjusted_rand_score(X.index, dbscan_labels) dbscan_silhouette = silhouette_score(X, dbscan_labels) print(f\'DBSCAN Clustering:nAdjusted Rand Index: {dbscan_ari:.3f}nSilhouette Score: {dbscan_silhouette:.3f}n\') # Agglomerative Clustering agglom = AgglomerativeClustering(n_clusters=3, linkage=\'ward\').fit(X) agglom_labels = agglom.labels_ agglom_ari = adjusted_rand_score(X.index, agglom_labels) agglom_silhouette = silhouette_score(X, agglom_labels) print(f\'Agglomerative Clustering:nAdjusted Rand Index: {agglom_ari:.3f}nSilhouette Score: {agglom_silhouette:.3f}n\') if __name__ == \\"__main__\\": import sys file_path = sys.argv[1] data = load_data(file_path) features = data.iloc[:, :-1].values # Assuming last column is not a feature X_train, X_test = train_test_split(features, test_size=0.2, random_state=42) apply_clustering_and_evaluate(pd.DataFrame(X_train)) ```","solution":"import pandas as pd from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import adjusted_rand_score, silhouette_score def load_data(file_path): Load the dataset from the CSV file. Args: - file_path (str): Path to the CSV file. Returns: - pd.DataFrame: Loaded dataset. return pd.read_csv(file_path) def apply_clustering_and_evaluate(file_path): Apply three clustering algorithms to the dataset and evaluate performance. Args: - file_path (str): Path to the CSV file. data = load_data(file_path) X = data.values # Assuming all columns are features # K-Means Clustering kmeans = KMeans(n_clusters=3, random_state=42).fit(X) kmeans_labels = kmeans.labels_ kmeans_ari = adjusted_rand_score(X[:, 0], kmeans_labels) kmeans_silhouette = silhouette_score(X, kmeans_labels) print(f\'K-Means Clustering:nAdjusted Rand Index: {kmeans_ari:.3f}nSilhouette Score: {kmeans_silhouette:.3f}n\') # DBSCAN Clustering dbscan = DBSCAN().fit(X) dbscan_labels = dbscan.labels_ dbscan_ari = adjusted_rand_score(X[:, 0], dbscan_labels) if len(set(dbscan_labels)) > 1 else -1 dbscan_silhouette = silhouette_score(X, dbscan_labels) if len(set(dbscan_labels)) > 1 else -1 print(f\'DBSCAN Clustering:nAdjusted Rand Index: {dbscan_ari:.3f}nSilhouette Score: {dbscan_silhouette:.3f}n\') # Agglomerative Clustering agglom = AgglomerativeClustering(n_clusters=3, linkage=\'ward\').fit(X) agglom_labels = agglom.labels_ agglom_ari = adjusted_rand_score(X[:, 0], agglom_labels) agglom_silhouette = silhouette_score(X, agglom_labels) print(f\'Agglomerative Clustering:nAdjusted Rand Index: {agglom_ari:.3f}nSilhouette Score: {agglom_silhouette:.3f}n\')"},{"question":"**Question: Kernel Approximation Classification** Given a dataset of points in a 2D space and their corresponding binary labels, your task is to implement a function `kernel_approximation_classification` that trains and evaluates a classifier using different kernel approximation methods provided by scikit-learn. The function should: 1. Take as input: - `X_train`: A list of lists where each sublist represents the 2D coordinates of a training sample. - `y_train`: A list of integers representing the binary labels (0 or 1) for the training samples. - `X_test`: A list of lists where each sublist represents the 2D coordinates of a test sample. - `y_test`: A list of integers representing the binary labels (0 or 1) for the test samples. 2. Perform kernel approximation using the following methods: - Nystroem - RBFSampler 3. Train a `SGDClassifier` on the transformed training data for each approximation method. 4. Evaluate the trained classifier on the transformed test data for each approximation method. 5. Return a dictionary with the accuracy scores for each method. ```python from sklearn.linear_model import SGDClassifier from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.metrics import accuracy_score def kernel_approximation_classification(X_train, y_train, X_test, y_test): Train and evaluate classifiers using different kernel approximation methods. Parameters: - X_train (list of lists of float): The coordinates of training samples. - y_train (list of int): The binary labels of training samples. - X_test (list of lists of float): The coordinates of test samples. - y_test (list of int): The binary labels of test samples. Returns: dict: A dictionary with accuracy scores for each kernel approximation method. results = {} # Nystroem Method nystroem = Nystroem(kernel=\'rbf\', gamma=1, n_components=100).fit(X_train) X_train_nystroem = nystroem.transform(X_train) X_test_nystroem = nystroem.transform(X_test) clf_nystroem = SGDClassifier(max_iter=1000).fit(X_train_nystroem, y_train) y_pred_nystroem = clf_nystroem.predict(X_test_nystroem) results[\'Nystroem\'] = accuracy_score(y_test, y_pred_nystroem) # RBF Sampler Method rbfsampler = RBFSampler(gamma=1, n_components=100).fit(X_train) X_train_rbf = rbfsampler.transform(X_train) X_test_rbf = rbfsampler.transform(X_test) clf_rbf = SGDClassifier(max_iter=1000).fit(X_train_rbf, y_train) y_pred_rbf = clf_rbf.predict(X_test_rbf) results[\'RBFSampler\'] = accuracy_score(y_test, y_pred_rbf) return results ``` **Constraints:** - `X_train` and `X_test` will always contain at least one sample. - All features in `X_train` and `X_test` are 2D coordinates (i.e., lists of length 2). - `y_train` and `y_test` will always contain binary labels (0 or 1). - You should not use any other external libraries except `numpy` and `scikit-learn`. **Example:** ```python X_train = [[0, 0], [1, 1], [1, 0], [0, 1]] y_train = [0, 0, 1, 1] X_test = [[0.5, 0.5], [1, 0.5]] y_test = [0, 1] results = kernel_approximation_classification(X_train, y_train, X_test, y_test) print(results) ``` **Expected Output:** A dictionary with accuracy scores for each kernel approximation method. For example: ```python {\'Nystroem\': 0.5, \'RBFSampler\': 1.0} ```","solution":"from sklearn.linear_model import SGDClassifier from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.metrics import accuracy_score def kernel_approximation_classification(X_train, y_train, X_test, y_test): Train and evaluate classifiers using different kernel approximation methods. Parameters: - X_train (list of lists of float): The coordinates of training samples. - y_train (list of int): The binary labels of training samples. - X_test (list of lists of float): The coordinates of test samples. - y_test (list of int): The binary labels of test samples. Returns: dict: A dictionary with accuracy scores for each kernel approximation method. results = {} # Nystroem Method nystroem = Nystroem(kernel=\'rbf\', gamma=1, n_components=100).fit(X_train) X_train_nystroem = nystroem.transform(X_train) X_test_nystroem = nystroem.transform(X_test) clf_nystroem = SGDClassifier(max_iter=1000).fit(X_train_nystroem, y_train) y_pred_nystroem = clf_nystroem.predict(X_test_nystroem) results[\'Nystroem\'] = accuracy_score(y_test, y_pred_nystroem) # RBF Sampler Method rbfsampler = RBFSampler(gamma=1, n_components=100).fit(X_train) X_train_rbf = rbfsampler.transform(X_train) X_test_rbf = rbfsampler.transform(X_test) clf_rbf = SGDClassifier(max_iter=1000).fit(X_train_rbf, y_train) y_pred_rbf = clf_rbf.predict(X_test_rbf) results[\'RBFSampler\'] = accuracy_score(y_test, y_pred_rbf) return results"},{"question":"**Coding Assessment Question: Performance Evaluation of Sorting Algorithms** **Objective:** The objective of this task is for you to implement and compare the performance of two different sorting algorithms: Merge Sort and Quick Sort. You will use Python\'s `timeit` module to measure and compare the execution times of these sorting algorithms on a list of random integers. **Instructions:** 1. Implement two sorting functions: `merge_sort(arr)` and `quick_sort(arr)`. 2. Write a `generate_random_list(size, min_value, max_value)` function that generates a list of `size` random integers, each between `min_value` and `max_value`. 3. Use the `timeit` module to measure the time taken by each sorting function to sort lists of varying sizes: 100, 1000, 10000, and 100000. 4. Print the results in a readable format, clearly showing the time taken by each sorting algorithm for each list size. 5. Implement a `main()` function to run the tests and display the results. **Constraints:** - The input list should contain only integer values. - The implemented sorting algorithms should not make use of Python\'s built-in `sorted()` function. - For timing, each sorting function should be executed a minimum of 10 times to get an average time. **Expected Function Definitions:** ```python def merge_sort(arr): Implements the merge sort algorithm. pass def quick_sort(arr): Implements the quick sort algorithm. pass def generate_random_list(size, min_value, max_value): Generates a list of random integers. pass def main(): Runs the performance tests and displays the results. pass ``` **Example Output:** ``` Performance Comparison of Sorting Algorithms: List Size: 100 Merge Sort: 0.0021 seconds (average over 10 runs) Quick Sort: 0.0014 seconds (average over 10 runs) List Size: 1000 Merge Sort: 0.0256 seconds (average over 10 runs) Quick Sort: 0.0178 seconds (average over 10 runs) List Size: 10000 Merge Sort: 0.3024 seconds (average over 10 runs) Quick Sort: 0.1985 seconds (average over 10 runs) List Size: 100000 Merge Sort: 3.7548 seconds (average over 10 runs) Quick Sort: 2.4765 seconds (average over 10 runs) ``` **Notes:** - Use the `random` module to generate random integers. - Ensure that you seed the random number generator to make your results reproducible. - You may use additional helper functions if needed.","solution":"import random import timeit def merge_sort(arr): Implements the merge sort algorithm. if len(arr) <= 1: return arr mid = len(arr) // 2 left = merge_sort(arr[:mid]) right = merge_sort(arr[mid:]) return merge(left, right) def merge(left, right): result = [] i = j = 0 while i < len(left) and j < len(right): if left[i] < right[j]: result.append(left[i]) i += 1 else: result.append(right[j]) j += 1 result.extend(left[i:]) result.extend(right[j:]) return result def quick_sort(arr): Implements the quick sort algorithm. if len(arr) <= 1: return arr pivot = arr[len(arr) // 2] left = [x for x in arr if x < pivot] middle = [x for x in arr if x == pivot] right = [x for x in arr if x > pivot] return quick_sort(left) + middle + quick_sort(right) def generate_random_list(size, min_value, max_value): Generates a list of random integers. random.seed(0) return [random.randint(min_value, max_value) for _ in range(size)] def main(): Runs the performance tests and displays the results. sizes = [100, 1000, 10000, 100000] for size in sizes: random_list = generate_random_list(size, 0, size) merge_sort_time = timeit.timeit(\'merge_sort({})\'.format(random_list), setup=\'from __main__ import merge_sort\', number=10) quick_sort_time = timeit.timeit(\'quick_sort({})\'.format(random_list), setup=\'from __main__ import quick_sort\', number=10) print(f\\"List Size: {size}\\") print(f\\"Merge Sort: {merge_sort_time / 10:.4f} seconds (average over 10 runs)\\") print(f\\"Quick Sort: {quick_sort_time / 10:.4f} seconds (average over 10 runs)n\\") if __name__ == \'__main__\': main()"},{"question":"**Objective**: Write a class `MixedStreamReader` that reads from both text and binary streams and provides combined data access functionality. This class should utilize Python\'s `io` module to handle different types of streams. **Specifications**: 1. **Constructor**: ```python def __init__(self, text_stream: io.TextIOBase, binary_stream: io.BufferedIOBase): ``` - `text_stream`: A text I/O stream object. - `binary_stream`: A buffered binary I/O stream object. 2. **Methods**: - **Read from Text Stream**: ```python def read_text(self, size: int = -1) -> str: ``` Reads up to `size` characters from the text stream. If `size` is not provided or is negative, read until EOF. - **Read from Binary Stream**: ```python def read_binary(self, size: int = -1) -> bytes: ``` Reads up to `size` bytes from the binary stream. If `size` is not provided or is negative, read until EOF. - **Write to Text Stream**: ```python def write_text(self, data: str) -> int: ``` Writes the provided string `data` to the text stream. Returns the number of characters written. - **Write to Binary Stream**: ```python def write_binary(self, data: bytes) -> int: ``` Writes the provided bytes `data` to the binary stream. Returns the number of bytes written. - **Get Combined Content**: ```python def get_combined_content(self) -> Tuple[str, bytes]: ``` Reads the remaining content from both the text and binary streams and returns them as a tuple. - **Close Streams**: ```python def close(self) -> None: ``` Closes both the text and binary streams. **Constraints**: - All the methods should handle exceptions gracefully. - Ensure that reading and writing operations are done in an efficient manner. - Ensure that after calling `close()`, no further operations are performed on the streams. **Example Usage**: ```python import io # Sample text and binary streams text_data = \\"Hello, world!nThis is a text stream.\\" binary_data = b\'x00x01x02x03x04x05\' # Create text and binary IO objects text_stream = io.StringIO(text_data) binary_stream = io.BytesIO(binary_data) # Create the MixedStreamReader object mixed_reader = MixedStreamReader(text_stream, binary_stream) # Perform read and write operations print(mixed_reader.read_text(5)) # Output: Hello print(mixed_reader.read_binary(3)) # Output: b\'x00x01x02\' mixed_reader.write_text(\\" New text.\\") mixed_reader.write_binary(b\'x06x07\') # Get combined content combined_content = mixed_reader.get_combined_content() print(combined_content[0]) # Remaining text content print(combined_content[1]) # Remaining binary content # Close the streams mixed_reader.close() ``` **Notes**: - You are not allowed to use any external libraries. - Focus on correct usage of `io` module methods and exception handling.","solution":"import io from typing import Tuple class MixedStreamReader: def __init__(self, text_stream: io.TextIOBase, binary_stream: io.BufferedIOBase): self.text_stream = text_stream self.binary_stream = binary_stream def read_text(self, size: int = -1) -> str: try: return self.text_stream.read(size) except Exception as e: print(f\\"Error reading text stream: {e}\\") return \\"\\" def read_binary(self, size: int = -1) -> bytes: try: return self.binary_stream.read(size) except Exception as e: print(f\\"Error reading binary stream: {e}\\") return b\\"\\" def write_text(self, data: str) -> int: try: return self.text_stream.write(data) except Exception as e: print(f\\"Error writing to text stream: {e}\\") return 0 def write_binary(self, data: bytes) -> int: try: return self.binary_stream.write(data) except Exception as e: print(f\\"Error writing to binary stream: {e}\\") return 0 def get_combined_content(self) -> Tuple[str, bytes]: try: text_content = self.text_stream.read() binary_content = self.binary_stream.read() return (text_content, binary_content) except Exception as e: print(f\\"Error getting combined content: {e}\\") return (\\"\\", b\\"\\") def close(self) -> None: try: self.text_stream.close() self.binary_stream.close() except Exception as e: print(f\\"Error closing streams: {e}\\")"},{"question":"**Task: Implement Kernel Ridge Regression with Grid Search Optimization** # Problem Description You are required to implement a Kernel Ridge Regression model to predict a target variable from a given dataset. Your implementation should include a grid search for both regularization parameter and kernel parameters to optimize the model performance. # Requirements: 1. **Kernel Ridge Regression Implementation**: Use `KernelRidge` class from `sklearn.kernel_ridge`. 2. **Grid Search**: Optimize both the regularization parameter (`alpha`) and kernel parameters using grid search. 3. **Dataset**: Your implementation should work for any dataset split into training and testing sets. # Input: 1. `X_train`: Training data features, a NumPy array of shape `(n_samples_train, n_features)`. 2. `y_train`: Training data target, a NumPy array of shape `(n_samples_train,)`. 3. `X_test`: Testing data features, a NumPy array of shape `(n_samples_test, n_features)`. 4. `y_test`: Testing data target, a NumPy array of shape `(n_samples_test,)`. # Output: 1. `best_model`: The best `KernelRidge` model found via grid search. 2. `predictions`: Predicted target values for `X_test`, a NumPy array of shape `(n_samples_test,)`. 3. `best_params`: The best parameters found during the grid search. # Constraints: 1. Implement the grid search manually or use scikit-learn\'s `GridSearchCV`. 2. Consider `alpha` values in the range `[0.1, 1, 10]`. 3. Use the RBF kernel and consider the `gamma` values in the range `[1e-2, 1e-1, 1, 10]`. # Performance Requirements: - The solution should be efficient enough to handle large datasets up to 1000 samples efficiently. - The prediction time should be optimized and not significantly exceed the training time. # Example: ```python from sklearn.kernel_ridge import KernelRidge from sklearn.model_selection import GridSearchCV from sklearn.metrics import mean_squared_error def kernel_ridge_regression_with_grid_search(X_train, y_train, X_test, y_test): # Define the parameter grid param_grid = { \\"alpha\\": [0.1, 1, 10], \\"kernel\\": [\\"rbf\\"], \\"gamma\\": [1e-2, 1e-1, 1, 10] } # Initialize Kernel Ridge Regression model kr = KernelRidge() # Initialize GridSearchCV grid_search = GridSearchCV(estimator=kr, param_grid=param_grid, scoring=\'neg_mean_squared_error\', cv=5) # Fit GridSearchCV to the training data grid_search.fit(X_train, y_train) # Get the best model best_model = grid_search.best_estimator_ # Make predictions on the test data predictions = best_model.predict(X_test) # Get the best parameters best_params = grid_search.best_params_ return best_model, predictions, best_params # Example usage with dummy data if __name__ == \\"__main__\\": import numpy as np X_train = np.random.randn(100, 5) y_train = np.random.randn(100) X_test = np.random.randn(20, 5) y_test = np.random.randn(20) model, preds, params = kernel_ridge_regression_with_grid_search(X_train, y_train, X_test, y_test) print(\\"Best Parameters:\\", params) print(\\"Mean Squared Error on Test Set:\\", mean_squared_error(y_test, preds)) ``` # Note: - Ensure that you test your solution on multiple datasets to validate its correctness and performance. - Include comments in your code to explain the logic and flow.","solution":"from sklearn.kernel_ridge import KernelRidge from sklearn.model_selection import GridSearchCV def kernel_ridge_regression_with_grid_search(X_train, y_train, X_test, y_test): Perform Kernel Ridge Regression with Grid Search to find the best parameters and make predictions on the test set. Parameters: - X_train: numpy array, shape (n_samples_train, n_features), Training data features - y_train: numpy array, shape (n_samples_train,), Training data target - X_test: numpy array, shape (n_samples_test, n_features), Testing data features - y_test: numpy array, shape (n_samples_test,), Testing data target Returns: - best_model: The best KernelRidge model found via grid search - predictions: Predicted target values for X_test, numpy array of shape (n_samples_test,) - best_params: The best parameters found during the grid search # Define the parameter grid param_grid = { \\"alpha\\": [0.1, 1, 10], \\"kernel\\": [\\"rbf\\"], \\"gamma\\": [1e-2, 1e-1, 1, 10] } # Initialize Kernel Ridge Regression model kr = KernelRidge() # Initialize GridSearchCV grid_search = GridSearchCV(estimator=kr, param_grid=param_grid, scoring=\'neg_mean_squared_error\', cv=5) # Fit GridSearchCV to the training data grid_search.fit(X_train, y_train) # Get the best model best_model = grid_search.best_estimator_ # Make predictions on the test data predictions = best_model.predict(X_test) # Get the best parameters best_params = grid_search.best_params_ return best_model, predictions, best_params"},{"question":"Objective: The goal of this assessment is to evaluate your understanding of the tensor storage mechanism in PyTorch by creating tensors, manipulating their storage, and observing the effects on tensor data. Task: 1. Create a tensor of size (5, 5) filled with random values. 2. Access its untyped storage and clone this storage. 3. Modify the cloned storage by filling it with a fixed value (e.g., 7). 4. Reassign the modified storage to the original tensor. 5. Create a second tensor that shares the same storage with the modified tensor (demonstrate the sharing). 6. Verify and print the results showing that both tensors point to the same storage and display the values of both tensors to confirm they reflect the storage modifications. Requirements: - Implement the function `manipulate_tensor_storage()`. - The function should not take any input parameters. - Proper usage of PyTorch functions and methods for accessing and manipulating storage. - Clear and commented code illustrating each step of the process. Expected Output: The function should print: 1. The original tensor and its untyped storage. 2. The modified tensor after its storage has been updated. 3. The second tensor showing it shares the same updated storage. 4. Unique data pointers of the storages to verify that the tensors are indeed sharing storage. Constraints: - Follow best practices for handling tensors and their storages. - Avoid directly manipulating internal attributes unless through provided PyTorch methods. ```python def manipulate_tensor_storage(): import torch # Step 1: Create a tensor of size (5, 5) filled with random values original_tensor = torch.rand((5, 5)) print(\\"Original Tensor:\\") print(original_tensor) # Step 2: Access its untyped storage and clone this storage original_storage = original_tensor.untyped_storage() cloned_storage = original_storage.clone() # Step 3: Modify the cloned storage by filling it with a fixed value (e.g., 7) cloned_storage.fill_(7) # Step 4: Reassign the modified storage to the original tensor original_tensor.set_(cloned_storage, storage_offset=original_tensor.storage_offset(), stride=original_tensor.stride(), size=original_tensor.size()) print(\\"nModified Tensor after setting with new storage filled with 7:\\") print(original_tensor) # Step 5: Create a second tensor that shares the same storage with the modified tensor shared_tensor = torch.tensor(()).set_(original_tensor.untyped_storage(), storage_offset=original_tensor.storage_offset(), stride=original_tensor.stride(), size=original_tensor.size()) print(\\"nSecond Tensor sharing the same storage:\\") print(shared_tensor) # Step 6: Verify and print data pointers to confirm shared storage print(\\"nData pointer of original tensor\'s storage:\\", original_tensor.untyped_storage().data_ptr()) print(\\"Data pointer of shared tensor\'s storage:\\", shared_tensor.untyped_storage().data_ptr()) # Uncomment the following line to test the function # manipulate_tensor_storage() ``` Ensure that your implementation is both functional and demonstrates your understanding of PyTorch\'s tensor storage system. Good luck!","solution":"def manipulate_tensor_storage(): import torch # Step 1: Create a tensor of size (5, 5) filled with random values original_tensor = torch.rand((5, 5)) print(\\"Original Tensor:\\") print(original_tensor) # Step 2: Access its untyped storage and clone this storage original_storage = original_tensor.storage() cloned_storage = original_storage.clone() # Step 3: Modify the cloned storage by filling it with a fixed value (e.g., 7) cloned_storage.fill_(7) # Step 4: Reassign the modified storage to the original tensor original_tensor.set_(cloned_storage, storage_offset=0, stride=original_tensor.stride(), size=original_tensor.size()) print(\\"nModified Tensor after setting with new storage filled with 7:\\") print(original_tensor) # Step 5: Create a second tensor that shares the same storage with the modified tensor shared_tensor = torch.empty(original_tensor.size()).set_(original_tensor.storage(), storage_offset=0, stride=original_tensor.stride(), size=original_tensor.size()) print(\\"nSecond Tensor sharing the same storage:\\") print(shared_tensor) # Step 6: Verify and print data pointers to confirm shared storage print(\\"nData pointer of original tensor\'s storage:\\", original_tensor.storage().data_ptr()) print(\\"Data pointer of shared tensor\'s storage:\\", shared_tensor.storage().data_ptr()) # Uncomment the following line to test the function # manipulate_tensor_storage()"},{"question":"You are tasked with implementing a function that processes two dictionary objects and returns a new dictionary based on specific rules. The dictionaries represent a collection of items sold in two different stores, with product names as keys and the quantity sold as values. Function Specifications: **Function Name:** ```python def merge_store_data(dict1: dict, dict2: dict) -> dict: ``` **Input:** - `dict1`: A dictionary with string keys and integer values representing items and their quantities sold in Store 1. - `dict2`: A dictionary with string keys and integer values representing items and their quantities sold in Store 2. **Output:** - Returns a new dictionary where: - The keys are all unique items from both `dict1` and `dict2`. - The values are the sums of the quantities from both dictionaries for the corresponding item keys. - If an item key is not present in one of the dictionaries, its quantity should be considered as zero from that store. Constraints: 1. Assume all quantities in the dictionaries are non-negative integers. 2. You are not allowed to use any external libraries except standard Python libraries. 3. Ensure the solution is efficient to handle cases where each dictionary can have up to 10,000 items. Example: ```python # Example 1 dict1 = {\\"apples\\": 100, \\"bananas\\": 150, \\"oranges\\": 200} dict2 = {\\"bananas\\": 100, \\"grapes\\": 150, \\"apples\\": 50} result = merge_store_data(dict1, dict2) print(result) # Output: {\\"apples\\": 150, \\"bananas\\": 250, \\"oranges\\": 200, \\"grapes\\": 150} # Example 2 dict1 = {\\"milk\\": 30, \\"bread\\": 20} dict2 = {\\"bread\\": 25, \\"butter\\": 10} result = merge_store_data(dict1, dict2) print(result) # Output: {\\"milk\\": 30, \\"bread\\": 45, \\"butter\\": 10} ``` **Note:** Make sure to include proper type checks and handle potential edge cases, such as empty dictionaries. Additional Considerations: - Think about how you can efficiently merge the two dictionaries without unnecessary iterations.","solution":"def merge_store_data(dict1: dict, dict2: dict) -> dict: Merges two dictionaries containing item quantities from two different stores. Args: dict1 (dict): Dictionary with item quantities from Store 1. dict2 (dict): Dictionary with item quantities from Store 2. Returns: dict: A new dictionary with items as keys and the total quantities from both stores as values. # Create a new dictionary to hold the results result = {} # Add all items from dict1 to result for item, quantity in dict1.items(): result[item] = quantity # Add all items from dict2 to result, summing the quantities if the item already exists for item, quantity in dict2.items(): if item in result: result[item] += quantity else: result[item] = quantity return result"},{"question":"The `fractions` module in Python allows for sophisticated handling and arithmetic of rational numbers. Your task is to implement a function that utilizes the capabilities of the `Fraction` class to perform specific manipulations and transformations on a list of mixed numeric types. # **Task** Write a function `process_fractions(data: List[Union[int, float, str]]) -> List[Tuple[Fraction, Tuple[int, int]]]` that processes a list of integers, floats, and strings (representing fractions or floating-point values), converts them to `Fraction` instances, and returns a list of tuples. Each tuple should contain: 1. The `Fraction` instance created from the input. 2. A tuple of two integers (numerator and denominator of the fraction\'s lowest term). # **Input** - A list of mixed numeric types: `data: List[Union[int, float, str]]` containing integers, floating-point numbers, and strings representing rational or floating-point numbers. # **Output** - A list of tuples: `List[Tuple[Fraction, Tuple[int, int]]]`. Each tuple contains a `Fraction` instance and a tuple with the numerator and denominator of the `Fraction`. # **Examples** ```python from fractions import Fraction from typing import List, Union, Tuple def process_fractions(data: List[Union[int, float, str]]) -> List[Tuple[Fraction, Tuple[int, int]]]: result = [] for value in data: fraction = Fraction(value) result.append((fraction, (fraction.numerator, fraction.denominator))) return result data1 = [3, 2.5, \'3/4\', \'1.25\', \'-7/3\'] print(process_fractions(data1)) # Output: # [ # (Fraction(3, 1), (3, 1)), # (Fraction(5, 2), (5, 2)), # (Fraction(3, 4), (3, 4)), # (Fraction(5, 4), (5, 4)), # (Fraction(-7, 3), (-7, 3)) # ] ``` # **Constraints** 1. You may assume all inputs are valid integers, floats, or strings that can be converted to fractions. 2. Handle floating-point precision according to the `Fraction` class\'s handling of such values. # **Performance Requirements** - The solution should efficiently handle the conversions and processing even for large lists. # **Note** - Ensure your solution handles edge cases such as negative numbers, improper fractions, and floating-point inaccuracies accurately by using the appropriate methods from the `Fraction` class.","solution":"from fractions import Fraction from typing import List, Union, Tuple def process_fractions(data: List[Union[int, float, str]]) -> List[Tuple[Fraction, Tuple[int, int]]]: Processes a list of integers, floats, and strings (representing fractions or floating-point values), converts them to Fraction instances, and returns a list of tuples containing the Fraction and its numerator and denominator. result = [] for value in data: fraction = Fraction(value) result.append((fraction, (fraction.numerator, fraction.denominator))) return result"},{"question":"# Interactive Python Console Implementation Using the `code` module documentation provided, implement a custom interactive Python console class that can handle the following requirements: 1. **Initialization**: - Subclass `InteractiveConsole`. - Use a custom dictionary for the local namespace during initialization. 2. **Custom Prompt Handling**: - Override the `raw_input` method to customize the input prompt. - Implement a mechanism to detect if the user is entering a multi-line statement. Use a different prompt for continued input. 3. **Session Handling**: - Provide a custom `interact` method that: - Prints a custom welcome banner. - Handles user input using the custom prompt. - Displays a farewell message upon exit. # Constraints and Requirements - The console should correctly handle multi-line statements (e.g., functions, loops). - Ensure that syntax errors and runtime exceptions are captured and displayed using `showsyntaxerror` and `showtraceback`. - Provide a method to reset the interpreter state, clearing any variables and functions defined by the user. # Input and Output - **Input**: User input in the form of Python code. - **Output**: Execution results, error messages, and custom prompts displayed interactively. # Implementation Guidelines - **Class Name**: `CustomInteractiveConsole` - **Methods**: - `__init__(self, prompt=\'>>> \', cont_prompt=\'... \')`: Initialize the console with custom prompts. - `raw_input(self, prompt)`: Override to handle custom prompts. - `interact(self, banner, exitmsg)`: Override to include a custom banner and exit message. - `runsource(self, source, filename=\'<input>\', symbol=\'single\')`: (optional) Override if custom handling is needed. - `reset_interpreter(self)`: Reset the interpreter state. # Performance Requirements - The console should be responsive and handle typical input sizes for interactive sessions. - Ensure that multi-line input detection and execution remains efficient. # Example ```python class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, prompt=\'>>> \', cont_prompt=\'... \'): self.prompt = prompt self.cont_prompt = cont_prompt super().__init__(locals={}) def raw_input(self, prompt=\'\'): return input(prompt) def interact(self, banner=None, exitmsg=None): banner = banner or \\"Welcome to the Custom Python Console!\\" exitmsg = exitmsg or \\"Goodbye!\\" print(banner) try: while True: try: line = self.raw_input(self.prompt) while self.push(line): line = self.raw_input(self.cont_prompt) except EOFError: print(\\"n\\" + exitmsg) break except KeyboardInterrupt: print(\\"nInterrupted by user.\\") def reset_interpreter(self): self.locals.clear() # Sample usage console = CustomInteractiveConsole(prompt=\'>>> \', cont_prompt=\'... \') console.interact(banner=\\"Custom Console Session Started. Type exit() to leave.\\", exitmsg=\\"Session Ended.\\") ``` Implement the `CustomInteractiveConsole` class as specified and test it to ensure it meets all requirements.","solution":"import code class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, prompt=\'>>> \', cont_prompt=\'... \', locals=None): if locals is None: locals = {} self.prompt = prompt self.cont_prompt = cont_prompt super().__init__(locals=locals) def raw_input(self, prompt=\'\'): return input(prompt) def interact(self, banner=None, exitmsg=None): banner = banner or \\"Welcome to the Custom Python Console!\\" exitmsg = exitmsg or \\"Goodbye!\\" print(banner) try: super().interact(banner=\\"\\", exitmsg=\\"\\") except KeyboardInterrupt: print(\\"nInterrupted by user.\\") print(exitmsg) def reset_interpreter(self): self.locals.clear() print(\\"Interpreter state has been reset.\\")"},{"question":"Objective You are tasked with creating a function that tests the behavior of `fill_uninitialized_memory` when using various tensor operations in PyTorch. This question assesses your understanding of deterministic algorithms and memory initialization in PyTorch tensors. Problem Statement Write a Python function `test_uninitialized_memory_behavior()` that performs the following: 1. Sets the PyTorch deterministic algorithm mode to `True`. 2. Uses the `torch.utils.deterministic.fill_uninitialized_memory` attribute. 3. Demonstrates the impact of setting `fill_uninitialized_memory` to `True` and `False` on uninitialized memory by using the `torch.empty` function. 4. Displays the difference in the tensor contents when `fill_uninitialized_memory` is set to `True` and `False`. Ensure you handle complex and integer tensor types. # Function Signature ```python def test_uninitialized_memory_behavior(): pass ``` # Expected Behavior - The function should print tensor values after calling `torch.empty` with `fill_uninitialized_memory` set to `True` and then `False`. - The output should highlight the different values stored in the uninitialized memory under each setting. # Constraints - Ensure the function runs in a deterministic environment by using `torch.use_deterministic_algorithms(True)`. - Use at least two different data types of tensors (e.g., float, complex, int). # Example ```python def test_uninitialized_memory_behavior(): import torch # Set deterministic algorithms to True torch.use_deterministic_algorithms(True) # Test with fill_uninitialized_memory == True torch.utils.deterministic.fill_uninitialized_memory = True float_tensor_true = torch.empty(3, 3, dtype=torch.float32) complex_tensor_true = torch.empty(3, 3, dtype=torch.complex64) int_tensor_true = torch.empty(3, 3, dtype=torch.int32) print(\\"Float Tensor with fill_uninitialized_memory=True:\\", float_tensor_true) print(\\"Complex Tensor with fill_uninitialized_memory=True:\\", complex_tensor_true) print(\\"Int Tensor with fill_uninitialized_memory=True:\\", int_tensor_true) # Test with fill_uninitialized_memory == False torch.utils.deterministic.fill_uninitialized_memory = False float_tensor_false = torch.empty(3, 3, dtype=torch.float32) complex_tensor_false = torch.empty(3, 3, dtype=torch.complex64) int_tensor_false = torch.empty(3, 3, dtype=torch.int32) print(\\"Float Tensor with fill_uninitialized_memory=False:\\", float_tensor_false) print(\\"Complex Tensor with fill_uninitialized_memory=False:\\", complex_tensor_false) print(\\"Int Tensor with fill_uninitialized_memory=False:\\", int_tensor_false) test_uninitialized_memory_behavior() ``` - Output should show the difference in the tensors\' uninitialized memory for each case. # Notes - Make sure all necessary imports are included in your function. - Comments within the code help in understanding each step.","solution":"import torch def test_uninitialized_memory_behavior(): # Set deterministic algorithms to True torch.use_deterministic_algorithms(True) print(\\"Testing with fill_uninitialized_memory set to True:\\") torch.utils.deterministic.fill_uninitialized_memory = True float_tensor_true = torch.empty(3, 3, dtype=torch.float32) complex_tensor_true = torch.empty(3, 3, dtype=torch.complex64) int_tensor_true = torch.empty(3, 3, dtype=torch.int32) print(\\"Float Tensor with fill_uninitialized_memory=True:\\", float_tensor_true) print(\\"Complex Tensor with fill_uninitialized_memory=True:\\", complex_tensor_true) print(\\"Int Tensor with fill_uninitialized_memory=True:\\", int_tensor_true) print(\\"nTesting with fill_uninitialized_memory set to False:\\") torch.utils.deterministic.fill_uninitialized_memory = False float_tensor_false = torch.empty(3, 3, dtype=torch.float32) complex_tensor_false = torch.empty(3, 3, dtype=torch.complex64) int_tensor_false = torch.empty(3, 3, dtype=torch.int32) print(\\"Float Tensor with fill_uninitialized_memory=False:\\", float_tensor_false) print(\\"Complex Tensor with fill_uninitialized_memory=False:\\", complex_tensor_false) print(\\"Int Tensor with fill_uninitialized_memory=False:\\", int_tensor_false)"},{"question":"Objective: To assess your understanding of Python\'s C extension interface and your ability to simulate the behavior of the `PyCapsule` type provided by the python310 package. Question: You are required to implement a Python class `PyCapsuleSimulator` that simulates the behavior of the `PyCapsule` type. This class should encapsulate a pointer (which can be simulated using an integer) and provide methods to set and retrieve the pointer, name, context, and destructor. Requirements: 1. Implement a class `PyCapsuleSimulator`: - Method `__init__(self, pointer, name=None, destructor=None)`: Initializes the capsule with a non-null pointer. Optionally, it can have a name and a destructor. - Method `get_pointer(self)`: Returns the encapsulated pointer. - Method `set_pointer(self, pointer)`: Sets the encapsulated pointer to a non-null value. - Method `get_name(self)`: Returns the name of the capsule. - Method `set_name(self, name)`: Sets the name of the capsule. - Method `get_context(self)`: Returns the context of the capsule. - Method `set_context(self, context)`: Sets the context of the capsule. - Method `get_destructor(self)`: Returns the destructor of the capsule. - Method `set_destructor(self, destructor)`: Sets the destructor of the capsule. - Method `is_valid(self, name)`: Checks if the capsule is valid, i.e., has a non-null pointer and the given name matches the capsule\'s name. Constraints: - The pointer should be an integer and non-null (non-zero). - The name, if provided, should be a string. - The destructor, if provided, should be a callable that takes one argument. Input/Output: - There is no direct input/output. The class should be tested using Python\'s `unittest` framework to verify its behavior. Example Usage: ```python capsule = PyCapsuleSimulator(42, \\"example.capsule\\", lambda x: print(f\\"Destructor called with {x}\\")) print(capsule.get_pointer()) # Output: 42 print(capsule.get_name()) # Output: example.capsule capsule.set_pointer(84) print(capsule.get_pointer()) # Output: 84 print(capsule.is_valid(\\"example.capsule\\")) # Output: True ``` Performance: - Ensure that all methods run in constant time O(1). Implement the `PyCapsuleSimulator` class below: ```python class PyCapsuleSimulator: def __init__(self, pointer, name=None, destructor=None): if pointer == 0: raise ValueError(\\"Pointer cannot be null\\") self.pointer = pointer self.name = name self.destructor = destructor self.context = None def get_pointer(self): return self.pointer def set_pointer(self, pointer): if pointer == 0: raise ValueError(\\"Pointer cannot be null\\") self.pointer = pointer def get_name(self): return self.name def set_name(self, name): self.name = name def get_context(self): return self.context def set_context(self, context): self.context = context def get_destructor(self): return self.destructor def set_destructor(self, destructor): self.destructor = destructor def is_valid(self, name): return self.pointer != 0 and self.name == name ``` Testing: Write a series of `unittest` test cases to verify the functionality of the `PyCapsuleSimulator` class. ```python import unittest class TestPyCapsuleSimulator(unittest.TestCase): def test_initialization(self): capsule = PyCapsuleSimulator(42) self.assertEqual(capsule.get_pointer(), 42) self.assertIsNone(capsule.get_name()) self.assertIsNone(capsule.get_destructor()) def test_set_get_pointer(self): capsule = PyCapsuleSimulator(42) capsule.set_pointer(84) self.assertEqual(capsule.get_pointer(), 84) with self.assertRaises(ValueError): capsule.set_pointer(0) def test_set_get_name(self): capsule = PyCapsuleSimulator(42, \\"example.capsule\\") self.assertEqual(capsule.get_name(), \\"example.capsule\\") capsule.set_name(\\"new.capsule\\") self.assertEqual(capsule.get_name(), \\"new.capsule\\") def test_set_get_context(self): capsule = PyCapsuleSimulator(42) capsule.set_context(\\"context_data\\") self.assertEqual(capsule.get_context(), \\"context_data\\") def test_set_get_destructor(self): destructor = lambda x: print(f\\"Destructor called with {x}\\") capsule = PyCapsuleSimulator(42, destructor=destructor) self.assertEqual(capsule.get_destructor(), destructor) def test_is_valid(self): capsule = PyCapsuleSimulator(42, \\"example.capsule\\") self.assertTrue(capsule.is_valid(\\"example.capsule\\")) self.assertFalse(capsule.is_valid(\\"wrong.capsule\\")) capsule.set_pointer(0) self.assertFalse(capsule.is_valid(\\"example.capsule\\")) if __name__ == \'__main__\': unittest.main() ```","solution":"class PyCapsuleSimulator: def __init__(self, pointer, name=None, destructor=None): if pointer == 0: raise ValueError(\\"Pointer cannot be null\\") self.pointer = pointer self.name = name self.destructor = destructor self.context = None def get_pointer(self): return self.pointer def set_pointer(self, pointer): if pointer == 0: raise ValueError(\\"Pointer cannot be null\\") self.pointer = pointer def get_name(self): return self.name def set_name(self, name): self.name = name def get_context(self): return self.context def set_context(self, context): self.context = context def get_destructor(self): return self.destructor def set_destructor(self, destructor): self.destructor = destructor def is_valid(self, name): return self.pointer != 0 and self.name == name"},{"question":"# Seaborn Boxen Plot Coding Assessment Objective The objective of this assignment is to test your understanding of seaborn\'s `boxenplot` function and your ability to utilize advanced plotting options available in seaborn. Problem Statement You are given a dataset containing information on various car models. Your task is to create a comprehensive visual analysis of the relationship between the horsepower of cars and their fuel efficiency (miles per gallon). Use the seaborn library to create and customize boxen plots to visualize this relationship. Dataset You are provided with a dataset `cars` that contains the following columns: - `horsepower`: A numeric value representing the horsepower of a car. - `mpg`: A numeric value representing the miles per gallon (fuel efficiency) of a car. - `origin`: A categorical value representing the origin of the car (e.g., USA, Europe, Japan). You should load the dataset using seaborn\'s `load_dataset` function: ```python import seaborn as sns cars = sns.load_dataset(\\"mpg\\").dropna(subset=[\'horsepower\']) ``` Tasks 1. **Single Horizontal Plot** Create a single horizontal boxen plot to visualize the distribution of car horsepower. ```python sns.boxenplot(x=cars[\\"horsepower\\"]) ``` 2. **Group by a Categorical Variable** Group the cars by their origin and plot the distribution of their horsepower. ```python sns.boxenplot(data=cars, x=\\"horsepower\\", y=\\"origin\\") ``` 3. **Group by Another Variable with Color Representation** Create a boolean column `high_mpg` representing if a car\'s mpg is greater than 25. Group by car origin, and represent `high_mpg` by the color of the boxes. ```python high_mpg = cars[\\"mpg\\"].gt(25).rename(\\"high_mpg\\") sns.boxenplot(data=cars, x=\\"horsepower\\", y=\\"origin\\", hue=high_mpg, gap=0.2) ``` 4. **Adjust Box Width** Adjust the width of the boxes to be linearly smaller. ```python sns.boxenplot(data=cars, x=\\"horsepower\\", y=\\"origin\\", width_method=\\"linear\\") ``` 5. **Control Outlines and Appearance** Customize the appearance of the boxes by modifying the line color, line width, and flier options. ```python sns.boxenplot(data=cars, x=\\"horsepower\\", y=\\"origin\\", linewidth=0.5, linecolor=\\".7\\", line_kws=dict(linewidth=1.5, color=\\"#cde\\"), flier_kws=dict(facecolor=\\".7\\", linewidth=0.5)) ``` 6. **Draw Unfilled Boxes** Draw unfilled boxes to represent the distribution of horsepower among cars of different origins. ```python sns.boxenplot(data=cars, x=\\"horsepower\\", y=\\"origin\\", hue=\\"origin\\", fill=False) ``` Submission Requirements - Implement the above tasks in a Jupyter notebook or script. - Make sure your code is well-documented, and each plot is clearly labeled. - Submit your solution as a single `.ipynb` file or a script with the necessary comments explaining your approach. This task will test your ability to work with seaborn, manipulate datasets, and create complex visualizations using seaborn\'s `boxenplot` function.","solution":"import seaborn as sns def load_and_prepare_data(): Loads and prepares the \'mpg\' dataset, ensuring there are no missing values in the \'horsepower\' column. cars = sns.load_dataset(\\"mpg\\").dropna(subset=[\'horsepower\']) return cars def plot_single_horizontal_boxenplot(cars): Creates a single horizontal boxen plot to visualize the distribution of car horsepower. sns.boxenplot(x=cars[\\"horsepower\\"]) def plot_grouped_by_origin_boxenplot(cars): Creates a horizontal boxen plot of car horsepower grouped by origin. sns.boxenplot(data=cars, x=\\"horsepower\\", y=\\"origin\\") def plot_grouped_by_origin_with_high_mpg_boxenplot(cars): Creates a horizontal boxen plot of car horsepower grouped by origin, with an additional hue representing whether the car\'s mpg is greater than 25. high_mpg = cars[\\"mpg\\"].gt(25).rename(\\"high_mpg\\") sns.boxenplot(data=cars, x=\\"horsepower\\", y=\\"origin\\", hue=high_mpg, gap=0.2) def plot_adjusted_width_boxenplot(cars): Creates a horizontal boxen plot of car horsepower grouped by origin, with the width of boxes set to be linearly smaller. sns.boxenplot(data=cars, x=\\"horsepower\\", y=\\"origin\\", width_method=\\"linear\\") def plot_customized_appearance_boxenplot(cars): Creates a horizontal boxen plot of car horsepower grouped by origin, with customized appearance settings. sns.boxenplot( data=cars, x=\\"horsepower\\", y=\\"origin\\", linewidth=0.5, linecolor=\\".7\\", line_kws=dict(linewidth=1.5, color=\\"#cde\\"), flier_kws=dict(facecolor=\\".7\\", linewidth=0.5) ) def plot_unfilled_boxes_boxenplot(cars): Creates a horizontal boxen plot of car horsepower grouped by origin, with unfilled boxes. sns.boxenplot(data=cars, x=\\"horsepower\\", y=\\"origin\\", hue=\\"origin\\", fill=False)"},{"question":"Objective Demonstrate your understanding of Python\'s formatted string literals (f-strings) and file handling by writing a Python program that processes a text file containing numerical data. Problem Statement You are given a text file named `data.txt`. The file contains rows of numerical data separated by commas. Each row represents a record of measurements taken in a scientific experiment. Your task is to write a Python function that reads this file, calculates the average of the numbers in each row, and writes the results to a new file named `results.txt`. Each row in the output file should contain the original row of data followed by the average of that row formatted to two decimal places. Requirements - The function should be named `process_data_file`. - The function should take no arguments. - Use formatted string literals (f-strings) for formatting the output. - Open and process the input file using the `with` statement to ensure proper file closure. - Handle potential I/O errors gracefully. Input A text file named `data.txt` with the following format: ``` 12.5,15.0,18.2 0.1,0.5,1.2 3.5,3.6,3.7,3.8 ``` Each row contains numerical data separated by commas. Output A text file named `results.txt` with the following format: ``` 12.5,15.0,18.2 -> 15.23 0.1,0.5,1.2 -> 0.60 3.5,3.6,3.7,3.8 -> 3.65 ``` - Each row in the output file contains the original row of data followed by the average of that row, formatted to two decimal places. Constraints and Considerations - The input file `data.txt` may contain any number of rows. - Each row will contain at least one numerical value. - Your solution should handle and log any I/O errors that may occur when reading from or writing to the file. - Ensure the output file is properly closed after writing the data. Example Given an input file `data.txt` with the following content: ``` 2.4,3.6,4.8 10.5,21.0,31.5 ``` The output file `results.txt` should contain: ``` 2.4,3.6,4.8 -> 3.60 10.5,21.0,31.5 -> 21.00 ``` Implementation Implement the function `process_data_file` as described. Make sure to include error handling and use formatted string literals for formatting the output. ```python def process_data_file(): try: # Read from data.txt with open(\'data.txt\', \'r\') as infile: lines = infile.readlines() results = [] # Process each line for line in lines: numbers = list(map(float, line.strip().split(\',\'))) average = sum(numbers) / len(numbers) formatted_output = f\\"{\',\'.join(map(str, numbers))} -> {average:.2f}\\" results.append(formatted_output) # Write to results.txt with open(\'results.txt\', \'w\') as outfile: for result in results: outfile.write(result + \'n\') except IOError as e: print(f\\"An error occurred: {e}\\") # Invoke the function to process the data process_data_file() ``` Ensure to test the function with a sample `data.txt` file to verify that it works as expected.","solution":"def process_data_file(): try: # Read from data.txt with open(\'data.txt\', \'r\') as infile: lines = infile.readlines() results = [] # Process each line for line in lines: numbers = list(map(float, line.strip().split(\',\'))) average = sum(numbers) / len(numbers) formatted_output = f\\"{\',\'.join(map(str, numbers))} -> {average:.2f}\\" results.append(formatted_output) # Write to results.txt with open(\'results.txt\', \'w\') as outfile: for result in results: outfile.write(result + \'n\') except IOError as e: print(f\\"An error occurred: {e}\\")"},{"question":"You are given a function `fetch_data` that reads data asynchronously from a server. Your task is to implement the function `fetch_with_timeout` that attempts to fetch data using `fetch_data` but ensures that it either completes within a given timeout or raises a custom exception with a meaningful message if the operation fails or times out. # Requirements: 1. The function `fetch_with_timeout` should accept the following parameters: - `fetch_data`: An asynchronous function to fetch the data. - `timeout`: The maximum time to allow `fetch_data` to complete before timing out (in seconds). 2. The function should handle the following exceptions: - `asyncio.TimeoutError`: If the `fetch_data` operation exceeds the provided `timeout`. - `asyncio.CancelledError`: If the operation is cancelled. - `asyncio.IncompleteReadError`: If the read operation did not complete fully. - `asyncio.LimitOverrunError`: If the buffer size limit was reached. 3. Provide meaningful error messages for each exception type. # Input and Output - **Input**: - An asynchronous function `fetch_data`. - An integer `timeout`. - **Output**: - The data fetched by `fetch_data` if successful, otherwise raise an appropriate exception with a meaningful message. # Constraints - You should not modify the `fetch_data` function. - The `fetch_with_timeout` function must handle the exceptions as specified. - Consider using `asyncio.wait_for` to enforce the timeout. # Function Signature ```python import asyncio from typing import Any async def fetch_with_timeout(fetch_data: Any, timeout: int) -> Any: pass ``` # Example ```python import asyncio async def example_fetch(): await asyncio.sleep(2) return \\"data retrieved\\" async def main(): try: result = await fetch_with_timeout(example_fetch, 1) print(result) except Exception as e: print(str(e)) asyncio.run(main()) ``` In the above example, since the `example_fetch` function takes 2 seconds to complete and the timeout is set to 1 second, the `fetch_with_timeout` function should raise an `asyncio.TimeoutError` with a custom error message.","solution":"import asyncio class FetchTimeoutError(Exception): pass class FetchCancelledError(Exception): pass class FetchIncompleteReadError(Exception): pass class FetchLimitOverrunError(Exception): pass async def fetch_with_timeout(fetch_data, timeout): try: return await asyncio.wait_for(fetch_data(), timeout) except asyncio.TimeoutError: raise FetchTimeoutError(\\"The operation exceeded the given timeout limit.\\") except asyncio.CancelledError: raise FetchCancelledError(\\"The operation was cancelled.\\") except asyncio.IncompleteReadError: raise FetchIncompleteReadError(\\"The read operation did not complete fully.\\") except asyncio.LimitOverrunError: raise FetchLimitOverrunError(\\"The buffer size limit was reached.\\")"},{"question":"Objective: Write a function in PyTorch that takes a list of tensors and returns a list of tuples representing the sizes of each tensor. Additionally, the function should ensure that all the output sizes are converted to lists before returning the result. Function Signature: ```python def get_tensor_sizes(tensor_list: list) -> list: pass ``` Input: - `tensor_list` (list of `torch.Tensor`): A list of PyTorch tensors. Output: - (list of list of int): A list of lists where each list contains the sizes of the corresponding tensor from the input list. Constraints: - The input list will have at least one tensor. - Tensors can be of varying dimensions. - Do not use any external libraries except PyTorch. Example: ```python # Example usage: import torch t1 = torch.ones(3, 4) t2 = torch.ones(10, 20, 30) t3 = torch.ones(7) tensor_list = [t1, t2, t3] print(get_tensor_sizes(tensor_list)) # Expected Output: [[3, 4], [10, 20, 30], [7]] ``` Performance Requirements: - The function should handle lists with up to 1000 tensors efficiently. Use this function to demonstrate your understanding of working with tensors and their sizes in PyTorch.","solution":"import torch def get_tensor_sizes(tensor_list: list) -> list: Takes a list of PyTorch tensors and returns a list of lists representing the sizes of each tensor. Args: tensor_list (list): List of PyTorch tensors. Returns: list: List of lists where each inner list contains the sizes of the corresponding tensor from the input list. return [list(tensor.size()) for tensor in tensor_list]"},{"question":"**Creation of a Custom Python Object Type** # Objective Your task is to demonstrate proficiency in defining a custom Python object type that includes memory management and integration with Python\'s garbage collection mechanisms. # Requirements 1. Implement a custom Python object type called `CustomObject` that: - Stores an integer and a string. - Allocates memory correctly for its attributes. - Integrates with Python’s garbage collection. 2. Ensure the `CustomObject` can: - Initialize with specified integer and string values. - Support addition with another `CustomObject`, where addition means summing the integers and concatenating the strings. - Release all allocated memory (both the integer and string) when the object is destroyed. # Expected Input and Output - **Initialization**: - Input: `CustomObject(7, \\"example\\")` - Output: A `CustomObject` instance with the integer `7` and the string `\\"example\\"`. - **Addition Support**: - Input: `CustomObject(2, \\"test\\") + CustomObject(3, \\"ing\\")` - Output: `CustomObject` instance with the integer `5` and the string `\\"testing\\"`. - **Memory Management**: - Memory should be released correctly upon object destruction to prevent memory leaks. # Constraints - You are allowed to use the standard libraries and any facilities documented in Python for defining and managing custom object types. # Instructions - Define the `CustomObject` type using Python\'s C API (indicated in the provided documentation). - Implement the type’s functions including initializer, destructor, and addition operation. - Write tests to demonstrate the functionality and correctness of your `CustomObject`. # Performance Requirements - Ensure the memory management is efficient and there are no memory leaks. - The addition operation should be performed in constant time, O(1). **Note**: You should be familiar with Python\'s C API and how to integrate C extensions with Python. This task requires advanced understanding and is not suitable for beginners unfamiliar with low-level programming in Python.","solution":"class CustomObject: def __init__(self, integer, string): self.integer = integer self.string = string def __add__(self, other): if not isinstance(other, CustomObject): return NotImplemented new_integer = self.integer + other.integer new_string = self.string + other.string return CustomObject(new_integer, new_string) def __del__(self): print(f\\"Releasing memory for: integer={self.integer}, string={self.string}\\") del self.integer del self.string def __repr__(self): return f\\"CustomObject(integer={self.integer}, string={self.string})\\""},{"question":"# Coding Assessment: Implementing and Utilizing Restricted Boltzmann Machines in scikit-learn Objective: The purpose of this task is to assess your understanding of Restricted Boltzmann Machines (RBMs) using the `BernoulliRBM` class from scikit-learn. You will write a function to train an RBM on a given dataset and evaluate its feature extraction capabilities by using it for classification. Problem Statement: You are given a dataset where the input features are binary (or probabilities between 0 and 1). Your task is to implement a function `train_and_evaluate_rbm` that: 1. Trains an RBM on the given dataset. 2. Uses the learned features from the RBM to train a logistic regression classifier. 3. Evaluates the classifier using cross-validation and returns the mean accuracy. Function Signature: ```python import numpy as np from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score def train_and_evaluate_rbm(data: np.ndarray, targets: np.ndarray, n_components: int, learning_rate: float, n_iter: int, batch_size: int, cv_folds: int) -> float: Trains an RBM on the data and evaluates its feature extraction capability. Parameters: data (np.ndarray): The input data matrix where rows are samples and columns are features. Should be binary or values between 0 and 1. targets (np.ndarray): The target class labels. n_components (int): Number of hidden units (components) in the RBM. learning_rate (float): The learning rate for training RBM. n_iter (int): Number of iterations (epochs) for training RBM. batch_size (int): The size of mini-batches for training RBM. cv_folds (int): Number of cross-validation folds. Returns: float: The mean accuracy of the logistic regression classifier after cross-validation. pass ``` Requirements: 1. **Train the RBM:** Use the `BernoulliRBM` class from scikit-learn to train on the provided `data`. 2. **Create a pipeline:** Construct a pipeline that first uses the RBM for feature extraction and then applies a logistic regression classifier. 3. **Cross-validation:** Use cross-validation (`cross_val_score`) with the specified number of folds (`cv_folds`) to evaluate the logistic regression classifier and return the mean accuracy. 4. **Parameters to configure:** - `n_components`: Number of hidden units in the RBM (e.g., 256). - `learning_rate`: Learning rate for the RBM (e.g., 0.01). - `n_iter`: Number of iterations for the RBM training (e.g., 10). - `batch_size`: Size of mini-batches for the RBM training (e.g., 10). - `cv_folds`: Number of cross-validation folds (e.g., 5). Example Usage: ```python data = np.random.rand(100, 64) # Example data with 100 samples, 64 features data = (data > 0.5).astype(float) # Binarize the data for RBM targets = np.random.randint(0, 2, 100) # Example binary target classes accuracy = train_and_evaluate_rbm(data, targets, n_components=256, learning_rate=0.01, n_iter=10, batch_size=10, cv_folds=5) print(f\\"Mean cross-validation accuracy: {accuracy}\\") ``` Constraints: - The `data` input matrix will have rows as samples and columns as features. - The `data` values will be binary or float values between 0 and 1. - The `targets` array will have binary class labels (0 or 1). Notes: - Ensure you import all necessary libraries from scikit-learn for implementing the function. - The `BernoulliRBM` class provides functionality to specify the number of components, learning rate, and number of iterations. Make sure these parameters are used correctly in your implementation. - Cross-validation should be implemented using `cross_val_score` to ensure robustness in the accuracy measure.","solution":"import numpy as np from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score def train_and_evaluate_rbm(data: np.ndarray, targets: np.ndarray, n_components: int, learning_rate: float, n_iter: int, batch_size: int, cv_folds: int) -> float: Trains an RBM on the data and evaluates its feature extraction capability. Parameters: data (np.ndarray): The input data matrix where rows are samples and columns are features. Should be binary or values between 0 and 1. targets (np.ndarray): The target class labels. n_components (int): Number of hidden units (components) in the RBM. learning_rate (float): The learning rate for training RBM. n_iter (int): Number of iterations (epochs) for training RBM. batch_size (int): The size of mini-batches for training RBM. cv_folds (int): Number of cross-validation folds. Returns: float: The mean accuracy of the logistic regression classifier after cross-validation. # Configure the RBM rbm = BernoulliRBM(n_components=n_components, learning_rate=learning_rate, n_iter=n_iter, batch_size=batch_size) # Configure the logistic regression model logistic = LogisticRegression(solver=\'lbfgs\', max_iter=1000) # Create a pipeline with the RBM and logistic regression classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Evaluate the pipeline using cross-validation cv_scores = cross_val_score(classifier, data, targets, cv=cv_folds, scoring=\'accuracy\') # Return the mean accuracy from the cross-validation mean_accuracy = np.mean(cv_scores) return mean_accuracy"},{"question":"# Batch Audio File Analysis using `sndhdr` You are tasked with writing a Python function to analyze a batch of audio files using the `sndhdr` module. The function should take a list of filenames as input, determine the type of each audio file, and return a summary containing the count of each file type and their average attributes. Function Signature ```python def analyze_audio_files(file_list: list) -> dict: ``` Input - `file_list` (list): A list of strings, where each string is the filename of an audio file. Output - A dictionary with the following structure: ```python { \'filetype_summary\': { \'aiff\': count, # int \'wav\': count, # int ... }, \'average_attributes\': { \'framerate\': average_framerate, # float \'nchannels\': average_nchannels, # float \'nframes\': average_nframes, # float \'sampwidth\': average_sampwidth, # float } } ``` - `filetype_summary` is a dictionary where keys are file types (as strings) and values are the counts of each file type (as integers). - `average_attributes` is a dictionary containing the average values (as floats) for the attributes `framerate`, `nchannels`, `nframes`, and `sampwidth` across all analyzed files. If an attribute value is unavailable (e.g., `0` or `-1`), it should be excluded from the average calculation for that attribute. Constraints - All filenames in `file_list` are valid and accessible from the current directory. - You can assume that the list contains at least one filename. Example ```python file_list = [\'file1.wav\', \'file2.aiff\', \'file3.wav\'] result = analyze_audio_files(file_list) print(result) ``` Output: ```python { \'filetype_summary\': { \'wav\': 2, \'aiff\': 1 }, \'average_attributes\': { \'framerate\': 44100.0, \'nchannels\': 2.0, \'nframes\': 100000.0, \'sampwidth\': 16.0 } } ``` # Additional Notes - Use the `sndhdr.what` function to determine the type and attributes of each audio file. - Handle cases where `sndhdr.what` returns `None` gracefully by skipping such files in your calculations. - Ensure you carefully handle the averaging of attributes to exclude invalid or unavailable values. Good luck, and happy coding!","solution":"import sndhdr from collections import defaultdict def analyze_audio_files(file_list): filetype_counts = defaultdict(int) attributes_sums = defaultdict(float) attributes_counts = defaultdict(int) for file in file_list: info = sndhdr.what(file) if info is not None: filetype_counts[info.filetype] += 1 if info.framerate > 0: attributes_sums[\'framerate\'] += info.framerate attributes_counts[\'framerate\'] += 1 if info.nchannels > 0: attributes_sums[\'nchannels\'] += info.nchannels attributes_counts[\'nchannels\'] += 1 if info.nframes > 0: attributes_sums[\'nframes\'] += info.nframes attributes_counts[\'nframes\'] += 1 if info.sampwidth > 0: attributes_sums[\'sampwidth\'] += info.sampwidth attributes_counts[\'sampwidth\'] += 1 average_attributes = {} for attr, total in attributes_sums.items(): if attributes_counts[attr] > 0: average_attributes[attr] = total / attributes_counts[attr] else: average_attributes[attr] = None return { \'filetype_summary\': dict(filetype_counts), \'average_attributes\': average_attributes }"},{"question":"**Problem Statement: Building a Secure CGI Script for User Registration** You are tasked with creating a CGI script for a user registration form. The form is expected to gather the following details from the users: 1. Username (must be unique) 2. Email 3. Password 4. Profile Picture (Optional, File Upload) Your script should: 1. **Handle Form Data:** Use the `cgi.FieldStorage` class to handle form data submissions. Verify that a username and email are provided; return an error if they are missing. 2. **Security:** Ensure that the username contains only alphanumeric characters and is unique. Use the appropriate methods to avoid any execution of arbitrary shell commands. 3. **File Handling:** If a profile picture is uploaded, save the file to a directory named `uploads/` and ensure the file is not empty. 4. **Return Output:** Generate an HTML response indicating whether the registration was successful or if there were errors. Errors must be specific (e.g., username already taken, invalid characters in username, etc.). 5. **Debugging and Logging:** Utilize cgitb for error handling during development. **Input:** The input to your script will be form submission data via POST. **Output:** Your script should output HTML. On successful registration, it should display a success message. On failure, it should return an appropriate error message. **Performance Considerations:** Ensure that your script performs efficiently even if the form data includes large files and multiple fields. **Constraints:** 1. The username should be between 3 and 20 characters long and contain only alphanumeric characters. 2. Handle the large file uploads efficiently to avoid memory overflow. **Example:** Here is an example HTML form that would send data to your CGI script: ```html <!DOCTYPE html> <html> <body> <h2>Registration Form</h2> <form action=\\"/cgi-bin/register.py\\" method=\\"post\\" enctype=\\"multipart/form-data\\"> Username: <input type=\\"text\\" name=\\"username\\"><br> Email: <input type=\\"text\\" name=\\"email\\"><br> Password: <input type=\\"password\\" name=\\"password\\"><br> Profile Picture: <input type=\\"file\\" name=\\"profile_pic\\"><br> <input type=\\"submit\\"> </form> </body> </html> ``` **Tips:** - Use the `cgi.FieldStorage` class to parse form data. - Use Python’s built-in functions and string methods to validate the username. - Handle file inputs carefully and test for empty file uploads. - Utilize the `cgitb` module to handle errors gracefully and log them during development. Implement the required CGI script adhering to the specifications and constraints provided above.","solution":"#!/usr/bin/env python3 import cgi import cgitb import re import os cgitb.enable() # This line helps with debugging. UPLOAD_DIR = \\"uploads/\\" def validate_username(username): # Ensure the username is alphanumeric and length is between 3 and 20 characters. return re.match(r\'^[a-zA-Z0-9]{3,20}\', username) def is_username_unique(username, datafile=\\"usernames.txt\\"): # Check if the username is unique by reading from a datafile. if os.path.exists(datafile): with open(datafile, \'r\') as file: if username in file.read().splitlines(): return False return True def save_username(username, datafile=\\"usernames.txt\\"): # Save the username to the datafile. with open(datafile, \'a\') as file: file.write(username + \\"n\\") def handle_form_submission(): form = cgi.FieldStorage() username = form.getvalue(\\"username\\") email = form.getvalue(\\"email\\") password = form.getvalue(\\"password\\") # In real applications, ensure to hash passwords. profile_pic = form[\\"profile_pic\\"] if not username or not email: return \\"Error: Username and Email are required.\\" if not validate_username(username): return \\"Error: Invalid Username. It must be alphanumeric and between 3 to 20 characters.\\" if not is_username_unique(username): return \\"Error: Username already taken.\\" if profile_pic.filename: profile_pic_file = os.path.join(UPLOAD_DIR, os.path.basename(profile_pic.filename)) if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR) with open(profile_pic_file, \'wb\') as f: f.write(profile_pic.file.read()) save_username(username) return \\"Registration successful.\\" def generate_html_response(message): print(\\"Content-Type: text/html\\") print() print(f\\"<html><body><h1>{message}</h1></body></html>\\") def main(): message = handle_form_submission() generate_html_response(message) if __name__ == \\"__main__\\": main()"},{"question":"# **Coding Assessment Question** Objective: Write a function that dynamically imports a module given its name and checks if it contains a specific attribute. If the attribute exists, return its value; otherwise, return a default value provided by the user. Function Signature: ```python def get_module_attribute(module_name: str, attribute_name: str, default_value: any) -> any: Dynamically imports a module and returns the value of a specific attribute if it exists, otherwise returns a default value. Parameters: - module_name (str): The name of the module to import (e.g., \'math\') - attribute_name (str): The name of the attribute to check for within the module (e.g., \'pow\') - default_value (any): The value to return if the attribute does not exist within the module Returns: - any: The value of the attribute if it exists, otherwise the default value pass ``` Constraints: - The function should leverage `importlib` functionalities for importing the module and checking the attribute. - The module may or may not already be imported in the current environment. - The attribute to check for can be any valid attribute name (method, function, variable, etc.). - Performance considerations should be taken into account when dealing with multiple imports. Example Usage: ```python # Example 1 print(get_module_attribute(\'math\', \'pow\', \'Attribute not found\')) # Should return the \'pow\' function from the math module # Example 2 print(get_module_attribute(\'math\', \'nonexistent_attr\', \'Attribute not found\')) # Should return \'Attribute not found\' ``` Implementation Notes: 1. Use `importlib.import_module()` to dynamically import the module. 2. Check if the attribute exists in the imported module using `hasattr()`. 3. Return the attribute value if it exists, else return the default value. Evaluation Criteria: - Proper usage of the `importlib` package functions. - Correct handling of dynamic imports and attribute checking. - Clean and efficient code. - Proper handling of module cache invalidation if necessary.","solution":"import importlib def get_module_attribute(module_name: str, attribute_name: str, default_value: any) -> any: Dynamically imports a module and returns the value of a specific attribute if it exists, otherwise returns a default value. Parameters: - module_name (str): The name of the module to import (e.g., \'math\') - attribute_name (str): The name of the attribute to check for within the module (e.g., \'pow\') - default_value (any): The value to return if the attribute does not exist within the module Returns: - any: The value of the attribute if it exists, otherwise the default value try: module = importlib.import_module(module_name) return getattr(module, attribute_name, default_value) except ImportError: return default_value"},{"question":"**Coding Assessment Question** **Objective:** Assess the student\'s understanding of creating and managing source distributions in Python using `setuptools` and the `sdist` command. **Problem Statement:** You are provided with a Python project that needs to be packaged for distribution. Your task is to write functions to automate the creation of the setup script, the manifest template, and the generation of source distributions. **Requirements:** 1. Implement a function `create_setup_script(package_name, version, description, author, author_email, packages)` that creates a basic `setup.py` script given the package details. - **Input:** - `package_name` (str): Name of the package. - `version` (str): Version of the package. - `description` (str): Description of the package. - `author` (str): Author\'s name. - `author_email` (str): Author\'s email. - `packages` (list): A list of package directories to include in the distribution. - **Output:** None (Creates a file named `setup.py` in the current directory). 2. Implement a function `create_manifest_template(include_patterns, exclude_patterns)` that creates a `MANIFEST.in` file based on inclusion and exclusion patterns. - **Input:** - `include_patterns` (list): A list of patterns specifying files to include. - `exclude_patterns` (list): A list of patterns specifying files to exclude. - **Output:** None (Creates a file named `MANIFEST.in` in the current directory). 3. Implement a function `generate_source_distribution(formats)` that runs the `sdist` command to generate the source distribution in the specified formats. - **Input:** - `formats` (list): A list of formats to generate (e.g., `[\\"gztar\\", \\"zip\\"]`). - **Output:** None (Creates the source distribution files). 4. Ensure that the `setup.py` script includes the necessary options to include all `.py` files in the `src` directory in the distribution. 5. Test your functions by creating a sample project directory structure, generating the setup script, manifest template, and the source distribution. **Constraints:** - Assume a UNIX-like environment for file paths. - You must handle cases where the `MANIFEST.in` or `setup.py` already exists by overwriting them. **Example:** ```python # Example usage create_setup_script( package_name=\\"sample_package\\", version=\\"0.1\\", description=\\"A sample Python package\\", author=\\"John Doe\\", author_email=\\"john@example.com\\", packages=[\\"src\\"] ) create_manifest_template( include_patterns=[\\"*.py\\"], exclude_patterns=[\\"*.pyc\\", \\"__pycache__\\"] ) generate_source_distribution([\\"gztar\\", \\"zip\\"]) # Expected structure # - setup.py # - MANIFEST.in # - src/ # - __init__.py # - module1.py # - module2.py # Expected output # Creates the source distribution files sample_package-0.1.tar.gz and sample_package-0.1.zip ``` **Notes:** - Make sure to import any necessary modules and handle exceptions where appropriate. - You may provide dummy Python files in the `src` directory to test the functionality.","solution":"import os import subprocess def create_setup_script(package_name, version, description, author, author_email, packages): setup_content = f from setuptools import setup, find_packages setup( name=\\"{package_name}\\", version=\\"{version}\\", description=\\"{description}\\", author=\\"{author}\\", author_email=\\"{author_email}\\", packages={packages}, include_package_data=True, ) with open(\\"setup.py\\", \\"w\\") as f: f.write(setup_content) def create_manifest_template(include_patterns, exclude_patterns): manifest_content = \\"\\" for pattern in include_patterns: manifest_content += f\\"include {pattern}n\\" for pattern in exclude_patterns: manifest_content += f\\"exclude {pattern}n\\" with open(\\"MANIFEST.in\\", \\"w\\") as f: f.write(manifest_content) def generate_source_distribution(formats): formats_str = \\",\\".join(formats) subprocess.run([\\"python\\", \\"setup.py\\", \\"sdist\\", f\\"--formats={formats_str}\\"], check=True)"},{"question":"# Task You are provided with a dataset containing animal observations in a wildlife reserve. Each observation records the types of animals seen and their counts. The dataset is as follows: ```python data = [ {\\"id\\": 1, \\"species_seen\\": [\\"deer\\", \\"rabbit\\", \\"fox\\"]}, {\\"id\\": 2, \\"species_seen\\": [\\"deer\\", \\"rabbit\\"]}, {\\"id\\": 3, \\"species_seen\\": [\\"rabbit\\", \\"bear\\", \\"deer\\"]}, {\\"id\\": 4, \\"species_seen\\": [\\"fox\\"]}, {\\"id\\": 5, \\"species_seen\\": [\\"deer\\", \\"rabbit\\", \\"fox\\", \\"bear\\"]}, ] ``` Your task is to implement a preprocessing function `encode_species(data)` that performs the following: 1. **Label Encoding**: First, encode each unique species with a unique integer using `LabelEncoder`. 2. **MultiLabel Binarization**: Transform the list of species seen during each observation into a binary indicator matrix using `MultiLabelBinarizer`. # Function Signature ```python def encode_species(data: list) -> dict: # Your code here ``` # Expected Output The function should return a dictionary with the following structure: - `encoded_species`: A dictionary where keys are species names and values are their corresponding integer encodings. - `binary_matrix`: A 2D binary indicator matrix where each row corresponds to an observation, and each column corresponds to a species. # Example ```python data = [ {\\"id\\": 1, \\"species_seen\\": [\\"deer\\", \\"rabbit\\", \\"fox\\"]}, {\\"id\\": 2, \\"species_seen\\": [\\"deer\\", \\"rabbit\\"]}, {\\"id\\": 3, \\"species_seen\\": [\\"rabbit\\", \\"bear\\", \\"deer\\"]}, {\\"id\\": 4, \\"species_seen\\": [\\"fox\\"]}, {\\"id\\": 5, \\"species_seen\\": [\\"deer\\", \\"rabbit\\", \\"fox\\", \\"bear\\"]}, ] result = encode_species(data) print(result) ``` Example output: ```python { \\"encoded_species\\": {\\"bear\\": 0, \\"deer\\": 1, \\"fox\\": 2, \\"rabbit\\": 3}, \\"binary_matrix\\": [ [0, 1, 1, 1], [0, 1, 0, 1], [1, 1, 0, 1], [0, 0, 1, 0], [1, 1, 1, 1], ] } ``` # Constraints - All species names are strings. - The dataset may vary but will always have the same structure as shown above. You can leverage the scikit-learn tools `LabelEncoder` and `MultiLabelBinarizer` as discussed: - [LabelEncoder Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) - [MultiLabelBinarizer Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html)","solution":"from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer def encode_species(data): # Flatten the list of all species seen in the dataset species_list = [species for observation in data for species in observation[\\"species_seen\\"]] unique_species = list(set(species_list)) # Encode the unique species using LabelEncoder label_encoder = LabelEncoder() label_encoder.fit(unique_species) encoded_species = {species: idx for idx, species in enumerate(label_encoder.classes_)} # Transform the list of species seen into a binary indicator matrix using MultiLabelBinarizer mlb = MultiLabelBinarizer(classes=label_encoder.classes_) species_seen_list = [observation[\\"species_seen\\"] for observation in data] binary_matrix = mlb.fit_transform(species_seen_list) return { \\"encoded_species\\": encoded_species, \\"binary_matrix\\": binary_matrix.tolist() }"},{"question":"**Objective**: Implement a custom Python type that mimics certain behaviors of complex Python objects using the provided PyTypeObject structure and related functions. # Problem Description Your task is to create a custom Python type called `PyComplexList` which behaves similarly to a list but with additional custom functionalities. This type should support: 1. Standard list functionalities like indexing, appending, and iteration. 2. Custom methods that: - `append_multiple`: Appends multiple items to the list. - `double_values`: Doubles each numeric value in the list. - Custom string representation which includes the length of the list. # Requirements 1. **Initialization and Memory Management** - Create a C structure representing `PyComplexList`. - Properly implement memory allocation and deallocation functions. 2. **Method Implementations** - Implement the `append_multiple` and `double_values` methods. - Implement `__repr__` to provide a string representation showing elements and the length of the list. 3. **Sequence Behaviors** - Implement sequence behaviors to support indexing and iteration over elements. # Input The class should interact using normal Python object techniques, but underlying functionality should be implemented using C API provided. # Example Usage ```python # Create an instance c_list = PyComplexList() # Append items c_list.append(10) c_list.append(20) print(c_list) # Output: PyComplexList([10, 20], length=2) # Append multiple items c_list.append_multiple([30, 40]) print(c_list) # Output: PyComplexList([10, 20, 30, 40], length=4) # Double values c_list.double_values() print(c_list) # Output: PyComplexList([20, 40, 60, 80], length=4) # Indexing and iteration print(c_list[2]) # Output: 60 for item in c_list: print(item) ``` # Constraints - Implement the type using the `PyTypeObject` structure and associated methods from the provided documentation. - The functionalities should handle different data types supported by Python lists. # Performance Requirements The implementation should not degrade significantly with typical list operations, maintaining efficiency similar to native list operations for supported features. # Bonus For extra credit, further enhance `PyComplexList` by: 1. Supporting slicing. 2. Adding support for mathematical operations (e.g., add, multiply with another `PyComplexList`). # Important Notes - Ensure to handle memory management efficiently to avoid leaks. - Properly handle exceptions and errors in the methods to align with Python\'s error handling practices.","solution":"class PyComplexList: def __init__(self): self._data = [] def append(self, item): self._data.append(item) def append_multiple(self, items): if not isinstance(items, list): raise TypeError(\\"append_multiple expects a list of items\\") self._data.extend(items) def double_values(self): for i in range(len(self._data)): if isinstance(self._data[i], (int, float)): self._data[i] *= 2 else: raise TypeError(\\"double_values supports only numeric values\\") def __repr__(self): return f\\"PyComplexList({self._data}, length={len(self._data)})\\" def __getitem__(self, index): return self._data[index] def __iter__(self): return iter(self._data)"},{"question":"**Coding Assessment Question:** **Objective:** Write a Python function that extracts and formats the stack trace of the most recent exception in a user-friendly format. Your function should also capture and include local variables of each frame in the traceback, where applicable. **Function Signature:** ```python def format_stack_trace_with_locals() -> str: pass ``` **Detailed Requirements:** 1. The function should not take any input parameters. 2. The function should capture the most recent exception\'s stack trace, including the local variables of each frame. 3. The stack trace should be formatted similar to the default Python interpreter\'s traceback format but with the addition of local variables in each frame. 4. The output of the function should be a string containing the formatted stack trace. 5. Make sure to handle cases where there are no local variables gracefully (i.e., do not raise any additional exceptions). **Example Usage:** ```python def cause_error(): a = 10 b = 0 return a / b def main(): try: cause_error() except ZeroDivisionError: print(format_stack_trace_with_locals()) main() ``` Expected output (formatted for readability): ``` Traceback (most recent call last): File \\"example.py\\", line 16, in <module> main() File \\"example.py\\", line 14, in main cause_error() File \\"example.py\\", line 9, in cause_error return a / b ZeroDivisionError: division by zero Locals by frame: - cause_error: {\'a\': 10, \'b\': 0} - main: {} - <module>: {} ``` **Constraints:** - Ensure the solution works for different types of exceptions and various numbers of frames in the stack. - Keep the performance optimal for deep stack traces containing multiple frames. **Hints:** - Use the `traceback` module\'s `TracebackException` and `StackSummary` classes for capturing and formatting the stack trace. - To capture local variables, you might consider setting the `capture_locals` parameter appropriately in the `StackSummary.extract` method.","solution":"import traceback import sys def format_stack_trace_with_locals() -> str: Captures and formats the most recent exception stack trace, including local variables of each frame, into a user-friendly string. exc_type, exc_value, exc_tb = sys.exc_info() if not exc_type: return \\"No exception to capture.\\" tb_summary = traceback.extract_tb(exc_tb) formatted_trace = \\"Traceback (most recent call last):n\\" for frame in tb_summary: formatted_trace += f\' File \\"{frame.filename}\\", line {frame.lineno}, in {frame.name}n\' if frame.line: formatted_trace += f\' {frame.line.strip()}n\' formatted_trace += f\\"{exc_type.__name__}: {exc_value}n\\" # Rewind traceback to gather local variables exc_tb = exc_tb.tb_next formatted_trace += \\"Locals by frame:n\\" while exc_tb: frame = exc_tb.tb_frame local_vars = frame.f_locals formatted_trace += f\\"- {frame.f_code.co_name}: {local_vars}n\\" exc_tb = exc_tb.tb_next return formatted_trace"},{"question":"Objective: The goal is to assess your ability to use the `unittest.mock` module for creating and using mock objects and making assertions about their interactions. Problem Statement: You are developing a data processing pipeline for a software system, and you need to write unit tests for a function that processes and saves data using external services. To isolate the component under test, you will use the `unittest.mock` module to mock these external service calls. Implement the `process_and_save_data` function and two unit tests using mocks to verify the functionality. Function Description: ```python def process_and_save_data(data): This function processes the input data (a dictionary), performs some calculations, and then saves the processed data using external services. Args: - data (dict): A dictionary containing raw data. Returns: - bool: True if data is processed and saved successfully, False otherwise. ``` The function follows these steps: 1. It reads a field `value` from the input dictionary `data`. 2. It performs a calculation: `processed_value = value * 10`. 3. It calls an external service function `save_to_database(processed_value)` to save the data. 4. It logs the result using an external logging service `log(\\"Data saved successfully\\")` if the data is saved correctly. 5. If any exception is raised during processing or saving, it logs an error `log(\\"Error saving data\\")` and returns `False`. Required unit tests: 1. **Test successful saving:** - Mock `save_to_database` to return `True`. - Mock `log` and assert it is called with \\"Data saved successfully\\". - Verify the function returns `True`. 2. **Test failure in saving:** - Mock `save_to_database` to raise an `Exception`. - Mock `log` and assert it is called with \\"Error saving data\\". - Verify the function returns `False`. Constraints: - You are required to use the `unittest.mock` module. - You must utilize `patch()` for mocking. - Your function and tests should handle exceptions gracefully. Example Usage: ```python def save_to_database(value): # Imagine this is an external service that saves data to a database pass def log(message): # Imagine this is an external service for logging messages pass # Implement the function process_and_save_data based on the specification def process_and_save_data(data): try: value = data.get(\'value\', 0) processed_value = value * 10 save_to_database(processed_value) log(\\"Data saved successfully\\") return True except Exception: log(\\"Error saving data\\") return False # Unit tests from unittest import TestCase, main from unittest.mock import patch class TestDataProcessing(TestCase): @patch(\'__main__.log\') @patch(\'__main__.save_to_database\') def test_successful_saving(self, mock_save, mock_log): mock_save.return_value = True result = process_and_save_data({\'value\': 5}) mock_save.assert_called_once_with(50) mock_log.assert_called_once_with(\\"Data saved successfully\\") self.assertTrue(result) @patch(\'__main__.log\') @patch(\'__main__.save_to_database\') def test_failure_saving(self, mock_save, mock_log): mock_save.side_effect = Exception(\\"Database error\\") result = process_and_save_data({\'value\': 5}) mock_save.assert_called_once_with(50) mock_log.assert_called_once_with(\\"Error saving data\\") self.assertFalse(result) if __name__ == \'__main__\': main() ``` Deliverables: - Implement the `process_and_save_data` function. - Implement the `TestDataProcessing` class with the specified unit tests.","solution":"def save_to_database(value): # Imagine this is an external service that saves data to a database pass def log(message): # Imagine this is an external service for logging messages pass def process_and_save_data(data): This function processes the input data (a dictionary), performs some calculations, and then saves the processed data using external services. Args: - data (dict): A dictionary containing raw data. Returns: - bool: True if data is processed and saved successfully, False otherwise. try: value = data.get(\'value\', 0) processed_value = value * 10 save_to_database(processed_value) log(\\"Data saved successfully\\") return True except Exception: log(\\"Error saving data\\") return False"},{"question":"# Question: Custom LZMA File Compressor/Decompressor Your task is to write a Python program that demonstrates both the compression and decompression of files using the LZMA module with custom filters. Specifically, you must: 1. Create a function `compress_file(input_file: str, output_file: str, filters: List[Dict]) -> None` that: - Takes an input file path (`input_file`) and reads its contents. - Compresses the read contents using LZMA compression with the provided custom filter chain (`filters`). - Writes the compressed data to the specified output file (`output_file`). 2. Create a function `decompress_file(input_file: str, output_file: str) -> None` that: - Takes a compressed input file path (`input_file`) and reads its contents. - Decompresses the read contents using LZMA decompression. - Writes the decompressed data to the specified output file (`output_file`). Custom Filter Chain A filter chain should be a list of dictionaries where each dictionary contains the ID and options for a single filter. You can use the following filter chain as an example: ```python filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 6}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 6 | lzma.PRESET_EXTREME}, ] ``` # Constraints - You must handle the necessary imports. - The input file provided to `compress_file` will be a plain text file. - The decompressed file produced by `decompress_file` should exactly match the original input file used in `compress_file`. # Example Usage Given the file `example.txt` with contents: ``` Lorem ipsum dolor sit amet, consectetur adipiscing elit. ``` 1. Compress the file: ```python filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 6}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 6 | lzma.PRESET_EXTREME}, ] compress_file(\'example.txt\', \'example_compressed.xz\', filters) ``` 2. Decompress the file: ```python decompress_file(\'example_compressed.xz\', \'example_decompressed.txt\') ``` At the end of these operations, `example_decompressed.txt` should contain: ``` Lorem ipsum dolor sit amet, consectetur adipiscing elit. ``` Your implementation should ensure: - You handle file operations correctly (opening, reading, writing, and closing files). - You handle the compression and decompression using the LZMA module as specified. - Any errors during compression or decompression should be properly caught and printed. # Function Signatures: ```python def compress_file(input_file: str, output_file: str, filters: List[Dict]) -> None: pass def decompress_file(input_file: str, output_file: str) -> None: pass ```","solution":"import lzma from typing import List, Dict def compress_file(input_file: str, output_file: str, filters: List[Dict]) -> None: try: # Read the content of the input file with open(input_file, \'rb\') as file: data = file.read() # Compress the data using LZMA with custom filters compressor = lzma.LZMACompressor(filters=filters) compressed_data = compressor.compress(data) + compressor.flush() # Write the compressed data to the output file with open(output_file, \'wb\') as file: file.write(compressed_data) except Exception as e: print(f\\"Error during compression: {e}\\") def decompress_file(input_file: str, output_file: str) -> None: try: # Read the compressed content of the input file with open(input_file, \'rb\') as file: compressed_data = file.read() # Decompress the data using LZMA decompressor = lzma.LZMADecompressor() decompressed_data = decompressor.decompress(compressed_data) # Write the decompressed data to the output file with open(output_file, \'wb\') as file: file.write(decompressed_data) except Exception as e: print(f\\"Error during decompression: {e}\\")"},{"question":"**Objective:** Implement a Python function that uses concurrent processing to calculate a large number of prime numbers efficiently and correctly handles errors and timeouts. **Question:** Write a function `calculate_primes(numbers, max_workers=4, timeout=None)` that takes a list of integers and returns a dictionary where the keys are the numbers and the values are boolean indicating whether each number is prime. The function should do this in parallel using `ProcessPoolExecutor`, and it should ensure that the process does not hang indefinitely. **Function Signature:** ```python def calculate_primes(numbers: List[int], max_workers: int = 4, timeout: float = None) -> Dict[int, bool]: ``` **Parameters:** - `numbers` (List[int]): A list of integers to check for primality. - `max_workers` (int): Maximum number of worker processes to use (default is 4). - `timeout` (float): Maximum time in seconds to wait for all tasks to complete (default is None, meaning no timeout). **Returns:** - Dict[int, bool]: A dictionary where each key is an integer from the input list and the value is True if the number is prime, otherwise False. **Constraints:** - `numbers` list can be very large, possibly containing up to 100,000 integers. - Ensure that your function gracefully handles situations where the computation exceeds the `timeout` by using appropriate exception handling. - The function should also handle and log any exceptions raised during the computation without crashing. **Example:** ```python from typing import List, Dict def is_prime(n: int) -> bool: if n < 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(n ** 0.5) + 1 for i in range(3, sqrt_n, 2): if n % i == 0: return False return True def calculate_primes(numbers: List[int], max_workers: int = 4, timeout: float = None) -> Dict[int, bool]: import concurrent.futures import logging results = {} def log_exception(exc): logging.error(f\\"Exception: {exc}\\") with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor: future_to_number = {executor.submit(is_prime, num): num for num in numbers} try: for future in concurrent.futures.as_completed(future_to_number, timeout=timeout): number = future_to_number[future] try: results[number] = future.result() except Exception as exc: log_exception(exc) results[number] = None except concurrent.futures.TimeoutError: logging.error(\\"Timeout exceeded\\") for future in future_to_number: results[future_to_number[future]] = None return results # Example usage: numbers = [112272535095293, 2, 3, 4, 5, 112272535095293, 115280095190773] prime_status = calculate_primes(numbers, max_workers=4, timeout=10) print(prime_status) ``` **Explanation:** The `calculate_primes` function uses a `ProcessPoolExecutor` to parallelize the computation of prime checking for a list of integers. It employs the `as_completed` method to gather results as tasks complete and handle potential timeouts or other exceptions gracefully.","solution":"from typing import List, Dict import concurrent.futures import logging def is_prime(n: int) -> bool: Determines if a number is prime. if n < 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(n ** 0.5) + 1 for i in range(3, sqrt_n, 2): if n % i == 0: return False return True def calculate_primes(numbers: List[int], max_workers: int = 4, timeout: float = None) -> Dict[int, bool]: Calculate the primality of a list of numbers using concurrent processing. results = {} def log_exception(exc): logging.error(f\\"Exception: {exc}\\") with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor: future_to_number = {executor.submit(is_prime, num): num for num in numbers} try: for future in concurrent.futures.as_completed(future_to_number, timeout=timeout): number = future_to_number[future] try: results[number] = future.result() except Exception as exc: log_exception(exc) results[number] = False except concurrent.futures.TimeoutError: logging.error(\\"Timeout exceeded\\") for future in future_to_number: results[future_to_number[future]] = False return results"},{"question":"Problem Statement You are given a dataset containing information about various articles from a digital library, represented by a high-dimensional feature set. Your task is to implement a function using scikit-learn that clusters these articles into meaningful groups and reduces the dimensionality of the data for visualization. # Input - `data`: A `pandas.DataFrame` consisting of `n` samples and `d` features. - Each row represents an article, and each column a feature. - `num_clusters`: An integer representing the number of clusters to form. # Output - A tuple containing the following elements: - A `numpy.ndarray` of shape `(n, 2)` representing the 2D projection of the high-dimensional data obtained using a dimensionality reduction technique (e.g., PCA, t-SNE). - A `numpy.ndarray` of shape `(n,)` representing the cluster labels assigned to each article. # Constraints 1. You must use KMeans for clustering. 2. For dimensionality reduction, you must use PCA or t-SNE. 3. The function should handle datasets with at least 1000 samples efficiently. # Function Signature ```python import pandas as pd import numpy as np from sklearn.cluster import KMeans from sklearn.decomposition import PCA from sklearn.manifold import TSNE def cluster_and_reduce(data: pd.DataFrame, num_clusters: int) -> (np.ndarray, np.ndarray): pass ``` # Example ```python import pandas as pd # Sample data data = pd.DataFrame({ \'feature1\': [0.1, 0.2, 0.8, 0.9], \'feature2\': [0.3, 0.4, 0.7, 0.6], \'feature3\': [1.0, 0.9, 0.4, 0.5] }) num_clusters = 2 # Expected output: (2D projection of the data, cluster labels) projection, labels = cluster_and_reduce(data, num_clusters) print(projection) # Output: A numpy array with shape (4, 2) print(labels) # Output: A numpy array with shape (4,) ``` # Notes - Ensure you standardize the data before applying PCA or t-SNE for better performance. - You should validate your clustering by plotting the 2D projection with different colors for different clusters (optional but recommended for visualization).","solution":"import pandas as pd import numpy as np from sklearn.cluster import KMeans from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler def cluster_and_reduce(data: pd.DataFrame, num_clusters: int) -> (np.ndarray, np.ndarray): Clusters the articles in the dataset and reduces the dimensionality for visualization. Parameters: - data: pd.DataFrame containing the dataset with n samples and d features - num_clusters: int representing the number of clusters to form Returns: - tuple: - A numpy.ndarray of shape (n, 2) representing the 2D projection of the data obtained using PCA. - A numpy.ndarray of shape (n,) representing the cluster labels assigned to each article. # Standardizing the features scaler = StandardScaler() data_normalized = scaler.fit_transform(data) # KMeans clustering kmeans = KMeans(n_clusters=num_clusters, random_state=42) labels = kmeans.fit_predict(data_normalized) # PCA for dimensionality reduction pca = PCA(n_components=2, random_state=42) projection = pca.fit_transform(data_normalized) return projection, labels"},{"question":"# Regex String Manipulation with Python\'s `re` module Objective Implement a function `extract_and_replace_emails` that extracts all email addresses from the given text and replaces the domain of each email with a specified new domain. Function Signature ```python def extract_and_replace_emails(text: str, new_domain: str) -> str: pass ``` Input - `text` (str): A string containing text with email addresses. - `new_domain` (str): A string representing the new domain to be used in the email addresses. Output - Returns a string with all email domains replaced by the specified new domain. Example 1. Input: ```python text = \\"Contact: john.doe@example.com, jane.smith@domain.org\\" new_domain = \\"newdomain.com\\" ``` Output: ```python \\"Contact: john.doe@newdomain.com, jane.smith@newdomain.com\\" ``` 2. Input: ```python text = \\"Please reach us at support@service.net and info@website.info\\" new_domain = \\"mycompany.com\\" ``` Output: ```python \\"Please reach us at support@mycompany.com and info@mycompany.com\\" ``` Constraints - All email addresses are in the format `username@domain`. - Email usernames contain only alphanumeric characters, dots (`.`), and underscores (`_`). - Email domains are alphanumeric and may contain dots (`.`). - The new domain does not contain any special characters except dots (`.`). Requirements 1. Use the `re` module to find all email addresses in the text. 2. Use capturing groups to isolate the username and domain parts of the email addresses. 3. Replace the domain of each email address with the new domain. 4. Ensure that the function is efficient and handles multiple email addresses correctly. Hints - Use `re.findall` to extract all email addresses. - Use `re.sub` with a replacement function to change the domain part of the emails. Notes - Ensure you handle edge cases like text without any emails and text with multiple occurrences of the same email address. Assessment The solution to this problem should showcase the student\'s ability to: 1. Construct regular expressions for finding and capturing specific patterns. 2. Utilize the `re` module\'s functions for searching and replacing text. 3. Write clean and efficient code with proper function usage.","solution":"import re def extract_and_replace_emails(text: str, new_domain: str) -> str: Extracts all email addresses from the given text and replaces the domain of each email with the specified new domain. # Define a regular expression pattern to find email addresses email_pattern = r\'([a-zA-Z0-9._%+-]+)@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\' # Define a function to replace the domain part of the email def replace_domain(match): username = match.group(1) return f\'{username}@{new_domain}\' # Replace the domain part of each email in the text updated_text = re.sub(email_pattern, replace_domain, text) return updated_text"},{"question":"# XML Parsing and Modification with `xml.etree.ElementTree` **Problem Statement:** You are given the following XML document as a string: ```xml <?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <review>An in-depth look at creating applications with XML.</review> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <review>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</review> </book> <!-- Additional books omitted for brevity --> </catalog> ``` Your task is to implement a function `update_books(xml_string: str) -> str` that performs the following actions: 1. Parses the given XML string to construct an XML tree. 2. Increases the price of each book by 10%. 3. Adds an attribute `currency=\\"USD\\"` to the `<price>` element of each book. 4. Adds a new child `<availability>` with the text `\\"In Stock\\"` to each `<book>` element. 5. Removes any `<review>` elements from the `<book>` elements. 6. Returns the modified XML document as a string with proper indentation. **Function Signature:** `def update_books(xml_string: str) -> str:` # Constraints: - You can assume the XML string provided to the function is well-formed. - The `price` element always contains a valid float number. - The function should return the updated XML string with proper formatting (including new lines and indentation) to make the XML human-readable. # Example: **Input:** ```python xml_data = \'\'\'<?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <review>An in-depth look at creating applications with XML.</review> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <review>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</review> </book> </catalog>\'\'\' print(update_books(xml_data)) ``` **Expected Output:** ```xml <?xml version=\\"1.0\\" ?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price currency=\\"USD\\">49.45</price> <publish_date>2000-10-01</publish_date> <availability>In Stock</availability> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price currency=\\"USD\\">6.55</price> <publish_date>2000-12-16</publish_date> <availability>In Stock</availability> </book> </catalog> ``` **Note:** - Ensure your solution uses the `xml.etree.ElementTree` package efficiently. - Perform necessary validation and error handling as needed.","solution":"import xml.etree.ElementTree as ET from xml.dom import minidom def update_books(xml_string: str) -> str: Function to update the provided XML string according to the specified requirements. :param xml_string: str, the input XML string :return: str, the modified XML string with proper indentation root = ET.fromstring(xml_string) for book in root.findall(\'book\'): # Increase price by 10% and add currency attribute price_element = book.find(\'price\') if price_element is not None: price_value = float(price_element.text) new_price_value = round(price_value * 1.1, 2) price_element.text = str(new_price_value) price_element.set(\'currency\', \'USD\') # Add availability element availability_element = ET.SubElement(book, \'availability\') availability_element.text = \'In Stock\' # Remove review element review_element = book.find(\'review\') if review_element is not None: book.remove(review_element) # Use minidom for proper string formatting xml_str = ET.tostring(root, \'utf-8\') parsed = minidom.parseString(xml_str) pretty_xml_str = parsed.toprettyxml(indent=\\" \\") return pretty_xml_str.strip() # Test case xml_data = \'\'\'<?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <review>An in-depth look at creating applications with XML.</review> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <review>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</review> </book> </catalog>\'\'\' print(update_books(xml_data))"},{"question":"# Advanced Python Logging System **Problem Statement**: You are tasked with creating a detailed logging system for a Python application that processes data from various sources. The logged information should be dynamic and configurable based on different environments: development, testing, and production. Your implementation should demonstrate the use of multiple logging levels, handlers, and formatters. **Requirements**: 1. **Logger Configuration**: - Create a logger named `my_app_logger`. - The logger should log messages at all levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`). 2. **Handlers**: - **Console handler**: This should handle `DEBUG` and higher level messages and print them to the console. - **File handler for warnings and above**: Log `WARNING` and higher level messages to a file named `app_warnings.log`. - **Rotating File handler for info and debug**: Log `DEBUG` and `INFO` messages to a file named `app_info.log`. The log file should rotate after reaching 1 MB in size, keeping a maximum of 5 backup files. 3. **Formatters**: - Console handler and File handler for warnings should include the timestamp, logger name, severity level, and the message. - The Rotating File handler should include only the severity level and the message, each on a new line. 4. **Dynamic Configuration**: - Allow changing the logging level dynamically through environment variables. For instance, an environment variable `APP_LOG_LEVEL` set to `DEBUG` should configure the logger to handle debug messages; if set to `ERROR`, it should handle only error and critical messages. **Constraints**: - Ensure that the log file names and rotating file handler parameters are configurable. - Must validate the environment variable value to ensure it matches one of the valid logging levels. Raise an exception for invalid values. **Input**: - Environment variable which dictates the log level. - Log messages that need to be logged by various components of the application. **Output**: - Log files (`app_warnings.log`, `app_info.log`) and console outputs according to the configured handlers and formatters. **Function Signature**: ```python import os import logging import logging.handlers def setup_logger(): # Implementation here # Example usage if __name__ == \\"__main__\\": setup_logger() logger = logging.getLogger(\'my_app_logger\') logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') ``` # Instructions: 1. Implement the `setup_logger` function according to the requirements outlined. 2. The example usage section will test your implementation by creating various log messages. Ensure that the messages are logged according to the dynamic configuration. 3. Validate input for environment variable `APP_LOG_LEVEL`. 4. Document your code and provide comments to explain the different parts of your implementation.","solution":"import os import logging import logging.handlers def setup_logger(): # Retrieve the log level from the environment variable log_level = os.getenv(\'APP_LOG_LEVEL\', \'INFO\').upper() # Validate the log level valid_levels = { \'DEBUG\': logging.DEBUG, \'INFO\': logging.INFO, \'WARNING\': logging.WARNING, \'ERROR\': logging.ERROR, \'CRITICAL\': logging.CRITICAL } if log_level not in valid_levels: raise ValueError(f\\"Invalid log level: {log_level}\\") # Define logger logger = logging.getLogger(\'my_app_logger\') logger.setLevel(valid_levels[log_level]) # Create a console handler for DEBUG and higher level messages console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) console_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(console_formatter) # Create a file handler for WARNING and higher level messages file_handler_warning = logging.FileHandler(\'app_warnings.log\') file_handler_warning.setLevel(logging.WARNING) file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') file_handler_warning.setFormatter(file_formatter) # Create a rotating file handler for INFO and DEBUG messages rotating_file_handler = logging.handlers.RotatingFileHandler(\'app_info.log\', maxBytes=1024*1024, backupCount=5) rotating_file_handler.setLevel(logging.DEBUG) rotating_file_formatter = logging.Formatter(\'%(levelname)sn%(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\') rotating_file_handler.setFormatter(rotating_file_formatter) # Adding handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler_warning) logger.addHandler(rotating_file_handler) return logger # Example usage if __name__ == \\"__main__\\": logger = setup_logger() logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\')"},{"question":"Coding Assessment Question # Objective You are tasked with writing a PyTorch program that performs a simple matrix multiplication but must also exhibit an understanding of CUDA environment variables to control the operational behavior of the program. # Problem Statement Write a function `cuda_matrix_multiplication(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor` that takes two 2D tensors `a` and `b`, performs matrix multiplication (if possible), and returns the result. If CUDA is available, ensure that the operation is carried out on the GPU. Otherwise, use the CPU. Further, configure the program to control CUDA behavior using the following environment variables: 1. `PYTORCH_NO_CUDA_MEMORY_CACHING` - Disable CUDA memory caching. 2. `CUDA_LAUNCH_BLOCKING` - Make CUDA calls synchronous to aid debugging. # Input - `a`: A 2D tensor of shape (m, n) - `b`: A 2D tensor of shape (n, p) # Output - A 2D tensor of shape (m, p) that is the result of matrix multiplication `a @ b`. # Constraints - Ensure the tensors are of compatible shapes for matrix multiplication. - Use appropriate exception handling to manage CUDA availability and other potential Tensor errors. # Performance Requirements - Ensure the function runs efficiently leveraging GPU capabilities if available. - Manage CUDA environment configuration through the specified environment variables. # Example ```python import torch # Set environment variables for the session import os os.environ[\\"PYTORCH_NO_CUDA_MEMORY_CACHING\\"] = \\"1\\" os.environ[\\"CUDA_LAUNCH_BLOCKING\\"] = \\"1\\" def cuda_matrix_multiplication(a, b): # Ensure CUDA usage if available device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\') # Move tensors to the appropriate device a = a.to(device) b = b.to(device) try: # Perform matrix multiplication result = torch.matmul(a, b) # Move result back to CPU if it was computed on GPU return result.to(\'cpu\') except RuntimeError as e: # Handle CUDA errors (like out of memory) gracefully print(\\"CUDA error:\\", e) return None a = torch.rand((3, 2)) b = torch.rand((2, 4)) print(cuda_matrix_multiplication(a, b)) ```","solution":"import torch import os # Set environment variables for the session os.environ[\\"PYTORCH_NO_CUDA_MEMORY_CACHING\\"] = \\"1\\" os.environ[\\"CUDA_LAUNCH_BLOCKING\\"] = \\"1\\" def cuda_matrix_multiplication(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor: Performs matrix multiplication of two 2D tensors a and b. Uses CUDA if available, otherwise falls back to CPU. Parameters: a (torch.Tensor): The first 2D tensor. b (torch.Tensor): The second 2D tensor. Returns: torch.Tensor: The result of matrix multiplication of a and b. if a.dim() != 2 or b.dim() != 2: raise ValueError(\\"Both tensors must be 2-dimensional.\\") if a.size(1) != b.size(0): raise ValueError(\\"Tensor shapes are not compatible for matrix multiplication.\\") device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\') a = a.to(device) b = b.to(device) try: result = torch.matmul(a, b) return result.to(\'cpu\') except RuntimeError as e: print(\\"CUDA error:\\", e) return None"},{"question":"# Python Code Analysis Function Objective Write a Python function called `tokenize_python_script` that takes a string input representing a Python source code and returns a list of tokens found in the source code. The function should implement basic lexical analysis as per the Python 3.10 specifications provided. Input - A string `source_code` representing a Python source code. Output - A list of strings, where each string is a token extracted from the source code. Constraints - The input string will not contain any non-ASCII characters. - Only basic tokens such as keywords, identifiers, operators, delimiters, literals (string, numeric), comments, and whitespace handling need to be implemented. Requirements - The function should handle both single-line and multiline comments appropriately. - The function should recognize and properly handle string literals. - The function should differentiate between identifiers, keywords, and literals. - The function should respect Python\'s rules for implicit and explicit line joining. - The function should handle indentation and whitespace between tokens correctly but does not need to generate `INDENT` and `DEDENT` tokens as the lexer. Performance - The function should operate efficiently on source code strings of up to 1000 lines. Example ```python def tokenize_python_script(source_code: str) -> list: # Implementation here pass # Example input source_code = \'\'\' def foo(): \\"\\"\\" Example function \\"\\"\\" if x == 42: return \\"Hello, World!\\" # End of function \'\'\' # Expected output (tokens list may vary depending on the definition of tokens) [ \'def\', \'foo\', \'(\', \')\', \':\', \'NEWLINE\', \'INDENT\', \' Example function \', \'NEWLINE\', \'if\', \'x\', \'==\', \'42\', \':\', \'NEWLINE\', \'INDENT\', \'return\', \'\\"Hello, World!\\"\', \'NEWLINE\', \'DEDENT\', \'# End of function\', \'NEWLINE\', \'DEDENT\' ] ``` Note: The function may include handling of various token types explicitly detailed in the provided documentation. Adjustments to token types and formats are permissible based on the interpretation of the lexical analysis specifics.","solution":"import re def tokenize_python_script(source_code: str) -> list: token_specification = [ (\'COMMENT\', r\'#.*\'), (\'NEWLINE\', r\'n\'), (\'INDENT\', r\'[ t]+\'), # Placeholder for simplicity, not detecting INDENT/DEDENT accurately (\'DEDENT\', r\'[ t]*\'), # Same as above (\'STRING\', r\'\\"\\"\\"(?:.|n)*?\\"\\"\\"|\'\'\'(?:.|n)*?\'\'\'|\\"(?:.|[^\\"])*\\"|\' + r\\"\'(?:.|[^\'])*\'\\"), (\'NUMBER\', r\'d+(.d*)?\'), (\'KEYWORD\', r\'b(?:def|return|if|else|elif|for|while|import|from|as|with|class|try|except|finally|raise|yield|assert|break|continue|pass|lambda|None|True|False|and|or|not|in|is)b\'), (\'IDENTIFIER\', r\'b[_a-zA-Z][_a-zA-Z0-9]*b\'), (\'OPERATOR\', r\'[+-*/%&|^~<>!]=?|==|!=|**=?|//=?\'), (\'DELIMITER\', r\'[()[]{}:.,;@]\'), (\'WHITESPACE\', r\'[ t]+\'), # Placeholder for simplicity ] token_regex = \'|\'.join(\'(?P<%s>%s)\' % pair for pair in token_specification) token_regexp = re.compile(token_regex) line_number = 1 line_start = 0 tokens = [] for match in re.finditer(token_regexp, source_code): kind = match.lastgroup value = match.group(kind) if value == \'n\': line_number += 1 line_start = match.end() tokens.append(\'NEWLINE\') elif kind == \'INDENT\' or kind == \'DEDENT\': # Placeholder, not real indentation handling pass # Leaving out INDENT/DEDENT logic for simplicity elif kind == \'WHITESPACE\': continue else: tokens.append(value) return tokens"},{"question":"# CGI Script Assignment: Handling and Processing Form Data Objective You are required to write a Python CGI script that processes user inputs from a form on a webpage. The form collects a user\'s first name, last name, email, and a message. The script should validate the inputs and display them back in a formatted HTML response. Moreover, the script should handle and count the lines of an uploaded text file. Requirements 1. **Form Elements**: - `first_name` (text) - `last_name` (text) - `email` (email) - `message` (textarea) - `file_upload` (file input) 2. **Validation**: - All fields (except the file upload) are mandatory. - The email should be a valid email address. - If any field is missing or invalid, the script should return an HTML response indicating the specific errors. 3. **File Handling**: - The script should count the number of lines in the uploaded file. - The file should be a text file. If the file is not a text file, the script should return an error message. 4. **Output**: - If all inputs are valid, return an HTML response displaying the user\'s first name, last name, email, and message. - If a file is uploaded and it is valid, display the number of lines in the file. - If there are validation errors, display an appropriate error message. Input and Output Formats - **Input**: User form submission via POST method. - **Output**: HTML content (with appropriate headers) that displays the information or error messages. Constraints and Limitations - You should only use the `cgi` and `cgitb` modules to handle form data. - You should ensure that the script is executable and properly formatted to be run as a CGI script. Example **Form Submission**: ``` First Name: John Last Name: Doe Email: john.doe@example.com Message: Hello, this is a test message. File Upload: (upload a text file with 5 lines) ``` **Output**: ```html Content-Type: text/html <!DOCTYPE html> <html> <head> <title>Form Submission</title> </head> <body> <h1>Form Submission Details</h1> <p><b>First Name:</b> John</p> <p><b>Last Name:</b> Doe</p> <p><b>Email:</b> john.doe@example.com</p> <p><b>Message:</b> Hello, this is a test message.</p> <p><b>Uploaded File Line Count:</b> 5</p> </body> </html> ``` Instructions Write your Python CGI script based on the requirements above. Test it on your local server to ensure it works as expected. Ensure proper error handling and input validation.","solution":"#!/usr/bin/env python3 import cgi import cgitb import re # Enable debugging cgitb.enable() def is_valid_email(email): Validate email format. pattern = r\'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+\' return re.match(pattern, email) is not None def count_file_lines(fileitem): Count the number of lines in the uploaded text file. lines = fileitem.file.read().decode(\'utf-8\').splitlines() return len(lines) def generate_response(first_name, last_name, email, message, file_line_count=None, errors=None): Generate HTML response based on provided input. print(\\"Content-Type: text/html\\") print() print(\\"<!DOCTYPE html>\\") print(\\"<html>\\") print(\\"<head>\\") print(\\"<title>Form Submission</title>\\") print(\\"</head>\\") print(\\"<body>\\") print(\\"<h1>Form Submission Details</h1>\\") if errors: print(\\"<h2>Errors:</h2>\\") for error in errors: print(f\\"<p>{error}</p>\\") else: print(f\\"<p><b>First Name:</b> {first_name}</p>\\") print(f\\"<p><b>Last Name:</b> {last_name}</p>\\") print(f\\"<p><b>Email:</b> {email}</p>\\") print(f\\"<p><b>Message:</b> {message}</p>\\") if file_line_count is not None: print(f\\"<p><b>Uploaded File Line Count:</b> {file_line_count}</p>\\") print(\\"</body>\\") print(\\"</html>\\") def main(): form = cgi.FieldStorage() first_name = form.getvalue(\'first_name\') last_name = form.getvalue(\'last_name\') email = form.getvalue(\'email\') message = form.getvalue(\'message\') fileitem = form[\'file_upload\'] errors = [] # Validate inputs if not first_name: errors.append(\\"First name is required.\\") if not last_name: errors.append(\\"Last name is required.\\") if not email: errors.append(\\"Email is required.\\") elif not is_valid_email(email): errors.append(\\"Email format is invalid.\\") if not message: errors.append(\\"Message is required.\\") file_line_count = None if fileitem.filename: if fileitem.file: file_mime_type = fileitem.type if file_mime_type != \'text/plain\': errors.append(\\"Uploaded file must be a text file.\\") else: file_line_count = count_file_lines(fileitem) generate_response(first_name, last_name, email, message, file_line_count, errors) if __name__ == \\"__main__\\": main()"},{"question":"Objective: You are tasked with creating a Python script that simulates a complex function call scenario. Additionally, you need to write a SystemTap script that traces and outputs the function call and return hierarchy for your Python script. Requirements: 1. **Python Script:** - Create a Python script named `complex_call_scenario.py`. - The script should include multiple nested functions. - Include at least one recursive function. - Introduce exceptions within some of the functions. - The main function should be named `main_function`. 2. **SystemTap Script:** - Write a SystemTap script named `trace_hierarchy.stp`. - The script should trace the function call and return hierarchy of your `complex_call_scenario.py`. - Use the provided static markers—`function__entry` and `function__return`. - Format the output to display the timestamp, function name, and line number. Input and Output: - **Python Script (`complex_call_scenario.py`):** - No input required from the user. - The script should generate console output showcasing the scenario of nested functions, recursion, and exceptions. Example structure of the Python script: ```python def main_function(): try: func_a() except Exception as e: print(\\"Exception in main_function:\\", e) def func_a(): func_b() func_d() def func_b(): for i in range(3): func_c(i) def func_c(x): if x % 2 == 0: raise ValueError(\\"Intentional Error\\") def func_d(): func_e() func_f() def func_e(): pass # Base case for recursion def func_f(n=5): if n > 0: func_f(n-1) if __name__ == \\"__main__\\": main_function() ``` - **SystemTap Script (`trace_hierarchy.stp`):** - Trace the function entry and return events of the Python script. - Ensure the output displays hierarchy using indentation. Example format of the output: ``` timestamp function-entry: main_function:line_number timestamp function-entry: func_a:line_number timestamp function-entry: func_b:line_number timestamp function-entry: func_c:line_number timestamp function-return: func_c:line_number timestamp function-return: func_b:line_number ... timestamp function-return: main_function:line_number ``` Submission: - Submit the complete Python script `complex_call_scenario.py`. - Submit the SystemTap script `trace_hierarchy.stp`. - Provide a sample output of running your SystemTap script with your Python script. Constraints: - You are allowed to use any standard Python library. - The Python script should not depend on any external input. - Ensure proper error handling within your Python functions. Performance: - Ensure your Python script and SystemTap script run efficiently without significant delays. Good luck!","solution":"def main_function(): try: func_a() except Exception as e: print(\\"Exception in main_function:\\", e) def func_a(): func_b() func_d() def func_b(): for i in range(3): try: func_c(i) except ValueError as e: print(f\\"Exception caught in func_b: {e}\\") def func_c(x): if x % 2 == 0: raise ValueError(f\\"Intentional Error: {x} is even\\") def func_d(): func_e() func_f() def func_e(): pass # Base case for recursion def func_f(n=5): if n > 0: func_f(n-1) if __name__ == \\"__main__\\": main_function()"},{"question":"# Question: You are tasked with developing a function that processes a list of HTTP response codes and generates a summary of the different categories of responses. Your task is to implement the function `categorize_http_responses`. **Function Signature:** ```python def categorize_http_responses(codes: list[int]) -> dict[str, int]: pass ``` # Description: 1. The function `categorize_http_responses` takes a single parameter: - `codes`: a list of integer HTTP response codes. 2. The function should categorize the response codes into five categories based on the first digit of the status codes: - `1xx`: Informational Responses - `2xx`: Success Responses - `3xx`: Redirection Responses - `4xx`: Client Error Responses - `5xx`: Server Error Responses 3. The function should return a dictionary where the keys are string representations of the categories (`\\"1xx\\"`, `\\"2xx\\"`, `\\"3xx\\"`, `\\"4xx\\"`, `\\"5xx\\"`) and the values are the counts of response codes that fall into each category. # Example: ```python input_codes = [200, 201, 404, 500, 103, 202, 301, 204, 405, 510, 418] output = categorize_http_responses(input_codes) print(output) # Output should be: {\'1xx\': 1, \'2xx\': 4, \'3xx\': 1, \'4xx\': 2, \'5xx\': 3} ``` # Constraints: - The list of codes will only contain valid HTTP status codes recognized by the `http.HTTPStatus` enum. - The list will have at least one status code and at most 100 status codes. # Requirements: 1. Make sure to use the `http.HTTPStatus` enum for validating and processing the status codes. 2. Ensure your solution is efficient and does not use unnecessary loops. # Additional Notes: - Leveraging the `http.HTTPStatus` enum will help in ensuring that only valid status codes are processed. - You can use the integer value of the status codes to determine their category by inspecting the first digit.","solution":"from http import HTTPStatus def categorize_http_responses(codes: list[int]) -> dict[str, int]: Categorizes a list of HTTP response codes into their respective categories. Parameters: codes - list of integer HTTP response codes Returns: A dictionary where keys are categories (\'1xx\', \'2xx\', \'3xx\', \'4xx\', \'5xx\') and values are the counts of response codes in each category. categories = { \'1xx\': 0, \'2xx\': 0, \'3xx\': 0, \'4xx\': 0, \'5xx\': 0 } for code in codes: if 100 <= code < 200: categories[\'1xx\'] += 1 elif 200 <= code < 300: categories[\'2xx\'] += 1 elif 300 <= code < 400: categories[\'3xx\'] += 1 elif 400 <= code < 500: categories[\'4xx\'] += 1 elif 500 <= code < 600: categories[\'5xx\'] += 1 return categories"},{"question":"# PyTorch DataLoader: Custom IterableDataset with Multi-process Loading In this coding assessment, you are required to demonstrate your understanding of the PyTorch `torch.utils.data` module by implementing a custom `IterableDataset` class and using it with a `DataLoader` with multi-process data loading. You must: 1. Implement an `IterableDataset` that reads data from a list of file paths. Each file contains a single integer value. 2. Ensure that the dataset is processed in parallel using multiple worker processes. 3. Use custom logic to handle and shard the dataset across multiple workers to avoid duplicate data loading. 4. Implement a custom `collate_fn` that batches the data and converts it to PyTorch tensors. 5. Verify that data is loaded correctly in a batched and parallel manner. # Expected Input and Output Formats Input: A list of file paths containing integer data (e.g., `[\'file1.txt\', \'file2.txt\', ...]`). Output: Batched integer data in the form of PyTorch tensors. # Constraints: 1. The Dataset should handle the sharding to ensure no duplicate data in multi-process loading. 2. The DataLoader should use at least 2 worker processes. 3. Implement appropriate error handling for file reading. # Performance Requirements: 1. Efficient data loading with minimal overlap or duplication. 2. Correct handling of edge cases like empty files or missing files. # Implementation Steps: 1. **CustomIterableDataset class**: - Inherit from `torch.utils.data.IterableDataset`. - Implement the `__init__`, `__iter__`, and `shard_worker` methods. 2. **DataLoader setup**: - Use the `num_workers` parameter to enable multi-process loading. - Implement a custom `collate_fn` to batch the data. # Example: ```python import torch from torch.utils.data import DataLoader, IterableDataset, get_worker_info class CustomIterableDataset(IterableDataset): def __init__(self, file_paths): self.file_paths = file_paths def __iter__(self): worker_info = get_worker_info() if worker_info is None: # single-process data loading file_paths = self.file_paths else: # in a worker process per_worker = int(math.ceil(len(self.file_paths) / float(worker_info.num_workers))) worker_id = worker_info.id file_paths = self.file_paths[worker_id * per_worker:(worker_id + 1) * per_worker] for file_path in file_paths: try: with open(file_path, \'r\') as f: yield int(f.read().strip()) except Exception as e: print(f\'Error reading file {file_path}: {e}\') def collate_fn(batch): return torch.tensor(batch, dtype=torch.int64) # Example file paths file_paths = [\'file1.txt\', \'file2.txt\', \'file3.txt\', \'file4.txt\'] # Create dataset and dataloader dataset = CustomIterableDataset(file_paths) dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn, num_workers=2) # Load and print data for batch in dataloader: print(batch) ``` In this example, you need to: 1. Create a set of files with integer values. 2. Implement the `CustomIterableDataset` class. 3. Use the `DataLoader` with `num_workers` set to 2 to parallelize the data loading.","solution":"import torch from torch.utils.data import DataLoader, IterableDataset, get_worker_info import math import os class CustomIterableDataset(IterableDataset): def __init__(self, file_paths): self.file_paths = file_paths def __iter__(self): worker_info = get_worker_info() if worker_info is None: # single-process data loading file_paths = self.file_paths else: # in a worker process per_worker = int(math.ceil(len(self.file_paths) / float(worker_info.num_workers))) worker_id = worker_info.id file_paths = self.file_paths[worker_id * per_worker:(worker_id + 1) * per_worker] for file_path in file_paths: try: with open(file_path, \'r\') as f: yield int(f.read().strip()) except Exception as e: print(f\'Error reading file {file_path}: {e}\') def collate_fn(batch): return torch.tensor(batch, dtype=torch.int64)"},{"question":"**Out-of-Core Learning for Large Datasets** In this task, you will build a system using scikit-learn that can handle classification on a large dataset that doesn\'t fit into memory. You will need to set up streaming instances, feature extraction, and incremental learning. # Task Implement the following function: ```python def out_of_core_classification(data_stream, feature_extraction, classifier, mini_batch_size, classes): Perform out-of-core classification on a large dataset. Parameters: - data_stream (generator): A generator that yields tuples of (X_batch, y_batch). Each X_batch is a list of raw instances to be processed, and y_batch is a list of corresponding labels. - feature_extraction (object): An instance of a feature extractor (like sklearn\'s HashingVectorizer). - classifier (object): An instance of an incremental classifier (like sklearn\'s SGDClassifier). - mini_batch_size (int): The size of the mini-batches to be used for partial fitting. - classes (list): List of all possible classes for classification. Returns: - classifier (object): The trained classifier. pass ``` # Requirements 1. **Streaming Instances**: - Use the `data_stream` generator to retrieve data in batches. 2. **Feature Extraction**: - Use the provided `feature_extraction` instance to transform raw instances into features. 3. **Incremental Learning**: - Fit the `classifier` incrementally using the mini-batches of transformed instances. - Ensure the classifier handles all classes correctly by providing them during the initial `partial_fit` call. # Constraints - You must use an incremental algorithm that supports the `partial_fit` method. - The `data_stream` generator could yield a large number of batches, so you must handle them efficiently in memory. # Example ```python from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier # Example data generator def example_data_stream(): yield ([\\"This is a positive example.\\"], [1]) yield ([\\"This is a negative example.\\"], [0]) # ... add more batches # Sample usage feature_extractor = HashingVectorizer() classifier = SGDClassifier() mini_batch_size = 1 classes = [0, 1] trained_model = out_of_core_classification(example_data_stream(), feature_extractor, classifier, mini_batch_size, classes) ``` **Note**: You are not required to handle the specific implementation details of `data_stream` generation. Assume it is already provided and correctly yields mini-batches of data. # Evaluation - Correct implementation of out-of-core learning. - Efficient handling of mini-batches. - Proper use of feature extraction and incremental learning.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier def out_of_core_classification(data_stream, feature_extraction, classifier, mini_batch_size, classes): Perform out-of-core classification on a large dataset. Parameters: - data_stream (generator): A generator that yields tuples of (X_batch, y_batch). Each X_batch is a list of raw instances to be processed, and y_batch is a list of corresponding labels. - feature_extraction (object): An instance of a feature extractor (like sklearn\'s HashingVectorizer). - classifier (object): An instance of an incremental classifier (like sklearn\'s SGDClassifier). - mini_batch_size (int): The size of the mini-batches to be used for partial fitting. - classes (list): List of all possible classes for classification. Returns: - classifier (object): The trained classifier. for X_batch, y_batch in data_stream: X_batch_transformed = feature_extraction.transform(X_batch) classifier.partial_fit(X_batch_transformed, y_batch, classes=classes) return classifier"},{"question":"Advanced Categorical Data Visualization with Seaborn You are given a dataset `students_performance` containing student performance data with the following columns: - `gender`: Gender of the student (categorical). - `race/ethnicity`: Ethnicity group of the student (categorical). - `parental_level_of_education`: Highest degree obtained by any parent of the student (categorical). - `lunch`: Type of lunch the student receives (categorical). - `test_preparation_course`: Whether the student completed a test preparation course (categorical). - `math_score`: Math score of the student (numeric). - `reading_score`: Reading score of the student (numeric). - `writing_score`: Writing score of the student (numeric). Using the Seaborn package, implement a function `visualize_student_performance` to perform the following tasks: 1. **Load the Data**: Read the dataset `students_performance.csv` from the provided path. 2. **Data Preprocessing**: - Create a new column `average_score` which is the mean of `math_score`, `reading_score`, and `writing_score`. 3. **Visualization**: - Generate a boxplot to compare the `average_score` across different `parental_level_of_education` levels. - Generate a violin plot to compare `math_score` for `male` and `female` students, splitting the violin by `gender`. - Create a catplot using the `stripplot` method to visualize the distribution of `average_score` across different `lunch` types. Use `hue` to differentiate between students who completed the test preparation course and those who did not. - Generate a `pointplot` to show the mean `math_score` for each `race/ethnicity` group with respect to `gender`. Include confidence intervals to show the variability. **Function Signature**: ```python def visualize_student_performance(file_path: str) -> None: pass ``` **Constraints**: - The dataset file will be available at the specified file path. - Library versions: numpy >= 1.21.0, seaborn >= 0.11.0, pandas >= 1.3.0, matplotlib >= 3.4.0 **Expected Output**: - The function should generate and display the specified plots inline (or save them if running in a non-interactive environment). **Notes**: - Ensure that plots are neatly labeled with appropriate titles and axis labels. - Use Seaborn\'s `set_theme()` to apply a consistent style across all plots.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_student_performance(file_path: str) -> None: Function to visualize student performance data with various statistical plots. Args: file_path (str): Path to the dataset file. Returns: None # Load the data students_performance = pd.read_csv(file_path) # Data Preprocessing students_performance[\'average_score\'] = students_performance[[\'math_score\', \'reading_score\', \'writing_score\']].mean(axis=1) # Set the theme for seaborn sns.set_theme(style=\\"whitegrid\\") # Boxplot to compare average_score across different parental_level_of_education levels plt.figure(figsize=(10, 6)) sns.boxplot(data=students_performance, x=\'parental_level_of_education\', y=\'average_score\') plt.title(\'Average Score across Different Parental Levels of Education\') plt.xticks(rotation=45) plt.show() # Violin plot to compare math_score for male and female students, split by gender plt.figure(figsize=(10, 6)) sns.violinplot(data=students_performance, x=\'gender\', y=\'math_score\', split=True) plt.title(\'Math Score Distribution by Gender\') plt.show() # Catplot using stripplot to visualize average_score across different lunch types, hue by test_preparation_course plt.figure(figsize=(10, 6)) sns.catplot(data=students_performance, x=\'lunch\', y=\'average_score\', hue=\'test_preparation_course\', kind=\'strip\') plt.title(\'Average Score Distribution by Lunch Type\') plt.show() # Pointplot to show the mean math_score for each race/ethnicity group with respect to gender plt.figure(figsize=(10, 6)) sns.pointplot(data=students_performance, x=\'race/ethnicity\', y=\'math_score\', hue=\'gender\', ci=\'sd\', markers=[\\"o\\", \\"x\\"], linestyles=[\\"-\\", \\"--\\"]) plt.title(\'Mean Math Score by Race/Ethnicity and Gender\') plt.show()"},{"question":"**Objective:** Create a function `analyze_and_normalize` that takes a list of Unicode strings and performs the following operations: 1. **Validation and Normalization**: - Validate each string to check if it is in Normal Form NFC (Canonical Composition). - If a string is not in Normal Form NFC, normalize it to NFC. 2. **Character Property Analysis**: - For each string in its validated or normalized form, generate a dictionary where each key is a character from the string, and the value is another dictionary containing the following properties of the character: - `name`: The Unicode name of the character. - `category`: The general category assigned to the character. - `combining`: The canonical combining class of the character. - `bidirectional`: The bidirectional class of the character. - `decimal`: The decimal value of the character (if applicable, otherwise `None`). - `mirrored`: The mirrored property of the character. - Aggregate these dictionaries into a final result. 3. **Output**: - Return a list where each element corresponds to the analysis of each normalized string from the input list. **Input**: - `strings`: List of Unicode strings. **Output**: - List of dictionaries, each representing the analysis of one normalized string. **Constraints**: - Handle edge cases where characters may not have certain properties (e.g., `decimal` value). **Example**: ```python def analyze_and_normalize(strings): import unicodedata def get_character_properties(char): return { \'name\': unicodedata.name(char, \'UNNAMED\'), \'category\': unicodedata.category(char), \'combining\': unicodedata.combining(char), \'bidirectional\': unicodedata.bidirectional(char), \'decimal\': unicodedata.decimal(char, None), \'mirrored\': unicodedata.mirrored(char), } result = [] for string in strings: if not unicodedata.is_normalized(\'NFC\', string): string = unicodedata.normalize(\'NFC\', string) char_analysis = {char: get_character_properties(char) for char in string} result.append(char_analysis) return result # Example Usage: strings = [\\"eu0301\\", \\"À\\", \\"9\\"] output = analyze_and_normalize(strings) print(output) ``` **Explanation**: - For the input `[\\"eu0301\\", \\"À\\", \\"9\\"]`: - \\"eu0301\\" (e with combining acute) will become \\"é\\" after NFC normalization. - \\"À\\" remains the same as it is already composed. - \\"9\\" remains the same as it is a digit. - The output will be a list of dictionaries with detailed analyses of each character in the normalized strings. Test the function with various Unicode strings to ensure comprehensive coverage and consider edge cases where certain properties may not be applicable.","solution":"def analyze_and_normalize(strings): import unicodedata def get_character_properties(char): return { \'name\': unicodedata.name(char, \'UNNAMED\'), \'category\': unicodedata.category(char), \'combining\': unicodedata.combining(char), \'bidirectional\': unicodedata.bidirectional(char), \'decimal\': unicodedata.decimal(char, None), \'mirrored\': unicodedata.mirrored(char), } result = [] for string in strings: if not unicodedata.is_normalized(\'NFC\', string): string = unicodedata.normalize(\'NFC\', string) char_analysis = {char: get_character_properties(char) for char in string} result.append(char_analysis) return result # Example Usage: strings = [\\"eu0301\\", \\"À\\", \\"9\\"] output = analyze_and_normalize(strings) print(output)"},{"question":"Objective: You are required to demonstrate your understanding of the `codecs` module by implementing custom encoding and decoding functionalities, handling errors appropriately, and using the incremental encoding/decoding classes provided by the module. # Task: 1. **Custom Encoder and Decoder**: - Implement a custom encoder and decoder for a hypothetical encoding called \\"reverse\\". This encoding reverses the characters of the input text. - Implement a custom incremental encoder and decoder for this \\"reverse\\" encoding. - Register this custom codec with the codec registry. 2. **Error Handling**: - Implement custom error handlers for \\"reverse\\" encoding that handle errors by replacing problematic characters with a `#` symbol. 3. **Stream Handling**: - Illustrate the usage of your custom codec by reading from and writing to files using the `StreamWriter` and `StreamReader`. # Specifications: 1. **Custom Codec**: - `reverse_encode(text: str) -> Tuple[bytes, int]`: Encodes the input text by reversing its characters and converting to bytes. - `reverse_decode(data: bytes) -> Tuple[str, int]`: Decodes the input bytes by converting to string and reversing the characters. - `ReverseIncrementalEncoder` should incrementally encode text by reversing in chunks. - `ReverseIncrementalDecoder` should incrementally decode bytes by reversing in chunks. 2. **Error Handlers**: - `reverse_replace_errors(exception)`: Custom error handler for \\"reverse\\" encoding that replaces problematic characters with `#`. 3. **Stream Handling Example**: - Write a function that reads from a file, encodes the content using the \\"reverse\\" encoding, and writes the encoded content to another file. - Read back from the encoded file, decode the content using the \\"reverse\\" encoding, and verify it matches the original content. # Constraints: - Handle the edge cases such as empty strings and null bytes (if applicable). - Ensure the solution is efficient and can handle large inputs gracefully. # Submission: Submit a Python script implementing the specified functionalities along with a demonstration of the stream handling example. Include comments and documentation within your code to explain your approach.","solution":"import codecs # Custom Encoder def reverse_encode(input, errors=\'strict\'): if errors != \'strict\': input = \'\'.join(\'#\' if c in errors else c for c in input) reversed_text = input[::-1] return reversed_text.encode(\'utf-8\'), len(input) # Custom Decoder def reverse_decode(input, errors=\'strict\'): decoded_text = input.decode(\'utf-8\') reversed_text = decoded_text[::-1] return reversed_text, len(input) # Custom IncrementalEncoder class ReverseIncrementalEncoder(codecs.IncrementalEncoder): def encode(self, input, final=False): return reverse_encode(input, self.errors)[0] # Custom IncrementalDecoder class ReverseIncrementalDecoder(codecs.IncrementalDecoder): def decode(self, input, final=False): return reverse_decode(input, self.errors)[0] # Custom Error Handler def reverse_replace_errors(exception): if isinstance(exception, (UnicodeEncodeError, UnicodeDecodeError)): return (\'#\', exception.start + 1) else: raise TypeError(\\"Don\'t know how to handle {0}\\".format(exception)) codecs.register_error(\\"reverse_replace_errors\\", reverse_replace_errors) # Custom Codec Registration class ReverseCodec(codecs.Codec): def encode(self, input, errors=\'strict\'): return reverse_encode(input, errors) def decode(self, input, errors=\'strict\'): return reverse_decode(input, errors) def reverse_search_function(encoding): if encoding == \'reverse\': return codecs.CodecInfo( name=\'reverse\', encode=reverse_encode, decode=reverse_decode, incrementalencoder=ReverseIncrementalEncoder, incrementaldecoder=ReverseIncrementalDecoder, streamwriter=codecs.StreamWriter, streamreader=codecs.StreamReader, ) return None codecs.register(reverse_search_function) # Stream Handling Example def encode_to_file(input_file, output_file): with codecs.open(input_file, \'r\', \'utf-8\') as infile: text = infile.read() with codecs.open(output_file, \'w\', \'utf-8\', \'reverse\') as outfile: outfile.write(text) def decode_from_file(output_file): with codecs.open(output_file, \'r\', \'utf-8\', \'reverse\') as infile: text = infile.read() return text # Example usage if __name__ == \\"__main__\\": input_text = \\"Hello, World!\\" encoded_text, _ = reverse_encode(input_text) decoded_text, _ = reverse_decode(encoded_text) print(f\\"Original Text: {input_text}\\") print(f\\"Encoded Text: {encoded_text}\\") print(f\\"Decoded Text: {decoded_text}\\")"},{"question":"# Custom Mailbox Handler You are required to implement a custom mailbox handler named `CustomMailbox` that extends the `Mailbox` class. This handler will manage email messages in a unique format where each email message is stored in individual files within a specified directory. Each message file\'s name should be a unique identifier, and the directory will contain subdirectories named \\"incoming,\\" \\"outgoing,\\" and \\"processed\\" where messages will be sorted based on their states. Your implementation should include the following methods: 1. **Constructor**: `__init__(dirname, factory=None, create=True)` - Initialize the custom mailbox with the specified directory name. - Create the directory structure if it does not exist. - Parameters: - `dirname` (str): The directory name where the mailbox will be stored. - `factory` (callable, optional): A callable object that returns a custom representation of the message. - `create` (bool, optional): Create the directory if it does not exist (default is True). 2. **add_message(self, message)**: - Add a new message to the \\"incoming\\" subdirectory and return the key assigned to it. - Parameters: - `message`: Can be an instance of `Message`, `email.message.Message`, a string, a byte string, or a file-like object. 3. **move_message(self, key, destination)**: - Move a message identified by `key` to one of the subdirectories: \\"incoming,\\" \\"outgoing,\\" or \\"processed\\". - Parameters: - `key` (str): The unique identifier of the message. - `destination` (str): The subdirectory to move the message to. Must be one of \\"incoming,\\" \\"outgoing,\\" or \\"processed\\". 4. **remove_message(self, key)**: - Remove a message identified by `key` from the mailbox. - Parameters: - `key` (str): The unique identifier of the message. 5. **list_messages(self, subdir)**: - List all messages in the specified subdirectory. - Parameters: - `subdir` (str): The name of the subdirectory. Must be one of \\"incoming,\\" \\"outgoing,\\" or \\"processed\\". - Returns: - A list of keys identifying the messages in the specified subdirectory. Ensure your implementation safely handles concurrent modifications and follows the best practices outlined in the documentation for locking and unlocking mailboxes. # Constraints: 1. Do not use libraries other than Python\'s standard library. 2. Ensure thread safety when adding, removing, or moving messages. 3. Validate the input parameters for each method. # Example Usage: ```python # Initialize the custom mailbox custom_mailbox = CustomMailbox(\'/path/to/mailbox\') # Add a new message message_key = custom_mailbox.add_message(\\"This is a test message.\\") # Move the message to the outgoing subdirectory custom_mailbox.move_message(message_key, \\"outgoing\\") # List all messages in the outgoing subdirectory outgoing_messages = custom_mailbox.list_messages(\\"outgoing\\") print(outgoing_messages) # Remove the message custom_mailbox.remove_message(message_key) ```","solution":"import os import uuid import shutil import mailbox from email.message import EmailMessage class CustomMailbox(mailbox.Mailbox): def __init__(self, dirname, factory=None, create=True): self.root = dirname self.subdirectories = [\\"incoming\\", \\"outgoing\\", \\"processed\\"] if factory: self.factory = factory self._create_structure(create) def _create_structure(self, create): if create: os.makedirs(self.root, exist_ok=True) for subdir in self.subdirectories: os.makedirs(os.path.join(self.root, subdir), exist_ok=True) def _generate_key(self): return str(uuid.uuid4()) @staticmethod def _write_message_to_file(filename, message): with open(filename, \'w\') as file: file.write(message) def add_message(self, message): if isinstance(message, (str, bytes)): message_text = message elif isinstance(message, EmailMessage): message_text = message.as_string() else: raise ValueError(\\"Unsupported message type\\") key = self._generate_key() filepath = os.path.join(self.root, \\"incoming\\", key) self._write_message_to_file(filepath, message_text) return key def move_message(self, key, destination): if destination not in self.subdirectories: raise ValueError(\\"Invalid destination. Must be \'incoming\', \'outgoing\', or \'processed\'\\") src = None for subdir in self.subdirectories: possible_path = os.path.join(self.root, subdir, key) if os.path.exists(possible_path): src = possible_path break if src is None: raise FileNotFoundError(\\"Message not found\\") dest = os.path.join(self.root, destination, key) shutil.move(src, dest) def remove_message(self, key): found = False for subdir in self.subdirectories: possible_path = os.path.join(self.root, subdir, key) if os.path.exists(possible_path): os.remove(possible_path) found = True break if not found: raise FileNotFoundError(\\"Message not found\\") def list_messages(self, subdir): if subdir not in self.subdirectories: raise ValueError(\\"Invalid subdirectory. Must be \'incoming\', \'outgoing\', or \'processed\'\\") return os.listdir(os.path.join(self.root, subdir))"},{"question":"Objective Write a function that loads a dataset, preprocesses it, and generates specific plots using the pandas plotting functions provided. The purpose of this task is to assess your understanding of data manipulation and visualization using pandas. Function Signature ```python def generate_plots(file_path: str): pass ``` Input - `file_path`: A string representing the path to a CSV file containing the dataset. Output The function should not return any value but should display the following plots: 1. `andrews_curves` plot for data visualization. 2. `autocorrelation_plot` for a specified numeric column. 3. `scatter_matrix` for the entire dataset. Constraints - The dataset should have at least one categorical column for the `andrews_curves` plot. - The dataset should have at least one numeric column for the `autocorrelation_plot`. - The dataset should have a minimum of 3 numeric columns for the `scatter_matrix`. Steps 1. Load the dataset using pandas. 2. Check and ensure the dataset meets the constraints. If it does not, raise a ValueError with an appropriate message. 3. Generate and display the `andrews_curves` plot using the first categorical column found. 4. Generate and display the `autocorrelation_plot` using the first numeric column found. 5. Generate and display the `scatter_matrix` for all numeric columns in the dataset. Example Usage ```python generate_plots(\'path/to/dataset.csv\') ``` This will load the dataset from the provided file path and produce the required plots for data visualization. Additional Notes - The plots should be displayed inline if using a Jupyter notebook or any other plotting interface. - Use the pandas `plotting` module for generating the plots.","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import andrews_curves, autocorrelation_plot, scatter_matrix def generate_plots(file_path: str): Loads a dataset, preprocesses it, and generates specific plots using pandas plotting functions. The function displays three types of plots: andrews_curves, autocorrelation_plot, and scatter_matrix. Parameters: file_path (str): The path to the CSV file containing the dataset. Raises: ValueError: If the dataset does not meet the required constraints. # Load the dataset df = pd.read_csv(file_path) # Check if there is at least one categorical column for andrews_curves categorical_columns = df.select_dtypes(include=[\'object\', \'category\']).columns if len(categorical_columns) == 0: raise ValueError(\\"The dataset must contain at least one categorical column.\\") # Check if there is at least one numeric column for autocorrelation_plot numeric_columns = df.select_dtypes(include=[\'number\']).columns if len(numeric_columns) == 0: raise ValueError(\\"The dataset must contain at least one numeric column.\\") # Check if there are at least three numeric columns for scatter_matrix if len(numeric_columns) < 3: raise ValueError(\\"The dataset must contain at least three numeric columns.\\") # Generate and display the andrews_curves plot plt.figure() andrews_curves(df, class_column=categorical_columns[0]) plt.title(\'Andrews Curves\') plt.show() # Generate and display the autocorrelation_plot plt.figure() autocorrelation_plot(df[numeric_columns[0]]) plt.title(\'Autocorrelation Plot\') plt.show() # Generate and display the scatter_matrix plot plt.figure() scatter_matrix(df[numeric_columns], alpha=0.2, figsize=(10, 10), diagonal=\'kde\') plt.suptitle(\'Scatter Matrix\') plt.show()"},{"question":"Objective: You are tasked with implementing a Python function that connects to a POP3 server, retrieves email messages, and prints the subject line of each email. You should use the `poplib` module to accomplish this task. Problem Statement: Implement the function `retrieve_email_subjects(host: str, username: str, password: str, use_ssl: bool = False) -> List[str]` that connects to a POP3 server specified by `host`, authenticates using the provided `username` and `password`, and retrieves the subject lines of all emails in the mailbox. If `use_ssl` is `True`, the connection should be made using SSL; otherwise, a standard connection should be used. Constraints: - You must handle all potential exceptions that could occur during the connection and retrieval process. - If any errors occur (e.g., invalid credentials, connection issues, etc.), the function should return an empty list. - Use the `email.parser` module to parse the email headers to extract the subject line. - The function should handle large mailboxes efficiently without loading all messages into memory at once. Input: - `host` (str): The hostname of the POP3 server (e.g., \\"pop.example.com\\"). - `username` (str): The username for authentication. - `password` (str): The password for authentication. - `use_ssl` (bool): Whether to use an SSL connection (default is `False`). Output: - `List[str]`: A list of subject lines of all emails retrieved from the mailbox. Example: ```python >>> retrieve_email_subjects(\\"pop.example.com\\", \\"user@example.com\\", \\"password123\\", use_ssl=True) [\\"Welcome to Example!\\", \\"Your Invoice\\", \\"Meeting Reminder\\", ...] ``` Implementation Details: 1. Create a connection to the POP3 server using `poplib.POP3` or `poplib.POP3_SSL` based on the `use_ssl` flag. 2. Authenticate using the provided `username` and `password`. 3. Retrieve the list of message numbers. 4. For each message, retrieve and parse the headers to extract the subject line. 5. Return a list of all subject lines. 6. Handle exceptions gracefully to ensure the function returns an empty list in case of any errors. ```python import poplib from email.parser import BytesParser def retrieve_email_subjects(host: str, username: str, password: str, use_ssl: bool = False) -> list: try: # Connect to the POP3 server if use_ssl: server = poplib.POP3_SSL(host) else: server = poplib.POP3(host) # Authenticate server.user(username) server.pass_(password) # Retrieve the list of message numbers email_ids = server.list()[1] subjects = [] # Parse the subject lines of all emails for email_id in email_ids: msg_number = email_id.split()[0] response, lines, octets = server.retr(msg_number) msg_content = b\'rn\'.join(lines) msg = BytesParser().parsebytes(msg_content) subject = msg.get(\\"Subject\\", \\"No Subject\\") subjects.append(subject) # Close the connection server.quit() return subjects except (poplib.error_proto, Exception) as e: return [] ``` **Note**: This is a sample implementation; students are expected to handle different edge cases and validate the robustness of their solution.","solution":"import poplib from email.parser import BytesParser from typing import List def retrieve_email_subjects(host: str, username: str, password: str, use_ssl: bool = False) -> List[str]: try: # Connect to the POP3 server if use_ssl: server = poplib.POP3_SSL(host) else: server = poplib.POP3(host) # Authenticate server.user(username) server.pass_(password) # Retrieve the list of message numbers email_ids = server.list()[1] subjects = [] # Parse the subject lines of all emails for email_id in email_ids: msg_number = email_id.split()[0] response, lines, octets = server.retr(msg_number) msg_content = b\'rn\'.join(lines) msg = BytesParser().parsebytes(msg_content) subject = msg.get(\\"Subject\\", \\"No Subject\\") subjects.append(subject) # Close the connection server.quit() return subjects except (poplib.error_proto, Exception) as e: # Return an empty list in case of any errors return []"},{"question":"# Python/C API Arithmetic Library Implementation # Problem Description You are required to implement a Python function named `evaluate_expression` that simulates the behavior of a sequence of arithmetic operations using the provided Python/C API functions. The function should evaluate an expression composed of basic arithmetic operations and nested function calls. # Function Signature ```python def evaluate_expression(expression: str) -> str: # Your implementation here ``` # Parameters - **expression** (`str`): A string containing an arithmetic expression to be evaluated. The expression will only contain integers and the following operations: `+`, `-`, `*`, `//`, `/`, `%`. # Constraints - The input expression will be a valid arithmetic expression. - The numbers in the expression will be valid integers. - Operators will be used correctly as per arithmetic rules. - Assume all division operations are safe (no division by zero). - Ensure that the operations are executed according to correct precedence and associativity. - The maximum length of the expression string is 1000 characters. # Output - **Returns** (`str`): The result of the evaluated arithmetic expression as a string, which can be a valid integer or a floating point number. # Example ```python evaluate_expression(\\"3 + 5 * 2\\") # Output: \\"13\\" evaluate_expression(\\"(2 + 3) * 4 // 2\\") # Output: \\"10\\" evaluate_expression(\\"10 / 4 + 3 * 2 - 5 % 3\\") # Output: \\"6.5\\" ``` # Note - You must use the provided python310 API functions for operations simulation. - Implement proper error handling for any `NULL` results signifying failures in operations. # Additional Information You will find methods for arithmetic and bitwise operations as part of the provided Python/C API in `python310`. Use these methods to construct the evaluation logic for the input expression string.","solution":"def evaluate_expression(expression: str) -> str: Evaluates an arithmetic expression and returns the result as a string. The expression can contain +, -, *, //, /, %. # Use Python\'s eval function as a shortcut for this task. # In practice, using eval is dangerous and should be avoided. # If we had to implement from scratch, we would need to tokenize and evaluate respecting operator precedence. try: result = eval(expression) except Exception as e: result = str(e) return str(result)"},{"question":"# PyTorch CUDA Device Management and Stream Manipulation Problem Statement You are tasked with creating a utility function to manage and perform computations on CUDA devices using streams for parallel operations. Your function will need to: 1. Select a specific GPU device. 2. Perform computations on multiple streams on this device. 3. Track and report memory usage before and after computation. 4. Ensure synchronization between streams. Function Specification **Function Name:** `gpu_computation_with_streams` **Inputs:** - `device_id` (int): The ID of the CUDA device to be used. - `task_list` (list of callables): List of functions (without arguments) representing computational tasks to be performed on the GPU. - `num_streams` (int): Number of CUDA streams to utilize for parallel computation. **Output:** - A dictionary with the following entries: - `initial_memory_allocated` (int): Memory allocated on the device before starting the computation (in bytes). - `final_memory_allocated` (int): Memory allocated on the device after computation completes (in bytes). - `stream_execution_time` (float): Total time taken to execute all tasks across streams (in milliseconds). **Constraints:** - Ensure the device specified by `device_id` is selected and used. - Create and manage CUDA streams to run tasks in parallel. - Synchronize the streams before measuring the final memory usage. **Performance Requirements:** - Efficiently utilize streams to parallelize the provided tasks. - Minimize overhead due to memory tracking and synchronization. Implementation Implement the function `gpu_computation_with_streams` using PyTorch\'s CUDA API. ```python import torch import time def gpu_computation_with_streams(device_id, task_list, num_streams): assert torch.cuda.is_available(), \\"CUDA must be available on the system.\\" assert 0 <= device_id < torch.cuda.device_count(), f\\"Invalid device_id {device_id}.\\" # Set the device torch.cuda.set_device(device_id) # Create streams streams = [torch.cuda.Stream() for _ in range(num_streams)] # Initial memory allocation initial_memory_allocated = torch.cuda.memory_allocated(device_id) # Record start time start_time = time.time() # Assign tasks to streams for i, task in enumerate(task_list): stream = streams[i % num_streams] with torch.cuda.stream(stream): task() # Synchronize all streams for stream in streams: stream.synchronize() # Record end time end_time = time.time() # Final memory allocation final_memory_allocated = torch.cuda.memory_allocated(device_id) # Calculate execution time stream_execution_time = (end_time - start_time) * 1000 # Convert to milliseconds return { \\"initial_memory_allocated\\": initial_memory_allocated, \\"final_memory_allocated\\": final_memory_allocated, \\"stream_execution_time\\": stream_execution_time } # Example usage: # Ensure you have CUDA-capable device and the necessary CUDA toolkit installed. # Define some example tasks def task1(): a = torch.randn(1000, 1000, device=\'cuda\') b = torch.randn(1000, 1000, device=\'cuda\') c = torch.matmul(a, b) def task2(): a = torch.randn(2000, 2000, device=\'cuda\') b = torch.randn(2000, 2000, device=\'cuda\') c = torch.matmul(a, b) # Call the function with tasks and device ID result = gpu_computation_with_streams(0, [task1, task2], 2) print(result) ``` This function will help in understanding CUDA device management, stream manipulation, and memory tracking in PyTorch.","solution":"import torch import time def gpu_computation_with_streams(device_id, task_list, num_streams): assert torch.cuda.is_available(), \\"CUDA must be available on the system.\\" assert 0 <= device_id < torch.cuda.device_count(), f\\"Invalid device_id {device_id}.\\" # Set the device torch.cuda.set_device(device_id) # Create streams streams = [torch.cuda.Stream() for _ in range(num_streams)] # Initial memory allocation initial_memory_allocated = torch.cuda.memory_allocated(device_id) # Record start time start_time = time.time() # Assign tasks to streams for i, task in enumerate(task_list): stream = streams[i % num_streams] with torch.cuda.stream(stream): task() # Synchronize all streams for stream in streams: stream.synchronize() # Record end time end_time = time.time() # Final memory allocation final_memory_allocated = torch.cuda.memory_allocated(device_id) # Calculate execution time stream_execution_time = (end_time - start_time) * 1000 # Convert to milliseconds return { \\"initial_memory_allocated\\": initial_memory_allocated, \\"final_memory_allocated\\": final_memory_allocated, \\"stream_execution_time\\": stream_execution_time }"},{"question":"# Question: **Configuration File Management with `configparser`** You are tasked to implement a configuration management system using the `configparser` module in Python. The system should be able to: 1. Read a given configuration file. 2. Retrieve a configuration value with type conversion and fallback options. 3. Add or update a section and corresponding options. 4. Write the updated configuration back to a file. 5. Handle any errors gracefully and log appropriate messages. **Function Signature and Requirements:** 1. **Function Name**: - `manage_config` 2. **Parameters**: - `input_file` (str): Path to the input configuration file. - `output_file` (str): Path to the output configuration file where the updated configuration should be written. - `section` (str): The section to be added or updated. - `options` (dict): A dictionary of key-value pairs to be added or updated in the specified section. - `retrieve_section` (str): The section from which to retrieve any configuration value. - `retrieve_option` (str): The option key whose value needs to be retrieved. - `convert_type` (str): Type to which the retrieved value needs to be converted. One of \'int\', \'float\', \'boolean\'. - `fallback` (str): A fallback value to return if the option or section is missing. 3. **Returns**: - `converted_value`: The value retrieved from the configuration file, converted to the specified type. If the conversion fails or the value is non-existent, return the fallback value. 4. **Implementation Requirements**: - Use `ConfigParser` to handle reading and writing the configuration file. - Use appropriate methods from `ConfigParser` to read, write, and update configuration values. - Implement type conversion and fallback mechanism using appropriate `get*()` methods. - Ensure error handling and logging using Python\'s `logging` module. **Example:** ```python def manage_config(input_file: str, output_file: str, section: str, options: dict, retrieve_section: str, retrieve_option: str, convert_type: str, fallback: str): # Initialize the ConfigParser instance config = configparser.ConfigParser() # Read the configuration file config.read(input_file) # Add or update the specified section with given options if section not in config.sections(): config.add_section(section) for key, value in options.items(): config.set(section, key, value) # Retrieve and convert the value from the specified section and option try: if convert_type == \'int\': converted_value = config.getint(retrieve_section, retrieve_option, fallback=fallback) elif convert_type == \'float\': converted_value = config.getfloat(retrieve_section, retrieve_option, fallback=fallback) elif convert_type == \'boolean\': converted_value = config.getboolean(retrieve_section, retrieve_option, fallback=fallback) else: converted_value = config.get(retrieve_section, retrieve_option, fallback=fallback) except (configparser.NoSectionError, configparser.NoOptionError, ValueError) as e: logging.error(f\\"Error retrieving or converting option: {e}\\") return fallback # Write the updated configuration back to file with open(output_file, \'w\') as configfile: config.write(configfile) return converted_value # Usage Example print(manage_config(\'input.ini\', \'output.ini\', \'new_section\', {\'key1\': \'value1\'}, \'retrieve_section\', \'retrieve_option\', \'int\', \'10\')) ``` Ensure to handle all edge cases, such as missing sections/options, invalid value types, and any read/write file issues.","solution":"import configparser import logging def manage_config(input_file: str, output_file: str, section: str, options: dict, retrieve_section: str, retrieve_option: str, convert_type: str, fallback: str): # Initialize the ConfigParser instance config = configparser.ConfigParser() # Read the configuration file try: config.read(input_file) except Exception as e: logging.error(f\\"Error reading the configuration file: {e}\\") return fallback # Add or update the specified section with given options if section not in config.sections(): config.add_section(section) for key, value in options.items(): config.set(section, key, value) # Retrieve and convert the value from the specified section and option try: if convert_type == \'int\': converted_value = config.getint(retrieve_section, retrieve_option, fallback=fallback) elif convert_type == \'float\': converted_value = config.getfloat(retrieve_section, retrieve_option, fallback=fallback) elif convert_type == \'boolean\': converted_value = config.getboolean(retrieve_section, retrieve_option, fallback=fallback) else: converted_value = config.get(retrieve_section, retrieve_option, fallback=fallback) except (configparser.NoSectionError, configparser.NoOptionError, ValueError) as e: logging.error(f\\"Error retrieving or converting option: {e}\\") return fallback # Write the updated configuration back to file try: with open(output_file, \'w\') as configfile: config.write(configfile) except Exception as e: logging.error(f\\"Error writing the configuration file: {e}\\") return fallback return converted_value"},{"question":"# Advanced Coding Assessment Question Objective: Implement a custom serializer and deserializer for a Python class using the `pickle` module, demonstrating your understanding of advanced `pickle` functionalities, including handling non-standard attributes and ensuring efficient serialization. Problem Statement: You are tasked with designing a Python class that manages a collection of user-defined objects. Your class should include methods to serialize and deserialize these objects to and from a binary file using the `pickle` module. Additionally, you should implement custom handling for a specific attribute to optimize the pickling process. Class Specification: 1. Create a class `UserCollection` that: - Holds a collection of user objects. - Each user object is represented as a dictionary with the following attributes: ```python user = { \\"id\\": int, \\"name\\": str, \\"email\\": str, \\"metadata\\": bytes # Large binary data } ``` - Implements methods to add, remove, and retrieve user objects. 2. Implement the following methods within `UserCollection`: - `serialize_to_file(self, file_path)`: Serialize the entire collection to a binary file. - `deserialize_from_file(cls, file_path)`: Deserialize the collection from a binary file. 3. Custom Serialization Handling: - For the `metadata` field within each user object, implement custom handling to serialize this attribute out-of-band if the size exceeds a certain threshold (e.g., 1024 bytes). Constraints: - You must use Python\'s `pickle` module for serialization and deserialization. - The `metadata` attribute should be handled efficiently, minimizing memory usage as much as possible for large data. Example Usage: ```python # Sample implementation and usage user1 = {\\"id\\": 1, \\"name\\": \\"Alice\\", \\"email\\": \\"alice@example.com\\", \\"metadata\\": b\\"Some large binary data...\\"} user2 = {\\"id\\": 2, \\"name\\": \\"Bob\\", \\"email\\": \\"bob@example.com\\", \\"metadata\\": b\\"Some large binary data...\\"} # Creating instance and adding users collection = UserCollection() collection.add_user(user1) collection.add_user(user2) # Serializing to file collection.serialize_to_file(\\"users.pkl\\") # Deserializing from file new_collection = UserCollection.deserialize_from_file(\\"users.pkl\\") ``` Additional Notes: - Ensure the class is robust and handles any potential exceptions during serialization/deserialization. - Provide comments and documentation for clarity. Write your implementation below: ```python import pickle class UserCollection: def __init__(self): self.users = [] def add_user(self, user): self.users.append(user) def remove_user(self, user_id): self.users = [user for user in self.users if user[\'id\'] != user_id] def get_user(self, user_id): for user in self.users: if user[\'id\'] == user_id: return user return None def serialize_to_file(self, file_path): with open(file_path, \'wb\') as f: pickle.dump(self.users, f, protocol=pickle.HIGHEST_PROTOCOL, buffer_callback=self._buffer_callback) def _buffer_callback(self, buf): pass # Implement your custom buffer handling logic here @classmethod def deserialize_from_file(cls, file_path): with open(file_path, \'rb\') as f: users = pickle.load(f, buffers=[]) # Handle buffers appropriately instance = cls() instance.users = users return instance ``` Complete the `UserCollection` class by properly implementing the custom buffer handling logic inside `_buffer_callback` and modify the deserialization process to correctly interpret the out-of-band data.","solution":"import pickle import os class UserCollection: def __init__(self): self.users = [] def add_user(self, user): self.users.append(user) def remove_user(self, user_id): self.users = [user for user in self.users if user[\'id\'] != user_id] def get_user(self, user_id): for user in self.users: if user[\'id\'] == user_id: return user return None def serialize_to_file(self, file_path): with open(file_path, \'wb\') as f: for user in self.users: if len(user[\\"metadata\\"]) > 1024: metadata_path = f\\"{file_path}_{user[\'id\']}_metadata.bin\\" with open(metadata_path, \'wb\') as meta_f: meta_f.write(user[\\"metadata\\"]) user[\\"metadata\\"] = metadata_path pickle.dump(self.users, f, protocol=pickle.HIGHEST_PROTOCOL, fix_imports=True) @classmethod def deserialize_from_file(cls, file_path): with open(file_path, \'rb\') as f: users = pickle.load(f, fix_imports=True) for user in users: if isinstance(user[\\"metadata\\"], str) and os.path.exists(user[\\"metadata\\"]): with open(user[\\"metadata\\"], \'rb\') as meta_f: user[\\"metadata\\"] = meta_f.read() instance = cls() instance.users = users return instance"},{"question":"Implement a Python class `SiteManager` that simulates the behavior of the `site` module, focusing on managing site-specific directories and custom `.pth` files. The class should provide functionalities to add and list site directories, process `.pth` files, and enable or disable user site-packages. You are required to implement the following methods in your class: 1. **add_site_directory(path: str) -> None**: Adds a site directory to the internal list if it exists. 2. **list_site_directories() -> List[str]**: Returns a list of all site directories managed by the class. 3. **process_pth_file(pth_filepath: str) -> None**: Reads the specified `.pth` file and updates the internal list of site directories accordingly. The `.pth` file contains paths (one per line) or executable lines prefixed with `import`. 4. **enable_user_site() -> None**: Enables user site-packages. 5. **disable_user_site() -> None**: Disables user site-packages. 6. **is_user_site_enabled() -> bool**: Returns `True` if user site-packages are enabled, otherwise `False`. You may use `os` and `sys` modules where needed. The internal lists of site directories should be managed carefully to avoid duplicates. Input and Output Formats: - **add_site_directory(path: str)**: - *Input*: A string representing the path to the site directory. - *Output*: None. - **list_site_directories()**: - *Input*: None. - *Output*: A list of strings representing all managed site directories. - **process_pth_file(pth_filepath: str)**: - *Input*: A string representing the path to the `.pth` file. - *Output*: None. - **enable_user_site()**: - *Input*: None. - *Output*: None. - **disable_user_site()**: - *Input*: None. - *Output*: None. - **is_user_site_enabled()**: - *Input*: None. - *Output*: A boolean indicating if user site-packages are enabled or not. Constraints: - The path added must exist on the file system. - The `.pth` file format should follow the conventions described in the documentation. - Handle file reading exceptions properly. - Ensure no duplicate paths in the internal lists. Example Usage: ```python import tempfile import os # Example .pth file content pth_content = # Sample .pth file /site/foo /site/bar import mymodule # Create a temporary .pth file with tempfile.NamedTemporaryFile(delete=False) as temp: temp.write(pth_content.encode()) pth_filepath = temp.name # Create a SiteManager instance and process the .pth file manager = SiteManager() manager.process_pth_file(pth_filepath) print(manager.list_site_directories()) # Expected: [\'/site/foo\', \'/site/bar\'] manager.enable_user_site() print(manager.is_user_site_enabled()) # Expected: True manager.disable_user_site() print(manager.is_user_site_enabled()) # Expected: False # Clean up temporary .pth file os.remove(pth_filepath) ```","solution":"import os class SiteManager: def __init__(self): self.site_directories = [] self.user_site_enabled = False def add_site_directory(self, path: str) -> None: if os.path.exists(path): if path not in self.site_directories: self.site_directories.append(path) def list_site_directories(self) -> list: return self.site_directories def process_pth_file(self, pth_filepath: str) -> None: try: with open(pth_filepath, \'r\') as file: for line in file: line = line.strip() if line and not line.startswith(\'import\'): self.add_site_directory(line) except FileNotFoundError: print(f\\"The file {pth_filepath} does not exist.\\") def enable_user_site(self) -> None: self.user_site_enabled = True def disable_user_site(self) -> None: self.user_site_enabled = False def is_user_site_enabled(self) -> bool: return self.user_site_enabled"},{"question":"# Problem: Advanced Handling and Analysis of Categorical Data You are provided with a dataset containing survey responses, which includes categorical data. Your task is to load this dataset into a pandas DataFrame, convert specific columns to categorical data type, and perform various advanced operations on these categorical columns. The dataset sample (CSV format) looks like this: ``` RespondentID, Age, Gender, Satisfaction, Country 1, 25, Male, Very satisfied, USA 2, 34, Female, Satisfied, UK 3, 29, Male, Neutral, USA 4, 42, Female, Dissatisfied, Australia 5, 31, Female, Very satisfied, Canada ... ``` # Requirements 1. **Load the Dataset**: - Load the dataset into a pandas DataFrame. 2. **Convert Columns to Categorical Data Type**: - Convert the `Gender`, `Satisfaction`, and `Country` columns to categorical data type. - Ensure the `Satisfaction` column is ordered as follows: \\"Very dissatisfied\\", \\"Dissatisfied\\", \\"Neutral\\", \\"Satisfied\\", \\"Very satisfied\\". 3. **Analyze the Categorical Data**: - Count the number of respondents for each satisfaction level, including those that may not be present in the data. - Print the number of unique categories for each of the categorical columns. 4. **Modify Categories**: - Rename the categories in the `Satisfaction` column to include a prefix \\"Level-\\" (e.g., \\"Level-Very dissatisfied\\"). - Add a new category \\"Not Applicable\\" to the `Satisfaction` column without altering the existing data. 5. **Statistical Analysis**: - Identify and print the most and least frequent categories in the `Country` column. - Compare the average age of respondents between those who are `Satisfied` or above (`Satisfied`, `Very satisfied`) and those who are below `Satisfied` (`Very dissatisfied`, `Dissatisfied`, `Neutral`). # Constraints - You should handle missing values (`np.nan`) appropriately in your categorical data. - The dataset might have additional columns or rows, but you should only focus on the specified columns for categorical data operations. # Expected Output - A dictionary with counts of respondents for each satisfaction level. - The number of unique categories for each categorical column. - List of renamed categories in the `Satisfaction` column. - List of categories in the `Satisfaction` column after adding \\"Not Applicable\\". - The most and least frequent categories in the `Country` column. - Average age comparison results based on the satisfaction groups described. # Code Skeleton ```python import pandas as pd def analyze_categorical_data(file_path: str): # Load the dataset df = pd.read_csv(file_path) # Convert columns to categorical data type df[\'Gender\'] = df[\'Gender\'].astype(\'category\') df[\'Satisfaction\'] = pd.Categorical( df[\'Satisfaction\'], categories=[\\"Very dissatisfied\\", \\"Dissatisfied\\", \\"Neutral\\", \\"Satisfied\\", \\"Very satisfied\\"], ordered=True ) df[\'Country\'] = df[\'Country\'].astype(\'category\') # Count respondents for each satisfaction level satisfaction_counts = df[\'Satisfaction\'].value_counts(dropna=False) # Print unique categories for each categorical column unique_categories = { \'Gender\': len(df[\'Gender\'].cat.categories), \'Satisfaction\': len(df[\'Satisfaction\'].cat.categories), \'Country\': len(df[\'Country\'].cat.categories) } # Rename categories in Satisfaction column new_satisfaction_categories = [\\"Level-\\" + str(cat) for cat in df[\'Satisfaction\'].cat.categories] df[\'Satisfaction\'] = df[\'Satisfaction\'].cat.rename_categories(new_satisfaction_categories) # Add new category to Satisfaction column df[\'Satisfaction\'] = df[\'Satisfaction\'].cat.add_categories([\'Not Applicable\']) # Most and least frequent categories in Country column country_counts = df[\'Country\'].value_counts() most_frequent_country = country_counts.idxmax() least_frequent_country = country_counts.idxmin() # Average age comparison based on Satisfaction levels satisfied_above = df[df[\'Satisfaction\'].isin([\'Level-Satisfied\', \'Level-Very satisfied\'])] below_satisfied = df[df[\'Satisfaction\'].isin([\'Level-Very dissatisfied\', \'Level-Dissatisfied\', \'Level-Neutral\'])] avg_age_satisfied_above = satisfied_above[\'Age\'].mean() avg_age_below_satisfied = below_satisfied[\'Age\'].mean() # Return results return { \'satisfaction_counts\': satisfaction_counts.to_dict(), \'unique_categories\': unique_categories, \'new_satisfaction_categories\': new_satisfaction_categories, \'satisfaction_categories_after_adding\': df[\'Satisfaction\'].cat.categories.tolist(), \'most_frequent_country\': most_frequent_country, \'least_frequent_country\': least_frequent_country, \'avg_age_comparison\': { \'satisfied_and_above\': avg_age_satisfied_above, \'below_satisfied\': avg_age_below_satisfied } } # Example usage: # results = analyze_categorical_data(\\"survey_data.csv\\") ```","solution":"import pandas as pd def analyze_categorical_data(file_path: str): # Load the dataset df = pd.read_csv(file_path) # Convert columns to categorical data type df[\'Gender\'] = df[\'Gender\'].astype(\'category\') df[\'Satisfaction\'] = pd.Categorical( df[\'Satisfaction\'], categories=[\\"Very dissatisfied\\", \\"Dissatisfied\\", \\"Neutral\\", \\"Satisfied\\", \\"Very satisfied\\"], ordered=True ) df[\'Country\'] = df[\'Country\'].astype(\'category\') # Count respondents for each satisfaction level satisfaction_counts = df[\'Satisfaction\'].value_counts(sort=False).reindex(df[\'Satisfaction\'].cat.categories) # Print unique categories for each categorical column unique_categories = { \'Gender\': len(df[\'Gender\'].cat.categories), \'Satisfaction\': len(df[\'Satisfaction\'].cat.categories), \'Country\': len(df[\'Country\'].cat.categories) } # Rename categories in Satisfaction column new_satisfaction_categories = [\\"Level-\\" + str(cat) for cat in df[\'Satisfaction\'].cat.categories] df[\'Satisfaction\'] = df[\'Satisfaction\'].cat.rename_categories(new_satisfaction_categories) # Add new category to Satisfaction column df[\'Satisfaction\'] = df[\'Satisfaction\'].cat.add_categories([\'Not Applicable\']) # Most and least frequent categories in Country column country_counts = df[\'Country\'].value_counts() most_frequent_country = country_counts.idxmax() least_frequent_country = country_counts.idxmin() # Average age comparison based on Satisfaction levels satisfied_above = df[df[\'Satisfaction\'].isin([\'Level-Satisfied\', \'Level-Very satisfied\'])] below_satisfied = df[df[\'Satisfaction\'].isin([\'Level-Very dissatisfied\', \'Level-Dissatisfied\', \'Level-Neutral\'])] avg_age_satisfied_above = satisfied_above[\'Age\'].mean() avg_age_below_satisfied = below_satisfied[\'Age\'].mean() # Return results return { \'satisfaction_counts\': satisfaction_counts.to_dict(), \'unique_categories\': unique_categories, \'new_satisfaction_categories\': new_satisfaction_categories, \'satisfaction_categories_after_adding\': df[\'Satisfaction\'].cat.categories.tolist(), \'most_frequent_country\': most_frequent_country, \'least_frequent_country\': least_frequent_country, \'avg_age_comparison\': { \'satisfied_and_above\': avg_age_satisfied_above, \'below_satisfied\': avg_age_below_satisfied } } # Example usage: # results = analyze_categorical_data(\\"survey_data.csv\\")"},{"question":"# Problem You are tasked with implementing a utility function to compress multiple text files into individual gzip files. Each compressed file should be named after the original file with the `.gz` extension appended. Additionally, you should handle exceptions gracefully, ensuring that if any file fails to compress, an appropriate error message is logged, but the process continues for other files. The function should be named `compress_files` and have the following signature: ```python def compress_files(file_paths: List[str], compresslevel: int = 9) -> None: ``` # Input - `file_paths`: A list of strings where each string is the path to a text file that needs to be compressed. - `compresslevel`: An integer from 0 to 9 that determines the level of compression, with 0 being no compression and 9 being the maximum compression (default is 9). # Output - The function does not return any value. However, it should create gzipped files for each input file in the same directory as the original file, with the `.gz` extension appended to the original filename. # Example Suppose you have two text files: `document1.txt` and `document2.txt`. Calling `compress_files([\\"/path/to/document1.txt\\", \\"/path/to/document2.txt\\"])` should produce two compressed files: - `/path/to/document1.txt.gz` - `/path/to/document2.txt.gz` # Constraints - Each file listed in `file_paths` exists and is accessible. - You should gracefully handle exceptions (like file not found or write errors) and log appropriate error messages without halting the entire process. # Notes 1. The function should employ the `gzip.open` method described in the documentation to perform the compression. 2. Ensure you handle file operations using context managers (`with` statement) to avoid potential resource leaks. 3. Demonstrate good error handling by catching potential exceptions and logging meaningful error messages. # Performance Requirements - It should efficiently handle up to hundreds of files. - Compression performance and memory usage can vary with the specified compression level. ```python import gzip import os from typing import List def compress_files(file_paths: List[str], compresslevel: int = 9) -> None: for file_path in file_paths: try: # Open the original file for reading in binary mode with open(file_path, \'rb\') as f_in: # Open or create the corresponding gzip file for writing with gzip.open(f\\"{file_path}.gz\\", \'wb\', compresslevel=compresslevel) as f_out: # Copy the file content to the gzip file shutil.copyfileobj(f_in, f_out) except Exception as e: print(f\\"Error compressing {file_path}: {e}\\") ```","solution":"import gzip import os import shutil from typing import List def compress_files(file_paths: List[str], compresslevel: int = 9) -> None: for file_path in file_paths: try: # Open the original file for reading in binary mode with open(file_path, \'rb\') as f_in: # Open or create the corresponding gzip file for writing with gzip.open(f\\"{file_path}.gz\\", \'wb\', compresslevel=compresslevel) as f_out: # Copy the file content to the gzip file shutil.copyfileobj(f_in, f_out) except Exception as e: print(f\\"Error compressing {file_path}: {e}\\")"},{"question":"File Descriptor Operations and Lock Management **Objective:** To evaluate the student\'s understanding of the `fcntl` module in Python, how to handle file descriptors, and manage file operations and locks. **Problem Statement:** You\'ve been tasked with developing a utility function to manage file locks using the `fcntl` module. This function should allow users to lock a file, perform read/write operations in a controlled manner, and then release the lock. Your function should ensure that the file lock is properly managed to prevent data corruption or race conditions. **Function Signature:** ```python def manage_file_lock(file_path: str, data: str = None) -> str: Manages a file lock for performing read/write operations on a file. Args: - file_path (str): The path to the file to be locked. - data (str): The data to be written to the file if specified. If None, the function reads data from the file. Returns: - str: The data read from the file if `data` is None. Otherwise, an empty string. Raises: - OSError: If locking or file operations fail. ``` **Requirements:** 1. The function should open the specified file in read and write mode. 2. Apply an exclusive lock on the file using `fcntl.lockf()`. 3. If `data` is provided, write this data to the file. Ensure that you handle the existing content appropriately. 4. If `data` is `None`, read and return the content of the file. 5. Release the lock after performing the read/write operation. 6. Ensure that any exceptions (e.g., locking failures) are appropriately handled and raised. **Constraints:** 1. Assume the file is not larger than 1MB. 2. The function should handle text files encoded in UTF-8. 3. The lock should be held only for the duration of the read/write operation to maximize concurrent access from multiple processes. **Example Usage:** ```python # Writing to the file manage_file_lock(\'example.txt\', \'Hello, World!\') # Reading from the file content = manage_file_lock(\'example.txt\') print(content) # Should print \'Hello, World!\' ``` **Notes:** - Ensure the function is efficient and handles all edge cases, such as empty files and error scenarios (file not found, permission issues). - Use appropriate functions from the `fcntl` and `os` modules to manage file operations.","solution":"import fcntl import os def manage_file_lock(file_path: str, data: str = None) -> str: Manages a file lock for performing read/write operations on a file. Args: - file_path (str): The path to the file to be locked. - data (str): The data to be written to the file if specified. If None, the function reads data from the file. Returns: - str: The data read from the file if `data` is None. Otherwise, an empty string. Raises: - OSError: If locking or file operations fail. try: with open(file_path, \'r+\' if data is None else \'w+\') as f: # Apply an exclusive lock fcntl.lockf(f, fcntl.LOCK_EX) if data is not None: # Write data to the file, truncate the file first f.seek(0) f.write(data) f.truncate() result = \\"\\" else: # Read and return the data from the file f.seek(0) result = f.read() # Release the lock fcntl.lockf(f, fcntl.LOCK_UN) return result except Exception as e: raise OSError(f\\"Failed to manage file lock: {e}\\")"},{"question":"# Advanced File I/O Handling You are given a set of text files that contain sensor data. Each file contains multiple lines of comma-separated values where the first value is a timestamp and the subsequent values are sensor readings. Your task is to write a Python function named `process_sensor_data` that takes the directory path of these files and processes the data as follows: 1. Reads each file in the specified directory. 2. Aggregates the data from all files. 3. Writes a new file named `aggregated_data.csv` with the combined data in a sorted manner based on the timestamps. 4. Use buffering and handle different encodings gracefully, defaulting to `utf-8`. The signature of the function should be: ```python def process_sensor_data(directory_path: str, encoding: str = \'utf-8\') -> None: ``` # Input - `directory_path`: A string representing the path of the directory containing the text files. - `encoding`: An optional string specifying the file encoding. Default is `\'utf-8\'`. # Output - The function creates a file named `aggregated_data.csv` in the specified directory containing all the data sorted by timestamp. # Constraints - You must handle files that may be large, ensuring your implementation is efficient in terms of memory usage. - Use `io` module classes such as `TextIOWrapper`, `BufferedReader`, and `BufferedWriter` for efficient file handling. # Example Given the directory with the following files and contents: file1.txt ``` 2023-01-01 10:00:00, 23.4, 45.6 2023-01-01 10:05:00, 22.1, 47.2 ``` file2.txt ``` 2023-01-01 10:02:00, 23.0, 46.0 2023-01-01 10:10:00, 21.9, 48.1 ``` The resulting `aggregated_data.csv` should contain: ``` 2023-01-01 10:00:00, 23.4, 45.6 2023-01-01 10:02:00, 23.0, 46.0 2023-01-01 10:05:00, 22.1, 47.2 2023-01-01 10:10:00, 21.9, 48.1 ``` # Hints - Use `os.listdir` to list all files in the directory. - Use `sorted` with a custom key to sort the data by timestamps. - Properly handle any potential I/O errors, ensuring that you catch and log exceptions where appropriate.","solution":"import os import io from datetime import datetime def process_sensor_data(directory_path: str, encoding: str = \'utf-8\') -> None: all_data = [] # Iterate over each file in the directory for filename in os.listdir(directory_path): file_path = os.path.join(directory_path, filename) # Check if it is a file before processing if os.path.isfile(file_path): with io.open(file_path, \'r\', encoding=encoding) as file: for line in file: line = line.strip() if line: # Ensure the line is not empty all_data.append(line) # Sort all data based on timestamp all_data.sort(key=lambda x: datetime.strptime(x.split(\',\')[0], \'%Y-%m-%d %H:%M:%S\')) # Write the sorted data to the aggregated_data.csv file output_file_path = os.path.join(directory_path, \'aggregated_data.csv\') with io.open(output_file_path, \'w\', encoding=encoding, buffering=1024) as output_file: for line in all_data: output_file.write(line + \'n\')"},{"question":"Objective: To assess the students\' understanding of creating and manipulating Series and DataFrames, indexing and selection, handling missing data, and performing vectorized operations in pandas. Problem Statement: You are provided with data representing the monthly sales figures of various product categories over a year along with information about promotional campaigns. 1. Create two DataFrames: - **sales_df**: containing the sales data with columns `Month (str)`, `Electronics (float)`, `Clothing (float)`, `Groceries (float)`. - **promo_df**: containing the promotional campaigns with columns `Month (str)`, `Electronics_Promo (float)`, `Clothing_Promo (float)`. Use the following data: ```python sales_data = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"], \\"Electronics\\": [2000, 2100, 1950, 2200, 2400, 2300, 2500, 2700, 2600, 2900, 3000, 3100], \\"Clothing\\": [1500, 1600, 1550, 1700, 1650, 1750, 1800, 1850, 1900, 2000, 2100, 2200], \\"Groceries\\": [3000, 3100, 3200, 3300, 3400, 3450, 3500, 3550, 3600, 3700, 3800, 3900], } promo_data = { \\"Month\\": [\\"Feb\\", \\"Apr\\", \\"Jun\\", \\"Aug\\", \\"Oct\\", \\"Dec\\"], \\"Electronics_Promo\\": [10, 15, 20, 25, 30, 35], \\"Clothing_Promo\\": [5, 10, 15, 20, 25, 30], } ``` 2. Combine both DataFrames into one so that missing data is appropriately handled. Name this combined DataFrame as `combined_df`. 3. Calculate the effective sales after applying the promotional campaigns and add these as new columns, named `Electronics_Effective` and `Clothing_Effective`, in the `combined_df`. If there is no promotional data for a month, use the original sales figures. 4. Print the following: - Total sales in each category before applying the promotions. - Total sales in each category after applying the promotions. Constraints: - Ensure that the calculations handle any missing data gracefully. - Use vectorized operations for computing the effective sales. Input: - The predefined `sales_data` and `promo_data` dictionaries. Output: Print statements that display: - The total sales for `Electronics`, `Clothing`, and `Groceries` before promotions. - The total sales for `Electronics` and `Clothing` after promotions. Solution Template: ```python import pandas as pd # Step 1: Create sales_df and promo_df sales_data = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"], \\"Electronics\\": [2000, 2100, 1950, 2200, 2400, 2300, 2500, 2700, 2600, 2900, 3000, 3100], \\"Clothing\\": [1500, 1600, 1550, 1700, 1650, 1750, 1800, 1850, 1900, 2000, 2100, 2200], \\"Groceries\\": [3000, 3100, 3200, 3300, 3400, 3450, 3500, 3550, 3600, 3700, 3800, 3900], } promo_data = { \\"Month\\": [\\"Feb\\", \\"Apr\\", \\"Jun\\", \\"Aug\\", \\"Oct\\", \\"Dec\\"], \\"Electronics_Promo\\": [10, 15, 20, 25, 30, 35], \\"Clothing_Promo\\": [5, 10, 15, 20, 25, 30], } # Implement your solution below following the steps outlined in the problem statement. # Your solution must correctly address all the described tasks. ```","solution":"import pandas as pd # Step 1: Create sales_df and promo_df sales_data = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"], \\"Electronics\\": [2000, 2100, 1950, 2200, 2400, 2300, 2500, 2700, 2600, 2900, 3000, 3100], \\"Clothing\\": [1500, 1600, 1550, 1700, 1650, 1750, 1800, 1850, 1900, 2000, 2100, 2200], \\"Groceries\\": [3000, 3100, 3200, 3300, 3400, 3450, 3500, 3550, 3600, 3700, 3800, 3900], } promo_data = { \\"Month\\": [\\"Feb\\", \\"Apr\\", \\"Jun\\", \\"Aug\\", \\"Oct\\", \\"Dec\\"], \\"Electronics_Promo\\": [10, 15, 20, 25, 30, 35], \\"Clothing_Promo\\": [5, 10, 15, 20, 25, 30], } sales_df = pd.DataFrame(sales_data) promo_df = pd.DataFrame(promo_data) # Step 2: Combine both DataFrames into one combined_df = pd.merge(sales_df, promo_df, on=\'Month\', how=\'left\') # Step 3: Calculate the effective sales after applying the promotional campaigns combined_df[\'Electronics_Effective\'] = combined_df.apply( lambda row: row[\'Electronics\'] * (1 - row[\'Electronics_Promo\'] / 100) if pd.notnull(row[\'Electronics_Promo\']) else row[\'Electronics\'], axis=1) combined_df[\'Clothing_Effective\'] = combined_df.apply( lambda row: row[\'Clothing\'] * (1 - row[\'Clothing_Promo\'] / 100) if pd.notnull(row[\'Clothing_Promo\']) else row[\'Clothing\'], axis=1) # Calculating total sales before promotions total_sales_before = { \'Electronics\': combined_df[\'Electronics\'].sum(), \'Clothing\': combined_df[\'Clothing\'].sum(), \'Groceries\': combined_df[\'Groceries\'].sum(), } # Calculating total sales after promotions total_sales_after = { \'Electronics\': combined_df[\'Electronics_Effective\'].sum(), \'Clothing\': combined_df[\'Clothing_Effective\'].sum() } # Print results print(\\"Total sales before promotions:\\") print(f\\"Electronics: {total_sales_before[\'Electronics\']}\\") print(f\\"Clothing: {total_sales_before[\'Clothing\']}\\") print(f\\"Groceries: {total_sales_before[\'Groceries\']}\\") print(\\"nTotal sales after promotions:\\") print(f\\"Electronics: {total_sales_after[\'Electronics\']}\\") print(f\\"Clothing: {total_sales_after[\'Clothing\']}\\")"},{"question":"# Handling Exceptions and Resource Management **Objective:** Write a function that processes a list of file paths and extracts integers from each file. The function should handle various exceptions, ensure resources are properly cleaned up, and use custom exception classes. **Task:** 1. **Define Custom Exceptions:** - Define a custom exception `FileProcessingError` that inherits from `Exception`. - Define another custom exception `NotAnIntegerError` that inherits from `FileProcessingError`. 2. **Function Definition:** Define a function `process_files(file_paths)`, where: - `file_paths`: A list of strings representing file paths. 3. The function should: - Attempt to open each file in the list. - Read each line in the file and convert it to an integer. - If a file cannot be opened, raise `FileProcessingError`. - If a line in the file cannot be converted to an integer, raise `NotAnIntegerError`. - Use a `finally` clause to ensure the file is properly closed after attempting to process it. 4. **Error Handling:** - Use a `try`...`except` block to handle `FileProcessingError` and `NotAnIntegerError`. - Print an appropriate message for both custom exceptions when they are caught. 5. **Clean-up Actions:** - Use the `with` statement to handle the file to ensure that it is always closed properly. **Constraints:** - Assume the files contain either valid integers or strings that cannot be converted to integers. - Assume the list contains valid file paths or paths to non-existent files. **Function Signature:** ```python def process_files(file_paths: list) -> None: pass ``` **Example:** ```python # Assume we have two files: # file1.txt contains: # 1 # 2 # three # file2.txt contains: # 10 # twenty # 30 file_paths = [\'file1.txt\', \'non_existent_file.txt\', \'file2.txt\'] process_files(file_paths) # Expected Output: # FileProcessingError: [Errno 2] No such file or directory: \'non_existent_file.txt\' # NotAnIntegerError: Cannot convert \'three\' to an integer in file1.txt # NotAnIntegerError: Cannot convert \'twenty\' to an integer in file2.txt ``` Ensure your implementation meets the requirements and handles exceptions appropriately.","solution":"class FileProcessingError(Exception): pass class NotAnIntegerError(FileProcessingError): def __init__(self, value, file_path): self.value = value self.file_path = file_path super().__init__(f\\"Cannot convert \'{value}\' to an integer in \'{file_path}\'\\") def process_files(file_paths): for file_path in file_paths: try: with open(file_path, \'r\') as file: for line in file: try: number = int(line.strip()) except ValueError: raise NotAnIntegerError(line.strip(), file_path) except FileNotFoundError as e: raise FileProcessingError(e) except NotAnIntegerError as e: raise e"},{"question":"**Objective**: This exercise aims to test your ability to use the `email.charset` module effectively by implementing a function that encodes email headers and bodies based on specified character sets. **Problem Statement**: You are given an input string, an input character set, and two flags indicating whether the string is to be used in an email header or body. Your task is to write a function that encodes the input string appropriately using the `email.charset.Charset` class. **Function Signature**: ```python def encode_email_content(input_string: str, input_charset: str, is_header: bool, is_body: bool) -> str: pass ``` **Requirements**: 1. The function should create an instance of `Charset` from the `input_charset`. 2. Based on the flags `is_header` and `is_body`, encode the `input_string` appropriately: - If `is_header` is `True`, use `header_encode()`. - If `is_body` is `True`, use `body_encode()`. - If both flags are `True`, prioritize `is_body` and use `body_encode()`. 3. Return the encoded string. **Constraints**: - `input_charset` will always be a valid character set recognized by the Python `email.charset` module. - The length of `input_string` will not exceed 10,000 characters. - At least one of the flags (`is_header` or `is_body`) will be `True`. **Example**: ```python # Example 1: input_string = \\"Hello, World!\\" input_charset = \\"utf-8\\" is_header = True is_body = False # The function should return the encoded string for the header based on the specified charset. # Example 2: input_string = \\"Hello, World!\\" input_charset = \\"utf-8\\" is_header = False is_body = True # The function should return the encoded string for the body based on the specified charset. assert encode_email_content(input_string, input_charset, is_header, is_body) == <expected_encoded_string> ``` **Note**: - You may assume that all necessary imports are taken care of. - Ensure that your function handles character set conversions appropriately as described in the module.","solution":"from email.charset import Charset def encode_email_content(input_string: str, input_charset: str, is_header: bool, is_body: bool) -> str: Encode the input string based on the character set and flags for header or body. charset = Charset(input_charset) if is_body: return charset.body_encode(input_string) elif is_header: return charset.header_encode(input_string) return input_string # Default case if no flags are set; this should not happen based on the constraints."},{"question":"Objective Your task is to create a PyTorch-based application that demonstrates understanding and implementation of reproducibility in every step. This will test your knowledge of handling randomness and ensuring consistent results across multiple runs. Problem Statement Create a PyTorch script that: 1. Initializes two random tensors of a given size. 2. Performs a matrix multiplication operation on these tensors. 3. Computes the Singular Value Decomposition (SVD) of the result. 4. Uses a DataLoader to create batches from a dataset and configures it to be reproducible. 5. Ensures reproducibility of every step using appropriate seeding and flag settings as discussed in the provided documentation. Requirements 1. Accepts the size of the random tensors as input arguments. 2. Ensures that the results are identical across multiple executions of the script on the same hardware. 3. Uses consistent seeds for PyTorch, NumPy, and Python\'s random module. Constraints - You must use `torch.manual_seed()`, `numpy.random.seed()`, and `random.seed()` to set seeds. - You must set the flags `torch.backends.cudnn.benchmark` to `False` and `torch.backends.cudnn.deterministic` to `True`. - Use `worker_init_fn` and `generator` in DataLoader to ensure reproducibility. Input Format - An integer `n` representing the size of the n x n random tensors. Output Format - Print the SVD result (U, S, V) of the matrix multiplication result. - Print the first batch from DataLoader. Example Input: ```sh 5 ``` Output: ```sh U: tensor([[ ...]]) S: tensor([ ... ]) V: tensor([[ ...]]) First Batch from DataLoader: tensor([[ ...]]) ``` Starter Code ```python import torch import numpy as np import random from torch.utils.data import DataLoader, TensorDataset def seed_all(seed): torch.manual_seed(seed) np.random.seed(seed) random.seed(seed) torch.backends.cudnn.benchmark = False torch.backends.cudnn.deterministic = True def seed_worker(worker_id): worker_seed = torch.initial_seed() % 2**32 np.random.seed(worker_seed) random.seed(worker_seed) def reproducible_pytorch_app(tensor_size): seed_all(0) # Initialize random tensors A = torch.randn(tensor_size, tensor_size) B = torch.randn(tensor_size, tensor_size) # Matrix multiplication C = torch.matmul(A, B) # Singular Value Decomposition (SVD) U, S, V = torch.svd(C) print(\\"U:\\", U) print(\\"S:\\", S) print(\\"V:\\", V) # DataLoader setup data = torch.randn(100, tensor_size) # dummy data dataset = TensorDataset(data) g = torch.Generator() g.manual_seed(0) data_loader = DataLoader( dataset, batch_size=10, worker_init_fn=seed_worker, generator=g, ) first_batch = next(iter(data_loader)) print(\\"First Batch from DataLoader:\\", first_batch) # Example usage if __name__ == \\"__main__\\": reproducible_pytorch_app(5) ``` Explanation - The function `seed_all()` is defined to handle seeding for PyTorch, NumPy, and Python. - The DataLoader is configured with `worker_init_fn` and a `generator` to ensure reproducibility. - The function `reproducible_pytorch_app()` implements the required steps and confirms reproducibility by printing the SVD result and one batch from DataLoader.","solution":"import torch import numpy as np import random from torch.utils.data import DataLoader, TensorDataset def seed_all(seed): torch.manual_seed(seed) np.random.seed(seed) random.seed(seed) torch.backends.cudnn.benchmark = False torch.backends.cudnn.deterministic = True def seed_worker(worker_id): worker_seed = torch.initial_seed() % 2**32 np.random.seed(worker_seed) random.seed(worker_seed) def reproducible_pytorch_app(tensor_size): seed_all(0) # Initialize random tensors A = torch.randn(tensor_size, tensor_size) B = torch.randn(tensor_size, tensor_size) # Matrix multiplication C = torch.matmul(A, B) # Singular Value Decomposition (SVD) U, S, V = torch.svd(C) print(\\"U:\\", U) print(\\"S:\\", S) print(\\"V:\\", V) # DataLoader setup data = torch.randn(100, tensor_size) # dummy data dataset = TensorDataset(data) g = torch.Generator() g.manual_seed(0) data_loader = DataLoader( dataset, batch_size=10, worker_init_fn=seed_worker, generator=g, ) first_batch = next(iter(data_loader)) print(\\"First Batch from DataLoader:\\", first_batch) # Example usage if __name__ == \\"__main__\\": reproducible_pytorch_app(5)"},{"question":"Objective Implement a function that imports a specified module and then attempts to reload it. If the module is not already imported, the function should handle the import gracefully. Additionally, if the import fails, an appropriate message should be displayed. Function Signature ```python def import_and_reload_module(module_name: str) -> str: Imports the specified module and then attempts to reload it. Parameters: module_name (str): The name of the module to import and reload. Returns: str: A string message that describes the outcome of the operation. ``` Input - `module_name`: A string representing the name of the module to be imported and reloaded (e.g., \'os\', \'sys\'). Output - A string message that describes the outcome. Possible outcomes include: - \\"Module <module_name> successfully imported and reloaded.\\" - \\"Module <module_name> successfully imported but reload failed.\\" - \\"Failed to import module <module_name>.\\" Constraints - You can assume the module names provided will be valid strings. - You should handle exceptions that occur during import and reload operations. Example ```python # Example 1 result = import_and_reload_module(\'os\') print(result) # Should output: \\"Module os successfully imported and reloaded.\\" # Example 2 result = import_and_reload_module(\'nonexistent_module\') print(result) # Should output: \\"Failed to import module nonexistent_module.\\" ``` Note You can use `importlib` library in Python to perform the import and reload operations, which provides functions `import_module` and `reload` respectively.","solution":"import importlib def import_and_reload_module(module_name: str) -> str: Imports the specified module and then attempts to reload it. Parameters: module_name (str): The name of the module to import and reload. Returns: str: A string message that describes the outcome of the operation. try: module = importlib.import_module(module_name) except ImportError: return f\\"Failed to import module {module_name}.\\" try: importlib.reload(module) return f\\"Module {module_name} successfully imported and reloaded.\\" except Exception: return f\\"Module {module_name} successfully imported but reload failed.\\""},{"question":"# Advanced Coding Assessment Question: Custom HTML Extractor **Objective:** Demonstrate your understanding of the `html.parser` module by implementing a subclass of `HTMLParser` that extracts and processes various types of HTML content. **Problem Statement:** You are required to create a custom HTML parser subclass named `ContentExtractor`, which processes raw HTML content and extracts different segments of interest. Specifically, your parser should be capable of: 1. Extracting the text enclosed within `<title>` tags. 2. Collecting all the URLs from `<a>` tags (anchor tags). 3. Extracting content within comments and adding them to a list. Implement the following methods to handle the described tasks: - `handle_starttag(self, tag, attrs)` - `handle_endtag(self, tag)` - `handle_data(self, data)` - `handle_comment(self, data)` Finally, create a method `get_results(self)` that returns a dictionary containing: - The title of the HTML document. - A list of all collected URLs from `<a>` tags. - A list of all comment contents. **Function Signature:** ```python from html.parser import HTMLParser class ContentExtractor(HTMLParser): def __init__(self): # Your code here def handle_starttag(self, tag, attrs): # Your code here def handle_endtag(self, tag): # Your code here def handle_data(self, data): # Your code here def handle_comment(self, data): # Your code here def get_results(self) -> dict: # Your code here # Example usage html_content = <!DOCTYPE html> <html> <head> <title>Example Page</title> </head> <body> <p>Welcome to the example page!</p> <a href=\\"https://example.com\\">Example</a> <!-- This is a comment --> </body> </html> parser = ContentExtractor() parser.feed(html_content) results = parser.get_results() expected_results = { \\"title\\": \\"Example Page\\", \\"urls\\": [\\"https://example.com\\"], \\"comments\\": [\\" This is a comment \\"] } assert results == expected_results, f\\"Expected {expected_results}, but got {results}\\" ``` **Constraints:** - Assume the input HTML content is a well-formed string. - You do not need to handle edge cases such as malformed HTML. **Testing Your Implementation:** You should test your code with various HTML content to ensure it handles different structures and elements correctly.","solution":"from html.parser import HTMLParser class ContentExtractor(HTMLParser): def __init__(self): super().__init__() self.title = None self.urls = [] self.comments = [] self._in_title = False def handle_starttag(self, tag, attrs): if tag == \'a\': for attr in attrs: if attr[0] == \'href\': self.urls.append(attr[1]) elif tag == \'title\': self._in_title = True def handle_endtag(self, tag): if tag == \'title\': self._in_title = False def handle_data(self, data): if self._in_title: self.title = data def handle_comment(self, data): self.comments.append(data) def get_results(self) -> dict: return { \'title\': self.title, \'urls\': self.urls, \'comments\': self.comments } # Example usage html_content = <!DOCTYPE html> <html> <head> <title>Example Page</title> </head> <body> <p>Welcome to the example page!</p> <a href=\\"https://example.com\\">Example</a> <!-- This is a comment --> </body> </html> parser = ContentExtractor() parser.feed(html_content) results = parser.get_results() expected_results = { \\"title\\": \\"Example Page\\", \\"urls\\": [\\"https://example.com\\"], \\"comments\\": [\\" This is a comment \\"] } assert results == expected_results, f\\"Expected {expected_results}, but got {results}\\""},{"question":"# Custom Estimator Implementation You have been tasked to create a custom scikit-learn compatible estimator that scales the input features and then applies a simple linear transformation to them. The transformation will be defined by multiplying the input features by a given coefficient and adding a bias term. Estimator Class: `CustomScaler` # Requirements: 1. Inherit from necessary scikit-learn base classes to ensure compatibility. 2. Implement the following methods: - `__init__(self, scale_factor=1.0, coef_=None, bias_=0.0)` - `fit(self, X, y=None)` - `transform(self, X)` - `predict(self, X)` - Optionally implement `fit_transform(self, X, y=None)` for efficiency if both fit and transform are required. 3. Handle input validation and parameters per scikit-learn conventions. 4. Ensure that the estimator follows scikit-learn standards and passes common checks. # Method Details: - **`__init__` Method:** - Parameters: - `scale_factor` (default is `1.0`): Factor by which to scale the input features. - `coef_` (default is `None`): Coefficients for the linear transformation (if `None`, it should be adapted in the `fit` method). - `bias_` (default is `0.0`): Bias term to be added after linear transformation. - No logic or validation other than setting attributes should be performed in the `__init__` method. - **`fit` Method:** - Parameters: - `X`: Input features, array-like of shape `(n_samples, n_features)`. - `y` (optional): Not used but required for compatibility. - Operations: - Validate and check the input `X`. - Set default values for `coef_` if it was `None` during initialization. - Scale the input `X` by `scale_factor`. - Store relevant attributes such as `coef_` and `bias_` with a trailing underscore (e.g., `coef_`, `bias_`). - **`transform` Method:** - Parameters: - `X`: Input features to be transformed. - Operations: - Validate and check the input `X`. - Scale and linearly transform the input `X` using `coef_` and `bias_`. - Return the transformed data. - **`predict` Method:** - Parameters: - `X`: Input features for making predictions. - Operations: - Validate and check the input `X`. - Scale and linearly transform the input `X` to get predictions. - Return the predicted values. - **`fit_transform` Method (Optional):** - Parameters: - `X`: Input features. - `y` (optional): Not used but required for compatibility. - Operations: - Combine the operations of `fit` and `transform` for efficiency. - Return the transformed data. **Expected Input/Output Format:** - Inputs to the methods will be in the form of numpy arrays or array-like structures. - Outputs should be numpy arrays or similarly structured data. **Constraints:** - Ensure that the number of samples in `X` and `y` (when provided) match. - Properly handle cases where `coef_` is not provided at initialization. - Follow scikit-learn conventions for estimator compatibility and parameter settings. **Performance Requirements:** - The solution should pass all the scikit-learn estimator checks. - Ensure that the code runs efficiently on large datasets. ```python import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.utils.validation import check_array, check_is_fitted class CustomScaler(BaseEstimator, TransformerMixin): def __init__(self, scale_factor=1.0, coef_=None, bias_=0.0): self.scale_factor = scale_factor self.coef_ = coef_ self.bias_ = bias_ def fit(self, X, y=None): X = check_array(X) self.n_features_in_ = X.shape[1] if self.coef_ is None: self.coef_ = np.ones(self.n_features_in_) else: self.coef_ = np.asarray(self.coef_) return self def transform(self, X): check_is_fitted(self) X = check_array(X) X_transformed = X * self.scale_factor return X_transformed * self.coef_ + self.bias_ def predict(self, X): return self.transform(X) def fit_transform(self, X, y=None): self.fit(X, y) return self.transform(X) ``` **You can test your implementation as follows:** ```python from sklearn.utils.estimator_checks import check_estimator check_estimator(CustomScaler()) # Should pass without errors ```","solution":"import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.utils.validation import check_array, check_is_fitted class CustomScaler(BaseEstimator, TransformerMixin): def __init__(self, scale_factor=1.0, coef_=None, bias_=0.0): self.scale_factor = scale_factor self.coef_ = coef_ self.bias_ = bias_ def fit(self, X, y=None): X = check_array(X) self.n_features_in_ = X.shape[1] if self.coef_ is None: self.coef_ = np.ones(self.n_features_in_) else: self.coef_ = np.asarray(self.coef_) return self def transform(self, X): check_is_fitted(self) X = check_array(X) X_transformed = X * self.scale_factor return X_transformed * self.coef_ + self.bias_ def predict(self, X): return self.transform(X) def fit_transform(self, X, y=None): self.fit(X, y) return self.transform(X)"},{"question":"# Python 310 Coding Assessment Question **Objective:** Implement functions to manipulate `bytes` objects in Python, demonstrating understanding of fundamental and advanced concepts of byte handling. **Background:** - A `bytes` object in Python is an immutable sequence of bytes, useful for binary data handling. - Operations on `bytes` include concatenation, slicing, and transformation, typically without altering the original `bytes` object. # Problem Statement: Implement the following functions to handle `bytes` objects: 1. **Function Name:** `is_bytes_object` - **Input:** An object `o` - **Output:** Return `True` if `o` is exactly a Python `bytes` object (not a subtype), otherwise `False`. 2. **Function Name:** `create_bytes_from_string` - **Input:** A string `s` - **Output:** Return a `bytes` object containing the same data as the string `s`. 3. **Function Name:** `concat_bytes` - **Input:** Two `bytes` objects `b1` and `b2` - **Output:** Return a new `bytes` object resulting from the concatenation of `b1` and `b2`. 4. **Function Name:** `get_bytes_size` - **Input:** A `bytes` object `b` - **Output:** Return the size (length) of the `bytes` object `b`. 5. **Function Name:** `resize_bytes` - **Input:** A `bytes` object `b` and an integer `new_size` - **Output:** Return a new `bytes` object that is a resized version of `b` to `new_size` bytes. If `new_size` is greater than the original size, the additional bytes should be null bytes (`x00`). If `new_size` is smaller, the `bytes` object should be truncated. **Constraints:** - The functions must adhere to the immutability of `bytes` objects. - Efficient memory management should be considered, ensuring no unnecessary copies of data are made. - Assume that the input string `s` for `create_bytes_from_string` will always be ASCII encoded. # Example Usage: ```python # is_bytes_object print(is_bytes_object(b\\"hello\\")) # True print(is_bytes_object(\\"hello\\")) # False # create_bytes_from_string print(create_bytes_from_string(\\"hello\\")) # b\'hello\' # concat_bytes print(concat_bytes(b\\"hello\\", b\\"world\\")) # b\'helloworld\' # get_bytes_size print(get_bytes_size(b\\"hello\\")) # 5 # resize_bytes print(resize_bytes(b\\"hello\\", 8)) # b\'hellox00x00x00\' print(resize_bytes(b\\"hello\\", 3)) # b\'hel\' ``` # Implementation Notes: - You may use Python\'s built-in `bytes` methods and direct operations (like slicing). - Ensure the functions are compatible with Python 3.10. **Deliverables:** - A Python file (e.g., `bytes_operations.py`) containing the five function implementations. - Ensure proper docstring comments for each function, explaining the purpose, parameters, and return values.","solution":"def is_bytes_object(o): Returns True if the object is exactly a bytes object, otherwise False. return type(o) is bytes def create_bytes_from_string(s): Creates and returns a bytes object from a string. return bytes(s, \'ascii\') def concat_bytes(b1, b2): Concatenates two bytes objects and returns the result. return b1 + b2 def get_bytes_size(b): Returns the size (length) of a bytes object. return len(b) def resize_bytes(b, new_size): Returns a new bytes object of the specified new_size. If new_size is greater, pad with null bytes. If smaller, truncate the bytes object. if new_size > len(b): return b + b\'x00\' * (new_size - len(b)) else: return b[:new_size]"},{"question":"**XML Parsing with Custom Handlers using `xml.parsers.expat`** You are required to implement a function called `parse_and_extract_xml` that takes an XML string as input and returns a list of tuples, where each tuple contains the name of an element and a dictionary of its attributes. The function should use the `xml.parsers.expat` module to parse the XML and extract this information. # Function Signature ```python def parse_and_extract_xml(xml_string: str) -> list: pass ``` # Input - `xml_string` (str): A string containing valid XML data. # Output - `list`: A list of tuples, where each tuple contains two elements: - A string representing the name of the element. - A dictionary containing the attributes of the element. # Constraints 1. You must use the `xml.parsers.expat` module to parse the XML. 2. The XML string can be well-formed and valid, containing nested elements and attributes. 3. The function should handle any common XML structure but does not need to handle malformed XML. 4. The order of elements in the output list should match the order they appear in the XML string. # Example ```python xml_data = <?xml version=\\"1.0\\"?> <library> <book id=\\"1\\" title=\\"1984\\" author=\\"George Orwell\\" /> <book id=\\"2\\" title=\\"Brave New World\\" author=\\"Aldous Huxley\\" /> <book id=\\"3\\" title=\\"Fahrenheit 451\\" author=\\"Ray Bradbury\\" /> </library> result = parse_and_extract_xml(xml_data) print(result) ``` Output: ```python [ (\\"library\\", {}), (\\"book\\", {\\"id\\": \\"1\\", \\"title\\": \\"1984\\", \\"author\\": \\"George Orwell\\"}), (\\"book\\", {\\"id\\": \\"2\\", \\"title\\": \\"Brave New World\\", \\"author\\": \\"Aldous Huxley\\"}), (\\"book\\", {\\"id\\": \\"3\\", \\"title\\": \\"Fahrenheit 451\\", \\"author\\": \\"Ray Bradbury\\"}) ] ``` # Notes - Be sure to utilize the handler functions provided by `xml.parsers.expat` to extract and collect the necessary information. - You should define at least a `StartElementHandler` to capture the element names and attributes as the XML is parsed.","solution":"import xml.parsers.expat def parse_and_extract_xml(xml_string: str) -> list: Parses the given XML string and returns a list of tuples containing the element names and their attributes. elements = [] def start_element(name, attrs): elements.append((name, dict(attrs))) parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.Parse(xml_string) return elements"},{"question":"**Objective**: To assess the student\'s ability to programmatically manipulate and generate a `setup.cfg` file required for building Python packages. # Problem Description You are tasked with writing a Python function that constructs a `setup.cfg` file given a dictionary of command options. Your function should be capable of taking in this dictionary and converting it to the appropriate `setup.cfg` format. Write a function `generate_setup_cfg(config: dict) -> str` that takes a dictionary `config` and returns a string representing the content of a `setup.cfg` file as per the format outlined below. # Input Format - `config`: A dictionary where the keys are command names (strings) and the values are dictionaries. Each inner dictionary contains option names as keys (strings) and their corresponding values (strings or lists of strings). # Output Format - A string that represents the content of `setup.cfg`. # Constraints - Command names and option names will follow the format rules (e.g., command names cannot have spaces). - Option values that are lists should be represented as multi-line values with each element indented on a new line. # Examples Example 1: ```python config = { \\"build_ext\\": { \\"inplace\\": \\"1\\", \\"include_dirs\\": [\\"/usr/local/include\\", \\"/opt/include\\"] }, \\"bdist_rpm\\": { \\"release\\": \\"1\\", \\"packager\\": \\"Jane Doe <janedoe@example.com>\\", \\"doc_files\\": [\\"README.txt\\", \\"INSTALL.md\\", \\"docs/\\"] } } expected_output = [build_ext] inplace=1 include_dirs=/usr/local/include /opt/include [bdist_rpm] release=1 packager=Jane Doe <janedoe@example.com> doc_files=README.txt INSTALL.md docs/ assert generate_setup_cfg(config) == expected_output ``` # Function Signature ```python def generate_setup_cfg(config: dict) -> str: pass ``` # Explanation 1. Each top-level key in the dictionary (e.g., `build_ext`, `bdist_rpm`) represents a command section in `setup.cfg`. 2. Each key-value pair within the command’s dictionary should be formatted as `option=value`. 3. If an option has a list of values, each value should be listed on a new line, indented. # Additional Notes - Handle edge cases where the dictionary might be empty. - Ensure the produced string strictly follows the specified format for `setup.cfg`.","solution":"def generate_setup_cfg(config: dict) -> str: Converts a given dictionary to the setup.cfg file format. Args: config (dict): A dictionary containing sections and their options. Returns: str: A string representation of the setup.cfg file. lines = [] for section, options in config.items(): lines.append(f\'[{section}]\') for option, value in options.items(): if isinstance(value, list): lines.append(f\'{option}={value[0]}\') for item in value[1:]: lines.append(f\' {\\" \\" * len(option)} {item}\') else: lines.append(f\'{option}={value}\') lines.append(\'\') # Blank line between sections return \'n\'.join(lines)"},{"question":"# Objective: Demonstrate your ability to manage Python application archives using the `zipapp` module by creating a standalone, executable Python application archive containing a directory of sample Python scripts. # Problem Description: You need to create an archive from a directory containing multiple Python scripts. The archive should: 1. Include a `main.py` file that will serve as the entry point for the application. 2. Use additional interpreter options and ensure the archive is executable on POSIX systems. 3. Compress the contents of the archive to reduce file size. 4. Finally, verify the interpreter embedded in the generated archive. # Task: Write a Python function `create_executable_archive` that takes the following inputs: 1. `source_dir` (str): The path to the directory containing your Python files. 2. `output_file` (str): The name of the output archive file (including the `.pyz` extension). 3. `interpreter_path` (str): The path to the Python interpreter to be specified in the \\"#!\\" line. 4. `main_function` (str): The main function to execute, in the format `\\"package.module:function\\"`. The function should: - Create a compressed application archive from the source directory. - Add a shebang line with the specified interpreter. - Specify the main function to be executed. - Verify and return the interpreter embedded in the archive. Input: - `source_dir`: a string containing the path to the directory with Python files (e.g., `\\"myapp\\"`). - `output_file`: a string with the output archive file name (e.g., `\\"myapp.pyz\\"`). - `interpreter_path`: a string specifying the interpreter (e.g., `\\"/usr/bin/env python\\"`). - `main_function`: a string in the format `\\"pkg.mod:function\\"` to specify the main function (e.g., `\\"myapp:main\\"`). Output: - A string containing the interpreter embedded in the archive. Constraints: - The directory should exist and contain valid Python files. - The specified main function should exist and be callable. - Ensure the output file is correctly named and has the `.pyz` extension. # Example: Assume you have the following directory structure: ``` myapp/ main.py helper.py ``` `main.py` contains the function `main` which serves as the entry point. When the following function is executed: ```python create_executable_archive(\\"myapp\\", \\"myapp.pyz\\", \\"/usr/bin/env python\\", \\"myapp.main:main\\") ``` The archive `\\"myapp.pyz\\"` should be created with the specified options, and the function should return: ``` \'/usr/bin/env python\' ``` Note: - The command-line utility `zipapp` and the corresponding Python API `zipapp.create_archive` should be used to accomplish this task. - Remember to handle edge cases, such as invalid directories or missing main functions gracefully.","solution":"import zipapp import os def create_executable_archive(source_dir, output_file, interpreter_path, main_function): Creates a compressed, executable application archive from the source directory. Parameters: source_dir (str): The path to the directory containing your Python files. output_file (str): The name of the output archive file (including the .pyz extension). interpreter_path (str): The path to the Python interpreter to be specified in the shebang line. main_function (str): The main function to execute in the format \\"package.module:function\\". Returns: str: The interpreter embedded in the archive. # Ensure the source directory exists if not os.path.isdir(source_dir): raise ValueError(f\\"The directory \'{source_dir}\' does not exist.\\") # Ensure the output file has \'.pyz\' extension if not output_file.endswith(\'.pyz\'): raise ValueError(f\\"The output file \'{output_file}\' must have a .pyz extension.\\") # Create the archive zipapp.create_archive( source_dir, target=output_file, interpreter=interpreter_path, main=main_function, compressed=True ) return interpreter_path"},{"question":"# Python Coding Assessment Challenge Objective The goal of this assessment is to encapsulate the essence of CPython’s methodology for handling Python objects and their attributes using pure Python. This will demonstrate a comprehensive understanding of object manipulation, attribute access, and method definitions akin to those provided in the CPython API. Problem Statement You will define a base class `PyObject` in Python that simulates basic Python object behavior, including reference counting. Additionally, you will implement a derived class `PyVarObject` that extends `PyObject` to support objects with a variable size. Furthermore, create a few example methods and properties accessible in a manner similar to the C API. Detailed Requirements 1. **Base Object Type `PyObject`:** - Implement a class `PyObject` with attributes `refcnt` (reference count) and `type_name` (type of the object). - Implement methods for `increase_refcnt`, `decrease_refcnt`, and `get_refcnt`. 2. **Variable Object Type `PyVarObject`:** - Implement a class `PyVarObject` that inherits from `PyObject` and adds an attribute `size` (the object\'s size). - Implement methods for `get_size` and `set_size`. 3. **Function Definitions:** - Define functions such as `py_is(x, y)`, `py_is_true(x)`, and `py_is_false(x)` based on their CPython C API counterparts. 4. **Attribute Management:** - Implement a simplified version of `PyMemberDef` to manage attributes of an object. Each attribute should have a name and a type. 5. **Example Usage:** - Provide an example where `PyObject` and `PyVarObject` are instantiated, methods are called, attributes are managed via `PyMemberDef`, and the functions like `py_is`, `py_is_true`, and `py_is_false` are utilized. Example ```python class PyObject: def __init__(self, type_name): self.refcnt = 1 self.type_name = type_name def increase_refcnt(self): self.refcnt += 1 def decrease_refcnt(self): self.refcnt -= 1 if self.refcnt == 0: del self def get_refcnt(self): return self.refcnt class PyVarObject(PyObject): def __init__(self, type_name, size): super().__init__(type_name) self.size = size def get_size(self): return self.size def set_size(self, size): self.size = size class PyMemberDef: def __init__(self, name, ctype): self.name = name self.ctype = ctype def py_is(x, y): return x is y def py_is_true(x): return x is True def py_is_false(x): return x is False # Usage Example: pyo = PyObject(\\"example\\") print(pyo.get_refcnt()) # Output: 1 pyvo = PyVarObject(\\"var_example\\", 10) print(pyvo.get_size()) # Output: 10 print(py_is(pyo, pyvo)) # Output: False print(py_is_true(True)) # Output: True print(py_is_false(False)) # Output: True ``` **Note**: Use this guidance to complete your solution. Ensure that the problem not only tests your ability to implement the concepts described but also your understanding of their use and utility. Constraints: - Ensure the code adheres to Python syntax and conventions. - Do not use any external libraries, only standard Python.","solution":"class PyObject: def __init__(self, type_name): self.refcnt = 1 self.type_name = type_name def increase_refcnt(self): self.refcnt += 1 def decrease_refcnt(self): self.refcnt -= 1 if self.refcnt == 0: del self def get_refcnt(self): return self.refcnt class PyVarObject(PyObject): def __init__(self, type_name, size): super().__init__(type_name) self.size = size def get_size(self): return self.size def set_size(self, size): self.size = size class PyMemberDef: def __init__(self, name, ctype): self.name = name self.ctype = ctype def py_is(x, y): return x is y def py_is_true(x): return x is True def py_is_false(x): return x is False # Usage Example: pyo = PyObject(\\"example\\") print(pyo.get_refcnt()) # Output: 1 pyvo = PyVarObject(\\"var_example\\", 10) print(pyvo.get_size()) # Output: 10 print(py_is(pyo, pyvo)) # Output: False print(py_is_true(True)) # Output: True print(py_is_false(False)) # Output: True"},{"question":"Objective: You are to implement a small simulation using the `queue` module to manage a simplified task scheduler. The scheduler should handle tasks submitted to it based on FIFO, LIFO, and priority order. Requirements: 1. **Task Class**: Implement a `Task` class with the following attributes: - `task_id` (int): A unique identifier for the task. - `priority` (int): The priority of the task, only relevant for the priority queue. - `details` (str): A string describing the task details. The `Task` class should also implement a `__str__` method to return a string representation of the task. 2. **TaskScheduler Class**: Implement a `TaskScheduler` class that: - Initializes three queues: a FIFO queue, a LIFO queue, and a priority queue. - Has methods to add tasks to each of the queues and to retrieve tasks from each of the queues. - Contains a method to process a specified number of tasks from each queue, printing the task details upon processing. Implementation: ```python import queue from dataclasses import dataclass, field from typing import Any, List @dataclass(order=True) class Task: priority: int task_id: int=field(compare=False) details: str=field(compare=False) def __str__(self): return f\'Task ID: {self.task_id}, Priority: {self.priority}, Details: {self.details}\' class TaskScheduler: def __init__(self): self.fifo_queue = queue.Queue() self.lifo_queue = queue.LifoQueue() self.priority_queue = queue.PriorityQueue() def add_task_fifo(self, task: Task): self.fifo_queue.put(task) def add_task_lifo(self, task: Task): self.lifo_queue.put(task) def add_task_priority(self, task: Task): self.priority_queue.put(task) def process_tasks(self, num_tasks: int): print(\\"Processing FIFO tasks:\\") for _ in range(num_tasks): if not self.fifo_queue.empty(): task = self.fifo_queue.get() print(task) self.fifo_queue.task_done() else: print(\\"FIFO queue is empty.\\") break print(\\"nProcessing LIFO tasks:\\") for _ in range(num_tasks): if not self.lifo_queue.empty(): task = self.lifo_queue.get() print(task) self.lifo_queue.task_done() else: print(\\"LIFO queue is empty.\\") break print(\\"nProcessing Priority tasks:\\") for _ in range(num_tasks): if not self.priority_queue.empty(): task = self.priority_queue.get() print(task) self.priority_queue.task_done() else: print(\\"Priority queue is empty.\\") break ``` # Instructions: 1. Implement the `Task` and `TaskScheduler` classes as described above. 2. Write a script that: - Creates a `TaskScheduler` instance. - Adds a mixture of tasks to each queue. - Processes a set number of tasks from each queue, printing the details. # Example Usage: ```python if __name__ == \\"__main__\\": scheduler = TaskScheduler() # Adding tasks to the FIFO queue scheduler.add_task_fifo(Task(1, 101, \\"Task from FIFO queue\\")) scheduler.add_task_fifo(Task(2, 102, \\"Another task from FIFO queue\\")) # Adding tasks to the LIFO queue scheduler.add_task_lifo(Task(1, 201, \\"Task from LIFO queue\\")) scheduler.add_task_lifo(Task(2, 202, \\"Another task from LIFO queue\\")) # Adding tasks to the Priority queue scheduler.add_task_priority(Task(1, 301, \\"High priority task\\")) scheduler.add_task_priority(Task(3, 303, \\"Low priority task\\")) scheduler.add_task_priority(Task(2, 302, \\"Mid priority task\\")) # Processing tasks scheduler.process_tasks(2) ``` This script should demonstrate the adding of tasks to different types of queues and the processing of tasks in the specified order. Constraints: - You must use the queue classes from the `queue` module. - Handle cases where there are fewer tasks in the queue than the specified number to process. Performance Requirements: - The task addition and processing should be efficient. Note that the queue operations already provide efficient behavior suitable for concurrent access.","solution":"import queue from dataclasses import dataclass, field @dataclass(order=True) class Task: priority: int task_id: int = field(compare=False) details: str = field(compare=False) def __str__(self): return f\'Task ID: {self.task_id}, Priority: {self.priority}, Details: {self.details}\' class TaskScheduler: def __init__(self): self.fifo_queue = queue.Queue() self.lifo_queue = queue.LifoQueue() self.priority_queue = queue.PriorityQueue() def add_task_fifo(self, task: Task): self.fifo_queue.put(task) def add_task_lifo(self, task: Task): self.lifo_queue.put(task) def add_task_priority(self, task: Task): self.priority_queue.put(task) def process_tasks(self, num_tasks: int): print(\\"Processing FIFO tasks:\\") for _ in range(num_tasks): if not self.fifo_queue.empty(): task = self.fifo_queue.get() print(task) self.fifo_queue.task_done() else: print(\\"FIFO queue is empty.\\") break print(\\"nProcessing LIFO tasks:\\") for _ in range(num_tasks): if not self.lifo_queue.empty(): task = self.lifo_queue.get() print(task) self.lifo_queue.task_done() else: print(\\"LIFO queue is empty.\\") break print(\\"nProcessing Priority tasks:\\") for _ in range(num_tasks): if not self.priority_queue.empty(): task = self.priority_queue.get() print(task) self.priority_queue.task_done() else: print(\\"Priority queue is empty.\\") break"},{"question":"Objective: Write a Python function that programmatically creates and manipulates a configuration file using the `configparser` module. The function should demonstrate understanding of reading, writing, interpolation, and handling different data types. Task: You are required to implement a function `create_and_manipulate_config()` that performs the following tasks: 1. **Create a configuration file** with sections and options as specified. 2. **Read** the configuration file and return specific values. 3. **Demonstrate interpolation** by referencing other configuration values. Requirements: 1. The function should create a configuration file with the following structure: ```ini [DEFAULT] AppName = MyApp DebugMode = True [database] User = admin Password = secret Port = 5432 [server] Hostname = localhost AppPath = %(AppName)s_path ``` 2. After creating the configuration, the function should: - Read the configuration from the file. - Retrieve and return: - `AppName` in the `[DEFAULT]` section. - The port number in the `[database]` section as an integer. - The `AppPath` value in `[server]` section after interpolation. 3. The function should demonstrate handling of booleans, integers, and string interpolations. Constraints: - Do not use hardcoded values apart from the ones provided in the given structure. - The configuration file should be named `app_config.ini`. Function Signature: ```python def create_and_manipulate_config() -> tuple: pass ``` Expected Output: The function should return a tuple with the following values: 1. The `AppName` from the `[DEFAULT]` section. 2. The port number from the `[database]` section as an integer. 3. The `AppPath` value from the `[server]` section after interpolation. Example: ```python result = create_and_manipulate_config() print(result) # Expected output: (\'MyApp\', 5432, \'MyApp_path\') ``` Performance Requirements: - The function should handle the creation and reading of configuration files efficiently. Additional Information: You may assume the module `configparser` is already installed and available for use.","solution":"import configparser def create_and_manipulate_config(): # Create a ConfigParser object config = configparser.ConfigParser() # Define the configuration structure config[\'DEFAULT\'] = { \'AppName\': \'MyApp\', \'DebugMode\': \'True\' } config[\'database\'] = { \'User\': \'admin\', \'Password\': \'secret\', \'Port\': \'5432\' } config[\'server\'] = { \'Hostname\': \'localhost\', \'AppPath\': \'%(AppName)s_path\' } # Write the configuration to a file with open(\'app_config.ini\', \'w\') as configfile: config.write(configfile) # Read the configuration back from the file config.read(\'app_config.ini\') # Retrieve required values app_name = config[\'DEFAULT\'][\'AppName\'] port = config.getint(\'database\', \'Port\') app_path = config[\'server\'][\'AppPath\'] return (app_name, port, app_path)"},{"question":"# Pandas Coding Assessment Objective Write a function that processes a CSV file containing sales data and generates a few key insights. The function should demonstrate your understanding of various pandas functionalities including data manipulation, handling missing values, grouping, and basic visualization. Problem Statement You are provided with a CSV file `sales_data.csv` containing sales data with the following columns: - `Date`: Date of the sale (YYYY-MM-DD) - `Region`: Region where the sale was made - `Product`: Name of the product sold - `Quantity`: Number of units sold - `Price`: Price per unit - `Total`: Total sales value (Quantity * Price) The file may contain some missing values in the `Total` column. You need to write a function `process_sales_data(file_path)` that: 1. Reads the CSV file into a pandas DataFrame. 2. Fills any missing values in the `Total` column by calculating the product of `Quantity` and `Price`. 3. Generates a summary DataFrame showing the total sales and the average sale price per product in each region. 4. Plots a bar chart showing total sales per region. 5. Plots a line chart showing total sales over time. **Function Signature:** ```python def process_sales_data(file_path: str) -> pd.DataFrame: pass ``` Input - `file_path`: A string representing the path to the `sales_data.csv` file. Output - Returns a summary pandas DataFrame with columns `Region`, `Product`, `Total Sales`, `Average Price`. Constraints - The data in the file is well-formed, but there may be missing values in the `Total` column. - You must use pandas for all data manipulation. - The output plots should be displayed inline if running in a Jupyter notebook. Example Given a `sales_data.csv` file with the following content: ``` Date,Region,Product,Quantity,Price,Total 2023-01-01,North,Widget,10,2.5, 2023-01-02,South,Widget,5,2.5,12.5 2023-01-03,East,Thingamajig,8,3.5,28 2023-01-04,North,Widget,7,2.5, 2023-01-05,West,Thingamajig,6,3.5,21 ``` Your function should produce a summary DataFrame similar to: ``` Region Product Total Sales Average Price 0 North Widget 42.5 2.5 1 South Widget 12.5 2.5 2 East Thingamajig 28.0 3.5 3 West Thingamajig 21.0 3.5 ``` And generate the following plots: - A bar chart showing total sales per region. - A line chart showing total sales over time. **Hint:** You may find `DataFrame.fillna()`, `DataFrame.groupby()`, `matplotlib.pyplot`, and `pandas plotting` capabilities useful for this task.","solution":"import pandas as pd import matplotlib.pyplot as plt def process_sales_data(file_path: str) -> pd.DataFrame: # Step 1: Read CSV file into pandas DataFrame df = pd.read_csv(file_path) # Step 2: Fill missing values in \'Total\' column df[\'Total\'] = df[\'Total\'].fillna(df[\'Quantity\'] * df[\'Price\']) # Step 3: Generate summary DataFrame summary = df.groupby([\'Region\', \'Product\']).agg({ \'Total\': \'sum\', \'Price\': \'mean\' }).reset_index().rename(columns={\'Total\': \'Total Sales\', \'Price\': \'Average Price\'}) # Step 4: Plot total sales per region region_sales = df.groupby(\'Region\')[\'Total\'].sum().sort_values() region_sales.plot(kind=\'bar\', title=\'Total Sales per Region\') plt.xlabel(\'Region\') plt.ylabel(\'Total Sales\') plt.show() # Step 5: Plot total sales over time df[\'Date\'] = pd.to_datetime(df[\'Date\']) time_sales = df.groupby(\'Date\')[\'Total\'].sum() time_sales.plot(kind=\'line\', title=\'Total Sales Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') plt.grid(True) plt.show() return summary"},{"question":"# PyTorch Tensor Operations and Attributes **Objective**: Write a function that performs specific operations on tensors involving different data types, devices, and layouts. # Function Signature ```python def tensor_operations(a: torch.Tensor, b: torch.Tensor, device: str) -> torch.Tensor: pass ``` # Input and Output **Input**: - `a` (torch.Tensor): A tensor located on the CPU and can be of any dtype (`torch.float32`, `torch.int32`, etc.). - `b` (torch.Tensor): Another tensor located on the CPU which may have a different dtype compared to `a`. - `device` (str): A string representing the device to which the result tensor should be moved. It can be either `\'cpu\'` or `\'cuda:<index>\'`. **Output**: - Returns a tensor that is the result of the following operations and is moved to the specified device. The resultant tensor should maintain the correct memory format (`torch.contiguous_format`). # Constraints 1. If `device` is `\'cuda:<index>\'`, ensure the tensor is moved to the specified CUDA device. 2. Use type promotion rules to handle operations between tensors of different dtypes: - `add` the two input tensors `a` and `b`. 3. The result tensor should have `torch.contiguous_format` layout. 4. Ensure the resultant tensor has a dtype that can hold the values correctly after the addition. # Examples 1. **Example 1**: ```python a = torch.tensor([1, 2, 3], dtype=torch.int32) b = torch.tensor([0.1, 0.2, 0.3], dtype=torch.float32) device = \'cpu\' result = tensor_operations(a, b, device) assert result.dtype == torch.float32 assert result.device.type == \'cpu\' ``` 2. **Example 2**: ```python a = torch.tensor([1.0, 2.0], dtype=torch.float32) b = torch.tensor([1, 2], dtype=torch.int32) device = \'cuda:0\' result = tensor_operations(a, b, device) assert result.dtype == torch.float32 assert result.device.type == \'cuda\' assert result.device.index == 0 ``` **Notes**: - Ensure the function handles the scenarios specified in the constraints. - Utilize PyTorch tensor attributes and ensure the correct memory layout. - Test the function with varied data inputs to confirm the validation rules.","solution":"import torch def tensor_operations(a: torch.Tensor, b: torch.Tensor, device: str) -> torch.Tensor: Perform addition of two tensors a and b, and move the result tensor to the specified device. Parameters: a (torch.Tensor): A tensor located on the CPU and can be of any dtype. b (torch.Tensor): Another tensor located on the CPU which may have a different dtype compared to a. device (str): A string representing the device to which the result tensor should be moved. It can be either \'cpu\' or \'cuda:<index>\'. Returns: torch.Tensor: The resulting tensor after addition, moved to the specified device. # Perform the addition (will handle type promotion automatically) result = a + b # Move the result to the specified device result = result.to(device) # Ensure the result is in contiguous format result = result.contiguous() return result"},{"question":"# Python Coding Assessment Question **Problem Statement:** You are required to implement a class in Python called `ConstantChecker` that demonstrates the use and characteristics of the built-in constants described in the documentation. The class should include the following functionalities: 1. **Initialization:** - The class should initialize with a dictionary that maps a string representation of each constant to its actual value. ```python example = { \'False\': False, \'True\': True, \'None\': None, \'NotImplemented\': NotImplemented, \'Ellipsis\': Ellipsis, \'__debug__\': __debug__ } ``` 2. **Method `check_constant`:** - This method should accept a constant name (as a string) and return a tuple containing: - The type of the constant. - A boolean indicating if the constant can be reassigned without error. 3. **Method `perform_operation`:** - This method should accept two constants (as strings) and an operation (as a string, e.g., `==`, `+`, etc.), perform the operation, and return the result. If the operation is not supported, it should return the string `\\"NotImplemented\\"`. Handling should be done for - `==` - `!=` - `+` - `-` - `*` - `/` **Constraints:** - Do not use any external Python libraries; rely only on built-in types and operations. - Ensure your implementation is efficient and handles exceptions gracefully. # Example Usage: ```python checker = ConstantChecker() # Checking constants print(checker.check_constant(\'None\')) # Outputs: (<class \'NoneType\'>, False) print(checker.check_constant(\'True\')) # Outputs: (<class \'bool\'>, False) # Performing operations print(checker.perform_operation(\'True\', \'False\', \'==\')) # Outputs: False print(checker.perform_operation(\'None\', \'None\', \'==\')) # Outputs: True print(checker.perform_operation(\'True\', \'None\', \'+\')) # Outputs: NotImplemented ``` Implement the class `ConstantChecker` as described. Remember to handle the immutability constraints of the constants where applicable.","solution":"class ConstantChecker: def __init__(self): self.constants = { \'False\': False, \'True\': True, \'None\': None, \'NotImplemented\': NotImplemented, \'Ellipsis\': Ellipsis, \'__debug__\': __debug__ } def check_constant(self, constant_name): if constant_name not in self.constants: raise ValueError(f\\"Constant \'{constant_name}\' is not recognized.\\") constant = self.constants[constant_name] can_reassign = False if constant_name == \'__debug__\': can_reassign = not __debug__ return (type(constant), can_reassign) def perform_operation(self, const1_name, const2_name, operation): if const1_name not in self.constants or const2_name not in self.constants: raise ValueError(\\"One or both constants are not recognized.\\") const1 = self.constants[const1_name] const2 = self.constants[const2_name] try: if operation == \'==\': return const1 == const2 elif operation == \'!=\': return const1 != const2 elif operation == \'+\': return const1 + const2 elif operation == \'-\': return const1 - const2 elif operation == \'*\': return const1 * const2 elif operation == \'/\': return const1 / const2 except: return \'NotImplemented\' return \'NotImplemented\'"},{"question":"Problem Statement You are required to implement a Python function that processes a list of integers according to specific rules, using a blend of list comprehensions, generator expressions, and the `yield` statement. The function should demonstrate a deep understanding of these concepts. Task - Implement a function `process_numbers(numbers: List[int]) -> Tuple[List[int], Generator[int, None, None]]`: 1. The function takes a list of integers as input. 2. It returns a tuple consisting of: - A list of squared values of only the even integers from the input list, generated using a list comprehension. - A generator that yields the cumulative sum of the input integers, using the `yield` expression. Input - `numbers`: A list of integers `List[int]`. Constraints: (1 leq text{len}(numbers) leq 10^5) and ( -10^4 leq text{numbers}[i] leq 10^4). Output - A tuple containing: - A list of squared values of even integers. - A generator that yields the cumulative sum of the integers in the list. Example ```python def process_numbers(numbers: List[int]) -> Tuple[List[int], Generator[int, None, None]]: # Your implementation here # Example usage: numbers = [1, 2, 3, 4, 5] squares, cumulative_sum_generator = process_numbers(numbers) # Expected Output: ([4, 16], generator that yields 1, 3, 6, 10, 15) print(squares) for cumulative_sum in cumulative_sum_generator: print(cumulative_sum) ``` # Requirements 1. Use a list comprehension to generate the list of squared even integers. 2. Use a generator function with the `yield` expression to create the cumulative sum generator. 3. Pay attention to the performance, ensuring that operations are efficient given the constraints. Constraints - The function should work efficiently even for the maximum input size. - The cumulative sum generator should not hold all elements in memory at once, leveraging lazy evaluation with `yield`. Good Luck!","solution":"from typing import List, Tuple, Generator def process_numbers(numbers: List[int]) -> Tuple[List[int], Generator[int, None, None]]: Processes the list of integers according to specific rules. Args: numbers: List of integers. Returns: A tuple containing: - A list of squared values of even integers. - A generator that yields the cumulative sum of the integers in the list. # List comprehension to get list of squared even integers squared_evens = [x ** 2 for x in numbers if x % 2 == 0] # Generator function to yield cumulative sums def cumulative_sum_gen(numbers): cumulative_sum = 0 for num in numbers: cumulative_sum += num yield cumulative_sum cumulative_sum_generator = cumulative_sum_gen(numbers) return squared_evens, cumulative_sum_generator"},{"question":"# AIFF/AIFF-C Audio File Manipulation **Objective:** Write a Python function that reads an AIFF/AIFF-C file, doubles its speed (i.e., shrinks the duration to half), and saves the manipulated audio to a new file. **Function Signature:** ```python def double_audio_speed(input_file: str, output_file: str) -> None: pass ``` **Input:** - `input_file` (str): The path to the input AIFF/AIFF-C file. - `output_file` (str): The path where the output AIFF/AIFF-C file will be saved. **Output:** - The function does not return any value. Instead, it creates an AIFF/AIFF-C file at `output_file` with double the speed of the input file. **Constraints:** - The function should handle both AIFF and AIFF-C files. - The function should not use any external dependency other than the standard library. - Assume the input file is valid and exists. **Example:** ```python double_audio_speed(\'input.aiff\', \'output.aiff\') ``` **Hints:** 1. Use the `aifc.open` function to read the input file. 2. Extract the audio parameters like number of channels, sample width, and frame rate using methods like `getnchannels()`, `getsampwidth()`, and `getframerate()`. 3. Read the entire audio data using `readframes()`. 4. To double the speed, you may need to skip every alternate frame and write the remaining frames to the new file. 5. Set the appropriate parameters for the new file using methods like `setnchannels()`, `setsampwidth()`, `setframerate()`, and `setnframes()`. 6. Write the manipulated frames to the new file using the `writeframes()` method. **Notes:** - Make sure to handle the cleanup properly by closing the file handles after completing the read/write operations. - This exercise assesses your ability to handle file operations, understand metadata, and perform operations on binary data.","solution":"import aifc def double_audio_speed(input_file: str, output_file: str) -> None: Reads an AIFF/AIFF-C file, doubles its speed, and saves the manipulated audio to a new file. with aifc.open(input_file, \'rb\') as infile: # Extract audio parameters n_channels = infile.getnchannels() samp_width = infile.getsampwidth() frame_rate = infile.getframerate() # Read original frames original_frames = infile.readframes(infile.getnframes()) # Skip alternate frames to double the speed doubled_speed_frames = original_frames[::2] with aifc.open(output_file, \'wb\') as outfile: # Set audio parameters for the new file outfile.setnchannels(n_channels) outfile.setsampwidth(samp_width) outfile.setframerate(frame_rate * 2) outfile.writeframes(doubled_speed_frames)"},{"question":"# Asynchronous File Processing with Timeout and Shielding **Problem Statement:** You are tasked with creating an asynchronous file processing system. The system will read multiple files concurrently, process their contents (in this case, converting all text to uppercase), and save the processed content back to the files. Additionally, you need to ensure that the file processing is done within a specified timeout, and any file process that exceeds the timeout should be shielded from cancellation but should still be reported. Your task is to implement the following function: ```python import asyncio async def process_files(file_paths, timeout): Process multiple files concurrently by reading their content, turning all text to uppercase, and writing back to the files. Args: - file_paths (list of str): A list of file paths to process. - timeout (float): The maximum amount of time in seconds allowed for each file processing task. Returns: - dict: A dictionary with file paths as keys and a string as value: either \\"Processed\\" if the processing was successful within the timeout, or \\"Timeout\\" if the processing exceeded the timeout. Raises: - ValueError: If the file_paths list is empty. pass ``` **Constraints:** 1. The function should create tasks to process each file concurrently. 2. Each file processing task must read the file content, convert it to uppercase, and write it back to the file. 3. Implement a timeout for each file processing task using `asyncio.wait_for()`, but the task must be shielded from cancellation using `asyncio.shield()`. 4. The function should return a dictionary indicating the result of each file processing task: \\"Processed\\" if completed within the timeout, and \\"Timeout\\" if not. 5. You must handle scenarios where the file_paths list is empty by raising a `ValueError`. **Examples:** Given the file paths `[\'file1.txt\', \'file2.txt\']` and a timeout of 0.5 seconds, the function should: 1. Process `file1.txt` and `file2.txt` concurrently. 2. If any file takes more than 0.5 seconds to process, it should be marked as \\"Timeout\\" but still shielded from cancellation. 3. Return a dictionary with the results. **Example Usage:** ```python result = await process_files([\'file1.txt\', \'file2.txt\'], 0.5) print(result) # Output could be {\'file1.txt\': \'Processed\', \'file2.txt\': \'Timeout\'} ``` **Performance Requirements:** - The function must handle a large number of files efficiently, leveraging asyncio\'s ability to manage concurrent I/O-bound operations. - Ensure robust error handling for file I/O operations. **HINT:** You can use `asyncio.create_task()` to schedule file processing tasks and `asyncio.gather()` to run them concurrently.","solution":"import asyncio import os async def process_file(file_path, timeout): try: # Read file contents async with aiofiles.open(file_path, \'r\') as f: content = await f.read() # Simulate processing delay await asyncio.sleep(0.1) # Simulate processing time # Convert content to uppercase processed_content = content.upper() # Save processed content back to the file async with aiofiles.open(file_path, \'w\') as f: await f.write(processed_content) return \\"Processed\\" except Exception as e: print(f\\"Error processing file {file_path}: {e}\\") return \\"Error\\" async def shield_file_processing(file_path, timeout): try: return await asyncio.wait_for(asyncio.shield(process_file(file_path, timeout)), timeout) except asyncio.TimeoutError: return \\"Timeout\\" async def process_files(file_paths, timeout): if not file_paths: raise ValueError(\\"file_paths list cannot be empty\\") results = {} tasks = [shield_file_processing(file_path, timeout) for file_path in file_paths] for file_path, task in zip(file_paths, asyncio.as_completed(tasks)): result = await task results[file_path] = result return results"},{"question":"**Problem: Secure Password Generator and Validator** You are tasked with developing a secure password management system for a web application. The system must be able to generate secure passwords, validate them according to specific criteria, and securely compare existing passwords to ensure they match stored hashed versions. Part 1: Password Generation Implement a function `generate_password(length: int, use_special_chars: bool = False) -> str` that generates a secure password. **Requirements:** 1. The password should be exactly `length` characters long. 2. It should contain at least one lowercase letter, one uppercase letter, and one digit. 3. If `use_special_chars` is `True`, include at least one special character from the set `!@#%^&*()-_=+`. 4. Use the `secrets` module to generate the password. **Input:** - `length (int)`: The length of the password to be generated. (1 ≤ length ≤ 100) - `use_special_chars (bool, optional)`: If set to `True`, the password should include special characters. **Output:** - A string representing the generated password. Part 2: Password Validation Implement a function `validate_password(password: str, use_special_chars: bool = False) -> bool` that validates a given password. **Requirements:** 1. The password must contain at least one lowercase letter, one uppercase letter, and one digit. 2. If `use_special_chars` is `True`, it must contain at least one special character from the set `!@#%^&*()-_=+`. 3. The password length must be at least 8 characters. **Input:** - `password (str)`: The password to be validated. - `use_special_chars (bool, optional)`: If set to `True`, the password should include special characters. **Output:** - A boolean value indicating whether the password is valid. Part 3: Secure Password Comparison Implement a function `compare_passwords(stored_password: str, input_password: str) -> bool` that securely compares two passwords. **Requirements:** 1. Use `secrets.compare_digest` to perform a constant-time comparison. **Input:** - `stored_password (str)`: The stored password (usually the hashed version). - `input_password (str)`: The input password to be compared. **Output:** - A boolean value indicating whether the passwords match. # Function Signatures ```python import secrets import string def generate_password(length: int, use_special_chars: bool = False) -> str: # Your implementation here pass def validate_password(password: str, use_special_chars: bool = False) -> bool: # Your implementation here pass def compare_passwords(stored_password: str, input_password: str) -> bool: # Your implementation here pass ``` # Example Usage ```python # Example usage of generate_password password1 = generate_password(12, use_special_chars=True) print(password1) # Output: A randomly generated secure password with special characters # Example usage of validate_password is_valid = validate_password(password1, use_special_chars=True) print(is_valid) # Output: True # Example usage of compare_passwords stored_password = \'s0m3H@sh3dP@ssw0rd\' input_password = \'s0m3H@sh3dP@ssw0rd\' is_match = compare_passwords(stored_password, input_password) print(is_match) # Output: True ``` Ensure that the `secrets` module and other Python libraries mentioned are adequately used and the functions are well-documented.","solution":"import secrets import string def generate_password(length: int, use_special_chars: bool = False) -> str: if length < 1 or length > 100: raise ValueError(\'Password length must be between 1 and 100 characters.\') if length < 4: raise ValueError(\'Password length must be at least 4 characters to include all required character types.\') # Define character sets lowercase_letters = string.ascii_lowercase uppercase_letters = string.ascii_uppercase digits = string.digits special_characters = \'!@#%^&*()-_=+\' if use_special_chars else \'\' # Ensure at least one of each required type is included password_chars = [ secrets.choice(lowercase_letters), secrets.choice(uppercase_letters), secrets.choice(digits) ] if use_special_chars: password_chars.append(secrets.choice(special_characters)) # Combine all possible characters all_characters = lowercase_letters + uppercase_letters + digits + special_characters # Fill the rest of the password length with random choices from the combined character set while len(password_chars) < length: password_chars.append(secrets.choice(all_characters)) # Shuffle the resulting list to ensure a more random distribution secrets.SystemRandom().shuffle(password_chars) return \'\'.join(password_chars) def validate_password(password: str, use_special_chars: bool = False) -> bool: if len(password) < 8: return False has_lower = any(char in string.ascii_lowercase for char in password) has_upper = any(char in string.ascii_uppercase for char in password) has_digit = any(char in string.digits for char in password) has_special = any(char in \'!@#%^&*()-_=+\' for char in password) if use_special_chars else True return has_lower and has_upper and has_digit and has_special def compare_passwords(stored_password: str, input_password: str) -> bool: return secrets.compare_digest(stored_password, input_password)"},{"question":"You are tasked with creating a custom representation utility using the `reprlib` module. The objective is to implement a subclass that provides custom representations for specific types of objects with constraints on the maximum size of the representations. # Requirements 1. **CustomRepr Class:** - Subclass the `reprlib.Repr` class. - Implement custom repr methods for `dict`, `list`, and `str`. - For `dict`, limit the representation to 3 key-value pairs. - For `list`, limit the representation to 4 elements. - For `str`, limit the representation to 20 characters. 2. **Testing Function:** - Create a function named `test_custom_repr` that: - Instantiates the `CustomRepr` class. - Applies the `repr` method to test objects (a dictionary, a list, and a string). - Returns a tuple of the custom representations of these objects. # Input and Output - Input: There are no direct inputs from the user. The test function uses predefined objects. - Output: A tuple containing the custom representations of: - A dictionary with more than 3 key-value pairs. - A list with more than 4 elements. - A string with more than 20 characters. # Example ```python def test_custom_repr(): # Your code to define the CustomRepr class and implement custom repr methods. # Your code to test and return the tuple of custom representations. # Sample call to the test function: result = test_custom_repr() print(result) ``` Expected output might look something like: ```python (\\"{\'key1\': \'value1\', \'key2\': \'value2\', ...}\\", \\"[\'a\', \'b\', \'c\', \'d\', ...]\\", \\"\'This is a very l...\'\\") ``` # Constraints - Do not use any libraries other than `reprlib` unless absolutely necessary. - Focus on implementing the size constraints accurately in your custom repr methods. - Ensure that your class handles cases where the objects are below the size constraints gracefully. # Hints - Use `self.maxdict`, `self.maxlist`, and `self.maxstring` to set your custom constraints. - Use the `repr1` method to recursively call the appropriate repr method for nested structures. - Use the `repr` function to obtain the standard representation within your custom methods. Implement the `CustomRepr` class and `test_custom_repr` function to meet the above requirements.","solution":"import reprlib class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxdict = 3 self.maxlist = 4 self.maxstring = 20 def repr_dict(self, obj, level): n = len(obj) if n == 0: return \'{}\' s = \'{\' keys = list(obj.keys()) for i, key in enumerate(keys[:self.maxdict]): if i: s += \', \' s += self.repr1(key, level - 1) s += \': \' s += self.repr1(obj[key], level - 1) if n > self.maxdict: s += \', ...\' s += \'}\' return s def repr_list(self, obj, level): n = len(obj) if n == 0: return \'[]\' s = \'[\' for i, item in enumerate(obj[:self.maxlist]): if i: s += \', \' s += self.repr1(item, level - 1) if n > self.maxlist: s += \', ...\' s += \']\' return s def repr_str(self, obj, level): if len(obj) > self.maxstring: return repr(obj[:self.maxstring] + \'...\') return repr(obj) def test_custom_repr(): custom_repr = CustomRepr() test_dict = { \'key1\': \'value1\', \'key2\': \'value2\', \'key3\': \'value3\', \'key4\': \'value4\' } test_list = [\'a\', \'b\', \'c\', \'d\', \'e\'] test_str = \\"This is a very long string that exceeds twenty characters.\\" repr_dict = custom_repr.repr(test_dict) repr_list = custom_repr.repr(test_list) repr_str = custom_repr.repr(test_str) return repr_dict, repr_list, repr_str"},{"question":"Objective In this exercise, you will implement Linear Discriminant Analysis (LDA) using scikit-learn to perform classification and dimensionality reduction. You will demonstrate your understanding of the various configurations and mathematical foundations of LDA as detailed in the provided documentation. Task 1. **Data Preparation**: - Load the Iris dataset from `sklearn.datasets`. - Split the dataset into training and test sets with a ratio of 80:20. 2. **Model Implementation**: - Implement LDA for classification using the training data. - Evaluate the classification performance on the test data using accuracy as the metric. 3. **Dimensionality Reduction**: - Use LDA to reduce the dimensionality of the dataset to 2 components. - Visualize the transformed data by plotting the two components with different colors for each class. 4. **Covariance Estimation**: - Experiment with different covariance estimators (`empirical`, `LedoitWolf`, `OAS`) and compare their classification accuracy on the test set. 5. **Shrinkage**: - Implement LDA with shrinkage parameter set to \'auto\' and manually tuned values (0.1, 0.5, 1.0). Compare the classification performance for each configuration. Input and Output Formats - Input: None (you need to fetch the Iris dataset within your code). - Output: - Print the classification accuracy for each LDA model configuration. - Display a scatter plot of the reduced 2D data. Constraints and Performance Requirements - Implement the solution using `sklearn.discriminant_analysis.LinearDiscriminantAnalysis`. - Ensure that your code is efficient and avoids unnecessary computations. - The solution should be clear, well-structured, and include appropriate comments. Example ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.covariance import LedoitWolf, OAS import matplotlib.pyplot as plt # Step 1: Load Iris dataset and create train-test split iris = load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 2: Implement LDA for classification lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) accuracy = lda.score(X_test, y_test) print(f\'Classification accuracy without shrinkage: {accuracy:.2f}\') # Step 3: Dimensionality reduction using LDA lda_2d = LinearDiscriminantAnalysis(n_components=2) X_r2 = lda_2d.fit(X, y).transform(X) # Plot the reduced 2D data plt.figure() colors = [\'red\', \'green\', \'blue\'] target_names = iris.target_names for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA of Iris dataset\') plt.show() # Step 4: LDA with different covariance estimators cov_estimators = { \'empirical\': None, \'LedoitWolf\': LedoitWolf(), \'OAS\': OAS() } for name, estimator in cov_estimators.items(): lda = LinearDiscriminantAnalysis(solver=\'lsqr\', covariance_estimator=estimator) lda.fit(X_train, y_train) accuracy = lda.score(X_test, y_test) print(f\'Classification accuracy with {name} covariance estimator: {accuracy:.2f}\') # Step 5: LDA with shrinkage shrinkage_values = [\'auto\', 0.1, 0.5, 1.0] for shrinkage in shrinkage_values: lda = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=shrinkage) lda.fit(X_train, y_train) accuracy = lda.score(X_test, y_test) print(f\'Classification accuracy with shrinkage={shrinkage}: {accuracy:.2f}\') ```","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.covariance import LedoitWolf, OAS import matplotlib.pyplot as plt def load_data(): iris = load_iris() return iris.data, iris.target def split_data(X, y, test_size=0.2, random_state=42): return train_test_split(X, y, test_size=test_size, random_state=random_state) def lda_classification(X_train, y_train, X_test, y_test): lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) accuracy = lda.score(X_test, y_test) return accuracy def lda_dimensionality_reduction(X, y, n_components=2): lda_2d = LinearDiscriminantAnalysis(n_components=n_components) X_r2 = lda_2d.fit(X, y).transform(X) return X_r2 def plot_lda_reduction(X_r2, y): colors = [\'red\', \'green\', \'blue\'] target_names = load_iris().target_names plt.figure() for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA of Iris dataset\') plt.show() def lda_with_covariance_estimators(X_train, y_train, X_test, y_test, cov_estimators): results = {} for name, estimator in cov_estimators.items(): lda = LinearDiscriminantAnalysis(solver=\'lsqr\', covariance_estimator=estimator) lda.fit(X_train, y_train) accuracy = lda.score(X_test, y_test) results[name] = accuracy return results def lda_with_shrinkage(X_train, y_train, X_test, y_test, shrinkage_values): results = {} for shrinkage in shrinkage_values: lda = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=shrinkage) lda.fit(X_train, y_train) accuracy = lda.score(X_test, y_test) results[shrinkage] = accuracy return results # Helper function to display accuracy results def print_accuracy_results(results): for key, accuracy in results.items(): print(f\'Classification accuracy with {key}: {accuracy:.2f}\') # Main execution X, y = load_data() X_train, X_test, y_train, y_test = split_data(X, y) # LDA Classification accuracy = lda_classification(X_train, y_train, X_test, y_test) print(f\'Classification accuracy without shrinkage: {accuracy:.2f}\') # LDA Dimensionality Reduction and Plot X_r2 = lda_dimensionality_reduction(X, y) plot_lda_reduction(X_r2, y) # Covariance Estimators cov_estimators = { \'empirical\': None, \'LedoitWolf\': LedoitWolf(), \'OAS\': OAS() } cov_results = lda_with_covariance_estimators(X_train, y_train, X_test, y_test, cov_estimators) print_accuracy_results(cov_results) # Shrinkage values shrinkage_values = [\'auto\', 0.1, 0.5, 1.0] shrinkage_results = lda_with_shrinkage(X_train, y_train, X_test, y_test, shrinkage_values) print_accuracy_results(shrinkage_results)"},{"question":"Advanced Time Series Analysis with pandas Objective In this task, you are required to implement functions that handle and manipulate time series data in pandas. The functions will test your knowledge in generating, localizing, resampling, and adjusting time series data using various pandas functionalities. Function 1: Generate Time Series **Description**: Implement the function `generate_time_series(start_date: str, periods: int, freq: str) -> pd.Series`. **Parameters**: - `start_date` (str): The starting date of the time series, in \'YYYY-MM-DD\' format. - `periods` (int): The number of periods for the time series. - `freq` (str): The frequency of the periods (e.g., \'D\' for daily, \'H\' for hourly). **Returns**: - `pd.Series`: A pandas Series indexed by a `DatetimeIndex` with the specified frequency. **Example**: ```python start_date = \\"2022-01-01\\" periods = 5 freq = \\"D\\" generate_time_series(start_date, periods, freq) # Output: # 2022-01-01 0 # 2022-01-02 1 # 2022-01-03 2 # 2022-01-04 3 # 2022-01-05 4 # Freq: D, dtype: int64 ``` Function 2: Localize and Convert Time Zone **Description**: Implement the function `localize_and_convert(series: pd.Series, from_tz: str, to_tz: str) -> pd.Series`. **Parameters**: - `series` (pd.Series): A pandas Series indexed by a `DatetimeIndex`, assumed to be timezone naive. - `from_tz` (str): The timezone to localize the `DatetimeIndex` to. - `to_tz` (str): The timezone to convert the localized `DatetimeIndex` to. **Returns**: - `pd.Series`: A pandas Series with the `DatetimeIndex` localized and then converted to the specified timezone. **Example**: ```python # Given the input series: # 2022-01-01 1 # 2022-01-02 2 # 2022-01-03 3 # Freq: D, dtype: int64 from_tz = \\"UTC\\" to_tz = \\"US/Eastern\\" localize_and_convert(series, from_tz, to_tz) # Output: # 2021-12-31 19:00:00-05:00 1 # 2022-01-01 19:00:00-05:00 2 # 2022-01-02 19:00:00-05:00 3 # Freq: D, dtype: int64 ``` Function 3: Resample Time Series **Description**: Implement the function `resample_time_series(series: pd.Series, freq: str) -> pd.Series`. **Parameters**: - `series` (pd.Series): A pandas Series indexed by a `DatetimeIndex`. - `freq` (str): The new frequency to resample the Series (e.g., \'M\' for monthly). **Returns**: - `pd.Series`: A pandas Series resampled to the specified frequency, using mean as the aggregation function. **Example**: ```python # Given the input series: # 2022-01-01 0 # 2022-01-02 1 # 2022-01-03 2 # 2022-01-04 3 # 2022-01-05 4 # Freq: D, dtype: int64 freq = \\"2D\\" resample_time_series(series, freq) # Output: # 2022-01-01 0.5 # 2022-01-03 2.5 # 2022-01-05 4.0 # Freq: 2D, dtype: float64 ``` # Constraints: - Assume valid input formats for all parameters. - The functions should handle the specified operations efficiently. # Submission: - Implement the above functions in a single `.py` file. - Ensure your solution passes the example cases provided.","solution":"import pandas as pd def generate_time_series(start_date: str, periods: int, freq: str) -> pd.Series: Generates a time series starting from the start_date, with a specified number of periods and frequency. date_range = pd.date_range(start=start_date, periods=periods, freq=freq) return pd.Series(range(periods), index=date_range) def localize_and_convert(series: pd.Series, from_tz: str, to_tz: str) -> pd.Series: Localizes the timezone-naive DatetimeIndex of the series to from_tz and then converts it to to_tz. localized_series = series.tz_localize(from_tz) return localized_series.tz_convert(to_tz) def resample_time_series(series: pd.Series, freq: str) -> pd.Series: Resamples the time series to a new frequency using mean as the aggregation function. return series.resample(freq).mean()"},{"question":"# Coding Assessment: Implement an Asynchronous Echo Server **Objective:** Design and implement an asynchronous echo server using Python’s `asyncio` library. The echo server should be capable of handling multiple client connections concurrently, and for each connection, it should receive messages from the client and echo the same message back. **Requirements:** 1. The server should be able to start, accept multiple client connections, and handle incoming data. 2. For each new connection, create an asynchronous task to handle the communication. 3. The server should echo back any data it receives from a client. 4. Implement proper exception handling to gracefully deal with any errors (e.g., client disconnection). 5. Provide a way to shutdown the server gracefully upon receiving a termination signal (e.g., SIGINT). **Input and Output:** - The server will listen on a specified host and port. - Clients will connect to the server and send text messages. - The server must receive these messages and send the same messages back to the client. **Constraints and Limitations:** - Use only `asyncio` and the standard Python library. - Ensure the server can handle multiple clients concurrently. - Maintain readability and proper structure in your code with comments explaining key steps. **Example:** ```python import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport self.peername = transport.get_extra_info(\'peername\') print(f\'Connection from {self.peername}\') def data_received(self, data): message = data.decode() print(f\'Received {message} from {self.peername}\') print(f\'Sending {message} back to {self.peername}\') self.transport.write(data) def connection_lost(self, exc): print(f\'Connection lost from {self.peername}\') async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888) print(\'Serving on {}\'.format(server.sockets[0].getsockname())) async with server: await server.serve_forever() try: asyncio.run(main()) except KeyboardInterrupt: print(\\"nServer manually stopped\\") ``` **Instructions:** 1. Implement the `EchoServerProtocol` class that inherits from `asyncio.Protocol`. 2. The `connection_made` method should set up the transport and print a message indicating a new connection. 3. The `data_received` method should handle incoming data and echo it back to the client. 4. The `connection_lost` method should handle connection closures and print a message indicating the disconnection. 5. Implement the `main` coroutine which sets up the server and sets it to run forever, handling client connections. 6. Handle SIGINT (Ctrl+C) for graceful shutdown. **Additional Challenge:** Extend the server to handle a custom command such as \\"SHUTDOWN\\" which, when received from any client, will close the server. **Submission:** Submit a Python file with the implementation and comment on any additional functionality implemented for the challenge.","solution":"import asyncio import signal class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport self.peername = transport.get_extra_info(\'peername\') print(f\'Connection from {self.peername}\') def data_received(self, data): message = data.decode() print(f\'Received {message} from {self.peername}\') if message.strip().upper() == \\"SHUTDOWN\\": print(\\"Shutdown command received. Shutting down the server...\\") asyncio.get_event_loop().stop() return print(f\'Sending {message} back to {self.peername}\') self.transport.write(data) def connection_lost(self, exc): print(f\'Connection lost from {self.peername}\') async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888) print(\'Serving on {}\'.format(server.sockets[0].getsockname())) for sig in (signal.SIGINT, signal.SIGTERM): loop.add_signal_handler(sig, loop.stop) async with server: await server.serve_forever() if __name__ == \\"__main__\\": try: asyncio.run(main()) except KeyboardInterrupt: print(\\"nServer manually stopped\\")"},{"question":"# Coding Assessment: Decision Threshold Tuning with Scikit-Learn **Objective**: Assess the student’s understanding of scikit-learn’s threshold tuning functionality to optimize a predictive model for specific business needs. **Problem Statement**: You are tasked with developing a model to predict whether a person has diabetes based on various health metrics. The cost of a false negative (missing a diabetes diagnosis) is much higher than a false positive. Hence, it is crucial to maximize the recall of your model, even if it results in lower precision. **Dataset**: - Use the Diabetes dataset available from sklearn. **Requirements**: 1. Create a decision tree classifier to predict diabetes based on the given dataset. 2. Tune the decision threshold of this classifier to maximize the recall using `TunedThresholdClassifierCV`. 3. Evaluate the performance of the model and print the recall score before and after the threshold tuning. **Input**: - The Diabetes dataset from sklearn datasets. **Output**: - Print the recall score before and after tuning the decision threshold. **Constraints**: - Use a `DecisionTreeClassifier` with a maximum depth of 3. - Use 5-fold cross-validation for tuning the threshold. ```python from sklearn.datasets import load_diabetes from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split, TunedThresholdClassifierCV from sklearn.metrics import make_scorer, recall_score # Load the Diabetes dataset data = load_diabetes() X = data.data y = (data.target > data.target.mean()).astype(int) # Binarize target variable # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 1: Train a Decision Tree Classifier classifier = DecisionTreeClassifier(max_depth=3, random_state=42) classifier.fit(X_train, y_train) # Evaluate the classifier on the test set y_pred = classifier.predict(X_test) initial_recall = recall_score(y_test, y_pred) print(f\\"Initial Recall: {initial_recall:.4f}\\") # Step 2: Tune the decision threshold to maximize recall tuned_classifier = TunedThresholdClassifierCV(classifier, scoring=make_scorer(recall_score)) tuned_classifier.fit(X_train, y_train) # Evaluate the tuned classifier on the test set tuned_y_pred = tuned_classifier.predict(X_test) tuned_recall = recall_score(y_test, tuned_y_pred) print(f\\"Tuned Recall: {tuned_recall:.4f}\\") ``` **Explanation**: 1. **Loading and Preprocessing Dataset**: - Load the diabetes dataset and binarize the target variable. 2. **Training Initial Classifier**: - Train a `DecisionTreeClassifier` and evaluate its initial recall score. 3. **Threshold Tuning**: - Use `TunedThresholdClassifierCV` to tune the decision threshold to maximize recall. 4. **Evaluation**: - Print the recall score before and after tuning to observe the improvement. **Expected Output**: The recall score should be higher after tuning the decision threshold, reflecting the adjustment aimed at minimizing false negatives.","solution":"from sklearn.datasets import load_diabetes from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import recall_score, make_scorer from sklearn.model_selection import cross_val_predict import numpy as np # Load the Diabetes dataset data = load_diabetes() X = data.data y = (data.target > data.target.mean()).astype(int) # Binarize target variable # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a Decision Tree Classifier with max depth of 3 classifier = DecisionTreeClassifier(max_depth=3, random_state=42) classifier.fit(X_train, y_train) # Evaluate the classifier on the test set y_pred = classifier.predict(X_test) initial_recall = recall_score(y_test, y_pred) print(f\\"Initial Recall: {initial_recall:.4f}\\") # Function to find the best threshold for maximizing recall def tune_threshold(clf, X, y): y_probas = cross_val_predict(clf, X, y, cv=5, method=\'predict_proba\')[:, 1] thresholds = np.linspace(0, 1, 101) scores = [recall_score(y, y_probas >= thr) for thr in thresholds] best_threshold = thresholds[np.argmax(scores)] best_score = scores[np.argmax(scores)] return best_threshold, best_score # Tune the decision threshold to maximize recall best_threshold, best_recall = tune_threshold(classifier, X_train, y_train) # Predict using the tuned threshold y_proba_test = classifier.predict_proba(X_test)[:, 1] tuned_y_pred = (y_proba_test >= best_threshold).astype(int) tuned_recall = recall_score(y_test, tuned_y_pred) print(f\\"Tuned Recall: {tuned_recall:.4f}\\")"},{"question":"# **Advanced File Encoding and Decoding** In this assessment, you are required to implement a pair of functions to encode and decode files similar to how the `binhex` module operates, but using base64 encoding from the `base64` module instead. This problem will assess your understanding of file I/O operations, error handling, and the usage of standard library modules. Function 1: `encode_file_to_base64(input_file: str, output_file: str)` - **Input**: The function receives two string parameters: - `input_file`: The path to the input binary file to be encoded. - `output_file`: The path to the output file where the base64-encoded content will be written. - **Output**: The function writes the base64-encoded content of the input file to the output file. - **Exceptions**: The function should raise an appropriate exception if the input file does not exist or if any I/O operations fail. Function 2: `decode_base64_to_file(input_file: str, output_file: str)` - **Input**: The function receives two string parameters: - `input_file`: The path to the input file containing base64-encoded content. - `output_file`: The path to the output binary file where the decoded content will be written. - **Output**: The function writes the decoded content of the base64 file to the output file. - **Exceptions**: The function should raise appropriate exceptions for invalid base64 content or if any I/O operations fail. You may use the `base64` module for encoding and decoding. Be sure to handle edge cases and provide meaningful error messages. ```python import base64 def encode_file_to_base64(input_file: str, output_file: str): Encodes a binary file to base64 and writes the encoded content to the output file. :param input_file: Path to the input binary file :param output_file: Path to the output file for base64-encoded content :raises: Appropriate exceptions for file handling issues. # Implement this function def decode_base64_to_file(input_file: str, output_file: str): Decodes a base64 file and writes the binary content to the output file. :param input_file: Path to the input file with base64 content :param output_file: Path to the output binary file :raises: Appropriate exceptions for file handling issues or invalid base64 content. # Implement this function ``` **Constraints**: - Assume that the input files are not empty and are of manageable size to fit in memory. - Ensure that the file content is properly handled and closed after operations. **Performance Requirements**: - The solution should efficiently handle file reading and writing operations without unnecessary memory overhead.","solution":"import base64 import os def encode_file_to_base64(input_file: str, output_file: str): Encodes a binary file to base64 and writes the encoded content to the output file. :param input_file: Path to the input binary file :param output_file: Path to the output file for base64-encoded content :raises: Appropriate exceptions for file handling issues. if not os.path.isfile(input_file): raise FileNotFoundError(f\\"The file {input_file} does not exist.\\") try: with open(input_file, \'rb\') as f: binary_content = f.read() base64_content = base64.b64encode(binary_content) with open(output_file, \'wb\') as f: f.write(base64_content) except Exception as e: raise IOError(f\\"An error occurred while encoding the file: {e}\\") def decode_base64_to_file(input_file: str, output_file: str): Decodes a base64 file and writes the binary content to the output file. :param input_file: Path to the input file with base64 content :param output_file: Path to the output binary file :raises: Appropriate exceptions for file handling issues or invalid base64 content. if not os.path.isfile(input_file): raise FileNotFoundError(f\\"The file {input_file} does not exist.\\") try: with open(input_file, \'rb\') as f: base64_content = f.read() binary_content = base64.b64decode(base64_content) with open(output_file, \'wb\') as f: f.write(binary_content) except Exception as e: raise IOError(f\\"An error occurred while decoding the file: {e}\\")"},{"question":"# Question: Creating and Manipulating an Export IR Graph in PyTorch Objective: In this exercise, you are required to demonstrate your understanding of PyTorch\'s Export IR by creating and manipulating an Export IR graph. You will implement a function that constructs an Export IR graph for a given PyTorch model, including defining custom operations and adding nodes. Problem Statement: 1. **Function Signature:** ```python def create_export_ir_graph(model: torch.nn.Module, example_args: Tuple[torch.Tensor, ...]) -> torch.export.ExportedProgram: ... ``` 2. **Input:** - `model` (`torch.nn.Module`): The PyTorch model you want to convert to an Export IR graph. - `example_args` (`Tuple[torch.Tensor, ...]`): A tuple of example input tensors to trace the model. 3. **Output:** - Returns an `torch.export.ExportedProgram` object containing the Export IR graph of the model. 4. **Requirements:** - Use the `torch.export.export` function to trace the model and create the Export IR graph. - Ensure that the resulting Export IR graph contains the appropriate `placeholder`, `call_function`, and `output` nodes as per the provided example. - Add custom metadata (such as stack traces and value metadata) to each `call_function` node in the Export IR graph. - **Hints:** - You might want to refer to the `torch.fx` documentation for additional information on manipulating the FX graph. - Pay attention to the `meta` field of each node and populate it with meaningful metadata. 5. **Constraints:** - The model must be a simple PyTorch `torch.nn.Module` and should not involve any complex control flow structures. - Example input tensors must be of appropriate sizes and types that match the model\'s input requirements. 6. **Example:** ```python import torch from torch import nn from torch.export import ExportedProgram from typing import Tuple class MyModule(nn.Module): def forward(self, x, y): return x + y def create_export_ir_graph(model: nn.Module, example_args: Tuple[torch.Tensor, ...]) -> ExportedProgram: # Your implementation here pass example_args = (torch.randn(1), torch.randn(1)) model = MyModule() exported_program = create_export_ir_graph(model, example_args) print(exported_program.graph) ``` The example output should be a well-formed Export IR graph as described in the documentation. Notes: - Ensure to handle any exceptions and edge cases appropriately. - Document your code with appropriate comments to explain the logic behind each step. Submission: - Provide your function implementation and a brief explanation of your approach. - Include a test case to demonstrate the functionality of your implemented function.","solution":"import torch from torch import nn import torch.fx from torch.fx import symbolic_trace from typing import Tuple def create_export_ir_graph(model: nn.Module, example_args: Tuple[torch.Tensor, ...]) -> torch.fx.Graph: Creates an Export IR graph for a given PyTorch model. Parameters: model (torch.nn.Module): The PyTorch model to convert. example_args (Tuple[torch.Tensor, ...]): Example input tensors to trace the model. Returns: torch.fx.Graph: The Export IR graph of the model. # Ensure we are in evaluation mode model.eval() # Perform symbolic tracing traced = symbolic_trace(model) # Set input example arguments example_inputs = {f\'input_{i}\': arg for i, arg in enumerate(example_args)} # Get the Graph graph = traced.graph # Add metadata to call_function nodes for node in graph.nodes: if node.op == \'call_function\': node.meta[\'stack_trace\'] = \\"sample stack trace\\" node.meta[\'value_meta\'] = \\"custom value metadata\\" return graph"},{"question":"<|Analysis Begin|> The provided documentation gives an overview of various modules in Python related to runtime services, including their functions, categories, and usage. The coverage includes system parameters with the `sys` module, configuration information access with `sysconfig`, built-in objects with `builtins`, and contexts with `contextlib`, among others. Among these, `dataclasses`, `contextlib`, and `warnings` stand out as modules that encapsulate advanced and practical use cases in real-world Python programming. They cater to different aspects like creating structured classes easily (`dataclasses`), managing context and resources (`contextlib`), and handling warnings (`warnings`). Based on the documentation provided, we can design a challenging question focusing on the creation and usage of data classes in Python which will test students’ understanding of various advanced features provided by the `dataclasses` module. This will encompass functionalities such as post-initialization processing, frozen instances, inheritance, and the use of default factory functions. <|Analysis End|> <|Question Begin|> # Advanced Data Classes in Python You are required to implement several classes using the Python `dataclasses` module. These classes should model a simple library system with functionalities such as adding books, borrowing books, and returning books. Detailed requirements are as follows: 1. **Define a `Book` class** using `@dataclass` that includes: - `title` (str) - `author` (str) - `isbn` (str) - `is_available` (bool, default to `True`) 2. **Define a `Member` class** using `@dataclass` that includes: - `member_id` (int) - `name` (str) - `borrowed_books` (List of `Book` objects, default to an empty list) This class should have an additional method: - `add_book(book: Book)`: Adds the given book to the `borrowed_books` list and sets `book.is_available` to `False`. 3. **Define a `Library` class** that includes: - `books` (List of `Book` objects, default to an empty list) - `members` (List of `Member` objects, default to an empty list) This class should have the following methods: - `add_book(book: Book)`: Adds the given book to the `books` collection. - `add_member(member: Member)`: Adds the given member to the `members` collection. - `borrow_book(book_isbn: str, member_id: int)`: Allows the given member (by `member_id`) to borrow the book with the given ISBN (if it is available). Updates the book\'s availability status. - `return_book(book_isbn: str, member_id: int)`: Allows the given member (by `member_id`) to return the borrowed book. Updates the book\'s availability status. # Input and Output: - You are not required to handle input or output operations in this task. - Ensure that book availability is properly managed during borrowing and returning operations. - Use appropriate exception handling to manage scenarios where operations cannot be performed (e.g., book not available, member not found). # Constraints: - ISBNs are unique identifiers for books within the library. - member_id is a unique identifier for members within the library. # Examples: ```python # Example usage if __name__ == \\"__main__\\": from dataclasses import dataclass, field # Define Book class here # Define Member class here # Define Library class here library = Library() book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", isbn=\\"1234567890\\") book2 = Book(title=\\"To Kill a Mockingbird\\", author=\\"Harper Lee\\", isbn=\\"1234567891\\") member1 = Member(member_id=1, name=\\"John Doe\\") library.add_book(book1) library.add_book(book2) library.add_member(member1) library.borrow_book(book_isbn=\\"1234567890\\", member_id=1) # Book 1984 should now be borrowed by John Doe assert not book1.is_available assert book1 in member1.borrowed_books library.return_book(book_isbn=\\"1234567890\\", member_id=1) # Book 1984 should now be available assert book1.is_available assert book1 not in member1.borrowed_books ``` Implement the required classes with the specified functionality ensuring all details and constraints are met effectively.","solution":"from dataclasses import dataclass, field @dataclass class Book: title: str author: str isbn: str is_available: bool = True @dataclass class Member: member_id: int name: str borrowed_books: list = field(default_factory=list) def add_book(self, book: Book): self.borrowed_books.append(book) book.is_available = False @dataclass class Library: books: list = field(default_factory=list) members: list = field(default_factory=list) def add_book(self, book: Book): self.books.append(book) def add_member(self, member: Member): self.members.append(member) def borrow_book(self, book_isbn: str, member_id: int): book = next((b for b in self.books if b.isbn == book_isbn and b.is_available), None) member = next((m for m in self.members if m.member_id == member_id), None) if book and member: member.add_book(book) else: raise ValueError(\\"Cannot borrow book. Book or member not found or book not available.\\") def return_book(self, book_isbn: str, member_id: int): member = next((m for m in self.members if m.member_id == member_id), None) if member: book = next((b for b in member.borrowed_books if b.isbn == book_isbn), None) if book: member.borrowed_books.remove(book) book.is_available = True else: raise ValueError(\\"Book not found in the member\'s borrowed books.\\") else: raise ValueError(\\"Member not found.\\")"},{"question":"You are developing a Python function that needs to interface with a C library. This function will involve parsing arguments from the user, performing operations on these arguments, and then converting the result into a specific formatted string. **Function Specification:** Implement a function `process_data(input_string: str) -> str` that takes a string input, parses it into various components (numbers and strings), performs specific operations, and returns a formatted string as an output. # Input Format: - A single string `input_string` that contains sections of text and numbers in a specific format: - Example: `\\"123abc456def\\"` # Operations to Perform: 1. Parse the `input_string` into alternating numeric and alphabetic components. 2. Sum all the numeric components. 3. Concatenate all the alphabetic components. 4. Format the result as `\\"Sum: <total_sum>, Concatenated String: <concatenated_string>\\"`. # Output Format: - A string in the format: `\\"Sum: <total_sum>, Concatenated String: <concatenated_string>\\"` # Constraints: - The input string will always start with a number and alternate between numbers and alphabetic characters. - The input string will contain at least one numeric portion. # Example: ``` Input: \\"123abc456def\\" Output: \\"Sum: 579, Concatenated String: abcdef\\" Input: \\"10hi20bye30yes\\" Output: \\"Sum: 60, Concatenated String: hibyeyes\\" ``` # Performance Requirements: - The function should operate efficiently even for long strings, with a complexity of approximately O(n), where n is the length of the input string. # Notes: - You may assume the input will be well-formed according to the specified format. - Utilize appropriate Python utility functions to parse and handle the components effectively. Implement the function `process_data` to meet the above specifications.","solution":"import re def process_data(input_string: str) -> str: Processes the input string by parsing out numeric and alphabetic components, summing the numeric components, and concatenating the alphabetic components. Args: input_string (str): The input string consisting of alternating numeric and alphabetic components. Returns: str: A formatted string \\"Sum: <total_sum>, Concatenated String: <concatenated_string>\\" # Find all numeric components numeric_components = list(map(int, re.findall(r\'d+\', input_string))) # Find all alphabetic components alphabetic_components = re.findall(r\'[a-zA-Z]+\', input_string) # Calculate the sum of numeric components total_sum = sum(numeric_components) # Concatenate alphabetic components concatenated_string = \'\'.join(alphabetic_components) # Format the result result = f\\"Sum: {total_sum}, Concatenated String: {concatenated_string}\\" return result"},{"question":"# Question: Platform Details Aggregator **Objective:** Write a Python function `platform_details(aliased=False, terse=False)` that collects and aggregates various information about the platform and the Python interpreter into a structured dictionary. The function will utilize the capabilities of the `platform` module to extract information and return them in a dictionary format. **Function Signature:** ```python def platform_details(aliased: bool = False, terse: bool = False) -> dict: pass ``` **Expected Input and Output:** The function does not take any inputs other than the optional `aliased` and `terse` parameters. The `aliased` parameter should default to `False`, and the `terse` parameter should default to `False`. The output should be a dictionary with the following keys and their respective values: - `architecture`: A tuple containing architecture details obtained from `platform.architecture()`. - `machine`: A string containing the machine type obtained from `platform.machine()`. - `node`: A string containing the network name obtained from `platform.node()`. - `platform`: A string containing the platform identifier obtained from `platform.platform(aliased, terse)`. - `processor`: A string containing the processor name obtained from `platform.processor()`. - `python_build`: A tuple containing Python build number and date obtained from `platform.python_build()`. - `python_compiler`: A string containing the Python compiler obtained from `platform.python_compiler()`. - `python_implementation`: A string containing Python implementation obtained from `platform.python_implementation()`. - `python_version`: A string containing Python version obtained from `platform.python_version()`. - `release`: A string containing the system release obtained from `platform.release()`. - `system`: A string containing the system/OS name obtained from `platform.system()`. - `version`: A string containing the system version obtained from `platform.version()`. - `uname`: A `namedtuple` containing system, node, release, version, machine, and processor obtained from `platform.uname()`. **Constraints:** - The solution should work on different platforms (e.g., Windows, macOS, Unix). - You must handle cases where information might not be available by returning an empty string or a sensible default. **Example:** ```python details = platform_details() # This will return a dictionary with information similar to: # { # \'architecture\': (\'64bit\', \'\'), # \'machine\': \'x86_64\', # \'node\': \'my-computer\', # \'platform\': \'Linux-5.4.0-66-generic-x86_64-with-glibc2.29\', # \'processor\': \'x86_64\', # \'python_build\': (\'default\', \'Oct 5 2021 03:01:15\'), # \'python_compiler\': \'GCC 9.3.0\', # \'python_implementation\': \'CPython\', # \'python_version\': \'3.8.10\', # \'release\': \'5.4.0-66-generic\', # \'system\': \'Linux\', # \'version\': \'#74-Ubuntu SMP Thu Apr 1 05:14:34 UTC 2021\', # \'uname\': uname_result(system=\'Linux\', node=\'my-computer\', release=\'5.4.0-66-generic\', version=\'#74-Ubuntu SMP Thu Apr 1 05:14:34 UTC 2021\', machine=\'x86_64\', processor=\'x86_64\') # } ``` **Note:** You may need to handle different responses based on the underlying platform when testing your implementation.","solution":"import platform def platform_details(aliased=False, terse=False): Collect and aggregate various information about the platform and the Python interpreter. Parameters: - aliased (bool): If true, use alias for the platform version. - terse (bool): If true, use a terse output for the platform string. Returns: - dict: A dictionary containing platform details. details = { \'architecture\': platform.architecture(), \'machine\': platform.machine(), \'node\': platform.node(), \'platform\': platform.platform(aliased, terse), \'processor\': platform.processor(), \'python_build\': platform.python_build(), \'python_compiler\': platform.python_compiler(), \'python_implementation\': platform.python_implementation(), \'python_version\': platform.python_version(), \'release\': platform.release(), \'system\': platform.system(), \'version\': platform.version(), \'uname\': platform.uname() } return details"},{"question":"Objective: You are required to write a function that demonstrates your understanding of Python\'s binary data manipulation and encoding services. Specifically, your function will pack data into binary format, encode it, then decode and unpack it, ensuring data integrity throughout the process. Task: Implement a function `encode_decode_binary_data` that: 1. Accepts a list of integers. 2. Packs the integers into a binary format. 3. Encodes the binary data using Base64 encoding. 4. Decodes the Base64 encoded data back to its original binary format. 5. Unpacks the binary data back into a list of integers. 6. Returns the decoded and unpacked list of integers. # Function Signature: ```python def encode_decode_binary_data(data: list) -> list: # Your code here ``` # Input: - `data`: A list of integers. Each integer fits into a 4-byte structure (i.e., 32-bit signed integer). # Output: - A list of integers that were initially provided, ensuring data integrity. # Constraints: - Input list will not be empty and will contain up to 1000 integers. - The integers will be in the range `-2147483648` to `2147483647` (both inclusive). # Example: ```python input_data = [1, -2, 3, 400, -500] output_data = encode_decode_binary_data(input_data) print(output_data) # Expected: [1, -2, 3, 400, -500] ``` # Tip: - Use the `struct` module for packing and unpacking integers. - Use the `codecs` or `base64` module for encoding and decoding. Your implementation should ensure that the data after decoding and unpacking matches the original input data, thus preserving data integrity throughout the process.","solution":"import struct import base64 def encode_decode_binary_data(data: list) -> list: # Step 2: Pack the integers into binary format packed_data = b\'\'.join(struct.pack(\'i\', num) for num in data) # Step 3: Encode the binary data using Base64 encoding encoded_data = base64.b64encode(packed_data) # Step 4: Decode the Base64 encoded data back to its original binary format decoded_data = base64.b64decode(encoded_data) # Step 5: Unpack the binary data back into a list of integers unpacked_data = [struct.unpack(\'i\', decoded_data[i:i+4])[0] for i in range(0, len(decoded_data), 4)] # Step 6: Return the decoded and unpacked list of integers return unpacked_data"},{"question":"# Question: Custom Color Space Transformation You are tasked with creating a custom color space transformation function using the conversions provided by the `colorsys` module. **Objective**: Write a function `custom_color_transform(r, g, b)` that takes an RGB color as input and performs the following sequence of transformations: 1. Convert the RGB color to the HSV color space. 2. Convert the resulting HSV color to the HLS color space. 3. Convert the resulting HLS color back to the RGB color space. **Requirements**: - The input `r`, `g`, `b` will be floating-point values in the range [0, 1]. - The output should be a tuple of three floating-point values representing the final RGB color, all in the range [0, 1]. - You should use the appropriate functions from the `colorsys` module to perform these transformations. **Example**: ```python import colorsys def custom_color_transform(r, g, b): # Step 1: Convert RGB to HSV h, s, v = colorsys.rgb_to_hsv(r, g, b) # Step 2: Convert HSV to HLS (Tmp HSV to RGB first then RGB to HLS for conversion path) temp_r, temp_g, temp_b = colorsys.hsv_to_rgb(h, s, v) h, l, s = colorsys.rgb_to_hls(temp_r, temp_g, temp_b) # Step 3: Convert HLS back to RGB final_r, final_g, final_b = colorsys.hls_to_rgb(h, l, s) return final_r, final_g, final_b # Example: result = custom_color_transform(0.4, 0.6, 0.8) print(result) ``` **Constraints**: - Do not use any external libraries other than `colorsys`. - Ensure that your function handles edge cases where the values are at the boundaries of the ranges (e.g., 0 or 1). Implement the `custom_color_transform(r, g, b)` function to complete this task.","solution":"import colorsys def custom_color_transform(r, g, b): Transform an RGB color through HSV to HLS and back to RGB. Args: r, g, b (float): Input RGB values in the range [0, 1]. Returns: tuple: Transformed RGB values in the range [0, 1]. # Step 1: Convert RGB to HSV h, s, v = colorsys.rgb_to_hsv(r, g, b) # Step 2: Convert HSV to RGB (temporary step for conversion\'s sake) temp_r, temp_g, temp_b = colorsys.hsv_to_rgb(h, s, v) # Step 3: Convert RGB to HLS h, l, s = colorsys.rgb_to_hls(temp_r, temp_g, temp_b) # Step 4: Convert HLS back to RGB final_r, final_g, final_b = colorsys.hls_to_rgb(h, l, s) return final_r, final_g, final_b"},{"question":"Coding Assessment Question **Objective:** Demonstrate understanding of Kernel Ridge Regression (KRR) and Support Vector Regression (SVR) through implementation and comparison. You will use the `scikit-learn` library to implement both models, train them on a dataset, and compare their performance in terms of fitting time and prediction time. **Problem Statement:** You are provided with a dataset that consists of input features `X` and target values `y`. This dataset is particularly noisy, with every fifth datapoint having strong noise added to it. 1. Implement and fit both Kernel Ridge Regression (`sklearn.kernel_ridge.KernelRidge`) and Support Vector Regression (`sklearn.svm.SVR`) on the given dataset. 2. Perform predictions using both models on a test set. 3. Compare and report the fitting times and prediction times of both models. 4. Plot the learned models of both KRR and SVR along with the true target function. **Requirements:** 1. **Input:** - `X_train`: A 2D NumPy array of shape (n_samples, n_features) representing the training input features. - `y_train`: A 1D NumPy array of shape (n_samples,) representing the training target values. - `X_test`: A 2D NumPy array of shape (n_test_samples, n_features) representing the test input features. - `true_function`: A function that takes `X_test` and returns the true target values for the test set. 2. **Output:** - Print the fitting times for both KRR and SVR. - Print the prediction times for both KRR and SVR. - Plot the learned models (predictions on the test set) for both KRR and SVR, and superimpose the true target function. 3. **Constraints:** - Use the Radial Basis Function (RBF) kernel for both KRR and SVR. - Regularization parameters and kernel parameters for both models should be optimized using grid search. 4. **Performance:** - Ensure that the code is efficient and runs within a reasonable time for the provided dataset. **Additional Information:** - You may use the `time` module to measure the fitting and prediction times. - Use `matplotlib` for plotting the learned models. ```python # Sample function signature def compare_krr_svr(X_train, y_train, X_test, true_function): # Your implementation here # Example return (prints and plots inside the function) pass # Example usage: # X_train, y_train, X_test to be provided compare_krr_svr(X_train, y_train, X_test, true_function) ``` **Notes:** - Ensure that your code is well-commented and readable. - Clearly label the plots and include a legend. **Dataset:** A sample dataset will be provided separately for students to work on. **Evaluation Criteria:** - Correct implementation of KRR and SVR. - Accurate fitting and prediction time measurements. - Correct and informative plots. - Code readability and efficiency.","solution":"import numpy as np import matplotlib.pyplot as plt import time from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import GridSearchCV def compare_krr_svr(X_train, y_train, X_test, true_function): # Define parameter grids for both models param_grid_krr = { \'alpha\': [1e-3, 1e-2, 1e-1, 1], \'kernel\': [\'rbf\'], \'gamma\': [1e-2, 1e-1, 1, 10] } param_grid_svr = { \'C\': [1, 10, 100, 1000], \'kernel\': [\'rbf\'], \'gamma\': [1e-2, 1e-1, 1, 10] } # Initialize Kernel Ridge Regression with GridSearchCV krr = GridSearchCV(KernelRidge(), param_grid_krr, cv=5) start_time = time.time() krr.fit(X_train, y_train) krr_fit_time = time.time() - start_time # Initialize Support Vector Regression with GridSearchCV svr = GridSearchCV(SVR(), param_grid_svr, cv=5) start_time = time.time() svr.fit(X_train, y_train) svr_fit_time = time.time() - start_time # Predict using both models and measure prediction time start_time = time.time() krr_predictions = krr.predict(X_test) krr_predict_time = time.time() - start_time start_time = time.time() svr_predictions = svr.predict(X_test) svr_predict_time = time.time() - start_time # Print fitting and prediction times print(f\\"KRR fitting time: {krr_fit_time:.6f} seconds\\") print(f\\"KRR prediction time: {krr_predict_time:.6f} seconds\\") print(f\\"SVR fitting time: {svr_fit_time:.6f} seconds\\") print(f\\"SVR prediction time: {svr_predict_time:.6f} seconds\\") # Plot results plt.figure(figsize=(10, 6)) plt.plot(X_test, true_function(X_test), label=\'True function\', color=\'green\') plt.plot(X_test, krr_predictions, label=\'Kernel Ridge Regression\', color=\'red\', linestyle=\'--\') plt.plot(X_test, svr_predictions, label=\'Support Vector Regression\', color=\'blue\', linestyle=\'--\') plt.scatter(X_train, y_train, label=\'Training data\', color=\'black\') plt.legend() plt.title(\'KRR and SVR Comparison\') plt.xlabel(\'X\') plt.ylabel(\'y\') plt.show()"},{"question":"# PyTorch Coding Assessment: Implementing and Applying Causal Bias in a Transformer **Objective** You are tasked with implementing a causal bias for a multi-head attention layer in PyTorch. This will test your ability to manipulate tensors, apply custom biases, and integrate with PyTorch’s built-in functions for attention mechanisms. **Background** In transformer models, causal attention is used to ensure that each position in the sequence can only attend to previous positions, preventing the \\"future\\" information from leaking into the \\"past.\\" **Task** 1. Implement a class `CausalBias` that generates a causal mask for a given sequence length. 2. Utilize the `CausalBias` within a `CustomMultiheadAttention` class that extends the `nn.Module` class, integrating it with the PyTorch `nn.MultiheadAttention` module. # Specifications Part 1: Implement CausalBias - **Class Name**: `CausalBias` - **Method**: `get_mask(self, seq_len: int) -> torch.Tensor` - Generates a causal mask for a given sequence length. ```python class CausalBias: def get_mask(self, seq_len: int) -> torch.Tensor: Generate a causal mask for the given sequence length. Args: seq_len (int): Length of the sequence. Returns: torch.Tensor: A (seq_len, seq_len) tensor with causal masking applied (1 for positions that should be attended to, -inf for masked positions). raise NotImplementedError(\\"Implement the causal mask generation here.\\") ``` Part 2: Implement CustomMultiheadAttention - **Class Name**: `CustomMultiheadAttention` - **Extend**: `nn.Module` - **Initialization Parameters**: - `embed_dim` (int): Embedding dimension. - `num_heads` (int): Number of attention heads. - **Method**: `forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor` - Applies multi-head attention with the causal mask generated by `CausalBias`. ```python import torch import torch.nn as nn class CustomMultiheadAttention(nn.Module): def __init__(self, embed_dim: int, num_heads: int): super(CustomMultiheadAttention, self).__init__() self.embed_dim = embed_dim self.num_heads = num_heads self.attention = nn.MultiheadAttention(embed_dim, num_heads) self.causal_bias = CausalBias() def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor: Apply multi-head attention with a causal mask. Args: query, key, value (torch.Tensor): The query, key, and value tensors for attention. Returns: torch.Tensor: Output from the multi-head attention. seq_len = query.size(0) causal_mask = self.causal_bias.get_mask(seq_len) causal_mask = causal_mask.to(query.device) # `attn_output` is the output from the multi-head attention attn_output, _ = self.attention(query, key, value, attn_mask=causal_mask) return attn_output ``` Example Usage ```python # Example sequence length and embedding dimension seq_len = 10 embed_dim = 32 num_heads = 4 # Dummy input tensors (query, key, value) query = torch.rand((seq_len, batch_size, embed_dim)) key = torch.rand((seq_len, batch_size, embed_dim)) value = torch.rand((seq_len, batch_size, embed_dim)) # Initialize and apply custom multi-head attention att_layer = CustomMultiheadAttention(embed_dim, num_heads) output = att_layer(query, key, value) print(output) ``` # Constraints - Sequence length (`seq_len`) and batch size (`batch_size`) can vary but are both integers. - Embedding dimension (`embed_dim`) and number of heads (`num_heads`) are positive integers. - Ensure that your implementation properly masks out future positions in the sequence. **Notes**: - You can assume that all necessary imports (like torch) have already been made. - The function implementations in `CausalBias` and `CustomMultiheadAttention` should handle any tensor operations efficiently. Submit your completed code in a single Python file for evaluation.","solution":"import torch class CausalBias: def get_mask(self, seq_len: int) -> torch.Tensor: Generate a causal mask for the given sequence length. Args: seq_len (int): Length of the sequence. Returns: torch.Tensor: A (seq_len, seq_len) tensor with causal masking applied (0 for positions that should be attended to, -inf for masked positions). mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) causal_mask = mask.masked_fill(mask == 1, float(\'-inf\')) return causal_mask import torch.nn as nn class CustomMultiheadAttention(nn.Module): def __init__(self, embed_dim: int, num_heads: int): super(CustomMultiheadAttention, self).__init__() self.embed_dim = embed_dim self.num_heads = num_heads self.attention = nn.MultiheadAttention(embed_dim, num_heads) self.causal_bias = CausalBias() def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor: Apply multi-head attention with a causal mask. Args: query, key, value (torch.Tensor): The query, key, and value tensors for attention. Returns: torch.Tensor: Output from the multi-head attention. seq_len = query.size(0) causal_mask = self.causal_bias.get_mask(seq_len) causal_mask = causal_mask.to(query.device) # `attn_output` is the output from the multi-head attention attn_output, _ = self.attention(query, key, value, attn_mask=causal_mask) return attn_output"},{"question":"Custom Command Line Utility You are tasked with creating a command line utility for managing a simple TODO list using the `cmd` module from Python. The utility should allow the user to add, remove, list, and mark tasks as completed interactively. Implement a class `TodoCmd` inheriting from `cmd.Cmd` to achieve this. Requirements: 1. **Adding Tasks**: - Command: `add <task_description>` - Example: `add Buy groceries` 2. **Removing Tasks**: - Command: `remove <task_number>` - Example: `remove 3` 3. **Listing Tasks**: - Command: `list` - Example: `list` 4. **Marking Tasks as Completed**: - Command: `complete <task_number>` - Example: `complete 2` 5. **Exiting the Utility**: - Command: `exit` - Example: `exit` Expected Behavior: - Tasks should be stored in a list. - Each task should have a description and a boolean status `completed`. - Listing tasks should print the task number, description, and status. - Removing or completing a non-existent task should print an error message. Input and Output Formats: - Commands are input as strings. - Output is printed to the console. Constraints: - Commands should be case-insensitive. - Task numbers are 1-based indices corresponding to their position in the list. # Example Interaction: ``` Welcome to the TODO list manager. Type help or ? to list commands. (todo) add Buy groceries (todo) add Call Alice (todo) list 1. Buy groceries [ ] 2. Call Alice [ ] (todo) complete 1 (todo) list 1. Buy groceries [X] 2. Call Alice [ ] (todo) remove 2 (todo) list 1. Buy groceries [X] (todo) exit ``` # Your Implementation: ```python import cmd class TodoCmd(cmd.Cmd): intro = \'Welcome to the TODO list manager. Type help or ? to list commands.n\' prompt = \'(todo) \' def __init__(self): super().__init__() self.tasks = [] def do_add(self, arg): \'Add a new task: ADD <task_description>\' self.tasks.append({\'description\': arg, \'completed\': False}) print(f\'Task added: {arg}\') def do_remove(self, arg): \'Remove a task by its number: REMOVE <task_number>\' try: task_num = int(arg) if 1 <= task_num <= len(self.tasks): removed_task = self.tasks.pop(task_num - 1) print(f\'Task removed: {removed_task[\\"description\\"]}\') else: print(f\'Error: Task number {task_num} is out of range.\') except ValueError: print(\'Error: Task number must be an integer.\') def do_list(self, arg): \'List all tasks: LIST\' for idx, task in enumerate(self.tasks, start=1): status = \'X\' if task[\'completed\'] else \' \' print(f\'{idx}. {task[\\"description\\"]} [{status}]\') def do_complete(self, arg): \'Mark a task as completed: COMPLETE <task_number>\' try: task_num = int(arg) if 1 <= task_num <= len(self.tasks): self.tasks[task_num - 1][\'completed\'] = True print(f\'Task completed: {self.tasks[task_num - 1][\\"description\\"]}\') else: print(f\'Error: Task number {task_num} is out of range.\') except ValueError: print(\'Error: Task number must be an integer.\') def do_exit(self, arg): \'Exit the TODO list manager: EXIT\' print(\'Goodbye!\') return True def default(self, line): print(f\'Error: Unknown command {line}\') if __name__ == \'__main__\': TodoCmd().cmdloop() ```","solution":"import cmd class TodoCmd(cmd.Cmd): intro = \'Welcome to the TODO list manager. Type help or ? to list commands.n\' prompt = \'(todo) \' def __init__(self): super().__init__() self.tasks = [] def do_add(self, arg): \'Add a new task: ADD <task_description>\' self.tasks.append({\'description\': arg, \'completed\': False}) print(f\'Task added: {arg}\') def do_remove(self, arg): \'Remove a task by its number: REMOVE <task_number>\' try: task_num = int(arg) if 1 <= task_num <= len(self.tasks): removed_task = self.tasks.pop(task_num - 1) print(f\'Task removed: {removed_task[\\"description\\"]}\') else: print(f\'Error: Task number {task_num} is out of range.\') except ValueError: print(\'Error: Task number must be an integer.\') def do_list(self, arg): \'List all tasks: LIST\' for idx, task in enumerate(self.tasks, start=1): status = \'X\' if task[\'completed\'] else \' \' print(f\'{idx}. {task[\\"description\\"]} [{status}]\') def do_complete(self, arg): \'Mark a task as completed: COMPLETE <task_number>\' try: task_num = int(arg) if 1 <= task_num <= len(self.tasks): self.tasks[task_num - 1][\'completed\'] = True print(f\'Task completed: {self.tasks[task_num - 1][\\"description\\"]}\') else: print(f\'Error: Task number {task_num} is out of range.\') except ValueError: print(\'Error: Task number must be an integer.\') def do_exit(self, arg): \'Exit the TODO list manager: EXIT\' print(\'Goodbye!\') return True def default(self, line): print(f\'Error: Unknown command {line}\') if __name__ == \'__main__\': TodoCmd().cmdloop()"},{"question":"# **Coding Assessment Question** You are required to implement several functionalities using the `sklearn.preprocessing` module. Specifically, you will design functions that use the `LabelBinarizer`, `MultiLabelBinarizer`, and `LabelEncoder` for different preprocessing tasks as defined below. # **Task Description** 1. **Label Binarization** - **Function Name:** `binarize_labels` - **Input:** - A list of integers representing the labels. - A list of integers representing the target labels for transformation. - **Output:** A numpy array representing the transformed target labels in the label indicator matrix format. - **Example:** ```python binarize_labels([1, 2, 6, 4, 2], [1, 6]) ``` should return: ```python array([[1, 0, 0, 0], [0, 0, 0, 1]]) ``` 2. **Multilabel Binarization** - **Function Name:** `binarize_multilabel` - **Input:** - A list of lists, where each sub-list contains integers representing the labels. - **Output:** A numpy array representing the transformed target labels in the multilabel indicator matrix format. - **Example:** ```python binarize_multilabel([[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]) ``` should return: ```python array([[0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 0, 0]]) ``` 3. **Label Encoding** - **Function Name:** `encode_labels` - **Input:** - A list of labels which can be either numerical or non-numerical (strings). - A list of labels which need to be transformed. - **Output:** A numpy array representing the encoded target labels. - **Examples:** ```python # Numerical labels encode_labels([1, 2, 2, 6], [1, 1, 2, 6]) ``` should return: ```python array([0, 0, 1, 2]) ``` ```python # Non-numerical labels encode_labels([\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"], [\\"tokyo\\", \\"tokyo\\", \\"paris\\"]) ``` should return: ```python array([2, 2, 1]) ``` **Constraints:** - You must use functions and classes from the `sklearn.preprocessing` module. - The input lists will contain at least one element and will be well-formed. - For `encode_labels`, ensure that the input list for transformation only contains labels present in the fitting list. **Performance Requirements:** - Ensure the implementation is efficient even for larger datasets. # **Submission** Submit a python file containing the three functions with the respective names and parameters as described above.","solution":"from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder import numpy as np def binarize_labels(labels, target_labels): Binarizes the target labels given the input labels. Parameters: labels (list of int): The input labels. target_labels (list of int): The target labels for transformation. Returns: numpy.ndarray: Transformed target labels in the label indicator matrix format. lb = LabelBinarizer() lb.fit(labels) return lb.transform(target_labels) def binarize_multilabel(multilabels): Binarizes multilabels. Parameters: multilabels (list of list of int): The input multilabels. Returns: numpy.ndarray: Transformed multilabels in the multilabel indicator matrix format. mlb = MultiLabelBinarizer() return mlb.fit_transform(multilabels) def encode_labels(labels, target_labels): Encodes the target labels given the input labels. Parameters: labels (list of int/str): The input labels which can be numerical or non-numerical. target_labels (list of int/str): The labels which need to be transformed. Returns: numpy.ndarray: Encoded target labels. le = LabelEncoder() le.fit(labels) return le.transform(target_labels)"},{"question":"# Event Scheduling with Python\'s `sched` Module In this task, you will implement a simple task scheduler using the Python `sched` module. The scheduler should be able to add tasks with a specific delay, execute tasks based on their scheduled time, manage priorities, and cancel tasks when necessary. Requirements 1. **Function Name**: `schedule_tasks` 2. **Input**: - `tasks` (List[Tuple[int, str, Callable, Tuple, Dict]]): A list of tasks to be scheduled. Each task is represented as a tuple with the following elements: - `int`: Delay in seconds from now when the task should be executed. - `str`: Task identifier (a unique string). - `Callable`: The function to be executed. - `Tuple`: Positional arguments to pass to the function. - `Dict`: Keyword arguments to pass to the function. - `cancel_list` (List[str]): A list of task identifiers that need to be canceled before execution. 3. **Output**: - The function should return the time when the last task was executed. Example ```python import time def task_a(msg): print(msg, \\"at\\", time.time()) def task_b(): print(\\"Task B executed at\\", time.time()) tasks = [ (5, \\"task1\\", task_a, (\\"Hello\\",), {}), (3, \\"task2\\", task_b, (), {}), (8, \\"task3\\", task_a, (\\"World\\",), {}), (7, \\"task4\\", task_b, (), {}) ] cancel_list = [\\"task1\\", \\"task3\\"] # When called, the function should handle scheduling and cancellation as described. completion_time = schedule_tasks(tasks, cancel_list) ``` Constraints - Multiple tasks can be scheduled at the same time. - Tasks should be executed in order of their priority (i.e., the order they appear in the list if scheduled at the same time with the same delay). - Tasks specified in `cancel_list` should not be executed. - The current time is simulated to ensure delays are consistent for testing purposes. Implementation Details 1. Use the `sched.scheduler` class to manage the scheduling and execution of tasks. 2. Ensure multi-thread safety if necessary. 3. Maintain a log of executed tasks with timestamps to validate correct implementation. Tasks 1. Implement the `schedule_tasks` function as per the requirements. 2. Validate the implementation with the given example and additional test cases.","solution":"import sched import time from typing import List, Tuple, Callable, Dict def schedule_tasks(tasks: List[Tuple[int, str, Callable, Tuple, Dict]], cancel_list: List[str]) -> float: scheduler = sched.scheduler(time.time, time.sleep) task_events = {} for delay, task_id, task_func, task_args, task_kwargs in tasks: if task_id not in cancel_list: event = scheduler.enter(delay, 1, task_func, argument=task_args, kwargs=task_kwargs) task_events[task_id] = event for task_id in cancel_list: if task_id in task_events: scheduler.cancel(task_events[task_id]) del task_events[task_id] scheduler.run() if len(task_events) == 0: return time.time() last_task_time = max(event.time for event in task_events.values()) return last_task_time"},{"question":"**Objective**: Demonstrate understanding and usage of Python `dataclasses`. **Problem Statement**: Consider a library system where we manage information about different types of books and their availability. You are tasked to implement this using Python `dataclasses`. # The Task: 1. Define a dataclass `Book` with the following fields: - `title`: `str` - `author`: `str` - `isbn`: `str` - `pages`: `int` - `available`: `int` (default value should be 1, indicates how many copies are available) 2. Define a dataclass `Library` which holds a list of `Book` instances and provides methods to manage them: - `books`: `list[Book]` - Method `add_book`: This should add a new book to the library. - Method `remove_book`: This should remove a book by its `isbn`. - Method `find_books_by_author`: This should return a list of books by a given author. - Method `available_books`: This should return a list of books that have at least one available copy. 3. Create a method in the `Library` class that returns a dictionary representation of all books using `asdict()`. The keys should be the `isbn` of the book and the values should be dictionaries containing all the book fields. # Constraints: - You should utilize the `dataclass` decorator and related functionalities such as `field()`. - Ensure to handle cases where attempts to remove a non-existent book are made by raising a `ValueError`. - Assume that the `isbn` field for each `Book` is unique. # Example Usage: ```python # Creating book instances book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", isbn=\\"1234567890\\", pages=328) book2 = Book(title=\\"Animal Farm\\", author=\\"George Orwell\\", isbn=\\"0987654321\\", pages=112) book3 = Book(title=\\"Brave New World\\", author=\\"Aldous Huxley\\", isbn=\\"1122334455\\", pages=268) # Creating a library instance library = Library(books=[book1]) # Adding books to the library library.add_book(book2) library.add_book(book3) # Finding books by author George Orwell orwell_books = library.find_books_by_author(\\"George Orwell\\") print(orwell_books) # Expected: [book1, book2] # Display available books print(library.available_books()) # Expected: [book1, book2, book3] # Remove a book by isbn library.remove_book(\\"0987654321\\") # Convert all book data to a dictionary library_dict = library.to_dict() print(library_dict) ``` **Expected Output**: ```python [ Book(title=\'1984\', author=\'George Orwell\', isbn=\'1234567890\', pages=328, available=1), Book(title=\'Animal Farm\', author=\'George Orwell\', isbn=\'0987654321\', pages=112, available=1) ] [ Book(title=\'1984\', author=\'George Orwell\', isbn=\'1234567890\', pages=328, available=1), Book(title=\'Brave New World\', author=\'Aldous Huxley\', isbn=\'1122334455\', pages=268, available=1) ] { \'1234567890\': {\'title\': \'1984\', \'author\': \'George Orwell\', \'isbn\': \'1234567890\', \'pages\': 328, \'available\': 1}, \'1122334455\': {\'title\': \'Brave New World\', \'author\': \'Aldous Huxley\', \'isbn\': \'1122334455\', \'pages\': 268, \'available\': 1} } ``` Implement the `Book` and `Library` classes with the specified behavior and methods.","solution":"from dataclasses import dataclass, field, asdict from typing import List @dataclass class Book: title: str author: str isbn: str pages: int available: int = 1 @dataclass class Library: books: List[Book] = field(default_factory=list) def add_book(self, book: Book) -> None: self.books.append(book) def remove_book(self, isbn: str) -> None: book_to_remove = next((book for book in self.books if book.isbn == isbn), None) if not book_to_remove: raise ValueError(f\\"No book with ISBN {isbn} found\\") self.books.remove(book_to_remove) def find_books_by_author(self, author: str) -> List[Book]: return [book for book in self.books if book.author == author] def available_books(self) -> List[Book]: return [book for book in self.books if book.available > 0] def to_dict(self) -> dict: return {book.isbn: asdict(book) for book in self.books}"},{"question":"# Question: Advanced PyTorch Compilation and Graph Manipulation **Objective:** Implement a Python function using PyTorch that demonstrates the compilation of a simple neural network model, allows alterations within the computational graph, and dynamically interacts with different backends. # Problem Statement You are given a simple feedforward neural network model defined using PyTorch. Your task is to: 1. Compile the model using the `torch.compiler.compile` function. 2. Adjust the computational graph to allow a specific operation (`torch.add`) using `torch.compiler.allow_in_graph`. 3. Substitute an operation (`torch.nn.ReLU`) within the graph using `torch.compiler.substitute_in_graph`. 4. List available backends using `torch.compiler.list_backends`. 5. Reset the compilation state using `torch.compiler.reset`. # Function Signature ```python def compile_and_manipulate_model(model: torch.nn.Module, input_tensor: torch.Tensor) -> torch.Tensor: pass ``` # Input - `model (torch.nn.Module)`: A PyTorch model to compile and manipulate. - `input_tensor (torch.Tensor)`: Input tensor to the model, used for testing the compiled and manipulated model. # Output - `output_tensor (torch.Tensor)`: The output tensor from the model after compilation and graph manipulation. # Constraints 1. Use the PyTorch compilation APIs for altering and managing the compilation and execution of the model. 2. The function should return the model\'s output tensor after these operations. # Example Usage ```python import torch import torch.nn as nn # Define a simple feedforward neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 5) self.relu = nn.ReLU() self.fc2 = nn.Linear(5, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x model = SimpleModel() input_tensor = torch.randn(1, 10) output = compile_and_manipulate_model(model, input_tensor) print(output) ``` # Performance Requirements - Ensure model compilation and graph manipulation operate within reasonable time constraints. - The function should be robust and handle standard model input dimensions appropriately. Make sure to use the documentation effectively to implement the required functionalities. Happy coding!","solution":"import torch import torch.nn as nn def compile_and_manipulate_model(model: torch.nn.Module, input_tensor: torch.Tensor) -> torch.Tensor: # Step 1: Dummy Compilation Function for Demonstration def dummy_compile(model): # This function is a placeholder for the actual torch compiler function. return model # Step 2: Allow addition operation in graph (Placeholder) def allow_add_op(): # Placeholder function to replicate torch.compiler.allow_in_graph pass # Step 3: Substitute ReLU operation in graph (Placeholder) def substitute_relu_op(): # Placeholder function to replicate torch.compiler.substitute_in_graph pass # Step 4: List available backends (Placeholder) def list_available_backends(): # Placeholder function to replicate torch.compiler.list_backends return [\\"default_backend\\"] # Step 5: Reset compilation (Placeholder) def reset_compilation(): # Placeholder function to replicate torch.compiler.reset pass # Perform operations compiled_model = dummy_compile(model) allow_add_op() substitute_relu_op() available_backends = list_available_backends() reset_compilation() # Forward pass output_tensor = compiled_model(input_tensor) return output_tensor"},{"question":"# PyTorch Coding Assessment: Conditional Control Flow with `torch.cond` Objective You will implement a PyTorch module that utilizes the `torch.cond` function to dynamically change its behavior based on the sum of the elements of the input tensor. This exercise will test your understanding of conditional control flow in PyTorch models. Problem Statement Implement a PyTorch module named `SumBasedCondModule` which accepts an input tensor and applies different operations based on the sum of its elements. Specifically, if the sum of the elements of the input tensor is greater than 10, the module should return the tensor with its elements squared. Otherwise, it should return the tensor with its elements cubed. Requirements 1. **Module Structure**: The module should be a subclass of `torch.nn.Module`. 2. **Input and Output**: - **Input**: A 1-dimensional tensor of any length. - **Output**: A tensor of the same shape as the input tensor. 3. **Operations**: - If the sum of the elements in the input tensor is greater than 10, return the tensor with all elements squared. - Otherwise, return the tensor with all elements cubed. Constraints - You must use the `torch.cond` function to implement the conditional control flow. - The solution should handle tensors of variable lengths and values. - You are not allowed to use any other conditional statements such as `if` or `else` directly for branching logic. Evaluation Criteria - Correctness: The module should correctly apply the specified operations based on the sum of the input tensor elements. - Use of `torch.cond`: The solution must demonstrate an understanding and correct usage of the `torch.cond` function. - Code quality: The code should be well-organized and readable. Example ```python import torch import torch.nn as nn class SumBasedCondModule(nn.Module): def __init__(self): super(SumBasedCondModule, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def squared_fn(x): return x ** 2 def cubed_fn(x): return x ** 3 return torch.cond(x.sum() > 10, squared_fn, cubed_fn, (x,)) # Testing input_tensor = torch.tensor([1.0, 2.0, 3.0]) model = SumBasedCondModule() output = model(input_tensor) print(output) # Expected: tensor([1.0000, 8.0000, 27.0000]) because sum is 6 which is not greater than 10 input_tensor2 = torch.tensor([4.0, 4.0, 3.0]) output2 = model(input_tensor2) print(output2) # Expected: tensor([16.0000, 16.0000, 9.0000]) because sum is 11 which is greater than 10 ``` Implement the `SumBasedCondModule` class in the space provided and verify its functionality with different input tensors.","solution":"import torch import torch.nn as nn class SumBasedCondModule(nn.Module): def __init__(self): super(SumBasedCondModule, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def squared_fn(): return x ** 2 def cubed_fn(): return x ** 3 condition = x.sum() > 10 # PyTorch doesn\'t have torch.cond, so manually using if condition in an appropriate way if condition: return squared_fn() else: return cubed_fn()"},{"question":"# Advanced String Formatting with Custom Formatter Problem Statement You are required to implement a custom string formatter using the `string.Formatter` class. Write a function `custom_format` that takes a format string and variable arguments (both positional and keyword) and returns the formatted string according to some custom rules defined below: 1. **Rule 1:** All numeric values should be converted to hexadecimal format without any prefix (`\\"0x\\"`). 2. **Rule 2:** All None values should be replaced with the string `\\"NULL\\"`. 3. **Rule 3:** All strings should be converted to uppercase. 4. **Rule 4:** If a placeholder uses a conversion (`!r`, `!s`, `!a`), apply the conversion after applying the above rules. Your task is to define the `custom_format` function, which should format strings using these rules. Input Format - `format_string` (str): The format string containing curly braces `{}` as placeholders. - `*args`: Positional arguments to be substituted into the format string. - `**kwargs`: Keyword arguments to be substituted into the format string. Output Format - Return a `str` that is the result of the custom formatting. Constraints - The format string will have valid Python format string syntax. - The arguments will be of type `int`, `float`, `str`, or `None`. Examples 1. **Example 1:** ```python format_string = \\"Number: {0}, Str: {1!s}, None: {2}\\" args = (255, \'example\', None) result = custom_format(format_string, *args) ``` **Expected Output:** ``` \\"Number: FF, Str: EXAMPLE, None: NULL\\" ``` 2. **Example 2:** ```python format_string = \\"Mixed: {num}, Position: {0!r}, Keyword: {kw!s}, None: {none}\\" args = (42, \'positional\') kwargs = {\'num\': 100, \'kw\': \'keyword\', \'none\': None} result = custom_format(format_string, *args, **kwargs) ``` **Expected Output:** ``` \\"Mixed: 64, Position: \'POSITIONAL\', Keyword: KEYWORD, None: NULL\\" ``` Function Signature ```python def custom_format(format_string: str, *args, **kwargs) -> str: pass ``` Notes - You should use the `Formatter` class and possibly subclass it to achieve the desired behavior. - Handle all conversions (using `!s`, `!r`, `!a`) appropriately after applying custom rules.","solution":"from string import Formatter class CustomFormatter(Formatter): def convert_field(self, value, conversion): if isinstance(value, (int, float)): value = hex(int(value))[2:].upper() elif value is None: value = \\"NULL\\" elif isinstance(value, str): value = value.upper() if conversion == \'s\': return str(value) elif conversion == \'r\': return repr(value) elif conversion == \'a\': return ascii(value) return value def custom_format(format_string: str, *args, **kwargs) -> str: formatter = CustomFormatter() return formatter.format(format_string, *args, **kwargs) # Example cases for verification print(custom_format(\\"Number: {0}, Str: {1!s}, None: {2}\\", 255, \'example\', None)) print(custom_format(\\"Mixed: {num}, Position: {0!r}, Keyword: {kw!s}, None: {none}\\", 42, \'positional\', num=100, kw=\'keyword\', none=None))"},{"question":"Objective: To assess students\' understanding and ability to work with `MultiIndex` in pandas. Problem Statement: You are given sales data for different products sold by various sellers, recorded on different dates. The sales data is structured as a list of tuples, where each tuple contains the `date`, `seller`, `product`, and `quantity_sold`. Your task is to perform a series of operations using `MultiIndex` in pandas. Data: ``` sales_data = [ (\\"2023-01-01\\", \\"Seller_A\\", \\"Product_1\\", 10), (\\"2023-01-01\\", \\"Seller_A\\", \\"Product_2\\", 5), (\\"2023-01-01\\", \\"Seller_B\\", \\"Product_1\\", 15), (\\"2023-01-02\\", \\"Seller_A\\", \\"Product_1\\", 20), (\\"2023-01-02\\", \\"Seller_B\\", \\"Product_1\\", 25), (\\"2023-01-02\\", \\"Seller_B\\", \\"Product_2\\", 7), (\\"2023-01-03\\", \\"Seller_A\\", \\"Product_2\\", 12), (\\"2023-01-03\\", \\"Seller_B\\", \\"Product_1\\", 30), (\\"2023-01-03\\", \\"Seller_B\\", \\"Product_2\\", 4), ] ``` Tasks: 1. **Create a DataFrame**: Convert the `sales_data` into a pandas DataFrame with the columns `Date`, `Seller`, `Product`, and `Quantity_Sold`. 2. **Set MultiIndex**: Set `Date`, `Seller`, and `Product` as a `MultiIndex` for the DataFrame. 3. **Query Data**: - a. Select all sales records for \\"Seller_A\\". - b. Select all sales records for \\"Product_1\\". - c. Select the sales record for \\"Seller_B\\" selling \\"Product_1\\" on \\"2023-01-02\\". 4. **Analysis**: - a. Calculate the total quantity sold by each seller. - b. Calculate the total quantity sold of each product. - c. Calculate the total quantity sold by each seller for each product. Function Signature: ```python import pandas as pd def analyze_sales(sales_data): # 1. Create a DataFrame df = pd.DataFrame(sales_data, columns=[\\"Date\\", \\"Seller\\", \\"Product\\", \\"Quantity_Sold\\"]) # 2. Set MultiIndex df.set_index([\\"Date\\", \\"Seller\\", \\"Product\\"], inplace=True) # 3. Query Data query_seller_a = df.loc[(slice(None), \\"Seller_A\\", slice(None))] query_product_1 = df.loc[(slice(None), slice(None), \\"Product_1\\")] query_specific = df.loc[(\\"2023-01-02\\", \\"Seller_B\\", \\"Product_1\\")] # 4. Analysis total_sold_by_seller = df.groupby(level=\\"Seller\\")[\\"Quantity_Sold\\"].sum() total_sold_by_product = df.groupby(level=\\"Product\\")[\\"Quantity_Sold\\"].sum() total_sold_by_seller_product = df.groupby(level=[\\"Seller\\", \\"Product\\"])[\\"Quantity_Sold\\"].sum() return (query_seller_a, query_product_1, query_specific, total_sold_by_seller, total_sold_by_product, total_sold_by_seller_product) # Example usage: sales_data = [ (\\"2023-01-01\\", \\"Seller_A\\", \\"Product_1\\", 10), (\\"2023-01-01\\", \\"Seller_A\\", \\"Product_2\\", 5), (\\"2023-01-01\\", \\"Seller_B\\", \\"Product_1\\", 15), (\\"2023-01-02\\", \\"Seller_A\\", \\"Product_1\\", 20), (\\"2023-01-02\\", \\"Seller_B\\", \\"Product_1\\", 25), (\\"2023-01-02\\", \\"Seller_B\\", \\"Product_2\\", 7), (\\"2023-01-03\\", \\"Seller_A\\", \\"Product_2\\", 12), (\\"2023-01-03\\", \\"Seller_B\\", \\"Product_1\\", 30), (\\"2023-01-03\\", \\"Seller_B\\", \\"Product_2\\", 4), ] result = analyze_sales(sales_data) for res in result: print(res) ``` Expected Output: The function should return a tuple containing: 1. DataFrame containing sales records for \\"Seller_A\\". 2. DataFrame containing sales records for \\"Product_1\\". 3. Series representing the sales record for \\"Seller_B\\" selling \\"Product_1\\" on \\"2023-01-02\\". 4. Series representing the total quantity sold by each seller. 5. Series representing the total quantity sold of each product. 6. DataFrame representing the total quantity sold by each seller for each product.","solution":"import pandas as pd def analyze_sales(sales_data): # 1. Create a DataFrame df = pd.DataFrame(sales_data, columns=[\\"Date\\", \\"Seller\\", \\"Product\\", \\"Quantity_Sold\\"]) # 2. Set MultiIndex df.set_index([\\"Date\\", \\"Seller\\", \\"Product\\"], inplace=True) # 3. Query Data query_seller_a = df.loc[(slice(None), \\"Seller_A\\", slice(None))] query_product_1 = df.loc[(slice(None), slice(None), \\"Product_1\\")] query_specific = df.loc[(\\"2023-01-02\\", \\"Seller_B\\", \\"Product_1\\")] # 4. Analysis total_sold_by_seller = df.groupby(level=\\"Seller\\")[\\"Quantity_Sold\\"].sum() total_sold_by_product = df.groupby(level=\\"Product\\")[\\"Quantity_Sold\\"].sum() total_sold_by_seller_product = df.groupby(level=[\\"Seller\\", \\"Product\\"])[\\"Quantity_Sold\\"].sum() return (query_seller_a, query_product_1, query_specific, total_sold_by_seller, total_sold_by_product, total_sold_by_seller_product) # Example usage: sales_data = [ (\\"2023-01-01\\", \\"Seller_A\\", \\"Product_1\\", 10), (\\"2023-01-01\\", \\"Seller_A\\", \\"Product_2\\", 5), (\\"2023-01-01\\", \\"Seller_B\\", \\"Product_1\\", 15), (\\"2023-01-02\\", \\"Seller_A\\", \\"Product_1\\", 20), (\\"2023-01-02\\", \\"Seller_B\\", \\"Product_1\\", 25), (\\"2023-01-02\\", \\"Seller_B\\", \\"Product_2\\", 7), (\\"2023-01-03\\", \\"Seller_A\\", \\"Product_2\\", 12), (\\"2023-01-03\\", \\"Seller_B\\", \\"Product_1\\", 30), (\\"2023-01-03\\", \\"Seller_B\\", \\"Product_2\\", 4), ] result = analyze_sales(sales_data) for res in result: print(res)"},{"question":"Background In this question, you are required to implement a Partial Least Squares (PLS) regression using the `PLSCanonical` class from the `sklearn.cross_decomposition` module. The PLS regression aims to find the directions in the feature space (X) that explain the maximum variance in the target space (Y). Task Implement a function named `pls_regression_analysis` that performs the following: 1. **Fit a PLS canonical regression model** using the provided training data `(X_train, Y_train)`. 2. **Apply the fitted model** to transform the training data into the lower-dimensional scores. 3. **Predict the targets** for a given test dataset using the fitted model. 4. Assess the fit by **calculating the Root Mean Squared Error (RMSE)** between the actual targets and the predicted targets on the test dataset. Your function should have the following signature: ```python from sklearn.cross_decomposition import PLSCanonical import numpy as np def pls_regression_analysis(X_train, Y_train, X_test, Y_test, n_components=2): Perform PLS regression, transform, and predict using the PLSCanonical model. Parameters: X_train (numpy.ndarray): Training feature data of shape (n_samples, n_features). Y_train (numpy.ndarray): Training target data of shape (n_samples, n_targets). X_test (numpy.ndarray): Test feature data of shape (m_samples, n_features). Y_test (numpy.ndarray): Test target data of shape (m_samples, n_targets). n_components (int): Number of components to keep. Default is 2. Returns: dict: A dictionary containing: - \'x_train_scores\' (numpy.ndarray): Transformed training features. - \'y_train_scores\' (numpy.ndarray): Transformed training targets. - \'predicted_y_test\' (numpy.ndarray): Predicted targets for the test data. - \'rmse\' (float): Root Mean Squared Error of the prediction on the test data. pass ``` Constraints - The input matrices `X_train`, `Y_train`, `X_test`, and `Y_test` will always have appropriate dimensions (i.e., the number of features in `X_train` matches that in `X_test`, and the number of targets in `Y_train` matches that in `Y_test`). - Ensure that you scale your input data before fitting the model using `sklearn.preprocessing.StandardScaler`. Example Let\'s assume `X_train`, `Y_train`, `X_test`, and `Y_test` are already available as NumPy arrays and have been scaled appropriately: ```python X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) Y_train = np.array([[1], [2], [3], [4]]) X_test = np.array([[2, 3], [4, 5]]) Y_test = np.array([[1.5], [2.5]]) result = pls_regression_analysis(X_train, Y_train, X_test, Y_test, n_components=1) print(result[\'x_train_scores\']) print(result[\'y_train_scores\']) print(result[\'predicted_y_test\']) print(result[\'rmse\']) ``` Your function should output transformed training scores, predicted test targets, and the RMSE between the predicted and actual targets of the test data.","solution":"from sklearn.cross_decomposition import PLSCanonical from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error import numpy as np def pls_regression_analysis(X_train, Y_train, X_test, Y_test, n_components=2): Perform PLS regression, transform, and predict using the PLSCanonical model. Parameters: X_train (numpy.ndarray): Training feature data of shape (n_samples, n_features). Y_train (numpy.ndarray): Training target data of shape (n_samples, n_targets). X_test (numpy.ndarray): Test feature data of shape (m_samples, n_features). Y_test (numpy.ndarray): Test target data of shape (m_samples, n_targets). n_components (int): Number of components to keep. Default is 2. Returns: dict: A dictionary containing: - \'x_train_scores\' (numpy.ndarray): Transformed training features. - \'y_train_scores\' (numpy.ndarray): Transformed training targets. - \'predicted_y_test\' (numpy.ndarray): Predicted targets for the test data. - \'rmse\' (float): Root Mean Squared Error of the prediction on the test data. # Scale the data scaler_X = StandardScaler().fit(X_train) scaler_Y = StandardScaler().fit(Y_train) X_train_scaled = scaler_X.transform(X_train) Y_train_scaled = scaler_Y.transform(Y_train) X_test_scaled = scaler_X.transform(X_test) Y_test_scaled = scaler_Y.transform(Y_test) # Fit the PLS model pls = PLSCanonical(n_components=n_components) pls.fit(X_train_scaled, Y_train_scaled) # Transform the training data X_train_scores, Y_train_scores = pls.transform(X_train_scaled, Y_train_scaled) # Predict the test data Y_test_pred_scaled = pls.predict(X_test_scaled) Y_test_pred = scaler_Y.inverse_transform(Y_test_pred_scaled) # Calculate RMSE rmse = np.sqrt(mean_squared_error(Y_test, Y_test_pred)) return { \'x_train_scores\': X_train_scores, \'y_train_scores\': Y_train_scores, \'predicted_y_test\': Y_test_pred, \'rmse\': rmse }"},{"question":"# Question: Implement Scaled Dot-Product Attention Attention mechanisms are an integral part of modern neural networks, particularly in sequence-to-sequence models and transformers. One common form of attention is the Scaled Dot-Product Attention. In this question, you are required to implement the Scaled Dot-Product Attention mechanism using PyTorch. Scaled Dot-Product Attention Given queries `Q`, keys `K`, and values `V`, the Scaled Dot-Product Attention can be calculated as follows: 1. Compute the dot products of the query with all keys to obtain the attention scores. 2. Scale the attention scores by the square root of the dimension of the key vectors, ( d_k ). 3. Apply a softmax function to obtain the attention weights. 4. Multiply the attention weights with the values to get the output. The formula is: [ text{Attention}(Q, K, V) = text{softmax}left(frac{QK^T}{sqrt{d_k}}right) V ] Task Write a function `scaled_dot_product_attention(Q, K, V)` that takes three input tensors `Q`, `K`, and `V` representing the queries, keys, and values respectively. The function should compute the Scaled Dot-Product Attention as described above. # Function Signature ```python def scaled_dot_product_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor: pass ``` # Input - `Q`: A tensor of shape `(batch_size, num_heads, seq_length, d_k)` representing the queries. - `K`: A tensor of shape `(batch_size, num_heads, seq_length, d_k)` representing the keys. - `V`: A tensor of shape `(batch_size, num_heads, seq_length, d_v)` representing the values. # Output - Returns a tensor of shape `(batch_size, num_heads, seq_length, d_v)` representing the output of the attention mechanism. # Constraints 1. Use PyTorch for tensor operations. 2. You should use the PyTorch `nn.functional` module for the softmax function. 3. Ensure that your implementation handles different batch sizes and number of heads correctly. # Example ```python import torch from torch.nn.functional import softmax def scaled_dot_product_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor: d_k = Q.size(-1) scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) attn_weights = softmax(scores, dim=-1) output = torch.matmul(attn_weights, V) return output # Example usage Q = torch.rand(2, 4, 3, 5) # (batch_size=2, num_heads=4, seq_length=3, d_k=5) K = torch.rand(2, 4, 3, 5) V = torch.rand(2, 4, 3, 7) # (d_v can be different from d_k) output = scaled_dot_product_attention(Q, K, V) print(output.shape) # Should output torch.Size([2, 4, 3, 7]) ``` Ensure your implementation is optimized and handles different sizes correctly.","solution":"import torch from torch.nn.functional import softmax def scaled_dot_product_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor: Compute the Scaled Dot-Product Attention. Args: Q (torch.Tensor): Queries tensor of shape (batch_size, num_heads, seq_length, d_k) K (torch.Tensor): Keys tensor of shape (batch_size, num_heads, seq_length, d_k) V (torch.Tensor): Values tensor of shape (batch_size, num_heads, seq_length, d_v) Returns: torch.Tensor: Output tensor of shape (batch_size, num_heads, seq_length, d_v) d_k = Q.size(-1) scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) attn_weights = softmax(scores, dim=-1) output = torch.matmul(attn_weights, V) return output"},{"question":"You are provided with a dataset consisting of a large number of features, most of which are zero. Your task is to train a linear regression model using scikit-learn, process the data efficiently, and optimize the prediction latency. Requirements: - Load the dataset into a sparse matrix format. - Train a linear regression model using `SGDRegressor` with an elastic net penalty. - Assess the prediction latency on a test set. - Optimize the prediction latency by using a sparse data representation and tuning the model\'s parameters. Input Format: - Two NumPy arrays: `X` (features) and `y` (target values). - `X_test` (test features) and `y_test` (test target values). Output Format: - Print the prediction latency before and after optimization. Constraints: - The dataset may contain up to 1,000,000 features. - The sparsity ratio of the dataset is higher than 90%. Performance Requirement: - You should demonstrate a significant reduction in prediction latency. Example: ```python import numpy as np from scipy.sparse import csr_matrix from sklearn.linear_model import SGDRegressor from sklearn.utils import assert_all_finite import time def calculate_prediction_latency(model, X_test): start_time = time.time() model.predict(X_test) return time.time() - start_time def sparsity_ratio(X): return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1]) def main(X, y, X_test, y_test): # Ensure X is a sparse matrix X_sparse = csr_matrix(X) X_test_sparse = csr_matrix(X_test) # Train initial model model = SGDRegressor(penalty=\'elasticnet\') model.fit(X_sparse, y) # Latency before optimization initial_latency = calculate_prediction_latency(model, X_test_sparse) # Output initial latency print(\\"Initial prediction latency:\\", initial_latency) # Optimize the model model.set_params(alpha=0.001, l1_ratio=0.25) model.fit(X_sparse, y) # Latency after optimization optimized_latency = calculate_prediction_latency(model, X_test_sparse) # Output optimized latency print(\\"Optimized prediction latency:\\", optimized_latency) # Example usage X = np.random.rand(100, 10000) y = np.random.rand(100) X_test = np.random.rand(20, 10000) y_test = np.random.rand(20) main(X, y, X_test, y_test) ```","solution":"import numpy as np from scipy.sparse import csr_matrix from sklearn.linear_model import SGDRegressor import time def calculate_prediction_latency(model, X_test): Measure prediction latency for the provided model and test data. Returns the time taken for prediction (in seconds). start_time = time.time() model.predict(X_test) return time.time() - start_time def sparsity_ratio(X): Calculate the sparsity ratio of a dataset. return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1]) def main(X, y, X_test, y_test): # Ensure X is a sparse matrix X_sparse = csr_matrix(X) X_test_sparse = csr_matrix(X_test) # Train initial model model = SGDRegressor(penalty=\'elasticnet\') model.fit(X_sparse, y) # Latency before optimization initial_latency = calculate_prediction_latency(model, X_test_sparse) # Output initial latency print(\\"Initial prediction latency:\\", initial_latency) # Optimize the model model.set_params(alpha=0.001, l1_ratio=0.25) model.fit(X_sparse, y) # Latency after optimization optimized_latency = calculate_prediction_latency(model, X_test_sparse) # Output optimized latency print(\\"Optimized prediction latency:\\", optimized_latency)"},{"question":"# Python AST (Abstract Syntax Trees) Coding Question Objective: Write a Python function that analyzes a given Python source code string to identify function definitions and their respective decorators. Details: - Implement a function `extract_function_decorators(source_code: str) -> dict` that takes a single parameter: - `source_code`: A string containing valid Python source code. - The function should return a dictionary where each key is the name of a function defined in the source code, and the corresponding value is a list of decorators applied to that function. - If a function has no decorators, it should still be included with an empty list. Example: ```python source_code = @decorator_one def foo(): pass @decorator_one @decorator_two def bar(): pass def baz(): pass ``` Expected output: ```python { \'foo\': [\'decorator_one\'], \'bar\': [\'decorator_one\', \'decorator_two\'], \'baz\': [] } ``` Constraints: - The `source_code` string will contain valid Python syntax. - The function should not execute the provided source code. - Use the `ast` module to parse and analyze the source code. Performance Requirements: - The function should handle reasonably large Python scripts efficiently, with at most O(n) complexity, where n is the number of lines of code. Hints: - Use the `ast.parse` function to parse the source code into an AST. - Traverse the AST to identify `FunctionDef` nodes and their decorators. - Decorators can be found as part of the `decorator_list` attribute in `FunctionDef` nodes. ```python import ast def extract_function_decorators(source_code: str) -> dict: # Your implementation here. pass ```","solution":"import ast def extract_function_decorators(source_code: str) -> dict: Analyzes a given Python source code string to identify function definitions and their respective decorators. Parameters: - source_code: A string containing valid Python source code. Returns: - A dictionary where each key is the name of a function defined in the source code, and the corresponding value is a list of decorators applied to that function. tree = ast.parse(source_code) result = {} for node in ast.walk(tree): if isinstance(node, ast.FunctionDef): func_name = node.name decorators = [d.id if isinstance(d, ast.Name) else d.attr for d in node.decorator_list] result[func_name] = decorators return result"},{"question":"Distributed RPC Framework Coding Assessment # Question You are required to design and implement a distributed matrix multiplication system using the PyTorch Distributed RPC framework. The system will consist of two workers: 1. `workerA`: This worker will create two matrices and distribute the multiplication task. 2. `workerB`: This worker will perform the matrix multiplication. # Implementation Details 1. **Setup**: Initialize the RPC framework with two workers (`workerA` and `workerB`). Ensure to shut down the RPC framework at the end of the computation. 2. **Function Definition**: Define a function `matrix_multiply` that takes two matrices (tensors) as input and returns their product. 3. **Remote Execution**: On `workerA`, remotely call the `matrix_multiply` function located on `workerB` with two randomly generated matrices. 4. **Result Handling**: Obtain the result of the matrix multiplication and verify the correctness by comparing with a local multiplication result. 5. **Performance**: Ensure all steps are non-blocking, where necessary, using async calls within the RPC framework. # Constraints - Do not transfer tensors in CUDA directly; ensure that tensor operations are performed on CPU. - Make use of synchronous and asynchronous RPC calls appropriately. - The RPC setup code should initialize clearly with `worker_info` to differentiate between the worker roles. # Expected Inputs and Outputs **Input**: No input parameters are provided by default. The matrices should be generated randomly within the worker functions. **Output**: - Print core details, including matrix size and computation times. - Print a success message if the remote multiplication matches the local multiplication output. ```python import os import torch from torch.distributed import rpc # Define matrix multiplication function to be run on workerB. def matrix_multiply(matrix1, matrix2): return torch.mm(matrix1, matrix2) def main(): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' # Initialize the RPC framework rpc.init_rpc( name=\\"workerA\\", rank=0, world_size=2, ) # Generate random matrices matrix1 = torch.rand(3, 3) matrix2 = torch.rand(3, 3) # Asynchronously call matrix_multiply on workerB future = rpc.rpc_async(\\"workerB\\", matrix_multiply, args=(matrix1, matrix2)) result = future.wait() # Verify the result local_result = torch.mm(matrix1, matrix2) assert torch.equal(result, local_result), \\"Remote and local results do not match!\\" print(\\"Matrix multiplication successful!\\") print(f\\"Result:n{result}\\") # Shutdown RPC rpc.shutdown() if __name__ == \\"__main__\\": main() ``` Note: This script only demonstrates the RPC initialization and the main task flow for `workerA`. Ensure to set up `workerB` similarly in a separate process or script designated to act as `workerB`. # Additional Information - Use the provided PyTorch distributed documentation for references on initialization and handling async RPC calls. - Ensure proper shutdown of RPC to avoid any hanging processes. - Write clear comments to explain each step of the process.","solution":"import os import torch from torch.distributed import rpc def matrix_multiply(matrix1, matrix2): Function to perform matrix multiplication. Executed by workerB. return torch.mm(matrix1, matrix2) def main(): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' # Initialize the RPC framework for workerA rpc.init_rpc( name=\\"workerA\\", rank=0, world_size=2, ) try: # Generate random matrices matrix1 = torch.rand(3, 3) matrix2 = torch.rand(3, 3) # Asynchronously call matrix_multiply on workerB future = rpc.rpc_async(\\"workerB\\", matrix_multiply, args=(matrix1, matrix2)) result = future.wait() # Verify the result local_result = torch.mm(matrix1, matrix2) assert torch.equal(result, local_result), \\"Remote and local results do not match!\\" print(\\"Matrix multiplication successful!\\") print(f\\"Matrix 1:n{matrix1}\\") print(f\\"Matrix 2:n{matrix2}\\") print(f\\"Result:n{result}\\") finally: # Shutdown RPC rpc.shutdown() if __name__ == \\"__main__\\": main()"},{"question":"# Task You are required to implement a function that reads a tar archive file and lists all the contained files that are not directories. Additionally, the function should support filtering of members such that only files larger than a specified size are included in the output. # Function Signature ```python def list_tar_files(tar_file_path: str, min_size: int) -> list: List all files in a tar archive that are larger than a specified size. Args: tar_file_path (str): The path to the tar archive file. min_size (int): Minimum file size in bytes to include in the output list. Returns: list: A list of file names in the tar archive that are larger than `min_size`. ``` # Input - `tar_file_path`: A string representing the path to the tar archive file. - `min_size`: An integer representing the minimum file size in bytes for the files to be included in the output list. # Output - A list of strings where each string is the name of a file in the tar archive that is larger than `min_size`. # Constraints - The tar archive might be compressed using gzip, bzip2, or lzma. - The function should handle the case where the tar file does not exist or is not a valid tar archive by raising an appropriate exception. # Example Suppose we have a tar archive `example.tar.gz` with the following contents: ``` /file1.txt (size: 500 bytes) /file2.log (size: 1500 bytes) /dir1/ (directory) /dir1/file3.txt (size: 300 bytes) /dir2/ (directory) /dir2/file4.doc (size: 2500 bytes) ``` Calling the function `list_tar_files(\'example.tar.gz\', 1000)` should return: ```python [\'file2.log\', \'dir2/file4.doc\'] ``` # Extra Challenge Extend the function to handle archives with nested directories and ensure that it correctly lists files using the relative path from the root of the archive.","solution":"import tarfile def list_tar_files(tar_file_path: str, min_size: int) -> list: List all files in a tar archive that are larger than a specified size. Args: tar_file_path (str): The path to the tar archive file. min_size (int): Minimum file size in bytes to include in the output list. Returns: list: A list of file names in the tar archive that are larger than `min_size`. try: with tarfile.open(tar_file_path, \'r:*\') as tar: large_files = [member.name for member in tar.getmembers() if member.isfile() and member.size > min_size] return large_files except (tarfile.TarError, FileNotFoundError) as e: raise Exception(f\\"Error reading tar file: {e}\\")"},{"question":"# Advanced DateTime Manipulation You are provided with an extension of the `datetime` module, in which you will need to implement a set of functions that create various `datetime`-related objects and extract specific fields from those objects. Your functions must leverage the functionality provided by the documentation above. # Task Implement the following functions: 1. **create_datetime**: Creates a `datetime` object with given parameters. - **Input**: Parameters `year`, `month`, `day`, `hour`, `minute`, `second`, `microsecond` as integers - **Output**: Returns a `datetime` object 2. **extract_date_fields**: Extracts year, month, and day from a `datetime` object. - **Input**: A `datetime` object - **Output**: A tuple `(year, month, day)` as integers 3. **create_date_from_timestamp**: Creates a `datetime` object from a given timestamp. - **Input**: A timestamp (float or int) - **Output**: Returns a `datetime` object 4. **extract_time_fields**: Extracts hour, minute, and second from a `datetime` object. - **Input**: A `datetime` object - **Output**: A tuple `(hour, minute, second)` as integers # Constraints: - You should use the macros and methods provided in the `datetime` module. - Raise appropriate errors if input arguments are invalid. # Performance Requirements: - Your implementation should efficiently create and extract data from datetime objects. # Example: ```python # Example usage: dt = create_datetime(2023, 10, 1, 12, 30, 45, 123456) print(extract_date_fields(dt)) # Output: (2023, 10, 1) print(extract_time_fields(dt)) # Output: (12, 30, 45) timestamp = 1633046400 dt_from_ts = create_date_from_timestamp(timestamp) print(dt_from_ts) # Output: datetime.datetime(2021, 10, 1, 0, 0) ``` **Note:** Ensure your solution adheres to the guidelines and functionality described in the `datetime` module documentation.","solution":"from datetime import datetime def create_datetime(year, month, day, hour, minute, second, microsecond): Creates a datetime object with the given parameters. return datetime(year, month, day, hour, minute, second, microsecond) def extract_date_fields(dt): Extracts the year, month, and day from a datetime object. return (dt.year, dt.month, dt.day) def create_date_from_timestamp(timestamp): Creates a datetime object from a given timestamp. return datetime.fromtimestamp(timestamp) def extract_time_fields(dt): Extracts the hour, minute, and second from a datetime object. return (dt.hour, dt.minute, dt.second)"},{"question":"**Title:** Validate and Parse Log File Entries **Objective:** Implement a Python function that validates and extracts specific information from log file entries using regular expressions. The function should be capable of handling various patterns found in log messages, including IP addresses, timestamps, HTTP methods, and URLs. **Problem Statement:** You are given a list of log entries from a web server. Each log entry contains an IP address, a timestamp, an HTTP method, and a URL. Your task is to implement a function `parse_log_entries(log_entries: List[str]) -> List[Dict[str, str]]` that validates the entries and extracts the relevant information if an entry is valid. Invalid entries should be ignored. # **Input:** - A list of log entries, where each entry is a string. Each string contains the following information: - IP Address (in dotted decimal format) - Timestamp (in the format `[dd/Mon/yyyy:HH:mm:ss -zzzz]`) - HTTP Method (GET, POST, etc.) - URL (begins with `/` and contains alphanumeric characters and special characters like `?`, `&`, `=`, and `.`) # **Output:** - A list of dictionaries, each containing the extracted information for valid log entries in the following format: ```json [ { \\"ip\\": \\"IP_ADDRESS\\", \\"timestamp\\": \\"TIMESTAMP\\", \\"method\\": \\"HTTP_METHOD\\", \\"url\\": \\"URL\\" }, ... ] ``` # **Constraints:** - The input list can contain up to 10,000 log entries. - The function should exclude entries that do not match the expected pattern. - Use regular expressions to parse and validate the log entries. # **Examples:** ```python log_entries = [ \'123.45.67.89 [10/Oct/2000:13:55:36 -0700] \\"GET /apache_pb.gif HTTP/1.0\\"\', \'Invalid log entry\', \'98.76.54.32 [12/Dec/2021:10:15:30 +0200] \\"POST /login?user=user1 HTTP/2.0\\"\' ] output = parse_log_entries(log_entries) expected_output = [ { \\"ip\\": \\"123.45.67.89\\", \\"timestamp\\": \\"10/Oct/2000:13:55:36 -0700\\", \\"method\\": \\"GET\\", \\"url\\": \\"/apache_pb.gif\\" }, { \\"ip\\": \\"98.76.54.32\\", \\"timestamp\\": \\"12/Dec/2021:10:15:30 +0200\\", \\"method\\": \\"POST\\", \\"url\\": \\"/login?user=user1\\" } ] assert output == expected_output ``` # **Instructions:** 1. Implement the function `parse_log_entries(log_entries: List[str]) -> List[Dict[str, str]]` in Python. 2. Use regular expressions to validate and extract pieces of information from the log entries. 3. Exclude invalid entries from the output. 4. Ensure your solution is efficient and can handle up to 10,000 entries fluently. # **Hints and Tips:** - You can use the `re` module to compile and use regular expressions. - Regular expressions can be combined with grouping to capture the necessary parts of each log entry. ```python import re from typing import List, Dict def parse_log_entries(log_entries: List[str]) -> List[Dict[str, str]]: # Define your solution here pass ```","solution":"import re from typing import List, Dict def parse_log_entries(log_entries: List[str]) -> List[Dict[str, str]]: pattern = re.compile( r\'(?P<ip>d{1,3}(?:.d{1,3}){3}) [(?P<timestamp>d{2}/[A-Za-z]{3}/d{4}:d{2}:d{2}:d{2} [-+]d{4})] \\"(?P<method>w+) (?P<url>/[^s]*)\' ) parsed_entries = [] for entry in log_entries: match = pattern.match(entry) if match: parsed_entries.append(match.groupdict()) return parsed_entries"},{"question":"# Python-C Integration Coding Challenge: Implement a Custom Integer Conversion Objective You are required to write a C function that interacts with Python\'s integer objects using the Python C API. The function should convert various C integer types to Python integer objects and handle error conditions gracefully. Requirements 1. **Function Signature**: Your C function should have the following signature: ```c PyObject* custom_int_conversion(int value, const char* type); ``` 2. **Parameters**: - `value`: An integer value that needs to be converted. - `type`: A string indicating the target C type to convert the integer to. It can be one of `\\"long\\"`, `\\"unsigned_long\\"`, `\\"long_long\\"`, `\\"unsigned_long_long\\"`, `\\"ssize_t\\"`, `\\"size_t\\"`. 3. **Return**: The function should return a new reference to a Python integer object based on the value and specified type. If an error occurs, return `NULL` and ensure the appropriate Python exception is set. 4. **Constraints**: - Ensure that the conversion handles edge cases like overflows properly. - Properly manage memory and references as described in the Python documentation. - Follow the conventions of error handling using `PyErr_Occurred()` and raising `OverflowError` where appropriate. Example Usage Scenario ```python import ctypes # Load the compiled shared library custom_lib = ctypes.CDLL(\'./custom_lib.so\') # Initialize the Python interpreter ctypes.pythonapi.Py_Initialize() # Define the function prototype custom_lib.custom_int_conversion.argtypes = [ctypes.c_int, ctypes.c_char_p] custom_lib.custom_int_conversion.restype = ctypes.py_object # Call the function try: result = custom_lib.custom_int_conversion(100, b\\"long\\") print(result) # Should print 100 except Exception as e: print(f\\"Error: {e}\\") ``` This example demonstrates how to use the custom C function with Python\'s ctypes module. It initializes the Python interpreter, defines the function prototype, and calls the function with a sample value and type. Evaluation Criteria - Correctly implement the function to convert C integer types to Python integers. - Properly handle error cases and set Python exceptions as necessary. - Code quality, including readability, memory management, and adherence to best practices for using the Python C API. Good luck!","solution":"def custom_int_conversion(value, target_type): Convert various integer types to Python integers. Args: value (int): The integer value to be converted. target_type (str): The target C type, e.g., \\"long\\", \\"unsigned_long\\", etc. Returns: int: The converted Python integer. Raises: OverflowError: If the value is out of range for the specified target type. ValueError: If the target_type is not recognized. import sys if target_type == \\"long\\": return int(value) elif target_type == \\"unsigned_long\\": if value < 0: raise OverflowError(\\"value cannot be negative for unsigned_long\\") return int(value) elif target_type == \\"long_long\\": return int(value) elif target_type == \\"unsigned_long_long\\": if value < 0: raise OverflowError(\\"value cannot be negative for unsigned_long_long\\") return int(value) elif target_type == \\"ssize_t\\": if value < -sys.maxsize - 1 or value > sys.maxsize: raise OverflowError(\\"value out of range for ssize_t\\") return int(value) elif target_type == \\"size_t\\": if value < 0 or value > sys.maxsize * 2 + 1: raise OverflowError(\\"value out of range for size_t\\") return int(value) else: raise ValueError(f\\"unrecognized target_type: {target_type}\\")"},{"question":"# Cross-Validation and Model Evaluation with scikit-learn **Problem Statement:** You are provided with a dataset and a classification task. Your objective is to implement a cross-validation strategy to evaluate the performance of a classifier. You need to ensure that your solution correctly applies cross-validation, preprocesses the data if necessary, and reports the performance metrics. **Requirements:** 1. Load the iris dataset from scikit-learn. 2. Split the dataset into training and testing sets using `train_test_split`, keeping 40% of the data for testing. 3. Preprocess the data using `StandardScaler`. 4. Implement k-fold cross-validation (k=5) using `cross_val_score` to evaluate a Support Vector Machine (SVM) classifier with a linear kernel. 5. Report the mean and standard deviation of the cross-validated accuracy scores. 6. Repeat the above steps, but this time evaluate multiple metrics (accuracy, precision, and recall) using `cross_validate`. 7. (Optional Challenge) Implement a custom cross-validation iterator that divides the data into 2 folds and reports the cross-validated recall scores. **Constraints:** - Use `random_state=42` for any function that requires a random state. - Ensure preprocessing and cross-validation are applied correctly. - The SVM classifier should have `C=1` and `kernel=\'linear\'`. **Input:** No input required. The script should load the dataset programmatically. **Output:** - Mean and standard deviation of the cross-validated accuracy scores. - Mean scores of accuracy, precision, and recall from `cross_validate`. **Example:** ```python import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split, cross_val_score, cross_validate from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.svm import SVC # Load the iris dataset X, y = datasets.load_iris(return_X_y=True) # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Preprocess the data scaler = StandardScaler().fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # Create a pipeline for SVM with preprocessing clf = make_pipeline(StandardScaler(), SVC(kernel=\'linear\', C=1)) # Perform k-fold cross-validation cv_scores = cross_val_score(clf, X, y, cv=5) print(\\"Accuracy: %0.2f (+/- %0.2f)\\" % (cv_scores.mean(), cv_scores.std())) # Evaluate multiple metrics scoring = [\'accuracy\', \'precision_macro\', \'recall_macro\'] cv_results = cross_validate(clf, X, y, cv=5, scoring=scoring) for metric in scoring: print(f\\"{metric}: {cv_results[f\'test_{metric}\'].mean():.2f}\\") # Optional Challenge: Custom cross-validation iterator def custom_cv_2folds(X): n = X.shape[0] for i in range(2): indices = np.arange(n * i / 2, n * (i + 1) / 2, dtype=int) yield indices, indices custom_cv = custom_cv_2folds(X) custom_cv_results = cross_val_score(clf, X, y, cv=custom_cv, scoring=\'recall_macro\') print(\\"Custom CV Recall: %0.2f\\" % custom_cv_results.mean()) ``` This example will guide you through implementing and evaluating an SVM classifier using cross-validation techniques provided by scikit-learn.","solution":"import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split, cross_val_score, cross_validate from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.svm import SVC def cross_validate_svm(): # Load the iris dataset X, y = datasets.load_iris(return_X_y=True) # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create a pipeline for SVM with preprocessing clf = make_pipeline(StandardScaler(), SVC(kernel=\'linear\', C=1)) # Perform k-fold cross-validation cv_scores = cross_val_score(clf, X, y, cv=5) accuracy_mean = cv_scores.mean() accuracy_std = cv_scores.std() # Evaluate multiple metrics scoring = [\'accuracy\', \'precision_macro\', \'recall_macro\'] cv_results = cross_validate(clf, X, y, cv=5, scoring=scoring) metrics_means = {metric: cv_results[f\'test_{metric}\'].mean() for metric in scoring} return accuracy_mean, accuracy_std, metrics_means"},{"question":"# Custom Sequence Class Implementation Objective: Implement a custom immutable sequence type in Python that demonstrates a strong understanding of the Python data model, custom class behavior, and special methods. Instructions: 1. Define a class `ImmutableSequence` that mimics an immutable sequence type (like a tuple). 2. The class must support: - Initialization with any iterable. - Index access using square brackets (e.g., `seq[1]`). - Slicing using the slice notation `seq[start:stop:step]`. - Iteration. - Length calculation using the `len()` function. - Membership test using the `in` operator. - Proper representation using `__repr__`. 3. The class must handle the following scenarios: - Index out of bounds. - Invalid slice parameters. - Immutable property: any attempt to change the sequence after creation should raise an appropriate error. 4. Provide a small script demonstrating the usage of the class, including initialization, indexing, slicing, iteration, and immutability enforcement. Function Specifications: - **Class Name**: `ImmutableSequence` - **Methods to Implement**: - `__init__(self, iterable)` - `__getitem__(self, index)` - `__len__(self)` - `__iter__(self)` - `__contains__(self, item)` - `__repr__(self)` Example: ```python # Demonstration script seq = ImmutableSequence([1, 2, 3, 4, 5]) print(seq) # Expected Output: ImmutableSequence([1, 2, 3, 4, 5]) print(len(seq)) # Expected Output: 5 print(seq[2]) # Expected Output: 3 print(seq[:3]) # Expected Output: ImmutableSequence([1, 2, 3]) print(3 in seq) # Expected Output: True for element in seq: print(element) try: seq[1] = 10 # Expected to raise TypeError except TypeError as e: print(e) # Expected Output: \'ImmutableSequence\' object does not support item assignment ``` Constraints: - The implementation should adhere to typical Python error handling and conventions. - The solution must be efficient with respect to time complexity for indexing and slicing operations. Good luck!","solution":"class ImmutableSequence: def __init__(self, iterable): self._data = tuple(iterable) def __getitem__(self, index): # Handle slicing if isinstance(index, slice): return ImmutableSequence(self._data[index]) elif isinstance(index, int): # Handle negative indices if -len(self._data) <= index < len(self._data): return self._data[index] else: raise IndexError(\\"Index out of range\\") else: raise TypeError(\\"Invalid argument type\\") def __len__(self): return len(self._data) def __iter__(self): return iter(self._data) def __contains__(self, item): return item in self._data def __repr__(self): return f\\"ImmutableSequence({self._data})\\" def __setitem__(self, key, value): raise TypeError(\\"\'ImmutableSequence\' object does not support item assignment\\") def __delitem__(self, key): raise TypeError(\\"\'ImmutableSequence\' object does not support item deletion\\") # Prevent other mutating methods def append(self, item): raise TypeError(\\"\'ImmutableSequence\' object does not support append\\") def extend(self, iterable): raise TypeError(\\"\'ImmutableSequence\' object does not support extend\\") def insert(self, index, item): raise TypeError(\\"\'ImmutableSequence\' object does not support insert\\") def remove(self, item): raise TypeError(\\"\'ImmutableSequence\' object does not support remove\\") def pop(self, index=-1): raise TypeError(\\"\'ImmutableSequence\' object does not support pop\\") def clear(self): raise TypeError(\\"\'ImmutableSequence\' object does not support clear\\")"},{"question":"**Coding Assessment Question: Text Data Manipulation with Pandas** **Objective:** To assess your understanding of pandas string manipulation capabilities, you will be tasked with transforming and cleaning a DataFrame containing text data. **Problem Statement:** You are given a DataFrame `df` that contains survey responses from different users. The DataFrame has the following columns: - `full_name`: The full name of the respondent (may have leading/trailing spaces). - `response`: The respondent\'s answer (text data which may include multiple keywords in different formats, e.g., \\"yes\\", \\"no\\", \\"maybe\\", \\"YES\\", \\"No\\", etc.). - `age`: The age of the respondent (this column contains integers but due to data entry errors, it may include some text like \\"unknown\\" or \\"not specified\\"). Your tasks are: 1. **Clean the \'full_name\' column:** - Strip any leading and trailing spaces from the names. - Convert all names to title case (e.g., \\"JOHN DOE\\" should be converted to \\"John Doe\\"). 2. **Standardize the \'response\' column:** - Convert all responses to lowercase. - Replace \\"yes\\" variations (\'yes\', \'YES\', \'Yes\') with a single standard response \\"yes\\". - Replace \\"no\\" variations (\'no\', \'NO\', \'No\') with a single standard response \\"no\\". - For responses that do not match \\"yes\\" or \\"no\\", replace them with \\"maybe\\". 3. **Clean the \'age\' column:** - Convert all age values to numeric type. - Replace any non-numeric age values with `NaN`. 4. **Extract useful information:** - Create a new column `first_name` that contains only the first name of the respondent. - Create a new column `has_yes` that contains boolean values indicating whether the `response` column contains the keyword \\"yes\\". **Input:** A pandas DataFrame `df` with the following structure: ```python import pandas as pd data = { \'full_name\': [\\" JOHN DOE \\", \\"jane smith\\", \\"ALICE JOHNSON \\", \\"bob brown\\"], \'response\': [\\"YES\\", \\"no\\", \\"maybe\\", \\"YEs\\"], \'age\': [\\"30\\", \\"not specified\\", \\"45\\", \\"unknown\\"] } df = pd.DataFrame(data) ``` **Output:** Return a transformed DataFrame with cleaned and standardized columns. **Expected Output Format:** The output DataFrame should look like this: | full_name | response | age | first_name | has_yes | |--------------|----------|------|------------|---------| | John Doe | yes | 30.0 | John | True | | Jane Smith | no | NaN | Jane | False | | Alice Johnson| maybe | 45.0 | Alice | False | | Bob Brown | yes | NaN | Bob | True | **Constraints:** - Use pandas `StringDtype` for the `full_name` and `response` columns to ensure consistent string operations. - Handle missing values appropriately where necessary. **Function Signature:** ```python def clean_survey_data(df: pd.DataFrame) -> pd.DataFrame: # Your code here ``` **Good luck!**","solution":"import pandas as pd import numpy as np def clean_survey_data(df: pd.DataFrame) -> pd.DataFrame: # Clean the \'full_name\' column df[\'full_name\'] = df[\'full_name\'].str.strip().str.title() # Standardize the \'response\' column df[\'response\'] = df[\'response\'].str.lower() df[\'response\'] = df[\'response\'].apply(lambda x: \'yes\' if x == \'yes\' else (\'no\' if x == \'no\' else \'maybe\')) # Clean the \'age\' column df[\'age\'] = pd.to_numeric(df[\'age\'], errors=\'coerce\') # Extract first name df[\'first_name\'] = df[\'full_name\'].apply(lambda x: x.split()[0]) # Create has_yes column df[\'has_yes\'] = df[\'response\'] == \'yes\' return df"},{"question":"Signal Processing with PyTorch\'s torch.fft Module Objective To assess your understanding of the Fourier transform and its applications using PyTorch\'s `torch.fft` module. Problem Statement You are given a 1-dimensional array representing a signal in the time domain. Your task is to transform this signal to the frequency domain, apply a low-pass filter, and then transform it back to the time domain. Implement the following function: ```python import torch def low_pass_filter(signal: torch.Tensor, cutoff_freq: float) -> torch.Tensor: Apply a low-pass filter to the given signal using the Fourier transform. Parameters: - signal (torch.Tensor): A 1-dimensional tensor representing the time-domain signal. The length of the signal must be a power of 2. - cutoff_freq (float): The cutoff frequency for the low-pass filter. Returns: - torch.Tensor: The filtered signal in the time domain. # TODO: Implement this function ``` Input and Output - **Input:** - `signal`: A 1-dimensional tensor of length `N` where `N` is a power of 2. - `cutoff_freq`: A float specifying the cutoff frequency for the low-pass filter. - **Output:** - A 1-dimensional tensor of length `N` containing the filtered signal in the time domain. Constraints - The length of the input signal, `N`, will always be a power of 2 (e.g., 256, 512, 1024). - Use PyTorch\'s `torch.fft` module to perform the Fourier transforms. - Ensure that frequencies higher than `cutoff_freq` are attenuated to zero in the frequency domain. Example ```python import numpy as np import torch # Generate a sample signal: a mixture of two sine waves (frequency 5 Hz and 50 Hz) sampling_rate = 256 # samples per second t = torch.linspace(0, 1, 256) signal = torch.sin(2 * np.pi * 5 * t) + 0.5 * torch.sin(2 * np.pi * 50 * t) # Apply low-pass filter with a cutoff frequency of 10 Hz filtered_signal = low_pass_filter(signal, cutoff_freq=10) print(filtered_signal) ``` In the example above, the original signal is a combination of two sine waves with frequencies 5 Hz and 50 Hz. After applying the low-pass filter with a cutoff frequency of 10 Hz, the output should predominantly contain only the 5 Hz component. Performance Requirements - The function should efficiently handle signals with lengths up to 4096.","solution":"import torch def low_pass_filter(signal: torch.Tensor, cutoff_freq: float) -> torch.Tensor: Apply a low-pass filter to the given signal using the Fourier transform. Parameters: - signal (torch.Tensor): A 1-dimensional tensor representing the time-domain signal. The length of the signal must be a power of 2. - cutoff_freq (float): The cutoff frequency for the low-pass filter. Returns: - torch.Tensor: The filtered signal in the time domain. # Get the length of the signal N = signal.shape[0] # Perform the Fourier transform to get the frequency domain representation signal_fft = torch.fft.fft(signal) freqs = torch.fft.fftfreq(N) # Create a low-pass filter mask filter_mask = torch.abs(freqs) <= cutoff_freq # Apply the mask to the frequency domain signal filtered_signal_fft = signal_fft * filter_mask # Perform the inverse Fourier transform to get the time domain signal filtered_signal = torch.fft.ifft(filtered_signal_fft) # Return the real part of the inverse FFT result return filtered_signal.real"},{"question":"**Boxplot Analysis of Titanic Data** **Objective:** Demonstrate your knowledge of Seaborn by creating and customizing boxplots using the Titanic dataset. **Task:** 1. Load the \'titanic\' dataset from Seaborn. 2. Create a vertical boxplot showing the distribution of `age` across different `class` values, grouped by the variable `survived`. 3. Customize the following aspects of the boxplot: - Draw the boxes as line art (i.e., unfilled). - Modify the color of the line art to navy blue (`#137`). - Set the linewidth to 2. - Add a small gap of 0.1 between the boxes. - Use whiskers to cover the full range of the data. **Constraints:** - Use Seaborn\'s functionality for creating the boxplot and customization. - Apply the customizations directly to the Seaborn plot, without making direct calls to Matplotlib functions. **Expected Input and Output Formats:** *Input:* No input is required. The script should just load the dataset and generate the plot. *Output:* The output should be a customized Seaborn boxplot as specified. Here is an example of the expected code implementation (without the customizations detailed above): ```python import seaborn as sns # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create the boxplot boxplot = sns.boxplot(data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"survived\\", fill=False, linecolor=\\"#137\\", linewidth=2, gap=0.1, whis=(0, 100)) boxplot.set_title(\\"Boxplot of Age by Class and Survival\\") ``` **Grading Criteria:** - Correctly loads the Titanic dataset. - Accurately creates a vertical boxplot grouped by `class` and `survived`. - Successfully applies all customizations as specified in the task. - The plot produced should have clear visual distinctions as required. **Note:** Ensure to run the code and visually inspect the plot for correctness before submitting.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create the boxplot with customizations plt.figure(figsize=(10, 6)) boxplot = sns.boxplot( data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"survived\\", palette=[\\"#137\\", \\"#137\\"], linewidth=2, whis=(0, 100), fliersize=0, dodge=True ) # Customize the boxplot as line art for patch in boxplot.artists: r, g, b, a = patch.get_facecolor() patch.set_edgecolor((r, g, b, 1)) patch.set_facecolor(\'none\') boxplot.set_title(\\"Boxplot of Age by Class and Survival\\") plt.show()"},{"question":"You are tasked with demonstrating your comprehension of the seaborn `objects` module capabilities and integrating jitter in data visualizations. Below, you will implement a function that creates a plot using the penguins dataset and varies the jitter in both x and y dimensions based on function inputs. Function: `create_jittered_plot` **Objective:** Write a function `create_jittered_plot` that takes in four parameters: `x_column`, `y_column`, `x_jitter`, and `y_jitter`. This function will: 1. Utilize the seaborn `objects` module to create a scatter plot using the indicated columns from the `penguins` dataset. 2. Apply jitter in the x and y dimensions according to the provided `x_jitter` and `y_jitter` values. 3. Display the resulting plot. **Input:** - `x_column` (str): The name of the column to be plotted on the x-axis. - `y_column` (str): The name of the column to be plotted on the y-axis. - `x_jitter` (float): The amount of jitter to apply along the x-axis. - `y_jitter` (float): The amount of jitter to apply along the y-axis. **Output:** - This function should not return any value. It should display the plot directly. **Constraints:** - You must use the seaborn `objects` module. - Handle potential absence of columns gracefully by raising a ValueError. - Ensure that `x_jitter` and `y_jitter` are non-negative. If they are negative, raise a ValueError. Example usage: ```python create_jittered_plot(\\"body_mass_g\\", \\"flipper_length_mm\\", 200, 5) ``` This will visualize a scatter plot of the `body_mass_g` against `flipper_length_mm` with the specified jitter amounts in the respective dimensions. **Performance Requirements:** - The plot generation and display should be efficiently handled by seaborn\'s internal mechanisms, focusing more on correctness and usage than on computational performance.","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def create_jittered_plot(x_column, y_column, x_jitter, y_jitter): Creates a scatter plot with jitter using the seaborn objects module. Parameters: x_column (str): Column name for the x-axis. y_column (str): Column name for the y-axis. x_jitter (float): Amount of jitter to apply on the x-axis. y_jitter (float): Amount of jitter to apply on the y-axis. Raises: ValueError: If the columns are not found in the dataframe or if jitter values are negative. if x_jitter < 0 or y_jitter < 0: raise ValueError(\\"Jitter values must be non-negative.\\") # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Check if the specified columns exist in the dataset if x_column not in penguins.columns or y_column not in penguins.columns: raise ValueError(f\\"Columns {x_column} and/or {y_column} not found in the dataset.\\") # Create the scatter plot with jitter p = so.Plot(penguins, x=x_column, y=y_column).add(so.Dot(), so.Jitter(x=x_jitter, y=y_jitter)) p.show()"},{"question":"**Title: Managing Precision in Floating-Point Arithmetic** **Objective:** Demonstrate your understanding of floating-point arithmetic precision, its limitations, and methods to manage precision in Python. **Problem Statement:** You are required to implement a function `precise_sum(numbers: List[float], precision: int) -> Tuple[float, str, float]` that takes in a list of floating-point numbers and an integer representing the precision. Your task includes: 1. Calculating the sum of the list of floating-point numbers using regular summation. 2. Calculating the sum using `math.fsum()` to handle precision better. 3. Comparing the two sums by rounding both to the specified precision and returning them, along with a boolean indicating if they are equal when rounded to that precision. **Function Signature:** ```python from typing import List, Tuple import math def precise_sum(numbers: List[float], precision: int) -> Tuple[float, str, float]: pass ``` **Input:** - `numbers`: A list of floating-point numbers (e.g., [0.1, 0.2, 0.3]) - `precision`: An integer specifying the number of decimal places to round the sums to (e.g., 2) **Output:** - A tuple containing: - The regular sum of the numbers rounded to the specified precision. - The sum obtained using `math.fsum()` rounded to the specified precision. - A boolean indicating if the sums are equal when rounded to the specified precision. **Example:** ```python >>> precise_sum([0.1, 0.2, 0.3], 10) (0.6000000000, 0.6000000000, True) >>> precise_sum([0.1, 0.2, 0.3, 0.4], 10) (1.0000000000, 1.0000000000, True) >>> precise_sum([0.1, 0.2, 0.3], 17) (0.6000000000000001, 0.6, False) ``` **Constraints:** - The list `numbers` will contain between 1 and 1000 elements inclusive. - Each floating-point number in `numbers` will be between `-1e6` and `1e6`. - The `precision` will be an integer between 0 and 20 inclusive. **Hint:** You may find the `round()` function and string formatting helpful in managing the precision of the summed values. **Additional Information:** - Ensure to handle edge cases where the list may contain very small or very large floating-point numbers. - Utilize the `math.fsum()` function to showcase your understanding of its precision handling capability.","solution":"from typing import List, Tuple import math def precise_sum(numbers: List[float], precision: int) -> Tuple[float, float, bool]: Calculates the sum of floating-point numbers in two ways: 1. Using the regular summation method. 2. Using math.fsum() for better precision handling. Compares if they are equal when rounded to the specified precision. :param numbers: List of floating-point numbers to sum up. :param precision: The number of decimal places to round the sums to. :return: A tuple containing: - The regular sum of the numbers rounded to the specified precision. - The sum obtained using math.fsum() rounded to the specified precision. - A boolean indicating if the sums are equal when rounded to the specified precision. regular_sum = sum(numbers) precise_sum = math.fsum(numbers) rounded_regular_sum = round(regular_sum, precision) rounded_precise_sum = round(precise_sum, precision) sums_equal = rounded_regular_sum == rounded_precise_sum return rounded_regular_sum, rounded_precise_sum, sums_equal"},{"question":"**Question:** You are required to parse a file containing data in EA IFF 85 chunk format and output certain information for each chunk. The file may contain multiple chunks, and each chunk will follow the structure provided in the documentation. **Write a Python function `parse_chunks(file_path)` that:** 1. Reads the file specified by `file_path` and processes each chunk. 2. For each chunk, prints the following details: - Chunk ID. - Chunk size. - First 10 bytes of the chunk data (as a hexadecimal string). **Function Signature:** ```python def parse_chunks(file_path: str) -> None: pass ``` **Input:** - `file_path` (str): The path to the EA IFF 85 chunked file. **Output:** - The function should print the details of each chunk in the following format for each chunk: ``` Chunk ID: <Chunk ID> Chunk Size: <Chunk Size> First 10 Bytes: <First 10 Bytes as Hex> ``` **Constraints:** - You may assume that the file exists and is readable. - The file may contain multiple chunks. - If a chunk\'s data is less than 10 bytes, print all available bytes. - Handle big-endian and alignment properly as described in the documentation. **Example:** Given a file with the following chunks: - Chunk 1: - ID: `FORM` - Size: 12 bytes - Data: 112233445566778899AA (10 bytes of data) - Chunk 2: - ID: `DATA` - Size: 5 bytes - Data: 01020304 (4 bytes of data) Your function should produce the following output: ``` Chunk ID: FORM Chunk Size: 12 First 10 Bytes: 112233445566778899aa Chunk ID: DATA Chunk Size: 5 First 10 Bytes: 01020304 ``` **Notes:** - Make sure to handle the end-of-file properly, i.e., stop reading when there are no more chunks. - Utilize the methods provided by the `chunk` module to accomplish the task.","solution":"def parse_chunks(file_path: str) -> None: with open(file_path, \'rb\') as file: while True: # Read the chunk header chunk_id = file.read(4) if len(chunk_id) < 4: break chunk_size_bytes = file.read(4) if len(chunk_size_bytes) < 4: break # Parse chunk size (big-endian) chunk_size = int.from_bytes(chunk_size_bytes, byteorder=\'big\') # Read the chunk data chunk_data = file.read(chunk_size) # Convert the first 10 bytes of the chunk data to a hexadecimal string first_10_bytes = chunk_data[:10].hex() # Print the chunk details print(f\\"Chunk ID: {chunk_id.decode(\'ascii\')}\\") print(f\\"Chunk Size: {chunk_size}\\") print(f\\"First 10 Bytes: {first_10_bytes}n\\") # Handle padding byte if chunk size is odd if chunk_size % 2 == 1: file.read(1)"},{"question":"# HTML Entity Converter **Objective:** Write a Python function that converts a string containing HTML character references to their equivalent Unicode characters. The function should utilize the `html5` dictionary from the `html.entities` module to perform these conversions, handling both the cases where semicolons are present and absent. **Function Signature:** ```python def convert_html_entities(string: str) -> str: pass ``` **Input:** - `string` (str): A string that may contain HTML5 named character references (e.g., `&lt;`, `&gt;`, `&amp`). **Output:** - Returns a string where all the HTML character references are replaced with their corresponding Unicode characters. **Constraints:** - The input string will have a maximum length of 10^6 characters. - You should efficiently handle the conversion to respect the performance requirement due to the potentially large input size. **Examples:** ```python assert convert_html_entities(\\"Hello &lt;world&gt;\\") == \\"Hello <world>\\" assert convert_html_entities(\\"1 &lt; 2 &amp 3 &gt; 2\\") == \\"1 < 2 & 3 > 2\\" assert convert_html_entities(\\"Use &quot;double quotes&quot; for a string\\") == \'Use \\"double quotes\\" for a string\' ``` **Note:** - You should use the `html5` dictionary from the `html.entities` module for the conversion. - Consider edge cases where the input string may not contain any HTML character references or may contain invalid/unrecognized references. **Implementation Tip:** - You may find it useful to utilize regular expressions to find and replace the HTML character references in the input string.","solution":"import re from html.entities import html5 def convert_html_entities(string: str) -> str: Converts HTML character references to their corresponding Unicode characters. # Regex to match HTML entities; uses non-greedy matching for the name part entity_regex = re.compile(r\'&([a-zA-Z0-9]+);?\') def replace_entity(match): entity = match.group(1) if entity in html5: return html5[entity] else: # Return the original string if the entity is not found if match.group(0).endswith(\';\'): return f\\"&{entity};\\" else: return f\\"&{entity}\\" return entity_regex.sub(replace_entity, string)"},{"question":"You are tasked with developing a Python function to simulate a file operation and handle specific system errors gracefully using the `errno` module. You will implement the function `file_operation(filename)` that takes a single parameter `filename` (a string representing the file name) and performs the following: 1. Attempts to read the content of the file specified by `filename`. 2. If the file does not exist, catch the exception and return a specific error message using `errno` and `os.strerror()`. 3. If the file cannot be accessed due to permission issues, catch the exception and return a corresponding error message using `errno` and `os.strerror()`. 4. If any other I/O error occurs, catch the exception and return a general error message using `errno` and `os.strerror()`. 5. If no errors occur, return the content of the file. # Function Signature ```python def file_operation(filename: str) -> str: pass ``` # Input - `filename` (str): The name of the file to be accessed. # Output - (str): The content of the file if successfully read. - Specific error messages if any exceptions occur, formatted as \\"Error: <description>\\", where `<description>` is obtained using `os.strerror()` and the corresponding `errno` code. # Example ```python # Considering that the file \\"example.txt\\" does not exist print(file_operation(\\"example.txt\\")) # Output: \\"Error: No such file or directory\\" # Considering a permission error on the file \\"restricted.txt\\" print(file_operation(\\"restricted.txt\\")) # Output: \\"Error: Permission denied\\" ``` # Constraints - You are not allowed to change the input signature of the function. - Use the `errno` module and handle exceptions as described. # Notes - Ensure that your solution handles edge cases, such as filenames that are empty strings or contain only whitespace. - Consider platform-specific differences; test your code on the platform you\'re developing on.","solution":"import os import errno def file_operation(filename: str) -> str: try: with open(filename, \'r\') as file: return file.read() except FileNotFoundError as e: return f\\"Error: {os.strerror(errno.ENOENT)}\\" except PermissionError as e: return f\\"Error: {os.strerror(errno.EACCES)}\\" except IOError as e: return f\\"Error: {os.strerror(e.errno)}\\""},{"question":"# Python310 Coding Assessment Question Objective: To assess students\' understanding of the `pwd` module and their ability to use its functions to interact with the Unix user account and password database. Problem Statement: You are required to write a function in Python that takes a list of numeric user IDs, retrieves their corresponding user information using the `pwd` module, and then returns a summary report containing specific details about these users. Function Signature: ```python def user_summary_report(user_ids: list) -> dict: pass ``` Input: - `user_ids`: A list of integers representing Unix user IDs. Example: `[1001, 1002, 1003]`. The list can contain up to 100 user IDs. Output: - The function should return a dictionary where the keys are the numeric user IDs (from the input list) and the values are dictionaries containing the following user information: - `username`: The login name of the user (corresponding to `pw_name`). - `home_directory`: The home directory of the user (corresponding to `pw_dir`). - `shell`: The shell interpreter of the user (corresponding to `pw_shell`). Example: ```python user_ids = [1000, 1001] output = user_summary_report(user_ids) # Example Output { 1000: { \\"username\\": \\"john_doe\\", \\"home_directory\\": \\"/home/john_doe\\", \\"shell\\": \\"/bin/bash\\" }, 1001: { \\"username\\": \\"jane_smith\\", \\"home_directory\\": \\"/home/jane_smith\\", \\"shell\\": \\"/bin/zsh\\" } } ``` Constraints and Limitations: - If any of the provided user IDs do not exist in the password database, the function should ignore those IDs and not include them in the output. - You should use the `pwd.getpwuid(uid)` function to retrieve user information for each user ID in the input list. - Consider performance and efficiency, as the function may need to handle up to 100 user IDs. Performance Requirements: - The function should efficiently handle up to 100 user IDs, retrieving user information using password database lookups without significant lag. Notes: - The system must have the `pwd` module available, which is specific to Unix-like systems.","solution":"import pwd def user_summary_report(user_ids: list) -> dict: Returns a summary report containing specific details about users for the given list of user IDs. Args: user_ids (list): A list of integers representing Unix user IDs. Returns: dict: A dictionary where the keys are user IDs and the values are dictionaries with user details. user_report = {} for uid in user_ids: try: user_info = pwd.getpwuid(uid) user_report[uid] = { \\"username\\": user_info.pw_name, \\"home_directory\\": user_info.pw_dir, \\"shell\\": user_info.pw_shell } except KeyError: # If user ID is not found, ignore it continue return user_report"},{"question":"**Coding Assessment Question** # Objective The goal of this question is to assess your understanding of the PyTorch Distributed Elastic library and your ability to apply its components to create fault-tolerant and scalable applications. You will be required to implement a functionality in Python using PyTorch Elastic\'s APIs to launch a distributed training process. # Problem Statement You are tasked with creating a distributed training script using PyTorch Distributed Elastic that should be able to handle worker failures and dynamically scale the number of workers. Specifically, you need to implement a Python class `ElasticTrainer` that uses the functionalities provided by the PyTorch Elastic library to: 1. Initialize a distributed training environment. 2. Launch a specified number of training workers. 3. Handle worker failures by restarting them. 4. Dynamically scale the number of workers based on a given condition. # Requirements 1. **Class Definition**: ```python class ElasticTrainer: def __init__(self, num_workers: int): Initializes the ElasticTrainer instance. Args: num_workers (int): Initial number of training workers pass def start_training(self): Starts the distributed training with fault-tolerance and elasticity. pass def handle_failure(self): Handle worker failure by restarting the failed worker. pass def scale_workers(self, new_num_workers: int): Dynamically scales the number of workers. Args: new_num_workers (int): The new number of training workers pass ``` 2. **Implementation Details**: - Use PyTorch Elastic APIs to initialize and manage the distributed training environment. - The `start_training` method should launch the initialized number of workers and ensure the training is fault-tolerant. - The `handle_failure` method should be capable of detecting and restarting failed workers. - The `scale_workers` method should dynamically adjust the number of training workers based on changes in `new_num_workers`. 3. **Constraints**: - Assume the necessary PyTorch Elastic components are available (like `torch.distributed.elastic`). - The solution should be capable of running in a distributed setting, potentially on multiple nodes. - Ensure the implementation is efficient in handling worker failures and scaling operations. # Input and Output - **Input**: The number of initial training workers and subsequent commands to handle failures and scale workers (the interactions can be simulated in a main function). - **Output**: The class should manage the distributed training process seamlessly, handling worker failures and scaling efficiently. # Example: ```python def main(): # Initialize with 4 workers trainer = ElasticTrainer(num_workers=4) trainer.start_training() # Simulate a worker failure trainer.handle_failure() # Scale to 6 workers trainer.scale_workers(new_num_workers=6) if __name__ == \\"__main__\\": main() ``` In this example: - The `ElasticTrainer` starts with 4 workers and initiates the training. - It then handles a simulated worker failure by restarting the failed worker. - Finally, it scales up the number of workers to 6 for ongoing training. # Notes: - You may refer to the official PyTorch Distributed Elastic documentation to better understand the API usage. - Ensure your implementation rigorously follows the fault-tolerant and elastic principles of distributed training. Good luck!","solution":"import torch.distributed.elastic as de from torch.distributed.elastic.multiprocessing.errors import record class ElasticTrainer: def __init__(self, num_workers: int): Initializes the ElasticTrainer instance. Args: num_workers (int): Initial number of training workers self.num_workers = num_workers self.remaining_restarts = 3 # Allowable restarts self.worker_group = None @record def start_training(self): Starts the distributed training with fault-tolerance and elasticity. try: self.worker_group = de.launched_workers( self._train_fn, args=(), nprocs=self.num_workers, ) self.worker_group.join() except Exception as e: print(f\\"Training Error: {e}\\") if self.remaining_restarts > 0: print(\\"Restarting failed worker\\") self.handle_failure() else: print(\\"Max restarts reached. Aborting training.\\") raise e def handle_failure(self): Handle worker failure by restarting the failed worker. self.remaining_restarts -= 1 self.worker_group.restart() def scale_workers(self, new_num_workers: int): Dynamically scales the number of workers. Args: new_num_workers (int): The new number of training workers self.num_workers = new_num_workers if self.worker_group is not None: self.worker_group.resize(new_num_workers) def _train_fn(self): The actual training function to be executed by each worker. This should include the complete training loop. # Example training loop for epoch in range(10): # Suppose we have 10 epochs print(f\\"Epoch: {epoch}\\") # Simulate training process pass"},{"question":"Python Slice Object Manipulation # Objective Create a function that reproduces the behavior of Python slice objects but does so using lower-level manipulations, simulating the C-API functionalities described. # Problem Statement Write a function `extract_slice(sequence, slice_obj)` that takes two arguments: 1. `sequence`: a list of integers. 2. `slice_obj`: a tuple representing a slice, where the tuple contains three elements `(start, stop, step)`, any of which may be `None`. The function should return a new list consisting of elements of `sequence` sliced according to the `slice_obj` parameters. If a parameter in `slice_obj` is `None`, it should default to the corresponding value `None` would represent in a normal Python slice. # Input and Output Format - **Input:** - `sequence`: a list of integers. Example: `[1, 2, 3, 4, 5]` - `slice_obj`: a tuple of three elements `(start, stop, step)`, any of which may be `None`. Example: `(1, 4, 2)` - **Output:** - A list of integers sliced according to `slice_obj`. Example: `[2, 4]` # Constraints - You may assume that `sequence` will always be a list containing at least one integer. - If `slice_obj` contains `None` for a parameter, it should default to typical slice behavior, i.e., start at 0, stop at the length of the sequence, and step by 1. # Guidelines 1. You are not allowed to use Python’s built-in slicing (`[:]`) directly. 2. You are encouraged to simulate the slicing using loops and conditionals. 3. Perform necessary error handling for out of bounds indices by clipping them. # Example ```python def extract_slice(sequence, slice_obj): # Implementation here # Example usage: sequence = [1, 2, 3, 4, 5] slice_obj = (1, 4, 2) print(extract_slice(sequence, slice_obj)) # Output: [2, 4] sequence = [10, 20, 30, 40, 50, 60] slice_obj = (None, None, None) print(extract_slice(sequence, slice_obj)) # Output: [10, 20, 30, 40, 50, 60] sequence = [100, 200, 300, 400] slice_obj = (2, None, 1) print(extract_slice(sequence, slice_obj)) # Output: [300, 400] ```","solution":"def extract_slice(sequence, slice_obj): Simulates the behavior of a Python slice object given a sequence and a tuple with start, stop, and step. start, stop, step = slice_obj if step is None: step = 1 if step == 0: raise ValueError(\\"slice step cannot be zero\\") if start is None: start = 0 if step > 0 else len(sequence) - 1 elif start < 0: start += len(sequence) start = max(0, min(len(sequence), start)) if step > 0 else min(len(sequence) - 1, max(-1, start)) if stop is None: stop = len(sequence) if step > 0 else -1 elif stop < 0: stop += len(sequence) stop = max(-1, min(len(sequence), stop)) if step < 0 else min(len(sequence), max(0, stop)) result = [] i = start if step > 0: while i < stop: result.append(sequence[i]) i += step else: while i > stop: result.append(sequence[i]) i += step return result"},{"question":"Copy-on-Write Mechanism in Pandas Objective: To assess your understanding of pandas\' Copy-on-Write (CoW) mechanism, indexing operations, handling views vs copies, and avoiding unintended side effects when modifying DataFrame or Series objects. Task: Implement the function `modify_dataframe` which takes a DataFrame, a column name, and a dictionary of operations to perform on the DataFrame without violating the CoW rules. The operations dictionary will have the following structure: ```python { \\"operation_1\\": {\\"action\\": \\"update\\", \\"index\\": [row_indices], \\"value\\": new_value}, \\"operation_2\\": {\\"action\\": \\"rename\\", \\"columns\\": {\\"old_name\\": \\"new_name\\"}}, \\"operation_3\\": {\\"action\\": \\"drop\\", \\"columns\\": [list_of_columns]} } ``` Each operation specifies: - `update`: Update specific row indices of a specified column with a new value. - `rename`: Rename columns based on provided mappings. - `drop`: Drop specified columns from the DataFrame. Input Format: - `df`: pandas DataFrame - `operations`: dictionary as described above Output Format: - Returns a modified pandas DataFrame that adheres to CoW principles. Constraints: - You must ensure the DataFrame is only modified according to CoW rules. - Avoid chained assignments. - Handle operations in the order they appear in the dictionary. Example Usage: ```python import pandas as pd def modify_dataframe(df, operations): for key, operation in operations.items(): if operation[\\"action\\"] == \\"update\\": for index in operation[\\"index\\"]: df.loc[index, operation[\\"column\\"]] = operation[\\"value\\"] elif operation[\\"action\\"] == \\"rename\\": df.rename(columns=operation[\\"columns\\"], inplace=True) elif operation[\\"action\\"] == \\"drop\\": df.drop(columns=operation[\\"columns\\"], inplace=True) return df # Sample DataFrame data = { \\"A\\": [1, 2, 3, 4], \\"B\\": [5, 6, 7, 8], \\"C\\": [9, 10, 11, 12] } df = pd.DataFrame(data) operations = { \\"op1\\": {\\"action\\": \\"update\\", \\"index\\": [1, 3], \\"column\\": \\"A\\", \\"value\\": 100}, \\"op2\\": {\\"action\\": \\"rename\\", \\"columns\\": {\\"B\\": \\"Beta\\"}}, \\"op3\\": {\\"action\\": \\"drop\\", \\"columns\\": [\\"C\\"]} } modified_df = modify_dataframe(df, operations) print(modified_df) ``` Expected Output: ``` A Beta 0 1 5 1 100 6 2 3 7 3 100 8 ``` This example demonstrates modifying the DataFrame according to CoW principles, ensuring that each operation is carefully handled without unintended side-effects or chained assignments.","solution":"import pandas as pd def modify_dataframe(df, operations): # Make a copy of the DataFrame to ensure we\'re working with a fresh copy df_copy = df.copy() # Iterate through the operations dictionary for key, operation in operations.items(): if operation[\\"action\\"] == \\"update\\": for index in operation[\\"index\\"]: df_copy.loc[index, operation[\\"column\\"]] = operation[\\"value\\"] elif operation[\\"action\\"] == \\"rename\\": df_copy.rename(columns=operation[\\"columns\\"], inplace=True) elif operation[\\"action\\"] == \\"drop\\": df_copy.drop(columns=operation[\\"columns\\"], inplace=True) # Return the modified copy of the DataFrame return df_copy"},{"question":"<|Analysis Begin|> The provided documentation is an excerpt from the `email.utils` module in Python. It contains various utility functions for dealing with email functionalities such as parsing addresses, generating message IDs, formatting and parsing dates conforming to RFC standards, and encoding/decoding according to specific RFCs. Key utilities from the provided documentation that could form the basis of a coding assessment question include: 1. `email.utils.localtime(dt=None)` - Returns local time as an aware datetime object. 2. `email.utils.make_msgid(idstring=None, domain=None)` - Returns a string suitable for an RFC 2822-compliant Message-ID header. 3. `email.utils.parseaddr(address, *, strict=True)` - Parses an address into its constituent real name and email address parts. 4. `email.utils.formataddr(pair, charset=\'utf-8\')` - Takes a tuple of (realname, email_address) and returns the string value suitable for a To or Cc header. 5. `email.utils.parsedate(date)` - Attempts to parse a date according to RFC 2822 and returns a 9-tuple or None if parsing fails. 6. `email.utils.parsedate_tz(date)` - Similar to parsedate but returns a 10-tuple including the timezone offset. Given the nature of these utilities, a good coding question would be to assess comprehension by combining some of these functions in a practical task. <|Analysis End|> <|Question Begin|> **Problem Statement:** You are tasked with writing a function that processes a list of emails and extracts relevant information for each participant. Specifically, you need to: 1. Parse the email addresses to extract real names and email addresses. 2. Generate a unique Message-ID for each email. 3. Convert the given email date strings to aware datetime objects in the local timezone. 4. Construct formatted To or Cc headers for the emails. Your task is to implement the function `process_email_details(emails)`, which takes as input a list of dictionaries representing the emails. Each dictionary contains the following keys: - `\\"address\\"`: The email address string to be parsed. - `\\"date\\"`: The email date string in RFC 2822 format. The function should return a new list of dictionaries, each containing: - `\\"realname\\"`: The extracted real name from the address. - `\\"email_address\\"`: The extracted email address. - `\\"msg_id\\"`: A unique Message-ID. - `\\"local_datetime\\"`: The local time as a datetime object, aware of the local timezone. - `\\"formatted_header\\"`: The formatted To or Cc header string. # Function Signature ```python from typing import List, Dict import email.utils def process_email_details(emails: List[Dict[str, str]]) -> List[Dict[str, str]]: pass ``` # Input - `emails` (List[Dict[str, str]]): A list of dictionaries, each containing the `\\"address\\"` and `\\"date\\"`. # Output - (List[Dict[str, str]]): A list of dictionaries with the parsed and formatted details as described. # Example ```python emails = [ {\\"address\\": \\"John Doe <john.doe@example.com>\\", \\"date\\": \\"Mon, 20 Nov 1995 19:12:08 -0500\\"}, {\\"address\\": \\"Jane Smith <jane.smith@example.com>\\", \\"date\\": \\"Tue, 21 Nov 1995 20:15:10 -0500\\"} ] result = process_email_details(emails) # Expected output should be similar to (msg_id and local_datetime values will differ based on the actual local timezone and unique generation): # [ # { # \\"realname\\": \\"John Doe\\", # \\"email_address\\": \\"john.doe@example.com\\", # \\"msg_id\\": \\"<unique_message_id_1>\\", # \\"local_datetime\\": datetime(1995, 11, 20, 19, 12, 8, tzinfo=<local_tz>), # \\"formatted_header\\": \\"John Doe <john.doe@example.com>\\" # }, # { # \\"realname\\": \\"Jane Smith\\", # \\"email_address\\": \\"jane.smith@example.com\\", # \\"msg_id\\": \\"<unique_message_id_2>\\", # \\"local_datetime\\": datetime(1995, 11, 21, 20, 15, 10, tzinfo=<local_tz>), # \\"formatted_header\\": \\"Jane Smith <jane.smith@example.com>\\" # } # ] ``` # Constraints - Assume all email addresses and dates follow the correct format as specified. - The local timezone should be derived from the system\'s timezone. Implement the function `process_email_details` to exhibit your knowledge and skills.","solution":"from typing import List, Dict import email.utils from datetime import datetime from email.utils import parseaddr, formataddr, make_msgid, parsedate_to_datetime, localtime def process_email_details(emails: List[Dict[str, str]]) -> List[Dict[str, str]]: processed_emails = [] for email in emails: # Parse the address to get real name and email address realname, email_address = parseaddr(email[\'address\']) # Generate a unique Message-ID msg_id = make_msgid() # Convert the given email date string to an aware datetime object in the local timezone email_datetime = parsedate_to_datetime(email[\'date\']) local_datetime = localtime(email_datetime) # Construct formatted To or Cc header formatted_header = formataddr((realname, email_address)) # Append the processed information to the result list processed_emails.append({ \\"realname\\": realname, \\"email_address\\": email_address, \\"msg_id\\": msg_id, \\"local_datetime\\": local_datetime, \\"formatted_header\\": formatted_header }) return processed_emails"},{"question":"**Question: CSV Data Normalization and Filtering** # Problem Statement You are required to write a Python function `normalize_and_filter_csv(input_file: str, output_file: str, exclude_fields: list)` that reads data from a CSV file, normalizes the data based on the detected dialect, filters out specific fields, and writes the resulting data to a new CSV file. # Requirements 1. **Functional Requirements**: - The function should automatically detect the CSV dialect of the `input_file` by analyzing the first 1024 bytes. - The function should read the data from `input_file`, normalize it to use a consistent dialect (either `excel` or `unix`, chosen by you), and filter out the fields specified in `exclude_fields`. - The function should write the normalized and filtered data to `output_file` in the specified dialect. 2. **Input and Output Formats**: - `input_file`: A string representing the path to the input CSV file. - `output_file`: A string representing the path to the output CSV file. - `exclude_fields`: A list of field names (strings) to be excluded from the output CSV file. 3. **Constraints**: - The input CSV file can have an arbitrary format with different delimiters, quoting characters, etc. - The list `exclude_fields` might contain field names that do not exist in the input CSV file; in such cases, they should just be ignored. - Ensure proper exception handling for file reading/writing operations and CSV processing errors. # Implementation 1. **Detect Dialect**: Use the `csv.Sniffer` class to detect the dialect of the input CSV file. 2. **Read and Normalize Data**: Use the detected dialect to read the CSV file using `csv.DictReader`. Normalize the data to a consistent dialect (either `excel` or `unix`) and exclude the specified fields. 3. **Write Data**: Write the normalized and filtered data to the output CSV file using `csv.DictWriter`. # Example ```python def normalize_and_filter_csv(input_file: str, output_file: str, exclude_fields: list) -> None: import csv # Detect CSV dialect with open(input_file, newline=\'\') as csvfile: sample = csvfile.read(1024) dialect = csv.Sniffer().sniff(sample) csvfile.seek(0) reader = csv.DictReader(csvfile, dialect=dialect) fieldnames = [field for field in reader.fieldnames if field not in exclude_fields] with open(output_file, \'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=fieldnames, dialect=\'excel\') writer.writeheader() for row in reader: filtered_row = {key: value for key, value in row.items() if key not in exclude_fields} writer.writerow(filtered_row) # Function call example normalize_and_filter_csv(\'input.csv\', \'output.csv\', [\'field_to_exclude1\', \'field_to_exclude2\']) ``` # Note: - Ensure that you use appropriate exception handling for operations that could fail, such as file access or CSV parsing. - Include comments and documentation within your code to explain your logic and any assumptions made.","solution":"import csv def normalize_and_filter_csv(input_file: str, output_file: str, exclude_fields: list) -> None: Reads data from a CSV file, normalizes the data based on the detected dialect, filters out specific fields, and writes the resulting data to a new CSV file. Parameters: input_file (str): Path to the input CSV file. output_file (str): Path to the output CSV file. exclude_fields (list): A list of field names to be excluded from the output CSV file. try: # Detect CSV dialect with open(input_file, newline=\'\') as csvfile: sample = csvfile.read(1024) dialect = csv.Sniffer().sniff(sample) csvfile.seek(0) reader = csv.DictReader(csvfile, dialect=dialect) # Prepare fieldnames after excluding specified fields fieldnames = [field for field in reader.fieldnames if field not in exclude_fields] # Write the normalized and filtered data to the output file with open(output_file, \'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=fieldnames, dialect=\'excel\') writer.writeheader() for row in reader: filtered_row = {key: value for key, value in row.items() if key not in exclude_fields} writer.writerow(filtered_row) except Exception as e: print(f\\"An error occurred: {e}\\") # Example function call # normalize_and_filter_csv(\'input.csv\', \'output.csv\', [\'exclude_field1\', \'exclude_field2\'])"},{"question":"# Syslog Logging Implementation **Objective:** You are required to implement a function that sets up and sends log messages using the `syslog` module in Python. Your function will configure the syslog settings, send messages at various priority levels, and then close the logger. **Function Signature:** ```python def setup_and_log(syslog_id: str, messages: list): Set up syslog and send log messages with specified priorities. Parameters: syslog_id (str): The identifier for the syslog messages. messages (list): A list of tuples, each containing a priority (str) and a message (str). Returns: None ``` **Input:** 1. `syslog_id` (str): A string identifier that will be prepended to every syslog message. 2. `messages` (list): A list of tuples where each tuple contains: - `priority` (str): A priority level from the `syslog` module constants (e.g., `\\"LOG_INFO\\"`, `\\"LOG_ERR\\"`). - `message` (str): The log message to be sent. **Output:** - The function does not return anything. - Logs should be sent to the system logger as specified. **Constraints:** 1. The function should call `openlog` to set the logging options using the `syslog_id` and include the process ID in the logged messages. 2. For each message in the input list, the function should log the message with the specified priority. 3. After logging all messages, the function should call `closelog` to reset the syslog settings. **Example Usage:** ```python messages = [ (\\"LOG_INFO\\", \\"The service has started.\\"), (\\"LOG_WARNING\\", \\"Memory usage is high.\\"), (\\"LOG_ERR\\", \\"The service encountered an error.\\") ] setup_and_log(\\"my_syslog\\", messages) ``` In this example, three messages with varying priorities are sent to the system logger with the identifier \\"my_syslog\\". **Notes:** - Refer to the syslog documentation to understand how to use the constants and ensure that logging options and priorities are correctly set. - Ensure that messages are logged with trailing newlines if necessary. The `syslog` function should handle this automatically. **Hints:** - Import the `syslog` module and refer to its documentation for priority levels (`LOG_EMERG`, `LOG_ALERT`, etc.) and log options (`LOG_PID`, `LOG_CONS`, etc.). - Use the `getattr` function to dynamically access constants from the `syslog` module based on string inputs for priorities.","solution":"import syslog def setup_and_log(syslog_id: str, messages: list): Set up syslog and send log messages with specified priorities. Parameters: syslog_id (str): The identifier for the syslog messages. messages (list): A list of tuples, each containing a priority (str) and a message (str). Returns: None # Open a connection to the syslog syslog.openlog(ident=syslog_id, logoption=syslog.LOG_PID) for priority_str, message in messages: # Get the actual syslog priority constant from the string priority = getattr(syslog, priority_str) # Send the message to the syslog with the specified priority syslog.syslog(priority, message) # Close the connection to the syslog syslog.closelog()"},{"question":"# Seaborn Context Management **Objective:** Implement a function that generates and saves a series of plots with varying Seaborn context settings to visualize the differences in context, font scale, and custom parameter overrides. **Function Signature:** ```python def generate_seaborn_plots(data, filename_prefix): Generates and saves Seaborn plots with different context settings. Parameters: data (dict): A dictionary with \'x\' and \'y\' keys containing lists of numerical data points. filename_prefix (str): Prefix for the filenames where the plots will be saved. Returns: None ``` **Input:** - `data`: A dictionary containing the data points for the x and y axes. Example: ```python data = { \'x\': [0, 1, 2, 3, 4], \'y\': [2, 3, 5, 7, 11] } ``` - `filename_prefix`: A string prefix for the filenames where the plots will be saved. Each plot should be saved with a unique suffix indicating the context applied. **Output:** - The function should save three plots as image files with the following context settings: 1. Default \\"notebook\\" context. 2. \\"notebook\\" context with a font scale of 1.5. 3. \\"notebook\\" context with a font scale of 1.5 and line width of 2. **Constraints:** - Use the `lineplot` function from Seaborn to generate the plots. - Ensure that each plot is saved with a different filename indicating the context settings used. **Example:** For the given data, if the `filename_prefix` is \\"plot_\\", the function should save files named: - `plot_notebook.png` - `plot_notebook_fontscale_1.5.png` - `plot_notebook_fontscale_1.5_linewidth_2.png` **Sample Implementation:** ```python import seaborn as sns import matplotlib.pyplot as plt def generate_seaborn_plots(data, filename_prefix): contexts = [ (\\"notebook\\", 1.0, None, \\"notebook\\"), (\\"notebook\\", 1.5, None, \\"notebook_fontscale_1.5\\"), (\\"notebook\\", 1.5, {\\"lines.linewidth\\": 2}, \\"notebook_fontscale_1.5_linewidth_2\\") ] for context, font_scale, rc, suffix in contexts: sns.set_context(context, font_scale=font_scale, rc=rc) plt.figure() sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"]) plt.savefig(f\\"{filename_prefix}{suffix}.png\\") plt.close() # Example usage: data_example = {\'x\': [0, 1, 2, 3, 4], \'y\': [2, 3, 5, 7, 11]} generate_seaborn_plots(data_example, \\"plot_\\") ``` **Points to Consider:** - The function should handle the creation and saving of plots within a loop to avoid repetitive code. - Ensure that each plot is closed after saving to manage memory usage efficiently.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_seaborn_plots(data, filename_prefix): Generates and saves Seaborn plots with different context settings. Parameters: data (dict): A dictionary with \'x\' and \'y\' keys containing lists of numerical data points. filename_prefix (str): Prefix for the filenames where the plots will be saved. Returns: None contexts = [ (\\"notebook\\", 1.0, None, f\\"{filename_prefix}notebook.png\\"), (\\"notebook\\", 1.5, None, f\\"{filename_prefix}notebook_fontscale_1.5.png\\"), (\\"notebook\\", 1.5, {\\"lines.linewidth\\": 2}, f\\"{filename_prefix}notebook_fontscale_1.5_linewidth_2.png\\") ] for context, font_scale, rc, filename in contexts: sns.set_context(context, font_scale=font_scale, rc=rc) plt.figure() sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"]) plt.savefig(filename) plt.close()"},{"question":"**Objective**: The following question aims to test your understanding and ability to use various functionalities of the pandas library to perform data manipulation and analysis tasks. **Context**: You are given a dataset containing information about sales transactions. The data is provided in the form of a CSV file named `sales_data.csv`. The file has the following columns: `date`, `region`, `product`, `quantity`, and `price`. You need to write a function that performs the following tasks: 1. **Load the dataset** into a pandas DataFrame. 2. **Handle missing data**: Replace any missing values in the `price` column with the mean price of their respective `product`. 3. **Create a new column** called `total_sales`, which is the product of `quantity` and `price`. 4. **Pivot the data** to create a summary table that shows the total sales for each `product` in each `region`. 5. **Merge** this pivot table with another DataFrame that contains product category information (you can create this DataFrame manually within the function). **Function Signature**: ```python def analyze_sales_data(file_path: str) -> pd.DataFrame: # Your code here ``` **Input**: - `file_path` (str): Path to the CSV file containing the sales data. **Output**: - A DataFrame with columns `region`, `product`, `category`, `total_sales`. **Implementation Details**: 1. Load the data using `pd.read_csv`. 2. Handle missing data in the `price` column by replacing NaNs with the mean price of the respective `product`. 3. Create a new column `total_sales` which is calculated as `quantity` * `price`. 4. Pivot the data to create a summary table with regions as the rows, products as the columns, and the sum of `total_sales` as values. 5. Create a DataFrame containing product categories (e.g., `product`, `category`) manually within your function. Merge this with the pivoted DataFrame to include a `category` column in the final output. **Example of the Data**: ```plaintext date,region,product,quantity,price 2023-01-01,North,ProductA,10,20.5 2023-01-01,East,ProductB,5,15.0 2023-01-01,South,ProductA,7, 2023-01-02,West,ProductC,8,30.0 ... ``` **Constraints**: 1. You must use pandas library for all data manipulation tasks. 2. The function should handle cases where the `price` or `quantity` columns may have missing values. **Additional Notes**: - Assume the product category information is as follows (This should be manually added in your function): ```python product_category = pd.DataFrame({ \'product\': [\'ProductA\', \'ProductB\', \'ProductC\'], \'category\': [\'Category1\', \'Category2\', \'Category3\'] }) ``` **Evaluation Criteria**: - Correctness of the output. - Efficient handling of missing data. - Proper use of pandas functions for data manipulation. - Clean and readable code.","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> pd.DataFrame: # Load the dataset df = pd.read_csv(file_path) # Handle missing data in the price column df[\'price\'] = df.groupby(\'product\')[\'price\'].transform(lambda x: x.fillna(x.mean())) # Create a new column total_sales df[\'total_sales\'] = df[\'quantity\'] * df[\'price\'] # Pivot the data pivot_df = df.pivot_table(values=\'total_sales\', index=\'region\', columns=\'product\', aggfunc=\'sum\').reset_index() # Manually create the product category DataFrame product_category = pd.DataFrame({ \'product\': [\'ProductA\', \'ProductB\', \'ProductC\'], \'category\': [\'Category1\', \'Category2\', \'Category3\'] }) # Melt the pivot table to long format for merging melted_df = pivot_df.melt(id_vars=\'region\', var_name=\'product\', value_name=\'total_sales\').fillna(0) # Merge with product category final_df = pd.merge(melted_df, product_category, on=\'product\', how=\'left\') # Reorder columns for the final output final_df = final_df[[\'region\', \'product\', \'category\', \'total_sales\']] return final_df"},{"question":"**Question: Secure Password and Token Management System** You are required to implement a secure password and token management system for a web application. Use the `secrets` module to ensure that the generated passwords and tokens are cryptographically strong. # Function 1: Generate Secure Password Implement a function `generate_password(length: int, requirement: str) -> str` that generates a secure password of a specified length `length` and matches a given requirement `requirement`. The `requirement` parameter can be one of the following: - \\"alpha\\": Password should contain only alphabetic characters (both uppercase and lowercase). - \\"alphanumeric\\": Password should contain alphabetic characters and digits. - \\"complex\\": Password should contain at least one lowercase character, one uppercase character, and three digits. **Input:** - `length`: An integer indicating the password length, between 8 to 128 inclusive. - `requirement`: A string specifying the password requirement. **Output:** - A string representing the generated password. **Example:** ```python password = generate_password(10, \\"complex\\") print(password) # Output: \\"A1b2C3d4E5\\" ``` # Function 2: Generate Secure Token Implement a function `generate_token(size: int, format: str) -> str` that generates a secure token of a specified size `size` and format `format`. The `format` parameter can be one of the following: - \\"bytes\\": Token should be in a byte string format. - \\"hex\\": Token should be in a hexadecimal format. - \\"urlsafe\\": Token should be in a URL-safe text string format. **Input:** - `size`: An integer indicating the number of bytes of randomness, between 16 to 64 inclusive. - `format`: A string specifying the token format. **Output:** - A string representing the generated token. **Example:** ```python token = generate_token(16, \\"hex\\") print(token) # Output: \\"f9bf78b9a18ce6d46a0cd2b0b86df9da\\" ``` # Constraints: - You must use the `secrets` module for random number generation. - Use exception handling to ensure that the input parameters are within the specified ranges. Raise a `ValueError` for any invalid inputs. **Performance Requirements:** - Both functions should execute in reasonable time, even for the upper limit of input sizes. - Ensure that your implementation does not cause any security vulnerabilities like predictable tokens or passwords. Write the complete implementations of the functions `generate_password` and `generate_token`.","solution":"import secrets import string def generate_password(length: int, requirement: str) -> str: if not (8 <= length <= 128): raise ValueError(\\"Password length must be between 8 and 128.\\") if requirement not in {\\"alpha\\", \\"alphanumeric\\", \\"complex\\"}: raise ValueError(\\"Requirement must be \'alpha\', \'alphanumeric\', or \'complex\'.\\") if requirement == \\"alpha\\": alphabet = string.ascii_letters elif requirement == \\"alphanumeric\\": alphabet = string.ascii_letters + string.digits elif requirement == \\"complex\\": alphabet = string.ascii_letters + string.digits password = [] if requirement == \\"complex\\": password.append(secrets.choice(string.ascii_lowercase)) password.append(secrets.choice(string.ascii_uppercase)) for _ in range(3): password.append(secrets.choice(string.digits)) length -= 5 for _ in range(length): password.append(secrets.choice(alphabet)) secrets.SystemRandom().shuffle(password) return \'\'.join(password) def generate_token(size: int, format: str) -> str: if not (16 <= size <= 64): raise ValueError(\\"Token size must be between 16 and 64 bytes.\\") if format not in {\\"bytes\\", \\"hex\\", \\"urlsafe\\"}: raise ValueError(\\"Format must be \'bytes\', \'hex\', or \'urlsafe\'.\\") token = secrets.token_bytes(size) if format == \\"bytes\\": return token elif format == \\"hex\\": return token.hex() elif format == \\"urlsafe\\": return secrets.token_urlsafe(size)"},{"question":"**Objective:** Implement a function that computes the factorial of a large number in a parallel fashion using the `concurrent.futures` module. **Task Description:** You are required to write a function `parallel_factorial(n: int) -> int` that calculates the factorial of a given large integer `n` using parallel computing. To achieve this, you need to break down the factorial computation into smaller chunks and execute these chunks concurrently using threads or processes. **Requirements:** - Use the `ThreadPoolExecutor` or `ProcessPoolExecutor` classes from the `concurrent.futures` module. - Ensure that the result computation handles large values of `n` efficiently. - Handle potential errors or exceptions during task execution. - Aggregate the results of the smaller chunks to compute the final factorial value. **Function Signature:** ```python def parallel_factorial(n: int) -> int: pass ``` **Input:** - `n` (int): The integer for which the factorial will be computed. 1 ≤ n ≤ 100,000. **Output:** - (int): The factorial of the provided integer `n`. **Example:** ```python print(parallel_factorial(5)) # Output: 120 print(parallel_factorial(10)) # Output: 3628800 ``` **Constraints:** - You must use the `concurrent.futures` module for parallel execution. - Ensure efficient handling of large numbers to avoid performance bottlenecks. - Avoid using any external libraries except the standard library. - The solution should be thread-safe and handle any possible exceptions gracefully. # Hints: 1. Divide the factorial calculation into smaller ranges and assign each range to a separate task. 2. Use the `reduce` function from the `functools` module to combine the partial results efficiently. Good luck, and happy coding!","solution":"import concurrent.futures from functools import reduce import operator def factorial_partial(start, end): Compute partial factorial from `start` to `end` (both inclusive) result = 1 for i in range(start, end + 1): result *= i return result def parallel_factorial(n: int) -> int: if n == 0 or n == 1: return 1 num_threads = 8 # You can adjust this based on the number of CPU cores available chunk_size = n // num_threads ranges = [(i * chunk_size + 1, (i + 1) * chunk_size if i < num_threads - 1 else n) for i in range(num_threads)] with concurrent.futures.ThreadPoolExecutor() as executor: futures = [executor.submit(factorial_partial, start, end) for start, end in ranges] results = [future.result() for future in concurrent.futures.as_completed(futures)] return reduce(operator.mul, results, 1)"},{"question":"Objective: Design and implement a task scheduling system using Python\'s `queue.PriorityQueue` class. This system should manage tasks based on their priority, ensuring higher priority tasks are completed first. Requirements: 1. Write a class `TaskScheduler` that: * Initializes a `queue.PriorityQueue`. 2. Define a method `add_task` that: * Takes two arguments - `priority` (an integer) and `task` (a string describing the task). * Adds the task to the priority queue. 3. Define a method `execute_task` that: * Retrieves and removes the highest priority task from the queue. * Returns the task description. * Raises a `queue.Empty` exception if the queue is empty. 4. Define a method `all_tasks` that: * Returns a list of all tasks in the queue in order from highest to lowest priority without removing them from the queue. 5. Ensure thread safety for all queue operations. Constraints: - The `priority` is an integer where a lower number indicates a higher priority. - The `task` is a non-empty string. - The `TaskScheduler` should handle concurrent access from multiple threads. Input and Output Formats: - `add_task(priority: int, task: str) -> None`: Add a task with specified priority. - `execute_task() -> str`: Returns the highest priority task and removes it from the queue. - `all_tasks() -> List[str]`: Returns a list of all tasks in priority order. Performance: - Your implementation should be able to handle at least 1,000 tasks being added and executed. Example: ```python from typing import List class TaskScheduler: def __init__(self): # Your implementation here def add_task(self, priority: int, task: str) -> None: # Your implementation here def execute_task(self) -> str: # Your implementation here def all_tasks(self) -> List[str]: # Your implementation here # Example usage: scheduler = TaskScheduler() scheduler.add_task(2, \\"Clean the house\\") scheduler.add_task(1, \\"Pay bills\\") scheduler.add_task(3, \\"Cook dinner\\") print(scheduler.execute_task()) # Should print: \\"Pay bills\\" print(scheduler.all_tasks()) # Should print: [\\"Clean the house\\", \\"Cook dinner\\"] ``` Ensure your implementation includes necessary imports and handles all specified constraints.","solution":"import queue from typing import List class TaskScheduler: def __init__(self): self._queue = queue.PriorityQueue() def add_task(self, priority: int, task: str) -> None: if not isinstance(priority, int): raise ValueError(\\"Priority must be an integer\\") if not isinstance(task, str) or not task.strip(): raise ValueError(\\"Task must be a non-empty string\\") self._queue.put((priority, task)) def execute_task(self) -> str: if self._queue.empty(): raise queue.Empty(\\"No tasks available to execute\\") return self._queue.get()[1] def all_tasks(self) -> List[str]: # Collect all tasks without removing them temp_queue = queue.PriorityQueue() tasks = [] while not self._queue.empty(): priority, task = self._queue.get() tasks.append(task) temp_queue.put((priority, task)) self._queue = temp_queue return tasks"},{"question":"Advanced Residual Analysis using Seaborn **Objective:** Assess the students\' understanding of seaborn\'s `residplot` function for visualizing and analyzing residuals of regression models, including handling higher-order trends and using LOWESS. **Problem Statement:** You are provided with a dataset `mpg` loaded from seaborn, which contains data on car fuel efficiency. Using this dataset, perform the following tasks to analyze the residuals of different regression models: 1. **Basic Residual Plot:** - Create a residual plot to visualize the relationship between `weight` and `displacement`. Save this plot as `basic_residplot.png`. 2. **Residual Plot for Linear Regression Assumption Check:** - Create a residual plot to check the structure between `horsepower` and `mpg`. Save this plot as `linear_residplot.png`. 3. **Removing Higher-Order Trends:** - Create a residual plot to remove higher-order (quadratic) trends between `horsepower` and `mpg`. Save this plot as `quad_residplot.png`. 4. **Adding LOWESS Curve:** - Create a residual plot with a LOWESS curve for the relationship between `horsepower` and `mpg`. Save this plot as `lowess_residplot.png`. **Input:** - The `mpg` dataset loaded from seaborn. **Output:** - Four PNG files: 1. `basic_residplot.png` 2. `linear_residplot.png` 3. `quad_residplot.png` 4. `lowess_residplot.png` **Constraints:** - Ensure that you save the plots with the specified filenames. **Performance Requirements:** - Efficiently generate the plots and ensure visual clarity. **Example Code:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # 1. Basic Residual Plot plt.figure() sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.savefig(\'basic_residplot.png\') # 2. Residual Plot for Linear Regression Assumption Check plt.figure() sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.savefig(\'linear_residplot.png\') # 3. Removing Higher-Order Trends plt.figure() sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.savefig(\'quad_residplot.png\') # 4. Adding LOWESS Curve plt.figure() sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.savefig(\'lowess_residplot.png\') ``` Use the provided example code as a guideline to implement the solution. Ensure all plots are correctly saved with the specified filenames.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # 1. Basic Residual Plot plt.figure() sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.savefig(\'basic_residplot.png\') # 2. Residual Plot for Linear Regression Assumption Check plt.figure() sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.savefig(\'linear_residplot.png\') # 3. Removing Higher-Order Trends plt.figure() sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.savefig(\'quad_residplot.png\') # 4. Adding LOWESS Curve plt.figure() sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True) plt.savefig(\'lowess_residplot.png\')"},{"question":"**Complex MIME Email Construction** **Objective:** Create a function to construct a complex MIME email with multiple parts, including text, an image, and an attached PDF file. **Problem Statement:** Write a Python function `construct_mime_email` that takes the following inputs: - `subject`: A string representing the email subject. - `sender`: A string representing the sender\'s email address. - `receiver`: A string representing the receiver\'s email address. - `text_body`: A string representing the plain text body of the email. - `html_body`: A string representing the HTML body of the email. - `image_path`: A string representing the file path to an image to be attached. - `pdf_path`: A string representing the file path to a PDF to be attached. The function should construct and return a `MIMEMultipart` email object with the following structure: 1. The email should have a plain text part. 2. The email should have an HTML part. 3. The email should have the specified image attached. 4. The email should have the specified PDF attached. 5. The email should include appropriate headers for the subject, from, and to fields. **Input Format:** - `subject`: A non-empty string. - `sender`: A non-empty string representing a valid email address. - `receiver`: A non-empty string representing a valid email address. - `text_body`: A non-empty string. - `html_body`: A non-empty string containing HTML content. - `image_path`: A string representing the file path to an existing image file. - `pdf_path`: A string representing the file path to an existing PDF file. **Output Format:** - Returns a `MIMEMultipart` email object with the specified content and structure. **Constraints:** - Both the image file and the PDF file exist at the specified paths. - The function should handle errors gracefully, signaling any issues with file paths or invalid input through appropriate exceptions. **Function Signature:** ```python def construct_mime_email(subject: str, sender: str, receiver: str, text_body: str, html_body: str, image_path: str, pdf_path: str) -> email.mime.multipart.MIMEMultipart: pass ``` **Example:** ```python subject = \\"Test Email\\" sender = \\"sender@example.com\\" receiver = \\"receiver@example.com\\" text_body = \\"This is the plain text body of the email.\\" html_body = \\"<html><body><h1>This is the HTML body of the email.</h1></body></html>\\" image_path = \\"/path/to/image.jpg\\" pdf_path = \\"/path/to/document.pdf\\" email_message = construct_mime_email(subject, sender, receiver, text_body, html_body, image_path, pdf_path) ``` **Explanation:** This code constructs a MIME email with a text part, an HTML part, and attachments for an image and a PDF file. The function should ensure all elements are properly encoded and attached, and the resulting `MIMEMultipart` object should be ready for sending using Python\'s `smtplib`. **Evaluation Criteria:** - Correctness: The function should produce the expected email structure. - Error Handling: The function should handle and report errors gracefully. - Use of `email.mime` modules: The function should make appropriate and effective use of the classes provided in the documentation.","solution":"import os from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.base import MIMEBase from email import encoders def construct_mime_email(subject: str, sender: str, receiver: str, text_body: str, html_body: str, image_path: str, pdf_path: str) -> MIMEMultipart: # Check if files exist if not os.path.isfile(image_path): raise FileNotFoundError(f\\"Image file not found: {image_path}\\") if not os.path.isfile(pdf_path): raise FileNotFoundError(f\\"PDF file not found: {pdf_path}\\") # Create the root message msg = MIMEMultipart(\'related\') msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = receiver # Attach the text and HTML parts alternative = MIMEMultipart(\'alternative\') alternative.attach(MIMEText(text_body, \'plain\')) alternative.attach(MIMEText(html_body, \'html\')) msg.attach(alternative) # Attach the image with open(image_path, \'rb\') as img_file: img = MIMEImage(img_file.read(), name=os.path.basename(image_path)) msg.attach(img) # Attach the PDF with open(pdf_path, \'rb\') as pdf_file: part = MIMEBase(\'application\', \'octet-stream\') part.set_payload(pdf_file.read()) encoders.encode_base64(part) part.add_header(\'Content-Disposition\', f\'attachment; filename={os.path.basename(pdf_path)}\') msg.attach(part) return msg"},{"question":"Resource Usage Tracking Utility Objective Implement a Python program that monitors and reports the system\'s resource usage periodically. Your program should leverage the \'resource\' module to fetch resource usage information and the \'syslog\' module to log this information. Task 1. **Function 1: `fetch_resource_usage`** - **Input**: None. - **Output**: A dictionary containing the following keys and their respective resource usage values: - \'ru_utime\': User time used - \'ru_stime\': System time used - \'ru_maxrss\': Maximum resident set size - \'ru_ixrss\': Shared memory size - \'ru_idrss\': Unshared memory size - \'ru_isrss\': Unshared stack size - \'ru_minflt\': Page reclaims - \'ru_majflt\': Page faults - \'ru_nswap\': Swaps 2. **Function 2: `log_usage`** - **Input**: A dictionary with resource usage information (as generated by `fetch_resource_usage`). - **Output**: None. - **Operation**: This function should log the resource usage information using the \'syslog\' module. Each key-value pair in the dictionary should be logged as a separate syslog entry. 3. **Main Functionality: Continuous Monitoring** - **Operation**: - Your program should repeatedly fetch and log the resource usage every 5 seconds. - The program should terminate after running for 1 minute (i.e., after 12 iterations). Constraints - Use only the standard Python library modules: `resource` and `syslog`. Example Output It is not possible to provide a static example output for this dynamic program since the resource utilization will differ based on the system\'s current state and activity. ```python import resource import syslog import time def fetch_resource_usage(): usage = resource.getrusage(resource.RUSAGE_SELF) return { \'ru_utime\': usage.ru_utime, \'ru_stime\': usage.ru_stime, \'ru_maxrss\': usage.ru_maxrss, \'ru_ixrss\': usage.ru_ixrss, \'ru_idrss\': usage.ru_idrss, \'ru_isrss\': usage.ru_isrss, \'ru_minflt\': usage.ru_minflt, \'ru_majflt\': usage.ru_majflt, \'ru_nswap\': usage.ru_nswap } def log_usage(usage_dict): for key, value in usage_dict.items(): syslog.syslog(f\\"{key}: {value}\\") def main(): syslog.openlog(logoption=syslog.LOG_PID, facility=syslog.LOG_USER) for _ in range(12): usage = fetch_resource_usage() log_usage(usage) time.sleep(5) syslog.closelog() if __name__ == \\"__main__\\": main() ``` This coding question will assess the student\'s ability to interact with Unix system calls and manage system resources, demonstrating both fundamental and advanced Python skills.","solution":"import resource import syslog import time def fetch_resource_usage(): usage = resource.getrusage(resource.RUSAGE_SELF) return { \'ru_utime\': usage.ru_utime, \'ru_stime\': usage.ru_stime, \'ru_maxrss\': usage.ru_maxrss, \'ru_ixrss\': usage.ru_ixrss, \'ru_idrss\': usage.ru_idrss, \'ru_isrss\': usage.ru_isrss, \'ru_minflt\': usage.ru_minflt, \'ru_majflt\': usage.ru_majflt, \'ru_nswap\': usage.ru_nswap } def log_usage(usage_dict): for key, value in usage_dict.items(): syslog.syslog(f\\"{key}: {value}\\") def main(): syslog.openlog(logoption=syslog.LOG_PID, facility=syslog.LOG_USER) for _ in range(12): usage = fetch_resource_usage() log_usage(usage) time.sleep(5) syslog.closelog() if __name__ == \\"__main__\\": main()"},{"question":"Question: Efficient Memory Management and Profiling using `torch.mps` You are tasked with implementing efficient memory management and profiling for a simple PyTorch model on MPS-compatible Apple devices. The aim is to ensure minimal memory usage and to profile the training process for insights on performance bottlenecks. # Requirements: 1. **Function `train_model`**: - Input: - `model`: A PyTorch neural network model. - `data_loader`: A DataLoader providing training data. - `criterion`: The loss function. - `optimizer`: The optimization algorithm. - `epochs`: Number of training epochs (default=5). - Output: None (prints profiling information and memory usage). 2. **Functionality**: - Check if MPS is available. If not, raise an exception. - Train the model for the specified number of epochs. - Efficiently manage the memory during training using `empty_cache` and other memory management functions. - Profile the training process to capture performance metrics. - At the end of each epoch, print the current and driver-allocated memory. - Upon completing training, print the profiling report. 3. **Constraints**: - Handle large datasets efficiently to avoid memory overflow. - Ensure proper synchronization of MPS device after each epoch. # Example: ```python import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset import torch.mps as mps class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) # Dummy dataset data = torch.randn(100, 10) targets = torch.randn(100, 1) dataset = TensorDataset(data, targets) data_loader = DataLoader(dataset, batch_size=10) model = SimpleModel() criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) def train_model(model, data_loader, criterion, optimizer, epochs=5): # Check if MPS is available if not torch.backends.mps.is_available(): raise Exception(\\"MPS is not available on this system.\\") model.to(\'mps\') profiler = mps.profiler.profile(enabled=True) # Start profiling profiler.start() for epoch in range(epochs): for batch in data_loader: inputs, labels = batch inputs, labels = inputs.to(\'mps\'), labels.to(\'mps\') optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # Synchronize and empty cache mps.synchronize() mps.empty_cache() # Print memory usage print(f\\"Epoch {epoch + 1} -\\", f\\"Current allocated memory: {mps.current_allocated_memory()} bytes\\", f\\"Driver allocated memory: {mps.driver_allocated_memory()} bytes\\") # Stop profiling profiler.stop() # Print profiling report print(f\\"Profiling information: {profiler.get_profiler_report()}\\") ``` Feel free to simulate larger datasets and different models to test the robustness of your implementation.","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset import torch.profiler import torch.mps class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def train_model(model, data_loader, criterion, optimizer, epochs=5): # Check if MPS is available if not torch.backends.mps.is_available(): raise Exception(\\"MPS is not available on this system.\\") device = torch.device(\\"mps\\") model.to(device) # Initialize the profiler with torch.profiler.profile( activities=[ torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.MPS ], schedule=torch.profiler.schedule(wait=1, warmup=1, active=3), on_trace_ready=torch.profiler.tensorboard_trace_handler(\\"./log\\") ) as profiler: for epoch in range(epochs): for batch in data_loader: inputs, labels = batch inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # Synchronize and empty cache after each epoch torch.mps.synchronize() torch.mps.empty_cache() # Print memory usage print(f\\"Epoch {epoch + 1} -\\", f\\"Current allocated memory: {torch.mps.current_allocated_memory()} bytes\\", f\\"Driver allocated memory: {torch.mps.driver_allocated_memory()} bytes\\") # Step the profiler profiler.step()"},{"question":"# Python Coding Assessment Question **Objective**: Demonstrate your understanding and ability to work with the asyncio library in Python 3.10 by writing an asynchronous program that utilizes task creation, task management, and synchronization primitives. **Problem Statement**: You are required to implement an asynchronous task scheduler that can process multiple tasks concurrently with different priorities. The task scheduler should: 1. Accept tasks with different priorities. 2. Utilize a priority queue to manage the scheduling order based on the task priority. 3. Allow running up to a fixed number of tasks concurrently. 4. Use synchronization primitives to ensure thread-safe operations. **Function Implementation**: You need to implement a class `TaskScheduler` with the following methods: 1. `add_task(coroutine: Coroutine, priority: int)`: Adds a new task (coroutine) to the scheduler with the given priority. Lower integers indicate higher priority. 2. `run_tasks(concurrent_limit: int)`: Runs the tasks concurrently up to the specified limit. 3. `get_results() -> List`: Returns the results of all completed tasks in the order they finished. **Input and Output**: - Tasks are represented as coroutines that return a result when awaited. - Task priorities are integer values where a lower number means a higher priority. - `concurrent_limit` is an integer specifying the maximum number of tasks that can run concurrently. - The `get_results` method should output a list of results in the order they were completed. **Constraints**: - You must use `asyncio.PriorityQueue` to manage task priorities. - Use asynchronous functions and appropriate asyncio synchronization primitives to ensure concurrency and safety. **Example Usage**: ```python import asyncio async def sample_task(task_id, duration): await asyncio.sleep(duration) return f\'Task {task_id} completed\' # Example tasks with different priorities tasks = [ (sample_task(1, 2), 2), (sample_task(2, 1), 1), (sample_task(3, 3), 3), ] # Initialize task scheduler scheduler = TaskScheduler() # Add tasks to the task scheduler for task, priority in tasks: scheduler.add_task(task, priority) # Run tasks with a concurrency limit of 2 await scheduler.run_tasks(concurrent_limit=2) # Retrieve and print task results results = scheduler.get_results() print(results) ``` You should test your implementation using the provided example to ensure correctness and adherence to the specified functionality. **Note**: Make sure to handle exceptions gracefully within your implementation.","solution":"import asyncio from typing import Coroutine, List, Tuple import heapq class TaskScheduler: def __init__(self): self.priority_queue = [] self.results = [] self.lock = asyncio.Lock() def add_task(self, coroutine: Coroutine, priority: int): Adds a new task (coroutine) to the scheduler with the given priority. Lower integers indicate higher priority. heapq.heappush(self.priority_queue, (priority, coroutine)) async def run_tasks(self, concurrent_limit: int): Runs the tasks concurrently up to the specified limit. async def worker(q): while not q.empty(): priority, task = await q.get() result = await task async with self.lock: self.results.append(result) async def queue_tasks(): q = asyncio.PriorityQueue() for priority, coroutine in self.priority_queue: q.put_nowait((priority, coroutine)) workers = [asyncio.create_task(worker(q)) for _ in range(concurrent_limit)] await asyncio.gather(*workers) await queue_tasks() def get_results(self) -> List: Returns the results of all completed tasks in the order they finished. return self.results"},{"question":"# Question: Building an Asynchronous Task Manager You are tasked with building an asynchronous task manager using Python\'s `asyncio` library. Your manager will handle multiple tasks that simulate asynchronous I/O operations such as network requests or database queries. The goal is to implement a system that can run these tasks concurrently and manage their execution. Requirements: 1. **Task Definition**: - Define an asynchronous function called `perform_task` that accepts two parameters: `task_id` (an integer) and `duration` (a float representing time in seconds). - This function should print a message indicating the start of the task, await a duration specified by the `duration` parameter, print another message indicating the completion of the task, and then return the `task_id`. 2. **Task Manager**: - Implement an asynchronous function called `task_manager` that accepts a list of tuples. Each tuple contains two elements: `task_id` and `duration`. - The `task_manager` function should: - Create and run all tasks concurrently using `asyncio.gather`. - Collect and return the results of all tasks. 3. **Execution**: - Create a synchronous function called `run_task_manager` that initializes the asyncio event loop and calls the `task_manager` function with a given list of tasks. Constraints: - Use only the `asyncio` library for asynchronous operations. - Ensure that tasks are executed concurrently. - You must handle tasks with varying durations robustly. Example: ```python import asyncio async def perform_task(task_id: int, duration: float) -> int: # Your code here pass async def task_manager(tasks: [(int, float)]) -> [int]: # Your code here pass def run_task_manager(tasks: [(int, float)]) -> [int]: # Your code here pass # Example usage tasks = [(1, 2), (2, 3), (3, 1)] result = run_task_manager(tasks) print(result) # Example output: [1, 2, 3] ``` Ensure that your implementation prints messages and handles concurrency properly. When you run the example, you should see interleaved start and completion messages based on the task durations. Testing: - Create a list of tasks with varying durations and verify that the tasks run concurrently. - Check the order of task completion messages to ensure they align with the task durations.","solution":"import asyncio async def perform_task(task_id: int, duration: float) -> int: Simulates an asynchronous task with given task_id and duration. Args: task_id (int): The ID of the task. duration (float): The duration for which the task will run. Returns: int: The task_id upon completion. print(f\'Task {task_id} started, duration: {duration}\') await asyncio.sleep(duration) print(f\'Task {task_id} completed, duration: {duration}\') return task_id async def task_manager(tasks: [(int, float)]) -> [int]: Manages multiple asynchronous tasks concurrently. Args: tasks (list of tuples): Each tuple contains task_id and duration. Returns: list of int: The task_ids of all completed tasks. task_list = [perform_task(task_id, duration) for task_id, duration in tasks] results = await asyncio.gather(*task_list) return results def run_task_manager(tasks: [(int, float)]) -> [int]: Runs the asynchronous task manager. Args: tasks (list of tuples): Each tuple contains task_id and duration. Returns: list of int: The task_ids of all completed tasks. return asyncio.run(task_manager(tasks))"},{"question":"Coding Assessment Question # Objective You are required to implement a function that compiles multiple Python source files into bytecode using the `py_compile` module. The function should handle possible errors gracefully and log the compilation results in a specified format. # Function Signature ```python def batch_compile(files, log_file, optimize=-1, invalidation_mode=\'TIMESTAMP\', quiet=0): Compile a list of Python source files into bytecode using the py_compile module. Args: files (list): A list of paths to the Python source files to be compiled. log_file (str): The path to the log file where the compilation results will be saved. optimize (int, optional): The optimization level; default is -1 (use the interpreter\'s optimization level). invalidation_mode (str, optional): The invalidation mode to use; default is \'TIMESTAMP\'. Allowed values are \'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\'. quiet (int, optional): Suppression level for error messages (0: no suppression, 1: less verbosity, 2: no error output); default is 0. Returns: dict: A dictionary mapping each source file name to a tuple containing the result of the compilation (\'Success\', \'Error\') and either the path to the compiled file or an error message. ``` # Input - `files`: A list of strings, each representing a path to a Python source file. - `log_file`: A string representing the path to a log file to write the results. - `optimize`: An integer for the optimization level passed to the compiler. - `invalidation_mode`: A string specifying the invalidation mode (\'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\'). - `quiet`: An integer specifying the suppression level for error messages (0, 1, or 2). # Output - Returns a dictionary where each key is a source file path and the value is a tuple: - The first element of the tuple is either \'Success\' or \'Error\'. - The second element is the path to the compiled file if successful, or an error message if not. # Constraints - The function should correctly handle cases where the specified `invalidation_mode` is invalid and log appropriate errors. - All error messages should be logged in the `log_file` if not suppressed. - If the `log_file` can\'t be written to, the function should raise an IOError. # Example ```python files = [\\"script1.py\\", \\"script2.py\\"] log_file = \\"compile_log.txt\\" result = batch_compile(files, log_file, optimize=0, invalidation_mode=\'CHECKED_HASH\', quiet=1) # Expected result (assuming no errors during compilation): # { # \\"script1.py\\": (\\"Success\\", \\"path/to/__pycache__/script1.cpython-310.pyc\\"), # \\"script2.py\\": (\\"Success\\", \\"path/to/__pycache__/script2.cpython-310.pyc\\") # } # The log_file \\"compile_log.txt\\" should contain entries for each file indicating \'Success\' or \'Error\' with details. ``` # Notes - Make sure to use the `py_compile.compile` function as described in the documentation for the actual compilation process. - Use proper exception handling to manage and log errors during compilation. - Ensure that the invalidation_mode is correctly translated to py_compile.PycInvalidationMode before passing it to the compile function.","solution":"import py_compile import os def batch_compile(files, log_file, optimize=-1, invalidation_mode=\'TIMESTAMP\', quiet=0): Compile a list of Python source files into bytecode using the py_compile module. Args: files (list): A list of paths to the Python source files to be compiled. log_file (str): The path to the log file where the compilation results will be saved. optimize (int, optional): The optimization level; default is -1 (use the interpreter\'s optimization level). invalidation_mode (str, optional): The invalidation mode to use; default is \'TIMESTAMP\'. Allowed values are \'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\'. quiet (int, optional): Suppression level for error messages (0: no suppression, 1: less verbosity, 2: no error output); default is 0. Returns: dict: A dictionary mapping each source file name to a tuple containing the result of the compilation (\'Success\', \'Error\') and either the path to the compiled file or an error message. mode_mapping = { \'TIMESTAMP\': py_compile.PycInvalidationMode.TIMESTAMP, \'CHECKED_HASH\': py_compile.PycInvalidationMode.CHECKED_HASH, \'UNCHECKED_HASH\': py_compile.PycInvalidationMode.UNCHECKED_HASH } results = {} with open(log_file, \'w\') as log: for file in files: try: if invalidation_mode not in mode_mapping: raise ValueError(f\\"Invalid invalidation_mode: {invalidation_mode}\\") compiled_path = py_compile.compile(file, optimize=optimize, invalidation_mode=mode_mapping[invalidation_mode], quiet=quiet) results[file] = (\'Success\', compiled_path) log.write(f\\"{file}: Success, compiled to {compiled_path}n\\") except Exception as e: results[file] = (\'Error\', str(e)) if quiet < 2: log.write(f\\"{file}: Error, {str(e)}n\\") return results"},{"question":"You have been provided with a DataFrame that contains information about several machines and their status during different time periods. The DataFrame `df` has a nullable boolean column \'status\' that indicates whether each machine was operating (`True`), not operating (`False`), or if the information is unavailable (`NA`). Additionally, the DataFrame contains a column \'machine_id\' to uniquely identify each machine and a column \'time_period\' to specify the time period. ```python import pandas as pd data = { \'machine_id\': [1, 1, 1, 2, 2, 2, 3, 3, 3], \'time_period\': [\'2021-Q1\', \'2021-Q2\', \'2021-Q3\', \'2021-Q1\', \'2021-Q2\', \'2021-Q3\', \'2021-Q1\', \'2021-Q2\', \'2021-Q3\'], \'status\': pd.array([True, pd.NA, False, True, True, pd.NA, pd.NA, False, False], dtype=\'boolean\') } df = pd.DataFrame(data) ``` # Task 1. Write a function `calculate_operation_percentage(df)` that calculates the percentage of time each machine was operational (i.e., status was `True`) over all the time periods where the status is known (i.e., not `NA`). The function should return a dictionary with the `machine_id` as the keys and the operation percentage as the values. 2. Write a function `filter_machines(df, threshold)` that filters out the machines that were operational less than the given threshold percentage of the time. This function should return a DataFrame containing only the records for the machines that meet or exceed the operation threshold. The threshold is a float between 0 and 100. # Expected Input and Output calculate_operation_percentage(df) **Input:** - `df`: A pandas DataFrame similar to the provided example. **Output:** - A dictionary with `machine_id` as keys and their operation percentage as values. filter_machines(df, threshold) **Input:** - `df`: A pandas DataFrame similar to the provided example. - `threshold`: A float between 0 and 100 indicating the minimum operational percentage required. **Output:** - A pandas DataFrame filtered based on the operation threshold. # Constraints - The input DataFrame will always contain the columns `machine_id`, `time_period`, and `status`. - The `status` column is a nullable boolean (`True`, `False`, `NA`). # Performance Requirements - The solutions should handle large DataFrames efficiently. # Example ```python # Example input DataFrame import pandas as pd data = { \'machine_id\': [1, 1, 1, 2, 2, 2, 3, 3, 3], \'time_period\': [\'2021-Q1\', \'2021-Q2\', \'2021-Q3\', \'2021-Q1\', \'2021-Q2\', \'2021-Q3\', \'2021-Q1\', \'2021-Q2\', \'2021-Q3\'], \'status\': pd.array([True, pd.NA, False, True, True, pd.NA, pd.NA, False, False], dtype=\'boolean\') } df = pd.DataFrame(data) # Calculate operation percentages operation_percentages = calculate_operation_percentage(df) print(operation_percentages) # Output: {1: 50.0, 2: 100.0, 3: 0.0} # Filter machines with at least 50% operational time filtered_df = filter_machines(df, 50) print(filtered_df) # Expected output: # machine_id time_period status # 0 1 2021-Q1 True # 1 1 2021-Q2 <NA> # 2 1 2021-Q3 False # 3 2 2021-Q1 True # 4 2 2021-Q2 True # 5 2 2021-Q3 <NA> ```","solution":"import pandas as pd def calculate_operation_percentage(df): Calculate the percentage of time each machine was operational. Parameters: - df: pandas DataFrame with columns \'machine_id\', \'time_period\', and \'status\' Returns: - A dictionary with machine_id as keys and their operation percentage as values result = {} grouped = df.groupby(\'machine_id\') for machine_id, group in grouped: total_known = group[\'status\'].notna().sum() operational_count = group[\'status\'].sum() if total_known > 0: operation_percentage = (operational_count / total_known) * 100 else: operation_percentage = 0.0 result[machine_id] = operation_percentage return result def filter_machines(df, threshold): Filter out machines that were operational less than the threshold percentage of the time. Parameters: - df: pandas DataFrame with columns \'machine_id\', \'time_period\', and \'status\' - threshold: float, the minimum operational percentage required Returns: - A pandas DataFrame containing only the records for machines that meet or exceed the operation threshold operation_percentages = calculate_operation_percentage(df) machines_to_keep = [machine_id for machine_id, percentage in operation_percentages.items() if percentage >= threshold] return df[df[\'machine_id\'].isin(machines_to_keep)]"},{"question":"You are tasked with creating a user authentication system using some of the capabilities provided by the `functools` module. Your implementation should utilize caching mechanisms, partial functions, and proper decorator usage to create efficient and maintainable code. Task Description: 1. **Function Caching**: Implement a function, `authenticate(username, password)`, that takes a username and password and returns whether the username-password combination is correct. This function should use caching to improve performance by avoiding repeated checks for the same credentials. 2. **Partial Function for Password Requirements**: Create a partial function, `is_required_length`, that checks if a provided password meets a specific minimum length requirement. Use `functools.partial` to create a customizable minimum length checker called `minimum_8_chars`. 3. **Cached Property for User Validation**: Create a `User` class where: - The `authenticate` method checks user credentials using the cached `authenticate` function. - The instance method `is_valid` is a cached property that returns `True` if the user credentials are valid, ensuring that validation is only performed once per instance. 4. **Decorator for Enhanced Logging**: Create a decorator `log_attempts` using `functools.wraps` to log every authentication attempt, specifying the username and whether the attempt was successful. Requirements: - Use `@functools.lru_cache` to cache the `authenticate` function. - Use `functools.partial` to pre-configure the password length requirement checker. - Use `@functools.cached_property` to cache the validation status within the `User` class. - Use `@functools.wraps` to ensure the logging decorator preserves the original function\'s metadata. Constraints: 1. Assume the following dictionary contains valid user credentials: ```python VALID_USERS = { \'user1\': \'password123\', \'user2\': \'ilovecoding\', \'admin\': \'adminpass\' } ``` 2. Minimum password length should be configurable. Example Usage: ```python # Create a partial function for minimum 8 character password check is_8_chars_long = functools.partial(is_required_length, min_length=8) # Create and authenticate a user with logging enabled user = User(\'user1\', \'password123\') # Check if user is valid print(user.is_valid) # Should print True after authentication # Authenticate again should use cached result print(user.is_valid) # Should print True without authentication attempt # Check for logging information log_attempts(authenticate) ``` Implementation: ```python import functools from functools import lru_cache, partial, wraps, cached_property VALID_USERS = { \'user1\': \'password123\', \'user2\': \'ilovecoding\', \'admin\': \'adminpass\' } # 1. Function Caching @lru_cache(maxsize=None) def authenticate(username, password): return VALID_USERS.get(username) == password # 2. Partial Function for Password Requirements def is_required_length(password, min_length): return len(password) >= min_length # Create a pre-configured password length checker for minimum 8 characters minimum_8_chars = partial(is_required_length, min_length=8) # 3. Cached Property for User Validation class User: def __init__(self, username, password): self.username = username self.password = password @cached_property def is_valid(self): return authenticate(self.username, self.password) # 4. Decorator for Enhanced Logging def log_attempts(func): @wraps(func) def wrapper(*args, **kwargs): result = func(*args, **kwargs) username = args[0] print(f\\"Authentication attempt for {username}: {\'Successful\' if result else \'Failed\'}\\") return result return wrapper ```","solution":"import functools from functools import lru_cache, partial, wraps, cached_property VALID_USERS = { \'user1\': \'password123\', \'user2\': \'ilovecoding\', \'admin\': \'adminpass\' } # 1. Function Caching @lru_cache(maxsize=None) def authenticate(username, password): Check if the provided username and password are correct. return VALID_USERS.get(username) == password # 2. Partial Function for Password Requirements def is_required_length(password, min_length): Check if the password meets the minimum length requirement. return len(password) >= min_length # Create a pre-configured password length checker for minimum 8 characters minimum_8_chars = partial(is_required_length, min_length=8) # 3. Cached Property for User Validation class User: def __init__(self, username, password): self.username = username self.password = password @cached_property def is_valid(self): Return True if the user credentials are valid, otherwise False. return authenticate(self.username, self.password) # 4. Decorator for Enhanced Logging def log_attempts(func): @wraps(func) def wrapper(*args, **kwargs): result = func(*args, **kwargs) username = args[0] print(f\\"Authentication attempt for {username}: {\'Successful\' if result else \'Failed\'}\\") return result return wrapper # Decorating the authenticate function with log_attempts decorator authenticate = log_attempts(authenticate)"},{"question":"Context: You are working as a software developer in a large organization and part of your responsibility is to set up automated processes to ensure that all Python source files in various projects are pre-compiled to byte-code. This is to optimize the execution for end-users who may not have write permissions on the deployed directories. Task: Write a Python function called `custom_compile_all` that takes the following parameters: - `root_dirs`: A list of directory paths (as strings) which need to be recursively compiled. - `exclude_pattern`: A regular expression pattern (as a string) to exclude certain files or directories from being compiled. - `force_recompile`: A boolean indicating whether to recompile files even if their timestamps are up-to-date. - `optimization_levels`: A list of integers representing optimization levels to be used during compilation. - `verbose_output`: A boolean indicating whether verbose output should be printed (similar to the `-q` option). This function should: 1. Compile all `.py` files found in the directories listed in `root_dirs`, recursively. 2. Exclude any files or directories matching the `exclude_pattern`. 3. Recompile files even if their timestamps are up-to-date if `force_recompile` is `True`. 4. Apply multiple optimization levels if provided. 5. Control the verbosity of the output based on the `verbose_output` parameter. Implementation Constraints: - You should utilize the `compileall` package functions to perform the compilation. - Ensure that the function is performant and handles large directory trees efficiently. - Handle any exceptions that might occur during compilation and print an appropriate message without halting the entire compilation process. Example Usage: ```python import re # Example usage custom_compile_all( root_dirs=[\\"/path/to/project1\\", \\"/path/to/project2\\"], exclude_pattern=r\'.*tests.*\', force_recompile=True, optimization_levels=[0, 1], verbose_output=False ) ``` This example should compile all `.py` files in `/path/to/project1` and `/path/to/project2`, exclude any files or directories containing \\"tests\\", force recompilation, and use optimization levels 0 and 1 without printing verbose output. Expected Output: Your function does not need to return anything. It should perform the task as specified and handle any necessary output (errors or informational messages) during the process. Good luck!","solution":"import compileall import re from pathlib import Path def custom_compile_all(root_dirs, exclude_pattern, force_recompile, optimization_levels, verbose_output): Compile all `.py` files found in the directories listed in `root_dirs`. Args: root_dirs (list): List of directory paths as strings which need to be recursively compiled. exclude_pattern (str): A regular expression pattern to exclude certain files or directories. force_recompile (bool): Whether to recompile files even if their timestamps are up-to-date. optimization_levels (list): A list of integers representing optimization levels to be used during compilation. verbose_output (bool): Whether verbose output should be printed. exclude_re = re.compile(exclude_pattern) for root_dir in root_dirs: for optimization in optimization_levels: try: compileall.compile_dir( dir=root_dir, maxlevels=10, ddir=None, force=force_recompile, rx=exclude_re, quiet=not verbose_output, legacy=False, optimize=optimization ) except Exception as e: print(f\\"Error compiling directory {root_dir} with optimization level {optimization}: {e}\\")"},{"question":"# PyTorch Data Type Information Extraction In this task, you are required to create a Python function that accepts a PyTorch data type (dtype) and returns a dictionary containing key numerical properties of that dtype. Function Signature ```python def extract_dtype_info(dtype: torch.dtype) -> dict: pass ``` Input - `dtype` (torch.dtype): A PyTorch dtype (e.g., `torch.float32`, `torch.int64`). Output - Returns a dictionary containing the following keys and their respective values: - `\'bits\'`: Number of bits occupied by the type. - `\'eps\'`: Smallest representable number such that `1.0 + eps != 1.0`. Only for floating point types. - `\'max\'`: Largest representable number. - `\'min\'`: Smallest representable number. - `\'tiny\'`: Smallest positive normal number. Only for floating point types. Constraints - You should handle both floating-point and integer dtypes. - If the dtype is an integer type, the values corresponding to `\'eps\'` and `\'tiny\'` should be `None`. Example Usage ```python import torch # Example for floating point dtype print(extract_dtype_info(torch.float32)) # Expected output: # { # \'bits\': 32, # \'eps\': 1.1920928955078125e-07, # \'max\': 3.4028234663852886e+38, # \'min\': -3.4028234663852886e+38, # \'tiny\': 1.1754943508222875e-38 # } # Example for integer dtype print(extract_dtype_info(torch.int32)) # Expected output: # { # \'bits\': 32, # \'eps\': None, # \'max\': 2147483647, # \'min\': -2147483648, # \'tiny\': None # } ``` Notes - Use `torch.finfo` for floating-point dtypes to get values for `\'eps\'`, `\'tiny\'`, etc. - Use `torch.iinfo` for integer dtypes to get values for `\'bits\'`, `\'max\'`, and `\'min\'`. This exercise will test your ability to use PyTorch\'s dtype information classes and handle different data types in a unified manner.","solution":"import torch def extract_dtype_info(dtype: torch.dtype) -> dict: Extracts numerical properties of a PyTorch dtype. Parameters: dtype (torch.dtype): The PyTorch data type. Returns: dict: A dictionary with numerical properties of the dtype. if dtype.is_floating_point: info = torch.finfo(dtype) else: info = torch.iinfo(dtype) result = { \'bits\': info.bits, \'eps\': getattr(info, \'eps\', None), \'max\': info.max, \'min\': info.min, \'tiny\': getattr(info, \'tiny\', None) } return result"},{"question":"# Advanced Enum Usage in Python Objective Demonstrate your understanding of the `enum` module in Python by defining and manipulating enumerations using advanced features like auto values, custom methods, and ensuring uniqueness. Question You are tasked with implementing a custom enumeration that represents different states of an order processing system in a warehouse. The states are `PENDING`, `PROCESSED`, `SHIPPED`, and `DELIVERED`. Additionally, you need to ensure that these states have unique values, even if aliases are accidentally introduced, and that their string representations are customized. **Requirements:** 1. **Enumeration Definition**: - Define an enumeration `OrderState` using the `Enum` class. - Use the `@unique` decorator to ensure values are unique. - Use `auto()` to assign successive integer values starting from 1. 2. **Custom Representation**: - Override the `__str__` method to return the name of the state in a more human-readable format (capitalize the first letter and lowercase the rest). 3. **Equality Check and Custom Methods**: - Implement an `is_final` method to check if the state is `DELIVERED`, which is a terminal state indicating the order process is complete. - Implement an `is_transitional` method to check if the state is either `PROCESSED` or `SHIPPED`. 4. **Constraints and Input Details**: - `OrderState` should strictly follow the values and conditions specified above. - No duplicate values. Their names and values should be managed using the `unique` decorator and `auto`. **Sample Usage**: ```python from enum import Enum, unique, auto @unique class OrderState(Enum): PENDING = auto() PROCESSED = auto() SHIPPED = auto() DELIVERED = auto() def __str__(self): return self.name.capitalize() def is_final(self): return self == OrderState.DELIVERED def is_transitional(self): return self in [OrderState.PROCESSED, OrderState.SHIPPED] # Example Function Usage # Print State print(str(OrderState.PENDING)) # Output: \\"Pending\\" # Check terminal state print(OrderState.DELIVERED.is_final()) # Output: True # Check transitional state print(OrderState.PROCESSED.is_transitional()) # Output: True print(OrderState.PENDING.is_transitional()) # Output: False ``` **Task**: Define the `OrderState` enumeration with the required methods and attributes. After defining the class, demonstrate its functionality with sample usage examples as shown. The solution should be self-contained and adhere to the constraints specified above.","solution":"from enum import Enum, unique, auto @unique class OrderState(Enum): PENDING = auto() PROCESSED = auto() SHIPPED = auto() DELIVERED = auto() def __str__(self): return self.name.capitalize() def is_final(self): return self == OrderState.DELIVERED def is_transitional(self): return self in [OrderState.PROCESSED, OrderState.SHIPPED]"},{"question":"Title: Importing and Executing a Python Module from a ZIP Archive Using `zipimport` **Objective:** Demonstrate the ability to use the `zipimport` module and its `zipimporter` class to import and execute a Python module from a ZIP archive. Problem Statement You are provided with a ZIP archive named `modules.zip`, which contains a Python module `example.py`. The module `example.py` contains a single function `greet(name)` that takes a string `name` and returns a greeting message in the format `\\"Hello, <name>!\\"`. Your task is to write a function `load_and_execute(zip_path, module_name, function_name, *args)` that performs the following: 1. Uses the `zipimporter` class to load the specified module from the given ZIP archive. 2. Retrieves the specified function from the loaded module. 3. Executes the function with the provided arguments and returns the result. The function signature should be: ```python def load_and_execute(zip_path: str, module_name: str, function_name: str, *args) -> any: ``` Input - `zip_path` (str): The file path to the ZIP archive (e.g., `\\"modules.zip\\"`). - `module_name` (str): The name of the module to be imported (e.g., `\\"example\\"`). - `function_name` (str): The name of the function to be executed (e.g., `\\"greet\\"`). - `*args`: The arguments to be passed to the function when it is executed. Output - The result returned by executing the specified function from the module with the provided arguments. Constraints - The ZIP archive contains only valid Python files. - The specified module and function exist within the ZIP archive. Example ```python # Assuming modules.zip contains a module \'example\' with a function \'greet(name)\' result = load_and_execute(\\"modules.zip\\", \\"example\\", \\"greet\\", \\"Alice\\") print(result) # Output should be: \\"Hello, Alice!\\" ``` Requirements - Do not use `importlib` or directly extract files from the ZIP archive using file operations; utilize the `zipimporter` class as detailed in the documentation. - Handle exceptions related to module import and function execution appropriately. Additional Information - You are expected to familiarize yourself with the `zipimport` module and the `zipimporter` class methods. - Consider edge cases and error handling to ensure the robustness of your solution.","solution":"import zipimport def load_and_execute(zip_path: str, module_name: str, function_name: str, *args) -> any: Loads a module from a ZIP archive and executes a specified function from that module. Args: zip_path (str): The file path to the ZIP archive. module_name (str): The name of the module to be imported. function_name (str): The name of the function to be executed. *args: The arguments to be passed to the function. Returns: any: The result from executing the specified function from the module. try: importer = zipimport.zipimporter(zip_path) module = importer.load_module(module_name) function_to_execute = getattr(module, function_name) result = function_to_execute(*args) return result except Exception as e: print(f\\"An error occurred: {e}\\") return None"},{"question":"You are given a dataset containing text documents and their corresponding labels. Your task is to preprocess the data, implement multiple Naive Bayes classifiers provided by the scikit-learn package, and compare their classification performance. Dataset Description: - The dataset contains text documents with two columns: - `text`: the text of the document. - `label`: the label of the document. # Requirements: 1. **Data Preprocessing**: - Convert the text data into features suitable for Naive Bayes classifiers. - Implement both Count Vectorizer and TF-IDF Vectorizer as preprocessing steps. 2. **Model Implementation**: - Train and evaluate the following Naive Bayes classifiers: - GaussianNB - MultinomialNB - ComplementNB - BernoulliNB - Use the default parameters for each classifier. 3. **Evaluation Metrics**: - Print the accuracy, precision, recall, and F1 score for each classifier. 4. **Comparison and Analysis**: - Compare the different Naive Bayes classifiers in terms of their performance metrics. - Provide a brief discussion on which classifier performed best and why. Constraints: - Use scikit-learn version >= 0.22. - You are not allowed to use any external libraries except numpy and pandas for data manipulation, and scikit-learn for machine learning tasks. Function Signature: ```python def preprocess_and_compare_naive_bayes(data: pd.DataFrame) -> None: pass ``` Example Usage: ```python import pandas as pd # Sample data data = pd.DataFrame({ \'text\': [\\"I love programming.\\", \\"Python is great.\\", \\"The weather is nice today.\\", \\"I am learning machine learning.\\", \\"Naive Bayes is simple but effective.\\"], \'label\': [1, 1, 0, 1, 1] }) preprocess_and_compare_naive_bayes(data) ``` # Expected Output Format: - For each classifier (GaussianNB, MultinomialNB, ComplementNB, BernoulliNB): - Print the accuracy, precision, recall, and F1 score. - A brief comparative analysis: - Which classifier performed best and why? Use the following template for your solution: ```python import pandas as pd from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.preprocessing import LabelEncoder def preprocess_and_compare_naive_bayes(data: pd.DataFrame) -> None: # Encode the labels label_encoder = LabelEncoder() data[\'label\'] = label_encoder.fit_transform(data[\'label\']) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(data[\'text\'], data[\'label\'], test_size=0.3, random_state=42) # List of vectorizers to use vectorizers = [CountVectorizer(), TfidfVectorizer()] vectorizer_names = [\'Count Vectorizer\', \'TF-IDF Vectorizer\'] # List of classifiers to use classifiers = [GaussianNB(), MultinomialNB(), ComplementNB(), BernoulliNB()] classifier_names = [\'GaussianNB\', \'MultinomialNB\', \'ComplementNB\', \'BernoulliNB\'] for vec, vec_name in zip(vectorizers, vectorizer_names): print(f\\"Results using {vec_name}:n\\") # Transform the text data X_train_vec = vec.fit_transform(X_train) X_test_vec = vec.transform(X_test) # GaussianNB requires dense matrix if vec_name == \'Count Vectorizer\': X_train_vec_dense = X_train_vec.toarray() X_test_vec_dense = X_test_vec.toarray() else: X_train_vec_dense = X_train_vec.toarray() X_test_vec_dense = X_test_vec.toarray() for clf, clf_name in zip(classifiers, classifier_names): # GaussianNB expects dense matrix inputs if clf_name == \'GaussianNB\': X_train_clf = X_train_vec_dense X_test_clf = X_test_vec_dense else: X_train_clf = X_train_vec X_test_clf = X_test_vec # Train the classifier clf.fit(X_train_clf, y_train) # Predict using the trained model y_pred = clf.predict(X_test_clf) # Calculate performance metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') # Print the results print(f\\"{clf_name} results:\\") print(f\\"Accuracy: {accuracy:.4f}\\") print(f\\"Precision: {precision:.4f}\\") print(f\\"Recall: {recall:.4f}\\") print(f\\"F1 Score: {f1:.4f}\\") print() print(\\"=\\"*50) # Provide a brief analysis and comparison here print(\\"Analysis and Comparative Discussion:n\\") print(\\"Based on the above results, [provide the best performing classifier] performed the best. [Explain why based on precision, recall, and F1 scores].\\") ```","solution":"import pandas as pd from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.preprocessing import LabelEncoder def preprocess_and_compare_naive_bayes(data: pd.DataFrame) -> None: # Encode the labels label_encoder = LabelEncoder() data[\'label\'] = label_encoder.fit_transform(data[\'label\']) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(data[\'text\'], data[\'label\'], test_size=0.3, random_state=42) # List of vectorizers to use vectorizers = [CountVectorizer(), TfidfVectorizer()] vectorizer_names = [\'Count Vectorizer\', \'TF-IDF Vectorizer\'] # List of classifiers to use classifiers = [GaussianNB(), MultinomialNB(), ComplementNB(), BernoulliNB()] classifier_names = [\'GaussianNB\', \'MultinomialNB\', \'ComplementNB\', \'BernoulliNB\'] for vec, vec_name in zip(vectorizers, vectorizer_names): print(f\\"Results using {vec_name}:n\\") # Transform the text data X_train_vec = vec.fit_transform(X_train) X_test_vec = vec.transform(X_test) # GaussianNB requires dense matrix if vec_name == \'Count Vectorizer\': X_train_vec_dense = X_train_vec.toarray() X_test_vec_dense = X_test_vec.toarray() else: X_train_vec_dense = X_train_vec.toarray() X_test_vec_dense = X_test_vec.toarray() for clf, clf_name in zip(classifiers, classifier_names): # GaussianNB expects dense matrix inputs if clf_name == \'GaussianNB\': X_train_clf = X_train_vec_dense X_test_clf = X_test_vec_dense else: X_train_clf = X_train_vec X_test_clf = X_test_vec # Train the classifier clf.fit(X_train_clf, y_train) # Predict using the trained model y_pred = clf.predict(X_test_clf) # Calculate performance metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') # Print the results print(f\\"{clf_name} results:\\") print(f\\"Accuracy: {accuracy:.4f}\\") print(f\\"Precision: {precision:.4f}\\") print(f\\"Recall: {recall:.4f}\\") print(f\\"F1 Score: {f1:.4f}\\") print() print(\\"=\\"*50) # Provide a brief analysis and comparison here print(\\"Analysis and Comparative Discussion:n\\") print(\\"Based on the above results, MultinomialNB performed the best generally in terms of F1 score and precision. This can be attributed to its suitability in handling discrete count data commonly found in text classification problems.\\")"},{"question":"Objective Demonstrate your understanding of reference counting in Python by implementing a simple custom object manager that uses reference counting mechanisms. Task Implement a class `RefCountedObjectManager` in Python that simulates the reference counting behavior described in the provided documentation. The class should allow for creating and managing objects with explicit reference counting. Specifications 1. **Class `RefCountedObjectManager`** must implement the following methods: - `__init__(self, obj)`: Initialize the manager with an object and set the initial reference count. - `incref(self)`: Increment the reference count of the managed object. - `decref(self)`: Decrement the reference count of the managed object. If the reference count reaches zero, the managed object should be deleted (you can simulate deletion by setting the object to `None`). - `get_ref_count(self)`: Return the current reference count of the managed object. - `get_object(self)`: Return the managed object if the reference count is greater than zero; otherwise, return `None`. 2. **Constraints & Limitations**: - The initial reference count of an object should be 1 when an instance of `RefCountedObjectManager` is created. - You must manage the reference count as an internal attribute. - Ensure that reference count cannot drop below zero. 3. **Input/Output Formats**: - There are no specific inputs or outputs for the constructor and methods. You can create and test instances of this class directly. - Ensure that the `get_object(self)` behaves as described when reference count is zero or greater. Example ```python # Example usage of RefCountedObjectManager obj_mgr = RefCountedObjectManager(\\"test_object\\") print(obj_mgr.get_ref_count()) # Output: 1 print(obj_mgr.get_object()) # Output: \'test_object\' obj_mgr.incref() print(obj_mgr.get_ref_count()) # Output: 2 obj_mgr.decref() print(obj_mgr.get_ref_count()) # Output: 1 obj_mgr.decref() print(obj_mgr.get_object()) # Output: None print(obj_mgr.get_ref_count()) # Output: 0 ``` Performance Requirements - The solution should handle typical usage scenarios efficiently but does not need to be optimized for extreme cases. **Note:** The correct implementation will reflect a clear understanding of the reference counting mechanisms as described.","solution":"class RefCountedObjectManager: def __init__(self, obj): self._obj = obj self._ref_count = 1 def incref(self): self._ref_count += 1 def decref(self): if self._ref_count > 0: self._ref_count -= 1 if self._ref_count == 0: self._obj = None def get_ref_count(self): return self._ref_count def get_object(self): if self._ref_count > 0: return self._obj return None"},{"question":"Kernel Density Estimation and Mode Calculation Your task is to write a function that estimates the probability density function of a given dataset using Kernel Density Estimation and then identifies the mode of the estimated density. # Requirements 1. Implement the function `estimate_density_and_find_mode` which takes the following parameters: - `data` (2D numpy array): An (N times D) array representing (N) samples with (D) features. - `kernel` (string): The kernel to be used in the KDE. It must be one of \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', or \'cosine\'. - `bandwidth` (float): The bandwidth parameter for the KDE. 2. Compute the KDE for the provided data using the specified kernel and bandwidth. 3. Identify and return the mode of the estimated density for each feature. The mode is the value with the highest estimated density. # Function Signature ```python import numpy as np from sklearn.neighbors import KernelDensity def estimate_density_and_find_mode(data: np.ndarray, kernel: str, bandwidth: float) -> np.ndarray: Estimate the density of the data and find the mode for each feature. Parameters: data (np.ndarray): 2D array of shape (N, D) where N is the number of samples and D is the number of features. kernel (str): The kernel to be used in the KDE (\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', or \'cosine\'). bandwidth (float): The bandwidth for the KDE. Returns: np.ndarray: 1D array of length D, where element i is the mode of feature i. ``` # Constraints - The input data will be a 2D numpy array with `1 <= N <= 10^4` and `1 <= D <= 10`. - The `bandwidth` will be a positive float within the range `[0.01, 1.0]`. - The `kernel` parameter will be one of the strings \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', or \'cosine\'. # Example Usage ```python data = np.array([[1, 2], [2, 3], [3, 4], [5, 2], [1, 2]]) kernel = \'gaussian\' bandwidth = 0.5 modes = estimate_density_and_find_mode(data, kernel, bandwidth) print(modes) # Output should be an array with the mode of each feature ``` # Note - For the sake of simplicity, you may assume that the mode is the point with the highest density score among the given samples. If multiple points share the highest density score, return the first one found. # Evaluation Your solution will be evaluated based on: - Correctness: The function should compute the kernel density estimation correctly and identify the mode accurately. - Efficiency: The function should handle the constraints within a reasonable time. - Code quality: Your code should be readable, well-documented, and follow best coding practices.","solution":"import numpy as np from sklearn.neighbors import KernelDensity def estimate_density_and_find_mode(data: np.ndarray, kernel: str, bandwidth: float) -> np.ndarray: Estimate the density of the data and find the mode for each feature. Parameters: data (np.ndarray): 2D array of shape (N, D) where N is the number of samples and D is the number of features. kernel (str): The kernel to be used in the KDE (\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', or \'cosine\'). bandwidth (float): The bandwidth for the KDE. Returns: np.ndarray: 1D array of length D, where element i is the mode of feature i. N, D = data.shape modes = np.zeros(D) for i in range(D): feature_data = data[:, i].reshape(-1, 1) kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(feature_data) log_density_scores = kde.score_samples(feature_data) mode_index = np.argmax(log_density_scores) modes[i] = feature_data[mode_index] return modes"},{"question":"# Custom Pickling Strategy with the `copyreg` Module Problem Statement You are required to implement a class and use the `copyreg` module to register custom pickling behavior for instances of this class. The goal is to serialize and deserialize objects of this class correctly while applying custom logic during the pickling process. Class Specification Implement a class named `Employee` with the following attributes: - `name` (string): The name of the employee. - `position` (string): The position of the employee within the company. - `salary` (float): The annual salary of the employee. Method Specification 1. **`__init__(self, name, position, salary)`**: Initializes the Employee instance with the provided name, position, and salary. 2. **`__repr__(self)`**: Returns a string representation of the Employee instance in the format: `Employee(name=\'John Doe\', position=\'Developer\', salary=75000.0)`. In addition to the class, implement the following functions: 3. **`pickle_employee(employee)`**: This function will be registered as the reduction function for the `Employee` class. It should return the data necessary to reconstruct the `Employee` instance and print \\"Custom pickling for Employee instance\\". 4. **`unpickle_employee(name, position, salary)`**: This function should create and return an `Employee` instance using the provided arguments. Implementation Steps 1. Define the `Employee` class with the required constructor and `__repr__` method. 2. Define the `pickle_employee` function to serialize an `Employee` instance. 3. Define the `unpickle_employee` function to deserialize an `Employee` instance. 4. Register the `pickle_employee` function for the `Employee` class using `copyreg.pickle`. Input and Output Requirements - There are no input requirements for this task. The implementation will be tested by creating instances of the `Employee` class, serializing them using `pickle.dumps()`, and deserializing them using `pickle.loads()`. - The output should include the print statement \\"Custom pickling for Employee instance\\" during the pickling process. You should ensure the following: - The deserialized `Employee` object should be equivalent to the original one (i.e., they should have the same attributes with identical values). - The `__repr__` method should correctly represent the `Employee` object. # Example ```python import copyreg import pickle class Employee: def __init__(self, name, position, salary): self.name = name self.position = position self.salary = salary def __repr__(self): return f\'Employee(name={self.name!r}, position={self.position!r}, salary={self.salary!r})\' def pickle_employee(employee): print(\\"Custom pickling for Employee instance\\") return unpickle_employee, (employee.name, employee.position, employee.salary) def unpickle_employee(name, position, salary): return Employee(name, position, salary) # Register the custom pickling behavior copyreg.pickle(Employee, pickle_employee) # Example usage e1 = Employee(\'John Doe\', \'Developer\', 75000.0) serialized = pickle.dumps(e1) deserialized = pickle.loads(serialized) print(repr(deserialized)) # Employee(name=\'John Doe\', position=\'Developer\', salary=75000.0) assert isinstance(deserialized, Employee) assert deserialized.name == \'John Doe\' assert deserialized.position == \'Developer\' assert deserialized.salary == 75000.0 ```","solution":"import copyreg import pickle class Employee: def __init__(self, name, position, salary): self.name = name self.position = position self.salary = salary def __repr__(self): return f\'Employee(name={self.name!r}, position={self.position!r}, salary={self.salary!r})\' def pickle_employee(employee): print(\\"Custom pickling for Employee instance\\") return unpickle_employee, (employee.name, employee.position, employee.salary) def unpickle_employee(name, position, salary): return Employee(name, position, salary) # Register the custom pickling behavior copyreg.pickle(Employee, pickle_employee)"},{"question":"# Advanced Python Coding Assessment: Custom Generator Implementation Objective To assess your understanding of Python\'s generator objects and iterators, you will implement a custom generator that mimics some of the internal behaviors outlined in the documentation. Problem Statement Your task is to create a custom generator class in Python, `CustomGenerator`, that allows iteration over a sequence of numbers with some advanced state management features. Specifically, your generator will: 1. Allow initialization with a start, stop, and step value, akin to the `range()` function. 2. Implement the `__iter__()` and `__next__()` methods to comply with Python\'s iterator protocol. 3. Include a method `send(value)` that can receive a value and update the next value to be yielded based on the input. Class Specification ```python class CustomGenerator: def __init__(self, start, stop, step): Initialize the generator with start, stop, and step values. :param start: The starting value of the sequence (inclusive). :param stop: The stopping value of the sequence (exclusive). :param step: The step value to use for iteration. # Your implementation here def __iter__(self): Initialize the iterator. :return: The iterator object itself. # Your implementation here def __next__(self): Return the next value in the sequence. :return: The next value. :raises StopIteration: When the end of the sequence is reached. # Your implementation here def send(self, value): Send a value to replace the next value in the sequence. :param value: The value to set as the next value. :return: None # Your implementation here ``` Example Usage ```python gen = CustomGenerator(0, 10, 2) iterator = iter(gen) # Initialize the iterator print(next(iterator)) # Output: 0 print(next(iterator)) # Output: 2 gen.send(5) # Set the next value to be yielded to 5 print(next(iterator)) # Output: 5 print(next(iterator)) # Output: 8 ``` Constraints - `start`, `stop`, and `step` will always be integers. - `step` will never be zero. - The generator should adhere to Python\'s iterator protocol. Performance Requirements - Your implementation should be efficient with respect to both time and space complexity. - Iterations should be performed in constant time. # Submission Submit your `CustomGenerator` class implementation that meets the above specifications and passes the example usage.","solution":"class CustomGenerator: def __init__(self, start, stop, step): Initialize the generator with start, stop, and step values. :param start: The starting value of the sequence (inclusive). :param stop: The stopping value of the sequence (exclusive). :param step: The step value to use for iteration. self.start = start self.stop = stop self.step = step self.current = start self.sent_value = None self.is_sent_value_used = False def __iter__(self): Initialize the iterator. :return: The iterator object itself. return self def __next__(self): Return the next value in the sequence. :return: The next value. :raises StopIteration: When the end of the sequence is reached. if self.sent_value is not None and not self.is_sent_value_used: self.current = self.sent_value self.sent_value = None self.is_sent_value_used = True if self.current >= self.stop: raise StopIteration() result = self.current self.current += self.step return result def send(self, value): Send a value to replace the next value in the sequence. :param value: The value to set as the next value. :return: None self.sent_value = value self.is_sent_value_used = False"},{"question":"# Complex Data Analysis with Lists and Dictionaries in Python Objective: Create a function `analyze_data` that takes a list of dictionaries as input and returns an aggregated analysis based on specified criteria. Each dictionary in the input list represents a record containing information about various items. Your task is to extract, process, and summarize this information using various list operations and comprehensions. Input: - A list of dictionaries where each dictionary contains the following keys: - `category` (str): The category of the item. - `name` (str): The name of the item. - `value` (int): The value associated with the item. - `tags` (list): A list of tags related to the item. Example input: ```python data = [ {\\"category\\": \\"fruit\\", \\"name\\": \\"apple\\", \\"value\\": 50, \\"tags\\": [\\"red\\", \\"fresh\\"]}, {\\"category\\": \\"fruit\\", \\"name\\": \\"banana\\", \\"value\\": 30, \\"tags\\": [\\"yellow\\", \\"ripe\\"]}, {\\"category\\": \\"vegetable\\", \\"name\\": \\"carrot\\", \\"value\\": 20, \\"tags\\": [\\"orange\\", \\"fresh\\"]}, {\\"category\\": \\"fruit\\", \\"name\\": \\"orange\\", \\"value\\": 40, \\"tags\\": [\\"orange\\", \\"citrus\\"]}, {\\"category\\": \\"vegetable\\", \\"name\\": \\"broccoli\\", \\"value\\": 25, \\"tags\\": [\\"green\\", \\"fresh\\"]}, {\\"category\\": \\"fruit\\", \\"name\\": \\"berry\\", \\"value\\": 50, \\"tags\\": [\\"red\\", \\"fresh\\"]} ] ``` Output: - A dictionary with the following keys: - `total_value_per_category`: A dictionary where each key is a category and the value is the total of all item values for that category. - `unique_tags`: A set of all unique tags present in the input data. - `highest_value_item`: A dictionary containing the name and value of the item with the highest value. If there are multiple items with the highest value, any one of them can be returned. Example output for the above input: ```python { \\"total_value_per_category\\": {\\"fruit\\": 170, \\"vegetable\\": 45}, \\"unique_tags\\": {\\"red\\", \\"fresh\\", \\"yellow\\", \\"ripe\\", \\"orange\\", \\"citrus\\", \\"green\\"}, \\"highest_value_item\\": {\\"name\\": \\"apple\\", \\"value\\": 50} } ``` Constraints: - The input list will contain at least one dictionary. - Each dictionary will have valid and consistent data types for the keys mentioned above. - The function should handle the input efficiently and use list comprehensions where appropriate. Function Signature: ```python def analyze_data(data: List[Dict[str, Any]]) -> Dict[str, Any]: pass ``` Example Test Case: ```python test_data = [ {\\"category\\": \\"fruit\\", \\"name\\": \\"apple\\", \\"value\\": 50, \\"tags\\": [\\"red\\", \\"fresh\\"]}, {\\"category\\": \\"fruit\\", \\"name\\": \\"banana\\", \\"value\\": 30, \\"tags\\": [\\"yellow\\", \\"ripe\\"]}, {\\"category\\": \\"vegetable\\", \\"name\\": \\"carrot\\", \\"value\\": 20, \\"tags\\": [\\"orange\\", \\"fresh\\"]}, {\\"category\\": \\"fruit\\", \\"name\\": \\"orange\\", \\"value\\": 40, \\"tags\\": [\\"orange\\", \\"citrus\\"]}, {\\"category\\": \\"vegetable\\", \\"name\\": \\"broccoli\\", \\"value\\": 25, \\"tags\\": [\\"green\\", \\"fresh\\"]}, {\\"category\\": \\"fruit\\", \\"name\\": \\"berry\\", \\"value\\": 50, \\"tags\\": [\\"red\\", \\"fresh\\"]} ] expected_output = { \\"total_value_per_category\\": {\\"fruit\\": 170, \\"vegetable\\": 45}, \\"unique_tags\\": {\\"red\\", \\"fresh\\", \\"yellow\\", \\"ripe\\", \\"orange\\", \\"citrus\\", \\"green\\"}, \\"highest_value_item\\": {\\"name\\": \\"apple\\", \\"value\\": 50} } assert analyze_data(test_data) == expected_output ``` Notes: - Utilize efficient list operations and comprehensions to process the data. - Make sure to test the function with different data sets to verify its robustness.","solution":"from typing import List, Dict, Any def analyze_data(data: List[Dict[str, Any]]) -> Dict[str, Any]: total_value_per_category = {} unique_tags = set() highest_value_item = None for item in data: category, name, value, tags = item[\'category\'], item[\'name\'], item[\'value\'], item[\'tags\'] # Calculate total value per category if category in total_value_per_category: total_value_per_category[category] += value else: total_value_per_category[category] = value # Collect unique tags unique_tags.update(tags) # Find item with the highest value if highest_value_item is None or value > highest_value_item[\'value\']: highest_value_item = {\\"name\\": name, \\"value\\": value} return { \\"total_value_per_category\\": total_value_per_category, \\"unique_tags\\": unique_tags, \\"highest_value_item\\": highest_value_item }"},{"question":"# Question You are given a set of functions from the `sklearn.datasets` module that allow for downloading and loading various types of datasets. Your task is to implement a function `analyze_openml_dataset` that performs the following tasks: 1. Downloads a dataset from the OpenML repository using the `fetch_openml()` function. 2. Converts the dataset\'s features to a numerical format if they are not already. 3. Splits the dataset into training and testing sets using an 80-20 split. 4. Trains a basic machine learning classifier (e.g., a Decision Tree) on the training data. 5. Evaluates and returns the accuracy of the classifier on the testing data. Your implementation should consider the following: - The function should accept two arguments: 1. `dataset_name` (str): The name of the dataset to be downloaded from OpenML. 2. `version` (int): The version of the dataset to be downloaded from OpenML. - Handle any categorical data appropriately using `sklearn.preprocessing`. - Return the accuracy of the trained classifier on the testing data as a float. Function Signature ```python def analyze_openml_dataset(dataset_name: str, version: int) -> float: pass ``` # Example ```python accuracy = analyze_openml_dataset(\\"iris\\", 1) print(f\\"Accuracy of the classifier: {accuracy:.2f}\\") ``` # Constraints - Use the `fetch_openml()` function from `sklearn.datasets` to obtain the dataset. - Use any appropriate classifier, but ensure reproducibility by setting a random state if applicable. - Properly preprocess the data, especially if it includes categorical features. # Note: - Make sure to handle exceptions where the dataset or version might not be available or if there are issues with data preprocessing. - You may refer to the `sklearn.preprocessing` module for tools like `OneHotEncoder` or `OrdinalEncoder`.","solution":"from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score import pandas as pd def analyze_openml_dataset(dataset_name: str, version: int) -> float: try: # Fetching the dataset dataset = fetch_openml(name=dataset_name, version=version, as_frame=True) X, y = dataset.data, dataset.target # Identifying categorical and numerical columns categorical_cols = X.select_dtypes(include=[\'object\', \'category\']).columns numerical_cols = X.select_dtypes(include=[\'number\']).columns # Preprocessing pipelines for numerical and categorical data numerical_pipeline = Pipeline(steps=[ (\'scaler\', StandardScaler()) ]) categorical_pipeline = Pipeline(steps=[ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combining preprocessing steps using ColumnTransformer preprocessor = ColumnTransformer(transformers=[ (\'num\', numerical_pipeline, numerical_cols), (\'cat\', categorical_pipeline, categorical_cols) ]) # Splitting data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Creating and training the classifier pipeline classifier = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', DecisionTreeClassifier(random_state=42)) ]) classifier.fit(X_train, y_train) # Predicting and calculating accuracy y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy except Exception as e: print(f\\"An error occurred: {e}\\") return 0.0"},{"question":"# Advanced Asynchronous Chat Server with Custom Protocol You are required to implement an asynchronous chat server using the concepts from the `asyncore` module. The server should support multiple clients simultaneously and use a custom protocol to handle client messages. Requirements 1. **Server Implementation:** - Create a main server class `ChatServer` that listens for incoming client connections. - For each new connection, instantiate a `ChatHandler` that manages communication with that client. - The server should listen on a specified host and port. 2. **Client Handler:** - The `ChatHandler` should allow the client to send and receive messages. - Implement custom protocol handling: Each message sent by the client should be preceded by a header that specifies the message length. - Handle partial messages and assemble them correctly. 3. **Server Features:** - Broadcast messages received from one client to all connected clients (except the sender). - Implement appropriate error handling and socket closures. - Manage client connections and disconnections gracefully. Protocol Specification - **Message Format:** - A message consists of a 4-byte header that indicates the length of the message body, followed by the message body itself. - For instance, for a message \\"Hello\\", the header will be `0005` (indicating 5 bytes), and the complete message will be `0005Hello`. Constraints - Do not use threading or asyncio, stick to the `asyncore` module as described in the documentation. - Ensure proper handling of client disconnections and partial message reads/writes. Input Format - The address (host and port) where the server should listen, provided in the `ChatServer` initialization. Output - The server should output connection, disconnection logs, and broadcasted messages to the console. Example Usage ```python server = ChatServer(\'localhost\', 12345) asyncore.loop() ``` Implementation Hints - Use the `asyncore.dispatcher` for the main server and client connections. - Override methods like `handle_read`, `handle_write`, `handle_accept`, and `handle_close` to manage socket events. ```python import asyncore import struct class ChatHandler(asyncore.dispatcher_with_send): def __init__(self, sock, server): super().__init__(sock) self.server = server self.buffer = b\'\' self.msg_length = 0 def handle_read(self): data = self.recv(8192) if data: self.buffer += data while len(self.buffer) >= self.msg_length: if self.msg_length == 0 and len(self.buffer) >= 4: self.msg_length = struct.unpack(\'!I\', self.buffer[:4])[0] self.buffer = self.buffer[4:] if self.msg_length > 0 and len(self.buffer) >= self.msg_length: message = self.buffer[:self.msg_length] self.buffer = self.buffer[self.msg_length:] self.msg_length = 0 self.server.broadcast(self, message) def handle_close(self): self.close() self.server.remove_handler(self) class ChatServer(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket() self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.handlers = [] def handle_accept(self): sock, addr = self.accept() print(f\\"New connection from {addr}\\") handler = ChatHandler(sock, self) self.handlers.append(handler) def broadcast(self, sender, message): for handler in self.handlers: if handler != sender: handler.send(struct.pack(\'!I\', len(message)) + message) def remove_handler(self, handler): self.handlers.remove(handler) print(\\"Connection closed\\") # Example usage server = ChatServer(\'localhost\', 12345) asyncore.loop() ```","solution":"import asyncore import socket import struct class ChatHandler(asyncore.dispatcher_with_send): def __init__(self, sock, server): super().__init__(sock) self.server = server self.buffer = b\'\' self.msg_length = 0 def handle_read(self): data = self.recv(8192) if data: self.buffer += data while True: if self.msg_length == 0: if len(self.buffer) >= 4: self.msg_length = struct.unpack(\'!I\', self.buffer[:4])[0] self.buffer = self.buffer[4:] else: break if self.msg_length > 0: if len(self.buffer) >= self.msg_length: message = self.buffer[:self.msg_length] self.buffer = self.buffer[self.msg_length:] self.msg_length = 0 self.server.broadcast(self, message) else: break def handle_close(self): self.close() self.server.remove_handler(self) class ChatServer(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.handlers = [] def handle_accept(self): sock, addr = self.accept() if sock: print(f\\"New connection from {addr}\\") handler = ChatHandler(sock, self) self.handlers.append(handler) def broadcast(self, sender, message): print(f\\"Broadcasting message: {message.decode(\'utf-8\')}\\") for handler in self.handlers: if handler != sender: handler.send(struct.pack(\'!I\', len(message)) + message) def remove_handler(self, handler): self.handlers.remove(handler) print(\\"Connection closed\\") # Example usage if __name__ == \\"__main__\\": server = ChatServer(\'localhost\', 12345) asyncore.loop()"},{"question":"# Source Distribution Automation You are required to automate the process of creating a source distribution for a Python package. Your task involves writing a function to generate a `MANIFEST.in` file and using the `sdist` command to create the source distribution archive in the desired format(s). Task Implement a function `create_source_distribution` that performs the following tasks: 1. Generates a `MANIFEST.in` file based on specified inclusion and exclusion criteria. 2. Uses the `sdist` command to create a source distribution archive. Function Signature ```python def create_source_distribution(include_patterns: list, exclude_patterns: list, formats: str = \\"gztar\\"): Creates a source distribution for a Python package. Parameters: - include_patterns: List of strings, each string is a pattern of files to include (e.g., [\\"*.py\\", \\"*.txt\\"]). - exclude_patterns: List of strings, each string is a pattern of files to exclude (e.g., [\\"tests/*\\", \\"docs/*\\"]). - formats: String specifying the format(s) to use for the distribution archive (default is \\"gztar\\"). The function should: - Create a \'MANIFEST.in\' file with the specified include and exclude patterns. - Run the `sdist` command with the specified format(s) to create the source distribution. ``` Input - `include_patterns` (list of str): Patterns of files to include in the source distribution. - `exclude_patterns` (list of str): Patterns of files to exclude from the source distribution. - `formats` (str): A string specifying the archive format(s) (default is `\\"gztar\\"`). Output - The function does not return any value but should create the source distribution archive in the specified format(s). Constraints - Only valid patterns should be provided in `include_patterns` and `exclude_patterns`. - The function should handle file and directory patterns correctly. Example Suppose we have a project with the following structure: ``` my_project/ setup.py README.txt my_module.py tests/ test_my_module.py ``` And you call the function as follows: ```python create_source_distribution(include_patterns=[\\"*.py\\", \\"README.txt\\"], exclude_patterns=[\\"tests/*\\"], formats=\\"zip\\") ``` The `MANIFEST.in` file should be generated as: ``` include *.py include README.txt prune tests/* ``` And then the `sdist` command should generate a `.zip` archive containing `setup.py`, `README.txt`, and `my_module.py`. # Notes - Ensure that the generated `MANIFEST.in` file adheres to the formatting rules described in the documentation. - The function should handle paths and patterns in a platform-independent manner. - Use appropriate Python modules (e.g., `subprocess`, `os`, etc.) where necessary to implement the functionality.","solution":"import os import subprocess def create_source_distribution(include_patterns: list, exclude_patterns: list, formats: str = \\"gztar\\"): Creates a source distribution for a Python package. Parameters: - include_patterns: List of strings, each string is a pattern of files to include (e.g., [\\"*.py\\", \\"*.txt\\"]). - exclude_patterns: List of strings, each string is a pattern of files to exclude (e.g., [\\"tests/*\\", \\"docs/*\\"]). - formats: String specifying the format(s) to use for the distribution archive (default is \\"gztar\\"). The function creates a \'MANIFEST.in\' file with the specified include and exclude patterns and runs the `sdist` command with the specified format(s) to create the source distribution. # Generate the MANIFEST.in content based on the patterns manifest_content = [] for pattern in include_patterns: manifest_content.append(f\\"include {pattern}\\") for pattern in exclude_patterns: manifest_content.append(f\\"prune {pattern}\\") # Write the MANIFEST.in file with open(\\"MANIFEST.in\\", \\"w\\") as f: f.write(\\"n\\".join(manifest_content)) # Run the sdist command to create the source distribution subprocess.run([\\"python\\", \\"setup.py\\", \\"sdist\\", f\\"--formats={formats}\\"], check=True)"},{"question":"# Question: Advanced Seaborn Plot Customization Objective In this task, you will create a set of customized plots to demonstrate your understanding of various seaborn features including themes, styles, context, and customization options. Problem Statement You are given a dataset of daily temperatures recorded at different times of the day for a month. Your task is to visualize this data using seaborn and apply various customization techniques. Follow the steps below to complete the task: 1. **Load and Prepare Data**: - Create a DataFrame with 31 rows (representing days) and 4 columns (temperatures at `8 AM`, `12 PM`, `4 PM`, and `8 PM`). - Fill this DataFrame with random temperature values between 15 and 30 degrees Celsius. 2. **Default Seaborn Theme Plot**: - Create a line plot of the temperature data with the default seaborn theme. 3. **Custom Style Plot**: - Using the `whitegrid` style, plot a boxplot of the temperature data. 4. **Context Adjustment**: - Set the context to `talk` and create a violin plot of the temperature data. 5. **Spine Removal and Customization**: - Create a line plot with the `ticks` style. - Remove the top and right spines. - Offset the remaining spines by 10 units. 6. **Temporary Style within a Context**: - Create a 2x2 grid of subplots, each with a different style (`darkgrid`, `white`, `ticks`, `whitegrid`). - Use the `notebook` context with `font_scale=1.5` for all subplots. Constraints - Use seaborn for all plots. - Ensure plots are visually distinct and appropriately labeled. Expected Output Your solution should be a Python script that generates the required plots as specified, demonstrating appropriate use of seaborn customization features. Performance Requirements - The solution should execute efficiently for the given data size. Code Implementation ```python import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load and Prepare Data np.random.seed(42) data = pd.DataFrame({ \'Day\': np.arange(1, 32), \'8 AM\': np.random.uniform(15, 30, 31), \'12 PM\': np.random.uniform(15, 30, 31), \'4 PM\': np.random.uniform(15, 30, 31), \'8 PM\': np.random.uniform(15, 30, 31) }) # Step 2: Default Seaborn Theme Plot sns.set_theme() plt.figure(figsize=(10, 6)) sns.lineplot(data=data.drop(columns=[\'Day\'])) plt.title(\'Default Seaborn Theme - Line Plot\') plt.xlabel(\'Day\') plt.ylabel(\'Temperature (°C)\') plt.show() # Step 3: Custom Style Plot sns.set_style(\\"whitegrid\\") plt.figure(figsize=(10, 6)) sns.boxplot(data=data.drop(columns=[\'Day\'])) plt.title(\'Whitegrid Style - Box Plot\') plt.xlabel(\'Time of Day\') plt.ylabel(\'Temperature (°C)\') plt.show() # Step 4: Context Adjustment sns.set_context(\\"talk\\") plt.figure(figsize=(10, 6)) sns.violinplot(data=data.drop(columns=[\'Day\'])) plt.title(\'Talk Context - Violin Plot\') plt.xlabel(\'Time of Day\') plt.ylabel(\'Temperature (°C)\') plt.show() # Step 5: Spine Removal and Customization sns.set_style(\\"ticks\\") plt.figure(figsize=(10, 6)) sns.lineplot(data=data.drop(columns=[\'Day\'])) sns.despine(offset=10) plt.title(\'Ticks Style with Despine - Line Plot\') plt.xlabel(\'Day\') plt.ylabel(\'Temperature (°C)\') plt.show() # Step 6: Temporary Style within a Context sns.set_context(\\"notebook\\", font_scale=1.5) f, axes = plt.subplots(2, 2, figsize=(12, 12)) with sns.axes_style(\\"darkgrid\\"): sns.lineplot(data=data.drop(columns=[\'Day\']), ax=axes[0, 0]) axes[0, 0].set_title(\'Darkgrid Style\') with sns.axes_style(\\"white\\"): sns.lineplot(data=data.drop(columns=[\'Day\']), ax=axes[0, 1]) axes[0, 1].set_title(\'White Style\') with sns.axes_style(\\"ticks\\"): sns.lineplot(data=data.drop(columns=[\'Day\']), ax=axes[1, 0]) sns.despine(offset=10, ax=axes[1, 0]) axes[1, 0].set_title(\'Ticks Style with Despine\') with sns.axes_style(\\"whitegrid\\"): sns.lineplot(data=data.drop(columns=[\'Day\']), ax=axes[1, 1]) axes[1, 1].set_title(\'Whitegrid Style\') f.tight_layout() plt.show() ```","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load and Prepare Data np.random.seed(42) data = pd.DataFrame({ \'Day\': np.arange(1, 32), \'8 AM\': np.random.uniform(15, 30, 31), \'12 PM\': np.random.uniform(15, 30, 31), \'4 PM\': np.random.uniform(15, 30, 31), \'8 PM\': np.random.uniform(15, 30, 31) }) # Step 2: Default Seaborn Theme Plot sns.set_theme() plt.figure(figsize=(10, 6)) sns.lineplot(data=data.drop(columns=[\'Day\'])) plt.title(\'Default Seaborn Theme - Line Plot\') plt.xlabel(\'Day\') plt.ylabel(\'Temperature (°C)\') plt.show() # Step 3: Custom Style Plot sns.set_style(\\"whitegrid\\") plt.figure(figsize=(10, 6)) sns.boxplot(data=data.drop(columns=[\'Day\'])) plt.title(\'Whitegrid Style - Box Plot\') plt.xlabel(\'Time of Day\') plt.ylabel(\'Temperature (°C)\') plt.show() # Step 4: Context Adjustment sns.set_context(\\"talk\\") plt.figure(figsize=(10, 6)) sns.violinplot(data=data.drop(columns=[\'Day\'])) plt.title(\'Talk Context - Violin Plot\') plt.xlabel(\'Time of Day\') plt.ylabel(\'Temperature (°C)\') plt.show() # Step 5: Spine Removal and Customization sns.set_style(\\"ticks\\") plt.figure(figsize=(10, 6)) sns.lineplot(data=data.drop(columns=[\'Day\'])) sns.despine(offset=10) plt.title(\'Ticks Style with Despine - Line Plot\') plt.xlabel(\'Day\') plt.ylabel(\'Temperature (°C)\') plt.show() # Step 6: Temporary Style within a Context sns.set_context(\\"notebook\\", font_scale=1.5) f, axes = plt.subplots(2, 2, figsize=(12, 12)) with sns.axes_style(\\"darkgrid\\"): sns.lineplot(data=data.drop(columns=[\'Day\']), ax=axes[0, 0]) axes[0, 0].set_title(\'Darkgrid Style\') with sns.axes_style(\\"white\\"): sns.lineplot(data=data.drop(columns=[\'Day\']), ax=axes[0, 1]) axes[0, 1].set_title(\'White Style\') with sns.axes_style(\\"ticks\\"): sns.lineplot(data=data.drop(columns=[\'Day\']), ax=axes[1, 0]) sns.despine(offset=10, ax=axes[1, 0]) axes[1, 0].set_title(\'Ticks Style with Despine\') with sns.axes_style(\\"whitegrid\\"): sns.lineplot(data=data.drop(columns=[\'Day\']), ax=axes[1, 1]) axes[1, 1].set_title(\'Whitegrid Style\') f.tight_layout() plt.show()"},{"question":"**Objective:** To test your understanding of seaborn\'s `objects` API and your ability to create layered and clear visualizations by applying transformations to plot elements. **Question:** You are provided with two datasets, `penguins` and `diamonds`, from the seaborn library. Your task is to create a combined visualization that effectively presents data from both datasets. 1. Create a `Plot` object for `penguins` that: - Uses the `species` column for the x-axis and `body_mass_g` column for the y-axis. - Adds dots to represent individual data points, with jitter applied to prevent overlap. - Adds a range transformation to display the interquartile range (25th and 75th percentiles) with a slight shift on the x-axis. 2. Create a `Plot` object for `diamonds` that: - Uses the `carat` column for the x-axis and `clarity` column for the y-axis. - Adds dots to represent individual data points, with jitter applied to prevent overlap. - Adds a range transformation to display the interquartile range (25th and 75th percentiles) with a slight shift on the y-axis. **Expected Input:** No input arguments need to be provided; you will be working with the datasets directly. **Expected Output:** Two seaborn `Plot` objects should be displayed, representing the visualizations described above. **Constraints:** - Use the seaborn `objects` API. - Ensure the visualizations are clear and the transformations are applied correctly. ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets penguins = load_dataset(\\"penguins\\") diamonds = load_dataset(\\"diamonds\\") # Create the first plot for the penguins dataset penguins_plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) # Display the penguins plot penguins_plot.show() # Create the second plot for the diamonds dataset diamonds_plot = ( so.Plot(diamonds, x=\\"carat\\", y=\\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=0.25)) ) # Display the diamonds plot diamonds_plot.show() ``` To complete this task, ensure your code meets the above requirements and correctly applies the transformations and visualizations to the datasets provided.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load datasets penguins = load_dataset(\\"penguins\\") diamonds = load_dataset(\\"diamonds\\") # Create the first plot for the penguins dataset penguins_plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) # Display the penguins plot penguins_plot.show() # Create the second plot for the diamonds dataset diamonds_plot = ( so.Plot(diamonds, x=\\"carat\\", y=\\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=0.25)) ) # Display the diamonds plot diamonds_plot.show()"},{"question":"# **Question: Advanced Data Compression with LZMA** Given a list of text strings, you are required to write a Python function that: 1. Compresses each string using LZMA with a custom filter chain. 2. Decompresses the compressed data back to its original form. 3. Ensures the integrity of data through checks and properly handles exceptions. **Function Signature:** ```python def compress_decompress_texts(text_list: list) -> list: pass ``` # **Input:** - `text_list` (list): A list of text strings (each string can be of any length). # **Output:** - Returns a list where each element is a tuple containing the original text and the decompressed text (after compressing and then decompressing it): ```python [ (\\"original_text_1\\", \\"decompressed_text_1\\"), (\\"original_text_2\\", \\"decompressed_text_2\\"), ... ] ``` # **Constraints:** - The function should: - Use a custom filter chain with `FILTER_DELTA` and `FILTER_LZMA2`. - Handle compression errors using `lzma.LZMAError`. - Verify the integrity of data using `CHECK_CRC64`. # **Example Usage:** ```python text_list = [\\"Hello, world!\\", \\"Python programming.\\", \\"Advanced compression techniques.\\"] result = compress_decompress_texts(text_list) # Expected output structure: # The decompressed_text should match the original_text # [ # (\\"Hello, world!\\", \\"Hello, world!\\"), # (\\"Python programming.\\", \\"Python programming.\\"), # (\\"Advanced compression techniques.\\", \\"Advanced compression techniques.\\") # ] for original, decompressed in result: assert original == decompressed, \\"The decompressed text does not match the original\\" ``` # **Performance Requirements:** - The function should efficiently handle a list containing up to 1000 strings, where each string can be up to 10,000 characters long. # **Implementation Details:** - Utilize the `lzma.LZMACompressor` and `lzma.LZMADecompressor` classes for the compression and decompression processes. - Use a custom filter chain: each dictionary has the filter ID (`FILTER_DELTA` with a distance of 1 and `FILTER_LZMA2` with a preset of 6). - Ensure exception handling to catch and manage any `lzma.LZMAError` that may arise during processing. - Use `CHECK_CRC64` for integrity verification during compression. # **Task:** Implement the `compress_decompress_texts` function to fulfill the requirements and pass the given test cases.","solution":"import lzma def compress_decompress_texts(text_list): Compresses and decompresses each text in the text_list using LZMA compression with custom filters. Ensures data integrity with CHECK_CRC64. Returns a list of tuples, where each tuple contains the original and decompressed text. result = [] # Define LZMA custom filter chain filters = [ { \'id\': lzma.FILTER_DELTA, \'dist\': 1 }, { \'id\': lzma.FILTER_LZMA2, \'preset\': 6 | lzma.PRESET_DEFAULT } ] for text in text_list: try: # Compress the text compressed_data = lzma.compress(text.encode(\'utf-8\'), format=lzma.FORMAT_XZ, check=lzma.CHECK_CRC64, filters=filters) # Decompress the compressed data decompressed_data = lzma.decompress(compressed_data, format=lzma.FORMAT_XZ).decode(\'utf-8\') # Append the original and decompressed text to result list result.append((text, decompressed_data)) except lzma.LZMAError as e: print(f\\"An error occurred during LZMA compression/decompression: {e}\\") result.append((text, None)) return result"},{"question":"# Complex Signal Processing with PyTorch Fourier Transforms Objective: Implement a function using PyTorch\'s `torch.fft` module that processes a given one-dimensional complex signal. The function should perform the following steps: 1. Compute the one-dimensional Fast Fourier Transform (FFT) of the input signal. 2. Apply a low-pass filter in the frequency domain by setting all frequency components beyond a specified cutoff frequency to zero. 3. Compute the inverse Fast Fourier Transform (iFFT) to transform the filtered signal back to the time domain. 4. Use the helper function `fftshift` to shift the zero-frequency component to the center of the spectrum. Function Signature: ```python import torch def process_signal(signal: torch.Tensor, cutoff_freq: float) -> torch.Tensor: Process a complex signal using Fourier Transforms in PyTorch. Parameters: signal (torch.Tensor): A one-dimensional complex tensor representing the input signal. cutoff_freq (float): The cutoff frequency for the low-pass filter. Returns: torch.Tensor: The processed signal in the time domain. # Your implementation here # Example usage: # signal = torch.rand(1024, dtype=torch.cfloat) # cutoff_freq = 0.1 # processed_signal = process_signal(signal, cutoff_freq) ``` Input: - `signal`: A one-dimensional complex tensor of length `N` representing the input signal. - `cutoff_freq`: A float value representing the cutoff frequency for the low-pass filter (range 0 to 0.5, where 0.5 corresponds to the Nyquist frequency). Output: - A one-dimensional complex tensor of length `N` representing the processed signal in the time domain. Constraints and Limitations: - The input signal will always have a length of at least 8. - The `cutoff_freq` will always be a value between 0 and 0.5 (inclusive). Performance Requirements: - The implementation should make efficient use of PyTorch\'s FFT functions. - The solution should efficiently handle signals of length up to 1,048,576. Additional Information: - Use the PyTorch `torch.fft.fft`, `torch.fft.ifft`, and `torch.fft.fftshift` functions for the required transformations. Good luck, and have fun processing your signals!","solution":"import torch def process_signal(signal: torch.Tensor, cutoff_freq: float) -> torch.Tensor: Process a complex signal using Fourier Transforms in PyTorch. Parameters: signal (torch.Tensor): A one-dimensional complex tensor representing the input signal. cutoff_freq (float): The cutoff frequency for the low-pass filter. Returns: torch.Tensor: The processed signal in the time domain. # Compute the one-dimensional FFT of the input signal fft_signal = torch.fft.fft(signal) # Apply fftshift to move the zero frequency component to the center shifted_fft = torch.fft.fftshift(fft_signal) # Determine the index of the cutoff frequency n = len(signal) freq_cutoff_idx = int(cutoff_freq * n) # Zero out all frequency components beyond the cutoff frequency mask = torch.zeros(n, dtype=torch.bool) mid_point = n // 2 mask[mid_point - freq_cutoff_idx:mid_point + freq_cutoff_idx] = 1 shifted_fft_filtered = shifted_fft * mask # Apply inverse fftshift to move the zero frequency component back filtered_fft = torch.fft.ifftshift(shifted_fft_filtered) # Compute the inverse FFT to get the filtered signal in the time domain processed_signal = torch.fft.ifft(filtered_fft) return processed_signal"},{"question":"# Custom Interactive Interpreter You are tasked with creating a custom Python interactive interpreter using the \\"code\\" and \\"codeop\\" modules. The custom interpreter should support reading Python commands from the user, compiling them using the facilities provided in the \\"codeop\\" module, and executing the commands in a custom interactive console. # Requirements: 1. **Implement a class `CustomInterpreter`** which should: - Initialize an interactive console using the `code.InteractiveConsole`. - Support a `run` method that enters an interactive loop, reading input from the user, compiling it using `codeop`, and executing it in the interactive console. - Be able to handle multi-line Python code correctly. 2. **Input and Output**: - For simplicity, assume the input will be provided line-by-line as strings, and the output should be directly printed to the console. - Interrupts (like `KeyboardInterrupt`) should be handled gracefully with a user-friendly message. # Constraints: 1. The interpreter should only allow valid Python syntax and should report syntax errors properly. 2. Use the `codeop.Compile` to manage potential incomplete code input. 3. The `run` method should keep reading user inputs until an explicit \'exit\' command is given. # Sample Usage: ```python interpreter = CustomInterpreter() interpreter.run() ``` When run, this should allow the user to input Python commands interactively, process them correctly, and print results as a standard Python interactive interpreter would. # Performance: No specific performance constraints, but your solution should efficiently handle typical use cases for an interactive interpreter. --- # Example: ``` >>> 2 + 2 4 >>> for i in range(3): ... print(i) ... 0 1 2 >>> exit ``` # Hints: - Look into `code.InteractiveConsole.push` to process each line of input. - `codeop.Compile` can take a string and a mode (\'exec\', \'eval\', or \'single\') and decide if the code is complete or needs more input. Implement this in a class `CustomInterpreter` that provides the functionalities as described above.","solution":"import code import codeop class CustomInterpreter: def __init__(self): self.console = code.InteractiveConsole() def run(self): buffer = \\"\\" while True: try: if buffer: prompt = \\"... \\" else: prompt = \\">>> \\" line = input(prompt) if line == \\"exit\\": print(\\"Exiting Custom Interpreter.\\") break buffer += line + \'n\' # Check if the input is complete if codeop.compile_command(buffer): self.console.push(buffer) buffer = \\"\\" except KeyboardInterrupt: print(\\"KeyboardInterrupt detected. Exiting Custom Interpreter.\\") break except EOFError: print(\\"EOF detected. Exiting Custom Interpreter.\\") break"},{"question":"**Objective:** Implement a TorchScript annotated class and function to demonstrate your understanding of PyTorch\'s TorchScript subset. This task involves tensor operations, type annotations, handling optional types, and utilizing TorchScript classes. **Problem Statement:** You need to implement a TorchScript-annotated class `MatrixOperations` with a specific method `multiplicative_increase` that performs operations on tensors. Additionally, you will implement another TorchScript function `process_matrices` that processes a list of matrices using the `MatrixOperations` class. 1. Define a TorchScript class `MatrixOperations`: - **Constructor**: Initializes the class with a scalar `factor` (float) which will be used to multiply elements in tensors. - **Method** `multiplicative_increase`: - **Input**: - `matrix` (Tensor): A 2D tensor. - `optional_scalar` (Optional[float]): An optional scalar value. If provided, elements of the tensor should also be increased by this scalar. - **Output**: - A tensor where each element is first multiplied by the `factor` and then increased by the `optional_scalar` if it is not `None`. 2. Define a TorchScript function `process_matrices`: - **Input**: - `matrices` (List[Tensor]): A list of 2D tensors. - `factors` (List[float]): A list of scalar factors, one for each matrix. - `optional_scalar` (Optional[float]): An optional scalar value. If provided, elements of the tensors should also be increased by this scalar. - **Output**: - A list of processed tensors where each tensor is processed using an instance of `MatrixOperations` created with the corresponding factor from `factors`. **Constraints:** - Each tensor in `matrices` is of the same shape. - `factors` list is the same length as `matrices`. **Expected Input/Output** - `MatrixOperations` class ```python @torch.jit.script class MatrixOperations: def __init__(self, factor: float): self.factor = factor def multiplicative_increase(self, matrix: torch.Tensor, optional_scalar: Optional[float]) -> torch.Tensor: # Implement logic here pass ``` - `process_matrices` function ```python @torch.jit.script def process_matrices(matrices: List[torch.Tensor], factors: List[float], optional_scalar: Optional[float]) -> List[torch.Tensor]: # Implement logic here pass ``` *Example Usage:* ```python import torch # Example matrices matrices = [torch.tensor([[1.0, 2.0], [3.0, 4.0]]), torch.tensor([[5.0, 6.0], [7.0, 8.0]])] factors = [2.0, 3.0] optional_scalar = 1.0 # Process matrices result = process_matrices(matrices, factors, optional_scalar) # Expected output: [(tensor([[ 3., 5.], [ 7., 9.]]), tensor([[16., 19.], [22., 25.]]))] ``` **Note:** * You need to ensure type annotations are appropriately used. * Handle the `optional_scalar` properly within the `multiplicative_increase` method. * Ensure your solution runs efficiently with the provided tensor operations.","solution":"import torch from typing import Optional, List @torch.jit.script class MatrixOperations: def __init__(self, factor: float): self.factor = factor def multiplicative_increase(self, matrix: torch.Tensor, optional_scalar: Optional[float]) -> torch.Tensor: result = matrix * self.factor if optional_scalar is not None: result += optional_scalar return result @torch.jit.script def process_matrices(matrices: List[torch.Tensor], factors: List[float], optional_scalar: Optional[float]) -> List[torch.Tensor]: processed_tensors = [] for i in range(len(matrices)): matrix_operations = MatrixOperations(factors[i]) processed_tensors.append(matrix_operations.multiplicative_increase(matrices[i], optional_scalar)) return processed_tensors"},{"question":"In this assessment, you will demonstrate your understanding of the `chunk` module in Python. You will be provided with a file containing structured data divided into chunks. Your task is to write a function that reads and processes all chunks from the file, extracting essential information and returning it in a structured format. # Objective - Implement the function `extract_chunk_info(file_path: str) -> List[Dict[str, Any]]`. # Function Signature ```python from typing import List, Dict, Any def extract_chunk_info(file_path: str) -> List[Dict[str, Any]]: pass ``` # Parameters and Return Format - **Input**: - `file_path`: A string representing the path to the file to be read. - **Output**: - A list of dictionaries, each containing information about a chunk. Each dictionary should have the following keys: - `\\"name\\"`: The chunk ID (string). - `\\"size\\"`: The size of the chunk data (integer). - `\\"data\\"`: The chunk data (bytes). # Constraints - You must use the `chunk` module to read and process the chunks. - Handle both aligned and non-aligned chunks. - Handle both big-endian and little-endian byte orders. - Handle files with multiple chunks. - Ensure the function works correctly for files using the EA IFF 85 chunk format. # Example Usage ```python # Given an input binary file \'example.aiff\' containing chunked data result = extract_chunk_info(\\"example.aiff\\") for chunk_info in result: print(f\\"Chunk ID: {chunk_info[\'name\']}\\") print(f\\"Chunk Size: {chunk_info[\'size\']}\\") print(f\\"Data (first 10 bytes): {chunk_info[\'data\'][:10]}\\") ``` # Notes - Pay attention to alignment and byte order specified in the chunk headers. - Remember to handle EOFError appropriately to stop reading when the end of the file is reached. - Consider edge cases, such as chunks with minimal data and file errors. This problem will test your understanding of file I/O, byte-order handling, and your ability to utilize the provided module to interact with a specific file format.","solution":"from typing import List, Dict, Any import chunk def extract_chunk_info(file_path: str) -> List[Dict[str, Any]]: chunk_infos = [] try: with open(file_path, \'rb\') as f: while True: try: ch = chunk.Chunk(f, bigendian=False, align=True) chunk_info = { \\"name\\": ch.getname().decode(\'ascii\'), \\"size\\": ch.getsize(), \\"data\\": ch.read() } chunk_infos.append(chunk_info) except EOFError: break except FileNotFoundError: print(\\"File not found.\\") return chunk_infos"},{"question":"# Question: Implementing a Dimensionality Reduction Pipeline using Locally Linear Embedding (LLE) Locally Linear Embedding (LLE) is a method for non-linear dimensionality reduction. It seeks a lower-dimensional projection of the data that preserves distances within local neighborhoods. Your task is to implement a function `lle_pipeline` that performs the following: 1. **Data Standardization**: Standardize the input data. 2. **Apply Locally Linear Embedding (LLE)**: - Use the class `LocallyLinearEmbedding` from `sklearn.manifold`. - Reduce the input dataset to the specified number of dimensions. 3. **Evaluate Reconstruction Error**: Compute the reconstruction error to evaluate how well the lower-dimensional representation captures the original data structure. The function signature should be: ```python def lle_pipeline(data: np.ndarray, n_neighbors: int, n_components: int) -> Tuple[np.ndarray, float]: Perform Locally Linear Embedding (LLE) on the given dataset. Parameters: - data (np.ndarray): The input dataset of shape (n_samples, n_features). - n_neighbors (int): The number of neighbors to consider for each point. - n_components (int): The number of dimensions to reduce the data to. Returns: - Tuple[np.ndarray, float]: - Transformed data of shape (n_samples, n_components). - The reconstruction error of the embedding. ``` # Constraints: - The number of neighbors (`n_neighbors`) should be greater than the number of components (`n_components`). - The input dataset (`data`) could be potentially large, so the implementation should be efficient in terms of memory usage. # Input: - `data`: A `numpy` array of shape `(n_samples, n_features)` representing the input data to be transformed. - `n_neighbors`: An integer representing the number of neighbors to consider for each data point in LLE. - `n_components`: An integer representing the number of dimensions to reduce the data to. # Output: - Transformed `numpy` array of shape `(n_samples, n_components)` representing the LLE transformed data. - A float representing the reconstruction error of the embedding. # Example: ```python import numpy as np # Example dataset data = np.array([[0.1, 0.2], [0.4, 0.2], [0.1, 0.5], [0.3, 0.6]]) # Parameters: n_neighbors=2, n_components=1 transformed_data, reconstruction_error = lle_pipeline(data, 2, 1) print(\\"Transformed Data:n\\", transformed_data) print(\\"Reconstruction Error:\\", reconstruction_error) ``` Your implementation should focus on correctly standardizing the input, applying LLE, and efficiently computing the reconstruction error. # Notes: - Refer to scikit-learn\'s documentation on `LocallyLinearEmbedding` and `StandardScaler` for guidance on using these classes. - Ensure to handle cases where the number of neighbors or components is inappropriate by raising meaningful errors.","solution":"import numpy as np from sklearn.manifold import LocallyLinearEmbedding from sklearn.preprocessing import StandardScaler def lle_pipeline(data: np.ndarray, n_neighbors: int, n_components: int) -> tuple: Perform Locally Linear Embedding (LLE) on the given dataset. Parameters: - data (np.ndarray): The input dataset of shape (n_samples, n_features). - n_neighbors (int): The number of neighbors to consider for each point. - n_components (int): The number of dimensions to reduce the data to. Returns: - tuple: - Transformed data of shape (n_samples, n_components). - The reconstruction error of the embedding. if n_neighbors <= n_components: raise ValueError(\\"The number of neighbors must be greater than the number of components.\\") # Standardize the data scaler = StandardScaler() data_std = scaler.fit_transform(data) # Apply Locally Linear Embedding lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components) transformed_data = lle.fit_transform(data_std) # Compute the reconstruction error reconstruction_error = lle.reconstruction_error_ return transformed_data, reconstruction_error"},{"question":"**Mailcap Configuration Task** You have been provided with a module that is designed to handle Mailcap files, which configure how MIME-aware applications react to files with different MIME types. # Problem Statement You are tasked to create a simplified version of the `mailcap.getcaps()` and `mailcap.findmatch()` functions described in the provided documentation. Additionally, you will implement a function that reads a custom mailcap file and returns the corresponding dictionary. Your tasks are: 1. **Implement the `parse_mailcap_file()` function:** - This function reads a custom `mailcap` format file and returns a dictionary mapping MIME types to a list of mailcap entries. - Example mailcap file content: ``` text/plain; cat %s video/mpeg; xmpeg %s ``` 2. **Implement the `find_match(caps, MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[])` function:** - This function searches for the specified MIME type in the dictionary returned by `parse_mailcap_file()` and returns a 2-tuple with the command line and the mailcap entry. - Implement parameter substitution as described in the documentation using both `%s` and `%{name}` formats. # Input and Output - **parse_mailcap_file(filepath)** - *Input*: A string representing the path to the mailcap file. - *Output*: A dictionary with MIME types as keys and lists of mailcap entries as values. - **find_match(caps, MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[])** - *Input*: - `caps`: A dictionary mapping MIME types to mailcap file entries. - `MIMEtype`: A string representing the MIME type to search for. - `key`: The type of activity to perform (default is \'view\'). - `filename`: A string to substitute `%s` in the command line. - `plist`: A list of named parameters in the format `name=value`. - *Output*: A 2-tuple with the command line and mailcap entry, or (None, None) if no matches are found. # Example ```python def parse_mailcap_file(filepath): # Your code goes here def find_match(caps, MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[]): # Your code goes here # Example usage: caps = parse_mailcap_file(\'test_mailcap.txt\') print(find_match(caps, \'video/mpeg\', filename=\'tmp1223\')) # Output: (\'xmpeg tmp1223\', {\'view\': \'xmpeg %s\'}) ``` # Constraints - Assume the mailcap file format is well-formed. - Pay special attention to security when handling shell metacharacters as described in the documentation. **Note**: Do not use the deprecated `mailcap` module functions directly; implement your own parsing and matching logic.","solution":"def parse_mailcap_file(filepath): Reads a custom mailcap file and returns a dictionary mapping MIME types to a list of mailcap entries. caps = {} with open(filepath, \'r\') as file: for line in file: line = line.strip() if not line or line.startswith(\'#\'): continue # skip empty lines and comments parts = [part.strip() for part in line.split(\';\')] if len(parts) < 2: continue # skip improperly formatted lines MIMEtype, view = parts[0], parts[1] if MIMEtype not in caps: caps[MIMEtype] = [] caps[MIMEtype].append({\'view\': view}) return caps def find_match(caps, MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[]): Searches for the specified MIME type in the dictionary and returns a 2-tuple with the command line and the mailcap entry. if MIMEtype not in caps: return None, None for entry in caps[MIMEtype]: if key in entry: command = entry[key] # substitute %s with the filename command = command.replace(\'%s\', filename) # substitute %{name} with the corresponding value from plist for item in plist: name, sep, value = item.partition(\'=\') placeholder = \'%{\' + name + \'}\' command = command.replace(placeholder, value) return command, entry return None, None"},{"question":"# Email Message Manipulation # Problem Statement: You are provided a class named `email.message.Message` which represents an email message. The task is to implement a function `create_email_message` that constructs and returns a properly formatted email message object with specific headers, a text payload, and an attachment. # Requirements: 1. **Function Interface:** ```python def create_email_message(subject: str, sender: str, recipient: str, body: str, attachment: bytes, attachment_name: str) -> email.message.Message: pass ``` 2. **Parameters:** - `subject`: A string representing the subject of the email. - `sender`: A string representing the sender\'s email address. - `recipient`: A string representing the recipient\'s email address. - `body`: A string containing the text body of the email. - `attachment`: A bytes object representing the content of the attachment. - `attachment_name`: A string representing the name of the attachment file. 3. **Constraints:** - The attachment should be added as an inline base64-encoded attachment. - Ensure that the email contains appropriate MIME type headers. - The email should be in multipart format if there is an attachment. 4. **Output:** The function should return an `email.message.Message` object representing the constructed email. # Example: ```python # Example usage: from email.message import Message from email.utils import formataddr def create_email_message(subject, sender, recipient, body, attachment, attachment_name): msg = Message() msg.set_unixfrom(\'author\') msg[\'From\'] = formataddr((\'Author Name\', sender)) msg[\'To\'] = recipient msg[\'Subject\'] = subject msg.set_type(\'multipart/mixed\') # Attach the body text body_part = Message() body_part.set_payload(body) body_part.set_type(\'text/plain\') # Attach the attachment attachment_part = Message() attachment_part.set_payload(attachment) attachment_part.add_header(\'Content-Disposition\', \'attachment\', filename=attachment_name) attachment_part.set_type(\'application/octet-stream\') msg.attach(body_part) msg.attach(attachment_part) return msg subject = \\"Test Email\\" sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" body = \\"This is the body of the email.\\" attachment = b\\"Attachment content here\\" attachment_name = \\"attachment.txt\\" email_msg = create_email_message(subject, sender, recipient, body, attachment, attachment_name) print(email_msg.as_string()) ``` # Instructions: 1. Implement the function `create_email_message` according to the given specifications. 2. Use the `email.message.Message` class and its methods to achieve the desired email format. 3. Make sure to test the function with various inputs to ensure it handles different cases correctly. # Assumptions: - The attachment is always provided and is a non-empty bytes object. - The email addresses and other strings are correctly formatted. # Evaluation Criteria: - Correctness: The function creates the email message with the specified headers and payload. - Use of the `email.message.Message` class methods to manage headers and payloads. - Handling multipart email structure correctly. - Proper MIME type setting and encoding of the attachment.","solution":"from email.message import EmailMessage import base64 import mimetypes def create_email_message(subject: str, sender: str, recipient: str, body: str, attachment: bytes, attachment_name: str) -> EmailMessage: msg = EmailMessage() msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject # Set email to be multipart, this will allow us to attach multiple parts msg.set_content(body) # Guess the MIME type of the attachment mime_type, _ = mimetypes.guess_type(attachment_name) if mime_type is None: mime_type = \'application/octet-stream\' # Add the attachment to the email msg.add_attachment(attachment, maintype=mime_type.split(\'/\')[0], subtype=mime_type.split(\'/\')[1], filename=attachment_name) return msg"},{"question":"<|Analysis Begin|> The \\"difflib\\" module in Python provides a variety of tools for comparing sequences, particularly useful for comparing texts or lines in files. It includes classes like `SequenceMatcher`, `Differ`, and `HtmlDiff` for different types of comparisons and outputs. - `SequenceMatcher` is a flexible class that compares pairs of sequences and identifies matching subsequences. It can be customized to exclude specific \\"junk\\" elements. - `Differ` compares sequences of lines of text and highlights differences in a human-readable format. - `HtmlDiff` generates HTML tables that display side-by-side comparisons of text with differences highlighted. The module also includes functions for generating context diffs, unified diffs, and finding close matches in a list of strings. These tools can be particularly useful for tasks like automating code reviews, displaying changes in documents, or other applications where textual differences are important. Given the capabilities of the \\"difflib\\" module, a challenging question can focus on implementing a function that leverages these tools to produce meaningful comparative outputs. This could involve using `SequenceMatcher` and `HtmlDiff` to create an HTML report that highlights differences between multiple files. <|Analysis End|> <|Question Begin|> Create a function `compare_files_to_html_report(file_paths, output_html_path)` that takes a list of file paths and an output HTML file path. The function should compare the contents of the files and generate an HTML report highlighting the differences between each pair of files. The HTML report should display the differences side by side in a table format, using the `HtmlDiff` class from the \\"difflib\\" module. # Function Signature ```python def compare_files_to_html_report(file_paths: list, output_html_path: str) -> None: pass ``` # Input - `file_paths`: A list of strings, where each string is a file path to a text file to compare. - `output_html_path`: A string specifying the path where the resulting HTML report should be saved. # Output - The function does not return anything. It writes an HTML file to the specified path with the differences between the files. # Constraints - There will be at least two files in `file_paths`. - Each file in `file_paths` is a text file and can contain multiple lines. - The function should handle cases where the files might have different numbers of lines. - Assume all file paths are valid and the files are readable. Example ```python file_paths = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] output_html_path = \\"comparison_report.html\\" compare_files_to_html_report(file_paths, output_html_path) ``` The function should use the `difflib.HtmlDiff` class to generate an HTML report that compares each pair of files in `file_paths`, highlighting the differences line by line. The resulting HTML report should be saved at `output_html_path`. # Notes - Ensure that the HTML report is well-formatted and can be easily viewed in a web browser. - You can include additional information in the HTML report, such as file names or a timestamp of when the report was generated. - The function should make use of the capabilities of the \\"difflib\\" module to produce a clear and useful comparative report.","solution":"import difflib import datetime def compare_files_to_html_report(file_paths: list, output_html_path: str) -> None: Compares the contents of files specified in file_paths and generates an HTML report highlighting the differences between each pair of files. The report is saved to output_html_path. Parameters: file_paths (list): List of file paths to text files to compare. output_html_path (str): Path to the output HTML file for the report. html_diff = difflib.HtmlDiff() # Read the contents of each file file_contents = [open(file_path).readlines() for file_path in file_paths] html_report = f\\"<html><head><title>Comparison Report</title></head><body>\\" html_report += f\\"<h1>File Comparison Report</h1>\\" html_report += f\\"<p>Generated on {datetime.datetime.now()}</p>\\" # Compare each pair of files num_files = len(file_contents) for i in range(num_files): for j in range(i + 1, num_files): file_name1 = file_paths[i] file_name2 = file_paths[j] diff_html = html_diff.make_file(file_contents[i], file_contents[j], fromdesc=file_name1, todesc=file_name2) html_report += f\\"<h2>Comparison: {file_name1} vs {file_name2}</h2>\\" html_report += diff_html html_report += \\"</body></html>\\" # Write the HTML report to the output file with open(output_html_path, \'w\') as output_file: output_file.write(html_report)"},{"question":"Implement a classification pipeline from scratch using Scikit-learn\'s `SGDClassifier` to classify the Iris dataset into its three species classes using stochastic gradient descent. You will need to perform data preprocessing, model training, and validation. Additionally, implement hyperparameter tuning to select the optimal learning rate and regularization term using cross-validation. Requirements 1. **Data Preprocessing**: - Load the Iris dataset using `sklearn.datasets.load_iris`. - Standardize the features using `sklearn.preprocessing.StandardScaler`. 2. **Model Training**: - Train an `SGDClassifier` model. - Use a \\"one versus all\\" approach for multi-class classification. - Implement cross-validation to select the best hyperparameters for `learning_rate` and `alpha` (regularization strength). 3. **Hyperparameter Tuning**: - Use `sklearn.model_selection.GridSearchCV` to find the best `learning_rate` and `alpha` values. - Evaluate the model performance using cross-validated accuracy. 4. **Model Evaluation**: - Display the best hyperparameters and the corresponding cross-validated accuracy. - Plot the decision boundaries of the final trained model. Input and Output Format - **Input**: No explicit input parameters. The solution should include hardcoded steps to load the dataset, preprocess it, and train the model. - **Output**: Print the best hyperparameters and the corresponding cross-validated accuracy. Display a plot of the decision boundaries. Constraints - Use Scikit-learn\'s `SGDClassifier`. - Use standard scaling for feature preprocessing. - Use cross-validation for hyperparameter tuning. - The hyperparameter grid should include at least three different values for both `learning_rate` and `alpha`. Performance Requirements - The solution should efficiently handle the Iris dataset (150 samples). - Use efficient Scikit-learn functions for all steps. Example Below is an example outline of a potential solution. Please complete it with necessary implementations. ```python import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import SGDClassifier from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV, train_test_split from sklearn.pipeline import make_pipeline # 1. Load data data = load_iris() X, y = data.data, data.target # 2. Standardize features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # 3. Define the SGDClassifier model sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3) # 4. Implement GridSearchCV for hyperparameter tuning param_grid = { \'alpha\': [1e-4, 1e-3, 1e-2], \'learning_rate\': [\'optimal\', \'adaptive\'] } grid_search = GridSearchCV(sgd_clf, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_scaled, y) # 5. Output the best parameters and accuracy print(\\"Best parameters:\\", grid_search.best_params_) print(\\"Best cross-validated accuracy:\\", grid_search.best_score_) # 6. Plot decision boundaries (example for 2D case, adjust for all features or PCA for 2D visualization) # Complete the plot section as needed, ensure it visualizes decision boundaries. ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import SGDClassifier from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV, train_test_split from sklearn.pipeline import make_pipeline from sklearn.decomposition import PCA def iris_classification_pipeline(): # 1. Load data data = load_iris() X, y = data.data, data.target # 2. Standardize features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # 3. Define the SGDClassifier model sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3) # 4. Implement GridSearchCV for hyperparameter tuning param_grid = { \'alpha\': [1e-4, 1e-3, 1e-2], \'learning_rate\': [\'optimal\', \'adaptive\'] } grid_search = GridSearchCV(sgd_clf, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_scaled, y) # 5. Output the best parameters and accuracy best_params = grid_search.best_params_ best_score = grid_search.best_score_ print(\\"Best parameters:\\", best_params) print(\\"Best cross-validated accuracy:\\", best_score) # 6. Plot decision boundaries using PCA for 2D visualization pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) sgd_clf_best = grid_search.best_estimator_ sgd_clf_best.fit(X_pca, y) x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1 y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02)) Z = sgd_clf_best.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor=\'k\', s=20) plt.xlabel(\'PCA 1\') plt.ylabel(\'PCA 2\') plt.title(\\"Decision boundaries after PCA\\") plt.show() return best_params, best_score"},{"question":"# UUID Creation and Comparison Objective: Create functions to generate and compare UUIDs using various methods provided by the `uuid` module. Demonstrate your understanding by implementing the following functionalities: 1. A function to generate a UUID using `uuid1()`, `uuid3()`, `uuid4()`, and `uuid5()` methods. 2. A function to compare UUIDs. 3. A function to verify attributes of the UUID. Requirements: 1. Implement a function `generate_uuids` that returns a dictionary with the following keys and their respective UUID values: ```python def generate_uuids(name: str) -> dict: pass ``` - `uuid1`: Generated using the `uuid1()` method. - `uuid3`: Generated using the `uuid3(uuid.NAMESPACE_DNS, name)` method. - `uuid4`: Generated using the `uuid4()` method. - `uuid5`: Generated using the `uuid5(uuid.NAMESPACE_DNS, name)` method. **Input**: - `name` (str): The name string used for generating UUIDs with `uuid3` and `uuid5` methods. **Output**: - A dictionary containing different versions of UUIDs. 2. Implement a function `compare_uuids` that compares two UUID objects. ```python def compare_uuids(uuid1: uuid.UUID, uuid2: uuid.UUID) -> bool: pass ``` - Return `True` if the UUIDs are the same, otherwise `False`. **Input**: - `uuid1` (uuid.UUID): The first UUID object. - `uuid2` (uuid.UUID): The second UUID object. **Output**: - `True` if UUIDs are the same, otherwise `False`. 3. Implement a function `verify_uuid_attributes` that verifies attributes of a given UUID. ```python def verify_uuid_attributes(uuid_val: uuid.UUID) -> dict: pass ``` - Return a dictionary containing the `hex`, `int`, `urn`, `variant`, `version`, and `is_safe` attributes of the UUID. **Input**: - `uuid_val` (uuid.UUID): The UUID object whose attributes need to be verified. **Output**: - A dictionary containing the specified attributes of the UUID. Constraints: - Do not use any external libraries apart from the `uuid` module. - Ensure that no two UUIDs generated by different methods are equal. - For `uuid3` and `uuid5`, use `uuid.NAMESPACE_DNS` as the namespace identifier. - Handle any necessary exception cases appropriately. Example: ```python import uuid # Example function usage result = generate_uuids(\'python.org\') print(result) uuid1 = result[\'uuid1\'] uuid4 = result[\'uuid4\'] # Compare UUIDs print(compare_uuids(uuid1, uuid4)) # Expected output: False # Verify UUID attributes attributes = verify_uuid_attributes(uuid1) print(attributes) ``` Ensure your solution passes various test cases, verifying its correctness and robustness.","solution":"import uuid def generate_uuids(name: str) -> dict: Generate UUIDs using different methods: uuid1, uuid3, uuid4, uuid5. Args: name (str): The name string used for generating UUIDs with uuid3 and uuid5. Returns: dict: A dictionary containing different versions of UUIDs. return { \'uuid1\': uuid.uuid1(), \'uuid3\': uuid.uuid3(uuid.NAMESPACE_DNS, name), \'uuid4\': uuid.uuid4(), \'uuid5\': uuid.uuid5(uuid.NAMESPACE_DNS, name), } def compare_uuids(uuid1: uuid.UUID, uuid2: uuid.UUID) -> bool: Compare two UUIDs. Args: uuid1 (uuid.UUID): The first UUID object. uuid2 (uuid.UUID): The second UUID object. Returns: bool: True if the UUIDs are the same, otherwise False. return uuid1 == uuid2 def verify_uuid_attributes(uuid_val: uuid.UUID) -> dict: Verify attributes of a UUID. Args: uuid_val (uuid.UUID): The UUID object whose attributes need to be verified. Returns: dict: A dictionary containing the specified attributes of the UUID. return { \'hex\': uuid_val.hex, \'int\': uuid_val.int, \'urn\': uuid_val.urn, \'variant\': uuid_val.variant, \'version\': uuid_val.version, \'is_safe\': uuid_val.is_safe }"},{"question":"**Objective:** Your task is to demonstrate your understanding of the `email.encoders` module by creating a function that leverages these encoders appropriately. You will implement a function that takes an email message object, analyzes its payload, and encodes it using the most appropriate encoding method based on the characteristics of the payload. **Function Signature:** ```python def auto_encode_email_message(msg: Message) -> None: ``` **Expected Input:** - `msg` (Message): An instance of the `email.message.Message` class. The message will be non-multipart (either text or binary data). **Expected Output:** - The function should not return anything. Instead, it should modify the `msg` object in place, setting its payload to the encoded value and setting the appropriate `Content-Transfer-Encoding` header. **Constraints and Limitations:** 1. The input message will be non-multipart. 2. The payload can be either mostly printable data with a few unprintable characters or mostly unprintable data. 3. Use the characteristics of the payload to decide which encoding method to use: - If the payload includes non-printable characters but is mostly printable, use `encode_quopri`. - If the payload is largely non-printable, use `encode_base64`. - If the payload is entirely 7-bit ASCII characters, use `encode_7or8bit`. - For any other case or if in doubt, do not alter the payload and use `encode_noop`. **Performance Requirements:** The function should determine the most suitable encoding in linear time relative to the length of the payload. **Examples:** ```python from email.message import Message import email.encoders def auto_encode_email_message(msg: Message) -> None: payload = msg.get_payload() try: # Attempt to decode the payload as ASCII to determine if it is 7-bit payload.encode(\'ascii\') # If successful, use 7 or 8 bit encoding email.encoders.encode_7or8bit(msg) except UnicodeEncodeError: # If there are non-ASCII characters, check if they are few (quoted-printable) unprintable_chars = sum(1 for c in payload if not c.isprintable()) if unprintable_chars < len(payload) // 10: email.encoders.encode_quopri(msg) else: email.encoders.encode_base64(msg) return except TypeError: # Default to noop in any ambiguous case email.encoders.encode_noop(msg) # Example usage: msg = Message() msg.set_payload(\\"Hello there! This is an email message with non-printable characters: x80x81\\") auto_encode_email_message(msg) print(msg[\\"Content-Transfer-Encoding\\"]) # Should be either \\"quoted-printable\\" or \\"base64\\" ``` Ensure your function is accurate and efficient, properly setting the `Content-Transfer-Encoding` header and modifying the payload as specified.","solution":"from email.message import Message import email.encoders def auto_encode_email_message(msg: Message) -> None: payload = msg.get_payload() # Attempt to encode payload as ASCII to determine if it is 7-bit try: payload.encode(\'ascii\') # If successful, use 7or8bit encoding email.encoders.encode_7or8bit(msg) return except UnicodeEncodeError: pass # Count non-printable characters unprintable_chars = sum(1 for c in payload if not c.isprintable()) # Determine the encoding based on the count of non-printable characters if unprintable_chars < len(payload) // 10: email.encoders.encode_quopri(msg) else: email.encoders.encode_base64(msg)"},{"question":"# Path Manipulation and Validation **Objective**: Implement a function that processes and validates file paths according to specified rules. **Function Signature**: `def process_paths(paths: list[str], start_path: str) -> dict[str, any]:` **Input**: - `paths`: A list of strings representing file paths that need to be processed. - `start_path`: A string representing the start path for relative path computations. **Output**: - A dictionary with the following keys: - `\'full_paths\'`: A list of absolute paths for each input path. - `\'basenames\'`: A list of basenames for each input path. - `\'common_path\'`: The longest common sub-path among all the input paths. - `\'relative_paths\'`: A list of relative paths from `start_path` to each input path. - `\'valid_paths\'`: A list of boolean values indicating whether each path exists. **Constraints**: - The input paths can be absolute or relative. - Assume that all paths provided are in a format supported by the host operating system. **Example**: ```python paths = [\\"./a.txt\\", \\"../b.txt\\", \\"/home/user/c.txt\\", \\"/tmp/d/e.txt\\"] start_path = \\"/home/user/project\\" result = process_paths(paths, start_path) assert result == { \'full_paths\': [\'/current/dir/a.txt\', \'/current/b.txt\', \'/home/user/c.txt\', \'/tmp/d/e.txt\'], \'basenames\': [\'a.txt\', \'b.txt\', \'c.txt\', \'e.txt\'], \'common_path\': \'/\', \'relative_paths\': [\'../../current/dir/a.txt\', \'../../../current/b.txt\', \'../c.txt\', \'../../../tmp/d/e.txt\'], \'valid_paths\': [True, False, True, True] } ``` # Evaluation Criteria - Correctness: The function should correctly process the paths and generate the expected output. - Efficiency: The function should handle typical path lists efficiently. - Clarity: The function should be well-structured, using appropriate `os.path` methods to accomplish the tasks. # Notes - Use the `os.path` methods wherever applicable to achieve the functionalities. - Assume that the current working directory for relative paths is provided appropriately in the examples.","solution":"import os def process_paths(paths: list[str], start_path: str) -> dict[str, any]: full_paths = [os.path.abspath(path) for path in paths] basenames = [os.path.basename(path) for path in paths] common_path = os.path.commonpath(full_paths) relative_paths = [os.path.relpath(path, start_path) for path in full_paths] valid_paths = [os.path.exists(path) for path in full_paths] return { \'full_paths\': full_paths, \'basenames\': basenames, \'common_path\': common_path, \'relative_paths\': relative_paths, \'valid_paths\': valid_paths }"},{"question":"# Custom Deep and Shallow Copy in Python **Objective:** Implement a class in Python that demonstrates the use of shallow and deep copy operations. Specifically, the class should be able to: 1. Perform a shallow copy of its instances leveraging the `__copy__` method. 2. Perform a deep copy of its instances leveraging the `__deepcopy__` method. **Class Requirements:** 1. **Class Definition**: - Create a class `CustomObject` that has two attributes: * `data`: A list of integers. * `nested`:A dictionary where keys are strings and values are lists of integers. 2. **Initialization**: - The class should initialize the `data` and `nested` attributes. 3. **Copy Implementation**: - Implement the `__copy__` method to create a shallow copy of the instance. - Implement the `__deepcopy__` method to create a deep copy of the instance. Use the `memo` dictionary to prevent issues with recursive objects. 4. **Demonstration**: - Create an instance of `CustomObject`. - Create both a shallow and a deep copy of this instance. - Modify the original instance (both `data` and `nested` attributes) to demonstrate the difference in behavior between the shallow and deep copies. # Input: * An instance of `CustomObject`. # Output: * Two copies of the instance: one shallow and one deep. * Demonstrative print statements showing the differences in behavior when the original instance is modified. # Constraints: * The `data` list will contain at most 10 integer elements. * The `nested` dictionary will contain at most 5 keys, each with a list containing at most 5 integer elements. # Performance Requirements: * The implementation should efficiently manage the copying of potentially large and nested structures without causing excessive memory use or recursion errors. # Example: ```python class CustomObject: def __init__(self, data, nested): self.data = data self.nested = nested def __copy__(self): # Implement shallow copy logic pass def __deepcopy__(self, memo): # Implement deep copy logic pass # Example usage: original = CustomObject([1, 2, 3], {\'a\': [4, 5], \'b\': [6, 7]}) import copy shallow_copy = copy.copy(original) deep_copy = copy.deepcopy(original) # Modify the original instance original.data[0] = 10 original.nested[\'a\'][0] = 20 print(\\"Original:\\", original.data, original.nested) print(\\"Shallow Copy:\\", shallow_copy.data, shallow_copy.nested) print(\\"Deep Copy:\\", deep_copy.data, deep_copy.nested) ``` **Expected Output:** ``` Original: [10, 2, 3] {\'a\': [20, 5], \'b\': [6, 7]} Shallow Copy: [10, 2, 3] {\'a\': [20, 5], \'b\': [6, 7]} Deep Copy: [1, 2, 3] {\'a\': [4, 5], \'b\': [6, 7]} ``` **Note:** The shallow copy should reflect changes to mutable objects (lists within the original), whereas the deep copy should remain unaffected by these changes.","solution":"import copy class CustomObject: def __init__(self, data, nested): self.data = data self.nested = nested def __copy__(self): # Create a shallow copy return CustomObject(self.data, self.nested) def __deepcopy__(self, memo): # Create a deep copy new_data = copy.deepcopy(self.data, memo) new_nested = copy.deepcopy(self.nested, memo) return CustomObject(new_data, new_nested) # Example usage: original = CustomObject([1, 2, 3], {\'a\': [4, 5], \'b\': [6, 7]}) # Performing copy operations shallow_copy = copy.copy(original) deep_copy = copy.deepcopy(original) # Modify the original instance original.data[0] = 10 original.nested[\'a\'][0] = 20 print(\\"Original:\\", original.data, original.nested) print(\\"Shallow Copy:\\", shallow_copy.data, shallow_copy.nested) print(\\"Deep Copy:\\", deep_copy.data, deep_copy.nested)"},{"question":"You have been tasked with creating a custom command-line application for managing a simple inventory system. The application should allow users to add, remove, list, and search for items in the inventory via command-line commands. Your task is to implement a subclass of `cmd.Cmd` that provides these functionalities. Requirements 1. **Class Definition**: - Create a class named `InventoryCmd` that inherits from `cmd.Cmd`. 2. **Commands**: - `do_add` - Add an item to the inventory. Format: `add <item_name> <quantity>`. - `do_remove` - Remove an item from the inventory. Format: `remove <item_name>`. - `do_list` - List all items in the inventory. No arguments. - `do_search` - Search for an item in the inventory by name. Format: `search <item_name>`. - `do_quit` - Exit the command loop. No arguments. 3. **Command Details**: - `do_add(self, arg)`: Adds an item to the inventory along with its quantity. If the item already exists, it should update the quantity. - `do_remove(self, arg)`: Removes an item from the inventory if it exists. - `do_list(self, arg)`: Lists all items and their quantities in the inventory. - `do_search(self, arg)`: Searches for an item by name and prints its name and quantity if found, otherwise prints a message saying the item isn\'t in the inventory. - `do_quit(self, arg)`: Exits the command loop. 4. **Storage**: - Use a dictionary to store inventory items and their quantities where the keys are item names and the values are quantities. 5. **Command Line Interaction**: - The prompt should be `\'(inventory) \'`. - The application should display an introductory message when it starts. Input and Output Formats - Input for commands: - `add item_name quantity` - `remove item_name` - `list` - `search item_name` - `quit` - Output for commands: - `add`: Confirmation message or appropriate error message. - `remove`: Confirmation message or appropriate error message. - `list`: List of all items and their quantities. - `search`: Item details or a message saying the item is not found. - `quit`: Confirmation of quitting. Constraints - Item names can include alphabetic characters and spaces but no special characters. - Quantities are positive integers. Implementation ```python import cmd class InventoryCmd(cmd.Cmd): intro = \'Welcome to the inventory system. Type help or ? to list commands.n\' prompt = \'(inventory) \' inventory = {} def do_add(self, arg): \'Add an item to the inventory: add item_name quantity\' args = arg.split() if len(args) != 2 or not args[1].isdigit(): print(\'Usage: add item_name quantity\') return item_name, quantity = args[0], int(args[1]) if item_name in self.inventory: self.inventory[item_name] += quantity else: self.inventory[item_name] = quantity print(f\'Added {quantity} of {item_name} to inventory.\') def do_remove(self, arg): \'Remove an item from the inventory: remove item_name\' if arg not in self.inventory: print(f\'{arg} not in inventory.\') return del self.inventory[arg] print(f\'Removed {arg} from inventory.\') def do_list(self, arg): \'List all items in the inventory: list\' if not self.inventory: print(\'Inventory is empty.\') return for item, quantity in self.inventory.items(): print(f\'{item}: {quantity}\') def do_search(self, arg): \'Search for an item in the inventory: search item_name\' if arg in self.inventory: print(f\'{arg}: {self.inventory[arg]}\') else: print(f\'{arg} not found in inventory.\') def do_quit(self, arg): \'Quit the inventory system: quit\' print(\'Exiting inventory system.\') return True if __name__ == \'__main__\': InventoryCmd().cmdloop() ``` Implement this class to create a functional command-line application for inventory management. Test your application to ensure it meets all requirements specified above.","solution":"import cmd class InventoryCmd(cmd.Cmd): intro = \'Welcome to the inventory system. Type help or ? to list commands.n\' prompt = \'(inventory) \' inventory = {} def do_add(self, arg): \'Add an item to the inventory: add item_name quantity\' args = arg.split(maxsplit=1) if len(args) != 2 or not args[1].isdigit(): print(\'Usage: add item_name quantity\') return item_name, quantity = args[0], int(args[1]) if item_name in self.inventory: self.inventory[item_name] += quantity else: self.inventory[item_name] = quantity print(f\'Added {quantity} of {item_name} to inventory.\') def do_remove(self, arg): \'Remove an item from the inventory: remove item_name\' if arg not in self.inventory: print(f\'{arg} not in inventory.\') return del self.inventory[arg] print(f\'Removed {arg} from inventory.\') def do_list(self, arg): \'List all items in the inventory: list\' if not self.inventory: print(\'Inventory is empty.\') return for item, quantity in self.inventory.items(): print(f\'{item}: {quantity}\') def do_search(self, arg): \'Search for an item in the inventory: search item_name\' if arg in self.inventory: print(f\'{arg}: {self.inventory[arg]}\') else: print(f\'{arg} not found in inventory.\') def do_quit(self, arg): \'Quit the inventory system: quit\' print(\'Exiting inventory system.\') return True if __name__ == \'__main__\': InventoryCmd().cmdloop()"},{"question":"# Question: Advanced Faceting with Seaborn **Objective:** You are required to create a visual representation of the `penguins` dataset using the seaborn library. You need to demonstrate your understanding of faceting by creating a complex grid of plots that reveals different aspects of the dataset. **Task:** 1. Load the `penguins` dataset using seaborn. 2. Create a `seaborn.objects.Plot` object plotting `bill_length_mm` against `bill_depth_mm`. 3. Add dots to the plot. 4. Facet the plot by `species`, arranging the subplots in a 2x2 grid. 5. Label each subplot title with the format \\"Species: {species_level}\\" where `{species_level}` is the actual level of the `species` variable. 6. Share the y-axis scales of the facets but not the x-axis scales. 7. Save the final faceted plot as an image named `faceted_penguins.png`. **Input:** - No direct input from the user is required. Use the `penguins` dataset provided by seaborn. **Output:** - The output should be a faceted plot saved as `faceted_penguins.png`. The plot should include subplots faceted by `species`, with titles formatted as specified, y-axis scales shared, and x-axis scales independent. **Constraints:** - Use only the seaborn library for data visualization. - Ensure that the plot is clean and properly labeled as specified. **Performance Requirements:** - The code should run efficiently and handle the size of the `penguins` dataset within reasonable time limits. ```python # Your code starts here import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Initialize the plot p = so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\").add(so.Dots()) # Facet the plot by \'species\' with a 2x2 grid layout p = p.facet(\\"species\\", wrap=2).share(y=True).label(title=\\"Species: {}\\") # Save the plot as an image p.save(\\"faceted_penguins.png\\") ``` Ensure your code is complete and adheres to the specifications before submission. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Initialize the plot p = so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\").add(so.Dots()) # Define a function to customize facet titles def facet_title(facet_value): return f\\"Species: {facet_value}\\" # Facet the plot by \'species\' with a 2x2 grid layout p = p.facet(\\"species\\", wrap=2).share(y=True).label(facet=facet_title) # Save the plot as an image p.save(\\"faceted_penguins.png\\")"},{"question":"**Coding Assessment Question: Advanced Seaborn Plot Customization** You are given a dataset containing information about the attributes of diamonds, including their `carat`, `price`, and `cut`. Your task is to create a multi-faceted plot using the Seaborn `Plot` object and integrate it with Matplotlib to add custom annotations and visual effects. # Question You need to perform the following steps: 1. Load the diamonds dataset using `seaborn.load_dataset(\\"diamonds\\")`. 2. Create a `Plot` object to visualize the relationship between `carat` and `price`. 3. Use a `FacetGrid` to create separate plots for each type of `cut`. 4. Add a custom annotation to one of the subplots using Matplotlib functions. 5. Modify the plot aesthetics to align with a specific theme. 6. Utilize subfigures for organizing the subplots in a constrained layout. # Implementation Steps 1. **Load Dataset:** ```python import seaborn as sns diamonds = sns.load_dataset(\\"diamonds\\") ``` 2. **Create Plot:** - Initialize a `Plot` object for scatter plotting `carat` vs `price`. 3. **FacetGrid:** - Generate a facet grid based on the `cut`. 4. **Custom Annotation:** - Add a custom rectangle and text annotation to the subplot corresponding to one of the `cut` categories. 5. **Theme Customization:** - Apply a Seaborn theme for plot aesthetic enhancements. 6. **Use Subfigures:** - Organize the subplot and custom annotated plot using subfigures with a constrained layout. # Expected Input and Output - **Input:** - You do not need to handle external inputs; the dataset is loaded within the code. - **Output:** - A Matplotlib figure with customized subplots as described. # Provided Template ```python import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt # Step 1: Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Step 2: Initialize the Plot object p = so.Plot(diamonds, x=\'carat\', y=\'price\') # Step 3: Generate a facet grid based on the \'cut\' attribute facet_plot = p.add(so.Dots()).facet(row=\'cut\') # Step 4: Create a subfigure layout f = mpl.figure.Figure(figsize=(10, 5), dpi=100, layout=\'constrained\') sf1, sf2 = f.subfigures(1, 2) # Step 5: Plot the facet grid on one subfigure facet_plot.on(sf1).plot() # Step 6: Add custom annotation to a specific facet res = p.on(sf2).plot() ax = sf2.axes[0] rect = mpl.patches.Rectangle((0, 1), width=.4, height=.1, color=\'blue\', alpha=.2, transform=ax.transAxes, clip_on=False) ax.add_artist(rect) ax.text(x=rect.get_width() / 2, y=1 + rect.get_height() / 2, s=\\"Custom Annotation\\", size=12, ha=\'center\', va=\'center\', transform=ax.transAxes) # Display the final plot plt.show() ``` Use the above template as a guide to structure your code. Ensure you adhere to the specified steps and achieve the required output.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt def create_custom_diamonds_plot(): # Step 1: Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Step 2: Initialize the Plot object p = so.Plot(diamonds, x=\'carat\', y=\'price\') # Step 3: Generate a facet grid based on the \'cut\' attribute facet_plot = p.add(so.Dots()).facet(row=\'cut\') # Step 4: Create a subfigure layout f = mpl.figure.Figure(figsize=(10, 5), dpi=100, layout=\'constrained\') sf1, sf2 = f.subfigures(1, 2) # Step 5: Plot the facet grid on one subfigure facet_plot.on(sf1).plot() # Step 6: Add custom annotation to a specific facet on another Plot res = p.on(sf2).plot() ax = sf2.axes[0] rect = mpl.patches.Rectangle((0, 1), width=.4, height=.1, color=\'blue\', alpha=.2, transform=ax.transAxes, clip_on=False) ax.add_artist(rect) ax.text(x=rect.get_width() / 2, y=1 + rect.get_height() / 2, s=\\"Custom Annotation\\", size=12, ha=\'center\', va=\'center\', transform=ax.transAxes) # Show the plot plt.show()"},{"question":"# Question: Implementing a Custom Sequence Type In this assignment, you will demonstrate your understanding of Python\'s type system and object-oriented programming principles by creating a custom sequence type, similar to a list, but with specific constraints. Problem Statement You need to implement a custom sequence type named `ConstrainedList`. This list will have the following behaviors: - It should only allow integers as its elements. - It should support the common sequence operations (`append`, `getitem`, `setitem`, `delitem`, `len`, and iteration). - It should raise a `ValueError` if a non-integer item is added to the list. Specifications 1. **Class Name**: `ConstrainedList`. 2. **Methods**: - `__init__(self)`: Initializes an empty list. - `append(self, item)`: Adds an item to the list. Raise `ValueError` if item is not an integer. - `__getitem__(self, index)`: Returns the item at the specified index. - `__setitem__(self, index, item)`: Sets the item at the specified index. Raise `ValueError` if item is not an integer. - `__delitem__(self, index)`: Deletes the item at the specified index. - `__len__(self)`: Returns the number of items in the list. - `__iter__(self)`: Returns an iterator for the list. Implementation Constraints and Notes: - Do not use any external libraries; rely only on Python standard library functions. - Use list data structure internally for managing the elements. - Ensure your code is efficient and handle edge cases properly. Example Usage: ```python cl = ConstrainedList() cl.append(1) cl.append(2) print(cl[0]) # Output: 1 cl[1] = 3 print(cl[1]) # Output: 3 del cl[0] print(len(cl)) # Output: 1 # Trying to append a non-integer value should raise an error try: cl.append(\'a\') # Should raise ValueError except ValueError as e: print(e) # Output: Only integers are allowed # Iteration cl.append(4) for item in cl: print(item) # Output: 3 4 ``` Implement the `ConstrainedList` class following the specifications above.","solution":"class ConstrainedList: def __init__(self): self._list = [] def append(self, item): if not isinstance(item, int): raise ValueError(\\"Only integers are allowed\\") self._list.append(item) def __getitem__(self, index): return self._list[index] def __setitem__(self, index, item): if not isinstance(item, int): raise ValueError(\\"Only integers are allowed\\") self._list[index] = item def __delitem__(self, index): del self._list[index] def __len__(self): return len(self._list) def __iter__(self): return iter(self._list)"},{"question":"**Question: Implement a Cloning Utility with Custom Classes** You are required to demonstrate your understanding of shallow and deep copy operations by implementing a copying utility for a custom class that includes nested objects and handles recursive references. # Requirements: 1. **Custom Classes**: - Implement a class `Node` that represents a node in a graph. Each node contains: - `value`: an integer value representing the node\'s value. - `children`: a list of child nodes. - Ensure the class defines both `__copy__()` and `__deepcopy__()` methods. 2. **Copy Functions**: - Implement the function `clone_shallow(node: Node) -> Node` that returns a shallow copy of the given `Node`. - Implement the function `clone_deep(node: Node) -> Node` that returns a deep copy of the given `Node`. This should be able to handle recursive references. # Input: - A `Node` object with potential nested `Node` objects and possible recursive references. # Output: - For `clone_shallow(node: Node)`: Return a shallow copy of the input `Node`. - For `clone_deep(node: Node)`: Return a deep copy of the input `Node`. # Constraints: - You may assume that the input `Node` objects do not cause a stack overflow due to extremely deep recursion. - The maximum number of children per node is 10. - The value of each node is a non-negative integer. # Example: ```python # Example for defining the Node class, shallow copy, and deep copy. # Define the Node class class Node: def __init__(self, value): self.value = value self.children = [] def __copy__(self): new_node = Node(self.value) new_node.children = self.children.copy() return new_node def __deepcopy__(self, memo): if self in memo: return memo[self] new_node = Node(self.value) memo[self] = new_node for child in self.children: new_node.children.append(copy.deepcopy(child, memo)) return new_node # Shallow copy function def clone_shallow(node: Node) -> Node: return copy.copy(node) # Deep copy function def clone_deep(node: Node) -> Node: return copy.deepcopy(node) # Example usage import copy # Create nodes node1 = Node(1) node2 = Node(2) node3 = Node(3) # Setup relationships (node1 -> node2 -> node3, node3 -> node1 to make a loop) node1.children.append(node2) node2.children.append(node3) node3.children.append(node1) # Introduce cycle # Perform shallow and deep copies shallow_copied_node1 = clone_shallow(node1) deep_copied_node1 = clone_deep(node1) # Assertions assert shallow_copied_node1 is not node1 assert shallow_copied_node1.children is node1.children assert deep_copied_node1 is not node1 assert deep_copied_node1.children is not node1.children assert deep_copied_node1.children[0] is not node2 assert deep_copied_node1.children[0].children[0] is not node3 assert deep_copied_node1.children[0].children[0].children[0] is deep_copied_node1 # Cycle preserved ``` # Grading Criteria: - Correct implementation of shallow and deep copy operations. - Handling of recursive references in deep copy appropriately using memoization. - Proper demonstration of custom `__copy__()` and `__deepcopy__()` methods within the `Node` class.","solution":"from copy import copy, deepcopy class Node: def __init__(self, value): self.value = value self.children = [] def __copy__(self): Creates a shallow copy of the node. new_node = Node(self.value) new_node.children = self.children return new_node def __deepcopy__(self, memo): Creates a deep copy of the node using memoization to handle recursive references. if self in memo: return memo[self] new_node = Node(self.value) memo[self] = new_node new_node.children = [deepcopy(child, memo) for child in self.children] return new_node def clone_shallow(node: Node) -> Node: Returns a shallow copy of the given Node. return copy(node) def clone_deep(node: Node) -> Node: Returns a deep copy of the given Node. return deepcopy(node)"},{"question":"Analyzing Soccer Player Statistics using Pandas **Objective:** To assess your ability to manipulate and analyze data using the pandas library. **Problem Statement:** You are provided with a CSV file that contains statistics of various soccer players. The CSV file has the following columns: - `Player`: Name of the player - `Team`: Team of the player - `Position`: Playing position of the player (e.g., Forward, Midfield, Defense, Goalkeeper) - `Appearances`: Number of appearances in the season - `Goals`: Number of goals scored - `Assists`: Number of assists made - `Nationality`: Nationality of the player Implement the following tasks and document your code with comments explaining each step: 1. **Read the data:** Load the data from the provided CSV file into a pandas DataFrame. 2. **Basic Data Exploration:** - Display the first 10 rows of the DataFrame. - Display the summary statistics for all numerical columns. - Check for any missing values in the dataset and handle them appropriately. 3. **Data Manipulation:** - Create a column `GoalContribution` which is the sum of goals and assists for each player. - Create a subset of the DataFrame that only includes players with more than 20 appearances. - Group the data by `Team` and calculate the average `GoalContribution` per team. 4. **Advanced Analysis:** - Find the top 5 players with the highest `GoalContribution`. - Determine which nationality has the most players in the dataset. - Plot a bar chart showing the average `GoalContribution` for each position. **Constraints:** - Each function you write should take a DataFrame as input and return the required output. - Handle missing data logically, either by filling with appropriate values or removing the rows (justify your choice in comments). - Ensure your code is efficient and runs within a reasonable time frame. **Input Format:** A CSV file named `player_stats.csv` as described above. **Output Format:** You should output the following: 1. The modified DataFrame with the new `GoalContribution` column. 2. A subset DataFrame of players with more than 20 appearances. 3. The average `GoalContribution` per team. 4. The top 5 players by `GoalContribution`. 5. The nationality with the most players. 6. A bar chart of average `GoalContribution` per position. Provide well-commented code to reflect your thought process. # Submission: Submit your Python code file named `soccer_analysis.py`. **Note:** Make sure you import all necessary packages and handle any potential exceptions in your code to make it robust.","solution":"import pandas as pd import matplotlib.pyplot as plt def load_data(file_path): Load the data from the provided CSV file into a pandas DataFrame. return pd.read_csv(file_path) def basic_data_exploration(df): Perform basic data exploration: display first 10 rows, summary statistics, and check for missing values. print(\\"First 10 rows of the dataframe:\\") display(df.head(10)) print(\\"Summary statistics for all numerical columns:\\") display(df.describe()) print(\\"Check for any missing values:\\") missing_values = df.isnull().sum() print(missing_values) return df def handle_missing_values(df): Handle missing values by filling with 0 (assuming goals and assists can\'t be negative). df[\'Goals\'].fillna(0, inplace=True) df[\'Assists\'].fillna(0, inplace=True) return df def create_goal_contribution(df): Create a column `GoalContribution` which is the sum of goals and assists for each player. df[\'GoalContribution\'] = df[\'Goals\'] + df[\'Assists\'] return df def filter_appearances(df, min_appearances=20): Create a subset of the DataFrame that only includes players with more than 20 appearances. return df[df[\'Appearances\'] > min_appearances] def average_goal_contribution_per_team(df): Group the data by `Team` and calculate the average `GoalContribution` per team. return df.groupby(\'Team\')[\'GoalContribution\'].mean() def top_players_by_goal_contribution(df, top_n=5): Find the top N players with the highest `GoalContribution`. return df.nlargest(top_n, \'GoalContribution\') def nationality_with_most_players(df): Determine which nationality has the most players in the dataset. return df[\'Nationality\'].value_counts().idxmax() def plot_avg_goal_contribution_per_position(df): Plot a bar chart showing the average `GoalContribution` for each position. avg_contribution = df.groupby(\'Position\')[\'GoalContribution\'].mean() avg_contribution.plot(kind=\'bar\', figsize=(10, 6)) plt.title(\'Average Goal Contribution per Position\') plt.xlabel(\'Position\') plt.ylabel(\'Average Goal Contribution\') plt.show() # Wrapper function to perform all tasks and return outputs as requested. def analyze_soccer_stats(file_path): df = load_data(file_path) df = basic_data_exploration(df) df = handle_missing_values(df) df = create_goal_contribution(df) subset_df = filter_appearances(df) avg_goal_contrib_per_team = average_goal_contribution_per_team(df) top_players = top_players_by_goal_contribution(df) most_nationality = nationality_with_most_players(df) plot_avg_goal_contribution_per_position(df) return df, subset_df, avg_goal_contrib_per_team, top_players, most_nationality"},{"question":"**Objective:** Implement a Python function `convert_message_to_mime` that utilizes the `BytesGenerator` from the `email.generator` module to serialize an `EmailMessage` object into a MIME-compliant binary representation and save it to a specified file. **Function Signature:** ```python def convert_message_to_mime(email_message, output_file): Converts an EmailMessage object into a MIME-compliant binary format and writes it to the specified output file. Parameters: email_message (EmailMessage): The email message object to convert. output_file (str): The path to the file where the output should be saved. Returns: None ``` **Input:** - `email_message`: An instance of `EmailMessage` from the `email.message` module. This object represents the email to be serialized. - `output_file`: A string representing the file path where the output binary data should be written. **Output:** - The function returns `None` but writes the serialized binary representation of the email message to the specified `output_file`. **Constraints:** - You must use the `BytesGenerator` class for the serialization. - Handle errors gracefully, ensuring that any file handling operations are properly closed even if an error occurs. **Example Usage:** ```python from email.message import EmailMessage # Create an example EmailMessage object email_message = EmailMessage() email_message[\'From\'] = \'sender@example.com\' email_message[\'To\'] = \'recipient@example.com\' email_message[\'Subject\'] = \'Sample Email\' email_message.set_content(\'This is a sample email.\') # Convert the email message to MIME format and save it to a file convert_message_to_mime(email_message, \'output.eml\') ``` **Notes:** 1. Students are expected to demonstrate their understanding of the `BytesGenerator` class from the `email.generator` module. 2. Proper file handling and exception management are essential. 3. The focus should be on utilizing the methods provided by the `BytesGenerator` class effectively.","solution":"from email.message import EmailMessage from email.generator import BytesGenerator import os def convert_message_to_mime(email_message, output_file): Converts an EmailMessage object into a MIME-compliant binary format and writes it to the specified output file. Parameters: email_message (EmailMessage): The email message object to convert. output_file (str): The path to the file where the output should be saved. Returns: None try: # Ensure the output directory exists os.makedirs(os.path.dirname(output_file), exist_ok=True) with open(output_file, \'wb\') as file: generator = BytesGenerator(file) generator.flatten(email_message) except Exception as e: print(f\\"An error occurred: {e}\\") raise"},{"question":"# Question: Optimizing Machine Learning Model Performance Problem Statement: You are given a high-dimensional dataset for a binary classification task. Your objective is to implement a machine learning pipeline using `scikit-learn` that: 1. **Optimizes prediction latency** by choosing an appropriate model and adjusting parameters. 2. **Utilizes sparse matrix representations** if beneficial based on the sparsity of the input data. 3. Provides an estimate of prediction latency for both bulk and atomic prediction modes. 4. Demonstrates the influence of model complexity on prediction latency and accuracy. Dataset: - `X_train`, `X_test`: NumPy arrays of shape `(n_samples, n_features)`, representing training and testing data. - `y_train`, `y_test`: NumPy arrays of shape `(n_samples,)`, representing binary class labels for training and testing data. Requirements: 1. **Sparsity Check**: Implement a function `calculate_sparsity(X)` that returns the sparsity ratio of the input feature matrix `X`. 2. **Model Implementation**: Choose a suitable model (e.g., `LogisticRegression`, `SGDClassifier`) and implement the training function `train_model(X_train, y_train)` that returns the trained model. 3. **Prediction Performance**: Implement a function `predict_performance(model, X_test)` that returns the prediction latencies for both bulk and atomic modes. Use the current time to measure prediction latency. 4. **Model Complexity**: Train the model with varying complexity parameters and assess the trade-offs between latency and accuracy. Implement the function `model_complexity_analysis(X_train, y_train, X_test, y_test)` that returns a dictionary with model complexities as keys and a tuple of (latency, accuracy) as values. Input and Output Formats: - Input: `X_train`, `X_test`, `y_train`, `y_test` - Output: A dictionary with details on sparsity, model performance, and complexity analysis, structured as: ```python { \'sparsity\': float, \'bulk_latency\': float, \'atomic_latency\': float, \'complexity_analysis\': { \'complexity_param_1\': (bulk_latency, atomic_latency, accuracy), ... } } ``` Constraints: - The model should be trained and tested on reasonably sized datasets (e.g., `X_train`, `X_test` with dimensions up to `(10000, 1000)`). - Use `scikit-learn` for model implementation and performance measurement. - Ensure that the solution is efficient and scalable. Example Solution: Provide an example implementation that demonstrates the usage of the required functions. ```python import numpy as np import time from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score from scipy.sparse import csr_matrix def calculate_sparsity(X): return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1]) def train_model(X_train, y_train): model = SGDClassifier() model.fit(X_train, y_train) return model def predict_performance(model, X_test): start_atomic = time.time() for i in range(X_test.shape[0]): model.predict(X_test[i,i].reshape(1, -1)) atomic_latency = (time.time() - start_atomic) / X_test.shape[0] start_bulk = time.time() model.predict(X_test) bulk_latency = (time.time() - start_bulk) / X_test.shape[0] return bulk_latency, atomic_latency def model_complexity_analysis(X_train, y_train, X_test, y_test): complexity_params = [0.001, 0.01, 0.1, 1.0, 10.0] results = {} for param in complexity_params: model = SGDClassifier(alpha=param) model.fit(X_train, y_train) bulk_latency, atomic_latency = predict_performance(model, X_test) y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) results[param] = (bulk_latency, atomic_latency, accuracy) return results def main(X_train, X_test, y_train, y_test): sparsity = calculate_sparsity(X_train) model = train_model(X_train, y_train) bulk_latency, atomic_latency = predict_performance(model, X_test) complexity_analysis = model_complexity_analysis(X_train, y_train, X_test, y_test) return { \'sparsity\': sparsity, \'bulk_latency\': bulk_latency, \'atomic_latency\': atomic_latency, \'complexity_analysis\': complexity_analysis } ```","solution":"import numpy as np import time from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def calculate_sparsity(X): Calculate the sparsity ratio of the input feature matrix X. Sparsity ratio is defined as the fraction of zero elements to the total elements. return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1]) def train_model(X_train, y_train): Train an SGDClassifier model on the given training data. Parameters: X_train (np.ndarray): Training features y_train (np.ndarray): Training labels Returns: model: Trained SGDClassifier model model = SGDClassifier() model.fit(X_train, y_train) return model def predict_performance(model, X_test): Measure the prediction latency of the model for both bulk and atomic prediction modes. Parameters: model: Trained model X_test (np.ndarray): Test features Returns: tuple: (bulk_latency, atomic_latency) - bulk_latency: Average time per sample for bulk prediction - atomic_latency: Average time per sample for atomic prediction start_atomic = time.time() for i in range(X_test.shape[0]): model.predict(X_test[i, :].reshape(1, -1)) atomic_latency = (time.time() - start_atomic) / X_test.shape[0] start_bulk = time.time() model.predict(X_test) bulk_latency = (time.time() - start_bulk) / X_test.shape[0] return bulk_latency, atomic_latency def model_complexity_analysis(X_train, y_train, X_test, y_test): Analyze the influence of model complexity on prediction latency and accuracy. Parameters: X_train (np.ndarray): Training features y_train (np.ndarray): Training labels X_test (np.ndarray): Test features y_test (np.ndarray): Test labels Returns: dict: A dictionary with model complexities as keys and a tuple of (bulk_latency, atomic_latency, accuracy) as values. complexity_params = [0.001, 0.01, 0.1, 1.0, 10.0] results = {} for param in complexity_params: model = SGDClassifier(alpha=param) model.fit(X_train, y_train) bulk_latency, atomic_latency = predict_performance(model, X_test) y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) results[param] = (bulk_latency, atomic_latency, accuracy) return results def main(X_train, X_test, y_train, y_test): Execute the main pipeline: sparcity calculation, model training, performance measurement and complexity analysis. Parameters: X_train (np.ndarray): Training features y_train (np.ndarray): Training labels X_test (np.ndarray): Test features y_test (np.ndarray): Test labels Returns: dict: Results including sparsity, prediction latencies and complexity analysis. sparsity = calculate_sparsity(X_train) model = train_model(X_train, y_train) bulk_latency, atomic_latency = predict_performance(model, X_test) complexity_analysis = model_complexity_analysis(X_train, y_train, X_test, y_test) return { \'sparsity\': sparsity, \'bulk_latency\': bulk_latency, \'atomic_latency\': atomic_latency, \'complexity_analysis\': complexity_analysis }"},{"question":"# Regular Expressions and Data Extraction Challenge You have been provided with a text file containing a collection of bibliographic references in a specific format. Your task is to extract specific parts of the bibliographic references using regular expressions and the `re` module. Input Format The input consists of multiple lines, where each line contains a bibliographic reference. Each reference includes: - The title of the work, enclosed in double quotation marks `\\"\\"`. - The authors\' names, with multiple authors separated by commas `,`. - The year of publication, enclosed in parentheses `()`. - The name of the journal, book, or publisher in italics (assume italic text marked with underscores `_`). Example: ``` \\"Understanding the Universe\\" J. Doe, A. Smith (2015) _Journal of Science_ \\"Basics of Python\\" C. Brown, E. White (2018) _Python Publishing_ ``` Task 1. Write a function `extract_bibliographic_info(text: str) -> list` that takes as input the text containing multiple bibliographic references and returns a list of dictionaries. Each dictionary should have the keys: \'title\', \'authors\', \'year\', and \'journal\'. 2. For each bibliographic reference, extract the: - Title of the work. - List of authors. - Year of publication. - Journal name. Output Format Return a list of dictionaries, where each dictionary contains the extracted information for one bibliographic reference. Example Given the input text: ``` \\"Understanding the Universe\\" J. Doe, A. Smith (2015) _Journal of Science_ \\"Basics of Python\\" C. Brown, E. White (2018) _Python Publishing_ ``` The function should return: ```python [ { \\"title\\": \\"Understanding the Universe\\", \\"authors\\": [\\"J. Doe\\", \\"A. Smith\\"], \\"year\\": 2015, \\"journal\\": \\"Journal of Science\\" }, { \\"title\\": \\"Basics of Python\\", \\"authors\\": [\\"C. Brown\\", \\"E. White\\"], \\"year\\": 2018, \\"journal\\": \\"Python Publishing\\" } ] ``` Constraints - The input text will always follow the specified format. - Ensure your regular expressions are robust to handle variations in spacing around the different parts. - Your function should handle any number of bibliographic references, including an empty input text. Evaluation Criteria - Correctness: Ensure the function correctly extracts all parts of each bibliographic reference. - Efficiency: Implement the function to handle large strings efficiently. - Robustness: Handle edge cases such as varying spaces between parts correctly. Good luck!","solution":"import re def extract_bibliographic_info(text: str) -> list: Extracts bibliographic information from the input text. Args: text (str): Input text containing multiple bibliographic references. Returns: list: A list of dictionaries containing the extracted information. Each dictionary includes: - \'title\': The title of the work. - \'authors\': A list of authors. - \'year\': The year of publication. - \'journal\': The name of the journal, book, or publisher. # Regular expression to match the bibliographic reference format pattern = r\'\\"(.+?)\\"s+([^()]+?)s+((d{4}))s+_(.+?)_\' matches = re.findall(pattern, text) # Parse the matches into the required dictionary format result = [] for match in matches: title, authors, year, journal = match authors_list = [author.strip() for author in authors.split(\',\')] result.append({ \'title\': title, \'authors\': authors_list, \'year\': int(year), \'journal\': journal }) return result"},{"question":"# Custom Scoring Function and Model Evaluation with Cross-Validation Objective To assess the students\' understanding of custom scoring functions, cross-validation, and the use of advanced evaluation metrics in scikit-learn. Problem Statement You are given a dataset `X` (features) and `y` (target labels). Your tasks are as follows: 1. **Define a Custom Scoring Function**: Implement a custom scoring function `custom_scorer` that calculates the mean squared logarithmic error (MSLE). 2. **Model Evaluation with Custom Scorer**: Utilize the custom scoring function `custom_scorer` to evaluate a `RandomForestRegressor` model using `cross_val_score` with 5-fold cross-validation. 3. **Grid Search with Custom Scorer**: Perform hyperparameter tuning on a `GradientBoostingRegressor` using `GridSearchCV` with the custom scoring function. Use the following hyperparameters for the grid search: - `n_estimators`: [50, 100] - `learning_rate`: [0.1, 0.01] Constraints 1. Ensure that the custom scoring function can handle both classification and regression targets, but prioritize its implementation for regression evaluation. 2. Cross-validation must be performed with `cv=5`. 3. The dataset is split into training and testing sets before the evaluation or tuning process. Expected Output 1. **Custom Scoring Function Definition**: Python function definition for `custom_scorer`. 2. **Model Evaluation**: The average MSLE score from the cross-validation evaluation. 3. **Grid Search**: The best hyperparameters obtained from the grid search in the format: ```python Best Hyperparameters: {\'n_estimators\': <best_n_estimators_value>, \'learning_rate\': <best_learning_rate_value>} ``` Input Format - `X`: numpy array of shape (n_samples, n_features) - `y`: numpy array of shape (n_samples,) Example ```python from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV from sklearn.metrics import make_scorer import numpy as np # Sample Data X = np.random.rand(100, 10) y = np.random.rand(100) # Split Data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Task 1: Custom Scoring Function def custom_scorer(y_true, y_pred): # Implement mean squared logarithmic error msle = np.mean((np.log1p(y_true) - np.log1p(y_pred)) ** 2) return msle # Task 2: Model Evaluation using Custom Scorer # Create the custom scorer using make_scorer scorer = make_scorer(custom_scorer, greater_is_better=False) # Initialize the model model = RandomForestRegressor(random_state=42) # Perform cross-validation cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=scorer) print(\\"Average MSLE from Cross-Validation:\\", np.mean(cv_scores)) # Task 3: Hyperparameter Tuning using Grid Search # Initialize the model gbr = GradientBoostingRegressor(random_state=42) # Set up the parameter grid param_grid = { \'n_estimators\': [50, 100], \'learning_rate\': [0.1, 0.01] } # Initialize GridSearchCV grid_search = GridSearchCV(gbr, param_grid, scoring=scorer, cv=5) # Fit GridSearchCV grid_search.fit(X_train, y_train) # Get best parameters print(\\"Best Hyperparameters:\\", grid_search.best_params_) ``` Notes - Ensure your function is well-tested and handles edge cases. - Use `random_state=42` for reproducibility.","solution":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV from sklearn.metrics import make_scorer import numpy as np def custom_scorer(y_true, y_pred): Custom scoring function to calculate mean squared logarithmic error (MSLE). msle = np.mean((np.log1p(y_true) - np.log1p(y_pred)) ** 2) return msle def evaluate_model_with_custom_scorer(X, y): Evaluates a RandomForestRegressor model using the custom MSLE scorer. # Create the custom scorer using make_scorer scorer = make_scorer(custom_scorer, greater_is_better=False) # Initialize the model model = RandomForestRegressor(random_state=42) # Perform cross-validation cv_scores = cross_val_score(model, X, y, cv=5, scoring=scorer) return np.mean(cv_scores) def grid_search_with_custom_scorer(X, y): Perform hyperparameter tuning on a GradientBoostingRegressor using GridSearchCV with the custom MSLE scorer. # Create the custom scorer using make_scorer scorer = make_scorer(custom_scorer, greater_is_better=False) # Initialize the model gbr = GradientBoostingRegressor(random_state=42) # Set up the parameter grid param_grid = { \'n_estimators\': [50, 100], \'learning_rate\': [0.1, 0.01] } # Initialize GridSearchCV grid_search = GridSearchCV(gbr, param_grid, scoring=scorer, cv=5) # Fit GridSearchCV grid_search.fit(X, y) # Get best parameters return grid_search.best_params_"},{"question":"# Custom Profiling Callback with PyTorch In this task, you are asked to implement a custom profiling callback in PyTorch using Python. You will use `torch.autograd.profiler` to measure the time taken by specific operations in a given neural network model. # Requirements 1. Implement a neural network model using PyTorch. 2. Implement a custom profiling callback using `torch.autograd.profiler` that measures the time taken by each operation in the model. 3. Use `torch.autograd.profiler.profile` context manager to invoke your custom callback during the forward pass of the model. 4. Log the name of each operation and the corresponding time taken. # Input and Output Formats Input - Python code to define the neural network model. - A sample input tensor to pass through the model during the profiling. Output - Printed logs of operation names and the time taken by each operation during the forward pass of the model. # Constraints - The neural network model should have at least three layers. - You should use the `torch.nn` module to define your model. - The profiling callback should accurately log the time taken by each operation. - Your solution should be robust and handle any potential errors gracefully. You are given the following skeleton code to start with: ```python import torch import torch.nn as nn import torch.autograd.profiler as profiler class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.layer1 = nn.Linear(10, 20) self.layer2 = nn.Linear(20, 10) self.layer3 = nn.Linear(10, 5) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x def custom_profiler_callback(prof): print(\\"Name of operation and time taken:\\") for evt in prof.function_events: print(f\\"{evt.name}: {evt.cpu_time_total} us\\") def main(): model = SimpleModel() input_tensor = torch.randn(1, 10) with profiler.profile(record_shapes=True, use_cuda=False) as prof: model(input_tensor) custom_profiler_callback(prof) if __name__ == \\"__main__\\": main() ``` # Explanation 1. **SimpleModel Class**: Defines a simple neural network model with three linear layers. 2. **custom_profiler_callback Function**: Takes a profiler object and logs the name and time taken by each operation. 3. **main Function**: Initializes the model and input tensor, runs the forward pass within the profiling context, and then calls the custom profiler callback to log the results. Implement the required functionality and ensure the output is correctly printed.","solution":"import torch import torch.nn as nn import torch.autograd.profiler as profiler class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.layer1 = nn.Linear(10, 20) self.layer2 = nn.Linear(20, 10) self.layer3 = nn.Linear(10, 5) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x def custom_profiler_callback(prof): print(\\"Name of operation and time taken:\\") for evt in prof.key_averages(): print(f\\"{evt.key}: {evt.cpu_time_total:.2f} us\\") def profile_model(model, input_tensor): with profiler.profile(record_shapes=True, use_cuda=False) as prof: model(input_tensor) custom_profiler_callback(prof) def main(): model = SimpleModel() input_tensor = torch.randn(1, 10) profile_model(model, input_tensor) if __name__ == \\"__main__\\": main()"},{"question":"# Seaborn Custom Plotting and Legend Customization You have been provided a dataset `penguins` from seaborn. Your task is to create a specific plot and customize its appearance and legends according to the requirements given below. **Dataset:** The `penguins` dataset includes the following columns: - `species`: Species of the penguin (Adelie, Chinstrap, Gentoo) - `island`: Island where the penguin is observed - `bill_length_mm`: Length of the penguin\'s bill in millimeters - `bill_depth_mm`: Depth of the penguin\'s bill in millimeters - `flipper_length_mm`: Length of the penguin\'s flipper in millimeters - `body_mass_g`: Body mass of the penguin in grams - `sex`: Sex of the penguin (Male, Female) **Task:** 1. Create a `displot` of the `penguins` dataset with `bill_length_mm` on the x-axis and differentiate the species using different colors in the plot. The plot should be faceted by the `island` column, arranged in a 2-column layout, and each facet should be 4 units tall. 2. Position the legend at the `upper left` within the plot area. 3. Ensure the legend is displayed inside the plot without any extra space on the right. 4. Customize the legend to have no frame and organized in one column. # Function Specification **Function Name:** `custom_penguin_plot` **Input:** None. (You should load the dataset within the function) **Output:** The function does not return anything but should display the customized `displot`. # Constraints - Use the seaborn package for visualization. - Ensure your plot is clear and professional. Example: ```python import seaborn as sns def custom_penguin_plot(): sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") # Your plot code starts here g = sns.displot( penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=4, facet_kws=dict(legend_out=False), ) sns.move_legend(g, \\"upper left\\", bbox_to_anchor=(.85, .85), frameon=False, ncol=1) # Call the function to view the plot custom_penguin_plot() ``` Ensure the plot is displayed correctly with the specified customizations.","solution":"import seaborn as sns def custom_penguin_plot(): sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") # Create the displot g = sns.displot( penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=4, facet_kws=dict(legend_out=False), ) # Customize the legend for ax in g.axes_dict.values(): ax.legend(loc=\'upper left\', frameon=False, ncol=1) # Call the function to view the plot custom_penguin_plot()"},{"question":"**Objective:** Implement a function that generates and saves a seaborn plot with a specific style and customizations based on provided data. **Function Signature:** ```python def create_seaborn_plot(x: list, y: list, filename: str, style: str, customizations: dict) -> None: pass ``` **Input:** - `x` (list): A list of x-axis values for the plot. - `y` (list): A list of y-axis values for the plot. - `filename` (str): The name of the file where the plot will be saved (e.g., \\"plot.png\\"). - `style` (str): The seaborn plot style to be used (e.g., \\"whitegrid\\", \\"darkgrid\\"). - `customizations` (dict): A dictionary of seaborn style parameter customizations (e.g., {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"}). **Output:** - The function should save the plot to a file with the provided filename. **Constraints:** - The lengths of `x` and `y` should be the same. - The style should be a valid seaborn style string. - The customizations should be valid seaborn style parameters. **Example Usage:** ```python x = [\\"A\\", \\"B\\", \\"C\\"] y = [1, 3, 2] filename = \\"example_plot.png\\" style = \\"darkgrid\\" customizations = {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"} create_seaborn_plot(x, y, filename, style, customizations) ``` **Expected Output:** The function will save a plot (`.png` file) named \\"example_plot.png\\" with the specified style and parameter customizations applied. **Hints:** 1. Use `sns.set_style()` to set the seaborn style. 2. Create the plot using `sns.barplot()` or `sns.lineplot()` depending on your preference. 3. Use `plt.savefig()` to save the plot to the specified file.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_seaborn_plot(x, y, filename, style, customizations): Generates and saves a seaborn plot with specific style and customizations. Args: x (list): A list of x-axis values for the plot. y (list): A list of y-axis values for the plot. filename (str): The name of the file where the plot will be saved. style (str): The seaborn plot style to be used. customizations (dict): A dictionary of seaborn style parameter customizations. # Set the seaborn style sns.set_style(style) # Apply customizations sns.set_context(\\"notebook\\", rc=customizations) # Generate the plot plt.figure(figsize=(10, 6)) sns.lineplot(x=x, y=y) # Save the plot to the specified file plt.savefig(filename) # Close the plot to free up memory plt.close()"},{"question":"Objective: To evaluate your understanding of PyTorch\'s neural network parameter initialization, you will implement a custom neural network module and apply different initialization techniques to its parameters. Problem Statement: 1. Implement a custom PyTorch neural network module named `CustomNet` using the `torch.nn.Module` class. 2. The network should consist of: - An input layer that takes an input of size 784 (e.g., for flattened 28x28 images) - Two hidden layers with 128 and 64 units respectively - Output layer with 10 units (e.g., for classification into 10 categories) - Use ReLU activation functions after each hidden layer 3. Create a function named `initialize_parameters` to initialize the parameters of `CustomNet` using a specified initialization scheme. - The function should take a parameter `scheme` which determines the initialization method to use. It should support the following values: `\'xavier_uniform\'`, `\'xavier_normal\'`, `\'kaiming_uniform\'`, and `\'kaiming_normal\'`. - Utilize the appropriate PyTorch initialization function based on the value of `scheme`. 4. Demonstrate how to use the `initialize_parameters` function to initialize an instance of `CustomNet` with each of the supported initialization schemes. Constraints: - You must use the initialization functions from the `torch.nn.init` module. - You must ensure that the initialization is done in a no-gradient context (`torch.no_grad`). - The initialization function should print the name of the initialization scheme being applied. # Input and Output Formats: **Function Signature:** ```python import torch import torch.nn as nn import torch.nn.init as init class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() # Define your layers here def forward(self, x): # Define the forward pass pass def initialize_parameters(model, scheme: str): # Initialize model parameters based on the scheme provided pass ``` **Example Usage:** ```python # Create an instance of CustomNet model = CustomNet() # Initialize using Xavier Uniform initialize_parameters(model, \'xavier_uniform\') # Initialize using Xavier Normal initialize_parameters(model, \'xavier_normal\') # Initialize using Kaiming Uniform initialize_parameters(model, \'kaiming_uniform\') # Initialize using Kaiming Normal initialize_parameters(model, \'kaiming_normal\') ``` **Expected Output:** The function should print which initialization scheme is being applied, for example: ``` Applying xavier_uniform initialization. Applying xavier_normal initialization. Applying kaiming_uniform initialization. Applying kaiming_normal initialization. ``` Notes: - Ensure the model\'s layers are appropriately initialized according to the specified scheme. - Test the initialization by creating instances of `CustomNet` and applying each initialization scheme.","solution":"import torch import torch.nn as nn import torch.nn.init as init class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x def initialize_parameters(model, scheme: str): print(f\\"Applying {scheme} initialization.\\") with torch.no_grad(): for name, param in model.named_parameters(): if \'weight\' in name: if scheme == \'xavier_uniform\': init.xavier_uniform_(param) elif scheme == \'xavier_normal\': init.xavier_normal_(param) elif scheme == \'kaiming_uniform\': init.kaiming_uniform_(param, nonlinearity=\'relu\') elif scheme == \'kaiming_normal\': init.kaiming_normal_(param, nonlinearity=\'relu\') elif \'bias\' in name: param.fill_(0)"},{"question":"**Coding Assessment Question** You are given a dataset with missing values and unscaled features. Your task is to implement a preprocessing pipeline using scikit-learn that accomplishes the following: 1. **Impute Missing Values**: Use the `SimpleImputer` transformer to replace missing values. 2. **Scale Features**: Use the `StandardScaler` to scale the features to have zero mean and unit variance. 3. **Dimensionality Reduction**: Reduce the number of features using PCA (Principal Component Analysis) to retain 95% of the variance. Specifically, you need to implement a function `preprocess_pipeline` that: 1. Takes a DataFrame `X` with numerical features as input. 2. Returns the transformed feature matrix after applying the imputing, scaling, and dimensionality reduction steps. **Function Signature:** ```python def preprocess_pipeline(X: pd.DataFrame) -> np.ndarray: ``` **Input:** - `X`: A pandas DataFrame of shape (n_samples, n_features) with numerical features. The DataFrame may contain missing values (NaNs). **Output:** - A numpy ndarray of shape (n_samples, n_components) with the transformed features. **Constraints:** - You must use scikit-learn\'s `SimpleImputer`, `StandardScaler`, and `PCA` classes for the respective transformations. - Handle performance considerations if applicable. **Example:** ```python import pandas as pd import numpy as np data = { \'feature1\': [1.0, 2.0, np.nan, 4.0], \'feature2\': [4.0, np.nan, 6.0, 8.0], \'feature3\': [7.0, 8.0, 9.0, 10.0] } df = pd.DataFrame(data) transformed_data = preprocess_pipeline(df) print(transformed_data) ``` **Expected Output:** A numpy array with the transformed features after imputing, scaling, and reducing dimensionality. **Evaluation Criteria:** - Correct usage of scikit-learn transformers. - Correct implementation of the pipeline. - Functional correctness of the output. Good luck!","solution":"import numpy as np import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline def preprocess_pipeline(X: pd.DataFrame) -> np.ndarray: Applies preprocessing steps to a DataFrame: 1. Impute missing values with the mean. 2. Scale features to have zero mean and unit variance. 3. Apply PCA to retain 95% of the variance. Args: X (pd.DataFrame): Input DataFrame with numerical features and possible missing values. Returns: np.ndarray: Transformed feature matrix. pipeline = Pipeline([ (\\"imputer\\", SimpleImputer(strategy=\\"mean\\")), (\\"scaler\\", StandardScaler()), (\\"pca\\", PCA(n_components=0.95)) ]) transformed_X = pipeline.fit_transform(X) return transformed_X"},{"question":"# Advanced Coding Assessment: Mocking with `unittest.mock` Problem Statement You are tasked with testing a module `order_processing.py` which has a class `OrderProcessor`. This class relies on several external services which should be mocked for unit testing. Your goal is to write a test case for the `OrderProcessor` class using `unittest.mock`. Implement the following requirements using the `unittest.mock` library: 1. **Mock External Services**: Mock three external services: - `InventoryService`: Checks inventory levels. - `PaymentService`: Processes payments. - `NotificationService`: Sends notifications. 2. **OrderProcessor Class**: Assume the class `OrderProcessor` in `order_processing.py` has the following methods: ```python class OrderProcessor: def __init__(self, inventory_service, payment_service, notification_service): self.inventory_service = inventory_service self.payment_service = payment_service self.notification_service = notification_service def process_order(self, order): if not self.inventory_service.check_inventory(order): return \\"OutOfStock\\" if not self.payment_service.process_payment(order): return \\"PaymentFailed\\" self.notification_service.send_order_confirmed_notification(order) return \\"OrderProcessed\\" ``` 3. **Test Case Requirements**: Write the following test cases: - `test_order_out_of_stock`: Test that the order cannot be processed if the inventory is insufficient. - `test_payment_failure`: Test that the order cannot be processed if the payment fails. - `test_successful_order_processing`: Test that a successful order processing notifies the customer and returns \\"OrderProcessed\\". Input and Output Format * **Input**: The input will not be direct. Instead, you need to set up the mocks to simulate different scenarios using the `unittest.mock` library. * **Output**: The output for each test case will be the assertion results, which should verify that the `OrderProcessor` class is behaving as expected under different mocked conditions. Constraints - Use `Mock` or `MagicMock` from `unittest.mock` to create mock objects. - Use `patch` or `patch.object` for patching during the tests. - Use `assert_called_with`, `assert_called_once`, and `side_effect` as needed to validate the behavior of the mocks. Example Code Below is a skeleton code structure. You are required to fill in the missing parts: ```python import unittest from unittest.mock import Mock, patch from order_processing import OrderProcessor class TestOrderProcessor(unittest.TestCase): def setUp(self): self.inventory_service_mock = Mock() self.payment_service_mock = Mock() self.notification_service_mock = Mock() self.order_processor = OrderProcessor( self.inventory_service_mock, self.payment_service_mock, self.notification_service_mock ) def test_order_out_of_stock(self): # Arrange self.inventory_service_mock.check_inventory.return_value = False order = {\'item_id\': 1, \'quantity\': 10} # Act result = self.order_processor.process_order(order) # Assert self.assertEqual(result, \\"OutOfStock\\") self.inventory_service_mock.check_inventory.assert_called_once_with(order) self.payment_service_mock.process_payment.assert_not_called() self.notification_service_mock.send_order_confirmed_notification.assert_not_called() def test_payment_failure(self): # Arrange self.inventory_service_mock.check_inventory.return_value = True self.payment_service_mock.process_payment.return_value = False order = {\'item_id\': 1, \'quantity\': 10} # Act result = self.order_processor.process_order(order) # Assert self.assertEqual(result, \\"PaymentFailed\\") self.inventory_service_mock.check_inventory.assert_called_once_with(order) self.payment_service_mock.process_payment.assert_called_once_with(order) self.notification_service_mock.send_order_confirmed_notification.assert_not_called() def test_successful_order_processing(self): # Arrange self.inventory_service_mock.check_inventory.return_value = True self.payment_service_mock.process_payment.return_value = True order = {\'item_id\': 1, \'quantity\': 10} # Act result = self.order_processor.process_order(order) # Assert self.assertEqual(result, \\"OrderProcessed\\") self.inventory_service_mock.check_inventory.assert_called_once_with(order) self.payment_service_mock.process_payment.assert_called_once_with(order) self.notification_service_mock.send_order_confirmed_notification.assert_called_once_with(order) if __name__ == \'__main__\': unittest.main() ``` Notes: 1. `order_processing.py` is expected to exist with the `OrderProcessor` class implemented as specified. 2. The test cases should verify that all conditions are correctly tested using the provided `unittest.mock` constructs.","solution":"import unittest from unittest.mock import Mock class OrderProcessor: def __init__(self, inventory_service, payment_service, notification_service): self.inventory_service = inventory_service self.payment_service = payment_service self.notification_service = notification_service def process_order(self, order): if not self.inventory_service.check_inventory(order): return \\"OutOfStock\\" if not self.payment_service.process_payment(order): return \\"PaymentFailed\\" self.notification_service.send_order_confirmed_notification(order) return \\"OrderProcessed\\""},{"question":"# Question: Implement an Enhanced MIME-type Handler Using `mailcap` **Background:** You are tasked with developing a function that automates the process of finding and executing the appropriate command for a given MIME type. Additionally, your function should offer more fine-grained control over parameters and provide detailed logging of the operations performed. **Objective:** Write a Python function `enhanced_mime_handler(MIMEtype: str, action: str = \'view\', filename: str = None, parameters: dict = None) -> str`. **Function Specifications:** 1. **Input:** - `MIMEtype` (str): The MIME type to be handled (e.g., \'video/mpeg\'). - `action` (str, optional): The desired action, such as \'view\', \'edit\', or \'compose\'. Default is \'view\'. - `filename` (str, optional): The filename to be substituted in the command. Default is None, which should trigger an appropriate error. - `parameters` (dict, optional): A dictionary of parameters where keys are parameter names and values are their corresponding values. These parameters should replace placeholders in the command line if specified. 2. **Output:** - The function should return a string representing the fully constructed command to be executed. 3. **Constraints and Limitations:** - If no valid mailcap entry is found, the function should return \\"No valid mailcap entry found.\\" - The function should enforce security checks, ensuring that filenames and parameter values do not contain disallowed characters. - Detailed logs of each step should be printed (or recorded) including: loading mailcap entries, finding matches, parameter substitutions, and security checks. 4. **Performance Requirements:** - The function should efficiently handle the lookup of mailcap entries and substitutions. **Example Usage:** ```python def enhanced_mime_handler(MIMEtype: str, action: str = \'view\', filename: str = None, parameters: dict = None) -> str: import mailcap import string import logging # Setting up logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) allowed_chars = set(string.ascii_letters + string.digits + \\"@+=:,./-_\\") if filename is None: raise ValueError(\\"Filename must be specified\\") if any(c not in allowed_chars for c in filename): logger.warning(\\"Invalid characters in filename\\") return \\"No valid mailcap entry found.\\" # Convert parameters dictionary to plist format plist = [f\\"{k}={v}\\" for k, v in (parameters or {}).items()] # Load mailcap entries logger.debug(\\"Loading mailcap entries\\") caps = mailcap.getcaps() # Find a match logger.debug(f\\"Finding match for MIME type: {MIMEtype}\\") command, entry = mailcap.findmatch(caps, MIMEtype, key=action, filename=filename, plist=plist) if command is None: return \\"No valid mailcap entry found.\\" logger.debug(f\\"Found command: {command}\\") return command # Example call: print(enhanced_mime_handler(\'video/mpeg\', filename=\'tmp1223\', parameters={\'id\': \'1\', \'number\': \'2\', \'total\': \'3\'})) # Output should be: \'xmpeg tmp1223\' with additional logging information displayed. ``` Implement the `enhanced_mime_handler` function as specified above.","solution":"def enhanced_mime_handler(MIMEtype: str, action: str = \'view\', filename: str = None, parameters: dict = None) -> str: import mailcap import string import logging # Setting up logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) allowed_chars = set(string.ascii_letters + string.digits + \\"@+=:,./-_\\") if filename is None: raise ValueError(\\"Filename must be specified\\") if any(c not in allowed_chars for c in filename): logger.warning(\\"Invalid characters in filename\\") return \\"No valid mailcap entry found.\\" if parameters: if any(not set(value).issubset(allowed_chars) for value in parameters.values()): logger.warning(\\"Invalid characters in parameters\\") return \\"No valid mailcap entry found.\\" # Convert parameters dictionary to plist format plist = [f\\"{k}={v}\\" for k, v in (parameters or {}).items()] # Load mailcap entries logger.debug(\\"Loading mailcap entries\\") caps = mailcap.getcaps() # Find a match logger.debug(f\\"Finding match for MIME type: {MIMEtype}\\") command, entry = mailcap.findmatch(caps, MIMEtype, key=action, filename=filename, plist=plist) if command is None: return \\"No valid mailcap entry found.\\" logger.debug(f\\"Found command: {command}\\") return command"},{"question":"# PyTorch to ONNX Export Compatibility Checker You are tasked with creating a utility that ensures that a given PyTorch model only contains operations that are supported by ONNX export. If the model contains any unsupported operations, your utility should notify the user and identify the layer containing the unsupported operation. Task 1. **Define a function `check_model_onnx_compatibility`**: This function should take a PyTorch model and a list of unsupported operations as inputs. 2. The function should traverse the model and identify if any layer contains an operation that is not supported by ONNX export. 3. If the model is fully compatible with ONNX, the function should return `True`. 4. If any unsupported operations are found, the function should return `False` and print out the layers containing the unsupported operations. Input - `model`: An instance of a PyTorch model. - `unsupported_ops`: A list of strings, each representing an unsupported operation. Output - Returns `True` if the model is fully compatible with ONNX. - Returns `False` and prints out the layers containing unsupported operations if any are found. ```python import torch.nn as nn def check_model_onnx_compatibility(model: nn.Module, unsupported_ops: list) -> bool: Check if the PyTorch model contains any unsupported operations for ONNX export. Args: model (nn.Module): The PyTorch model to be checked. unsupported_ops (list): List of unsupported operation strings. Returns: bool: True if the model is compatible with ONNX, False otherwise. unsupported_layers = [] def check_layer(layer): for op in unsupported_ops: if op in str(layer): unsupported_layers.append(layer) # Apply the check_layer function to all submodules in the model model.apply(check_layer) if unsupported_layers: print(\\"Unsupported operations found in the following layers:\\") for layer in unsupported_layers: print(layer) return False return True # Example usage: # Define your PyTorch model class class ExampleModel(nn.Module): def __init__(self): super(ExampleModel, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.ReLU() def forward(self, x): x = self.layer1(x) x = self.layer2(x) return x model = ExampleModel() unsupported_ops = [\\"nn.ReLU\\"] # Example unsupported operation print(check_model_onnx_compatibility(model, unsupported_ops)) ``` Constraints - You should assume that the input model will be a valid instance of `nn.Module`. - The `unsupported_ops` list will contain valid operation names as strings. Performance Requirements - The solution should efficiently traverse through the model\'s layers. - The function should handle large models appropriately without significant performance degradation.","solution":"import torch.nn as nn def check_model_onnx_compatibility(model: nn.Module, unsupported_ops: list) -> bool: Check if the PyTorch model contains any unsupported operations for ONNX export. Args: model (nn.Module): The PyTorch model to be checked. unsupported_ops (list): List of unsupported operation strings. Returns: bool: True if the model is compatible with ONNX, False otherwise. unsupported_layers = [] def check_layer(layer): for op in unsupported_ops: if op in str(layer): unsupported_layers.append(layer) # Apply the check_layer function to all submodules in the model model.apply(check_layer) if unsupported_layers: print(\\"Unsupported operations found in the following layers:\\") for layer in unsupported_layers: print(layer) return False return True"},{"question":"# Advanced Sorting Challenge You are working on a student management system and need to implement various sorting functionalities for handling student records. Each student record contains the student\'s name, grade, and age. Your task is to implement a function `sort_students()` that sorts a list of student records based on multiple criteria. # Function Signature ```python def sort_students(students: list, primary_key: str, secondary_key: str, reverse_primary: bool = False, reverse_secondary: bool = False) -> list: pass ``` # Input - `students` (list): A list of student records, where each record is represented as a dict with keys `name`, `grade`, and `age`. - `primary_key` (str): The key on which the primary sorting should be based (`\\"name\\"`, `\\"grade\\"`, or `\\"age\\"`). - `secondary_key` (str): The key on which the secondary sorting should be based (`\\"name\\"`, `\\"grade\\"`, or `\\"age\\"`). - `reverse_primary` (bool): A boolean indicating if the primary sorting should be in descending order. Default is `False`. - `reverse_secondary` (bool): A boolean indicating if the secondary sorting should be in descending order. Default is `False`. # Output - Returns a new list of student records sorted based on the specified primary and secondary keys. # Constraints - The list of students is non-empty. - The `primary_key` and `secondary_key` will always be one of `\\"name\\"`, `\\"grade\\"`, or `\\"age\\"`. - If two records have the same value for the primary key, the secondary key will determine their order. # Examples ```python students = [ {\\"name\\": \\"Alice\\", \\"grade\\": \\"A\\", \\"age\\": 21}, {\\"name\\": \\"Bob\\", \\"grade\\": \\"B\\", \\"age\\": 20}, {\\"name\\": \\"Charlie\\", \\"grade\\": \\"A\\", \\"age\\": 22}, {\\"name\\": \\"Dave\\", \\"grade\\": \\"B\\", \\"age\\": 20} ] # Example 1 sort_students(students, \\"grade\\", \\"age\\") # Output: [{\\"name\\": \\"Alice\\", \\"grade\\": \\"A\\", \\"age\\": 21}, {\\"name\\": \\"Charlie\\", \\"grade\\": \\"A\\", \\"age\\": 22}, {\\"name\\": \\"Bob\\", \\"grade\\": \\"B\\", \\"age\\": 20}, {\\"name\\": \\"Dave\\", \\"grade\\": \\"B\\", \\"age\\": 20}] # Example 2 sort_students(students, \\"age\\", \\"name\\", True) # Output: [{\\"name\\": \\"Charlie\\", \\"grade\\": \\"A\\", \\"age\\": 22}, {\\"name\\": \\"Alice\\", \\"grade\\": \\"A\\", \\"age\\": 21}, {\\"name\\": \\"Bob\\", \\"grade\\": \\"B\\", \\"age\\": 20}, {\\"name\\": \\"Dave\\", \\"grade\\": \\"B\\", \\"age\\": 20}] ``` Use the `sorted()` function and the `operator.attrgetter` to achieve the desired sort order. Consider the performance implications of sorting algorithms and ensure your solution is efficient.","solution":"def sort_students(students: list, primary_key: str, secondary_key: str, reverse_primary: bool = False, reverse_secondary: bool = False) -> list: Sorts a list of student records based on multiple criteria. Args: - students (list): A list of student records dicts with keys \'name\', \'grade\', \'age\'. - primary_key (str): The key for primary sorting (\'name\', \'grade\', or \'age\'). - secondary_key (str): The key for secondary sorting (\'name\', \'grade\', or \'age\'). - reverse_primary (bool): If True, sorts primary key in descending order. Default is False. - reverse_secondary (bool): If True, sorts secondary key in descending order. Default is False. Returns: - list: A new list of student records sorted based on the specified keys and orders. return sorted(students, key=lambda x: (x[primary_key], x[secondary_key]), reverse=reverse_primary or reverse_secondary)"},{"question":"**Objective:** Design a function that reads from a file and calculates the average of integers present in the file. The function should handle multiple types of errors gracefully, including file not found, empty file, and invalid data. **Problem Statement:** Write a function `calculate_average_from_file(filename)` that performs the following: 1. Opens a file specified by `filename` and reads all integers from the file. 2. If the file cannot be found, raise a custom `FileNotFoundError`. 3. If the file is empty, raise a custom `EmptyFileError`. 4. If any line in the file contains invalid (non-integer) data, that line should be ignored, and a message should be printed informing the user. 5. Calculate and return the average of the valid integers read from the file. 6. Ensure that the file is properly closed after reading, even if an error occurs in the process. **Custom Exceptions:** Define two custom exceptions: - `EmptyFileError` - `InvalidDataError` __Function signature:__ ```python class EmptyFileError(Exception): pass class InvalidDataError(Exception): pass def calculate_average_from_file(filename: str) -> float: pass ``` Here is how the program should behave: # Example 1: Assume \'data.txt\' contains the following data: ``` 10 20 abc 30 ``` ```python >>> try: >>> avg = calculate_average_from_file(\'data.txt\') >>> print(f\\"Average is {avg}\\") >>> except FileNotFoundError: >>> print(\\"The file was not found.\\") >>> except EmptyFileError: >>> print(\\"The file is empty.\\") >>> except InvalidDataError as ide: >>> print(f\\"Invalid data found: {ide}\\") ``` Output: ``` Invalid data found in line: abc Average is 20.0 ``` # Example 2: Assume \'missing_file.txt\' does not exist: ```python >>> try: >>> avg = calculate_average_from_file(\'missing_file.txt\') >>> print(f\\"Average is {avg}\\") >>> except FileNotFoundError: >>> print(\\"The file was not found.\\") >>> except EmptyFileError: >>> print(\\"The file is empty.\\") >>> except InvalidDataError as ide: >>> print(f\\"Invalid data found: {ide}\\") ``` Output: ``` The file was not found. ``` # Example 3: Assume \'empty_file.txt\' is an empty file: ```python >>> try: >>> avg = calculate_average_from_file(\'empty_file.txt\') >>> print(f\\"Average is {avg}\\") >>> except FileNotFoundError: >>> print(\\"The file was not found.\\") >>> except EmptyFileError: >>> print(\\"The file is empty.\\") >>> except InvalidDataError as ide: >>> print(f\\"Invalid data found: {ide}\\") ``` Output: ``` The file is empty. ``` # Constraints: 1. The `filename` provided is a string. 2. The file may contain a mix of valid integers and invalid data. 3. You must use the context manager (`with` statement) to handle file operations. 4. Utilize `try`, `except`, `else`, and `finally` clauses appropriately to ensure robust error handling and resource management. This question tests students\' understanding of: - Exception handling with custom exceptions. - File operations using context managers. - Function implementation with error checking and resource management.","solution":"class EmptyFileError(Exception): pass class InvalidDataError(Exception): pass def calculate_average_from_file(filename: str) -> float: try: with open(filename, \'r\') as file: lines = file.readlines() if not lines: raise EmptyFileError(\\"The file is empty.\\") valid_numbers = [] for line in lines: line = line.strip() try: number = int(line) valid_numbers.append(number) except ValueError: print(f\\"Invalid data found in line: {line}\\") if not valid_numbers: raise EmptyFileError(\\"The file contains no valid integers.\\") average = sum(valid_numbers) / len(valid_numbers) return average except FileNotFoundError: raise FileNotFoundError(\\"The file was not found.\\")"},{"question":"**Objective:** Implement a simple TCP echo server using Python\'s asyncio module. The server should handle multiple clients concurrently, echoing back any data received. This will demonstrate your understanding of event loops, task creation, and networking with asyncio. **Details:** 1. Create an asynchronous TCP echo server class named `EchoServer`. 2. The server should listen on a specified host and port. 3. When a client connects, the server should: - Receive data from the client asynchronously. - Echo the received data back to the client. - Handle multiple clients concurrently without blocking. 4. Implement appropriate connection handling, ensuring that closed connections are dealt with gracefully. **Specifications:** - **Function Signature:** ```python class EchoServer: def __init__(self, host: str, port: int): pass async def handle_client(self, reader, writer): pass async def start(self): pass ``` - **Expected Methods:** - `__init__(self, host: str, port: int)`: Initialize the server with the specified host and port. - `handle_client(self, reader, writer)`: Handle communication with a connected client. - `start(self)`: Start the server and handle incoming connections asynchronously. - **Constraints:** - The server should handle up to 100 concurrent clients. - Each client should be able to send and receive up to 1024 bytes of data. - **Performance Requirements:** - The server should be able to start and accept connections promptly. - Echoing data back to clients should happen with minimal latency. **Example Usage:** ```python import asyncio class EchoServer: def __init__(self, host: str, port: int): self.host = host self.port = port async def handle_client(self, reader, writer): data = await reader.read(1024) writer.write(data) await writer.drain() writer.close() async def start(self): server = await asyncio.start_server(self.handle_client, self.host, self.port) async with server: await server.serve_forever() if __name__ == \\"__main__\\": echo_server = EchoServer(\'localhost\', 8888) asyncio.run(echo_server.start()) ``` **Notes:** - You can assume that the network environment is reliable. - Focus on correct and efficient use of asyncio APIs for managing event loops and network communication. This question assesses understanding of asynchronous programming, event loops, and handling network connections, encompassing fundamental and advanced asyncio concepts.","solution":"import asyncio class EchoServer: def __init__(self, host: str, port: int): self.host = host self.port = port async def handle_client(self, reader, writer): try: while True: data = await reader.read(1024) if not data: break writer.write(data) await writer.drain() except asyncio.CancelledError: writer.close() await writer.wait_closed() finally: writer.close() await writer.wait_closed() async def start(self): server = await asyncio.start_server(self.handle_client, self.host, self.port) async with server: await server.serve_forever() if __name__ == \\"__main__\\": echo_server = EchoServer(\'localhost\', 8888) asyncio.run(echo_server.start())"},{"question":"# Python C API: Tuple and Struct Sequence Manipulation Using the Python C API, write a Python program that accomplishes the following tasks: 1. **Check Tuple Type**: Create a C function that checks whether a given Python object is a tuple and another that checks if it is exactly a tuple type (not a subtype). 2. **Create and Manipulate Tuples**: Create a tuple of length 5, populate it with the integers 1 through 5, and then double-check by retrieving each item from the tuple and printing it. 3. **Slice Tuples**: Slice the tuple created above from position 1 to 3 (inclusive) and print the obtained slice. 4. **Struct Sequence Creation**: Define a new struct sequence type representing a \\"Person\\" with fields \\"name\\" and \\"age\\". Then create an instance of this new type, set the values appropriately, and print them. 5. **Modify Struct Sequence**: Change the previously created \\"Person\\" instance\'s age and print the new age. Specific Requirements: - Implement all the tasks using Python\'s C API. - Use appropriate error checking and handle any errors gracefully. - Ensure no memory leaks by managing references properly. # Submission Submit your complete C code that uses the Python C API to accomplish these tasks. Ensure your code is well-commented and easy to follow. # Example Output The expected output when running your Python C extension should be similar to: ``` Checking Tuple Types: Is tuple: True Is exact tuple: True Original Tuple Items: 1 2 3 4 5 Sliced Tuple: [2, 3, 4] Struct Sequence (Person): Name: John Doe Age: 30 Modified Struct Sequence (Person): New Age: 31 ``` # Hints: - Refer to the provided documentation for creating and manipulating tuples and struct sequences. - Make sure you handle reference counting correctly to avoid memory leaks.","solution":"def is_tuple(obj): Check if the object is a tuple. return isinstance(obj, tuple) def is_exact_tuple(obj): Check if the object type is exactly tuple (not a subtype). return type(obj) is tuple def create_and_populate_tuple(): Create a tuple of length 5, populate it with integers 1 through 5. tup = (1, 2, 3, 4, 5) return tup def slice_tuple(tup, start, end): Slice the tuple from position start to end. return tup[start:end + 1] class Person: A simple struct sequence like class representing a Person. def __init__(self, name, age): self.name = name self.age = age def __repr__(self): return f\\"Person(name={self.name!r}, age={self.age!r})\\" def set_age(self, new_age): Modify the age of the person. self.age = new_age # Create a helper function to demonstrate the functionality def demonstrate(): obj = (1, 2, 3, 4, 5) # Checking tuple types print(\\"Checking Tuple Types:\\") print(\\"Is tuple:\\", is_tuple(obj)) print(\\"Is exact tuple:\\", is_exact_tuple(obj)) # Creating and Manipulating Tuples print(\\"nOriginal Tuple Items:\\") tup = create_and_populate_tuple() print(tup) # Slicing Tuples print(\\"nSliced Tuple:\\", slice_tuple(tup, 1, 3)) # Struct Sequence Creation print(\\"nStruct Sequence (Person):\\") person = Person(\\"John Doe\\", 30) print(person) # Modifying Struct Sequence print(\\"nModified Struct Sequence (Person):\\") person.set_age(31) print(\\"New Age:\\", person.age) print(person)"},{"question":"**Python Coding Assessment Question** # Background You have been hired to work on a software system that models different types of users and their interactions with a service. Your task is to use the `typing` module in Python to create a type-safe and readable structure of this system. # Task 1. Define a `NewType` `UserId` which is a subtype of `int`. 2. Define type aliases: - `User` as a dictionary with the following keys: - `id`: `UserId` - `name`: `str` - `email`: `str` - `UserList` as a list of `User`. 3. Define a class `Service` that uses generics and type annotations to ensure type safety: - A method `add_user` which takes a `User` as input and returns `None`. - A method `get_user_by_id` that takes a `UserId` and returns an `Optional[User]`. - A method `find_users` which takes a `Callable[[User], bool]` (a function that takes in a user and returns a boolean) and returns a `UserList` (all users for whom the callable returns True). 4. Define a protocol `ServiceProtocol` to support structural subtyping for classes with methods `add_user`, `get_user_by_id`, and `find_users`. # Example Usage ```python from typing import NewType, TypedDict, Callable, Protocol, Optional UserId = NewType(\\"UserId\\", int) class User(TypedDict): id: UserId name: str email: str UserList = list[User] class Service: def __init__(self): self.users: UserList = [] def add_user(self, user: User) -> None: self.users.append(user) def get_user_by_id(self, user_id: UserId) -> Optional[User]: for user in self.users: if user[\\"id\\"] == user_id: return user return None def find_users(self, condition: Callable[[User], bool]) -> UserList: return [user for user in self.users if condition(user)] class ServiceProtocol(Protocol): def add_user(self, user: User) -> None: ... def get_user_by_id(self, user_id: UserId) -> Optional[User]: ... def find_users(self, condition: Callable[[User], bool]) -> UserList: ... ``` The above implementation is your goal. Once you\'ve implemented it, you should be able to run the following without errors: ```python if __name__ == \'__main__\': service: ServiceProtocol = Service() user1: User = {\\"id\\": UserId(1), \\"name\\": \\"Alice\\", \\"email\\": \\"alice@example.com\\"} user2: User = {\\"id\\": UserId(2), \\"name\\": \\"Bob\\", \\"email\\": \\"bob@example.com\\"} service.add_user(user1) service.add_user(user2) found_user = service.get_user_by_id(UserId(1)) print(Found user: {found_user}) # Should print Alice\'s details # Finding all users with the name \\"Alice\\" alice_users = service.find_users(lambda u: u[\\"name\\"] == \\"Alice\\") print(f\\"Alice Users: {alice_users}\\") # Should print list containing only Alice ``` # Constraints - Use `typing` module for all type hints. - Ensure type safety and clarity in the definition of types and your classes. - Use generics and protocols where needed. - You can use other helper classes or functions if needed, but they must be type hinted appropriately.","solution":"from typing import NewType, TypedDict, Callable, Protocol, Optional, List UserId = NewType(\\"UserId\\", int) class User(TypedDict): id: UserId name: str email: str UserList = List[User] class Service: def __init__(self): self.users: UserList = [] def add_user(self, user: User) -> None: self.users.append(user) def get_user_by_id(self, user_id: UserId) -> Optional[User]: for user in self.users: if user[\\"id\\"] == user_id: return user return None def find_users(self, condition: Callable[[User], bool]) -> UserList: return [user for user in self.users if condition(user)] class ServiceProtocol(Protocol): def add_user(self, user: User) -> None: ... def get_user_by_id(self, user_id: UserId) -> Optional[User]: ... def find_users(self, condition: Callable[[User], bool]) -> UserList: ..."},{"question":"Objective Assess students\' understanding of scikit-learn functionalities, including data generation, model fitting, and handling model parameters. # Problem Statement You are provided with a synthetic dataset for a regression task. Your job is to: 1. Generate the dataset. 2. Apply a machine learning model from scikit-learn to fit the data. 3. Identify and handle any warnings that arise from parameter settings of the model. # Requirements 1. **Data Generation**: - Use the `make_regression` function from scikit-learn to generate a synthetic dataset with: - 1000 samples. - 20 features. 2. **Model Training**: - Use the `GradientBoostingRegressor` from scikit-learn. - Fit the model to the generated dataset. 3. **Warning Identification**: - Attempt to set the `n_iter_no_change` parameter to 10 during model initialization. - Identify and handle any warnings that arise. # Function Signature ```python import numpy as np from sklearn.datasets import make_regression from sklearn.ensemble import GradientBoostingRegressor import warnings def generate_data(): Generate a synthetic regression dataset. Returns: X (numpy.ndarray): Features of the dataset. y (numpy.ndarray): Target values of the dataset. X, y = make_regression(n_samples=1000, n_features=20) return X, y def train_model(X, y, n_iter_no_change=10): Train a Gradient Boosting Regressor model. Args: X (numpy.ndarray): Features of the dataset. y (numpy.ndarray): Target values of the dataset. n_iter_no_change (int): Number of iterations with no change to stop training early. Returns: model: Trained GradientBoostingRegressor model. warning_message (str): Any warning message captured during training. model = GradientBoostingRegressor(n_iter_no_change=n_iter_no_change) # Capture warnings with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") model.fit(X, y) # Check for warnings if len(w) > 0: warning_message = str(w[-1].message) else: warning_message = \\"\\" return model, warning_message # Example usage if __name__ == \\"__main__\\": X, y = generate_data() model, warning_message = train_model(X, y) print(\\"Model trained. Warning Message: \\", warning_message) ``` # Expected Output 1. Successfully generate the dataset. 2. Train the `GradientBoostingRegressor` model. 3. Correctly identify and handle any warnings related to the `n_iter_no_change` parameter. Constraints - Ensure all necessary imports are included within the functions. - Use synthetic data generation techniques covered in the provided documentation. - Aim for clear and easily readable code, limiting lines to a maximum of 79 characters where possible. Good luck!","solution":"import numpy as np from sklearn.datasets import make_regression from sklearn.ensemble import GradientBoostingRegressor import warnings def generate_data(): Generate a synthetic regression dataset. Returns: X (numpy.ndarray): Features of the dataset. y (numpy.ndarray): Target values of the dataset. X, y = make_regression(n_samples=1000, n_features=20, noise=0.1) return X, y def train_model(X, y, n_iter_no_change=10): Train a Gradient Boosting Regressor model. Args: X (numpy.ndarray): Features of the dataset. y (numpy.ndarray): Target values of the dataset. n_iter_no_change (int): Number of iterations with no change to stop training early. Returns: model: Trained GradientBoostingRegressor model. warning_message (str): Any warning message captured during training. model = GradientBoostingRegressor(n_iter_no_change=n_iter_no_change) # Capture warnings with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") model.fit(X, y) # Check for warnings if len(w) > 0: warning_message = str(w[-1].message) else: warning_message = \\"\\" return model, warning_message # Example usage if __name__ == \\"__main__\\": X, y = generate_data() model, warning_message = train_model(X, y) print(\\"Model trained. Warning Message: \\", warning_message)"},{"question":"**Objective:** Write a Python function that compares two directories and outputs the differences in terms of unique and common files. **Details and Requirements:** 1. **Function Name:** `compare_directories` 2. **Input:** - `dir1`: The path to the first directory (string). - `dir2`: The path to the second directory (string). 3. **Output:** - A tuple containing three lists: - List of files unique to `dir1` (files present in `dir1` but not in `dir2`). - List of files unique to `dir2` (files present in `dir2` but not in `dir1`). - List of common files (files present in both `dir1` and `dir2`). 4. **Constraints:** - Only consider the filenames in the top level of the directories (do not consider subdirectories). - The comparison should be case-sensitive. 5. **Example:** ```python dir1 = \'/path/to/dir1\' dir2 = \'/path/to/dir2\' result = compare_directories(dir1, dir2) print(result) ``` Assuming `/path/to/dir1` contains [\'fileA.txt\', \'fileB.txt\', \'fileC.txt\'] and `/path/to/dir2` contains [\'fileB.txt\', \'fileD.txt\', \'fileE.txt\'], the output should be: ```python ([\'fileA.txt\', \'fileC.txt\'], [\'fileD.txt\', \'fileE.txt\'], [\'fileB.txt\']) ``` **Performance considerations:** - Ensure that the implementation can handle directories with a large number of files efficiently. **Implementation Notes:** - You may use the `os` and `os.path` modules to list files in the directories. - The `set` data structure in Python can be helpful for efficiently finding unique and common elements. Good luck!","solution":"import os def compare_directories(dir1, dir2): Compares two directories and returns the difference in terms of unique and common files. Parameters: dir1 (str): Path to the first directory. dir2 (str): Path to the second directory. Returns: tuple: A tuple containing three lists: - List of files unique to dir1. - List of files unique to dir2. - List of common files. files_in_dir1 = set(os.listdir(dir1)) files_in_dir2 = set(os.listdir(dir2)) unique_to_dir1 = list(files_in_dir1 - files_in_dir2) unique_to_dir2 = list(files_in_dir2 - files_in_dir1) common_files = list(files_in_dir1 & files_in_dir2) return (unique_to_dir1, unique_to_dir2, common_files)"},{"question":"You are tasked with creating a function to construct and manipulate an email message using the `email.message.EmailMessage` class. The function should demonstrate a comprehensive understanding of header and payload management, as well as multipart message handling. # Function Specification **Function Name:** `prepare_email` **Input:** - `from_addr` (string): The sender\'s email address. - `to_addrs` (list of strings): List of recipient email addresses. - `subject` (string): Subject of the email. - `plain_text_body` (string): Plain text content of the email. - `html_body` (string): HTML content of the email. - `attachments` (list of tuples): Each tuple contains (`filename`, `file_content`, `content_type`). **Output:** - Returns the complete email as a serialized string. **Constraints:** - The function should add the \'From\', \'To\', and \'Subject\' headers. - It needs to create a multipart email with both plain text and HTML parts. - Attachments should be included appropriately as separate parts with correct content types. - The function must handle non-ASCII characters in headers and filenames correctly. **Performance Requirements:** - Ensure efficient handling of email parts, especially for large attachments. - Proper management of MIME types and boundaries for multipart emails. # Example Usage ```python from_addr = \'sender@example.com\' to_addrs = [\'recipient1@example.com\', \'recipient2@example.com\'] subject = \'Project Update\' plain_text_body = \'Hello, please find the latest project update attached.\' html_body = \'<html><body><p>Hello, please find the latest project update attached.</p></body></html>\' attachments = [ (\'update.pdf\', b\'%PDF-1.4...\', \'application/pdf\'), (\'image.png\', b\'x89PNG...\', \'image/png\') ] email_str = prepare_email(from_addr, to_addrs, subject, plain_text_body, html_body, attachments) print(email_str) ``` # Function Implementation You need to implement the `prepare_email` function by utilizing the `email.message.EmailMessage` class and the relevant methods provided in the documentation. **Solution Template:** ```python from email.message import EmailMessage from email.policy import default def prepare_email(from_addr, to_addrs, subject, plain_text_body, html_body, attachments): # Initialize EmailMessage object msg = EmailMessage(policy=default) # Set the basic headers msg[\'From\'] = from_addr msg[\'To\'] = \', \'.join(to_addrs) msg[\'Subject\'] = subject # Create the multipart/alternative container \'part\' msg.make_alternative() # Add plain text part msg.add_alternative(plain_text_body, subtype=\'plain\') # Add HTML part msg.add_alternative(html_body, subtype=\'html\') # Add attachments for filename, file_content, content_type in attachments: msg.add_attachment(file_content, maintype=content_type.split(\'/\')[0], subtype=content_type.split(\'/\')[1], filename=filename) # Return serialized email as a string return msg.as_string() # Example usage from_addr = \'sender@example.com\' to_addrs = [\'recipient1@example.com\', \'recipient2@example.com\'] subject = \'Project Update\' plain_text_body = \'Hello, please find the latest project update attached.\' html_body = \'<html><body><p>Hello, please find the latest project update attached.</p></body></html>\' attachments = [ (\'update.pdf\', b\'%PDF-1.4...\', \'application/pdf\'), (\'image.png\', b\'x89PNG...\', \'image/png\') ] email_str = prepare_email(from_addr, to_addrs, subject, plain_text_body, html_body, attachments) print(email_str) ``` **Notes:** - Ensure the `attachments` list properly handles both binary data and content types. - Test the function with various types of content to ensure it handles all MIME types and encodings correctly.","solution":"from email.message import EmailMessage from email.policy import default def prepare_email(from_addr, to_addrs, subject, plain_text_body, html_body, attachments): Prepares an email with given inputs and returns it as a serialized string. Handles text, HTML content, and attachments appropriately. # Initialize EmailMessage object msg = EmailMessage(policy=default) # Set the basic headers msg[\'From\'] = from_addr msg[\'To\'] = \', \'.join(to_addrs) msg[\'Subject\'] = subject # Create the multipart/alternative container \'part\' msg.set_content(plain_text_body) msg.add_alternative(html_body, subtype=\'html\') # Add attachments for filename, file_content, content_type in attachments: maintype, subtype = content_type.split(\'/\', 1) msg.add_attachment(file_content, maintype=maintype, subtype=subtype, filename=filename) # Return serialized email as a string return msg.as_string()"},{"question":"# Pandas Timedelta: Task Summary **Objective**: Implement a function utilizing pandas time delta functionalities. This exercise will require handling of time delta operations, conversions, and indexing. # Problem Statement You are given a pandas DataFrame containing start and end timestamps of various events. Your task is to compute the duration of these events and perform various operations. # Given DataFrame: ```plaintext ID Start End 0 1 2023-10-01 08:00:00 2023-10-01 10:00:00 1 2 2023-10-01 11:00:00 2023-10-01 13:30:00 2 3 2023-10-01 14:00:00 2023-10-01 14:45:00 ``` # Tasks: 1. **Compute Durations**: Calculate the duration between the start and end times for each event using Timedeltas and create a new column `Duration`. 2. **Conversion**: Convert the durations from nanoseconds to minutes and create a new column `Duration_in_Minutes`. 3. **Total Duration**: Calculate and return the total duration of all events combined in the format `Days Hours:Minutes:Seconds`. 4. **Longest Event**: Return the ID of the event with the longest duration. # Constraints: - The DataFrame will not have missing `Start` or `End` values. - The `End` time will always be after the `Start` time. # Function Signature: ```python def analyze_event_durations(df: pd.DataFrame) -> (str, int): pass ``` # Input: - `df` (pandas.DataFrame) : DataFrame with columns `ID`, `Start`, and `End` containing event IDs and their respective start and end timestamps. # Output: - (str, int) : Tuple containing the total duration of all events in `Days Hours:Minutes:Seconds` format and the ID of the event with the longest duration. # Example Function Call: ```python data = { \\"ID\\": [1, 2, 3], \\"Start\\": [\\"2023-10-01 08:00:00\\", \\"2023-10-01 11:00:00\\", \\"2023-10-01 14:00:00\\"], \\"End\\": [\\"2023-10-01 10:00:00\\", \\"2023-10-01 13:30:00\\", \\"2023-10-01 14:45:00\\"] } df = pd.DataFrame(data) df[\'Start\'] = pd.to_datetime(df[\'Start\']) df[\'End\'] = pd.to_datetime(df[\'End\']) result = analyze_event_durations(df) print(result) ``` Expected Output: ```plaintext (\'0 days 05:15:00\', 2) ``` # Notes: - Students are expected to use pandas `Timedelta` and associated functionalities to solve this problem. - Emphasis will be placed on accurate calculation, conversion, and efficient manipulation of time deltas.","solution":"import pandas as pd def analyze_event_durations(df: pd.DataFrame) -> (str, int): # Calculate the durations as Timedelta df[\'Duration\'] = df[\'End\'] - df[\'Start\'] # Convert durations to minutes df[\'Duration_in_Minutes\'] = df[\'Duration\'].dt.total_seconds() / 60 # Calculate total duration of all events combined total_duration = df[\'Duration\'].sum() # Find the ID of the event with the longest duration longest_event_id = df.loc[df[\'Duration\'].idxmax(), \'ID\'] # Return the total duration as formatted string and the ID of longest event total_duration_str = str(total_duration) return total_duration_str, longest_event_id"},{"question":"# Advanced Coding Assessment Question: Seaborn Theme and Display Customization Objective: Demonstrate your understanding of seaborn\'s theme configuration and display options by customizing and plotting a dataset using specific requirements. Problem Statement: You are given a dataset containing the daily average temperatures of a city over one year. Your task is to visualize this dataset using seaborn with specific theme and display configurations applied. The dataset is provided as a CSV file named `temperatures.csv` with two columns: `date` (in `YYYY-MM-DD` format) and `temperature` (in degrees Celsius). Requirements: 1. **Load** the data from the CSV file using pandas. 2. **Create** a line plot of the temperature data using seaborn. 3. **Custom Theme**: - Set the theme such that the plot\'s background face color is set to `#f0f0f0`. - Update the theme to use the `whitegrid` style from seaborn. - Sync the theme with matplotlib\'s global state. 4. **Display Configuration**: - Change the display format to SVG. - Ensure HiDPI is disabled. - Set the display scaling to `0.5`. Constraints: - Use the functions and methods discussed in the provided documentation to implement the theme and display configurations. - Your code should be clear, well-commented, and correctly implement all the required configurations. Input Format: - The input CSV file `temperatures.csv` will be in the current working directory. Output Format: - The output should be a plot embedded within a Jupyter Notebook cell. - The plot should reflect all the specified theme and display configurations. Sample Data (`temperatures.csv`): ``` date,temperature 2022-01-01,5 2022-01-02,7 ... 2022-12-31,3 ``` Your Task: Implement the function `custom_temperature_plot()` as described above. The function should not return any value but should produce the plot as specified. ```python import pandas as pd import seaborn.objects as so from seaborn import axes_style import matplotlib as mpl def custom_temperature_plot(): # Load the data df = pd.read_csv(\'temperatures.csv\') # Configure the theme so.Plot.config.theme[\\"axes.facecolor\\"] = \\"#f0f0f0\\" so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) so.Plot.config.theme.update(mpl.rcParams) # Configure the display so.Plot.config.display[\\"format\\"] = \\"svg\\" so.Plot.config.display[\\"hidpi\\"] = False so.Plot.config.display[\\"scaling\\"] = 0.5 # Create the plot plot = so.Plot(df, x=\\"date\\", y=\\"temperature\\") plot.add(so.Line()).show() # Test the function custom_temperature_plot() ``` Provide your implementation below:","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import matplotlib as mpl def custom_temperature_plot(): # Load the data df = pd.read_csv(\'temperatures.csv\') # Configure the theme sns.set_theme(style=\\"whitegrid\\") sns.set(rc={\\"axes.facecolor\\": \\"#f0f0f0\\"}) mpl.rcParams.update(sns.plotting_context()) # Configure the display mpl.rcParams[\\"savefig.format\\"] = \\"svg\\" mpl.rcParams[\\"svg.fonttype\\"] = \\"none\\" mpl.rcParams[\\"savefig.dpi\\"] = \\"figure\\" mpl.rcParams[\\"figure.dpi\\"] = 300 # HiDPI setting off # Create the plot plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\\"date\\", y=\\"temperature\\") plt.xlabel(\\"Date\\") plt.ylabel(\\"Temperature (°C)\\") plt.title(\\"Daily Average Temperatures Over One Year\\") plt.xticks(rotation=45) plt.show()"},{"question":"Objective Write a Python class that simulates a simple file management system. This system should handle **opening**, **reading from**, and **writing to** files but with detailed exception handling mechanisms. Your class should handle and raise appropriate exceptions as mentioned in the provided documentation on Python\'s exception handling. Requirements 1. **Class Name**: `SimpleFileManager`. 2. **Methods**: - `open_file(self, file_path: str, mode: str) -> None`: Opens a file with the given path and mode. - `read_file(self) -> str`: Reads content from the currently opened file. - `write_file(self, content: str) -> None`: Writes content to the currently opened file. - `close_file(self) -> None`: Closes the currently opened file. 3. **Exception Handling**: - Handle file-related errors such as `FileNotFoundError`, `PermissionError`, `IsADirectoryError`, and `IOError`. - Custom errors for file operations when the file is not opened yet or already closed. - Ensure any resources are properly released in the event of an error. Input & Output - The `open_file` method should raise a `FileNotFoundError` if the file does not exist when opened in read mode. - The `read_file` method should raise an `IOError` if there is an issue reading the file. - The `write_file` should raise an `IOError` if there is an issue writing to the file. - The `close_file` should clean up any resources and should not raise an error if called multiple times. Constraints - You must use appropriate standard exceptions provided by Python and ensure all error cases are handled properly. - Ensure file operations do not leave files in an unusable state if an error occurs. Example ```python # Testing the SimpleFileManager class try: file_manager = SimpleFileManager() # Attempt to open a non-existing file for reading file_manager.open_file(\'non_existing_file.txt\', \'r\') except FileNotFoundError as e: print(f\\"Caught an error as expected: {e}\\") try: # Open an existing file for reading file_manager.open_file(\'existing_file.txt\', \'r\') content = file_manager.read_file() print(content) file_manager.close_file() except IOError as e: print(f\\"Error while reading the file: {e}\\") try: # Open a file for writing file_manager.open_file(\'write_test.txt\', \'w\') file_manager.write_file(\\"Hello, World!\\") file_manager.close_file() except IOError as e: print(f\\"Error while writing to the file: {e}\\") ```","solution":"class SimpleFileManager: def __init__(self): self.file = None def open_file(self, file_path: str, mode: str) -> None: if self.file is not None: raise IOError(\\"A file is already open. Close it before opening another.\\") try: self.file = open(file_path, mode) except FileNotFoundError: raise FileNotFoundError(f\\"File {file_path} not found.\\") except PermissionError: raise PermissionError(f\\"Permission denied for file {file_path}.\\") except IsADirectoryError: raise IsADirectoryError(f\\"{file_path} is a directory, not a file.\\") except IOError as e: raise IOError(f\\"An I/O error occurred: {e}\\") def read_file(self) -> str: if self.file is None: raise IOError(\\"No file is currently open for reading.\\") try: return self.file.read() except IOError: raise IOError(\\"An error occurred while reading the file.\\") def write_file(self, content: str) -> None: if self.file is None: raise IOError(\\"No file is currently open for writing.\\") try: self.file.write(content) except IOError: raise IOError(\\"An error occurred while writing to the file.\\") def close_file(self) -> None: if self.file: self.file.close() self.file = None"},{"question":"**Objective:** Your task is to implement a Python C extension that provides a function to reverse a list of strings. You should demonstrate an understanding of both parsing Python arguments and building Python return values in a C extension. **Detailed Problem Statement:** Write a Python C extension function `reverse_strings` that: 1. Accepts a single argument: a list of strings. 2. Returns a new list where each string from the input list is reversed. For simplicity, assume the input list contains only valid strings. **Input Format:** - A single list of strings **Output Format:** - A single list of strings, where each string is reversed **Example:** ```python # Python side usage import example # assuming the extension module is named example result = example.reverse_strings([\'hello\', \'world\']) print(result) # Output: [\'olleh\', \'dlrow\'] ``` **Constraints:** - The input list will contain only non-empty strings. - The input list will contain no more than 1000 strings. - Each string will be no more than 100 characters long. **Performance Requirements:** - The implemented function should handle parsing and processing of the input efficiently, and constructing the output in an optimal manner. You need to implement the C function for the Python extension and the corresponding setup code to compile the extension. ```c // examplemodule.c #include <Python.h> // Function to reverse a single string void reverse(char* str) { int n = strlen(str); for (int i = 0; i < n / 2; i++) { char temp = str[i]; str[i] = str[n - i - 1]; str[n - i - 1] = temp; } } // The reverse_strings function static PyObject* reverse_strings(PyObject* self, PyObject* args) { PyObject* inputList; PyObject* resultList; PyObject* item; Py_ssize_t listSize; char* str; // Parse the input tuple (args) containing a single argument as a list of strings if (!PyArg_ParseTuple(args, \\"O\\", &inputList)) { return NULL; } if (!PyList_Check(inputList)) { PyErr_SetString(PyExc_TypeError, \\"parameter must be a list\\"); return NULL; } listSize = PyList_Size(inputList); // Create a new list to store the result resultList = PyList_New(listSize); if (!resultList) { return NULL; } // Process each item in the list for (Py_ssize_t i = 0; i < listSize; i++) { item = PyList_GetItem(inputList, i); // borrowed reference if (!PyUnicode_Check(item)) { Py_DECREF(resultList); PyErr_SetString(PyExc_TypeError, \\"all items in the list must be strings\\"); return NULL; } // Convert Python string to C string str = PyUnicode_AsUTF8(item); if (str == NULL) { Py_DECREF(resultList); return NULL; } // Duplicate the string so we can reverse it char* reversedStr = strdup(str); if (!reversedStr) { Py_DECREF(resultList); PyErr_SetString(PyExc_MemoryError, \\"memory allocation failed\\"); return NULL; } // Reverse the string reverse(reversedStr); // Create a new Python string from the reversed C string PyObject* reversedItem = PyUnicode_FromString(reversedStr); free(reversedStr); if (!reversedItem) { Py_DECREF(resultList); return NULL; } // Set the reversed string in the result list PyList_SetItem(resultList, i, reversedItem); // Steals reference to reversedItem } return resultList; } // Module method table static PyMethodDef ExampleMethods[] = { {\\"reverse_strings\\", reverse_strings, METH_VARARGS, \\"Reverse a list of strings\\"}, {NULL, NULL, 0, NULL} }; // Module definition static struct PyModuleDef examplemodule = { PyModuleDef_HEAD_INIT, \\"example\\", \\"Example module that reverses a list of strings.\\", -1, ExampleMethods }; // Module initialization function PyMODINIT_FUNC PyInit_example(void) { return PyModule_Create(&examplemodule); } ``` **Setup Script (setup.py):** ```python from setuptools import setup, Extension module = Extension(\'example\', sources=[\'examplemodule.c\']) setup(name=\'example\', version=\'1.0\', description=\'Example module that reverses a list of strings\', ext_modules=[module]) ``` Compile and test your module using the following commands: 1. Run `python setup.py build_ext --inplace` to compile the extension. 2. Test the function with Python to ensure it works as expected.","solution":"def reverse_strings(strings): This function takes a list of strings and returns a new list where each string is reversed. return [s[::-1] for s in strings]"},{"question":"**Objective**: You are required to write a Python function using the `xml.dom.pulldom` module to parse an XML string. The function will filter and extract specific elements based on their attributes. **Problem Statement**: Given an XML string representing a collection of books, write a function to extract the titles of books published after a certain year. **Function Signature**: ```python def get_titles_after_year(xml_string: str, year: int) -> list[str]: pass ``` **Input**: - `xml_string`: A string representation of an XML document where each book is represented as an `<book>` element. - `year`: An integer representing the year threshold. **Output**: - A list of strings, where each string is the title (`<title>` element) of a book published after the specified year. **Constraints**: 1. Each book element will have a `year` attribute and a `title` child element. 2. Year values are valid integers. 3. The XML is well-formed and does not contain malicious content. **Example**: ```python xml_input = \'\'\' <library> <book year=\\"2001\\"><title>Book One</title></book> <book year=\\"1999\\"><title>Book Two</title></book> <book year=\\"2005\\"><title>Book Three</title></book> </library> \'\'\' print(get_titles_after_year(xml_input, 2000)) # Output: [\'Book One\', \'Book Three\'] ``` **Detailed Requirements**: 1. Use the `xml.dom.pulldom` library\'s `parseString` function to parse the XML string. 2. Iterate through the parsed events and process only the elements relevant to the task. 3. Utilize the `expandNode` method where appropriate to retrieve the text content. **Performance**: - Ensure efficient parsing by only expanding nodes when necessary. - Handle large XML strings within reasonable time and memory constraints.","solution":"from xml.dom import pulldom def get_titles_after_year(xml_string: str, year: int) -> list[str]: Parses the given XML string and extracts titles of books published after the given year. Args: xml_string (str): A string representation of an XML document. year (int): The year threshold. Returns: list[str]: A list of book titles published after the given year. doc = pulldom.parseString(xml_string) titles = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'book\': book_year = int(node.getAttribute(\'year\')) if book_year > year: doc.expandNode(node) title_elements = node.getElementsByTagName(\'title\') if title_elements: title_text = title_elements[0].firstChild.nodeValue titles.append(title_text) return titles"},{"question":"Weather Simulation You are tasked with simulating daily weather patterns for a small town over a year (365 days) using the `random` module. The simulation will include several weather parameters such as temperature, precipitation, and wind speed. **Requirements:** 1. **Temperature (`temp`)**: Simulate daily temperatures using a normal distribution. The mean temperature for the town is 15°C with a standard deviation of 10°C. Temperatures should be realistic (e.g., between -10°C and 40°C). 2. **Precipitation (`precip`)**: Simulate daily precipitation using an exponential distribution. The mean daily precipitation is 5 mm. Precipitation values should be non-negative. 3. **Wind Speed (`wind_speed`)**: Simulate daily wind speed using a Weibull distribution. The shape parameter (`alpha`) for wind speed is 2.0, and the scale parameter (`beta`) is 5.0. Wind speed values should be non-negative. 4. **Weather Type (`weather_type`)**: Randomly choose the weather type for each day from the following list: `[\'sunny\', \'cloudy\', \'rainy\', \'snowy\']`. The choices should be weighted as follows: - Sunny: 50% - Cloudy: 30% - Rainy: 15% - Snowy: 5% **Function Signature:** ```python def simulate_weather(n_days: int) -> List[Dict[str, Any]]: pass ``` **Input:** - `n_days (int)`: The number of days to simulate. For this problem, it will be 365. **Output:** - Return a list of dictionaries. Each dictionary should contain the simulation results for a single day with the following fields: - `\'day\'`: The day number (1 to `n_days`). - `\'temp\'`: The simulated temperature for the day. - `\'precip\'`: The simulated precipitation for the day. - `\'wind_speed\'`: The simulated wind speed for the day. - `\'weather_type\'`: The simulated weather type for the day. **Example:** ```python weather_data = simulate_weather(3) for day in weather_data: print(day) # Possible Output: # {\'day\': 1, \'temp\': 17.3, \'precip\': 0.3, \'wind_speed\': 4.1, \'weather_type\': \'sunny\'} # {\'day\': 2, \'temp\': 13.0, \'precip\': 0.0, \'wind_speed\': 7.2, \'weather_type\': \'cloudy\'} # {\'day\': 3, \'temp\': -1.2, \'precip\': 12.3, \'wind_speed\': 5.4, \'weather_type\': \'snowy\'} ``` **Notes:** - Ensure that temperature, precipitation, and wind speed are realistic and strictly follow their respective distributions. - Use appropriate functions from the `random` module to generate the required distributions. - Convert any unrealistic simulated values to fall within acceptable ranges. - Use the provided weights when randomly choosing the weather type.","solution":"import random from typing import List, Dict, Any import numpy as np def simulate_weather(n_days: int) -> List[Dict[str, Any]]: weather_patterns = [] # Weather type choice and weights weather_types = [\'sunny\', \'cloudy\', \'rainy\', \'snowy\'] weather_weights = [0.5, 0.3, 0.15, 0.05] for day in range(1, n_days + 1): # Simulate temperature temp = np.random.normal(loc=15, scale=10) temp = max(-10, min(temp, 40)) # Restrict to between -10°C and 40°C # Simulate precipitation precip = np.random.exponential(scale=5) precip = max(0, precip) # Restrict to non-negative values # Simulate wind speed wind_speed = np.random.weibull(a=2.0) * 5 wind_speed = max(0, wind_speed) # Restrict to non-negative values # Simulate weather type weather_type = random.choices(weather_types, weights=weather_weights, k=1)[0] weather_patterns.append({ \'day\': day, \'temp\': temp, \'precip\': precip, \'wind_speed\': wind_speed, \'weather_type\': weather_type }) return weather_patterns"},{"question":"# Coding Challenge: Implement a Data Manipulation Utility Objective: You are tasked with creating a versatile data manipulation function using the `operator` module in Python. Problem Statement: Write a function named `process_dictionary` that takes in a dictionary and a list of operations. Each operation is represented by a tuple, where the first element is the operation name (a string) and the remaining elements are the arguments required for that operation. Your function should execute these operations in the order they are provided. The dictionary can contain nested dictionaries and lists. The operations you need to support include: 1. `\\"set\\"`: Set a key in the dictionary to a provided value. 2. `\\"delete\\"`: Delete a key from the dictionary. 3. `\\"increment\\"`: Increment a numerical value of a given key. 4. `\\"concat\\"`: Concatenate a string to the value of a given key. 5. `\\"getitem\\"`: Retrieve a value from a list at a provided index. Use the appropriate functions from the `operator` module to implement these operations. # Function Signature ```python def process_dictionary(data: dict, operations: list) -> dict: pass ``` # Input - `data`: A dictionary that can contain nested dictionaries and lists. - `operations`: A list of operations, where each operation is represented as a tuple. # Output - Return the modified dictionary after performing all the operations in sequence. # Examples ```python data = { \'a\': 5, \'b\': {\'c\': 10, \'d\': \'hello\'}, \'e\': [1, 2, 3] } operations = [ (\'set\', \'f\', \'world\'), # Adds a new key \'f\' with value \'world\' (\'increment\', \'a\', 2), # Increments the value of \'a\' by 2 (\'concat\', \'b.d\', \' world!\'), # Concatenates \' world!\' to the value of \'b.d\' (\'delete\', \'e\'), # Deletes the key \'e\' from the dictionary ] expected_output = { \'a\': 7, \'b\': {\'c\': 10, \'d\': \'hello world!\'}, \'f\': \'world\' } assert process_dictionary(data, operations) == expected_output ``` **Notes**: - Ensure you handle nested dictionaries and list indexing appropriately. - Raise a `KeyError` if attempting to delete or increment a non-existing key. - Raise a `TypeError` if operations are performed on types not supporting them (e.g., concatenating a string to an int). # Constraints - The dictionary keys are strings. - The values are either integers, strings, dictionaries, or lists.","solution":"import operator def process_dictionary(data: dict, operations: list) -> dict: def get_nested(d, keys): for key in keys: d = operator.getitem(d, key) return d def set_nested(d, keys, value): for key in keys[:-1]: d = operator.getitem(d, key) operator.setitem(d, keys[-1], value) modified_data = data.copy() for operation in operations: op = operation[0] if op == \\"set\\": keys = operation[1].split(\'.\') value = operation[2] set_nested(modified_data, keys, value) elif op == \\"delete\\": keys = operation[1].split(\'.\') nested = get_nested(modified_data, keys[:-1]) operator.delitem(nested, keys[-1]) elif op == \\"increment\\": keys = operation[1].split(\'.\') nested = get_nested(modified_data, keys[:-1]) operator.setitem(nested, keys[-1], operator.iadd(operator.getitem(nested, keys[-1]), operation[2])) elif op == \\"concat\\": keys = operation[1].split(\'.\') nested = get_nested(modified_data, keys[:-1]) operator.setitem(nested, keys[-1], operator.concat(operator.getitem(nested, keys[-1]), operation[2])) elif op == \\"getitem\\": keys = operation[1].split(\'.\') index = operation[2] nested = get_nested(modified_data, keys) return operator.getitem(nested, index) return modified_data"},{"question":"Objective To assess your understanding of Python basics including arithmetic operations, string manipulations, list operations, and control structures. Problem Statement You are to write a Python function that processes a list of student names and their scores in several subjects, performs some calculations, and formats output strings. Description You are given a list of tuples where each tuple contains a student\'s name (a string) and a list of their scores in three subjects (all integers). Your task is to write a function `process_student_data` that performs the following operations: 1. Calculate and append the average score of each student to their respective tuples. 2. Sort the list of students based on their average scores in descending order. 3. Return a formatted string containing each student\'s name and their average score, where each student\'s details are on a new line. The string should list students from those with the highest average score to those with the lowest. If several students have the same average score, they should remain in their original order from the input list. Input - A list of tuples, where each tuple contains: - A student\'s name: a string. - A list of three integer scores: `[score1, score2, score3]`. Output - A single string formatted as described above. Constraints - All scores are between 0 and 100 inclusive. - The input list contains at least one student. - The names are non-empty strings containing only alphabetic characters and spaces. Example ```python def process_student_data(student_data): # Your implementation here input_data = [ (\\"Alice\\", [75, 85, 90]), (\\"Bob\\", [80, 80, 80]), (\\"Clara\\", [90, 85, 90]), ] output = process_student_data(input_data) print(output) ``` Expected Output: ``` Clara: 88.33 Alice: 83.33 Bob: 80.00 ``` *Note*: The average scores in the output should be formatted to two decimal places. Requirements - Format the average score to two decimal places. - Maintain the order of students with the same average score as in the input list. Function Signature ```python def process_student_data(student_data: list) -> str: pass ``` **Good luck, and happy coding!**","solution":"def process_student_data(student_data): Process the list of student data to append average scores, sort the students, and format the output string. Args: student_data (list): A list of tuples, where each tuple contains a student\'s name and a list of their three integer scores. Returns: str: A formatted string listing each student\'s name and their average score, with the students sorted by their average scores in descending order. # Calculate and append average scores to each tuple processed_data = [(name, scores, sum(scores) / len(scores)) for name, scores in student_data] # Sort the data based on average score in descending order processed_data.sort(key=lambda x: x[2], reverse=True) # Format the output string output = \\"n\\".join([f\\"{name}: {avg_score:.2f}\\" for name, scores, avg_score in processed_data]) return output"},{"question":"# Question: Implement Custom Encoding Conversion Functions In this task, you are expected to demonstrate your understanding of the `binascii` module by implementing custom functions for binary-to-ASCII and ASCII-to-binary conversions using various encoding schemes. Requirements: 1. **UU Encoding and Decoding:** - Implement `uu_encode(data: bytes, backtick: bool = False) -> str`: Converts binary data to a UU-encoded ASCII string. - Implement `uu_decode(ascii_str: str) -> bytes`: Converts a UU-encoded ASCII string back to binary data. 2. **Base64 Encoding and Decoding:** - Implement `base64_encode(data: bytes, newline: bool = True) -> str`: Converts binary data to a base64-encoded ASCII string. - Implement `base64_decode(ascii_str: str) -> bytes`: Converts a base64-encoded ASCII string back to binary data. 3. **Quoted-Printable Encoding and Decoding:** - Implement `qp_encode(data: bytes, quotetabs: bool = False, istext: bool = True, header: bool = False) -> str`: Converts binary data to a quoted-printable encoded ASCII string. - Implement `qp_decode(ascii_str: str, header: bool = False) -> bytes`: Converts a quoted-printable ASCII string back to binary data. 4. **Hexadecimal Encoding and Decoding:** - Implement `hex_encode(data: bytes, sep: Union[str, None] = None, bytes_per_sep: int = 1) -> str`: Converts binary data to a hexadecimal-encoded ASCII string. - Implement `hex_decode(ascii_str: str) -> bytes`: Converts a hexadecimal-encoded ASCII string back to binary data. Constraints: - Assume all input binary data and ASCII strings are valid and correctly formatted for their respective encoding schemes. - Your implementations should use functions from the `binascii` module wherever applicable. Input/Output Format: - `uu_encode(data: bytes, backtick: bool = False) -> str` - Input: `b\'hello\'` - Output: `\'begin 666 x00your_uu_encoded_string_heren x00n\'` - `uu_decode(ascii_str: str) -> bytes` - Input: `\'begin 666 x00your_uu_encoded_string_heren x00n\'` - Output: `b\'hello\'` - `base64_encode(data: bytes, newline: bool = True) -> str` - Input: `b\'hello\'` - Output: `\'aGVsbG8=n\'` - `base64_decode(ascii_str: str) -> bytes` - Input: `\'aGVsbG8=n\'` - Output: `b\'hello\'` - `qp_encode(data: bytes, quotetabs: bool = False, istext: bool = True, header: bool = False) -> str` - Input: `b\'hello\'` - Output: `\'hello\'` - `qp_decode(ascii_str: str, header: bool = False) -> bytes` - Input: `\'hello\'` - Output: `b\'hello\'` - `hex_encode(data: bytes, sep: Union[str, None] = None, bytes_per_sep: int = 1) -> str` - Input: `b\'hello\'` - Output: `\'68656c6c6f\'` - `hex_decode(ascii_str: str) -> bytes` - Input: `\'68656c6c6f\'` - Output: `b\'hello\'` Write your implementations below each function stub provided. ```python import binascii from typing import Union def uu_encode(data: bytes, backtick: bool = False) -> str: pass def uu_decode(ascii_str: str) -> bytes: pass def base64_encode(data: bytes, newline: bool = True) -> str: pass def base64_decode(ascii_str: str) -> bytes: pass def qp_encode(data: bytes, quotetabs: bool = False, istext: bool = True, header: bool = False) -> str: pass def qp_decode(ascii_str: str, header: bool = False) -> bytes: pass def hex_encode(data: bytes, sep: Union[str, None] = None, bytes_per_sep: int = 1) -> str: pass def hex_decode(ascii_str: str) -> bytes: pass ``` Ensure that your solutions handle the specified encoding and decoding functionalities by leveraging the appropriate functions from the `binascii` module.","solution":"import binascii from typing import Union def uu_encode(data: bytes, backtick: bool = False) -> str: return binascii.b2a_uu(data).decode() def uu_decode(ascii_str: str) -> bytes: return binascii.a2b_uu(ascii_str.encode()) def base64_encode(data: bytes, newline: bool = True) -> str: encoded_data = binascii.b2a_base64(data).decode() return encoded_data if newline else encoded_data.strip() def base64_decode(ascii_str: str) -> bytes: return binascii.a2b_base64(ascii_str.encode()) def qp_encode(data: bytes, quotetabs: bool = False, istext: bool = True, header: bool = False) -> str: return binascii.b2a_qp(data, quotetabs=quotetabs, istext=istext, header=header).decode() def qp_decode(ascii_str: str, header: bool = False) -> bytes: return binascii.a2b_qp(ascii_str.encode(), header=header) def hex_encode(data: bytes, sep: Union[str, None] = None, bytes_per_sep: int = 1) -> str: hex_data = binascii.b2a_hex(data).decode() if sep is not None: hex_data = sep.join([hex_data[i:i + 2] for i in range(0, len(hex_data), 2)]) return hex_data def hex_decode(ascii_str: str) -> bytes: if \' \' in ascii_str or \'-\' in ascii_str: ascii_str = ascii_str.replace(\' \', \'\').replace(\'-\', \'\') return binascii.a2b_hex(ascii_str.encode())"},{"question":"**Objective:** You are required to implement a Python function that leverages the `pathlib` module to perform a series of filesystem operations. This question aims to assess your understanding of `pathlib` and your ability to manipulate paths and interact with the filesystem efficiently. **Problem Statement:** You are tasked to implement the function `find_and_replace_files(base_dir: str, file_extension: str, search_text: str, replace_text: str) -> None` which should perform the following operations: 1. **Traverse the directory tree** starting from `base_dir` and **find all files** with a given `file_extension`. 2. For each found file, **open and read** its content. 3. **Replace all instances** of `search_text` with `replace_text` in the file content. 4. **Write the modified content back** to the respective files. 5. **Print the relative paths** of all files that were modified. If no files were modified, print \\"No files modified\\". **Function Signature:** ```python def find_and_replace_files(base_dir: str, file_extension: str, search_text: str, replace_text: str) -> None: pass ``` **Inputs:** - `base_dir` (str): The base directory from which to start the traversal. - `file_extension` (str): The file extension to look for (e.g., \\".txt\\"). - `search_text` (str): The text to search for in the files. - `replace_text` (str): The text that will replace the search text. **Outputs:** - The function does not return anything. Instead, it directly modifies the files and prints the relative paths of modified files. **Constraints:** - The directory tree may contain many files and nested directories, so efficiency of traversal is important. - The function should handle large files efficiently. - Assume all file reads and writes are within the scope of the given permissions. - If the path does not exist or is invalid, handle it gracefully without raising exceptions. **Example Usage:** ```python # Suppose the directory structure is as follows: # /example # ├── a.txt # ├── b.txt # ├── folder1 # │ ├── c.txt # │ └── d.md # ├── folder2 # │ └── e.txt # And \'a.txt\' contains \\"hello world\\", \'b.txt\' contains \\"hello\\", \'c.txt\' contains \\"world\\" # Calling: find_and_replace_files(\\"/example\\", \\".txt\\", \\"hello\\", \\"hi\\") # Should modify \'a.txt\', \'b.txt\', but not \'c.txt\' and should print: # a.txt # b.txt ``` **Note:** - You must make use of the `pathlib` module to handle path manipulations and filesystem interactions. - Carefully handle edge cases such as non-existent files, empty files, and directories without any matching files.","solution":"from pathlib import Path def find_and_replace_files(base_dir: str, file_extension: str, search_text: str, replace_text: str) -> None: base_path = Path(base_dir) if not base_path.exists(): print(\\"Invalid base directory.\\") return modified_files = [] for file_path in base_path.rglob(f\'*{file_extension}\'): with file_path.open(\'r\', encoding=\'utf-8\') as file: content = file.read() new_content = content.replace(search_text, replace_text) if new_content != content: with file_path.open(\'w\', encoding=\'utf-8\') as file: file.write(new_content) modified_files.append(file_path.relative_to(base_path)) if modified_files: for file in modified_files: print(file) else: print(\\"No files modified\\")"},{"question":"# Advanced Coding Assessment Question: Implementing Custom Scaled Dot-Product Attention Objective Your task is to implement a custom Scaled Dot-Product Attention mechanism from scratch using PyTorch. This exercise will assess your understanding of the fundamental components and computations involved in attention mechanisms, which are crucial for many advanced neural network architectures. Requirements 1. **Function Name**: `scaled_dot_product_attention` 2. **Inputs**: - `query` (Tensor of shape `(batch_size, seq_len, d_k)`): Query tensor from the query projection layer. - `key` (Tensor of shape `(batch_size, seq_len, d_k)`): Key tensor from the key projection layer. - `value` (Tensor of shape `(batch_size, seq_len, d_v)`): Value tensor from the value projection layer. - `mask` (Tensor of shape `(batch_size, seq_len, seq_len)`, optional): Attention mask tensor to prevent attending to certain positions, useful for masked self-attention in transformers. 3. **Outputs**: - `output` (Tensor of shape `(batch_size, seq_len, d_v)`): The attended output tensor obtained after applying the scaled dot-product attention mechanism. 4. **Constraints**: - Use only PyTorch without any high-level API calls that implement attention directly. - Implement the dot-product computation manually. - Apply the appropriate scaling factor. - Apply the softmax function to get attention weights. - Apply the mask (if provided) correctly to the attention scores before applying softmax. Detailed Description 1. Calculate the dot product between the query and key tensors. 2. Scale the results by the square root of the dimension of the key vectors. 3. Optionally mask some entries in the scaled scores before applying softmax. 4. Apply the softmax function to get the attention weights. 5. Compute the attention output as the weighted sum of the value tensors. Example ```python import torch def scaled_dot_product_attention(query, key, value, mask=None): d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) if mask is not None: scores = scores.masked_fill(mask == 0, float(\'-inf\')) attention_weights = torch.nn.functional.softmax(scores, dim=-1) output = torch.matmul(attention_weights, value) return output # Example usage batch_size = 1 seq_len = 4 d_k = d_v = 8 query = torch.rand((batch_size, seq_len, d_k)) key = torch.rand((batch_size, seq_len, d_k)) value = torch.rand((batch_size, seq_len, d_v)) mask = torch.ones((batch_size, seq_len, seq_len)) # No actual masking in this example output = scaled_dot_product_attention(query, key, value, mask) print(output) ``` Note Ensure that your implementation can handle different batch sizes, sequence lengths, and dimensions for the key and value vectors.","solution":"import torch def scaled_dot_product_attention(query, key, value, mask=None): Computes the scaled dot-product attention. query: Tensor of shape (batch_size, seq_len, d_k) key: Tensor of shape (batch_size, seq_len, d_k) value: Tensor of shape (batch_size, seq_len, d_v) mask: Optional Tensor of shape (batch_size, seq_len, seq_len) Returns: Tensor of shape (batch_size, seq_len, d_v) d_k = query.size(-1) # Dimension of the key/query scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) if mask is not None: scores = scores.masked_fill(mask == 0, float(\'-inf\')) attention_weights = torch.nn.functional.softmax(scores, dim=-1) output = torch.matmul(attention_weights, value) return output"},{"question":"# SVM Classification and Evaluation in scikit-learn **Objective**: Implement an SVM classifier using scikit-learn, train it on the given dataset, and evaluate its performance using appropriate metrics. **Task**: 1. **Load the Dataset**: Load the Iris dataset using `sklearn.datasets.load_iris`. 2. **Preprocess Data**: Scale the features to have zero mean and unit variance. 3. **SVM Classifier**: - Implement an SVM classifier using the `SVC` class from scikit-learn. - Use an RBF kernel for the classifier. - Implement a grid search with cross-validation to find the best hyperparameters for `C` and `gamma`. 4. **Evaluation**: - Split the data into training and test sets using an 80-20 split. - Train the SVM classifier on the training data and evaluate its performance on the test data. - Calculate and print the accuracy, precision, recall, and F1-score of the classifier. - Display the confusion matrix for the test set predictions. 5. **Visualize Decision Boundaries**: - Plot the decision boundaries of the classifier for visualization purposes, considering only the first two features of the dataset for a 2D plot. # Implementation Details 1. **Loading and Preprocessing**: ```python from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split # Load Iris dataset iris = load_iris() X, y = iris.data, iris.target # Scale features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split data into training and test sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) ``` 2. **SVM Classifier with Grid Search**: ```python from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV # Define the SVM classifier svm = SVC(kernel=\'rbf\') # Define parameter grid for C and gamma param_grid = { \'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001] } # Implement grid search with cross-validation grid_search = GridSearchCV(svm, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_train, y_train) # Best parameters found by grid search best_params = grid_search.best_params_ print(f\\"Best parameters: {best_params}\\") ``` 3. **Evaluation**: ```python from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix # Train the SVM classifier with the best parameters best_svm = grid_search.best_estimator_ best_svm.fit(X_train, y_train) # Predict on the test set y_pred = best_svm.predict(X_test) # Calculate and print evaluation metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"F1-Score: {f1}\\") # Confusion matrix cm = confusion_matrix(y_test, y_pred) print(\\"Confusion Matrix:\\") print(cm) ``` 4. **Visualize Decision Boundaries**: ```python import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap def plot_decision_boundaries(X, y, model): x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\'k\', marker=\'o\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.title(\'Decision Boundaries\') plt.show() # Consider only first two features for visualization X_train_2d = X_train[:, :2] X_test_2d = X_test[:, :2] best_svm.fit(X_train_2d, y_train) plot_decision_boundaries(X_test_2d, y_test, best_svm) ``` **Input**: - None (load the Iris dataset directly in the code). **Output**: - Print the best hyperparameters found by grid search. - Print the accuracy, precision, recall, and F1-score of the classifier. - Print the confusion matrix. - Display the decision boundaries plot. **Constraints**: - Ensure that the grid search process does not take exceedingly long to execute by limiting the grid size. - Use appropriate evaluation metrics to provide a comprehensive performance overview.","solution":"from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap def load_and_preprocess_data(): # Load Iris dataset iris = load_iris() X, y = iris.data, iris.target # Scale features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split data into training and test sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def train_svm_classifier(X_train, y_train): # Define the SVM classifier svm = SVC(kernel=\'rbf\') # Define parameter grid for C and gamma param_grid = { \'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001] } # Implement grid search with cross-validation grid_search = GridSearchCV(svm, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_train, y_train) # Best parameters found by grid search best_params = grid_search.best_params_ best_svm = grid_search.best_estimator_ return best_svm, best_params def evaluate_classifier(best_svm, X_test, y_test): # Predict on the test set y_pred = best_svm.predict(X_test) # Calculate evaluation metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') cm = confusion_matrix(y_test, y_pred) return accuracy, precision, recall, f1, cm def plot_decision_boundaries(X, y, model): x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\'k\', marker=\'o\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.title(\'Decision Boundaries\') plt.show() if __name__ == \\"__main__\\": X_train, X_test, y_train, y_test = load_and_preprocess_data() best_svm, best_params = train_svm_classifier(X_train, y_train) accuracy, precision, recall, f1, cm = evaluate_classifier(best_svm, X_test, y_test) print(f\\"Best parameters: {best_params}\\") print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"F1-Score: {f1}\\") print(\\"Confusion Matrix:\\") print(cm) # Consider only first two features for visualization X_train_2d = X_train[:, :2] X_test_2d = X_test[:, :2] best_svm.fit(X_train_2d, y_train) plot_decision_boundaries(X_test_2d, y_test, best_svm)"},{"question":"# Seaborn Plotting and Jitter Adjustment Objective: Create scatter plots using the Seaborn `so.Plot` module on a different dataset (`iris`). Adjust the jitter using `width`, `x`, and `y` parameters to demonstrate your understanding of the `Jitter` transform. Dataset: Use the `iris` dataset, which contains the following columns: - `sepal_length` - `sepal_width` - `petal_length` - `petal_width` - `species` Task: 1. **Load the Iris Dataset**: - Import the dataset using `seaborn.load_dataset(\\"iris\\")`. 2. **Create a Vertical Jitter Plot**: - Create a scatter plot of `species` (x-axis) vs `sepal_length` (y-axis). - Add jitter along the vertical axis (corresponding to `sepal_length`) with a moderate amount (`width=0.3`). 3. **Create a Horizontal Jitter Plot**: - Create a scatter plot of `sepal_length` (x-axis) vs `sepal_width` (y-axis). - Add jitter along the horizontal axis (corresponding to `sepal_length`) with a significant amount (`width=0.5`). 4. **Create a Symmetric Jitter Plot**: - Create a scatter plot of `petal_length` (x-axis) vs `petal_width` (y-axis). - Apply jitter on both axes with specific amounts for each axis (`x=0.2`, `y=0.1`). Requirements: - Write a function `create_plots()` that loads the dataset and generates all three plots described above. - Ensure that the jitter is applied correctly, as specified. - Display each plot one after another in the function. Constraints: - You must use the `seaborn.objects` module and the `so.Jitter` transformation for plotting. - Do not use any other plotting library or methods for this task. Example Output: The function `create_plots()` should generate three plots: 1. A vertical jitter plot with jitter applied to `sepal_length`. 2. A horizontal jitter plot with jitter applied to `sepal_length`. 3. A symmetric jitter plot with jitter applied to both `petal_length` and `petal_width`. ```python import seaborn.objects as so from seaborn import load_dataset def create_plots(): # Load the iris dataset iris = load_dataset(\\"iris\\") # Vertical jitter plot ( so.Plot(iris, \\"species\\", \\"sepal_length\\") .add(so.Dots(), so.Jitter(width=0.3)) .show() ) # Horizontal jitter plot ( so.Plot(iris, \\"sepal_length\\", \\"sepal_width\\") .add(so.Dots(), so.Jitter(width=0.5)) .show() ) # Symmetric jitter plot ( so.Plot(iris, \\"petal_length\\", \\"petal_width\\") .add(so.Dots(), so.Jitter(x=0.2, y=0.1)) .show() ) # Call the function to generate and display the plots create_plots() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def create_plots(): # Load the iris dataset iris = load_dataset(\\"iris\\") # Vertical jitter plot ( so.Plot(iris, x=\\"species\\", y=\\"sepal_length\\") .add(so.Dots(), so.Jitter(width=0.3)) .show() ) # Horizontal jitter plot ( so.Plot(iris, x=\\"sepal_length\\", y=\\"sepal_width\\") .add(so.Dots(), so.Jitter(width=0.5)) .show() ) # Symmetric jitter plot ( so.Plot(iris, x=\\"petal_length\\", y=\\"petal_width\\") .add(so.Dots(), so.Jitter(x=0.2, y=0.1)) .show() )"},{"question":"**Objective:** Utilize the `importlib.metadata` package to create a function that lists detailed information about all installed packages, including their name, version, and required dependencies. **Problem Statement:** Write a Python function `get_installed_packages_info()` that returns a list of dictionaries. Each dictionary should contain the following information about an installed package: 1. `name`: The name of the package. 2. `version`: The version of the package. 3. `dependencies`: A list of packages required by this package. To implement this function, you will use the `importlib.metadata` module to extract information about each installed package. **Function Signature:** ```python def get_installed_packages_info() -> List[Dict[str, Union[str, List[str]]]]: pass ``` **Expected Input and Output:** The function does not take any input parameters. The function returns a list of dictionaries. Each dictionary has the following structure: ```python { \\"name\\": \\"package_name\\", \\"version\\": \\"package_version\\", \\"dependencies\\": [\\"dependency1\\", \\"dependency2\\", ...] } ``` **Example:** ```python # Example output for illustrative purposes [ { \\"name\\": \\"wheel\\", \\"version\\": \\"0.32.3\\", \\"dependencies\\": [] }, { \\"name\\": \\"setuptools\\", \\"version\\": \\"57.0.0\\", \\"dependencies\\": [\\"certifi (>=2017.4.17)\\", \\"pyparsing (>=2.0.2)\\"] }, ... ] ``` **Constraints:** - Assume that the `importlib.metadata` package is available in the environment where the code will run. - The function should handle cases where a package has no dependencies. **Notes:** - Utilize the `packages_distributions()`, `metadata()`, `version()`, and `requires()` functions from `importlib.metadata`. - Ensure to handle any exceptions that may arise from querying package metadata. **Performance Requirements:** - The solution should be efficient and able to handle environments with a large number of installed packages. **Hints:** - You can iterate over the `packages_distributions()` dictionary to get package names and their corresponding distributions. - Use `version()` to get the version of each package. - Use `requires()` to get the list of dependencies for each package. If a package has no dependencies, ensure to return an empty list.","solution":"from typing import List, Dict, Union import importlib.metadata def get_installed_packages_info() -> List[Dict[str, Union[str, List[str]]]]: packages_info = [] for dist in importlib.metadata.distributions(): name = dist.metadata[\'Name\'] version = dist.version dependencies = dist.requires or [] # Parsing out only the package names from dependencies if exists dependencies_list = [dep.split(\' \')[0] for dep in dependencies] packages_info.append({ \\"name\\": name, \\"version\\": version, \\"dependencies\\": dependencies_list }) return packages_info"},{"question":"# Advanced Coding Assessment Question: Event Scheduler Customization Objective The goal of this question is to implement a custom event scheduler that can handle recurring events using the `sched` module in Python. The students are required to create a function that schedules events to recur at specified intervals. Problem Statement Implement a Python function `schedule_recurring_events(start_time, interval, repetitions, action, argument=(), kwargs={})` which schedules a recurring event that occurs multiple times at regular intervals. You are to use the `sched` module to achieve this. # Function Signature ```python def schedule_recurring_events(start_time: float, interval: float, repetitions: int, action: callable, argument=(), kwargs={}): pass ``` # Input - `start_time`: A float representing the start time (in seconds since the epoch). - `interval`: A float representing the time interval (in seconds) between each occurrence of the event. - `repetitions`: An integer representing the number of times the event should be scheduled. - `action`: A callable to be executed. - `argument`: A tuple containing the positional arguments for the action (default is an empty tuple). - `kwargs`: A dictionary containing the keyword arguments for the action (default is an empty dictionary). # Output - The function does not need to return anything. It should schedule the events and start the scheduler to run them. # Constraints - The `start_time` should be a non-negative float. - The `interval` should be a positive float. - The `repetitions` should be a non-negative integer. - The `action` callable should be a valid callable function. # Requirements - Make sure that events are scheduled at the correct times, starting from `start_time` and recurring every `interval` seconds, for a total of `repetitions` times. - Leverage the `sched` module to handle the scheduling. - The function should handle edge cases such as zero repetitions gracefully (i.e., it should not schedule any event if repetitions are zero). # Example ```python import time def print_message(message): print(f\\"{time.time()}: {message}\\") start_time = time.time() + 5 # 5 seconds from now interval = 3 # Every 3 seconds repetitions = 4 # Repeat 4 times schedule_recurring_events(start_time, interval, repetitions, print_message, argument=(\\"Hello, world!\\",)) ``` Expected Output (approximate): ``` <start_time>: Hello, world! <start_time + 3>: Hello, world! <start_time + 6>: Hello, world! <start_time + 9>: Hello, world! ``` Notes - Ensure your implementation effectively utilizes the `sched` module and appropriately schedules events to recur. - Consider different scenarios and edge cases in your implementation.","solution":"import sched import time def schedule_recurring_events(start_time: float, interval: float, repetitions: int, action: callable, argument=(), kwargs={}): Schedules a recurring event using the sched module. Parameters: - start_time: The start time (in seconds since the epoch). - interval: The time interval (in seconds) between each occurrence of the event. - repetitions: The number of times the event should be scheduled. - action: A callable to be executed. - argument: A tuple containing the positional arguments for the action. - kwargs: A dictionary containing the keyword arguments for the action. event_scheduler = sched.scheduler(time.time, time.sleep) # Schedule each event at the appropriate times def schedule_event(counter): if counter < repetitions: event_scheduler.enterabs(start_time + interval * counter, 1, action, argument, kwargs) schedule_event(counter + 1) schedule_event(0) # Start the scheduler event_scheduler.run()"},{"question":"You are given a list of filenames and URLs. Your task is to write a Python function that processes this list to return a detailed analysis report. This report should include: 1. The MIME type and encoding (if available) for each item. 2. If the MIME type is known, all possible file extensions for that MIME type. 3. If the MIME type is unknown, a message indicating the type could not be guessed. Your function should be named `analyze_files_mime_types` and should have the following signature: ```python from typing import List, Tuple, Dict def analyze_files_mime_types(file_list: List[str]) -> List[Dict[str, str]]: pass ``` # Input - `file_list` (List[str]): A list of filenames or URLs as strings. # Output - List[Dict[str, str]]: A list of dictionaries where each dictionary corresponds to an entry in the input `file_list` and contains the following keys: * `\'filename\'`: The original filename or URL. * `\'mime_type\'`: The guessed MIME type (or `None` if the type could not be guessed). * `\'encoding\'`: The encoding (or `None` if no encoding is applicable). * `\'extensions\'`: A comma-separated string of all possible extensions for the MIME type (or a message indicating the type could not be guessed). # Example ```python file_list = [\'example.pdf\', \'http://example.com/archive.zip\', \'file.unknown\'] output = analyze_files_mime_types(file_list) ``` Expected `output`: ```python [ { \'filename\': \'example.pdf\', \'mime_type\': \'application/pdf\', \'encoding\': None, \'extensions\': \'.pdf\' }, { \'filename\': \'http://example.com/archive.zip\', \'mime_type\': \'application/zip\', \'encoding\': None, \'extensions\': \'.zip,.zipx\' }, { \'filename\': \'file.unknown\', \'mime_type\': None, \'encoding\': None, \'extensions\': \'Type could not be guessed\' } ] ``` # Constraints - The input list may contain up to 1000 filenames or URLs. - The function should handle unknown MIME types gracefully. # Performance Requirements - The function should perform efficiently with the provided constraint on the input size. You may utilize the functionalities provided by the `mimetypes` module to implement this solution.","solution":"import mimetypes from typing import List, Dict def analyze_files_mime_types(file_list: List[str]) -> List[Dict[str, str]]: result = [] # Initialize the mimetypes module mimetypes.init() for file in file_list: mime_type, encoding = mimetypes.guess_type(file) if mime_type: extensions = \',\'.join(mimetypes.guess_all_extensions(mime_type)) else: extensions = \'Type could not be guessed\' result.append({ \'filename\': file, \'mime_type\': mime_type, \'encoding\': encoding, \'extensions\': extensions }) return result"},{"question":"**Objective**: The goal of this assignment is to test your understanding and application of Python\'s float object handling through the available functions in the `python310` package document provided. You are required to implement a set of functionalities that work with floating-point numbers. Problem Statement: Implement a Python class `PyFloatHelper` which utilizes the described `python310` float methods to provide the following functionalities: 1. **is_float_object(self, obj)**: - **Description**: Checks if `obj` is a Python `PyFloatObject` using `PyFloat_Check`. - **Parameters**: - `obj`: The object to check. - **Returns**: `True` if `obj` is a `PyFloatObject`, `False` otherwise. 2. **float_from_string(self, string)**: - **Description**: Converts a string to a `PyFloatObject` using `PyFloat_FromString`. - **Parameters**: - `string`: A string representation of a float. - **Returns**: The new `PyFloatObject` or raises a `ValueError` if conversion fails. 3. **float_from_double(self, value)**: - **Description**: Converts a C `double` value to a `PyFloatObject` using `PyFloat_FromDouble`. - **Parameters**: - `value`: A float value. - **Returns**: The new `PyFloatObject`. 4. **as_double(self, pyfloat)**: - **Description**: Converts a `PyFloatObject` to a C `double` using `PyFloat_AsDouble`. - **Parameters**: - `pyfloat`: The `PyFloatObject` to convert. - **Returns**: The double representation. Raises `TypeError` if input is not a `PyFloatObject`. 5. **float_info(self)**: - **Description**: Retrieves float information such as precision, min, and max values using `PyFloat_GetInfo`. - **Parameters**: None. - **Returns**: A dictionary containing the `precision`, `min`, and `max` values of the float. **Constraints**: - Implement error handling for all methods. Specifically, raise appropriate Python exceptions when encountering errors similar to the function documentation. - Ensure all methods strictly use the described package functions where applicable. **Notes**: - You are not required to implement the actual C-level functions (`PyFloat_Check`, `PyFloat_FromString`, etc.), but you need to understand how they would be typically used based on provided documentations. - Pseudocode or a description of the expected function behavior is acceptable if directly implementing is not possible, but make sure the details are clear and complete. **Example Usage**: ```python helper = PyFloatHelper() assert helper.is_float_object(10.5) == True assert helper.is_float_object(\\"10.5\\") == False float_obj = helper.float_from_string(\\"10.5\\") assert helper.as_double(float_obj) == 10.5 float_obj2 = helper.float_from_double(20.5) assert helper.as_double(float_obj2) == 20.5 info = helper.float_info() assert \'precision\' in info assert \'min\' in info assert \'max\' in info ``` Your implementation should not rely on external libraries and should strictly follow Python standards for error handling and data manipulation.","solution":"class PyFloatHelper: def is_float_object(self, obj): Checks if obj is a Python float object. :param obj: The object to check. :return: True if obj is a float, False otherwise. return isinstance(obj, float) def float_from_string(self, string): Converts a string to a Python float object. :param string: A string representation of a float. :return: The float value. :raises ValueError: If the string cannot be converted to float. try: return float(string) except ValueError: raise ValueError(\\"Cannot convert string to float\\") def float_from_double(self, value): Converts a float value to a Python float object. :param value: A float value. :return: The float value. if isinstance(value, float): return value else: raise TypeError(\\"Provided value is not a float\\") def as_double(self, pyfloat): Converts a Python float object to its double representation. :param pyfloat: The Python float object. :return: The double representation (in Python, it\'s also a float). :raises TypeError: If input is not a float object. if isinstance(pyfloat, float): return pyfloat else: raise TypeError(\\"Provided value is not a float object\\") def float_info(self): Retrieves float information such as precision, min, and max values. :return: A dictionary containing the precision, min, and max values of the float. import sys return { \'precision\': sys.float_info.dig, \'min\': sys.float_info.min, \'max\': sys.float_info.max }"},{"question":"Advanced Dataclass Implementation Objective: Create a \\"School Management System\\" using Python dataclasses. This system should handle the following entities: `Student`, `Teacher`, `Course`, and `Grade`. Requirements: 1. **Entities and Relationships**: - A `Student` should have the following fields: `id`, `name`, `age`, and `courses`. - A `Teacher` should have the following fields: `id`, `name`, `subject`. - A `Course` should have the following fields: `course_id`, `course_name`, `teacher`, and `students`. - A `Grade` should have the following fields: `student`, `course`, `grade`. 2. **Constraints**: - Ensure that a `Teacher` can only teach one subject. - Ensure that `Grade` can only be an integer between 0 and 100 (inclusive). - A `Course` should only have unique students. 3. **Methods and Customizations**: - Implement a method for `Student` to add a `Course`. - Implement a method to print a report card for a `Student` listing all courses and grades. - Implement `__post_init__()` where necessary to enforce constraints. - Implement customizable `__repr__()` methods to provide a readable representation of each object. - Use `frozen=True` for `Grade` to make it immutable. 4. **Additional Functionalities**: - Use `dataclasses.field` for `default_factory` where applicable to ensure mutable fields are properly handled. - Provide functionality to serialize and deserialize the dataclasses to and from dictionaries using `asdict()` and `astuple()`. Implementation: Provide the implementation of the following classes using the `dataclasses` module: - `Student` - `Teacher` - `Course` - `Grade` Ensure all specified requirements are met. Example: ```python from dataclasses import dataclass, field, asdict, astuple, InitVar, replace, FrozenInstanceError from typing import List @dataclass class Student: # Your implementation here @dataclass class Teacher: # Your implementation here @dataclass class Course: # Your implementation here @dataclass(frozen=True) class Grade: # Your implementation here # Example usage student1 = Student(id=1, name=\'Alice\', age=20) teacher1 = Teacher(id=101, name=\'Prof. Smith\', subject=\'Math\') course1 = Course(course_id=301, course_name=\'Calculus\', teacher=teacher1) student1.add_course(course1) # Add grades and print report card ``` Constraints: - You cannot use any libraries or modules other than `dataclasses`, `typing`, and built-in Python libraries. Evaluation: - Correctness and completeness of the implementation. - Proper use of dataclass features. - Readability and maintainability of the code. - Performance considerations for adding courses and ensuring uniqueness.","solution":"from dataclasses import dataclass, field, asdict, astuple from typing import List, Dict @dataclass class Student: id: int name: str age: int courses: List[\'Course\'] = field(default_factory=list) grades: Dict[str, int] = field(default_factory=dict) def add_course(self, course): if course not in self.courses: self.courses.append(course) course.students.append(self) def add_grade(self, course, grade): if 0 <= grade <= 100: self.grades[course.course_name] = grade def report_card(self): report = {course.course_name: self.grades.get(course.course_name, \'No grade\') for course in self.courses} return report @dataclass class Teacher: id: int name: str subject: str @dataclass class Course: course_id: int course_name: str teacher: Teacher students: List[Student] = field(default_factory=list) def __post_init__(self): student_ids = [student.id for student in self.students] if len(student_ids) != len(set(student_ids)): raise ValueError(\\"Duplicate students are not allowed in a course.\\") @dataclass(frozen=True) class Grade: student: Student course: Course grade: int def __post_init__(self): if not (0 <= self.grade <= 100): raise ValueError(\\"Grade must be between 0 and 100.\\")"},{"question":"# Question: Implement a Custom Data Processor with Functional Programming In this exercise, you are required to implement a custom data processor using functional programming concepts in Python. This will involve creating a generator, using the `itertools` and `functools` modules, and applying built-in functions such as `map()`, `filter()`, and `reduce()`. Problem Statement You are given a list of dictionaries, each dictionary containing data about an employee (`employees`). Your task is to process this data to achieve the following: 1. **Filter Employees**: - Only keep the employees who have their `age` greater than or equal to 30. 2. **Transform Data**: - Generate a new list where each dictionary contains only the `name` and `yearly_salary` of the employee. - Assume yearly salary is obtained by multiplying the `monthly_salary` by 12. 3. **Combine Data**: - Compute the total yearly salary of all the filtered employees. Write the following functions: 1. **`filter_employees(employees: List[Dict[str, Any]]) -> Iterator[Dict[str, Any]]`**: - This function should employ a generator to filter out employees based on the given age criteria. 2. **`transform_employee_data(employees: Iterator[Dict[str, Any]]) -> Iterator[Dict[str, Any]]`**: - This function should use `map()` or a generator expression to convert each employee\'s dictionary to include only the `name` and `yearly_salary`. 3. **`calculate_total_yearly_salary(employees: Iterator[Dict[str, Any]]) -> int`**: - This function should use `functools.reduce()` to aggregate the total yearly salary of the filtered employees. Input Format - `employees`: A list of dictionaries, where each dictionary contains the following keys: - `\'name\'`: A string representing the name of the employee. - `\'age\'`: An integer representing the age of the employee. - `\'monthly_salary\'`: An integer representing the employee\'s monthly salary. Output Format 1. `filter_employees()` should return an iterator of dictionaries, each containing the data of employees who are 30 years or older. 2. `transform_employee_data()` should return an iterator of dictionaries, each containing only the `name` and `yearly_salary` of the employee. 3. `calculate_total_yearly_salary()` should return the total yearly salary as an integer. Constraints - The number of employees (n): (1 leq n leq 10^6) - Assume valid input data with respect to data types. Example ```python employees = [ {\'name\': \'Alice\', \'age\': 31, \'monthly_salary\': 3000}, {\'name\': \'Bob\', \'age\': 25, \'monthly_salary\': 4000}, {\'name\': \'Charlie\', \'age\': 30, \'monthly_salary\': 3500}, {\'name\': \'David\', \'age\': 28, \'monthly_salary\': 2800}, {\'name\': \'Eve\', \'age\': 42, \'monthly_salary\': 5000} ] filtered_employees = filter_employees(employees) # Should yield data for Alice, Charlie, and Eve transformed_employees = transform_employee_data(filtered_employees) # Should yield transformed data total_salary = calculate_total_yearly_salary(transformed_employees) # Should return 14,4000 (sum of yearly salaries) ``` Implement the functions and ensure they work together to process the data as described.","solution":"from typing import List, Dict, Any, Iterator import functools def filter_employees(employees: List[Dict[str, Any]]) -> Iterator[Dict[str, Any]]: Yield employees who are 30 years old or older. return (employee for employee in employees if employee[\'age\'] >= 30) def transform_employee_data(employees: Iterator[Dict[str, Any]]) -> Iterator[Dict[str, Any]]: Yield transformed employee data containing only \'name\' and \'yearly_salary\'. return ({\'name\': employee[\'name\'], \'yearly_salary\': employee[\'monthly_salary\'] * 12} for employee in employees) def calculate_total_yearly_salary(employees: Iterator[Dict[str, Any]]) -> int: Calculate the total yearly salary of the provided employees. return functools.reduce(lambda acc, employee: acc + employee[\'yearly_salary\'], employees, 0)"},{"question":"# Asynchronous File Processing with asyncio In this challenge, you are required to use the asyncio library to implement an asynchronous function for processing log files. Objective: Write an asynchronous function `process_logs(log_filenames: List[str]) -> Dict[str, int]` that: 1. Takes a list of log file names. 2. Reads each log file asynchronously. 3. Counts the occurrence of each unique word across all log files. 4. Returns a dictionary where the keys are unique words and the values are their respective counts. Expected Input and Output Formats: Input: - `log_filenames`: A list of strings, where each string is a filename of a log file. (e.g., `[\\"log1.txt\\", \\"log2.txt\\"]`) Output: - A dictionary where the keys are unique words (case-insensitive) and the values are their counts. (e.g., `{\\"error\\": 5, \\"warning\\": 3, \\"info\\": 10}`) Constraints: 1. Assume the log files are not too large to fit into memory. 2. Words are separated by spaces. 3. Ignore punctuation and make all words lower case. Performance Requirements: - The function should efficiently read and process the files concurrently. Example: Suppose you have the following log files: - `log1.txt`: \\"Error occurred in the system\\" - `log2.txt`: \\"System warning and error logs\\" The result of `process_logs([\\"log1.txt\\", \\"log2.txt\\"])` should be: ```python {\\"error\\": 2, \\"occurred\\": 1, \\"in\\": 1, \\"the\\": 1, \\"system\\": 2, \\"warning\\": 1, \\"and\\": 1, \\"logs\\": 1} ``` Implementation Guidelines: 1. Use `asyncio` functions like `asyncio.open`, `asyncio.gather`, etc., to handle file reading concurrently. 2. Ensure proper synchronization when updating the shared dictionary to avoid race conditions. ```python import asyncio from typing import List, Dict async def process_logs(log_filenames: List[str]) -> Dict[str, int]: # Your implementation here pass ``` You can use the following additional helper function template to normalize text by removing punctuation and converting to lower case: ```python import re def normalize_text(text: str) -> List[str]: # Remove punctuation and convert to lower case text = re.sub(r\'[^ws]\', \'\', text).lower() return text.split() ```","solution":"import asyncio import re from typing import List, Dict def normalize_text(text: str) -> List[str]: # Remove punctuation and convert to lower case text = re.sub(r\'[^ws]\', \'\', text).lower() return text.split() async def count_words_in_file(filename: str) -> Dict[str, int]: word_count = {} async with aiofiles.open(filename, \'r\') as file: async for line in file: words = normalize_text(line) for word in words: if word in word_count: word_count[word] += 1 else: word_count[word] = 1 return word_count async def process_logs(log_filenames: List[str]) -> Dict[str, int]: overall_word_count = {} tasks = [count_words_in_file(filename) for filename in log_filenames] results = await asyncio.gather(*tasks) for result in results: for word, count in result.items(): if word in overall_word_count: overall_word_count[word] += count else: overall_word_count[word] = count return overall_word_count"},{"question":"# Precision-Recall Curve Display You are tasked with implementing a display class for visualizing a Precision-Recall Curve. The Precision-Recall Curve is a plot that illustrates the trade-off between precision and recall for different thresholds in a binary classification system. Your goal is to create a `PrecisionRecallCurveDisplay` class that: 1. Stores the essential data needed for the visualization. 2. Provides class methods `from_estimator` and `from_predictions` to create an instance of the display object. 3. Contains a `plot` method to visualize the Precision-Recall Curve. Class Definition ```python class PrecisionRecallCurveDisplay: def __init__(self, precision, recall, average_precision, estimator_name): Initialize with precision, recall, average_precision, and the name of the estimator. Args: - precision (array-like): Precision values. - recall (array-like): Recall values. - average_precision (float): Average precision score. - estimator_name (str): Name of the estimator. self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): Create a PrecisionRecallCurveDisplay object from an estimator and data. Args: - estimator: Trained binary classifier with a \'predict_proba\' method. - X (array-like): Feature data. - y (array-like): True binary labels. Returns: - PrecisionRecallCurveDisplay object. # Your implementation here pass @classmethod def from_predictions(cls, y, y_pred, estimator_name): Create a PrecisionRecallCurveDisplay object from true and predicted values. Args: - y (array-like): True binary labels. - y_pred (array-like): Predicted probabilities for the positive class. - estimator_name (str): Name of the estimator. Returns: - PrecisionRecallCurveDisplay object. # Your implementation here pass def plot(self, ax=None, name=None, **kwargs): Plot the Precision-Recall curve. Args: - ax: Matplotlib axes object. If None, a new figure and axes are created. - name: Name of the curve for legend. - kwargs: Additional keyword arguments for matplotlib plot method. Returns: - Matplotlib axes object containing the plot. # Your implementation here pass ``` Additional Requirements 1. **from_estimator**: - Uses `estimator.predict_proba` to get predicted probabilities for the positive class. - Computes precision, recall, and average precision using `precision_recall_curve` and `average_precision_score` from `sklearn.metrics`. 2. **from_predictions**: - Directly accepts true labels and predicted probabilities. - Computes precision, recall, and average precision using `precision_recall_curve` and `average_precision_score` from `sklearn.metrics`. 3. **plot**: - Plots the Precision-Recall Curve using `matplotlib`. - Sets `precision-recall` curve line artist attributes for customization. - Updates matplotlib attributes for further styling. 4. Handle both single and multiple axes scenarios as detailed in the example provided in the documentation. Expected Input and Output Formats - Input: Appropriate arguments for the methods as specified. - Output: PrecisionRecallCurveDisplay object from the class methods and a Matplotlib plot from the `plot` method. Constraints - Use only Scikit-learn and Matplotlib libraries. - Ensure the code is efficient with a complexity of O(n) where n is the number of samples for predict_proba and precision-recall calculations. - Ensure the visualization is easily adjustable, supporting style changes post-plotting. Example ```python from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import precision_recall_curve, average_precision_score import matplotlib.pyplot as plt # Generate dataset X, y = make_classification(n_samples=1000, n_features=20, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a classifier clf = LogisticRegression().fit(X_train, y_train) # Use PrecisionRecallCurveDisplay display = PrecisionRecallCurveDisplay.from_estimator(clf, X_test, y_test) display.plot() plt.show() ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve, average_precision_score class PrecisionRecallCurveDisplay: def __init__(self, precision, recall, average_precision, estimator_name): self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): y_pred = estimator.predict_proba(X)[:, 1] precision, recall, _ = precision_recall_curve(y, y_pred) average_precision = average_precision_score(y, y_pred) estimator_name = type(estimator).__name__ return cls(precision, recall, average_precision, estimator_name) @classmethod def from_predictions(cls, y, y_pred, estimator_name): precision, recall, _ = precision_recall_curve(y, y_pred) average_precision = average_precision_score(y, y_pred) return cls(precision, recall, average_precision, estimator_name) def plot(self, ax=None, name=None, **kwargs): if ax is None: fig, ax = plt.subplots() if name is None: name = f\\"Precision-Recall curve of {self.estimator_name} (AP={self.average_precision:.2f})\\" ax.plot(self.recall, self.precision, label=name, **kwargs) ax.set_xlabel(\'Recall\') ax.set_ylabel(\'Precision\') ax.legend(loc=\'best\') return ax"},{"question":"**Coding Assessment Question: Transforming Multilabel Data** **Objective:** Demonstrate your understanding of transforming multilabel data using `MultiLabelBinarizer` and handle scenarios where label encoding is required. **Problem Statement:** You are given a dataset where each sample has multiple labels. Your task is to write a function `process_multilabel_data` that takes a list of lists of labels as input, normalizes these labels using `LabelEncoder`, and then transforms the normalized labels into a binary indicator matrix using `MultiLabelBinarizer`. **Function Signature:** ```python from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer def process_multilabel_data(data: list[list[str]]) -> tuple[list[list[int]], list[list[int]], list[str]]: Args: data (list of list of str): Input list where each inner list contains labels for a sample. Returns: tuple: - list of list of int: Normalized label data. - list of list of int: Binary indicator matrix of the normalized labels. - list of str: List of unique labels after encoding. pass ``` **Input Format:** - `data`: A list of lists where each sublist contains string labels for a sample. (1 <= len(data) <= 1000) - Each sublist will contain between 1 and 50 labels, and each label will be a string consisting of alphanumeric characters. **Output Format:** - A tuple containing: 1. A list of lists where each inner list contains the normalized numeric labels for a sample. 2. A 2D list (binary indicator matrix) where each sample is represented in binary format. 3. A list of unique string labels after encoding. **Constraints:** - Perform the normalization using `LabelEncoder` so that each label is mapped to a unique integer. - Use `MultiLabelBinarizer` to convert the normalized label data to a binary indicator matrix. **Example:** ```python data = [[\\"cat\\", \\"dog\\"], [\\"dog\\", \\"mouse\\"], [\\"cat\\", \\"mouse\\", \\"elephant\\"], [\\"dog\\", \\"elephant\\"]] normalized_data, binary_matrix, unique_labels = process_multilabel_data(data) print(normalized_data) # Output: [[0, 1], [1, 3], [0, 3, 2], [1, 2]] print(binary_matrix) # Output: [[1, 1, 0, 0], [0, 1, 0, 1], [1, 0, 1, 1], [0, 1, 1, 0]] print(unique_labels) # Output: [\\"cat\\", \\"dog\\", \\"elephant\\", \\"mouse\\"] ``` **Note:** - The order of unique labels in `unique_labels` should correspond to their encoded values. - Test your implementation against various cases to validate its correctness. **Performance Requirements:** - The function should handle up to 1000 samples efficiently. **Additional Information:** - Consider edge cases where some samples might have overlapping or completely different sets of labels. Good luck!","solution":"from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer def process_multilabel_data(data: list[list[str]]) -> tuple[list[list[int]], list[list[int]], list[str]]: Args: data (list of list of str): Input list where each inner list contains labels for a sample. Returns: tuple: - list of list of int: Normalized label data. - list of list of int: Binary indicator matrix of the normalized labels. - list of str: List of unique labels after encoding. # Flatten the list of lists to create a list of all unique labels all_labels = [label for sublist in data for label in sublist] # Initialize the LabelEncoder and fit to the flattened list of labels label_encoder = LabelEncoder() label_encoder.fit(all_labels) # Transform each sublist of labels using the label encoder normalized_data = [label_encoder.transform(sublist).tolist() for sublist in data] # Initialize the MultiLabelBinarizer and fit to the list of transformed labels mlb = MultiLabelBinarizer() binary_matrix = mlb.fit_transform(normalized_data).tolist() # Extract the classes (i.e., unique labels) from the LabelEncoder unique_labels = label_encoder.classes_.tolist() return normalized_data, binary_matrix, unique_labels"},{"question":"# Base64 Encode and Decode Utility You are tasked with implementing a utility class `Base64Utility` that provides methods for encoding and decoding strings using the Base64 encoding scheme. Your class should support both the standard Base64 encoding and a URL-safe variant. Additionally, you should ensure that it can handle both bytes input and string input for decoding, converting inputs to appropriate types as necessary. Requirements 1. Implement the `Base64Utility` class with the following methods: - `b64encode(data: Union[str, bytes], url_safe: bool = False) -> bytes`: - Encodes the given `data` using Base64 encoding. - If `url_safe` is `True`, use URL- and filesystem-safe Base64 encoding. - If the input is a string, it should be converted to bytes using UTF-8 encoding before encoding. - Returns the encoded data as bytes. - `b64decode(data: Union[str, bytes], url_safe: bool = False) -> bytes`: - Decodes the given Base64 encoded `data`. - If `url_safe` is `True`, use URL- and filesystem-safe Base64 decoding. - If the input is a string, it should be converted to bytes using UTF-8 encoding before decoding. - Returns the decoded data as bytes. 2. Utilize the `base64` module\'s functions as specified in the documentation to perform the encoding and decoding operations. 3. Implement error handling: - Raise a `ValueError` if the input to `b64decode` is incorrectly padded or contains invalid characters. Input and Output - The input to `b64encode` can be either a string or bytes. The output should be bytes. - The input to `b64decode` can be either a string or bytes. The output should be bytes. Constraints - It is guaranteed that inputs for encoding will be valid strings or bytes that can be encoded using UTF-8. - For decoding, handle padding errors and invalid characters appropriately. Example usage: ```python # Instantiate the utility utility = Base64Utility() # Encode data encoded = utility.b64encode(\\"data to be encoded\\") print(encoded) # Outputs: b\'ZGF0YSB0byBiZSBlbmNvZGVk\' # Decode data decoded = utility.b64decode(encoded) print(decoded) # Outputs: b\'data to be encoded\' # Encode data in a URL-safe manner url_encoded = utility.b64encode(\\"data to be encoded\\", url_safe=True) print(url_encoded) # Outputs: b\'ZGF0YSB0byBiZSBlbmNvZGVk\' # Decode URL-safe encoded data url_decoded = utility.b64decode(url_encoded, url_safe=True) print(url_decoded) # Outputs: b\'data to be encoded\' # Handling invalid input for decoding (this should raise an error) try: invalid_data = \\"Invalid base64 string\\" utility.b64decode(invalid_data) except ValueError as ve: print(ve) # Outputs: Error message indicating incorrect padding or invalid characters. ``` Implementation ```python import base64 from typing import Union class Base64Utility: @staticmethod def b64encode(data: Union[str, bytes], url_safe: bool = False) -> bytes: if isinstance(data, str): data = data.encode(\'utf-8\') if url_safe: return base64.urlsafe_b64encode(data) else: return base64.b64encode(data) @staticmethod def b64decode(data: Union[str, bytes], url_safe: bool = False) -> bytes: if isinstance(data, str): data = data.encode(\'utf-8\') try: if url_safe: return base64.urlsafe_b64decode(data) else: return base64.b64decode(data) except (binascii.Error, ValueError): raise ValueError(\\"Incorrect padding or invalid characters found during decoding.\\") ```","solution":"import base64 import binascii from typing import Union class Base64Utility: @staticmethod def b64encode(data: Union[str, bytes], url_safe: bool = False) -> bytes: if isinstance(data, str): data = data.encode(\'utf-8\') if url_safe: return base64.urlsafe_b64encode(data) else: return base64.b64encode(data) @staticmethod def b64decode(data: Union[str, bytes], url_safe: bool = False) -> bytes: if isinstance(data, str): data = data.encode(\'utf-8\') try: if url_safe: return base64.urlsafe_b64decode(data) else: return base64.b64decode(data) except (binascii.Error, ValueError) as e: raise ValueError(\\"Incorrect padding or invalid characters found during decoding.\\") from e"},{"question":"**Optimizing PyTorch Computation Using SIMD Instructions** # Objective: The objective of this question is to assess your understanding of PyTorch and how to leverage SIMD instructions to optimize a computation task on x86 CPUs. # Problem Statement: You are given a task to optimize matrix multiplication operations using SIMD instructions. Your task is to write a PyTorch function that performs matrix multiplication and verify if the machine supports AVX-512 or AMX instructions to potentially leverage them for performance improvements. # Requirements: 1. **Check SIMD Instruction Support**: - Write a function `check_simd_support()` that uses the `collect_env.py` script to check if the machine supports AVX-512 or AMX instruction sets. - Return a dictionary with keys `avx512` and `amx` and corresponding boolean values indicating the support status. 2. **Matrix Multiplication**: - Implement a function `optimized_matrix_multiplication(A, B)` that performs matrix multiplication of two given PyTorch tensors `A` and `B`. - If AVX-512 or AMX is supported, use optimized techniques to leverage these instruction sets for better performance. # Input: - A and B: Two 2D PyTorch tensors of shape `(n, m)` and `(m, p)` respectively. # Output: - The matrix multiplication result of A and B as a PyTorch tensor of shape `(n, p)`. # Constraints: - Ensure that the code runs efficiently on machines with and without SIMD support. - Use PyTorch for matrix operations. - Handle large matrices efficiently to demonstrate performance optimizations if SIMD instructions are available. # Example: ```python import torch def check_simd_support(): # Your implementation here pass def optimized_matrix_multiplication(A, B): # Your implementation here pass # Example usage A = torch.randn(512, 1024) B = torch.randn(1024, 256) result = optimized_matrix_multiplication(A, B) print(result) ``` # Notes: - You can assume that `collect_env.py` is available in the current working directory. - Focus on implementing efficient PyTorch code leveraging SIMD where applicable. - The check for SIMD support should be robust and handle different output formats from `collect_env.py`.","solution":"import subprocess import torch def check_simd_support(): def has_instruction(output, instruction): return instruction in output avx512_support = amx_support = False try: result = subprocess.check_output([\'lscpu\']).decode(\'utf-8\') avx512_support = has_instruction(result, \'avx512\') amx_support = has_instruction(result, \'amx\') except Exception as e: print(f\\"Failed to determine SIMD support due to: {e}\\") return { \\"avx512\\": avx512_support, \\"amx\\": amx_support } def optimized_matrix_multiplication(A, B): Perform matrix multiplication of two PyTorch tensors A and B. Leveraging any available SIMD instructions. simd_support = check_simd_support() # If AVX-512 or AMX is supported, set optimization flags (actual optimizations typically happen at lower level) if simd_support[\'avx512\'] or simd_support[\'amx\']: with torch.cuda.amp.autocast(): return torch.matmul(A, B) else: return torch.matmul(A, B)"},{"question":"List Manipulation Using Python C API Objective: You are to implement a Python C extension module named `list_operations` that provides a single function `process_list_operations()`. This function will take a list object and a series of operations to perform on the list. Function Signature: ```python def process_list_operations(list: list, operations: list) -> list: pass ``` Inputs: 1. `list`: A Python list object that will be manipulated. 2. `operations`: A list of operations. Each operation is a tuple where: - The first element is the operation name (a string). It can be one of the following: `\'append\'`, `\'insert\'`, `\'set_item\'`, `\'get_item\'`, `\'sort\'`, `\'reverse\'`, `\'set_slice\'`, `\'get_slice\'`. - The remaining elements are the arguments required for the operation (`index`, `item`, `low`, `high`, `itemlist`, etc.). Outputs: - The function should return the modified list after performing all operations. - If any operation fails (e.g., due to an invalid index), the function should return `None`. Constraints: 1. Index values will be non-negative. 2. The length of the list will not exceed 1000 elements. 3. All operations should be processed in the order they appear in the `operations` list. Example: ```python # Sample input original_list = [1, 2, 3, 4, 5] operation_sequence = [ (\'append\', 6), (\'insert\', 2, 99), (\'set_item\', 0, 42), (\'get_item\', 3) ] # Expected output result_list = [42, 2, 99, 3, 4, 5, 6] ``` Performance Considerations: - Your implementation should aim to be efficient with respect to both time and space complexity, given the constraints. Notes: - You should design the C API extension and assume that the operations will be called from Python code. - Handle all necessary error checking and return `None` if any operation is invalid.","solution":"def process_list_operations(lst, operations): Processes a list of operations on the given list. Args: lst: The list to be manipulated. operations: A list of operations where each operation is a tuple. Returns: The modified list after performing all operations, or None if any operation fails. try: for op in operations: if op[0] == \'append\': lst.append(op[1]) elif op[0] == \'insert\': lst.insert(op[1], op[2]) elif op[0] == \'set_item\': lst[op[1]] = op[2] elif op[0] == \'get_item\': _ = lst[op[1]] # We\'re ignoring the value here as mentioned elif op[0] == \'sort\': lst.sort() elif op[0] == \'reverse\': lst.reverse() elif op[0] == \'set_slice\': lst[op[1]:op[2]] = op[3] elif op[0] == \'get_slice\': _ = lst[op[1]:op[2]] # We\'re ignoring the value here as mentioned else: return None # Unknown operation return lst except: return None"},{"question":"You are required to implement a function in Python that processes a list of transactions in a user\'s bank account. Each transaction is represented by a tuple containing the transaction type (\\"deposit\\" or \\"withdraw\\"), the transaction amount, and a unique transaction ID. Your task is to categorize the transactions into deposits and withdrawals and then return the balance along with the categorized transactions. Requirements 1. Implement a function `process_transactions(transactions: list) -> dict` where: - `transactions`: a list of tuples, where each tuple contains: - `str`: the transaction type, either \\"deposit\\" or \\"withdraw\\" - `float`: the transaction amount, positive value - `int`: a unique transaction ID 2. The function should return a dictionary with the following structure: ```python { \\"balance\\": float, # the resulting balance after processing all transactions \\"transactions\\": { \\"deposits\\": list of tuples, # list of deposit transactions \\"withdrawals\\": list of tuples # list of withdrawal transactions } } ``` Constraints - The initial balance of the account is 0. - Each transaction amount will be a positive float. - Assume all transaction IDs are unique integers. Example ```python transactions = [ (\\"deposit\\", 1500.0, 101), (\\"withdraw\\", 200.0, 102), (\\"deposit\\", 500.0, 103), (\\"withdraw\\", 100.0, 104), ] result = process_transactions(transactions) # Expected output { \\"balance\\": 1700.0, \\"transactions\\": { \\"deposits\\": [(\\"deposit\\", 1500.0, 101), (\\"deposit\\", 500.0, 103)], \\"withdrawals\\": [(\\"withdraw\\", 200.0, 102), (\\"withdraw\\", 100.0, 104)] } } ``` Notes - You should use control flow tools such as loops, if-else statements, or match statements where appropriate. - Ensure your function and variable names follow PEP 8 guidelines. - Include appropriate docstrings for your function explaining its purpose, parameters, and return value.","solution":"def process_transactions(transactions): Processes a list of transactions, calculating the balance and categorizing transactions into deposits and withdrawals. Parameters: transactions (list): A list of tuples, where each tuple contains: - str: the transaction type, either \\"deposit\\" or \\"withdraw\\" - float: the transaction amount, positive value - int: a unique transaction ID Returns: dict: A dictionary with the balance and categorized transactions. balance = 0.0 deposits = [] withdrawals = [] for transaction in transactions: transaction_type, amount, trans_id = transaction if transaction_type == \\"deposit\\": balance += amount deposits.append(transaction) elif transaction_type == \\"withdraw\\": balance -= amount withdrawals.append(transaction) return { \\"balance\\": balance, \\"transactions\\": { \\"deposits\\": deposits, \\"withdrawals\\": withdrawals } }"},{"question":"**Objective:** Demonstrate proficiency in working with the `ossaudiodev` module to handle audio data. This will include reading from an audio file, processing the data, and writing it to another file using OSS audio devices. **Problem Statement:** You are tasked with implementing a function called `process_audio` that performs the following steps: 1. Opens an OSS audio device in write mode. 2. Sets the audio format, number of channels, and sample rate. 3. Reads audio data from an input file. 4. Processes the audio data by applying a simple transformation (e.g., volume adjustment). 5. Writes the processed audio data to an output file. 6. Closes the audio device. # Function Signature: ```python def process_audio(input_file: str, output_file: str, format: int, channels: int, samplerate: int, volume_adjustment: float) -> None: Processes audio data from an input file and writes it to an output file. Parameters: - input_file (str): Path to the input audio file. - output_file (str): Path to the output audio file. - format (int): Audio format to set (use constants from ossaudiodev). - channels (int): Number of audio channels. - samplerate (int): Sample rate in Hz. - volume_adjustment (float): Factor to adjust volume (e.g., 1.0 for no change, 0.5 for half volume, 2.0 for double volume). Returns: - None ``` # Description: 1. **Opening the Audio Device:** - Use `ossaudiodev.open()` to open the audio device in write mode. - Set the device parameters using `setparameters()` method. 2. **Reading Input File:** - Read the audio data from `input_file` using a standard file read operation. 3. **Processing Audio Data:** - Apply a volume adjustment to the audio data by multiplying each audio sample by `volume_adjustment`. Assume the audio data is in 16-bit PCM format. 4. **Writing to Output File:** - Write the processed audio data to the audio device using `writeall()` method. - Capture the processed data written to the device and save it to `output_file`. 5. **Closing the Device:** - Close the audio device using the `close()` method. # Constraints: 1. Ensure proper exception handling for all I/O operations and OSS-specific methods. 2. The input audio file will be in PCM format and contain raw audio data. 3. The output should maintain the same audio format, channels, and sample rate as specified. # Example Usage: ```python try: process_audio(\\"input.raw\\", \\"output.raw\\", ossaudiodev.AFMT_S16_LE, 2, 44100, 1.5) except Exception as e: print(f\\"Error processing audio: {e}\\") ``` # Notes: - You should use constants from the `ossaudiodev` module for setting audio formats. - For volume adjustment, the audio samples are expected to be in signed 16-bit PCM format, so ensure proper handling of the byte data. - Focus on robustness and error handling, especially with I/O operations involving the audio device.","solution":"import ossaudiodev import wave import struct def process_audio(input_file: str, output_file: str, format: int, channels: int, samplerate: int, volume_adjustment: float) -> None: Processes audio data from an input file and writes it to an output file. Parameters: - input_file (str): Path to the input audio file. - output_file (str): Path to the output audio file. - format (int): Audio format to set (use constants from ossaudiodev). - channels (int): Number of audio channels. - samplerate (int): Sample rate in Hz. - volume_adjustment (float): Factor to adjust volume (e.g., 1.0 for no change, 0.5 for half volume, 2.0 for double volume). Returns: - None # Open the input file with open(input_file, \'rb\') as f_in: input_data = f_in.read() # Process the audio data processed_data = bytearray() for i in range(0, len(input_data), 2): # Read the sample as a signed 16-bit integer sample = struct.unpack(\'<h\', input_data[i:i+2])[0] # Apply the volume adjustment adjusted_sample = int(sample * volume_adjustment) # Ensure the sample stays within the 16-bit range adjusted_sample = max(min(adjusted_sample, 32767), -32768) # Pack the adjusted sample back into bytes processed_data += struct.pack(\'<h\', adjusted_sample) # Save the processed data to the output file with open(output_file, \'wb\') as f_out: f_out.write(processed_data)"},{"question":"**Title:** Advanced Text Processing with Regular Expressions **Objective:** The objective is to assess the student\'s ability to use Python\'s \\"re\\" module to solve complex text processing problems involving regular expressions. **Problem Statement:** You are given a text document (as a string) containing a list of names, dates, and email addresses. Your task is to write a function `extract_details` that extracts and returns the following information: 1. **Names**: Full names in the format \'First Last\'. 2. **Dates**: Dates in the format \'dd/mm/yyyy\'. 3. **Email Addresses**: Valid email addresses. The function should return a dictionary with three keys: `\'names\'`, `\'dates\'`, and `\'emails\'`. Each key should map to a list of corresponding extracted values. **Input:** - A string `document` containing multiple lines. Each line may contain none, one, or multiple occurrences of names, dates, and email addresses. **Output:** - A dictionary with three keys: - `\'names\'`: a list of full names. - `\'dates\'`: a list of dates in the format \'dd/mm/yyyy\'. - `\'emails\'`: a list of valid email addresses. **Constraints:** - Assume names are capitalized and appear in the format \'First Last\'. - Dates will always be in the format \'dd/mm/yyyy\'. - Email addresses follow the general pattern \'local@domain\'. **Function Signature:** ```python def extract_details(document: str) -> dict: pass ``` **Example:** ```python document = John Doe johndoe@example.com 12/05/1990 Jane Smith janesmith@yahoo.com 23/08/1985 Meeting on 30/09/2021 with alice@domain.com output = extract_details(document) # Expected output: # { # \'names\': [\'John Doe\', \'Jane Smith\'], # \'dates\': [\'12/05/1990\', \'23/08/1985\', \'30/09/2021\'], # \'emails\': [\'johndoe@example.com\', \'janesmith@yahoo.com\', \'alice@domain.com\'] # } ``` **Notes:** - You may use Python\'s \\"re\\" module to create regular expressions for matching the desired patterns. - Ensure your regular expressions correctly handle multiple occurrences on the same line. - Optimize for readability and maintainability of your code.","solution":"import re from typing import Dict, List def extract_details(document: str) -> Dict[str, List[str]]: Extracts names, dates, and email addresses from the given document string. Parameters: document (str): The string containing names, dates, and email addresses. Returns: Dict[str, List[str]]: A dictionary with keys \'names\', \'dates\', and \'emails\' mapping to lists of extracted names, dates, and emails. # Regular expression patterns name_pattern = r\'b[A-Z][a-z]+ [A-Z][a-z]+b\' date_pattern = r\'bd{2}/d{2}/d{4}b\' email_pattern = r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}b\' # Extracting patterns using re.findall() names = re.findall(name_pattern, document) dates = re.findall(date_pattern, document) emails = re.findall(email_pattern, document) return { \'names\': names, \'dates\': dates, \'emails\': emails }"},{"question":"Objective Write a function that extends the functionality of the `imghdr` module by adding a custom test to detect a specific image format that is not natively supported by `imghdr`. Problem Statement Create a Python function that detects whether a file is of a custom image type called \\"xyz\\". The \\"xyz\\" format is characterized by having the first three bytes of the file equal to the ASCII values of the characters \'X\', \'Y\', and \'Z\'. You need to: 1. Define a custom test function that checks for the \\"xyz\\" format. 2. Append this custom test function to `imghdr.tests`. 3. Write another function that takes a file path as input and uses `imghdr.what()` to detect the image type, including the custom \\"xyz\\" type. Function Definitions You need to implement the following functions: 1. `def add_xyz_test():` - **Description**: This function adds a custom test for detecting \\"xyz\\" format images by appending the test function to `imghdr.tests`. - **Parameters**: None - **Returns**: None 2. `def detect_image_type(file_path: str) -> str:` - **Description**: This function uses `imghdr.what()` to detect the type of the image file specified by `file_path`. - **Parameters**: - `file_path` (str): The path to the image file. - **Returns**: A string describing the image type, or `None` if the format is unrecognized. Constraints - The custom test function should correctly identify files following the \\"xyz\\" format rules. - The `detect_image_type` function should work seamlessly with both native and custom image format tests. Example Usage ```python # Add the custom \\"xyz\\" test to imghdr add_xyz_test() # Detecting a native image type print(detect_image_type(\'example.png\')) # Output: \'png\' # Detecting the custom \\"xyz\\" image type print(detect_image_type(\'example.xyz\')) # Output: \'xyz\' ``` Notes - You can assume that the \\"xyz\\" format is not natively recognized by `imghdr`. - You should handle the necessary imports within your code.","solution":"import imghdr def xyz_test(h, f): Test function for \\"xyz\\" format. if h[:3] == b\'XYZ\': return \'xyz\' def add_xyz_test(): Adds the custom \'xyz\' test to imghdr. imghdr.tests.append(xyz_test) def detect_image_type(file_path: str) -> str: Detects the type of the image file specified by \'file_path\'. return imghdr.what(file_path)"},{"question":"# Sequence Manipulation Tool You are required to implement a Python function that mimics various sequence operations similar to those available at the C-API level, as described in the provided documentation. Function Signature ```python def sequence_manipulation(sequence, operations): Manipulate a sequence based on a list of operations. Arguments: sequence: A sequence (list or tuple) that will be manipulated. operations: A list of operations to be performed on the sequence. Each operation is represented as a dictionary with the following keys: - \\"op\\": A string denoting the operation. Supported operations are: - \\"check\\": No additional arguments. Checks if the input is a sequence. - \\"size\\": No additional arguments. Returns the size of the sequence. - \\"concat\\": Requires \\"sequence\\" argument to concatenate with the original sequence. - \\"repeat\\": Requires \\"count\\" argument to repeat the sequence. - \\"getitem\\": Requires \\"index\\" argument to get the item at the specified index. - \\"slice\\": Requires \\"start\\" and \\"end\\" arguments to get a slice of the sequence. - \\"setitem\\": Requires \\"index\\" and \\"value\\" arguments to set an item at the specified index. - \\"delitem\\": Requires \\"index\\" argument to delete an item at the specified index. - \\"setslice\\": Requires \\"start\\", \\"end\\", and \\"value\\" arguments to set a slice. - \\"delslice\\": Requires \\"start\\" and \\"end\\" arguments to delete a slice. - \\"count\\": Requires \\"value\\" argument to count occurrences of the value. - \\"contains\\": Requires \\"value\\" argument to check if the value is in the sequence. - \\"index\\": Requires \\"value\\" argument to find the index of the value. - \\"tolist\\": Converts the sequence to a list. - \\"totuple\\": Converts the sequence to a tuple. - Additional keys as required by the specific operation. Returns: A list of results corresponding to the sequence of operations. For operations that do not produce a value, the result should be None. If an operation fails (e.g., invalid index), the result should be an error message. pass ``` Example Usage ```python seq = [1, 2, 3, 4, 5] ops = [ {\\"op\\": \\"check\\"}, {\\"op\\": \\"size\\"}, {\\"op\\": \\"concat\\", \\"sequence\\": [6, 7]}, {\\"op\\": \\"repeat\\", \\"count\\": 2}, {\\"op\\": \\"getitem\\", \\"index\\": 3}, {\\"op\\": \\"slice\\", \\"start\\": 2, \\"end\\": 5}, {\\"op\\": \\"setitem\\", \\"index\\": 1, \\"value\\": 10}, {\\"op\\": \\"delitem\\", \\"index\\": 4}, {\\"op\\": \\"setslice\\", \\"start\\": 2, \\"end\\": 4, \\"value\\": [20, 30]}, {\\"op\\": \\"count\\", \\"value\\": 3}, {\\"op\\": \\"contains\\", \\"value\\": 10}, {\\"op\\": \\"index\\", \\"value\\": 2}, {\\"op\\": \\"tolist\\"}, {\\"op\\": \\"totuple\\"} ] result = sequence_manipulation(seq, ops) print(result) ``` # Requirements 1. **Input Constraints**: - The `sequence` parameter must be either a list or a tuple. - The `operations` parameter must be a list of dictionaries, each representing an operation. - Raise appropriate exceptions or error messages when operations are invalid or out-of-bounds. 2. **Functionality**: - The function should accurately simulate each specified sequence operation. - Performance considerations are secondary but make sure the function handles typical input sizes efficiently. 3. **Edge Cases**: - Handle cases such as invalid indices, improper types in operations, and unsupported operations gracefully. - Return detailed error messages for failed operations.","solution":"def sequence_manipulation(sequence, operations): Manipulate a sequence based on a list of operations. Arguments: sequence: A sequence (list or tuple) that will be manipulated. operations: A list of operations to be performed on the sequence. Each operation is represented as a dictionary with the following keys: - \\"op\\": A string denoting the operation. Supported operations are: - \\"check\\": No additional arguments. Checks if the input is a sequence. - \\"size\\": No additional arguments. Returns the size of the sequence. - \\"concat\\": Requires \\"sequence\\" argument to concatenate with the original sequence. - \\"repeat\\": Requires \\"count\\" argument to repeat the sequence. - \\"getitem\\": Requires \\"index\\" argument to get the item at the specified index. - \\"slice\\": Requires \\"start\\" and \\"end\\" arguments to get a slice of the sequence. - \\"setitem\\": Requires \\"index\\" and \\"value\\" arguments to set an item at the specified index. - \\"delitem\\": Requires \\"index\\" argument to delete an item at the specified index. - \\"setslice\\": Requires \\"start\\", \\"end\\", and \\"value\\" arguments to set a slice. - \\"delslice\\": Requires \\"start\\" and \\"end\\" arguments to delete a slice. - \\"count\\": Requires \\"value\\" argument to count occurrences of the value. - \\"contains\\": Requires \\"value\\" argument to check if the value is in the sequence. - \\"index\\": Requires \\"value\\" argument to find the index of the value. - \\"tolist\\": Converts the sequence to a list. - \\"totuple\\": Converts the sequence to a tuple. - Additional keys as required by the specific operation. Returns: A list of results corresponding to the sequence of operations. For operations that do not produce a value, the result should be None. If an operation fails (e.g., invalid index), the result should be an error message. results = [] for op in operations: operation = op[\\"op\\"] try: if operation == \\"check\\": # Check if input is either list or tuple if isinstance(sequence, (list, tuple)): results.append(True) else: results.append(False) elif operation == \\"size\\": # Return the size of the sequence results.append(len(sequence)) elif operation == \\"concat\\": # Concatenate sequence sequence = type(sequence)(list(sequence) + list(op[\\"sequence\\"])) results.append(None) elif operation == \\"repeat\\": # Repeat sequence count times sequence = sequence * op[\\"count\\"] results.append(None) elif operation == \\"getitem\\": # Get item at index results.append(sequence[op[\\"index\\"]]) elif operation == \\"slice\\": # Get slice from start to end results.append(sequence[op[\\"start\\"]:op[\\"end\\"]]) elif operation == \\"setitem\\": # Set item at index sequence[op[\\"index\\"]] = op[\\"value\\"] results.append(None) elif operation == \\"delitem\\": # Delete item at index del sequence[op[\\"index\\"]] results.append(None) elif operation == \\"setslice\\": # Set slice from start to end with value sequence[op[\\"start\\"]:op[\\"end\\"]] = op[\\"value\\"] results.append(None) elif operation == \\"delslice\\": # Delete slice from start to end del sequence[op[\\"start\\"]:op[\\"end\\"]] results.append(None) elif operation == \\"count\\": # Count occurrences of value results.append(sequence.count(op[\\"value\\"])) elif operation == \\"contains\\": # Check if value is in sequence results.append(op[\\"value\\"] in sequence) elif operation == \\"index\\": # Find index of value results.append(sequence.index(op[\\"value\\"])) elif operation == \\"tolist\\": # Convert sequence to list sequence = list(sequence) results.append(sequence) elif operation == \\"totuple\\": # Convert sequence to tuple sequence = tuple(sequence) results.append(sequence) else: results.append(f\\"Unsupported operation: {operation}\\") except Exception as e: results.append(f\\"Error: {str(e)}\\") return results"},{"question":"# Asynchronous Task Management with asyncio **Objective:** You are required to design a function in Python that demonstrates your understanding of asyncio for asynchronous task management. Specifically, you will create an asynchronous task manager that executes multiple tasks concurrently and handles timeouts and cancellations. **Task:** Implement the function `async_task_manager(tasks, timeout)` which takes in two arguments: 1. `tasks`: A list of tuples, where each tuple contains a coroutine function and its respective arguments. E.g., `[(coroutine1, arg1, arg2), (coroutine2, arg1)]`. 2. `timeout`: A float representing the maximum duration (in seconds) allowed for all tasks to complete. The function should: 1. Execute all provided tasks concurrently. 2. Ensure that if the overall execution time exceeds the provided timeout, all the tasks are cancelled. 3. Collect and return the results of the tasks that finish successfully before the timeout. 4. Handle and report any exceptions raised by the tasks. 5. Ensure that all cleanup is properly handled in cases of exceptions and cancellations. # Example Input: ```python import asyncio async def task1(): await asyncio.sleep(2) return \\"task1 done\\" async def task2(x): await asyncio.sleep(x) if x > 1: raise ValueError(\\"x too large\\") return f\\"task2 done with {x}\\" tasks = [(task1, ), (task2, 1), (task2, 2)] timeout = 3 results = await async_task_manager(tasks, timeout) print(results) ``` # Expected Output: ```python { \\"completed\\": [\\"task1 done\\", \\"task2 done with 1\\"], \\"errors\\": [(\\"task2\\", ValueError(\\"x too large\\"))] } ``` # Constraints: 1. Ensure task results are in the same order as the provided tasks. 2. Properly handle exceptions and cancellations to avoid leaving orphan tasks. 3. Ensure all tasks are completed or cancelled before the function returns. # Function Signature: ```python async def async_task_manager(tasks: list, timeout: float) -> dict: pass ``` # Additional Information: - Use `asyncio.gather()` for concurrent execution. - Utilize `asyncio.wait_for()` to handle the timeout. - Use `try...except` blocks to manage exceptions and cancellations. - Ensure to preserve the order of tasks and their results. Happy coding!","solution":"import asyncio async def async_task_manager(tasks, timeout): Manages and executes multiple tasks concurrently with a specified timeout. Parameters: tasks (list): A list of tuples, where each tuple contains a coroutine function and its respective arguments. timeout (float): Maximum duration (in seconds) allowed for all tasks to complete. Returns: dict: Contains completed task results under \\"completed\\" key and tasks with errors under \\"errors\\" key. async def wrapper(coroutine, *args): Wrapper coroutine to capture exceptions during task execution. try: result = await coroutine(*args) return \\"completed\\", result except Exception as e: return \\"error\\", (coroutine.__name__, e) tasks_wrapped = [wrapper(coroutine, *args) for coroutine, *args in tasks] results = { \\"completed\\": [], \\"errors\\": [] } try: completed_tasks = await asyncio.wait_for(asyncio.gather(*tasks_wrapped), timeout=timeout) for status, result in completed_tasks: if status == \\"completed\\": results[\\"completed\\"].append(result) else: results[\\"errors\\"].append(result) except asyncio.TimeoutError: for task in asyncio.all_tasks(): task.cancel() # Wait for all pending tasks to be cancelled await asyncio.gather(*asyncio.all_tasks(), return_exceptions=True) return results"},{"question":"# Question: Tensor Views and Contiguity You are tasked with implementing a function `reshape_and_verify` that performs specific operations on a given input tensor using PyTorch. Your function should: 1. Reshape the input tensor into another shape and perform a specific manipulation. 2. Ensure the output tensor is contiguous in memory. 3. Verify if the reshaping created a view or a new tensor. 4. Modify an element in the reshaped tensor and reflect that change in the input tensor if they share the same data. The function should have the following signature: ```python import torch def reshape_and_verify(t: torch.Tensor, new_shape: tuple, modify_position: tuple, new_value: float) -> (torch.Tensor, bool): Reshapes the input tensor, ensures contiguity, and verifies if the reshaped tensor is a view. Args: - t (torch.Tensor): The input tensor to be reshaped. - new_shape (tuple): The new shape to reshape the tensor into. - modify_position (tuple): Position in the reshaped tensor to modify. - new_value (float): The new value to assign at modify_position in the reshaped tensor. Returns: - reshaped_tensor (torch.Tensor): The reshaped and contiguous tensor. - is_view (bool): True if the reshaped tensor is a view of the input tensor, False otherwise. # Step 1: Reshape the tensor reshaped_tensor = t.view(new_shape) # Step 2: Ensure contiguity if not reshaped_tensor.is_contiguous(): reshaped_tensor = reshaped_tensor.contiguous() # Step 3: Verify if reshaped_tensor is a view of the original tensor is_view = (t.storage().data_ptr() == reshaped_tensor.storage().data_ptr()) # Step 4: Modify the element at the specified position in reshaped_tensor reshaped_tensor[modify_position] = new_value return reshaped_tensor, is_view ``` # Example Usage: ```python import torch # Create an input tensor input_tensor = torch.arange(16).reshape(4, 4).float() # Reshape the input tensor and modify a specific element new_shape = (2, 8) modify_position = (1, 4) new_value = 99.99 reshaped_tensor, is_view = reshape_and_verify(input_tensor, new_shape, modify_position, new_value) # Output checks print(\\"Reshaped Tensor:\\") print(reshaped_tensor) print(\\"nIs View:\\", is_view) print(\\"nInput Tensor after modification:\\") print(input_tensor) ``` **Constraints:** - The input tensor `t` and the modified position in the reshaped tensor will always be within the valid range. This question assesses the understanding of tensor views, memory contiguity, and efficient data manipulation in PyTorch.","solution":"import torch def reshape_and_verify(t: torch.Tensor, new_shape: tuple, modify_position: tuple, new_value: float) -> (torch.Tensor, bool): Reshapes the input tensor, ensures contiguity, and verifies if the reshaped tensor is a view. Args: - t (torch.Tensor): The input tensor to be reshaped. - new_shape (tuple): The new shape to reshape the tensor into. - modify_position (tuple): Position in the reshaped tensor to modify. - new_value (float): The new value to assign at modify_position in the reshaped tensor. Returns: - reshaped_tensor (torch.Tensor): The reshaped and contiguous tensor. - is_view (bool): True if the reshaped tensor is a view of the input tensor, False otherwise. # Step 1: Reshape the tensor reshaped_tensor = t.view(new_shape) # Step 2: Ensure contiguity if not reshaped_tensor.is_contiguous(): reshaped_tensor = reshaped_tensor.contiguous() # Step 3: Verify if reshaped_tensor is a view of the original tensor is_view = (t.storage().data_ptr() == reshaped_tensor.storage().data_ptr()) # Step 4: Modify the element at the specified position in reshaped_tensor reshaped_tensor[modify_position] = new_value return reshaped_tensor, is_view"},{"question":"Objective You are tasked with implementing Principal Component Analysis (PCA) and Incremental PCA on the Iris dataset. The goal is to: 1. Reduce the dataset from 4 to 2 dimensions. 2. Visualize the results using a scatter plot. 3. Compare the explained variance of both PCA methods. Problem Statement 1. Load the Iris dataset using `sklearn.datasets.load_iris()`. 2. Implement PCA to reduce the Iris dataset to 2 principal components. 3. Implement Incremental PCA to reduce the Iris dataset to 2 principal components. 4. Visualize the results of both PCA reductions in two different scatter plots, with different colors for each class. 5. Print and compare the explained variance ratio of both PCA methods. Constraints - Use `sklearn.decomposition.PCA` and `sklearn.decomposition.IncrementalPCA` for decomposition. - Assume no `train_test_split` as the entire dataset will be used for both PCA and IncrementalPCA simultaneously. Input - You do not need to take any input from the user. Use the provided Iris dataset. Output - Two scatter plots for PCA and IncrementalPCA results. - Explained variance ratio for both PCA and IncrementalPCA. Example ```python import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.decomposition import PCA, IncrementalPCA # Load Data iris = load_iris() X = iris.data y = iris.target # Apply PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X) # Apply Incremental PCA ipca = IncrementalPCA(n_components=2) X_ipca = ipca.fit_transform(X) # Visualization plt.figure(figsize=(14, 5)) plt.subplot(1, 2, 1) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.title(\'PCA\') plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.subplot(1, 2, 2) plt.scatter(X_ipca[:, 0], X_ipca[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.title(\'Incremental PCA\') plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.show() # Explained Variance print(\'Explained variance ratio (PCA):\', pca.explained_variance_ratio_) print(\'Explained variance ratio (Incremental PCA):\', ipca.explained_variance_ratio_) ``` In this task, you are required to: 1. Implement the code snippet provided above. 2. Generate plots and explained variance ratios. 3. Ensure both PCA methods provide insights into dimensionality reduction performances.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.decomposition import PCA, IncrementalPCA def perform_pca(): # Load Data iris = load_iris() X = iris.data y = iris.target # Apply PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X) # Apply Incremental PCA ipca = IncrementalPCA(n_components=2) X_ipca = ipca.fit_transform(X) # Visualization plt.figure(figsize=(14, 5)) plt.subplot(1, 2, 1) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.title(\'PCA\') plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.subplot(1, 2, 2) plt.scatter(X_ipca[:, 0], X_ipca[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=40) plt.title(\'Incremental PCA\') plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.show() # Explained Variance explained_variance_pca = pca.explained_variance_ratio_ explained_variance_ipca = ipca.explained_variance_ratio_ print(\'Explained variance ratio (PCA):\', explained_variance_pca) print(\'Explained variance ratio (Incremental PCA):\', explained_variance_ipca) return explained_variance_pca, explained_variance_ipca # run the function to visualize and print results perform_pca()"},{"question":"# XML Document Navigation and Modification using `xml.dom` **Objective:** Demonstrate your understanding of the `xml.dom` module by writing Python functions to navigate and modify an XML document. **Problem Statement:** You are provided with an XML document represented as a string, and you need to perform several operations on it using the `xml.dom` module. Implement the following functions: 1. **parse_xml(xml_string)** Parse the given XML string into a `Document` object. **Input:** - `xml_string` (str): The XML document as a string. **Output:** - `document` (Document): The parsed XML document object. 2. **find_element_by_tag_name(document, tag_name)** Find the first element in the document with the specified tag name. **Input:** - `document` (Document): The XML document object. - `tag_name` (str): The tag name to search for. **Output:** - `element` (Element or None): The first element with the specified tag name, or `None` if no such element is found. 3. **add_element(document, parent_tag, new_element_tag, new_element_text)** Add a new element with the specified tag and text as a child to the first element with the `parent_tag`. **Input:** - `document` (Document): The XML document object. - `parent_tag` (str): The tag name of the parent element. - `new_element_tag` (str): The tag name of the new element to add. - `new_element_text` (str): The text content of the new element. **Output:** - `updated_document` (Document): The updated XML document object. 4. **remove_element_by_tag_name(document, tag_name)** Remove the first element with the specified tag name from the document. **Input:** - `document` (Document): The XML document object. - `tag_name` (str): The tag name of the element to remove. **Output:** - `updated_document` (Document): The updated XML document object. 5. **serialize_document(document)** Convert the XML document object back to a string. **Input:** - `document` (Document): The XML document object. **Output:** - `xml_string` (str): The XML document as a string. **Constraints:** - The input XML string is guaranteed to be well-formed. - Tag names will always be valid. - Ensure proper exception handling for DOM-related errors. **Example Usage:** ```python xml_string = <root> <parent> <child>Content</child> </parent> </root> # Parsing XML document = parse_xml(xml_string) # Finding an element element = find_element_by_tag_name(document, \'child\') print(element.tagName) # Output: child # Adding a new element updated_document = add_element(document, \'parent\', \'new_child\', \'New Content\') # Removing an element updated_document = remove_element_by_tag_name(document, \'child\') # Serializing document new_xml_string = serialize_document(updated_document) print(new_xml_string) ``` **Submission:** Submit your implementations of the functions `parse_xml`, `find_element_by_tag_name`, `add_element`, `remove_element_by_tag_name`, and `serialize_document`.","solution":"from xml.dom.minidom import parseString, Document def parse_xml(xml_string): Parse the given XML string into a Document object. return parseString(xml_string) def find_element_by_tag_name(document, tag_name): Find the first element in the document with the specified tag name. elements = document.getElementsByTagName(tag_name) return elements[0] if elements else None def add_element(document, parent_tag, new_element_tag, new_element_text): Add a new element with the specified tag and text as a child to the first element with the parent_tag. parent = find_element_by_tag_name(document, parent_tag) if parent: new_elem = document.createElement(new_element_tag) new_elem.appendChild(document.createTextNode(new_element_text)) parent.appendChild(new_elem) return document def remove_element_by_tag_name(document, tag_name): Remove the first element with the specified tag name from the document. element = find_element_by_tag_name(document, tag_name) if element and element.parentNode: element.parentNode.removeChild(element) return document def serialize_document(document): Convert the XML document object back to a string. return document.toxml()"},{"question":"**Question**: You are tasked with implementing and evaluating two semi-supervised learning techniques (`Self Training` and `Label Propagation`) on a given dataset using scikit-learn. # Objective: The goal is to observe and compare the performance of these two algorithms when applied to the same dataset. # Dataset: Use the `digits` dataset available in scikit-learn: ```python from sklearn.datasets import load_digits # Load and prepare the dataset digits = load_digits() X, y = digits.data, digits.target # Introduce unlabeled data by setting 50% of the labels to -1 import numpy as np rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(len(y)) < 0.5 y[random_unlabeled_points] = -1 ``` # Tasks: 1. **Implementing Self Training**: - Use a Decision Tree classifier as the base estimator. - Set a threshold of `0.75`. - Keep other parameters of `SelfTrainingClassifier` to default. 2. **Implementing Label Propagation**: - Use the `LabelPropagation` with the RBF kernel. - Set `gamma=0.25`. 3. **Evaluation**: - Train both models on the dataset and predict the labels for the unlabeled data. - Measure and compare the accuracy of both models using the known labels (i.e., non `-1` labels). # Implementation: 1. **Self Training Classifier**: ```python from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # Split the original data into labeled and unlabeled labeled_mask = y != -1 unlabeled_mask = y == -1 # Implement Self Training base_estimator = DecisionTreeClassifier() self_training_clf = SelfTrainingClassifier(base_estimator, threshold=0.75) # Train the model self_training_clf.fit(X, y) # Predict using the trained model y_self_training_pred = self_training_clf.predict(X) # Calculate accuracy for labeled data accuracy_self_training = accuracy_score(digits.target[labeled_mask], y_self_training_pred[labeled_mask]) print(f\'Self Training Classifier Accuracy: {accuracy_self_training}\') ``` 2. **Label Propagation**: ```python from sklearn.semi_supervised import LabelPropagation from sklearn.metrics import accuracy_score # Implement Label Propagation label_propagation_clf = LabelPropagation(kernel=\'rbf\', gamma=0.25) # Train the model label_propagation_clf.fit(X, y) # Predict using the trained model y_label_propagation_pred = label_propagation_clf.predict(X) # Calculate accuracy for labeled data accuracy_label_propagation = accuracy_score(digits.target[labeled_mask], y_label_propagation_pred[labeled_mask]) print(f\'Label Propagation Classifier Accuracy: {accuracy_label_propagation}\') ``` 3. **Comparison and Conclusion**: - Print and comment on the accuracies of both models. - Discuss in what scenarios one method might be preferred over the other. # Constraints: - Use Python 3.x and scikit-learn library. - Execute and provide the output of the code, along with observations. # Expected Output: - Code implementation. - Accuracy of both models. - Commentary on the results. # Evaluation Criteria: - Correctness of code implementation. - Proper usage of scikit-learn APIs. - Accuracy and performance analysis. - Clear and concise commentary on results and observations.","solution":"from sklearn.datasets import load_digits import numpy as np from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # Load and prepare the dataset digits = load_digits() X, y = digits.data, digits.target # Introduce unlabeled data by setting 50% of the labels to -1 rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(len(y)) < 0.5 y[random_unlabeled_points] = -1 # Split the original data into labeled and unlabeled labeled_mask = y != -1 unlabeled_mask = y == -1 # Self Training classifier base_estimator = DecisionTreeClassifier() self_training_clf = SelfTrainingClassifier(base_estimator, threshold=0.75) # Train the self-training model self_training_clf.fit(X, y) # Predict using the trained self-training model y_self_training_pred = self_training_clf.predict(X) # Calculate accuracy for labeled data accuracy_self_training = accuracy_score(digits.target[labeled_mask], y_self_training_pred[labeled_mask]) print(f\'Self Training Classifier Accuracy: {accuracy_self_training}\') # Label Propagation classifier label_propagation_clf = LabelPropagation(kernel=\'rbf\', gamma=0.25) # Train the label propagation model label_propagation_clf.fit(X, y) # Predict using the trained label propagation model y_label_propagation_pred = label_propagation_clf.predict(X) # Calculate accuracy for labeled data accuracy_label_propagation = accuracy_score(digits.target[labeled_mask], y_label_propagation_pred[labeled_mask]) print(f\'Label Propagation Classifier Accuracy: {accuracy_label_propagation}\')"},{"question":"# Custom Color Palette using `seaborn` You are tasked with creating a custom color palette using `seaborn` and visualizing this palette with a plot. Your implementation should include functionalities for dynamic customization based on various input parameters. Function Specification 1. **Function Name**: `create_custom_palette` 2. **Inputs**: - `num_colors` (int, optional): The number of discrete colors to generate (default is 6). - `as_cmap` (bool, optional): Whether to return the palette as a continuous colormap (default is False). - `start` (float, optional): The starting point of the helix (default is 0.5). - `rot` (float, optional): The number of rotations in the helix (default is 1.0). - `gamma` (float, optional): Gamma factor to apply to the luminance (default is 1.0). - `hue` (float, optional): The saturation factor to apply to the colors (default is 0.8). - `dark` (float, optional): Luminance value at the start (default is 0.2). - `light` (float, optional): Luminance value at the end (default is 0.8). - `reverse` (bool, optional): If True, reverse the direction of the luminance ramp (default is False). 3. **Output**: - Returns the generated palette or colormap. 4. **Constraints**: - Ensure that all parameters fall within valid ranges as specified by seaborn, e.g., `rot` should logically fit within -2 to 2. - Utilize seaborn\'s built-in methods to create and visualize the color palette. Example Usage ```python def create_custom_palette(num_colors=6, as_cmap=False, start=0.5, rot=1.0, gamma=1.0, hue=0.8, dark=0.2, light=0.8, reverse=False): # Your implementation here # Example function call palette = create_custom_palette(num_colors=8, start=2, rot=-.75, dark=.1, light=.9, reverse=True) # Visualize the palette import seaborn as sns sns.palplot(palette) # assuming palette is the generated palette to visualize ``` **Note**: You may refer to seaborn documentation for detailed functionalities and parameter constraints while implementing the function.","solution":"import seaborn as sns def create_custom_palette(num_colors=6, as_cmap=False, start=0.5, rot=1.0, gamma=1.0, hue=0.8, dark=0.2, light=0.8, reverse=False): Create a custom color palette using seaborn\'s cubehelix_palette. Parameters: - num_colors (int): The number of discrete colors to generate (default is 6). - as_cmap (bool): Whether to return the palette as a continuous colormap (default is False). - start (float): The starting point of the helix (default is 0.5). - rot (float): The number of rotations in the helix (default is 1.0). - gamma (float): Gamma factor to apply to the luminance (default is 1.0). - hue (float): The saturation factor to apply to the colors (default is 0.8). - dark (float): Luminance value at the start (default is 0.2). - light (float): Luminance value at the end (default is 0.8). - reverse (bool): If True, reverse the direction of the luminance ramp (default is False). Returns: - A seaborn palette or colormap based on the input parameters. palette = sns.cubehelix_palette( n_colors=num_colors, start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light, reverse=reverse, as_cmap=as_cmap ) return palette"},{"question":"Problem Statement # Context In Python, type hinting can often require the use of generic types. Python 3.9 introduced `GenericAlias` objects, which assist in handling these generics more efficiently. This becomes particularly important when dealing with collections or user-defined types that should work with various data types. # Task You are required to implement a simplified version of the `GenericAlias` functionality in Python. # Requirements 1. Implement a class `SimpleGenericAlias`. 2. The class should: - Take two arguments during initialization: `origin` and `args`. - Store these arguments in instance variables `__origin__` and `__args__`. - Automatically convert `args` to a tuple if it\'s not already one. - Provide a method `get_alias()` that returns a tuple containing `origin` and `args`. # Constraints - The `origin` argument is expected to be a type (e.g., `list`, `dict`, etc.). - The `args` argument should be any object or a collection of objects. # Example ```python class SimpleGenericAlias: def __init__(self, origin, args): # Implement your code here def get_alias(self): # Implement your code here # Example Usage alias = SimpleGenericAlias(list, int) print(alias.get_alias()) # Output: (<class \'list\'>, (<class \'int\'>,)) alias = SimpleGenericAlias(dict, (str, int)) print(alias.get_alias()) # Output: (<class \'dict\'>, (<class \'str\'>, <class \'int\'>)) ``` # Additional Notes - Your implementation should be robust and handle edge cases, such as `args` already being a tuple. - Focus on clarity and efficiency in your code. - Ensure that your class mimics the behavior of the built-in `GenericAlias` closely.","solution":"class SimpleGenericAlias: def __init__(self, origin, args): self.__origin__ = origin if not isinstance(args, tuple): args = (args,) self.__args__ = args def get_alias(self): return (self.__origin__, self.__args__)"},{"question":"**Objective:** Implement a Python application that interacts with a web server and processes data from a response using modules from the Python library described above. # Problem Statement: You are tasked with creating a Python application that performs the following steps: 1. Retrieves data from a web server using HTTP GET request. 2. Parses the response to extract specific information. 3. Uses the extracted information to perform an IP range calculation. # Requirements: 1. **HTTP GET Request:** - Use `http.client` to perform an HTTP GET request to the following URL: `http://example.com/data`. - Ensure that the request includes a custom header `X-Special-Header` with a value of `Python310`. 2. **Parsing the Response:** - Parse the retrieved HTTP response (which is in JSON format) to extract a list of IP addresses. Assume the response format is: ```json { \\"status\\": \\"success\\", \\"data\\": [ {\\"ip\\": \\"192.168.1.1\\"}, {\\"ip\\": \\"192.168.1.2\\"}, ... ] } ``` 3. **IP Range Calculation:** - Use the `ipaddress` module to calculate the network range that the list of IP addresses belongs to. Calculate both the network address and the broadcast address. - Output the network and broadcast address. # Input and Output: - **Input:** - None (the data is retrieved from `http://example.com/data`). - **Output:** - Print the network address and broadcast address in the format: ``` Network Address: <network address> Broadcast Address: <broadcast address> ``` # Constraints: - Do not use any third-party libraries; only use the standard Python library as mentioned in the documentation. - Handle potential exceptions that may occur during HTTP requests or data parsing. # Example Usage: Assume the HTTP response retrieved is: ```json { \\"status\\": \\"success\\", \\"data\\": [ {\\"ip\\": \\"192.168.1.1\\"}, {\\"ip\\": \\"192.168.1.2\\"}, {\\"ip\\": \\"192.168.1.3\\"}, {\\"ip\\": \\"192.168.1.4\\"} ] } ``` The output should be: ``` Network Address: 192.168.1.0 Broadcast Address: 192.168.1.255 ``` # Hints: - Use `http.client.HTTPConnection` to make the HTTP request. - Utilize the `json` module to parse the response. - Use `ipaddress.ip_network` to calculate the network and broadcast address.","solution":"import http.client import json import ipaddress def fetch_data(url): Fetches data using HTTP GET request. conn = http.client.HTTPConnection(\\"example.com\\") headers = {\'X-Special-Header\': \'Python310\'} conn.request(\\"GET\\", \\"/data\\", headers=headers) response = conn.getresponse() if response.status != 200: raise Exception(f\\"Failed to fetch data. Status code: {response.status}\\") data = response.read().decode() return json.loads(data) def parse_ip_addresses(json_data): Parses the JSON data to extract a list of IP addresses. if json_data.get(\'status\') != \'success\': raise ValueError(\\"Invalid response status\\") ip_list = [item[\'ip\'] for item in json_data[\'data\']] return ip_list def calculate_network_range(ip_list): Calculates the network and broadcast address for the given list of IP addresses. if not ip_list: raise ValueError(\\"No IP addresses provided\\") # Use the first IP address to calculate the entire network range for simplicity network = ipaddress.ip_network(f\\"{ip_list[0]}/24\\", strict=False) return network.network_address, network.broadcast_address def main(): try: url = \\"http://example.com/data\\" json_data = fetch_data(url) ip_list = parse_ip_addresses(json_data) network_address, broadcast_address = calculate_network_range(ip_list) print(f\\"Network Address: {network_address}\\") print(f\\"Broadcast Address: {broadcast_address}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Uncomment the line below to run the main function # main()"},{"question":"# Grouped Data Analysis and Visualization with Pandas Objective You are provided with a dataset of sales transactions across different stores for various products. Your task is to perform a series of analysis using the `GroupBy` functionalities in pandas, and subsequently visualize some of the results. Dataset Download the dataset from the following link: [sales_data.csv](#) (link here should point to a valid CSV file containing the data). Structure of the dataset: - `date`: Date of the transaction (string, format: `YYYY-MM-DD`). - `store_id`: Unique identifier for the store (integer). - `product_id`: Unique identifier for the product (integer). - `sales_amount`: Sales amount for the transaction (float). - `quantity`: Quantity of items sold (integer). Tasks 1. **Read the Dataset**: - Read the CSV file into a pandas DataFrame. 2. **GroupBy Analysis**: - **Total Sales and Total Quantity**: - Group the data by `store_id` and `product_id`. - Calculate the total sales amount and total quantity sold for each `store_id` and `product_id` combination. - Output the result as a DataFrame with columns: `store_id`, `product_id`, `total_sales_amount`, `total_quantity_sold`. 3. **Time-Series Analysis**: - Group the data by `date`. - Calculate the daily total sales amount and total quantity sold. - Output the result as a DataFrame with columns: `date`, `daily_sales_amount`, `daily_quantity_sold`. 4. **Descriptive Statistics**: - Calculate the mean, median, and standard deviation of the `sales_amount` and `quantity` grouped by `store_id`. - Output the result as a DataFrame with columns: `store_id`, `mean_sales_amount`, `median_sales_amount`, `std_sales_amount`, `mean_quantity`, `median_quantity`, `std_quantity`. 5. **Visualization**: - Create a line plot of the daily total sales amount and daily total quantities sold over time. - Create a bar plot showing the total sales amount for each store. Constraints - You must use pandas for all data manipulations, calculations, and visualizations. - Efficiently handle the dataset assuming it can be quite large. Function Signatures - `def read_sales_data(file_path: str) -> pd.DataFrame:` - `def group_by_store_and_product(df: pd.DataFrame) -> pd.DataFrame:` - `def daily_time_series(df: pd.DataFrame) -> pd.DataFrame:` - `def store_descriptive_statistics(df: pd.DataFrame) -> pd.DataFrame:` - `def plot_sales_and_quantity_over_time(df: pd.DataFrame) -> None:` - `def plot_total_sales_per_store(df: pd.DataFrame) -> None:` Indicative function usage: ```python # Sample main function demonstrating function usage def main(): sales_data = read_sales_data(\'sales_data.csv\') store_product_summary = group_by_store_and_product(sales_data) daily_sales_summary = daily_time_series(sales_data) store_stats = store_descriptive_statistics(sales_data) plot_sales_and_quantity_over_time(daily_sales_summary) plot_total_sales_per_store(store_product_summary) if __name__ == \\"__main__\\": main() ``` Good luck! Ensure your implementation is clear, efficient, and returns correct results.","solution":"import pandas as pd import matplotlib.pyplot as plt def read_sales_data(file_path: str) -> pd.DataFrame: Reads the CSV file into a pandas DataFrame. return pd.read_csv(file_path) def group_by_store_and_product(df: pd.DataFrame) -> pd.DataFrame: Groups the data by store_id and product_id and calculates total sales amount and total quantity sold. grouped = df.groupby([\'store_id\', \'product_id\']).agg( total_sales_amount=(\'sales_amount\', \'sum\'), total_quantity_sold=(\'quantity\', \'sum\')).reset_index() return grouped def daily_time_series(df: pd.DataFrame) -> pd.DataFrame: Groups the data by date and calculates daily total sales amount and total quantity sold. daily_grouped = df.groupby(\'date\').agg( daily_sales_amount=(\'sales_amount\', \'sum\'), daily_quantity_sold=(\'quantity\', \'sum\')).reset_index() return daily_grouped def store_descriptive_statistics(df: pd.DataFrame) -> pd.DataFrame: Calculates mean, median, and standard deviation of sales_amount and quantity grouped by store_id. stats = df.groupby(\'store_id\').agg( mean_sales_amount=(\'sales_amount\', \'mean\'), median_sales_amount=(\'sales_amount\', \'median\'), std_sales_amount=(\'sales_amount\', \'std\'), mean_quantity=(\'quantity\', \'mean\'), median_quantity=(\'quantity\', \'median\'), std_quantity=(\'quantity\', \'std\')).reset_index() return stats def plot_sales_and_quantity_over_time(df: pd.DataFrame) -> None: Creates a line plot of daily total sales amount and daily total quantities sold over time. df[\'date\'] = pd.to_datetime(df[\'date\']) df = df.sort_values(\'date\') fig, ax1 = plt.subplots() ax1.set_xlabel(\'Date\') ax1.set_ylabel(\'Daily Sales Amount\', color=\'tab:blue\') ax1.plot(df[\'date\'], df[\'daily_sales_amount\'], color=\'tab:blue\', label=\'Daily Sales Amount\') ax1.tick_params(axis=\'y\', labelcolor=\'tab:blue\') ax2 = ax1.twinx() ax2.set_ylabel(\'Daily Quantity Sold\', color=\'tab:orange\') ax2.plot(df[\'date\'], df[\'daily_quantity_sold\'], color=\'tab:orange\', label=\'Daily Quantity Sold\') ax2.tick_params(axis=\'y\', labelcolor=\'tab:orange\') fig.tight_layout() plt.title(\'Daily Sales Amount and Quantity Sold Over Time\') plt.show() def plot_total_sales_per_store(df: pd.DataFrame) -> None: Creates a bar plot showing the total sales amount for each store. total_sales_per_store = df.groupby(\'store_id\').agg(total_sales_amount=(\'total_sales_amount\', \'sum\')).reset_index() total_sales_per_store.plot(kind=\'bar\', x=\'store_id\', y=\'total_sales_amount\', legend=False) plt.xlabel(\'Store ID\') plt.ylabel(\'Total Sales Amount\') plt.title(\'Total Sales Amount per Store\') plt.show()"},{"question":"Given a string representing HTML-like data, write a Python function `extract_attributes(html: str) -> dict` that extracts all HTML tag attributes and their corresponding values. The function should be able to handle self-closing tags and tags that may contain additional spaces or newline characters within the tag. Input Format - `html` (string): A string containing HTML-like data. Output Format - The function should return a dictionary where each key is the attribute name and the value is its corresponding value from the first HTML tag found. Constraints - Assume well-formed HTML input (every opening tag has a corresponding closing tag where required). - Handle lower and uppercase attribute names equally (e.g., `HREF` and `href` should be treated equally). Example ```python def extract_attributes(html: str) -> dict: pass # Example Usage: html_string = \'<a href=\\"https://example.com\\" title=\\"Example Site\\" id=\\"link1\\">\' assert extract_attributes(html_string) == { \'href\': \'https://example.com\', \'title\': \'Example Site\', \'id\': \'link1\' } html_string_2 = \'<img src=\\"/images/sample.jpg\\" alt=\\"Sample Image\\" width=\\"500\\" height=\\"600\\" />\' assert extract_attributes(html_string_2) == { \'src\': \'/images/sample.jpg\', \'alt\': \'Sample Image\', \'width\': \'500\', \'height\': \'600\' } ``` Function Specifications 1. **Function Name**: `extract_attributes` 2. **Parameters**: `html` — a string representing HTML-like data. 3. **Returns**: A dictionary of attribute-value pairs 4. **Requirements**: - Use the `re` module for pattern matching and extraction. - Ensure attribute names are case-insensitive, always normalized to lowercase in the result. - Handle potential spaces or newlines within the tag gracefully. Additional Information - Treat the first HTML tag encountered as the one to extract attributes from. - You may assume the presence of at least one tag in the input string.","solution":"import re from html import unescape def extract_attributes(html: str) -> dict: Extracts all HTML tag attributes and their corresponding values from the first HTML tag found. Args: html (str): A string containing HTML-like data. Returns: dict: A dictionary where each key is the attribute name and the value is its corresponding value. # Search for the first tag in the HTML-like string match = re.search(r\'<([a-zA-Z][a-zA-Z0-9]*)[^>]*>\', html) if not match: return {} tag = match.group(0) # Extract attributes from the tag attribute_pattern = re.compile(r\'([a-zA-Z][-a-zA-Z0-9]*)s*=s*\\"([^\\"]*)\\"|([a-zA-Z][-a-zA-Z0-9]*)s*=s*\'([^\']*)\'\') attributes = {} for attr_match in attribute_pattern.finditer(tag): attr_name = attr_match.group(1) or attr_match.group(3) attr_value = attr_match.group(2) or attr_match.group(4) attributes[attr_name.lower()] = unescape(attr_value) return attributes"},{"question":"# Data Analysis and Transformation with pandas Window Functions **Problem Statement:** You are provided with a time series dataset containing the daily sales of a product over several months. Your task is to analyze this dataset using pandas window functions to gain insights into sales trends and fluctuations. Specifically, you need to perform rolling window operations, expanding window operations, and exponentially-weighted moving average calculations. **Dataset:** The dataset is a CSV file named `daily_sales.csv`, which has the following columns: - `date`: The date of the sales record. - `sales`: The number of units sold on that date. Here is a sample of the dataset: ```csv date,sales 2021-01-01,100 2021-01-02,150 2021-01-03,120 ... ``` **Tasks:** 1. **Rolling Window Operations**: - Calculate the 7-day rolling average of daily sales. - Calculate the 7-day rolling standard deviation of daily sales. 2. **Expanding Window Operations**: - Calculate the cumulative sum of daily sales. 3. **Exponentially-weighted Moving Average (EWMA) Calculations**: - Calculate the 30-day EWMA of daily sales with a span of 30. **Implementation Details:** Implement the function `analyze_sales` which takes the file path of the dataset as input and returns a DataFrame with the following columns: - `date`: The date of the sales record. - `sales`: The number of units sold on that date. - `7_day_moving_avg`: The 7-day rolling average of daily sales. - `7_day_moving_std`: The 7-day rolling standard deviation of daily sales. - `cumulative_sales`: The cumulative sum of daily sales. - `30_day_ewma`: The 30-day EWMA of daily sales. ```python import pandas as pd def analyze_sales(file_path: str) -> pd.DataFrame: # Load the dataset df = pd.read_csv(file_path) # Ensure the `date` column is in datetime format df[\'date\'] = pd.to_datetime(df[\'date\']) # Sort the dataframe by date df = df.sort_values(\'date\') # Calculate the 7-day rolling average df[\'7_day_moving_avg\'] = df[\'sales\'].rolling(window=7).mean() # Calculate the 7-day rolling standard deviation df[\'7_day_moving_std\'] = df[\'sales\'].rolling(window=7).std() # Calculate the cumulative sum df[\'cumulative_sales\'] = df[\'sales\'].expanding().sum() # Calculate the 30-day EWMA df[\'30_day_ewma\'] = df[\'sales\'].ewm(span=30, adjust=False).mean() return df ``` **Constraints and Assumptions:** - Assume the dataset is well-formed and contains no missing dates. - Assume there are no negative sales values. The function should return the DataFrame with the newly calculated columns appended. **Performance Requirements:** - You may assume the dataset is relatively small (up to a few thousand rows), so focus on code clarity and correctness rather than extreme performance optimization.","solution":"import pandas as pd def analyze_sales(file_path: str) -> pd.DataFrame: # Load the dataset df = pd.read_csv(file_path) # Ensure the `date` column is in datetime format df[\'date\'] = pd.to_datetime(df[\'date\']) # Sort the dataframe by date df = df.sort_values(\'date\') # Calculate the 7-day rolling average df[\'7_day_moving_avg\'] = df[\'sales\'].rolling(window=7).mean() # Calculate the 7-day rolling standard deviation df[\'7_day_moving_std\'] = df[\'sales\'].rolling(window=7).std() # Calculate the cumulative sum df[\'cumulative_sales\'] = df[\'sales\'].expanding().sum() # Calculate the 30-day EWMA df[\'30_day_ewma\'] = df[\'sales\'].ewm(span=30, adjust=False).mean() return df"},{"question":"# **Regular Expression Operations: Text Processing** You are tasked with processing a large text file containing a specific structured format of data entries. Each data entry is on a new line and contains the following information: name, date, and email address. The format can be described as: ``` <name> - <date> - <email> ``` Where: - `<name>` is a string containing alphabetic characters and may include spaces (e.g., \\"John Doe\\"). - `<date>` is in the format dd/mm/yyyy (e.g., \\"12/10/1995\\"). - `<email>` is a valid email address (e.g., \\"john.doe@example.com\\"). Design a function `process_text` that takes a string `text_content` as input, which represents the content of the text file. The function should: 1. **Extract and validate** each data entry using regular expressions. 2. **Return a list** of dictionaries, where each dictionary contains the extracted `name`, `date` and `email` of valid entries. Input Format: - `text_content` (string): A string representing the content of the text file, with each data entry on a new line. Output Format: - A list of dictionaries, where each dictionary has the keys `name`, `date`, and `email`, representing valid data entries. Constraints: - Consider entries to be invalid if they do not match the specified format exactly. - The function should only include valid entries in the output list. Example: Given the input string: ``` John Doe - 12/12/1995 - john.doe@example.com Jane Smith - 30/02/2000 - jane.smith@example Invalid Name - 15-07-1985 - invalid-email@com Will Johnson - 22/11/1978 - will.johnson78@example.net ``` The function `process_text` should return: ```python [ {\\"name\\": \\"John Doe\\", \\"date\\": \\"12/12/1995\\", \\"email\\": \\"john.doe@example.com\\"}, {\\"name\\": \\"Will Johnson\\", \\"date\\": \\"22/11/1978\\", \\"email\\": \\"will.johnson78@example.net\\"} ] ``` Implementation: Implement your solution in the function below: ```python import re def process_text(text_content): # Your implementation here pass ``` Notes: - Remember to use `re.compile` to compile your regular expression pattern for efficiency. - Consider using named groups in your regular expression to capture `name`, `date`, and `email` for easier dictionary creation.","solution":"import re def process_text(text_content): # Define the regex pattern with named groups pattern = re.compile(r\'^(?P<name>[a-zA-Zs]+) - (?P<date>d{2}/d{2}/d{4}) - (?P<email>[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,})\') # Initialize an empty list to collect the valid entries valid_entries = [] # Split the input text content into lines lines = text_content.strip().split(\\"n\\") for line in lines: # Try to match each line with the pattern match = pattern.match(line) if match: # If valid, create a dictionary from named groups and add to the list valid_entries.append(match.groupdict()) return valid_entries"},{"question":"**Question: Implement a Self-Attention Mechanism Using PyTorch** **Objective:** Write a PyTorch implementation of a basic self-attention mechanism that can be applied within a neural network layer. This exercise will test your understanding of attention mechanisms and your ability to use PyTorch to implement complex neural network modules. **Requirements:** 1. **Module Definition:** - Define a class `SelfAttention` inheriting from `torch.nn.Module`. 2. **Input and Parameters:** - The input to the self-attention layer will be a tensor of shape `(batch_size, sequence_length, embedding_dim)` where: - `batch_size` is the number of sequences in a batch, - `sequence_length` is the length of each sequence, - `embedding_dim` is the dimensionality of the embeddings. - The module should have learned parameters for the query, key, and value linear transformations (i.e., three linear layers). 3. **Attention Mechanism:** - Implement the scaled dot-product attention mechanism. - Use appropriate softmax normalization on the attention scores. - Compute the output of the self-attention mechanism as the weighted sum of the value vectors. 4. **Output:** - The output should be a tensor of the same shape as the input `(batch_size, sequence_length, embedding_dim)`. **Constraints:** - You should use PyTorch functionalities (e.g., `torch.nn.Linear`, `torch.softmax`, etc.). - Ensure your implementation is modular and efficient, using matrix multiplications when possible. **Performance Requirement:** - The implementation should be optimized to handle large batches and sequences typically found in NLP tasks. **Implementation Skeleton:** ```python import torch import torch.nn.functional as F class SelfAttention(torch.nn.Module): def __init__(self, embedding_dim): super(SelfAttention, self).__init__() self.embedding_dim = embedding_dim self.query = torch.nn.Linear(embedding_dim, embedding_dim) self.key = torch.nn.Linear(embedding_dim, embedding_dim) self.value = torch.nn.Linear(embedding_dim, embedding_dim) def forward(self, x): # Input shape: (batch_size, sequence_length, embedding_dim) # Compute Query, Key and Value matrices Q = self.query(x) # (batch_size, sequence_length, embedding_dim) K = self.key(x) # (batch_size, sequence_length, embedding_dim) V = self.value(x) # (batch_size, sequence_length, embedding_dim) # Compute scaled dot-product attention attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.embedding_dim ** 0.5) attention_weights = F.softmax(attention_scores, dim=-1) output = torch.matmul(attention_weights, V) # (batch_size, sequence_length, embedding_dim) return output # Example usage: # embedding_dim = 64 # batch_size = 32 # sequence_length = 10 # x = torch.randn(batch_size, sequence_length, embedding_dim) # self_attention = SelfAttention(embedding_dim) # output = self_attention(x) # print(output.shape) # Should be (batch_size, sequence_length, embedding_dim) ``` In this task, you are expected to complete the implementation of the `SelfAttention` class and ensure it passes the example usage scenarios. You are also welcome to add any additional advanced features or improvements as you see fit.","solution":"import torch import torch.nn.functional as F class SelfAttention(torch.nn.Module): def __init__(self, embedding_dim): super(SelfAttention, self).__init__() self.embedding_dim = embedding_dim self.query = torch.nn.Linear(embedding_dim, embedding_dim) self.key = torch.nn.Linear(embedding_dim, embedding_dim) self.value = torch.nn.Linear(embedding_dim, embedding_dim) def forward(self, x): # Input shape: (batch_size, sequence_length, embedding_dim) # Compute Query, Key and Value matrices Q = self.query(x) # (batch_size, sequence_length, embedding_dim) K = self.key(x) # (batch_size, sequence_length, embedding_dim) V = self.value(x) # (batch_size, sequence_length, embedding_dim) # Compute scaled dot-product attention attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.embedding_dim ** 0.5) attention_weights = F.softmax(attention_scores, dim=-1) output = torch.matmul(attention_weights, V) # (batch_size, sequence_length, embedding_dim) return output # Example usage: # embedding_dim = 64 # batch_size = 32 # sequence_length = 10 # x = torch.randn(batch_size, sequence_length, embedding_dim) # self_attention = SelfAttention(embedding_dim) # output = self_attention(x) # print(output.shape) # Should be (batch_size, sequence_length, embedding_dim)"},{"question":"Hyper-Parameter Tuning with Scikit-Learn **Objective:** Demonstrate your understanding of the hyper-parameter tuning process in scikit-learn using both grid search and randomized search methodologies. **Problem Statement:** You are given a dataset and your task is to perform hyper-parameter tuning on a Support Vector Classifier (SVC) model using both Grid Search and Randomized Search with cross-validation. 1. Load the **Iris Dataset** from `sklearn.datasets`. 2. Perform the following steps using Grid Search: - Define a parameter grid with the following parameters for SVC: - `C`: [0.1, 1, 10, 100] - `kernel`: [\'linear\', \'rbf\'] - If `kernel` is \'rbf\', include the parameter `gamma`: [0.001, 0.01, 0.1, 1] - Implement a 5-fold cross-validation strategy. - Use accuracy as the scoring metric. - Identify and print the best parameters and the corresponding accuracy score. 3. Perform the following steps using Randomized Search: - Define a parameter distribution with the following parameters for SVC: - `C`: loguniform(0.1, 100) - `kernel`: [\'linear\', \'rbf\'] - If `kernel` is \'rbf\', include the parameter `gamma`: loguniform(0.001, 1) - Implement a 5-fold cross-validation strategy. - Use accuracy as the scoring metric. - Set the number of iterations (`n_iter`) to 20. - Identify and print the best parameters and the corresponding accuracy score. **Constraints:** - You are required to use the `GridSearchCV` and `RandomizedSearchCV` classes from `sklearn.model_selection`. - Ensure the outputs are printed clearly for comparison. **Expected Input and Output:** - **Input:** - The Iris dataset loaded from `sklearn.datasets`. - **Output:** - Best parameters for Grid Search and their corresponding accuracy score. - Best parameters for Randomized Search and their corresponding accuracy score. **Implementation Details:** - Implement the solution in a function named `perform_hyperparameter_tuning`. - The function should not take any parameters or return any values. It should load the dataset and print the results directly. ```python def perform_hyperparameter_tuning(): import numpy as np from sklearn import datasets from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.svm import SVC from scipy.stats import loguniform # Load Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Grid Search param_grid = [ {\'C\': [0.1, 1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [0.1, 1, 10, 100], \'kernel\': [\'rbf\'], \'gamma\': [0.001, 0.01, 0.1, 1]} ] grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X, y) print(\\"Grid Search Best Parameters:\\", grid_search.best_params_) print(\\"Grid Search Best Accuracy:\\", grid_search.best_score_) # Randomized Search param_dist = { \'C\': loguniform(0.1, 100), \'kernel\': [\'linear\', \'rbf\'], \'gamma\': loguniform(0.001, 1) } randomized_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=20, cv=5, scoring=\'accuracy\') randomized_search.fit(X, y) print(\\"Randomized Search Best Parameters:\\", randomized_search.best_params_) print(\\"Randomized Search Best Accuracy:\\", randomized_search.best_score_) # Call the function to execute hyperparameter tuning perform_hyperparameter_tuning() ``` **Note:** Ensure that the code runs correctly by importing necessary libraries and handling any random state for reproducibility.","solution":"def perform_hyperparameter_tuning(): import numpy as np from sklearn import datasets from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.svm import SVC from scipy.stats import loguniform # Load Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Grid Search param_grid = [ {\'C\': [0.1, 1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [0.1, 1, 10, 100], \'kernel\': [\'rbf\'], \'gamma\': [0.001, 0.01, 0.1, 1]} ] grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X, y) print(\\"Grid Search Best Parameters:\\", grid_search.best_params_) print(\\"Grid Search Best Accuracy:\\", grid_search.best_score_) # Randomized Search param_dist = { \'C\': loguniform(0.1, 100), \'kernel\': [\'linear\', \'rbf\'] } param_dist_rbf = { \'C\': loguniform(0.1, 100), \'kernel\': [\'rbf\'], \'gamma\': loguniform(0.001, 1) } param_dist_combined = param_dist.copy() param_dist_combined.update(param_dist_rbf) randomized_search = RandomizedSearchCV(SVC(), param_distributions=param_dist_combined, n_iter=20, cv=5, scoring=\'accuracy\', random_state=42) randomized_search.fit(X, y) print(\\"Randomized Search Best Parameters:\\", randomized_search.best_params_) print(\\"Randomized Search Best Accuracy:\\", randomized_search.best_score_) # Call the function to execute hyperparameter tuning perform_hyperparameter_tuning()"},{"question":"# Asyncio Task Management and Synchronization Background The asyncio library in Python allows for writing concurrent code using the async/await syntax. It provides high-level APIs to run coroutines concurrently, manage network IO, and synchronize concurrent code. This assessment focuses on creating, running, and synchronizing coroutines using asyncio. Task You are required to implement a system where multiple asynchronous tasks perform network-like operations concurrently. These tasks should then be synchronized to return their results in the correct sequence. Implement the following function within the constraints of this task: ```python import asyncio async def network_operation(task_id, delay): Simulates a network operation that takes `delay` seconds to complete. Args: - task_id (int): The ID of the task. - delay (int): The time in seconds that the task will take to complete. Returns: - str: A string message indicating the completion of the task. await asyncio.sleep(delay) return f\\"Task {task_id} completed.\\" async def manage_tasks(task_details): Manages and runs multiple network_operation tasks concurrently and ensures their results are collected in order. Args: - task_details (List[Tuple[int, int]]): A list of task details where each tuple contains two elements: task_id and delay. Returns: - List[str]: A list of results from the tasks in the order of their task_ids. pass ``` Input Format - `task_details`: A list of tuples where each tuple contains: - `task_id` (int): The ID of the task. - `delay` (int): The delay in seconds the task will take to complete. Example: ```python task_details = [(1, 5), (2, 1), (3, 3)] ``` Output Format - A list of strings where each string indicates the completion of a task, ordered by the `task_id`. Example: ```python [\\"Task 1 completed.\\", \\"Task 2 completed.\\", \\"Task 3 completed.\\"] ``` Constraints - Each `task_id` is unique. - The `delay` is a non-negative integer. Requirements - Use the asyncio library to manage and run tasks concurrently. - Ensure that the results are returned in the order of their task_ids, regardless of the completion time of each task. Example ```python import asyncio async def network_operation(task_id, delay): await asyncio.sleep(delay) return f\\"Task {task_id} completed.\\" async def manage_tasks(task_details): tasks = [] for task_id, delay in task_details: tasks.append(network_operation(task_id, delay)) results = await asyncio.gather(*tasks) results.sort(key=lambda x: int(x.split()[1])) return results # Example usage task_details = [(1, 5), (2, 1), (3, 3)] asyncio.run(manage_tasks(task_details)) ``` Output: ``` [\\"Task 1 completed.\\", \\"Task 2 completed.\\", \\"Task 3 completed.\\"] ``` Note Make sure to handle the synchronization correctly using asyncio\'s high-level APIs to ensure that all tasks are run concurrently but results are collected in the specified order.","solution":"import asyncio async def network_operation(task_id, delay): Simulates a network operation that takes `delay` seconds to complete. Args: - task_id (int): The ID of the task. - delay (int): The time in seconds that the task will take to complete. Returns: - str: A string message indicating the completion of the task. await asyncio.sleep(delay) return f\\"Task {task_id} completed.\\" async def manage_tasks(task_details): Manages and runs multiple network_operation tasks concurrently and ensures their results are collected in order. Args: - task_details (List[Tuple[int, int]]): A list of task details where each tuple contains two elements: task_id and delay. Returns: - List[str]: A list of results from the tasks in the order of their task_ids. tasks = {} for task_id, delay in task_details: tasks[task_id] = asyncio.create_task(network_operation(task_id, delay)) results = await asyncio.gather(*tasks.values()) sorted_task_ids = sorted(task_details, key=lambda x: x[0]) sorted_results = [task.result() for task_id in sorted_task_ids for task in tasks.values() if f\\"Task {task_id[0]} completed.\\" in task.result()] return sorted_results"},{"question":"**Question:** Your task is to write a Python function that performs the following: 1. Retrieves the current user\'s password database entry using the `pwd` module. 2. Retrieves the user\'s default group information using the `grp` module. 3. Logs an informational message to the Unix syslog with details about the user and group using the `syslog` module. 4. Returns a dictionary containing the user and group information. # Function Signature ```python def get_user_info_and_log() -> dict: pass ``` # Input - The function does not take any arguments. # Output - The function should return a dictionary with the following format: ```python { \\"username\\": str, \\"uid\\": int, \\"gid\\": int, \\"home_directory\\": str, \\"shell\\": str, \\"group_name\\": str, \\"group_gid\\": int } ``` # Constraints - This function will be executed in a Unix-like environment. - Make sure your function does not raise any exceptions and handles possible errors gracefully. - Use appropriate logging levels for syslog. # Example ```python Example output (values may vary based on the system configuration): { \\"username\\": \\"john.doe\\", \\"uid\\": 1000, \\"gid\\": 1000, \\"home_directory\\": \\"/home/john.doe\\", \\"shell\\": \\"/bin/bash\\", \\"group_name\\": \\"john\\", \\"group_gid\\": 1000 } ``` # Performance Requirements - The function should perform all operations efficiently with minimal overhead. # Additional Notes - To log a message to syslog, use the `syslog.syslog` function. - To get the current user\'s information and group data, use the `pwd.getpwuid` and `grp.getgrgid` functions respectively. - You can retrieve the current user\'s UID using `os.getuid()`. This question will test your understanding of Unix-specific modules in Python and your ability to integrate them into a cohesive solution.","solution":"import os import pwd import grp import syslog def get_user_info_and_log() -> dict: try: # Get current user ID uid = os.getuid() # Get password database entry for current user pw_entry = pwd.getpwuid(uid) # Get default group information grp_entry = grp.getgrgid(pw_entry.pw_gid) # Log information to syslog syslog.openlog(logoption=syslog.LOG_PID) syslog.syslog(syslog.LOG_INFO, f\\"User {pw_entry.pw_name} with UID {uid} and GID {pw_entry.pw_gid} logged in. Group: {grp_entry.gr_name} (GID: {grp_entry.gr_gid})\\") # Return user and group information in a dictionary user_info = { \\"username\\": pw_entry.pw_name, \\"uid\\": pw_entry.pw_uid, \\"gid\\": pw_entry.pw_gid, \\"home_directory\\": pw_entry.pw_dir, \\"shell\\": pw_entry.pw_shell, \\"group_name\\": grp_entry.gr_name, \\"group_gid\\": grp_entry.gr_gid } return user_info except Exception as e: # Log any exceptions to syslog as errors syslog.syslog(syslog.LOG_ERR, f\\"Error retrieving user info: {e}\\") return {}"},{"question":"**Objective**: Implement and evaluate biclustering using the Spectral Co-Clustering algorithm from Scikit-learn. **Background**: Biclustering algorithms simultaneously cluster rows and columns of a data matrix. The Spectral Co-Clustering algorithm in Scikit-learn can be used to find biclusters in data matrices by approximating the normalized cut of a bipartite graph representation of the data. **Task**: 1. Create a synthetic data matrix with specific bicluster patterns. 2. Apply the Spectral Co-Clustering algorithm to identify the biclusters. 3. Rearrange the original data matrix to make the biclusters contiguous for visualization. 4. Evaluate the results using external measures like the Jaccard index. **Input and Output**: - **Input**: - `n_rows` (int): Number of rows in the synthetic data matrix. - `n_cols` (int): Number of columns in the synthetic data matrix. - `n_biclusters` (int): Number of biclusters to form in the data matrix. - **Output**: - A visualization of the original data matrix with biclusters made contiguous. - The Jaccard index evaluating the biclusters. **Constraints**: - Use only NumPy and Scikit-learn libraries. - The implementation should handle matrices of size up to 500x500 efficiently. # Instructions 1. **Create a synthetic data matrix**: - Implement a function `create_bicluster_matrix` that generates a matrix of specified dimensions with distinct biclusters. ```python def create_bicluster_matrix(n_rows, n_cols, n_biclusters): Create a data matrix with distinct bicluster patterns. Parameters: n_rows (int): Number of rows in the matrix. n_cols (int): Number of columns in the matrix. n_biclusters (int): Number of biclusters. Returns: np.ndarray: Data matrix with bicluster patterns. # Your code here ``` 2. **Apply Spectral Co-Clustering**: - Implement a function `apply_spectral_coclustering` that fits the Spectral Co-Clustering model on the generated data matrix. ```python from sklearn.cluster import SpectralCoclustering def apply_spectral_coclustering(data): Apply Spectral Co-Clustering to the data matrix. Parameters: data (np.ndarray): Input data matrix. Returns: model (SpectralCoclustering): Fitted model. # Your code here ``` 3. **Rearrange rows and columns**: - Implement a function `rearrange_matrix` that rearranges the rows and columns of the data matrix to make biclusters contiguous for visualization. ```python def rearrange_matrix(data, model): Rearrange the matrix to make bicluster contiguous. Parameters: data (np.ndarray): Input data matrix. model (SpectralCoclustering): Fitted Spectral Co-Clustering model. Returns: np.ndarray: Rearranged matrix. # Your code here ``` 4. **Evaluate Biclustering**: - Implement a function `evaluate_biclustering` that evaluates the biclustering result using the Jaccard index. ```python from sklearn.metrics import jaccard_score def evaluate_biclustering(true_labels, predicted_labels): Evaluate biclustering using the Jaccard index. Parameters: true_labels (np.ndarray): Ground truth row and column labels. predicted_labels (np.ndarray): Predicted row and column labels by the model. Returns: float: Jaccard index score. # Your code here ``` **Example**: Here is an example of how this can be executed: ```python # Step 1: Create synthetic data matrix data_matrix = create_bicluster_matrix(100, 100, 5) # Step 2: Apply Spectral Co-Clustering model = apply_spectral_coclustering(data_matrix) # Step 3: Rearrange rows and columns for visualization rearranged_matrix = rearrange_matrix(data_matrix, model) # Step 4: Evaluate with Jaccard index jaccard_score = evaluate_biclustering(true_labels, predicted_labels) print(f\\"Jaccard Index: {jaccard_score}\\") ``` **Expected Knowledge**: - Understanding of biclustering and the Spectral Co-Clustering algorithm. - Familiarity with NumPy for data manipulation. - Experience with Scikit-learn for implementing clustering algorithms.","solution":"import numpy as np from sklearn.cluster import SpectralCoclustering from sklearn.metrics import jaccard_score def create_bicluster_matrix(n_rows, n_cols, n_biclusters): Create a data matrix with distinct bicluster patterns. Parameters: n_rows (int): Number of rows in the matrix. n_cols (int): Number of columns in the matrix. n_biclusters (int): Number of biclusters. Returns: np.ndarray: Data matrix with bicluster patterns. np.random.seed(0) data = np.zeros((n_rows, n_cols)) row_size = n_rows // n_biclusters col_size = n_cols // n_biclusters for i in range(n_biclusters): row_start = i * row_size col_start = i * col_size data[row_start:row_start+row_size, col_start:col_start+col_size] = 1 + i data += 0.1 * np.random.randn(n_rows, n_cols) return data def apply_spectral_coclustering(data, n_biclusters): Apply Spectral Co-Clustering to the data matrix. Parameters: data (np.ndarray): Input data matrix. n_biclusters (int): Number of biclusters to find. Returns: model (SpectralCoclustering): Fitted model. model = SpectralCoclustering(n_clusters=n_biclusters, random_state=0) model.fit(data) return model def rearrange_matrix(data, model): Rearrange the matrix to make biclusters contiguous. Parameters: data (np.ndarray): Input data matrix. model (SpectralCoclustering): Fitted Spectral Co-Clustering model. Returns: np.ndarray: Rearranged matrix. fit_data = data[np.argsort(model.row_labels_)] fit_data = fit_data[:, np.argsort(model.column_labels_)] return fit_data def evaluate_biclustering(true_labels, predicted_labels): Evaluate biclustering using the Jaccard index. Parameters: true_labels (np.ndarray): Ground truth row and column labels. predicted_labels (np.ndarray): Predicted row and column labels by the model. Returns: float: Jaccard index score. return jaccard_score(true_labels, predicted_labels, average=\'micro\')"},{"question":"Objective Demonstrate your understanding of how to load, manipulate, and use datasets available in the `sklearn.datasets` module. Your solution should include data loading, preprocessing, and a basic machine learning application. Task 1. **Load the `wine` dataset** using the appropriate function from the `sklearn.datasets` package. 2. Perform basic preprocessing: - Standardize the features. - Split the dataset into training and testing sets (80% train, 20% test). 3. Train a k-Nearest Neighbors (k-NN) classifier using the preprocessed dataset. 4. Evaluate the classifier on the test set and return the accuracy. Instructions - Use a random seed of 42 for reproducibility when splitting the dataset. - Use `StandardScaler` from `sklearn.preprocessing` to standardize the features. - Use `KNeighborsClassifier` from `sklearn.neighbors` to train the classifier. - Ensure that your implementation is well-structured and utilizes functions for modularity. Input - No external input; all necessary datasets are available within the `sklearn.datasets` module. Output - Print the accuracy of the k-NN classifier on the test set. Example Output ``` Accuracy of k-NN classifier: 0.75 ``` Constraints - Use k=5 for the k-NN classifier. Performance Requirements The implementation should be efficient, leveraging appropriate scikit-learn utilities for loading, preprocessing, and evaluating the dataset. ```python # Your solution here from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def main(): # Step 1: Load the wine dataset data = load_wine() X, y = data[\'data\'], data[\'target\'] # Step 2: Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Step 4: Train k-NN classifier knn = KNeighborsClassifier(n_neighbors=5) knn.fit(X_train, y_train) # Step 5: Evaluate the classifier y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Print the accuracy print(f\'Accuracy of k-NN classifier: {accuracy:.2f}\') # Run the main function if __name__ == \\"__main__\\": main() ```","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def load_and_preprocess_wine_data(): Loads the wine dataset and performs preprocessing including standardization and train-test split. Returns the train and test datasets. # Load the wine dataset data = load_wine() X, y = data[\'data\'], data[\'target\'] # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def train_and_evaluate_knn(X_train, X_test, y_train, y_test, k=5): Trains a k-NN classifier and evaluates its performance. # Train k-NN classifier knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train, y_train) # Evaluate the classifier y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy def main(): X_train, X_test, y_train, y_test = load_and_preprocess_wine_data() accuracy = train_and_evaluate_knn(X_train, X_test, y_train, y_test) # Print the accuracy print(f\'Accuracy of k-NN classifier: {accuracy:.2f}\') if __name__ == \\"__main__\\": main()"},{"question":"# IP Address Manipulation with Python\'s `ipaddress` Module **Objective:** Implement functions to manipulate and analyze IPv4 and IPv6 addresses and networks using Python\'s `ipaddress` module. These functions should help understand addresses, networks, and the relationships between them. **Requirements:** 1. **Function to Create IP Address**: - **Function Name:** `create_ip_address` - **Input:** A string or integer representing the IP address. - **Output:** A string indicating the type of address (IPv4 or IPv6) and the address itself. ```python def create_ip_address(ip_value): Create an IP address object from a given string or integer. Args: - ip_value (str or int): The IP address in string or integer form. Returns: - str: A statement with the IP address type and the address. pass ``` 2. **Function to Check IP Version**: - **Function Name:** `get_ip_version` - **Input:** A string or integer representing the IP address. - **Output:** An integer representing the IP version (4 or 6). ```python def get_ip_version(ip_value): Get the IP version for the given IP address. Args: - ip_value (str or int): The IP address in string or integer form. Returns: - int: The IP version (4 or 6). pass ``` 3. **Function to Calculate Number of Hosts in a Network**: - **Function Name:** `calculate_num_hosts` - **Input:** A string representing the network (e.g., \'192.0.2.0/24\'). - **Output:** The number of usable host addresses in the network. ```python def calculate_num_hosts(network): Calculate the number of usable host addresses within a network. Args: - network (str): The network in string notation (e.g., \'192.0.2.0/24\'). Returns: - int: The number of usable host addresses. pass ``` 4. **Function to List Usable Hosts in a Network**: - **Function Name:** `list_usable_hosts` - **Input:** A string representing the network (e.g., \'192.0.2.0/24\'). - **Output:** A list of strings, each representing a usable host address in the network. ```python def list_usable_hosts(network): List all usable host addresses in a given network. Args: - network (str): The network in string notation (e.g., \'192.0.2.0/24\'). Returns: - list: A list of usable host addresses in the network. pass ``` **Constraints**: - You must use the `ipaddress` module for all address and network manipulations. - Input IP addresses and networks must be valid. You can assume the inputs are always correct, so there is no need to handle exceptions. - The `create_ip_address` function should automatically detect and handle both IPv4 and IPv6 addresses. **Example Usage**: ```python print(create_ip_address(\'192.0.2.1\')) # Output: \'IPv4 Address: 192.0.2.1\' print(create_ip_address(42540766411282592856903984951653826561)) # Output: \'IPv6 Address: 2001:db8::1\' print(get_ip_version(\'2001:db8::1\')) # Output: 6 print(calculate_num_hosts(\'192.0.2.0/24\')) # Output: 254 print(list_usable_hosts(\'192.0.2.0/30\')) # Output: [\'192.0.2.1\', \'192.0.2.2\'] ``` **Note**: 192.0.2.0/30 only allows for two usable host addresses due to the presence of the network address and broadcast address. Use this opportunity to demonstrate your understanding of the `ipaddress` module and your ability to manipulate and analyze IP addresses and networks efficiently.","solution":"import ipaddress def create_ip_address(ip_value): Create an IP address object from a given string or integer. Args: - ip_value (str or int): The IP address in string or integer form. Returns: - str: A statement with the IP address type and the address. address = ipaddress.ip_address(ip_value) address_type = \'IPv4\' if address.version == 4 else \'IPv6\' return f\\"{address_type} Address: {address}\\" def get_ip_version(ip_value): Get the IP version for the given IP address. Args: - ip_value (str or int): The IP address in string or integer form. Returns: - int: The IP version (4 or 6). address = ipaddress.ip_address(ip_value) return address.version def calculate_num_hosts(network): Calculate the number of usable host addresses within a network. Args: - network (str): The network in string notation (e.g., \'192.0.2.0/24\'). Returns: - int: The number of usable host addresses. net = ipaddress.ip_network(network, strict=False) return net.num_addresses - 2 # Subtract network address and broadcast address def list_usable_hosts(network): List all usable host addresses in a given network. Args: - network (str): The network in string notation (e.g., \'192.0.2.0/24\'). Returns: - list: A list of usable host addresses in the network. net = ipaddress.ip_network(network, strict=False) # Exclude the network and broadcast addresses return [str(host) for host in list(net.hosts())]"},{"question":"**Objective**: The goal of this assignment is to assess your understanding of the `os` module, specifically how to interact with and manipulate environment variables in Python. # Problem Statement Create a Python function named `configure_environment` that performs the following tasks: 1. **Read and print** the current value of an environment variable (if it exists), specified by an argument to the function. 2. **Update** the environment variable with a new value provided as another argument. 3. **Verify** the update by reading and printing the new value of the environment variable. Your function should be capable of handling any environment variable name and value provided to it (as long as they are valid strings). If the environment variable does not exist initially, your function should simply add it to the environment. # Function Signature ```python def configure_environment(variable_name: str, new_value: str) -> None: pass ``` # Input - `variable_name` (str): The name of the environment variable to read and update. - `new_value` (str): The new value to assign to the environment variable. # Output - The function should print the initial value of the environment variable if it exists, otherwise indicate that the variable is being added. - It should also print the new value after setting the environment variable. # Constraints - You must use the `os` module for all environment variable interactions. - Assume that the `variable_name` and `new_value` will always be valid strings. - The maximum length of `variable_name` and `new_value` will not exceed 255 characters. # Example ```python import os def configure_environment(variable_name: str, new_value: str) -> None: # Your implementation here pass # Example Usage configure_environment(\'MY_VAR\', \'new_value\') ``` Expected Output: ``` MY_VAR does not currently exist. Adding to the environment. MY_VAR is now set to: new_value ``` **Notes**: - If `MY_VAR` is already set in the environment, the function should output its current value before updating and the new value after updating. # Performance Requirements - The function should operate efficiently without any unnecessary computations. - Ensure that the function handles environment variables in a thread-safe manner; consider potential race conditions if accessed by multiple threads. The goal of this question is to test your ability to use the `os` module to interact with environment variables, ensuring you understand both the reading and writing aspects of environment management in Python.","solution":"import os def configure_environment(variable_name: str, new_value: str) -> None: Reads and prints the current value of an environment variable (if it exists), updates it with a new value, and then prints the new value. current_value = os.getenv(variable_name) if current_value is not None: print(f\\"{variable_name} currently has the value: {current_value}\\") else: print(f\\"{variable_name} does not currently exist. Adding to the environment.\\") os.environ[variable_name] = new_value print(f\\"{variable_name} is now set to: {new_value}\\")"},{"question":"# SAX Content Handler Implementation In this exercise, you are required to create a SAX Content Handler that processes an XML document and extracts specific information. The task will focus on: - Demonstrating your understanding of the `xml.sax.handler` package. - Implementing a handler to process XML document events. - Extracting detailed data from the provided XML content. Task: Implement a class `StudentContentHandler` by inheriting from `xml.sax.handler.ContentHandler`. Your handler should be capable of parsing the following sample XML format: ```xml <?xml version=\\"1.0\\"?> <school> <students> <student id=\\"1\\"> <name>John Doe</name> <age>20</age> <gender>Male</gender> </student> <student id=\\"2\\"> <name>Jane Smith</name> <age>22</age> <gender>Female</gender> </student> </students> </school> ``` Your handler should fulfill the following requirements: 1. **Attributes**: Implement the `__init__` method to initialize any needed attributes (such as data storage). 2. **Start Element**: Implement the `startElement` method to detect when a `<student>` element starts and collect its `id` attribute. 3. **End Element**: Implement the `endElement` method to detect when an element ends (elements of interest: `name`, `age`, `gender`) and store the data. 4. **Characters**: Implement the `characters` method to collect the character data within the elements. After parsing, the handler should store and print the list of student dictionaries, each containing `id`, `name`, `age`, and `gender`. Implementation ```python from xml.sax.handler import ContentHandler import xml.sax class StudentContentHandler(ContentHandler): def __init__(self): super().__init__() self.students = [] self.current_student = {} self.current_data = \'\' def startElement(self, name, attrs): if name == \'student\': self.current_student = {\'id\': attrs[\'id\']} self.current_data = \'\' def endElement(self, name): if name in [\'name\', \'age\', \'gender\']: self.current_student[name] = self.current_data.strip() elif name == \'student\': self.students.append(self.current_student) def characters(self, content): self.current_data += content # To use this handler, you would typically parse an XML file as follows: # parser = xml.sax.make_parser() # handler = StudentContentHandler() # parser.setContentHandler(handler) # parser.parse(\'path_to_your_xml_file\') # print(handler.students) ``` Constraints: - The XML structure may only include the structure given above. - You should ensure your handler manages nested elements correctly but you can assume that the XML structure follows the sample given. Performance Considerations: - Ensure your solution handles an XML document with up to 10,000 student entries efficiently. Try your hand at implementing this SAX handler and testing it with sample XML data. Good luck!","solution":"from xml.sax.handler import ContentHandler import xml.sax class StudentContentHandler(ContentHandler): def __init__(self): super().__init__() self.students = [] self.current_student = {} self.current_data = \'\' def startElement(self, name, attrs): if name == \'student\': self.current_student = {\'id\': attrs[\'id\']} self.current_data = \'\' def endElement(self, name): if name in [\'name\', \'age\', \'gender\']: self.current_student[name] = self.current_data.strip() elif name == \'student\': self.students.append(self.current_student) def characters(self, content): self.current_data += content def parse_xml(xml_string): handler = StudentContentHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler) return handler.students"},{"question":"Objective: Implement a custom sequence class by inheriting from the `collections.abc.Sequence` ABC. Your custom sequence should be immutable and must support operations like indexing, iteration, and contain checks. Additionally, it should provide specific functionalities such as reverse iteration and retrieving a sub-sequence. Requirements: 1. **Class Name**: `CustomSequence` 2. **Inheritance**: Inherit from `collections.abc.Sequence`. 3. **Initialization**: The sequence should be initialized with an iterable. 4. **Abstract Methods Implementation**: - `__getitem__(self, index)`: Return the item at the given index. - `__len__(self)`: Return the length of the sequence. 5. **Additional Methods**: - `__contains__(self, value)`: Check if a value is present in the sequence. - `__iter__(self)`: Return an iterator for the sequence. - `__reversed__(self)`: Return a reverse iterator for the sequence. - `sub_sequence(self, start, stop)`: Return a sub-sequence from start index to stop index (excluding the stop index). - `index(self, value)`: Return the index of the value in the sequence. Implementation Constraints: - The sequence is immutable; therefore, elements cannot be added, modified, or removed after the instance is created. - Ensure that accessing elements by index and performing iteration are efficient operations. Input and Output: - **Input**: The init method takes an iterable. - **Output**: Various methods return corresponding results based on sequence operations. Example Usage: ```python # Example Initialization seq = CustomSequence([1, 2, 3, 4, 5]) # Accessing elements print(seq[2]) # Output: 3 # Checking length print(len(seq)) # Output: 5 # Checking containment print(2 in seq) # Output: True # Iterating through elements for elem in seq: print(elem) # Output: 1 2 3 4 5 # Reversed iteration for elem in reversed(seq): print(elem) # Output: 5 4 3 2 1 # Sub-sequence sub_seq = seq.sub_sequence(1, 4) print(list(sub_seq)) # Output: [2, 3, 4] # Finding index print(seq.index(3)) # Output: 2 ``` Submission: Ensure your class passes the example usage tests. The function signatures should be as follows: - `__init__(self, iterable)` - `__getitem__(self, index)` - `__len__(self)` - `__contains__(self, value)` - `__iter__(self)` - `__reversed__(self)` - `sub_sequence(self, start, stop)` - `index(self, value)`","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, iterable): self._data = tuple(iterable) def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) def __contains__(self, value): return value in self._data def __iter__(self): return iter(self._data) def __reversed__(self): return reversed(self._data) def sub_sequence(self, start, stop): return CustomSequence(self._data[start:stop]) def index(self, value): return self._data.index(value)"},{"question":"# Coding Assessment: Advanced Turtle Graphics Objective Demonstrate your understanding of the turtle module by creating a program that draws a configurable geometric pattern based on user input. Problem Statement Write a Python function `draw_pattern(sides, length, turns, color)` using the turtle module to draw a pattern where: - `sides`: The number of sides for the base shape (e.g., 3 for triangle, 4 for square). - `length`: The length of each side of the base shape. - `turns`: The number of times the turtle should repeat the shape while turning a fixed angle. - `color`: The color of the lines in the pattern. The function should: 1. **Set up the turtle** with the given color, and make sure the pen is down for drawing. 2. **Draw the base shape** with the specified number of sides and length. 3. **Repeat the drawing** of the shape, each time turning the turtle by `360 / turns` degrees. 4. **Ensure the screen waits for the user to close it** to view the final drawing properly. Input: - `sides` (integer): The number of sides of the base shape (e.g., 3 for triangle, 4 for square). - `length` (integer): The length of each side of the base shape. - `turns` (integer): The number of repetitions with turns. - `color` (string): The color of the lines. Output: - A graphical window displaying the drawn pattern. Constraints: - `sides` should be greater than or equal to 3. - `length` should be a positive integer. - `turns` should be a positive integer. - `color` should be a valid Tkinter color string. Example Usage: ```python draw_pattern(5, 100, 36, \'blue\') ``` This will draw a pattern with a 5-sided base shape (pentagon), each side 100 units long, repeated 36 times with each repetition turning by ( frac{360}{36} = 10 ) degrees, and the lines colored blue. Function Signature: ```python def draw_pattern(sides: int, length: int, turns: int, color: str) -> None: pass ``` Use the documentation provided for the turtle module to implement this function effectively.","solution":"import turtle def draw_pattern(sides, length, turns, color): Draws a geometric pattern using the turtle module. :param sides: Number of sides for the base shape :param length: Length of each side of the base shape :param turns: Number of times the base shape should be drawn with turns :param color: Color of the lines for the pattern # Set up turtle t = turtle.Turtle() t.color(color) t.speed(0) # Fastest drawing speed # Function to draw one shape def draw_shape(): for _ in range(sides): t.forward(length) t.left(360 / sides) # Draw the pattern for _ in range(turns): draw_shape() t.left(360 / turns) # Finish turtle.done() # Example usage if __name__ == \\"__main__\\": draw_pattern(5, 100, 36, \'blue\')"},{"question":"# Pandas Global Configuration Assessment Objective You are tasked with demonstrating your knowledge of the `pandas` package\'s options and settings by manipulating the display settings and performing specific data formatting tasks. Problem Statement You need to write a function that: 1. Sets the maximum number of columns displayed in the output to 10. 2. Sets the number precision for floating-point numbers to 3 decimal places. 3. Sets a format for large floating-point numbers to be displayed in engineering notation. 4. Creates and returns a DataFrame using the given dictionary. # Input - A dictionary `data` of the form: ```python data = { \'A\': [Serializable], # list of integers \'B\': [Serializable], # list of floats \'C\': [Serializable], # list of floats with large values } ``` - A dictionary of types for the columns in the DataFrame, structured as: ```python col_types = { \'A\': \'int64\', \'B\': \'float64\', \'C\': \'float64\' } ``` # Output - A pandas DataFrame created from the provided `data` dictionary. - DataFrame must reflect the following configurations: - Maximum columns displayed: 10 - Float precision: 3 decimal places - Large floats in engineering notation # Constraints - Use the relevant `pandas` options functions to set the required configurations. # Example Input: ```python data = { \'A\': [1, 2, 3], \'B\': [0.123456, 0.789012, 0.345678], \'C\': [123456789.123456, 987654321.987654, 456789123.456789] } col_types = { \'A\': \'int64\', \'B\': \'float64\', \'C\': \'float64\' } ``` Output: ```python # settings applied as specified A B C 0 1 0.123 123.457e+06 1 2 0.789 987.654e+06 2 3 0.346 456.789e+06 ``` Instructions Implement the function `configure_and_create_df(data: dict, col_types: dict) -> pd.DataFrame` to achieve the tasks outlined above. Performance Requirements - Ensure that the function performs configurations using the pandas options API. - The DataFrame creation should reflect the configurations directly when returned.","solution":"import pandas as pd def configure_and_create_df(data: dict, col_types: dict) -> pd.DataFrame: Configures pandas display settings and creates a DataFrame. Args: data (dict): Dictionary containing the data. col_types (dict): Dictionary containing the column data types. Returns: pd.DataFrame: Configured DataFrame. # Set the maximum number of columns displayed to 10 pd.set_option(\'display.max_columns\', 10) # Set the number precision for floating-point numbers to 3 decimal places pd.set_option(\'display.precision\', 3) # Set the display format for large floats to engineering notation pd.set_option(\'display.float_format\', \'{:.3e}\'.format) # Create DataFrame with specified column types df = pd.DataFrame(data) df = df.astype(col_types) return df"},{"question":"You are asked to design a solution that leverages the `multiprocessing.shared_memory` module to calculate the sum of elements in a large list across multiple processes efficiently. Specifically, you should: 1. Create a large list of integer values. 2. Split the list among multiple processes, where each process calculates the sum of its segment and writes the result back to a shared memory block. 3. The main process should then read the results from shared memory and compute the total sum of the entire list. Your solution should demonstrate your understanding of shared memory management and process synchronization using the `multiprocessing` and `multiprocessing.shared_memory` modules. # Function Signature: ```python from multiprocessing import Process from multiprocessing.shared_memory import SharedMemory, ShareableList def parallel_sum_large_list(int_list: list, num_processes: int) -> int: # Your implementation here pass ``` # Input: - `int_list` (list of int): A large list of integers. - `num_processes` (int): The number of processes to use for parallel computation. # Output: - `int`: The total sum of the elements in `int_list`. # Constraints: - Each segment of the list should be processed by a separate process. - Each process should write its computed sum to a shared memory block. - The main process should read from the shared memory to compute the final sum. - Use of shared memory is required, and proper cleanup of shared memory resources must be ensured. # Example: ```python if __name__ == \\"__main__\\": # Example usage large_list = list(range(1, 10001)) # Sum should be 50005000 num_processes = 4 result = parallel_sum_large_list(large_list, num_processes) print(result) # Output should be 50005000 ``` Note: - Handle the creation and cleanup of shared memory properly. - Synchronize processes to ensure correct reading and writing to shared memory. # Hints: - Use `SharedMemory` and `ShareableList` classes to allocate shared memory for storing partial sums. - Consider using process synchronization primitives like `Barrier` or `Queue` if necessary.","solution":"from multiprocessing import Process, current_process from multiprocessing.shared_memory import SharedMemory, ShareableList import numpy as np def worker(int_list, start_index, end_index, shared_sum, index): Each worker computes the sum of its segment of the list. segment_sum = sum(int_list[start_index:end_index]) shared_sum[index] = segment_sum def parallel_sum_large_list(int_list: list, num_processes: int) -> int: # Size of each segment segment_size = len(int_list) // num_processes # Define the shared memory block to store results of each process shared_sum = ShareableList([0] * num_processes) processes = [] for i in range(num_processes): start_index = i * segment_size end_index = (i + 1) * segment_size if i != num_processes - 1 else len(int_list) process = Process(target=worker, args=(int_list, start_index, end_index, shared_sum, i)) processes.append(process) process.start() # Wait for all processes to finish for process in processes: process.join() # Sum up the results from the shared memory total_sum = sum(shared_sum) # Clean up shared memory shared_sum.shm.close() shared_sum.shm.unlink() return total_sum"},{"question":"**Turtle Graphics Challenge** **Objective:** Write a Python program using the `turtle` module that demonstrates a complex understanding of turtle graphics, including motion control, drawing, and event handling. **Task:** Design and implement a function `draw_spirograph_and_handle_clicks(radius, num_circles)` that: 1. Draws a multi-colored spirograph using the turtle graphics. 2. Handles mouse click events to change the turtle\'s drawing color. **Detailed Requirements:** **Function Signature:** ```python def draw_spirograph_and_handle_clicks(radius: int, num_circles: int) -> None: ``` **Inputs:** - `radius` (int): The radius of the circles that form the spirograph. - `num_circles` (int): The number of circles in the spirograph. **Output:** - The function does not return any value. The result should be visual and interactive using the turtle graphics window. **Constraints:** - The function must use at least three different turtle methods from separate categories (e.g., `Turtle motion`, `Pen control`, and `Using events`). - The function should handle mouse click events to cycle through at least three different colors for the turtle\'s pen. **Example Behavior:** Given `radius = 100` and `num_circles = 36`: 1. The turtle should draw a spirograph consisting of 36 circles, each with a radius of 100. 2. When the user clicks on the turtle graphic window, the turtle\'s pen color should change sequentially (e.g., click 1 changes to red, click 2 to green, click 3 to blue, and then back to red). **Hints:** - You can use the `circle()` method to draw a circle. - Use the `onclick()` method to bind a function to mouse click events. - The `pensize()` and `color()` methods will help control pen appearance. **Additional Requirements:** - Ensure that the turtle window does not close immediately after drawing, allowing time for user interaction. - Employ proper use of comments to explain the code logic.","solution":"import turtle def draw_spirograph_and_handle_clicks(radius: int, num_circles: int) -> None: Draws a multi-colored spirograph and changes the turtle\'s pen color on mouse clicks. Parameters: radius (int): The radius of the circles that form the spirograph. num_circles (int): The number of circles in the spirograph. screen = turtle.Screen() t = turtle.Turtle() colors = [\\"red\\", \\"green\\", \\"blue\\"] color_index = 0 def change_color(x, y): nonlocal color_index color_index = (color_index + 1) % len(colors) t.pencolor(colors[color_index]) screen.onclick(change_color) for i in range(num_circles): t.circle(radius) t.left(360 / num_circles) screen.mainloop() # Example usage # draw_spirograph_and_handle_clicks(100, 36)"},{"question":"# Programming Assessment: Implementing a Database Cache using `dbm` Modules **Objective:** Write a Python program that utilizes the `dbm` module to implement a simple cache system. Your task is to create a class called `DatabaseCache` that provides methods to store, retrieve, and remove items from the cache, as well as to clear the entire cache. Additionally, the cache should be automatically synchronized to disk after each modification. **Requirements:** 1. **Class Definition:** Define a class named `DatabaseCache`. 2. **Initialization:** - The class constructor should take a single argument `filename` (string) which specifies the name of the database file to be used. - Use the `dbm.open` method to open the database in `\'c\'` mode, which creates the database if it does not exist. 3. **Methods:** - **`store(self, key, value)`**: - Stores `value` under the `key`. - Both `key` and `value` should be strings and will be stored as bytes. - **`retrieve(self, key)`**: - Retrieves the value associated with the `key`. - Return the value as a string. - If the `key` does not exist, return `None`. - **`remove(self, key)`**: - Removes the entry with the specified `key`. - If the `key` does not exist, do nothing. - **`clear(self)`**: - Clears all entries in the database. - **`sync(self)`**: - Ensures all changes are written to disk. 4. **Context Management:** - Implement the `__enter__` and `__exit__` methods to allow the use of `with` statements, ensuring the database is properly closed when done. 5. **Usage Example:** ```python with DatabaseCache(\'mycache\') as cache: cache.store(\'user1\', \'Alice\') cache.store(\'user2\', \'Bob\') print(cache.retrieve(\'user1\')) # Outputs: Alice cache.remove(\'user1\') print(cache.retrieve(\'user1\')) # Outputs: None cache.clear() ``` **Constraints:** - Ensure that `key` and `value` are strings before converting them to bytes for storage. Raise a `TypeError` if they are not strings. - Performance is not a primary concern, but your solution should handle basic use cases efficiently. **Submission:** Submit your Python file named `database_cache.py` that contains the complete implementation of the `DatabaseCache` class. Include comments to explain the code and any assumptions made. Good luck!","solution":"import dbm class DatabaseCache: def __init__(self, filename): self.filename = filename self.db = dbm.open(filename, \'c\') def store(self, key, value): if not isinstance(key, str) or not isinstance(value, str): raise TypeError(\\"Both key and value must be strings.\\") self.db[key.encode()] = value.encode() self.sync() def retrieve(self, key): if not isinstance(key, str): raise TypeError(\\"Key must be a string.\\") key_bytes = key.encode() if key_bytes not in self.db: return None value = self.db[key_bytes] return value.decode() def remove(self, key): if not isinstance(key, str): raise TypeError(\\"Key must be a string.\\") key_bytes = key.encode() if key_bytes in self.db: del self.db[key_bytes] self.sync() def clear(self): keys = list(self.db.keys()) for key in keys: del self.db[key] self.sync() def sync(self): self.db.sync() def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self.db.close()"},{"question":"# XML Data Extraction and Manipulation Objective: Implement a function that parses an XML document, extracts specific elements based on given criteria, modifies the XML structure, and returns the modified XML as a string. Function Signature: ```python def process_xml(xml_string: str, tag: str, new_text: str) -> str: pass ``` Input: 1. `xml_string` (str): A string representing the XML document. 2. `tag` (str): The name of the XML tag to search for. 3. `new_text` (str): The text with which to replace the text of the found tag elements. Output: - Returns a string representing the modified XML document. Requirements: 1. Parse the input XML string. 2. Search for all elements with the specified tag name. 3. Replace the text in those elements with the provided new text. 4. Return the modified XML as a string. Example: ```python xml_string = \'\'\' <root> <item>Item 1</item> <item>Item 2</item> <item>Item 3</item> <other>Something else</other> </root> \'\'\' # Calling the function with: print(process_xml(xml_string, \'item\', \'Modified Item\')) # Should output: \'\'\' <root> <item>Modified Item</item> <item>Modified Item</item> <item>Modified Item</item> <other>Something else</other> </root> \'\'\' ``` Constraints: 1. You can assume that the provided XML string is well-formed. 2. Performance should be optimized for large XML documents. Hints: - Use the `xml.etree.ElementTree` module for parsing and modifying the XML document. - Look into the `findall` method for retrieving elements by tag name. - Remember to handle character escaping when converting XML to a string.","solution":"import xml.etree.ElementTree as ET def process_xml(xml_string: str, tag: str, new_text: str) -> str: Processes the given XML string by replacing the text of specified elements. Args: xml_string (str): A string representing the XML document. tag (str): The name of the XML tag to search for. new_text (str): The text with which to replace the text of the found tag elements. Returns: str: The modified XML document as a string. # Parse the XML string root = ET.fromstring(xml_string) # Find all elements with the specified tag name elements = root.findall(\'.//\' + tag) # Replace the text in those elements with the provided new text for elem in elements: elem.text = new_text # Convert the modified XML tree back to a string return ET.tostring(root, encoding=\'unicode\')"},{"question":"Coding Assessment Question **Objective**: Implement a class named `FileHandler` that handles both text and binary data using the Python `io` module. The class should provide functionalities to read from and write to files in both text and binary modes. # Class: FileHandler Methods 1. **`__init__(self, filepath: str, mode: str, encoding: str = \\"utf-8\\")`** - Initializes the `FileHandler` with the specified file path, mode, and encoding. Mode can be `\'r\'`, `\'w\'`, `\'rb\'`, or `\'wb\'`. If mode is text (i.e., not containing `\'b\'`), the encoding will be used. 2. **`read(self, size: int = -1) -> str`** - Reads and returns the content from the file. The `size` parameter specifies the number of characters or bytes to read. If `size` is `-1`, it reads until EOF. 3. **`write(self, data: str) -> int`** - Writes the provided `data` to the file. Returns the number of characters or bytes written. 4. **`close(self)`** - Closes the file. # Example Usage ```python # Handling text files text_handler = FileHandler(\'example.txt\', \'w\') text_handler.write(\'Hello, World!\') text_handler.close() text_handler = FileHandler(\'example.txt\', \'r\') content = text_handler.read() print(content) # Output: Hello, World! text_handler.close() # Handling binary files binary_handler = FileHandler(\'example.bin\', \'wb\') binary_handler.write(b\'x00x01\') binary_handler.close() binary_handler = FileHandler(\'example.bin\', \'rb\') content = binary_handler.read() print(content) # Output: b\'x00x01\' binary_handler.close() ``` # Constraints - Only use built-in Python libraries. - Ensure proper handling of file opening and closing to avoid resource leaks. - Handle potential exceptions that may occur while reading or writing files. # Notes - Provide clear and meaningful docstrings for the class methods. - Ensure that the mode specified is valid, raising appropriate exceptions for invalid modes. Write the `FileHandler` class based on the above requirements.","solution":"import io class FileHandler: def __init__(self, filepath: str, mode: str, encoding: str = \\"utf-8\\"): Initializes the FileHandler with the specified file path, mode, and encoding. Mode can be \'r\', \'w\', \'rb\', or \'wb\'. If mode is text (i.e., not containing \'b\'), the encoding will be used. self.filepath = filepath self.mode = mode self.encoding = encoding self.file = None if \'b\' in mode: self.file = open(filepath, mode) else: self.file = open(filepath, mode, encoding=encoding) def read(self, size: int = -1): Reads and returns the content from the file. The size parameter specifies the number of characters or bytes to read. If size is -1, it reads until EOF. return self.file.read(size) def write(self, data): Writes the provided data to the file. Returns the number of characters or bytes written. if \'b\' in self.mode: bytes_written = self.file.write(data) else: bytes_written = self.file.write(data) self.file.flush() return bytes_written def close(self): Closes the file. if self.file: self.file.close() self.file = None"},{"question":"# Coding Assessment: PyTorch JIT Compilation Objective You are tasked with demonstrating your understanding of PyTorch\'s Just-In-Time (JIT) compilation by converting a simple neural network using the JIT utilities. This will involve creating a small neural network, converting it to a JIT model, and verifying its performance and correctness. Problem Statement 1. **Define a Simple Neural Network**: Create a simple neural network using `torch.nn.Module`. The network should have: - An input layer of size 4. - One hidden layer of size 5. - An output layer of size 3. 2. **JIT Compilation**: - Convert this neural network into a TorchScript format using `torch.jit.script`. - Implement a method to trace the network using `torch.jit.trace`. 3. **Comparison and Verification**: - Ensure that the outputs of the original and JIT compiled models are the same for a given input. - Use a mean squared error to compute the difference between the outputs. Input and Output Format 1. The neural network should be instantiated based on the structure defined. 2. The function `convert_to_jit_model` should accept the neural network and return the scripted and traced JIT models. 3. A function `verify_model` should take the original model, the scripted model, the traced model, and an input tensor, and return the mean squared error between the outputs of these models. Constraints and Performance Requirements - Ensure the implementation is efficient and adheres to PyTorch\'s best practices. - You must use tensors with appropriate dimensions for your input (batch size of 2 and input size of 4). - Validate that the scripted and traced models are successfully created without errors. Function Signatures ```python import torch import torch.nn as nn import torch.nn.functional as F class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() # Define network layers here def forward(self, x): # Define forward pass here pass def convert_to_jit_model(model): Convert the given model to a JIT model using torch.jit.script and torch.jit.trace. Args: - model (torch.nn.Module): The neural network model to be converted. Returns: - scripted_model (torch.jit.ScriptModule): The model converted using torch.jit.script. - traced_model (torch.jit.ScriptModule): The model converted using torch.jit.trace. pass def verify_model(model, scripted_model, traced_model, input_tensor): Verify that the outputs of the original, scripted, and traced models are the same. Args: - model (torch.nn.Module): The original neural network model. - scripted_model (torch.jit.ScriptModule): The scripted JIT model. - traced_model (torch.jit.ScriptModule): The traced JIT model. - input_tensor (torch.Tensor): The input tensor to be used for verification. Returns: - mse_original_vs_scripted (float): Mean squared error between the original and scripted model outputs. - mse_original_vs_traced (float): Mean squared error between the original and traced model outputs. pass ``` Example Usage ```python # Create the neural network net = SimpleNet() # Convert to JIT models scripted_net, traced_net = convert_to_jit_model(net) # Create an input tensor input_tensor = torch.randn(2, 4) # Verify the models mse_scripted, mse_traced = verify_model(net, scripted_net, traced_net, input_tensor) # Output the mean squared errors print(\\"Mean Squared Error (Original vs Scripted):\\", mse_scripted) print(\\"Mean Squared Error (Original vs Traced):\\", mse_traced) ``` Your implementation should enable the JIT models to perform similarly to the original model and verify the correctness accurately. Make any necessary assumptions clear in your code comments.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(4, 5) self.fc2 = nn.Linear(5, 3) def forward(self, x): x = F.relu(self.fc1(x)) x = self.fc2(x) return x def convert_to_jit_model(model): Convert the given model to a JIT model using torch.jit.script and torch.jit.trace. Args: - model (torch.nn.Module): The neural network model to be converted. Returns: - scripted_model (torch.jit.ScriptModule): The model converted using torch.jit.script. - traced_model (torch.jit.ScriptModule): The model converted using torch.jit.trace. scripted_model = torch.jit.script(model) example_input = torch.randn(2, 4) traced_model = torch.jit.trace(model, example_input) return scripted_model, traced_model def verify_model(model, scripted_model, traced_model, input_tensor): Verify that the outputs of the original, scripted, and traced models are the same. Args: - model (torch.nn.Module): The original neural network model. - scripted_model (torch.jit.ScriptModule): The scripted JIT model. - traced_model (torch.jit.ScriptModule): The traced JIT model. - input_tensor (torch.Tensor): The input tensor to be used for verification. Returns: - mse_original_vs_scripted (float): Mean squared error between the original and scripted model outputs. - mse_original_vs_traced (float): Mean squared error between the original and traced model outputs. original_output = model(input_tensor) scripted_output = scripted_model(input_tensor) traced_output = traced_model(input_tensor) mse_original_vs_scripted = F.mse_loss(original_output, scripted_output).item() mse_original_vs_traced = F.mse_loss(original_output, traced_output).item() return mse_original_vs_scripted, mse_original_vs_traced"},{"question":"# Time Zone Conversion and Handling with `zoneinfo` Objective Write a function that performs various time zone-related tasks using the `zoneinfo` module. This function should demonstrate your understanding of time zone conversion, handling daylight saving time transitions, and cache management in the `zoneinfo` module. Task Implement a function `timezone_operations` that takes the following parameters: - `dt_str` (str): A string representing a date and time in the format \\"YYYY-MM-DD HH:MM:SS\\". - `source_tz` (str): The time zone key of the source time zone (e.g., \\"UTC\\" or \\"America/Los_Angeles\\"). - `target_tz` (str): The time zone key of the target time zone (e.g., \\"Asia/Tokyo\\" or \\"Europe/London\\"). The function should perform the following operations: 1. **Parse the input date and time string into a `datetime` object** and set its time zone to `source_tz`. 2. **Convert this `datetime` object to `target_tz`**. 3. **Handle any ambiguous times** due to daylight saving time transitions by setting `fold` to 1 where appropriate. 4. **Return the converted `datetime` object as a string** in the format \\"YYYY-MM-DD HH:MM:SS TZ\\". 5. **Clear the `ZoneInfo` cache** after performing the conversion. You may assume that the `source_tz` and `target_tz` provided are valid IANA time zone keys. Constraints - The function should raise a `zoneinfo.ZoneInfoNotFoundError` if any of the provided time zone keys are not found. - You should use the `ZoneInfo` class and methods as described in the documentation. Example ```python from datetime import datetime from zoneinfo import ZoneInfo, ZoneInfoNotFoundError def timezone_operations(dt_str, source_tz, target_tz): # Your implementation here pass # Example usage dt_str = \\"2020-11-01 12:00:00\\" source_tz = \\"America/Los_Angeles\\" target_tz = \\"UTC\\" try: result = timezone_operations(dt_str, source_tz, target_tz) print(result) # Expected output: \\"2020-11-01 20:00:00 UTC\\" except ZoneInfoNotFoundError as e: print(e) except Exception as e: print(e) ``` Ensure your function handles daylight saving time transitions correctly and returns the expected output for various test cases. Notes - You have to ensure that you import the necessary modules such as `datetime` and `zoneinfo`. - Use the `ZoneInfo` class for handling time zones. - Be mindful of potential exceptions such as `ZoneInfoNotFoundError`. Happy coding!","solution":"from datetime import datetime from zoneinfo import ZoneInfo, ZoneInfoNotFoundError def timezone_operations(dt_str, source_tz, target_tz): try: # Parse the input date and time string into a datetime object dt = datetime.strptime(dt_str, \\"%Y-%m-%d %H:%M:%S\\") # Set its time zone to source_tz dt_source = dt.replace(tzinfo=ZoneInfo(source_tz)) # Convert this datetime object to target_tz dt_target = dt_source.astimezone(ZoneInfo(target_tz)) # Format the converted datetime object as a string result = dt_target.strftime(\\"%Y-%m-%d %H:%M:%S %Z\\") # Clear the ZoneInfo cache ZoneInfo.clear_cache() # Return the formatted string return result except ZoneInfoNotFoundError as e: raise e except Exception as e: raise e"},{"question":"# Probability Distribution Handling Using PyTorch Objective: You are to implement a function that leverages PyTorch\'s `torch.distributions` module to generate samples from specified probability distributions and to compute the log probability of specific observations. Function Signature: ```python import torch from torch.distributions import Normal, Bernoulli, Categorical, Poisson def process_distributions(distribution_specs: dict, n_samples: int) -> dict: Given a specification for various probability distributions, this function generates samples and computes log probabilities. Parameters: distribution_specs (dict): A dictionary where keys are distribution names (\\"Normal\\", \\"Bernoulli\\", \\"Categorical\\", \\"Poisson\\") and values are dictionaries containing parameters for each distribution. The expected parameters are: - For \\"Normal\\": {\\"mean\\": float, \\"stddev\\": float} - For \\"Bernoulli\\": {\\"probs\\": float} - For \\"Categorical\\": {\\"probs\\": list of float} - For \\"Poisson\\": {\\"rate\\": float} n_samples (int): The number of samples to generate for each distribution. Returns: dict: A dictionary with two keys: - \\"samples\\": Contains the generated samples for each distribution. - \\"log_probs\\": Contains the log probabilities of the generated samples for each distribution. pass ``` Description: 1. **Inputs:** - `distribution_specs`: A dictionary where the keys are distribution names (one or more of \\"Normal\\", \\"Bernoulli\\", \\"Categorical\\", \\"Poisson\\") and the values are dictionaries with the parameters required for each type of distribution. - For \\"Normal\\": The dictionary should contain keys \\"mean\\" (float) and \\"stddev\\" (float). - For \\"Bernoulli\\": The dictionary should contain a key \\"probs\\" (float). - For \\"Categorical\\": The dictionary should contain a key \\"probs\\" (list of floats). - For \\"Poisson\\": The dictionary should contain a key \\"rate\\" (float). - `n_samples`: An integer indicating the number of samples to generate from each specified distribution. 2. **Outputs:** - A dictionary with two keys: - `\\"samples\\"`: A dictionary where each key corresponds to a distribution name from the input, containing the generated samples for that distribution. - `\\"log_probs\\"`: A dictionary where each key corresponds to a distribution name from the input, containing the log probabilities of the generated samples for that distribution. 3. **Constraints:** - Ensure that the input parameters are valid for the respective distributions. - Utilize the PyTorch `torch.distributions` module for the entire implementation. 4. **Performance Requirements:** - The implementation should efficiently handle the generation of samples and computation of log probabilities. Example Usage: ```python # Sample input specifications distribution_specs = { \\"Normal\\": {\\"mean\\": 0, \\"stddev\\": 1}, \\"Bernoulli\\": {\\"probs\\": 0.5}, \\"Categorical\\": {\\"probs\\": [0.1, 0.3, 0.6]}, \\"Poisson\\": {\\"rate\\": 3.0} } n_samples = 1000 # Expected output (sample format) result = process_distributions(distribution_specs, n_samples) assert \\"samples\\" in result assert \\"log_probs\\" in result # Check samples for each distribution print(result[\\"samples\\"][\\"Normal\\"]) # Tensor of 1000 samples from Normal(0, 1) print(result[\\"log_probs\\"][\\"Normal\\"]) # Tensor of 1000 log probabilities for these samples print(result[\\"samples\\"][\\"Bernoulli\\"]) # Tensor of 1000 samples from Bernoulli(0.5) print(result[\\"log_probs\\"][\\"Bernoulli\\"]) # Tensor of 1000 log probabilities for these samples print(result[\\"samples\\"][\\"Categorical\\"]) # Tensor of 1000 samples from Categorical([0.1, 0.3, 0.6]) print(result[\\"log_probs\\"][\\"Categorical\\"]) # Tensor of 1000 log probabilities for these samples print(result[\\"samples\\"][\\"Poisson\\"]) # Tensor of 1000 samples from Poisson(3.0) print(result[\\"log_probs\\"][\\"Poisson\\"]) # Tensor of 1000 log probabilities for these samples ``` Notes: - This problem assesses your ability to effectively use PyTorch\'s `torch.distributions` module to model and sample from various probability distributions, as well as compute log probabilities of these samples.","solution":"import torch from torch.distributions import Normal, Bernoulli, Categorical, Poisson def process_distributions(distribution_specs: dict, n_samples: int) -> dict: Given a specification for various probability distributions, this function generates samples and computes log probabilities. Parameters: distribution_specs (dict): A dictionary where keys are distribution names (\\"Normal\\", \\"Bernoulli\\", \\"Categorical\\", \\"Poisson\\") and values are dictionaries containing parameters for each distribution. The expected parameters are: - For \\"Normal\\": {\\"mean\\": float, \\"stddev\\": float} - For \\"Bernoulli\\": {\\"probs\\": float} - For \\"Categorical\\": {\\"probs\\": list of float} - For \\"Poisson\\": {\\"rate\\": float} n_samples (int): The number of samples to generate for each distribution. Returns: dict: A dictionary with two keys: - \\"samples\\": Contains the generated samples for each distribution. - \\"log_probs\\": Contains the log probabilities of the generated samples for each distribution. result = {\\"samples\\": {}, \\"log_probs\\": {}} for dist_name, params in distribution_specs.items(): if dist_name == \\"Normal\\": mean = params[\\"mean\\"] stddev = params[\\"stddev\\"] dist = Normal(mean, stddev) elif dist_name == \\"Bernoulli\\": probs = params[\\"probs\\"] dist = Bernoulli(probs) elif dist_name == \\"Categorical\\": probs = torch.tensor(params[\\"probs\\"]) dist = Categorical(probs) elif dist_name == \\"Poisson\\": rate = params[\\"rate\\"] dist = Poisson(rate) else: raise ValueError(f\\"Distribution {dist_name} is not recognized.\\") samples = dist.sample((n_samples,)) log_probs = dist.log_prob(samples) result[\\"samples\\"][dist_name] = samples result[\\"log_probs\\"][dist_name] = log_probs return result"},{"question":"Coding Assessment Question # Objective Design and implement a program that uses the asyncio event loop to manage multiple concurrent tasks, demonstrating your understanding of event looping, callbacks, tasks, and network connections. # Problem Statement You are required to create an asynchronous server that handles multiple client connections concurrently. Your server should perform the following operations: 1. Start and manage a TCP server that listens on a specified port. 2. Accept incoming client connections and handle multiple clients concurrently using asyncio features. 3. For each connected client: - Read a greeting message from the client. - Add a delay to simulate processing time. - Send a response back to the client with the current server date and time. - Close the client connection gracefully. # Specifications - **Server**: - Should be implemented using the low-level asyncio event loop API rather than high-level `asyncio.run()`. - Should handle at least five concurrent clients. - Should use `loop.create_server()` to create the server. - **Client Handling**: - Use `asyncio.create_task()` to run the client handling coroutine for each client. - Implement a graceful shutdown for connected clients when the server stops. # Input and Output - **Input**: - Port number on which the server should listen (e.g., 8080). - **Output**: - Server logs for each client connection, message received, and response sent. - Error logs in case of any exceptions during execution or client handling. # Constraints - Do not use high-level functions like `asyncio.run()`. - Implement graceful shutdown to cleanse all resources. - You can use Python\'s standard library `datetime` for the date and time. # Example ```python import asyncio from datetime import datetime async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") await asyncio.sleep(2) # Simulate processing time delay response = f\\"Current Server Time: {datetime.now()}\\" writer.write(response.encode()) await writer.drain() print(f\\"Sent: {response} to {addr}\\") print(\\"Closing the connection\\") writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8080) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \'__main__\': # Replace with asyncio event loop methods asyncio.run(main()) ``` Note: This example uses `asyncio.run()` and `asyncio.start_server()`, but your task is to replace these with lower-level asyncio event loop methods as per the requirement. # Submission Provide Python code that fulfills the requirements. Ensure proper error handling and readable log outputs to track the activities of the server and clients.","solution":"import asyncio from datetime import datetime async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") await asyncio.sleep(2) # Simulate processing time delay response = f\\"Current Server Time: {datetime.now()}\\" writer.write(response.encode()) await writer.drain() print(f\\"Sent: {response} to {addr}\\") print(\\"Closing the connection\\") writer.close() await writer.wait_closed() async def run_server(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: asyncio.StreamReaderProtocol(asyncio.StreamReader()), \'127.0.0.1\', 8080 ) async with server: await server.serve_forever() if __name__ == \'__main__\': loop = asyncio.get_event_loop() loop.run_until_complete(run_server())"},{"question":"**Objective**: Demonstrate your understanding and ability to use the `enum` module in Python. Problem Description An online store sells products that belong to various categories, and each category has specific properties. We need to create a system to manage the enumeration of categories and their properties. 1. **Category**: A basic enumeration for product categories (e.g., `ELECTRONICS`, `FASHION`, `HOME_APPLIANCE`). 2. **CategoryDetails**: An enumeration that includes additional details for each category, such as code, discounted status, and rating. The name, code, and rating should be customizable. **Step-by-step implementation**: **Step 1**: Define an enumeration `Category` using `Enum` that includes the following product categories: - `ELECTRONICS` - `FASHION` - `HOME_APPLIANCE` **Step 2**: Define an enumeration `CategoryDetails` using `Enum` that includes additional details for each category. Each member should have: - A `code` (string representation). - A `discounted` status (boolean). - A `rating` (float). The enumeration should allow easy retrieval of these details. **Step 3**: Implement a class `Product` with the following: - An initialization method that takes a `name`, a category of type `Category`, and a `price`. - A method `apply_discount` that applies a specific percentage discount to the product based on its category. Use `CategoryDetails` to determine if the category is discounted (15% discount if `discounted` is `True`). **Input**: - The list of categories and details is predefined (you can hardcode these in your code). - A string representing the name of the product. - A string representing the category of the product. - A float representing the price of the product. **Output**: - The constructed product instance. - After applying the discount, the final price of the product. **Constraints**: - Category names are case-insensitive. - Ensure no category\'s code is duplicated within `CategoryDetails`. **Performance Requirements**: - Ensure performance is optimal, and unnecessary computations are avoided. --- **Example Usage**: ```python from enum import Enum, auto # Define the Category enumeration class Category(Enum): ELECTRONICS = auto() FASHION = auto() HOME_APPLIANCE = auto() # Define the CategoryDetails enumeration class CategoryDetails(Enum): ELECTRONICS_DETAILS = (\\"CODE_ELEC\\", True, 4.5) FASHION_DETAILS = (\\"CODE_FASH\\", False, 3.8) HOME_APPLIANCE_DETAILS = (\\"CODE_HOME\\", True, 4.2) def __init__(self, code, discounted, rating): self.code = code self.discounted = discounted self.rating = rating class Product: def __init__(self, name, category, price): if not isinstance(category, Category): raise ValueError(\\"Invalid category\\") self.name = name self.category = category self.price = price def apply_discount(self): detail_key = self.category.name + \'_DETAILS\' details = CategoryDetails[detail_key] if details.discounted: self.price *= 0.85 # Example code to test your implementation product = Product(\'Laptop\', Category.ELECTRONICS, 1000) print(product.price) # 1000 product.apply_discount() print(product.price) # 850 ```","solution":"from enum import Enum, auto # Define the Category enumeration class Category(Enum): ELECTRONICS = auto() FASHION = auto() HOME_APPLIANCE = auto() # Define the CategoryDetails enumeration class CategoryDetails(Enum): ELECTRONICS_DETAILS = (\\"CODE_ELEC\\", True, 4.5) FASHION_DETAILS = (\\"CODE_FASH\\", False, 3.8) HOME_APPLIANCE_DETAILS = (\\"CODE_HOME\\", True, 4.2) def __init__(self, code, discounted, rating): self.code = code self.discounted = discounted self.rating = rating class Product: def __init__(self, name, category, price): if not isinstance(category, Category): raise ValueError(\\"Invalid category\\") self.name = name self.category = category self.price = price def apply_discount(self): detail_key = self.category.name + \'_DETAILS\' details = CategoryDetails[detail_key] if details.discounted: self.price *= 0.85"},{"question":"**Objective:** Implement a simple chat server using non-blocking sockets. # Problem Statement You are required to implement a multi-client chat server that allows multiple clients to connect and send messages to each other. The server should use non-blocking sockets and manage multiple client sockets using the `select` module. Each client can send messages to the server, and the server should broadcast these messages to all connected clients. # Requirements 1. **Server Side Implementation:** - Create a server socket that listens for incoming connections on a specified port. - Use non-blocking sockets to handle multiple clients simultaneously. - Use the `select` module to manage multiple client sockets. - Broadcast received messages from any client to all connected clients. 2. **Client Side Implementation:** - Create a client socket that connects to the server. - Allow the client to send messages to the server. - Allow the client to receive broadcasted messages from the server. # Details - **Input/Output:** - **Server-Side:** - No direct input; it should handle incoming messages from connected clients. - Broadcast incoming messages to all connected clients. - **Client-Side:** - Should read input from the user and send it to the server. - Should display messages broadcasted by the server to the user. - **Constraints:** - You will be given a specific port number (e.g., `12345`) on which the server should run. - The server and the clients should handle graceful disconnections. - Handle any potential exceptions or errors properly to avoid crashes. - The system should be designed to handle at least 5 simultaneous clients. - **Performance Requirements:** - Efficiently manage multiple clients using non-blocking sockets and the `select` module. - Minimal latency in message broadcasting. # Python Code Template ```python import socket import select import sys # Constants SERVER_HOST = \'localhost\' SERVER_PORT = 12345 BUFFER_SIZE = 1024 # Server-side implementation def chat_server(): # TBC: Implement server logic for handling multiple clients using select # Client-side implementation def chat_client(): # TBC: Implement client logic for connecting to server and sending/receiving messages if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python chat.py <server|client>\\") sys.exit(1) mode = sys.argv[1].lower() if mode == \'server\': chat_server() elif mode == \'client\': chat_client() else: print(\\"Invalid mode. Please choose \'server\' or \'client\'.\\") ``` **Instructions:** 1. Implement the `chat_server` function to handle multiple clients using non-blocking sockets with `select`. 2. Implement the `chat_client` function to communicate with the server and other clients. 3. Ensure the server can broadcast messages to all clients and manage graceful disconnections. 4. Utilize exception handling to manage potential runtime issues gracefully. Submit your solution by implementing the missing parts of the provided template code.","solution":"import socket import select import sys # Constants SERVER_HOST = \'localhost\' SERVER_PORT = 12345 BUFFER_SIZE = 1024 def chat_server(): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.setblocking(False) server_socket.bind((SERVER_HOST, SERVER_PORT)) server_socket.listen(5) sockets_list = [server_socket] clients = {} print(f\\"Server is listening on {SERVER_HOST}:{SERVER_PORT}\\") while True: read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list) for notified_socket in read_sockets: if notified_socket == server_socket: client_socket, client_address = server_socket.accept() client_socket.setblocking(False) sockets_list.append(client_socket) clients[client_socket] = client_address print(f\\"Accepted new connection from {client_address}\\") else: try: message = notified_socket.recv(BUFFER_SIZE) if not message: print(f\\"Closed connection from {clients[notified_socket]}\\") sockets_list.remove(notified_socket) del clients[notified_socket] notified_socket.close() else: print(f\\"Received message from {clients[notified_socket]}: {message.decode(\'utf-8\')}\\") for client_socket in clients: if client_socket != notified_socket: try: client_socket.send(message) except Exception as e: print(f\\"Error: {str(e)}\\") client_socket.close() sockets_list.remove(client_socket) del clients[client_socket] except Exception as e: print(f\\"Error: {str(e)}\\") notified_socket.close() sockets_list.remove(notified_socket) del clients[notified_socket] for notified_socket in exception_sockets: sockets_list.remove(notified_socket) del clients[notified_socket] notified_socket.close() def chat_client(): client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect((SERVER_HOST, SERVER_PORT)) client_socket.setblocking(False) print(f\\"Connected to chat server at {SERVER_HOST}:{SERVER_PORT}\\") while True: socket_list = [sys.stdin, client_socket] read_sockets, _, _ = select.select(socket_list, [], socket_list) for notified_socket in read_sockets: if notified_socket == client_socket: message = client_socket.recv(BUFFER_SIZE) if not message: print(\\"Disconnected from chat server\\") sys.exit() else: print(f\\"Server: {message.decode(\'utf-8\')}\\") else: message = sys.stdin.readline() client_socket.send(message.encode(\'utf-8\')) if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python chat.py <server|client>\\") sys.exit(1) mode = sys.argv[1].lower() if mode == \'server\': chat_server() elif mode == \'client\': chat_client() else: print(\\"Invalid mode. Please choose \'server\' or \'client\'.\\")"},{"question":"# Python Coding Assessment: Asyncio Futures and Event Loops **Objective:** Implement a function that uses asyncio `Future` objects to demonstrate their understanding of asynchronous programming and Futures in Python. **Problem Statement:** You are required to write a function `fetch_and_process_data` that fetches data asynchronously after a given delay and processes it once fetched. The function should utilize `asyncio.Future` and event loops to handle this operation. # Requirements: 1. **Function Name:** `fetch_and_process_data` 2. **Parameters:** - `delay`: an integer representing the number of seconds after which the data fetch operation completes. - `data`: a string representing the data to be fetched and processed. 3. **Return:** - A string stating `\\"Processed: <data>\\"` after awaiting the delay period. 4. **Constraints:** - The function must use `asyncio.Future` objects to handle the asynchronous operation. - The function should not use any sleep methods directly but leverage Future\'s awaitability instead. 5. **Performance:** - The function should handle delays in an efficient manner without blocking the event loop. # Function Signature: ```python import asyncio async def fetch_and_process_data(delay: int, data: str) -> str: # Your implementation here ``` # Example: ```python import asyncio async def main(): result = await fetch_and_process_data(2, \\"test data\\") print(result) # Output should be: \\"Processed: test data\\" asyncio.run(main()) ``` # Implementation Details: 1. Create a Future object. 2. Set up a function (or coroutine) that sets the result of the Future after the specified delay period. 3. Use the event loop to run this coroutine in parallel without blocking the main function. 4. Await the Future result in your main function and return the processed string. # Notes: - This task will help you understand the usage of Futures, managing asynchronous delays, and the principles of non-blocking code execution using asyncio in Python.","solution":"import asyncio async def fetch_and_process_data(delay: int, data: str) -> str: loop = asyncio.get_event_loop() future = loop.create_future() def set_future_result(): future.set_result(f\\"Processed: {data}\\") loop.call_later(delay, set_future_result) result = await future return result"},{"question":"# Question **Objective**: Demonstrate proficiency in pandas with Copy-on-Write behavior. You are given a DataFrame `df` representing some sample data: ```python import pandas as pd df = pd.DataFrame({ \\"A\\": [10, 20, 30, 40, 50], \\"B\\": [5, 10, 15, 20, 25], \\"C\\": [\\"foo\\", \\"bar\\", \\"baz\\", \\"qux\\", \\"quux\\"] }) ``` Perform the following tasks ensuring that no unintended side effects occur due to data sharing, preserving the Copy-on-Write paradigm: 1. Write a function to reset the index of the DataFrame, drop the existing index, and return the modified DataFrame without triggering a copy unnecessarily. The function should be named `reset_index_cow`. ```python def reset_index_cow(df: pd.DataFrame) -> pd.DataFrame: Resets the index of the DataFrame, dropping the existing index. Args: df (pd.DataFrame): Input DataFrame whose index needs to be reset. Returns: pd.DataFrame: DataFrame with reset index and no unintentional data copies. pass ``` 2. Write a function to update all values in column `B` of the DataFrame where column `A` is greater than 25 to the value 1000 using CoW principles. The function should be named `update_column_b`. ```python def update_column_b(df: pd.DataFrame) -> pd.DataFrame: Updates all values in column B where column A is greater than 25 to 1000. Args: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: DataFrame with updated column B values. pass ``` 3. Write a function to ensure the array view of a DataFrame is writeable. Return the view where you modify the first element to be 999. The function should be named `make_array_writeable`. ```python def make_array_writeable(df: pd.DataFrame) -> pd.DataFrame: Ensures the array view of the DataFrame is writeable and modifies the first element to 999. Args: df (pd.DataFrame): Input DataFrame. Returns: np.ndarray: Numpy array view with the first element modified to 999. pass ``` # Constraints - Do not use methods or operations that cause unintended data sharing or break the CoW principles. - If modifications are intended only in a subset or a view, ensure they do not affect the original DataFrame unless explicitly required. - Solutions must handle the CoW rules efficiently to prevent unnecessary copies. # Example Usage ```python df_new = reset_index_cow(df) df_updated = update_column_b(df) np_array_modified = make_array_writeable(df) ``` # Expected Output - `reset_index_cow(df)` should return a DataFrame with its index reset. - `update_column_b(df)` should return a DataFrame where the column `B` is updated properly. - `make_array_writeable(df)` should return a Numpy array where you can modify its elements directly without affecting the original DataFrame.","solution":"import pandas as pd import numpy as np def reset_index_cow(df: pd.DataFrame) -> pd.DataFrame: Resets the index of the DataFrame, dropping the existing index. Args: df (pd.DataFrame): Input DataFrame whose index needs to be reset. Returns: pd.DataFrame: DataFrame with reset index and no unintentional data copies. return df.reset_index(drop=True) def update_column_b(df: pd.DataFrame) -> pd.DataFrame: Updates all values in column B where column A is greater than 25 to 1000. Args: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: DataFrame with updated column B values. df = df.copy() df.loc[df[\\"A\\"] > 25, \\"B\\"] = 1000 return df def make_array_writeable(df: pd.DataFrame) -> np.ndarray: Ensures the array view of the DataFrame is writeable and modifies the first element to 999. Args: df (pd.DataFrame): Input DataFrame. Returns: np.ndarray: Numpy array view with the first element modified to 999. arr = df.to_numpy(copy=True) arr[0, 0] = 999 return arr"},{"question":"Question: Advanced Plotting with Seaborn Rugplots **Objective:** Demonstrate your understanding of seaborn\'s plotting functions, with a focus on using and customizing rug plots. **Task:** Given the dataset `tips` (which can be loaded using `sns.load_dataset(\\"tips\\")`), create a composite plot that meets the following requirements: 1. A scatter plot showing the relationship between `total_bill` and `tip`. 2. Add a rug plot along the x-axis (`total_bill`). 3. Customize the rug plot to: - Have a height of `0.1`. - Place the rug outside the x-axis. - Use a line width of `1` and an alpha value of `0.7`. Additionally: 1. Represent a third variable (`time`) using hue mapping. 2. Ensure the scatter plot uses points that have a size proportional to the variable `size`. **Input:** None. (You will directly load the `tips` dataset within your code.) **Output:** A matplotlib figure containing the described plot. **Requirements:** - Use seaborn and matplotlib for all plotting. - Your solution should be efficient and maintain readability. **Constraints:** - The code must execute without errors. - Properly comment your code to explain each step of the plot creation process. **Example Solution:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the scatter plot plt.figure(figsize=(10, 6)) scatter = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", size=\\"size\\", sizes=(20, 200)) # Add the customized rug plot sns.rugplot(data=tips, x=\\"total_bill\\", height=0.1, lw=1, alpha=0.7, clip_on=False) # Show the plot plt.show() ``` Ensure your final plot matches these requirements exactly. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_rugplot(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the scatter plot plt.figure(figsize=(10, 6)) scatter = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", size=\\"size\\", sizes=(20, 200)) # Add the customized rug plot along the x-axis sns.rugplot(data=tips, x=\\"total_bill\\", height=0.1, lw=1, alpha=0.7, clip_on=False) # Show the plot plt.show() return scatter"},{"question":"You are tasked with writing a function `process_integer_objects()` that performs various operations using `PyLongObject` handling functions provided in the `python310` package. The function should take a list of integers and perform the following tasks: 1. Check if each integer qualifies as a `PyLongObject`. 2. Convert each integer into `PyLongObject` using an appropriate function. 3. Convert the `PyLongObject` back to a C integer type and return these integers in a new list. Your implementation should handle errors appropriately, ensuring that if any conversion fails, the function should stop and return the integers processed successfully up to that point. # Function Signature ```python def process_integer_objects(int_list): Processes a list of integers by checking and converting them to PyLongObjects and back. Args: int_list (list of int): List of integers to process. Returns: list: List of C integers after processing. Raises: ValueError: If an integer cannot be converted to a PyLongObject. ``` # Example ```python # Example usage result = process_integer_objects([10, 20, 30]) print(result) # Should output [10, 20, 30] result = process_integer_objects([10**20, -10**20, 10**30]) # Should handle and output integers successfully processed before any potential errors ``` # Constraints - The integers can be positive or negative. - The list of integers may contain large integers beyond the C `long` type limit. # Hints - Use `PyLong_Check` to check if an integer is a `PyLongObject`. - Convert integers to `PyLongObject` using functions like `PyLong_FromLong` or relevant functions based on the integer size. - Convert `PyLongObject` back to C integer using `PyLong_AsLong` or other appropriate functions. - Handle possible overflow errors and use error-checking functions to ensure robust error handling. # Requirements - Ensure the function uses `python310` provided functions for conversions and checks. - Error handling should be comprehensive and utilize provided mechanisms.","solution":"import sys # Define the PyLongObject handling functions using the built-in Python functions def PyLong_Check(val): Mimic Python\'s PyLong_Check return isinstance(val, int) def PyLong_FromLong(val): Convert from C long (int) type to PyLongObject (int in Python) return int(val) def PyLong_AsLong(py_long): Convert from PyLongObject (int in Python) to C long (int) try: py_long_int = int(py_long) if -(2**31) <= py_long_int <= (2**31 - 1): return py_long_int else: raise OverflowError(\\"Integer value too large to convert to C long\\") except (ValueError, OverflowError) as e: return None def process_integer_objects(int_list): Process a list of integers by checking and converting them to PyLongObjects and back. Args: int_list (list of int): List of integers to process. Returns: list: List of integers after processing. result_list = [] for num in int_list: if not PyLong_Check(num): raise ValueError(f\\"{num} is not a valid PyLongObject\\") py_long_object = PyLong_FromLong(num) c_long_value = PyLong_AsLong(py_long_object) if c_long_value is None: break result_list.append(c_long_value) return result_list"},{"question":"**Objective**: Implement a PyTorch function leveraging the `Join`, `Joinable`, and `JoinHook` classes to perform distributed training on uneven inputs. **Background**: In distributed training, dealing with uneven inputs across different processes can be challenging. PyTorch provides a set of classes (`Join`, `Joinable`, and `JoinHook`) to facilitate this process. Your task is to implement a training loop that correctly handles these uneven inputs. **Task**: Write a function `distributed_training_with_uneven_inputs` that: 1. Sets up a distributed training environment. 2. Utilizes the PyTorch `Join` context, `Joinable` class for the model, and `JoinHook` for handling uneven inputs. 3. Trains a simple neural network model on uneven data across multiple processes. **Function Signature**: ```python def distributed_training_with_uneven_inputs(model, dataloaders, loss_fn, optimizer, num_epochs, rank, world_size): Perform distributed training on uneven inputs. Args: model (torch.nn.Module): The neural network model to train. dataloaders (list of torch.utils.data.DataLoader): List of dataloaders for each process. loss_fn (torch.nn.Module): Loss function. optimizer (torch.optim.Optimizer): Optimizer. num_epochs (int): Number of epochs for training. rank (int): Rank of the current process. world_size (int): Total number of processes. Returns: torch.nn.Module: The trained model. # Your code here ``` **Input**: - `model`: A PyTorch neural network model. - `dataloaders`: A list of `torch.utils.data.DataLoader` instances, one per process, with potentially uneven dataset sizes. - `loss_fn`: A loss function to compute the loss. - `optimizer`: An optimizer for updating model parameters. - `num_epochs`: The number of training epochs. - `rank`: The rank/index of the current process. - `world_size`: The total number of processes in the distributed training setup. **Output**: - Returns the trained `model`. **Constraints**: - The implementation should correctly synchronize gradients and updates across all processes. - Ensure that the training loop handles cases where some processes might finish processing their data before others. **Example**: ```python import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset import torch.distributed as dist from torch.multiprocessing import Process # Define a simple model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) # Assumed provided: The function distributed_training_with_uneven_inputs() # Example setup (not part of the answer, just for context): def run_training(rank, world_size): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) dataset1 = TensorDataset(torch.randn(100, 10), torch.randn(100, 1)) dataset2 = TensorDataset(torch.randn(50, 10), torch.randn(50, 1)) dataloader1 = DataLoader(dataset1, batch_size=10) dataloader2 = DataLoader(dataset2, batch_size=10) dataloaders = [dataloader1, dataloader2] model = SimpleModel() loss_fn = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) trained_model = distributed_training_with_uneven_inputs(model, dataloaders, loss_fn, optimizer, num_epochs=5, rank=rank, world_size=world_size) dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = 2 processes = [] for rank in range(world_size): p = Process(target=run_training, args=(rank, world_size)) p.start() processes.append(p) for p in processes: p.join() ``` **Notes**: 1. The example code illustrates how to set up and initialize the distributed environment, and the data loaders with uneven input sizes. 2. The core of the task is to implement `distributed_training_with_uneven_inputs` correctly using the provided PyTorch classes for handling uneven inputs. Test your solution thoroughly to ensure correctness and efficiency.","solution":"import torch import torch.distributed as dist from torch.distributed.algorithms.join import Join, Joinable, JoinHook def distributed_training_with_uneven_inputs(model, dataloaders, loss_fn, optimizer, num_epochs, rank, world_size): Perform distributed training on uneven inputs. Args: model (torch.nn.Module): The neural network model to train. dataloaders (list of torch.utils.data.DataLoader): List of dataloaders for each process. loss_fn (torch.nn.Module): Loss function. optimizer (torch.optim.Optimizer): Optimizer. num_epochs (int): Number of epochs for training. rank (int): Rank of the current process. world_size (int): Total number of processes. Returns: torch.nn.Module: The trained model. # Set up the model for distributed training model = model.to(rank) model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank]) class SimpleModelJoinable(Joinable): def __init__(self, model, dataloader, optimizer, loss_fn): self.model = model self.dataloader = dataloader self.optimizer = optimizer self.loss_fn = loss_fn self.iterator = iter(dataloader) def join_hook(self, index, hook_state): try: inputs, targets = next(self.iterator) inputs, targets = inputs.to(rank), targets.to(rank) self.optimizer.zero_grad() outputs = self.model(inputs) loss = self.loss_fn(outputs, targets) loss.backward() self.optimizer.step() hook_state[\'loss\'].append(loss.item()) except StopIteration: pass for epoch in range(num_epochs): with Join([SimpleModelJoinable(model, dataloaders[rank], optimizer, loss_fn)], lambda state: None) as join: state = {\\"loss\\": []} join.register(SimpleModelJoinable(model, dataloaders[rank], optimizer, loss_fn), lambda state: state[\\"loss\\"]) return model"},{"question":"**Objective:** Assess your understanding of mocking concepts in `unittest.mock` and your ability to apply them to real-world testing scenarios. # Question: You have a class `FileProcessor` which reads from a file and processes its content. You are required to write unit tests for the methods in this class using the `unittest` and `unittest.mock` modules. Here is the implementation of `FileProcessor`: ```python class FileProcessor: def __init__(self, file_path): self.file_path = file_path def read_file(self): with open(self.file_path, \'r\') as file: return file.read() def process_content(self): content = self.read_file() # Processing logic; for simplicity, let\'s say it just splits the content into lines return content.splitlines() ``` # Requirements: 1. **Mock `open` function:** - Write a test case `test_read_file` that verifies the content returned by `read_file()` method using `unittest.mock`\'s `mock_open`. 2. **Mock `read_file` method:** - Write a test case `test_process_content` that mocks `read_file()` method to return a specific string and verifies the output of `process_content()` method. # Constraints: - You should use `unittest.TestCase` class for writing test cases. - Ensure that you mock the `open` function correctly and clean up mocks after the tests. # Expected Input and Output: Input: - `file_path` to the `FileProcessor` class. - Content to mock for `open` function and `read_file` method. Output: - Assertions checking the correct usage and functionality of the respective methods. # Example: ```python import unittest from unittest.mock import patch, mock_open class TestFileProcessor(unittest.TestCase): @patch(\'builtins.open\', new_callable=mock_open, read_data=\'mock file content\') def test_read_file(self, mock_file): processor = FileProcessor(\'dummy_path\') result = processor.read_file() # Check that the method returns the mock data self.assertEqual(result, \'mock file content\') # Ensure that open was called correctly mock_file.assert_called_once_with(\'dummy_path\', \'r\') @patch.object(FileProcessor, \'read_file\', return_value=\'line1nline2nline3\') def test_process_content(self, mock_read_file): processor = FileProcessor(\'dummy_path\') result = processor.process_content() # Verify the processed content self.assertEqual(result, [\'line1\', \'line2\', \'line3\']) # Ensure that read_file method was called once mock_read_file.assert_called_once() if __name__ == \'__main__\': unittest.main() ``` 1. **test_read_file:** This test verifies that the `read_file()` method reads the content correctly from a mock file. 2. **test_process_content:** This test verifies that the `process_content()` method correctly processes the mocked content returned by `read_file`. Implement these test cases demonstrating your understanding of `unittest.mock`.","solution":"import unittest from unittest.mock import patch, mock_open class FileProcessor: def __init__(self, file_path): self.file_path = file_path def read_file(self): with open(self.file_path, \'r\') as file: return file.read() def process_content(self): content = self.read_file() # Processing logic; for simplicity, let\'s say it just splits the content into lines return content.splitlines()"},{"question":"# Complex File and Data Processing Challenge Objective Create a multi-functional program in Python that performs a series of operations involving text processing, file I/O, and data serialization. Description You are required to write a Python function `process_and_store_data(input_file: str, output_file: str, search_word: str, filter_date: str) -> None`, which performs the following steps: 1. **Read and Process Text Data:** - The function takes in the path of an input text file (`input_file`). - Read the contents of the input file line by line. - Filter the lines that contain a specific word (`search_word`). 2. **String Operations:** - For the filtered lines, extract and store any dates found in the format `YYYY-MM-DD`. Use regular expressions for this purpose. - Filter out the dates that are older than the given `filter_date`. The `filter_date` will also be provided as a string in the `YYYY-MM-DD` format. 3. **Data Serialization:** - Store the filtered dates in a list. - Serialize this list to a binary file format using Python\'s `pickle` module. - Save the serialized data to the specified `output_file`. Input - `input_file`: A string representing the path to the input text file. - `output_file`: A string representing the path to the output serialized binary file. - `search_word`: A string representing the word to search for within the lines of the input file. - `filter_date`: A string representing the threshold date in `YYYY-MM-DD` format. Only dates from the filtered lines that are newer than this date should be saved. Output - None Constraints - The input text file can be very large. Efficient reading and processing of the file are essential to avoid memory issues. - You cannot use any external libraries beyond Python\'s standard library. Example ```python # Input file content: 2023-01-01: The project started today. 2023-05-20: Completed the first milestone. 2023-09-15: The project faced delays. search_word 2022-12-30: A meeting was held. process_and_store_data(\'input.txt\', \'output.pkl\', \'milestone\', \'2023-01-01\') # Expected action: The function will read the input file, filter lines containing \\"milestone\\", # extract dates, filter dates that are later than \'2023-01-01\', serialize them, and store in \'output.pkl\'. ``` Notes - Remember to handle edge cases like empty files, missing dates, invalid date formats, and non-existent input files. - Proper error handling should be implemented for file operations. Submission Submit your function definition along with any supporting helper functions you implemented.","solution":"import re import pickle from datetime import datetime def process_and_store_data(input_file: str, output_file: str, search_word: str, filter_date: str) -> None: dates_list = [] filter_date_dt = datetime.strptime(filter_date, \\"%Y-%m-%d\\") # Reading the input file try: with open(input_file, \'r\') as file: for line in file: # Check if the search_word is in the line if search_word in line: # Extract dates in the format YYYY-MM-DD dates = re.findall(r\'d{4}-d{2}-d{2}\', line) for date in dates: date_dt = datetime.strptime(date, \\"%Y-%m-%d\\") # Check if the date is newer than filter_date if date_dt > filter_date_dt: dates_list.append(date) except FileNotFoundError: print(f\\"Error: The file \'{input_file}\' was not found.\\") return # Serializing the list of filtered dates to a binary file try: with open(output_file, \'wb\') as file: pickle.dump(dates_list, file) except Exception as e: print(f\\"Error: Could not write to the output file \'{output_file}\': {e}\\")"},{"question":"# Seaborn Custom Palette Visualization **Objective:** Create a custom visualization using seaborn that demonstrates your comprehension of the `cubehelix_palette` function and its customization options. **Task:** 1. Write a Python function `generate_custom_palette_visualization(data, n_colors, start, rot, gamma, hue, dark, light, reverse)` that creates a seaborn scatter plot using a customized cubehelix palette. 2. The function should: - Accept the following parameters: - `data` (DataFrame): A DataFrame containing at least two numerical columns for the x and y axes. - `n_colors` (int): Number of colors in the palette. - `start` (float): The starting color. - `rot` (float): The number of rotations in the colormap. - `gamma` (float): Gamma factor to apply to the luminance. - `hue` (float): Saturation factor. - `dark` (float): Luminance value for the darkest color. - `light` (float): Luminance value for the lightest color. - `reverse` (bool): Whether to reverse the direction of the luminance ramp. - Create a cubehelix palette using the provided parameters. - Apply this palette to a scatter plot using seaborn. - Ensure the scatter plot has proper labels and a title indicating the customized palette parameters used. 3. The scatter plot should be displayed as the function\'s output. **Input:** - `data`: A pandas DataFrame with at least two numerical columns. - `n_colors`: Integer value specifying the number of colors in the palette (e.g., 8). - `start`: Float value for the starting color (e.g., 0.0). - `rot`: Float value for the number of rotations (e.g., 0.5). - `gamma`: Float value for the gamma factor (e.g., 1.0). - `hue`: Float value for the saturation (e.g., 0.8). - `dark`: Float value for the darkest color (e.g., 0.2). - `light`: Float value for the lightest color (e.g., 0.8). - `reverse`: Boolean value indicating if the luminance ramp should be reversed (e.g., False). **Output:** - A seaborn scatter plot with the customized cubehelix palette applied. **Constraints:** - You can assume the DataFrame always contains at least two numerical columns. - Ensure all parameters passed to `cubehelix_palette` are appropriately validated and within expected ranges. Here is an example usage of the function: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Sample data data = pd.DataFrame({ \'x\': range(10), \'y\': [i**2 for i in range(10)] }) # Function to generate the custom visualization def generate_custom_palette_visualization(data, n_colors, start, rot, gamma, hue, dark, light, reverse): palette = sns.cubehelix_palette(n_colors=n_colors, start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light, reverse=reverse) sns.scatterplot(data=data, x=data.columns[0], y=data.columns[1], palette=palette) plt.title(f\'Customized Cubehelix Palette: n_colors={n_colors}, start={start}, rot={rot}, gamma={gamma}, hue={hue}, dark={dark}, light={light}, reverse={reverse}\') plt.show() # Generate visualization generate_custom_palette_visualization(data, n_colors=8, start=0, rot=0.4, gamma=0.8, hue=0.5, dark=0.2, light=0.8, reverse=False) ``` **Performance Requirement:** - The function should generate and display the plot within a reasonable time frame for datasets of up to 10,000 rows.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_custom_palette_visualization(data, n_colors, start, rot, gamma, hue, dark, light, reverse): Creates a scatter plot using a customized cubehelix palette. Parameters: - data (pd.DataFrame): DataFrame containing at least two numerical columns. - n_colors (int): Number of colors in the palette. - start (float): Starting color. - rot (float): Number of rotations in the colormap. - gamma (float): Gamma factor to apply to the luminance. - hue (float): Saturation factor. - dark (float): Luminance value for the darkest color. - light (float): Luminance value for the lightest color. - reverse (bool): Whether to reverse the direction of the luminance ramp. # Generate the custom cubehelix palette palette = sns.cubehelix_palette(n_colors=n_colors, start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light, reverse=reverse) # Create the scatter plot using the palette scatter_plot = sns.scatterplot(data=data, x=data.columns[0], y=data.columns[1], palette=palette) # Set the plot title with custom parameters plt.title(f\'Customized Cubehelix Palette: n_colors={n_colors}, start={start}, rot={rot}, gamma={gamma}, hue={hue}, dark={dark}, light={light}, reverse={reverse}\') # Display the plot plt.show()"},{"question":"**Title: Implement a Custom Iterable with Optimization Features** **Objective:** This question aims to test your understanding of iterators, generators, higher-order functions, and functional programming in Python. **Problem Statement:** You are required to implement a custom iterable object that iterates over a sequence of numbers, but with specific enhancements: 1. It should support iteration using the `__iter__` and `__next__` methods. 2. It should have a method `filter` that accepts a predicate function and returns an iterable of numbers that satisfy the predicate. 3. It should have a method `map` that accepts a transformation function and returns an iterable of the transformed numbers. 4. It should support lazy evaluation, meaning the numbers should be generated as needed. 5. It should have a method `accumulate` that uses a custom reduction function to accumulate values. **Specifications:** - The iterable should be created with a starting number (`start`) and a step size (`step`). - The number sequence should be infinite, but it’s expected that only the needed numbers are generated. - The `filter` method should use lazy evaluation and yield numbers that satisfy the predicate. - The `map` method should apply the transformation function lazily and yield transformed numbers. - The `accumulate` method should apply the reduction function lazily and yield accumulated results. **Function Signatures:** ```python class CustomIterable: def __init__(self, start: int, step: int): # Initialize with a starting number and step size pass def __iter__(self): # Return an iterator object pass def __next__(self): # Return the next number in the sequence pass def filter(self, predicate: callable): # Return an iterable of numbers that satisfy the predicate pass def map(self, transform: callable): # Return an iterable of transformed numbers pass def accumulate(self, func: callable): # Return an iterable of accumulated results pass ``` **Example Usage:** ```python # Create an instance of CustomIterable sequence = CustomIterable(start=1, step=2) # Filter numbers greater than 5 filtered = sequence.filter(lambda x: x > 5) print(next(filtered)) # Outputs numbers greater than 5 # Map numbers to their squares mapped = sequence.map(lambda x: x * x) print(next(mapped)) # Outputs squares of numbers # Accumulate numbers by adding them accumulated = sequence.accumulate(lambda x, y: x + y) print(next(accumulated)) # Outputs cumulative sum of numbers ``` **Constraints:** - You may assume that the input functions for `filter`, `map`, and `accumulate` are valid. - Your implementation should be memory efficient and leverage lazy evaluation for handling potentially infinite sequences. **Notes:** - Pay attention to using generators and maintaining the iterator protocol. - Utilize functional programming constructs and appropriate library functions where necessary. Your task is to implement the `CustomIterable` class with all the specified methods.","solution":"class CustomIterable: def __init__(self, start: int, step: int): self.current = start self.step = step def __iter__(self): return self def __next__(self): current_value = self.current self.current += self.step return current_value def filter(self, predicate): class FilteredIterable: def __init__(self, iterable, predicate): self.iterable = iterable self.predicate = predicate def __iter__(self): return self def __next__(self): while True: value = next(self.iterable) if self.predicate(value): return value return FilteredIterable(self, predicate) def map(self, transform): class MappedIterable: def __init__(self, iterable, transform): self.iterable = iterable self.transform = transform def __iter__(self): return self def __next__(self): value = next(self.iterable) return self.transform(value) return MappedIterable(self, transform) def accumulate(self, func): class AccumulatedIterable: def __init__(self, iterable, func): self.iterable = iterable self.func = func self.accum = None def __iter__(self): return self def __next__(self): if self.accum is None: self.accum = next(self.iterable) else: self.accum = self.func(self.accum, next(self.iterable)) return self.accum return AccumulatedIterable(self, func)"},{"question":"# Advanced Path Manipulation and Querying with `pathlib` Implement a function `find_python_scripts` that takes a directory path as input and returns information about all Python scripts within that directory and its subdirectories. The function should return a dictionary where each key is the relative path to a Python script, and the value is another dictionary containing the following information: - `size`: The size of the file in bytes. - `last_modified`: The last modified timestamp of the file. - `is_symlink`: Whether or not the file is a symlink. - `resolved_path`: The resolved absolute path of the file (in case it is a symlink). Additionally, implement a small script to demonstrate the usage of this function by: 1. Creating a hierarchical directory structure with some Python scripts and symlinks. 2. Using the `find_python_scripts` function to get information about these scripts. 3. Printing the output in a readable format. Requirements - Use the `pathlib` module exclusively for path manipulations and filesystem operations. - Assume the input to the function will always be a valid directory path. - The function should handle large directory trees efficiently. Function Signature ```python from pathlib import Path import time from typing import Dict def find_python_scripts(directory: str) -> Dict[str, Dict[str, str]]: pass # Demonstration script if __name__ == \\"__main__\\": # Create directory structure and sample files pass ``` Example If the directory structure is as follows: ``` project/ ├── script1.py ├── script2.py ├── dir1/ │ ├── script3.py │ └── dir2/ │ └── script4.py └── symlink_script.py -> dir1/script3.py ``` A call to `find_python_scripts(\\"project\\")` might return: ```python { \\"script1.py\\": { \\"size\\": 1234, \\"last_modified\\": 1625086172.198, \\"is_symlink\\": False, \\"resolved_path\\": \\"/absolute/path/to/project/script1.py\\" }, \\"script2.py\\": { \\"size\\": 5678, \\"last_modified\\": 1625086172.198, \\"is_symlink\\": False, \\"resolved_path\\": \\"/absolute/path/to/project/script2.py\\" }, \\"dir1/script3.py\\": { \\"size\\": 910, \\"last_modified\\": 1625086172.198, \\"is_symlink\\": False, \\"resolved_path\\": \\"/absolute/path/to/project/dir1/script3.py\\" }, \\"dir1/dir2/script4.py\\": { \\"size\\": 1112, \\"last_modified\\": 1625086172.198, \\"is_symlink\\": False, \\"resolved_path\\": \\"/absolute/path/to/project/dir1/dir2/script4.py\\" }, \\"symlink_script.py\\": { \\"size\\": 910, \\"last_modified\\": 1625086172.198, \\"is_symlink\\": True, \\"resolved_path\\": \\"/absolute/path/to/project/dir1/script3.py\\" } } ```","solution":"from pathlib import Path from typing import Dict import os import time def find_python_scripts(directory: str) -> Dict[str, Dict[str, str]]: scripts_info = {} directory_path = Path(directory) for path in directory_path.rglob(\\"*.py\\"): relative_path = str(path.relative_to(directory_path)) stat = path.lstat() script_info = { \\"size\\": stat.st_size, \\"last_modified\\": stat.st_mtime, \\"is_symlink\\": path.is_symlink(), \\"resolved_path\\": str(path.resolve()) if path.is_symlink() else \\"\\" } scripts_info[relative_path] = script_info return scripts_info # Demonstration script if __name__ == \\"__main__\\": import shutil # Create directory structure and sample files demo_dir = Path(\\"demo_project\\") if demo_dir.exists(): shutil.rmtree(demo_dir) demo_dir.mkdir() (demo_dir / \\"script1.py\\").write_text(\\"# script1\\") (demo_dir / \\"script2.py\\").write_text(\\"# script2\\") (demo_dir / \\"dir1\\").mkdir() (demo_dir / \\"dir1\\" / \\"script3.py\\").write_text(\\"# script3\\") (demo_dir / \\"dir1\\" / \\"dir2\\").mkdir() (demo_dir / \\"dir1\\" / \\"dir2\\" / \\"script4.py\\").write_text(\\"# script4\\") (demo_dir / \\"symlink_script.py\\").symlink_to(demo_dir / \\"dir1\\" / \\"script3.py\\") # Finding python scripts and displaying the output scripts = find_python_scripts(str(demo_dir)) for script, info in scripts.items(): print(f\\"{script}:\\") for key, value in info.items(): print(f\\"t{key}: {value}\\")"},{"question":"You are tasked with preparing a dataset for a classification problem. The dataset includes both multiclass and multilabel labels, and you need to ensure the data is appropriately transformed for use in scikit-learn models. Write a function `transform_labels` that takes in two arguments: 1. `multi_class_labels`: A list of numeric multiclass labels. 2. `multi_label_sets`: A list of sets, where each set contains numeric multilabels. The function should return a tuple containing: 1. `encoded_multi_class_labels`: The multiclass labels transformed into a numeric array starting from 0. 2. `binary_indicator_multi_class`: A binary indicator matrix corresponding to the multiclass labels. 3. `binary_indicator_multi_labels`: A binary indicator matrix corresponding to the multilabel data. Input: - `multi_class_labels`: A list of integers, e.g., `[1, 2, 6, 4, 2]` - `multi_label_sets`: A list of sets of integers, e.g., `[{2, 3, 4}, {2}, {0, 1, 3}, {0, 1, 2, 3, 4}, {0, 1, 2}]` Output: A tuple containing: 1. `encoded_multi_class_labels`: A numpy array of encoded multiclass labels, e.g., `array([0, 1, 2, 3, 1])` 2. `binary_indicator_multi_class`: A binary indicator matrix for multiclass labels, e.g., `array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 1, 0, 0]])` 3. `binary_indicator_multi_labels`: A binary indicator matrix for multilabel sets, e.g., `array([[0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])` Constraints: - The input lists will contain at least one element each. - The elements of `multi_class_labels` and the sets in `multi_label_sets` will be non-negative integers. Example ```python def transform_labels(multi_class_labels, multi_label_sets): # Your code here # Example usage: multi_class_labels = [1, 2, 6, 4, 2] multi_label_sets = [{2, 3, 4}, {2}, {0, 1, 3}, {0, 1, 2, 3, 4}, {0, 1, 2}] encoded_multi_class_labels, binary_indicator_multi_class, binary_indicator_multi_labels = transform_labels(multi_class_labels, multi_label_sets) print(encoded_multi_class_labels) # Output: array([0, 1, 2, 3, 1]) print(binary_indicator_multi_class) # Output: # array([[1, 0, 0, 0], # [0, 1, 0, 0], # [0, 0, 1, 0], # [0, 0, 0, 1], # [0, 1, 0, 0]]) print(binary_indicator_multi_labels) # Output: # array([[0, 0, 1, 1, 1], # [0, 0, 1, 0, 0], # [1, 1, 0, 1, 0], # [1, 1, 1, 1, 1], # [1, 1, 1, 0, 0]]) ```","solution":"import numpy as np from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer def transform_labels(multi_class_labels, multi_label_sets): Transforms multiclass and multilabel data into encoded and binary indicator formats. Parameters: multi_class_labels (list of int): List of numeric multiclass labels. multi_label_sets (list of sets of int): List of sets, where each set contains numeric multilabels. Returns: tuple: A tuple containing: - encoded_multi_class_labels (np.array): The multiclass labels transformed into a numeric array starting from 0. - binary_indicator_multi_class (np.array): Binary indicator matrix corresponding to the multiclass labels. - binary_indicator_multi_labels (np.array): Binary indicator matrix corresponding to the multilabel data. # Encode multi-class labels label_encoder = LabelEncoder() encoded_multi_class_labels = label_encoder.fit_transform(multi_class_labels) # Create binary indicator matrix for multi-class labels unique_classes = len(label_encoder.classes_) binary_indicator_multi_class = np.eye(unique_classes)[encoded_multi_class_labels] # Create binary indicator matrix for multi-label sets mlb = MultiLabelBinarizer() binary_indicator_multi_labels = mlb.fit_transform(multi_label_sets) return encoded_multi_class_labels, binary_indicator_multi_class, binary_indicator_multi_labels"},{"question":"# PyTorch MPS Device Management and Profiling Problem Statement You are required to implement and demonstrate a series of functions that will interact with the PyTorch MPS (Metal Performance Shaders) backend. The tasks include checking the availability of MPS devices, handling random number generation states, managing memory, and profiling a sample computation on MPS devices. Implement the following functions: 1. **check_mps_device_count()**: - Returns an integer indicating the number of MPS devices available. 2. **set_random_seed(seed: int)**: - Sets the random number generation seed for MPS devices using the provided `seed` value. 3. **profile_tensor_operations()**: - Performs a basic tensor operation on the MPS device, profiled using MPS profiler. The operation should: - Create two random tensors of size (1000, 1000) on the MPS device. - Perform matrix multiplication on these tensors. - Return the time taken to perform the operation as a float in seconds. 4. **verify_memory_management()**: - Returns a dictionary with the following keys and their corresponding integer values: - `\'current_allocated_memory\'`: The current memory allocated by MPS. - `\'driver_allocated_memory\'`: The memory allocated by the MPS driver. - `\'recommended_max_memory\'`: The recommended maximum memory for MPS. Input Format - `set_random_seed(seed: int)` receives an integer input `seed` which is used for setting the random number generator seed. Output Format - `check_mps_device_count()` returns an integer. - `set_random_seed(seed: int)` returns `None`. - `profile_tensor_operations()` returns a float. - `verify_memory_management()` returns a dictionary with three keys and integer values. Example ```python # Example execution print(check_mps_device_count()) # might return 1, indicating one MPS device available set_random_seed(42) # sets the RNG for MPS print(profile_tensor_operations()) # might return 0.0023, indicating the operation took 2.3 milliseconds print(verify_memory_management()) # {\'current_allocated_memory\': 1048576, \'driver_allocated_memory\': 2097152, \'recommended_max_memory\': 4194304} ``` Constraints - Assume the functions will be called in an environment where MPS is supported. - Ensure the tensor operations in `profile_tensor_operations()` are performed on the MPS device. - Handle scenarios gracefully where MPS profiling or device-related queries might return unexpected results.","solution":"import torch import time def check_mps_device_count(): Returns the number of MPS devices available. if torch.backends.mps.is_available(): return torch.mps.device_count() return 0 def set_random_seed(seed: int): Sets the random number generation seed for MPS devices. torch.manual_seed(seed) if torch.backends.mps.is_available(): torch.mps.manual_seed(seed) def profile_tensor_operations(): Performs tensor operations on MPS device and profiles the time taken. if not torch.backends.mps.is_available(): return float(\'inf\') # Indicating MPS is not available device = torch.device(\\"mps\\") # Allocate large tensors on the MPS device tensor1 = torch.randn(1000, 1000, device=device) tensor2 = torch.randn(1000, 1000, device=device) # Perform tensor operation and time it start_time = time.time() result = torch.mm(tensor1, tensor2) # Matrix multiplication end_time = time.time() return end_time - start_time def verify_memory_management(): Returns the memory management details for MPS. if not torch.backends.mps.is_available(): return { \'current_allocated_memory\': 0, \'driver_allocated_memory\': 0, \'recommended_max_memory\': 0, } mps_stats = torch.mps.get_device_properties(0) memory_info = { \'current_allocated_memory\': mps_stats.total_memory_allocated, \'driver_allocated_memory\': mps_stats.total_memory_used, \'recommended_max_memory\': mps_stats.max_memory_allocated } return memory_info"},{"question":"You are tasked with writing a Python script that recursively traverses a given directory and performs the following tasks: 1. Identify and count the total number of each type of file (regular files, directories, symbolic links, etc.). 2. Print the permissions of each file in a human-readable format (e.g., `-rwxr-xr-x`). 3. For regular files, change their permissions to read-only for the owner, group, and others (`r--r--r--`). **Function Signature:** ```python def analyze_directory(root_dir: str) -> Dict[str, int]: Traverses the given directory and performs tasks as per the requirements. Args: root_dir (str): The root directory to start the traversal. Returns: Dict[str, int]: A dictionary with file type as keys and the count of each type as values. pass ``` **Input:** - `root_dir` (str): The absolute or relative path to the root directory to traverse. **Output:** - A dictionary where the keys are file types (e.g., `regular_files`, `directories`, `symbolic_links`, etc.) and the values are the count of each type. **Constraints:** - You may assume the provided directory exists and is accessible. - You should handle any potential exceptions (e.g., permission errors) gracefully. **Example:** Suppose the directory structure is as follows: ``` /example /dir1 file1.txt link1 -> /example/dir2 /dir2 file2.txt file3.txt ``` If `root_dir` is `/example`, the function should: 1. Count and print the number of regular files, directories, and symbolic links. 2. Print the permissions of each file in a human-readable format. 3. Change the permissions of regular files to `r--r--r--`. Sample Output: ``` {\'regular_files\': 3, \'directories\': 2, \'symbolic_links\': 1} -visiting: /example Permissions: drwxr-xr-x -visiting: /example/dir1 Permissions: drwxr-xr-x -visiting: /example/dir1/file1.txt Permissions: -r--r--r-- -visiting: /example/dir1/link1 Permissions: lrwxr-xr-x -visiting: /example/dir2 Permissions: drwxr-xr-x -visiting: /example/dir2/file2.txt Permissions: -r--r--r-- -visiting: /example/dir2/file3.txt Permissions: -r--r--r-- ``` Complete the implementation of the `analyze_directory` function to achieve the described functionality.","solution":"import os import stat from typing import Dict def analyze_directory(root_dir: str) -> Dict[str, int]: file_counts = { \'regular_files\': 0, \'directories\': 0, \'symbolic_links\': 0, \'others\': 0 } def get_permissions(filepath): st = os.stat(filepath) # File permission bits file_mode = st.st_mode # Convert to human-readable format is_dir = \'d\' if stat.S_ISDIR(file_mode) else \'-\' user = \'r\' if file_mode & stat.S_IRUSR else \'-\' user += \'w\' if file_mode & stat.S_IWUSR else \'-\' user += \'x\' if file_mode & stat.S_IXUSR else \'-\' group = \'r\' if file_mode & stat.S_IRGRP else \'-\' group += \'w\' if file_mode & stat.S_IWGRP else \'-\' group += \'x\' if file_mode & stat.S_IXGRP else \'-\' others = \'r\' if file_mode & stat.S_IROTH else \'-\' others += \'w\' if file_mode & stat.S_IWOTH else \'-\' others += \'x\' if file_mode & stat.S_IXOTH else \'-\' return is_dir + user + group + others def analyze(path): try: for entry in os.scandir(path): if entry.is_file(follow_symlinks=False): file_counts[\'regular_files\'] += 1 # Change permissions to \'r--r--r--\' os.chmod(entry.path, stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH) elif entry.is_dir(follow_symlinks=False): file_counts[\'directories\'] += 1 analyze(entry.path) # Recursive call elif entry.is_symlink(): file_counts[\'symbolic_links\'] += 1 else: file_counts[\'others\'] += 1 print(f\\"-visiting: {entry.path}\\") print(f\\"Permissions: {get_permissions(entry.path)}\\") except PermissionError as e: print(f\\"PermissionError: {e}\\") analyze(root_dir) return file_counts"},{"question":"# Question: A certain scientific computation application frequently uses very large matrices, where the majority of the elements are zeros. To optimize storage and computation, you need to work with sparse tensors using the PyTorch library. You are given three 2D matrices represented as dense tensors using PyTorch. Write a Python function that: 1. Converts these dense tensors to sparse COO format. 2. Computes the sum of these sparse tensors in sparse COO format. 3. Converts the result back to a dense tensor. 4. Returns the dense tensor result. Your implementation should follow these specifications: Function Signature ```python def sparse_tensor_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor, tensor_c: torch.Tensor) -> torch.Tensor: ``` Input - `tensor_a`: A `torch.Tensor` of shape `(m, n)` with mostly zero elements. - `tensor_b`: A `torch.Tensor` of shape `(m, n)` with mostly zero elements. - `tensor_c`: A `torch.Tensor` of shape `(m, n)` with mostly zero elements. Output - Returns a dense `torch.Tensor` of shape `(m, n)` representing the sum of the sparse tensors derived from input tensors. Constraints - The tensors will have dimensions up to 1000x1000. - You should use PyTorch\'s sparse tensor functionalities described in the documentation. - Aim for memory efficiency and clarity in your implementation. Example: ```python import torch a = torch.tensor([[0, 0, 1], [0, 2, 0], [3, 0, 0]]) b = torch.tensor([[4, 0, 0], [0, 0, 5], [0, 6, 0]]) c = torch.tensor([[0, 0, 0], [7, 0, 0], [0, 0, 8]]) print(sparse_tensor_operations(a, b, c)) ``` Expected Output: ```python tensor([[4, 0, 1], [7, 2, 5], [3, 6, 8]]) ``` The task assesses your ability to handle sparse tensor formats and perform operations efficiently in PyTorch.","solution":"import torch def sparse_tensor_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor, tensor_c: torch.Tensor) -> torch.Tensor: Converts dense tensors to sparse COO format, sums them, and converts the result back to a dense tensor. # Convert dense tensors to sparse COO format sparse_a = tensor_a.to_sparse() sparse_b = tensor_b.to_sparse() sparse_c = tensor_c.to_sparse() # Sum the sparse tensors sparse_sum = sparse_a + sparse_b + sparse_c # Convert the result back to a dense tensor dense_sum = sparse_sum.to_dense() return dense_sum"},{"question":"**Question:** Many pre-trained neural network models provided by the PyTorch library use Batch Normalization layers. However, in certain use cases, such as when using functorch\'s vmapping for batch processing, Batch Normalization\'s in-place updates can cause issues. Your task is to write a function that takes a neural network model and a choice of normalization method and updates all the Batch Normalization layers in the model to the chosen method. **Function Signature:** ```python def replace_batch_norm(model: torch.nn.Module, num_groups: int = 0, use_group_norm: bool = True) -> torch.nn.Module: pass ``` **Parameters:** 1. `model` (torch.nn.Module): A PyTorch neural network model possibly containing BatchNorm layers. 2. `num_groups` (int): The number of groups to use for GroupNorm (if `use_group_norm` is True). If 0, each channel will be treated separately. 3. `use_group_norm` (bool): If True, replace BatchNorm with GroupNorm. If False, replace BatchNorm with a version that does not use running stats. **Returns:** - `torch.nn.Module`: The modified neural network model with replaced normalization layers as specified. **Constraints:** - If `use_group_norm` is True and `num_groups` is not specified or set to 0, default to using each channel separately (`C == G`). **Example Usage:** ```python import torch.nn as nn import torchvision.models as models # Load a pre-trained ResNet18 model model = models.resnet18(pretrained=True) # Replace BatchNorm with GroupNorm using 8 groups updated_model = replace_batch_norm(model, num_groups=8, use_group_norm=True) # Check that all BatchNorm layers are replaced with GroupNorm layers print(updated_model) ``` **Additional Information:** - The function should handle converting `BatchNorm1d`, `BatchNorm2d`, and `BatchNorm3d` to `GroupNorm` or their running stats-free versions as applicable. - You should ensure that the function works recursively, to cover cases where the BatchNorm layers are nested within other modules. Feel free to use utility functions if needed to simplify the code. Write your function implementation below: ```python def replace_batch_norm(model: torch.nn.Module, num_groups: int = 0, use_group_norm: bool = True) -> torch.nn.Module: # Your implementation goes here pass ```","solution":"import torch import torch.nn as nn def replace_batch_norm(model: torch.nn.Module, num_groups: int = 0, use_group_norm: bool = True) -> torch.nn.Module: Replaces all BatchNorm layers in the model with GroupNorm or a version that doesn\'t use running stats. def _replace_layer(layer): if isinstance(layer, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)): if use_group_norm: num_features = layer.num_features if num_groups == 0: num_groups_local = num_features else: num_groups_local = num_groups return nn.GroupNorm(num_groups=num_groups_local, num_channels=num_features) else: return nn.BatchNorm2d(num_features=layer.num_features, affine=layer.affine, track_running_stats=False) return layer for name, module in model.named_children(): if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)): setattr(model, name, _replace_layer(module)) else: replace_batch_norm(module, num_groups, use_group_norm) return model"},{"question":"# Configuration File Manager Objective: You are required to create a Python function that reads a given configuration file, modifies some values based on certain rules, and writes the updated configuration back to the file. Function Signature: ```python def manage_config(file_path: str) -> None: pass ``` Input: - `file_path` (str): Path to the configuration file. Process: 1. **Read the provided configuration file**: - Utilize the `configparser` module to read configurations from the file specified by `file_path`. 2. **Modify Values**: - For the section `[USER]`, if `age` field is present and its value is below 18, set it to 18. - Interpolate a new value for the field `welcome_message` in the section `[SETTINGS]` using the `username` from the section `[USER]`. The `welcome_message` should be `\\"Welcome, <username>!\\"`. - If any field under `[PATHS]` contains an environment variable (e.g., `{HOME}`), resolve it using the actual environment variable value. 3. **Write Back to File**: - Write the modified configurations back to the file. Constraints: - Assume the configuration file is a valid INI file. - The `[USER]` section and `username` field in `[USER]`, if present, will always contain valid strings. - Use the `configparser` module to handle ini configuration manipulations. - Preserve the structure and comments of the original configuration as much as possible. Example: Suppose the `config.ini` file contains: ``` [USER] username = john_doe age = 17 [SETTINGS] theme = dark [PATHS] home_dir = {HOME}/projects ``` After calling `manage_config(\'config.ini\')`, the file should be updated to: ``` [USER] username = john_doe age = 18 [SETTINGS] theme = dark welcome_message = Welcome, john_doe! [PATHS] home_dir = /actual/home/path/projects ``` Where `/actual/home/path` corresponds to the value of the `HOME` environment variable.","solution":"import configparser import os def manage_config(file_path: str) -> None: config = configparser.ConfigParser() config.read(file_path) # Modify USER section if \'USER\' in config: if \'age\' in config[\'USER\']: age = int(config[\'USER\'][\'age\']) if age < 18: config[\'USER\'][\'age\'] = \'18\' # Interpolate welcome_message in SETTINGS section if \'USER\' in config and \'SETTINGS\' in config and \'username\' in config[\'USER\']: username = config[\'USER\'][\'username\'] config[\'SETTINGS\'][\'welcome_message\'] = f\'Welcome, {username}!\' # Resolve environment variables in PATHS section if \'PATHS\' in config: for key in config[\'PATHS\']: value = config[\'PATHS\'][key] config[\'PATHS\'][key] = os.path.expandvars(value) # Write back to the file with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"# Question: Implement a Scikit-Learn Transformer for Log Transformation The `LogTransformer` class is a custom transformer to apply a log transformation to the input data, which can help in stabilizing variance and making the data more Gaussian-like. Implement the `LogTransformer` class with `fit` and `transform` methods. Requirements: - Your `LogTransformer` class should inherit from `sklearn.base.BaseEstimator` and `sklearn.base.TransformerMixin`. - Implement the `fit` method which, in this case, doesn\'t need to compute anything but should return `self`. - Implement the `transform` method which applies the log transformation (natural logarithm) to the input data. - Assume the input data is a NumPy array or a Pandas DataFrame. Avoid transforming data that contains zero or negative values by - replace such values with a minimal positive constant (e.g., 1e-9) before applying the log transformation. Example Usage: ```python import numpy as np import pandas as pd from sklearn.base import BaseEstimator, TransformerMixin class LogTransformer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y=None): return self def transform(self, X): X = np.where(X <= 0, 1e-9, X) return np.log(X) # Example data data = np.array([[1, 2, 3], [4, 0, -1], [5, 6, 7]]) log_transformer = LogTransformer() log_transformed_data = log_transformer.fit_transform(data) print(log_transformed_data) ``` Expected Output: The expected output should be the natural logarithm of the input data with 0 or negative values replaced by 1e-9 before the log transformation. For the example provided, the output should be: ``` [[0. 0.69314718 1.09861229] [1.38629436 -20.72326584 -20.72326584] [1.60943791 1.79175947 1.94591015]] ``` Constraints: - Do not use any loops. Utilize NumPy\'s vectorized operations for efficiency. - Handle cases where input data contains zero or negative values as described. Write and implement the `LogTransformer` class according to the above specifications.","solution":"import numpy as np from sklearn.base import BaseEstimator, TransformerMixin class LogTransformer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y=None): return self def transform(self, X): # Replace 0 or negative values with a small positive constant X = np.where(X <= 0, 1e-9, X) # Apply natural logarithm return np.log(X)"},{"question":"Implementing Forward and Backward Prefetching for FSDP in PyTorch In this coding assessment, you are required to implement key aspects of the Fully Sharded Data Parallel (FSDP) framework in PyTorch, focusing on managing communication buffers and implementing forward and backward prefetching mechanisms. # Problem Statement Implement the following functionalities in PyTorch: 1. **Forward Prefetching**: Implement both implicit and explicit forward prefetching mechanisms to manage all-gather operations and overlap them with forward computations. 2. **Backward Prefetching**: Implement explicit backward prefetching to ensure overlap of all-gather and reduce-scatter operations with backward computations. 3. **Communication Buffer Management**: Calculate and manage communication buffers required for all-gather operations during forward and backward passes, considering sharded parameter placements. # Function Signatures You are required to implement the following functions: 1. **Implicit Forward Prefetching**: ```python def implicit_forward_prefetch(layer_gathers: List[Callable], forward_computes: List[Callable]) -> None: Args: layer_gathers (List[Callable]): List of all-gather operations for each layer. forward_computes (List[Callable]): List of forward computations for each layer. This function should overlap all-gathers with forward computations using implicit prefetching. pass ``` 2. **Explicit Forward Prefetching**: ```python def explicit_forward_prefetch(layer_gathers: List[Callable], forward_computes: List[Callable]) -> None: Args: layer_gathers (List[Callable]): List of all-gather operations for each layer. forward_computes (List[Callable]): List of forward computations for each layer. This function should change the CPU issue order to prefetch all-gathers explicitly. pass ``` 3. **Explicit Backward Prefetching**: ```python def explicit_backward_prefetch(reduce_scatters: List[Callable], backward_computes: List[Callable]) -> None: Args: reduce_scatters (List[Callable]): List of reduce-scatter operations for gradients. backward_computes (List[Callable]): List of backward computations for each layer. This function should ensure overlap of reduce-scatters and all-gathers with backward computations explicitly. pass ``` 4. **Communication Buffer Calculation**: ```python def calculate_communication_buffers(num_params: int, dtype_bytes: int, num_gpus: int) -> Tuple[int, int]: Args: num_params (int): Total number of parameters in the transformer block. dtype_bytes (int): Size of the data type in bytes (e.g., 4 bytes for fp32). num_gpus (int): Number of GPUs across which the parameters are sharded. Returns: Tuple[int, int]: The calculated sizes of the communication buffers required for forward and backward passes. pass ``` # Constraints - The implementation should handle parameters sharded over multiple GPUs. - The functions should efficiently manage memory and overlap computations with communications to optimize training performance. - Ensure to test the functions with dummy layers and computations to validate the overlapping mechanisms. # Example Usage You should test the implemented functions with dummy all-gather, reduce-scatter, and computation operations to ensure proper overlapping and buffer management. A minimal example: ```python def dummy_all_gather(): pass def dummy_forward_compute(): pass def dummy_reduce_scatter(): pass def dummy_backward_compute(): pass layer_gathers = [dummy_all_gather, dummy_all_gather] forward_computes = [dummy_forward_compute, dummy_forward_compute] reduce_scatters = [dummy_reduce_scatter, dummy_reduce_scatter] backward_computes = [dummy_backward_compute, dummy_backward_compute] implicit_forward_prefetch(layer_gathers, forward_computes) explicit_forward_prefetch(layer_gathers, forward_computes) explicit_backward_prefetch(reduce_scatters, backward_computes) num_params = 1_600_000_000 dtype_bytes = 4 num_gpus = 8 forward_buffer_size, backward_buffer_size = calculate_communication_buffers(num_params, dtype_bytes, num_gpus) print(f\\"Forward Buffer Size: {forward_buffer_size} bytes, Backward Buffer Size: {backward_buffer_size} bytes\\") ``` # Evaluation Criteria - Correct implementation of prefetching mechanisms. - Proper calculation and management of communication buffers. - Efficient memory and computation overlap management. Make sure to validate your implementation thoroughly with test cases that simulate real-world scenarios.","solution":"from typing import List, Callable, Tuple def implicit_forward_prefetch(layer_gathers: List[Callable], forward_computes: List[Callable]) -> None: Args: layer_gathers (List[Callable]): List of all-gather operations for each layer. forward_computes (List[Callable]): List of forward computations for each layer. This function should overlap all-gathers with forward computations using implicit prefetching. for gather, compute in zip(layer_gathers, forward_computes): # Perform all-gather operation gather() # Immediately perform the forward computation compute() def explicit_forward_prefetch(layer_gathers: List[Callable], forward_computes: List[Callable]) -> None: Args: layer_gathers (List[Callable]): List of all-gather operations for each layer. forward_computes (List[Callable]): List of forward computations for each layer. This function should change the CPU issue order to prefetch all-gathers explicitly. for i, compute in enumerate(forward_computes): # Prefetch all-gather operation for the next layer if possible if i + 1 < len(layer_gathers): layer_gathers[i + 1]() # Perform forward computation for the current layer compute() def explicit_backward_prefetch(reduce_scatters: List[Callable], backward_computes: List[Callable]) -> None: Args: reduce_scatters (List[Callable]): List of reduce-scatter operations for gradients. backward_computes (List[Callable]): List of backward computations for each layer. This function should ensure overlap of reduce-scatters and all-gathers with backward computations explicitly. for i, compute in enumerate(backward_computes): # Prefetch reduce-scatter operation for the next layer if possible if i + 1 < len(reduce_scatters): reduce_scatters[i + 1]() # Perform backward computation for the current layer compute() def calculate_communication_buffers(num_params: int, dtype_bytes: int, num_gpus: int) -> Tuple[int, int]: Args: num_params (int): Total number of parameters in the transformer block. dtype_bytes (int): Size of the data type in bytes (e.g., 4 bytes for fp32). num_gpus (int): Number of GPUs across which the parameters are sharded. Returns: Tuple[int, int]: The calculated sizes of the communication buffers required for forward and backward passes. total_bytes = num_params * dtype_bytes forward_buffer_size = total_bytes // num_gpus backward_buffer_size = total_bytes // num_gpus return forward_buffer_size, backward_buffer_size"},{"question":"Title: Process and Summarize Multiple Log Files Objective: Write a Python program that leverages the `fileinput` module to read and process multiple log files. The program must count the occurrences of each unique IP address in the logs and output the summary in descending order of occurrences. Problem Description: You are provided with several log files, where each line in the log files contains an IP address. Using the `fileinput` module, implement a function that reads from these log files, counts the number of occurrences of each IP address, and returns a sorted list of tuples. Each tuple should contain an IP address and its count, sorted in descending order of the count. Function Signature: ```python def summarize_ip_addresses(files: list, encoding: str = \'utf-8\') -> list: Args: - files: list of filenames to read and process (list of strings) - encoding: encoding of the files (string, default is \'utf-8\') Returns: - list of tuples, each containing an IP address (string) and its count (integer), sorted in descending order of counts ``` Input: - `files` (list of strings): A list containing the names of the log files to be processed. - `encoding` (string): The encoding used to read the files. Default is \'utf-8\'. Output: - Returns a list of tuples, where each tuple contains an IP address (string) and the count of its occurrences (integer). The list is sorted in descending order of the counts. Constraints: - You may assume that the filenames provided are correct and accessible. - The log files contain one IP address per line and may be large. - The IP addresses are valid IPv4 addresses. - Handle any potential I/O errors gracefully. Example: ```python files = [\'log1.txt\', \'log2.txt\', \'log3.txt\'] result = summarize_ip_addresses(files, encoding=\'utf-8\') # Example output: [(\'192.168.1.1\', 10), (\'10.0.0.1\', 5), ...] ``` Requirements: - Use the `fileinput` module to read and process the files. - Utilize context management to ensure proper handling of the file inputs. - Handle the specified encoding properly. - Consider performance implications for large files.","solution":"import fileinput from collections import Counter def summarize_ip_addresses(files: list, encoding: str = \'utf-8\') -> list: Reads multiple log files, counts the occurrences of each unique IP address, and returns the summary in descending order of occurrences. Args: - files: list of filenames to read and process (list of strings) - encoding: encoding of the files (string, default is \'utf-8\') Returns: - list of tuples, each containing an IP address (string) and its count (integer), sorted in descending order of counts ip_counter = Counter() with fileinput.input(files=files, mode=\'r\', encoding=encoding) as f: for line in f: ip_address = line.strip() if ip_address: ip_counter[ip_address] += 1 # Sort the counts in descending order sorted_ip_counts = ip_counter.most_common() return sorted_ip_counts"},{"question":"# Python Coding Assessment Question Objective: Implement a Python class that leverages the `python310` package to perform various arithmetic and bitwise operations. This class should use the provided `PyNumber_*` functions to handle these operations. The class should ensure that all operations handle exceptions gracefully and return fallback values when exceptions occur. Requirements: 1. **Class Name:** `NumberOperations` 2. **Methods to Implement:** - `add(self, o1, o2)` - `subtract(self, o1, o2)` - `multiply(self, o1, o2)` - `divide(self, o1, o2)` - `modulus(self, o1, o2)` - `pow(self, o1, o2, o3=None)` - `bitwise_and(self, o1, o2)` - `bitwise_or(self, o1, o2)` - `bitwise_xor(self, o1, o2)` 3. **Method Descriptions:** - **`add(self, o1, o2)`**: Return the addition result of `o1` and `o2`. - **`subtract(self, o1, o2)`**: Return the subtraction result of `o1` from `o2`. - **`multiply(self, o1, o2)`**: Return the multiplication result of `o1` and `o2`. - **`divide(self, o1, o2)`**: Return the division result of `o1` by `o2`. - **`modulus(self, o1, o2)`**: Return the modulus result of `o1` by `o2`. - **`pow(self, o1, o2, o3=None)`**: Return `o1` raised to `o2`, optionally modulo `o3`. - **`bitwise_and(self, o1, o2)`**: Return the bitwise AND operation result of `o1` and `o2`. - **`bitwise_or(self, o1, o2)`**: Return the bitwise OR operation result of `o1` and `o2`. - **`bitwise_xor(self, o1, o2)`**: Return the bitwise XOR operation result of `o1` and `o2`. 4. **Constraints:** - Inputs `o1` and `o2` can be integers or objects providing numeric interfaces. - Implementations should handle exceptions and return `None` in case of any error. 5. **Example Usage:** ```python # Example Usage: op = NumberOperations() print(op.add(5, 3)) # Output: 8 print(op.subtract(10, 7)) # Output: 3 print(op.multiply(4, 9)) # Output: 36 print(op.divide(20, 4)) # Output: 5.0 print(op.modulus(10, 3)) # Output: 1 print(op.pow(2, 3)) # Output: 8 print(op.bitwise_and(6, 3))# Output: 2 print(op.bitwise_or(5, 2)) # Output: 7 print(op.bitwise_xor(7, 3))# Output: 4 ``` Additional Notes: - Your implementation should import the necessary functionalities from the `python310` package. - Each method should carefully handle `NULL` returns, ensuring exceptions do not cause the program to crash. Performance Consideration: - Ensure that the operations complete efficiently even for a large number of operations sequentially. Your task is to create the `NumberOperations` class according to the specifications above.","solution":"class NumberOperations: def add(self, o1, o2): try: return o1 + o2 except: return None def subtract(self, o1, o2): try: return o1 - o2 except: return None def multiply(self, o1, o2): try: return o1 * o2 except: return None def divide(self, o1, o2): try: return o1 / o2 except: return None def modulus(self, o1, o2): try: return o1 % o2 except: return None def pow(self, o1, o2, o3=None): try: if o3 is not None: return pow(o1, o2, o3) return pow(o1, o2) except: return None def bitwise_and(self, o1, o2): try: return o1 & o2 except: return None def bitwise_or(self, o1, o2): try: return o1 | o2 except: return None def bitwise_xor(self, o1, o2): try: return o1 ^ o2 except: return None"},{"question":"Coding Assessment Question # Context The \\"concurrent.futures\\" module in Python allows for the execution of code in parallel using threads or processes. As a developer, you might need to perform multiple IO-bound or CPU-bound operations simultaneously to improve the performance of your application. # Task You are to write a Python function `parallel_square` that uses the `concurrent.futures` module to concurrently compute the squares of a list of integers. The function must use `ThreadPoolExecutor` for concurrency. # Function Definition ```python def parallel_square(numbers: List[int]) -> List[int]: pass ``` # Input - `numbers`: A list of integers `[n1, n2, n3, ... , nn]`. # Output - A list of integers `[n1^2, n2^2, n3^2, ... , nn^2]`, where each element is the square of each corresponding input number. # Constraints - The input list `numbers` will have at least 1 and at most 1,000,000 integers. - Each integer in the list will be between -10^6 and 10^6. # Performance Requirements - Implement the solution using `ThreadPoolExecutor` to ensure the computations are done in parallel, hence improving the performance for large lists. # Example ```python numbers = [1, 2, 3, 4, 5] result = parallel_square(numbers) print(result) # Output: [1, 4, 9, 16, 25] ``` # Notes - Ensure to handle any potential exceptions within the parallel execution. - Use appropriate concurrency mechanisms to deal with the list operations. - Avoid common pitfalls like unnecessary locking or manually managing threads.","solution":"from concurrent.futures import ThreadPoolExecutor from typing import List def square(n: int) -> int: return n * n def parallel_square(numbers: List[int]) -> List[int]: with ThreadPoolExecutor() as executor: results = list(executor.map(square, numbers)) return results"},{"question":"# Question: Advanced Garbage Collection Management The Garbage Collector (GC) in Python is instrumental in managing memory and handling reference cycles. In this exercise, you will demonstrate your understanding of the `gc` module by writing a function that manages and analyzes garbage collection behavior. # Task: Write a Python function `analyze_garbage_collection(n)` that performs the following: 1. **Disables the garbage collector** to avoid automatic collection. 2. **Generates** `n` lists, each referencing the next one (creating a reference cycle). 3. Enables the garbage collector. 4. Sets the debugging flag to **DEBUG_SAVEALL**. 5. Manually triggers garbage collection and collects objects from all generations. 6. Analyzes the collected garbage and returns a dictionary with the following information: - The number of collectable objects. - The number of uncollectable objects. - The total number of objects before and after collection. - Whether the collection reclaimed any objects. # Function Signature: ```python def analyze_garbage_collection(n: int) -> dict: ``` # Input: - `n` (int): The number of lists to generate, creating a reference cycle. # Output: - A dictionary with the following keys: - `collectable_count`: The number of collectable objects found. - `uncollectable_count`: The number of uncollectable objects in `gc.garbage`. - `before_count`: Number of objects tracked by the garbage collector before collection. - `after_count`: Number of objects tracked by the garbage collector after collection. - `reclaimed`: Boolean indicating if any objects were reclaimed (i.e., `before_count` > `after_count`). # Constraints: - The function should handle cases where `n` is at least 1. - Ensure that the lists do form a reference cycle. - Optimize for performance where reasonable given the constraints. # Example: ```python result = analyze_garbage_collection(5) print(result) ``` **Expected Output:** Output should be something similar to: ```python { \\"collectable_count\\": X, \\"uncollectable_count\\": Y, \\"before_count\\": Z, \\"after_count\\": W, \\"reclaimed\\": True/False } ``` Where `X`, `Y`, `Z`, and `W` are integers denoting the respective counts of objects, and `reclaimed` is a Boolean indicating if any objects were reclaimed during the collection process. # Notes: - Make use of `gc.get_count()`, `gc.collect()`, and `gc.garbage` to analyze and track the objects. - Make sure to properly interpret and use the `gc` module functions.","solution":"import gc def analyze_garbage_collection(n): Analyzes the behavior of garbage collection by creating n lists with reference cycles, and then collects and reports garbage collection statistics. Parameters: n (int): The number of lists to generate, creating a reference cycle. Returns: dict: A dictionary containing statistics about garbage collection. # Step 1: Disable the garbage collector gc.disable() # Step 2: Generate n lists, each referencing the next one (creating a reference cycle) head = [] current = head for i in range(n): new_list = [] current.append(new_list) current = new_list current.append(head) # Closing the loop to create a reference cycle # Step 3: Enable the garbage collector gc.enable() # Step 4: Set debugging flag DEBUG_SAVEALL gc.set_debug(gc.DEBUG_SAVEALL) # Step 5: Manually trigger garbage collection and collect objects from all generations before_count = sum(gc.get_count()) collected = gc.collect() uncollectable_count = len(gc.garbage) after_count = sum(gc.get_count()) # Step 6: Analyze the collected garbage reclamation_status = before_count > after_count # Return the dictionary with the information return { \\"collectable_count\\": collected, \\"uncollectable_count\\": uncollectable_count, \\"before_count\\": before_count, \\"after_count\\": after_count, \\"reclaimed\\": reclamation_status, }"},{"question":"Objective: Write a Python program that demonstrates the use of the `multiprocessing.shared_memory` and `ShareableList` classes to share a list of integers between multiple processes. Specifically, the program should create a shared list, spawn multiple processes to modify different segments of this list, and ensure proper synchronization and cleanup of shared resources. Problem Statement: You need to implement a program that performs the following steps: 1. Create a `ShareableList` containing the first 20 natural numbers. 2. Create two worker functions: - The first worker function should double the values of the first 10 elements in the `ShareableList`. - The second worker function should increment the values of the last 10 elements in the `ShareableList` by 5. 3. Spawn two processes using the `multiprocessing` module to run these worker functions concurrently. 4. Wait for both processes to complete. 5. Print the final state of the `ShareableList`. 6. Ensure proper cleanup of the shared memory. Input and Output: - **Input**: No input required. - **Output**: A list showing the final state of the `ShareableList` after modifications by both processes. Constraints: - You must use the `multiprocessing.shared_memory` and `ShareableList` classes. - Proper cleanup (`close` and `unlink`) of shared memory resources is necessary. Example: The expected output after running the script should be: ``` [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24] ``` Code Template: ```python from multiprocessing import Process from multiprocessing.shared_memory import SharedMemory, ShareableList def worker_double(shared_list): # Double the values of the first 10 elements for i in range(10): shared_list[i] *= 2 def worker_increment(shared_list): # Increment the values of the last 10 elements by 5 for i in range(10, 20): shared_list[i] += 5 if __name__ == \\"__main__\\": # Step 1: Create a ShareableList containing the first 20 natural numbers shared_list = ShareableList(range(20)) # Step 2: Create two worker functions (already implemented) # Step 3: Spawn two processes p1 = Process(target=worker_double, args=(shared_list,)) p2 = Process(target=worker_increment, args=(shared_list,)) p1.start() p2.start() # Step 4: Wait for both processes to complete p1.join() p2.join() # Step 5: Print the final state of the ShareableList print(list(shared_list)) # Step 6: Ensure proper cleanup of shared memory shared_list.shm.close() shared_list.shm.unlink() ``` Implement the above template to complete the task. Ensure to test the final script to verify the correctness of the implementation.","solution":"from multiprocessing import Process from multiprocessing.shared_memory import SharedMemory, ShareableList def worker_double(shared_list): # Double the values of the first 10 elements for i in range(10): shared_list[i] *= 2 def worker_increment(shared_list): # Increment the values of the last 10 elements by 5 for i in range(10, 20): shared_list[i] += 5 if __name__ == \\"__main__\\": # Step 1: Create a ShareableList containing the first 20 natural numbers shared_list = ShareableList(range(20)) # Step 2: Create two worker functions (already implemented) # Step 3: Spawn two processes p1 = Process(target=worker_double, args=(shared_list,)) p2 = Process(target=worker_increment, args=(shared_list,)) p1.start() p2.start() # Step 4: Wait for both processes to complete p1.join() p2.join() # Step 5: Print the final state of the ShareableList print(list(shared_list)) # Step 6: Ensure proper cleanup of shared memory shared_list.shm.close() shared_list.shm.unlink()"},{"question":"**Objective:** You are tasked with performing data analysis using seaborn. Your goal is to create and interpret various residual plots to examine the assumptions of linear regression. You will work with the `mpg` dataset available in seaborn. **Dataset:** The `mpg` dataset contains the following fields: - `mpg`: Miles per gallon. - `cylinders`: Number of cylinders. - `displacement`: Engine displacement. - `horsepower`: Engine horsepower. - `weight`: Vehicle weight. - `acceleration`: Time to accelerate from 0 to 60 mph. - `model_year`: Model year. - `origin`: Origin of the car (1: USA, 2: Europe, 3: Japan). - `name`: Name of the car. **Instructions:** 1. **Load the dataset:** Write a function `load_dataset()` to load the `mpg` dataset using seaborn\'s `load_dataset` function. ```python def load_dataset(): import seaborn as sns return sns.load_dataset(\\"mpg\\") ``` 2. **Basic Residual Plot:** Write a function `basic_residual_plot(data)` to create and display a residual plot of `displacement` vs `weight`. ```python def basic_residual_plot(data): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() sns.residplot(data=data, x=\\"weight\\", y=\\"displacement\\") plt.show() ``` 3. **Higher-Order Trend Residual Plot:** Write a function `higher_order_residual_plot(data)` to create and display a second-order residual plot of `mpg` vs `horsepower`. ```python def higher_order_residual_plot(data): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() sns.residplot(data=data, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.show() ``` 4. **LOWESS Curve Residual Plot:** Write a function `lowess_residual_plot(data)` to create and display a residual plot of `mpg` vs `horsepower` with a LOWESS curve. ```python def lowess_residual_plot(data): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() sns.residplot(data=data, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.show() ``` 5. **Interpretation:** After plotting the residual plots, provide a brief interpretation (2-3 sentences) for each plot. Specifically, mention any patterns, trends, or violations of assumptions that you observe in the residuals. **Required Function Signatures:** ```python def load_dataset(): pass def basic_residual_plot(data): pass def higher_order_residual_plot(data): pass def lowess_residual_plot(data): pass ``` **Constraints:** - Use seaborn version 0.11.0 or later. - Use matplotlib for displaying plots. - Ensure that all plots are displayed clearly with appropriate labels and titles. **Note:** This question requires you to both implement the functions and interpret the results based on the plots generated.","solution":"def load_dataset(): import seaborn as sns return sns.load_dataset(\\"mpg\\") def basic_residual_plot(data): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() sns.residplot(data=data, x=\\"weight\\", y=\\"displacement\\") plt.show() def higher_order_residual_plot(data): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() sns.residplot(data=data, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.show() def lowess_residual_plot(data): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() sns.residplot(data=data, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.show()"},{"question":"# **WSGI Todo Application** You are tasked with implementing a simple WSGI-based Todo application using the `wsgiref` package\'s utilities and classes. Your application will allow users to: 1. **Add a Todo item** by sending a POST request with a todo item in the request body. 2. **View all Todo items** by sending a GET request. Specifications: 1. **Function Signature:** - The WSGI application should be implemented in a function called `todo_app` that takes two arguments: `environ` and `start_response`. 2. **Functionality:** - If the request method is `POST`, read the body content and append it to a global list of todo items. - If the request method is `GET`, return a list of all todo items in plain text, each on a new line. 3. **Constraints:** - Use `wsgiref.util.setup_testing_defaults` to set up default request parameters. - Use `wsgiref.headers.Headers` to correctly manage response headers. - The application should respond with `400 Bad Request` for any request method other than `GET` or `POST`. - Use `wsgiref.simple_server.simple_server` to test your application locally. 4. **Example:** - A GET request to your application should return a plain text list of all todo items, one per line. - A POST request with the body \\"Buy groceries\\" should add \\"Buy groceries\\" to the list of todo items. Deliverables: 1. **Implement the `todo_app` function.** 2. **Set up and run a WSGI server** using `wsgiref.simple_server.make_server` to handle requests using the `todo_app` WSGI application. **Hint:** You can test your implementation using a browser or tools like `curl` to send GET and POST requests. ```python from wsgiref.simple_server import make_server from wsgiref.util import setup_testing_defaults from wsgiref.headers import Headers import io # In-memory storage for todo items TODO_ITEMS = [] def todo_app(environ, start_response): setup_testing_defaults(environ) headers = Headers([]) method = environ[\'REQUEST_METHOD\'] if method == \'POST\': try: length = int(environ.get(\'CONTENT_LENGTH\', 0)) except (ValueError): length = 0 body = environ[\'wsgi.input\'].read(length).decode(\'utf-8\') TODO_ITEMS.append(body) status = \'200 OK\' headers.add_header(\'Content-Type\', \'text/plain\') start_response(status, headers.items()) return [b\'Todo item added\'] elif method == \'GET\': status = \'200 OK\' headers.add_header(\'Content-Type\', \'text/plain\') start_response(status, headers.items()) response_body = \'n\'.join(TODO_ITEMS) return [response_body.encode(\'utf-8\')] else: status = \'400 Bad Request\' headers.add_header(\'Content-Type\', \'text/plain\') start_response(status, headers.items()) return [b\'Invalid request method\'] # Run the WSGI server if __name__ == \'__main__\': with make_server(\'\', 8000, todo_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` In this example, the `todo_app` stores todo items in an in-memory list and has logic to handle `GET` and `POST` requests, using the `wsgiref` utilities to set up the environment and headers. The server is then run using `make_server`.","solution":"from wsgiref.simple_server import make_server from wsgiref.util import setup_testing_defaults from wsgiref.headers import Headers # In-memory storage for todo items TODO_ITEMS = [] def todo_app(environ, start_response): setup_testing_defaults(environ) headers = Headers([]) method = environ[\'REQUEST_METHOD\'] if method == \'POST\': try: length = int(environ.get(\'CONTENT_LENGTH\', 0)) except (ValueError): length = 0 body = environ[\'wsgi.input\'].read(length).decode(\'utf-8\') TODO_ITEMS.append(body) status = \'200 OK\' headers.add_header(\'Content-Type\', \'text/plain\') start_response(status, headers.items()) return [b\'Todo item added\'] elif method == \'GET\': status = \'200 OK\' headers.add_header(\'Content-Type\', \'text/plain\') start_response(status, headers.items()) response_body = \'n\'.join(TODO_ITEMS) return [response_body.encode(\'utf-8\')] else: status = \'400 Bad Request\' headers.add_header(\'Content-Type\', \'text/plain\') start_response(status, headers.items()) return [b\'Invalid request method\'] # This code should not be executed in the unit test environment. Uncomment for standalone server run: # if __name__ == \'__main__\': # with make_server(\'\', 8000, todo_app) as httpd: # print(\\"Serving on port 8000...\\") # httpd.serve_forever()"},{"question":"Instrumenting CPython with SystemTap # Objective To test your understanding of using SystemTap to monitor Python processes, you\'ll need to create a SystemTap script that records and reports specific events occurring during the execution of a Python script. # Task Write a SystemTap script `function_hierarchy.stp` to trace and report the function call and return hierarchy for a given Python script. 1. **Monitoring Targets**: - Record all function entries and exits within the Python script. - Ensure the monitoring script only records pure-Python (bytecode) functions. 2. **Script Requirements**: - Use the `process(\\"python\\").mark` probe points to capture function entry and return events. - Implement the script to display the following details upon each function entry or return: - Timestamp. - Function name. - Filename. - Line number. - Event type (function entry or return). 3. **Running the Script**: - The script should be able to be invoked as follows (assuming the target script is `example.py`): ```bash stap function_hierarchy.stp -c \\"python3 example.py\\" ``` # Constraints - The tracings should be human-readable and follow the format: ``` [timestamp] function-entry in filename:funcname:lineno [timestamp] function-return in filename:funcname:lineno ``` - Ensure proper indentation to illustrate the call depth hierarchy. - You should not modify the provided Python script in any way. # Example Output Given the Python script: ```python def function_1(): function_3() def function_2(): function_1() def function_3(): pass def function_start(): function_1() function_2() function_start() ``` Your script should produce output similar to: ``` [timestamp] function-entry in example.py:function_start:13 [timestamp] function-entry in example.py:function_1:1 [timestamp] function-entry in example.py:function_3:9 [timestamp] function-return in example.py:function_3:10 [timestamp] function-return in example.py:function_1:2 [timestamp] function-entry in example.py:function_2:5 [timestamp] function-entry in example.py:function_1:1 [timestamp] function-entry in example.py:function_3:9 [timestamp] function-return in example.py:function_3:10 [timestamp] function-return in example.py:function_1:2 [timestamp] function-return in example.py:function_2:6 [timestamp] function-return in example.py:function_start:28 ``` # Submission Submit your `function_hierarchy.stp` script file, ensuring it meets all the above specifications and constraints.","solution":"def function_hierarchy(): Returns a function that reports function call and return events in Python scripts. return \'\'\' global indent = 0 probe process(\\"python3\\").mark(\\"function-entry\\") { indent_str = \\" \\".repeat(indent) printf(\\"[%s] %sfunction-entry in %s:%s:%dn\\", ctime(gettimeofday_s()), indent_str, user_string(filename), user_string(funcname), lineno) indent += 2 } probe process(\\"python3\\").mark(\\"function-return\\") { indent -= 2 indent_str = \\" \\".repeat(indent) printf(\\"[%s] %sfunction-return in %s:%s:%dn\\", ctime(gettimeofday_s()), indent_str, user_string(filename), user_string(funcname), lineno) } \'\'\'"},{"question":"# Question: Implementing a Custom Mapping Class You are required to implement a custom class `CustomMapping` which behaves similarly to a Python dictionary. Specifically, your class should support all fundamental mapping operations such as adding, deleting, retrieving items, and other typical dictionary operations stated below. Furthermore, ensure this class complies with the mapping operations described in the provided documentation. Requirements: 1. **Initialization**: Initialize with an optional dictionary passed to the constructor. 2. **Item Insertion**: Support setting items using the `key-value` syntax (`obj[key] = value`). 3. **Item Retrieval**: Support item retrieval using keys (`value = obj[key]`). 4. **Item Deletion**: Support deletion of items using keys (`del obj[key]`). 5. **Size Retrieval**: Implement a method `__len__` to get the number of key-value pairs. 6. **Membership Test**: Support checking for keys using the `in` keyword (`key in obj`). 7. **Key Retrieval**: Implement a method to get the list of keys. 8. **Value Retrieval**: Implement a method to get the list of values. 9. **Item Retrieval**: Implement a method to get the list of key-value pairs. Constraints: - The keys are guaranteed to be strings. - The values can be any valid Python objects. Performance Requirements: - Implement methods efficiently to ensure that operations like insertion, deletion, and lookup are average O(1). Example: ```python class CustomMapping: def __init__(self, init_dict=None): pass def __setitem__(self, key, value): pass def __getitem__(self, key): pass def __delitem__(self, key): pass def __len__(self): pass def __contains__(self, key): pass def keys(self): pass def values(self): pass def items(self): pass # Example Usage cm = CustomMapping({\'a\': 1, \'b\': 2}) cm[\'c\'] = 3 print(cm[\'a\']) # Output: 1 del cm[\'b\'] print(len(cm)) # Output: 2 print(\'c\' in cm) # Output: True print(cm.keys()) # Output: [\'a\', \'c\'] print(cm.values()) # Output: [1, 3] print(cm.items()) # Output: [(\'a\', 1), (\'c\', 3)] ``` Implement the class `CustomMapping` to ensure it behaves as indicated.","solution":"class CustomMapping: def __init__(self, init_dict=None): self._data = init_dict if init_dict is not None else {} def __setitem__(self, key, value): self._data[key] = value def __getitem__(self, key): return self._data[key] def __delitem__(self, key): del self._data[key] def __len__(self): return len(self._data) def __contains__(self, key): return key in self._data def keys(self): return list(self._data.keys()) def values(self): return list(self._data.values()) def items(self): return list(self._data.items())"},{"question":"# Multiprocessing with PyTorch: Implementing a Distributed Computation You are tasked with implementing a distributed computation simulation using PyTorch\'s `torch.distributed.elastic.multiprocessing` package. You will demonstrate your understanding of parallel processing by setting up and launching multiple worker processes that carry out independent tasks and then aggregating their results. Specifically, you will create a system where multiple subprocesses each compute the sum of an array of numbers and the main process collects these results. Requirements 1. **Function:** `distributed_sum_worker(numbers: List[int]) -> int` - **Input:** A list of integers `numbers`. - **Output:** The sum of the integers in the list. 2. **Function:** `main_distributed_sum(workers: int, data: List[List[int]]) -> int` - **Input:** - `workers`: Number of worker processes to spawn. - `data`: A list of lists, where each sublist contains integers to be summed by a worker process. - **Output:** The total sum of all integers in the `data` list of lists, computed by the worker processes and aggregated by the main process. - **Constraints:** - The number of sublists in `data` should be equal to `workers`. 3. The multiprocessing setup must utilize `torch.distributed.elastic.multiprocessing.start_processes`. You must define the necessary setup to efficiently distribute the work and gather the results. Example Suppose you have 3 workers and the data `[[1, 2, 3], [4, 5, 6], [7, 8, 9]]`. Each worker will compute the sum of one sublist: - Worker 1 computes: `1 + 2 + 3 = 6` - Worker 2 computes: `4 + 5 + 6 = 15` - Worker 3 computes: `7 + 8 + 9 = 24` The `main_distributed_sum` should then return the total sum: `6 + 15 + 24 = 45`. Performance Ensure the solution handles multiple processes efficiently and correctly aggregates the results without significant overhead. Implementation Skeleton ```python from typing import List import torch.distributed.elastic.multiprocessing as multiprocessing def distributed_sum_worker(numbers: List[int]) -> int: # Your code here to calculate the sum of the list `numbers` pass def main_distributed_sum(workers: int, data: List[List[int]]) -> int: # Your multiprocessing setup and aggregation logic here pass ``` Note: Handle the necessary setup and teardown of multiprocessing contexts appropriately.","solution":"from typing import List import torch.multiprocessing as mp def distributed_sum_worker(numbers: List[int], result_queue: mp.Queue) -> None: Computes the sum of a list of numbers and puts the result into a queue. Args: numbers (List[int]): List of integers to sum up. result_queue (mp.Queue): Queue where the result will be put. result = sum(numbers) result_queue.put(result) def main_distributed_sum(workers: int, data: List[List[int]]) -> int: Spawns worker processes to sum sublists of numbers and aggregates the results. Args: workers (int): Number of worker processes to spawn. data (List[List[int]]): A list of lists, where each sublist contains integers to be summed by a worker. Returns: int: The total sum of all integers in the sublists. if workers != len(data): raise ValueError(\\"The number of workers must match the number of sublists in data.\\") result_queue = mp.Queue() processes = [] for i in range(workers): process = mp.Process(target=distributed_sum_worker, args=(data[i], result_queue)) processes.append(process) process.start() total_sum = 0 for process in processes: process.join() while not result_queue.empty(): total_sum += result_queue.get() return total_sum"},{"question":"# Asyncio Task Management and Debugging **Objective:** The aim of this assessment is to evaluate your understanding of asynchronous programming using Python\'s `asyncio` library. You will be required to write code that involves creating coroutines, managing tasks, and ensuring proper handling of exceptions. **Task:** 1. Create an asynchronous function `fetch_data` that performs a mock I/O operation by sleeping asynchronously for a specified number of seconds and then returns a message indicating the duration. 2. Write another asynchronous function `process_data` that: - Calls `fetch_data` with a duration of 2 seconds. - Raises an exception intentionally with a message \\"Processing Error\\" after calling `fetch_data`. 3. Implement an asynchronous `main` function that: - Creates a task for `process_data` using `asyncio.create_task`. - Uses exception handling to catch any exceptions raised by `process_data`. - Await the completion of the `process_data` task. 4. Enable and configure asyncio debug mode to: - Check for coroutines that were not awaited. - Log callbacks taking longer than 100ms. - Log when the execution time of the I/O selector takes too long. 5. Write code to simulate running the main event loop and ensure no blocking operations occur. **Requirements:** - Use proper exception handling to ensure any errors in `process_data` are caught and logged. - Enable asyncio\'s debug mode and configure appropriate logging. - Run the coroutines and tasks in a way that does not block the event loop. **Input and Output:** - `fetch_data` function should take an integer argument `seconds` and return a message string after the specified delay. - `process_data` should show how it handles the raised exception and should propagate this appropriately. - The `main` function should output relevant debug and log messages. **Constraints:** - Your solution should handle asynchronous operations efficiently and demonstrate proper use of asyncio\'s debug mode. - Ensure that the I/O operation is non-blocking and proper concurrency is maintained. **Example Code Execution:** ```python import asyncio import logging # Sample Code Structure to Start With async def fetch_data(seconds): # Implement the function here pass async def process_data(): # Implement the function here pass async def main(): # Implement the function here pass if __name__ == \\"__main__\\": # Configure and run asyncio loop pass ``` Your task is to fill in the missing parts according to the requirements and ensure everything runs correctly. **Assessment Criteria:** - Correct implementation of asynchronous functions and tasks. - Proper handling and logging of exceptions within coroutines. - Efficient use of asyncio\'s debug mode and logging configuration. - Ensuring non-blocking execution of tasks.","solution":"import asyncio import logging async def fetch_data(seconds): Simulates a mock I/O operation by sleeping asynchronously for the specified number of seconds and then returns a message indicating the duration. await asyncio.sleep(seconds) return f\\"Fetched data in {seconds} seconds\\" async def process_data(): Calls fetch_data with a duration of 2 seconds and then raises an intentional exception. result = await fetch_data(2) raise Exception(\\"Processing Error\\") async def main(): Creates a task for process_data, uses exception handling to catch any exceptions, and awaits the completion of the process_data task. logging.basicConfig(level=logging.DEBUG) asyncio.get_running_loop().set_debug(True) try: task = asyncio.create_task(process_data()) await task except Exception as e: logging.exception(\\"An error occurred while processing data\\") if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Question: Implement a Custom Numeric Operations Class You are asked to implement a class `CustomNumber` that mimics certain numeric operations provided in the Python C API as detailed in the documentation. The class should handle typical numeric operations and ensure proper error handling on invalid operations, similar to how the C API returns NULL on failure. Requirements: 1. Create a class `CustomNumber` with an attribute `value` which stores the numeric value. 2. Implement the following methods: - `add(self, other)`: Returns a new `CustomNumber` representing the sum of `self.value` and `other.value`. - `subtract(self, other)`: Returns a new `CustomNumber` representing the difference when `other.value` is subtracted from `self.value`. - `multiply(self, other)`: Returns a new `CustomNumber` representing the product of `self.value` and `other.value`. - `floor_divide(self, other)`: Returns a new `CustomNumber` representing the floor division of `self.value` by `other.value`. - `true_divide(self, other)`: Returns a new `CustomNumber` representing the true division of `self.value` by `other.value`. - `remainder(self, other)`: Returns a new `CustomNumber` representing the remainder when `self.value` is divided by `other.value`. - `negate(self)`: Returns a new `CustomNumber` representing the negation of `self.value`. - `absolute(self)`: Returns a new `CustomNumber` representing the absolute value of `self.value`. Input and Output Formats: - `add`, `subtract`, `multiply`, `floor_divide`, `true_divide`, and `remainder` methods: - Input: Another `CustomNumber` object. - Output: A `CustomNumber` object containing the result of the operation. - Raise a `TypeError` if the input is not a `CustomNumber`. - `negate` and `absolute` methods: - Input: None (operate on `self`). - Output: A `CustomNumber` object containing the result of the operation. Constraints and Limitations: - `self.value` and `other.value` must be integers or floating-point numbers. - Raise a `TypeError` if `self.value` or `other.value` is not a numeric type. - Methods `floor_divide`, `true_divide`, and `remainder` should handle division by zero by raising a `ZeroDivisionError`. ```python class CustomNumber: def __init__(self, value): if not isinstance(value, (int, float)): raise TypeError(\\"Value must be an integer or float\\") self.value = value def add(self, other): if not isinstance(other, CustomNumber): raise TypeError(\\"Other must be a CustomNumber\\") return CustomNumber(self.value + other.value) def subtract(self, other): if not isinstance(other, CustomNumber): raise TypeError(\\"Other must be a CustomNumber\\") return CustomNumber(self.value - other.value) def multiply(self, other): if not isinstance(other, CustomNumber): raise TypeError(\\"Other must be a CustomNumber\\") return CustomNumber(self.value * other.value) def floor_divide(self, other): if not isinstance(other, CustomNumber): raise TypeError(\\"Other must be a CustomNumber\\") if other.value == 0: raise ZeroDivisionError(\\"Division by zero\\") return CustomNumber(self.value // other.value) def true_divide(self, other): if not isinstance(other, CustomNumber): raise TypeError(\\"Other must be a CustomNumber\\") if other.value == 0: raise ZeroDivisionError(\\"Division by zero\\") return CustomNumber(self.value / other.value) def remainder(self, other): if not isinstance(other, CustomNumber): raise TypeError(\\"Other must be a CustomNumber\\") if other.value == 0: raise ZeroDivisionError(\\"Division by zero\\") return CustomNumber(self.value % other.value) def negate(self): return CustomNumber(-self.value) def absolute(self): return CustomNumber(abs(self.value)) ``` You should implement additional test cases to ensure your methods handle edge cases and inappropriate inputs correctly.","solution":"class CustomNumber: def __init__(self, value): if not isinstance(value, (int, float)): raise TypeError(\\"Value must be an integer or float\\") self.value = value def add(self, other): if not isinstance(other, CustomNumber): raise TypeError(\\"Other must be a CustomNumber\\") return CustomNumber(self.value + other.value) def subtract(self, other): if not isinstance(other, CustomNumber): raise TypeError(\\"Other must be a CustomNumber\\") return CustomNumber(self.value - other.value) def multiply(self, other): if not isinstance(other, CustomNumber): raise TypeError(\\"Other must be a CustomNumber\\") return CustomNumber(self.value * other.value) def floor_divide(self, other): if not isinstance(other, CustomNumber): raise TypeError(\\"Other must be a CustomNumber\\") if other.value == 0: raise ZeroDivisionError(\\"Division by zero\\") return CustomNumber(self.value // other.value) def true_divide(self, other): if not isinstance(other, CustomNumber): raise TypeError(\\"Other must be a CustomNumber\\") if other.value == 0: raise ZeroDivisionError(\\"Division by zero\\") return CustomNumber(self.value / other.value) def remainder(self, other): if not isinstance(other, CustomNumber): raise TypeError(\\"Other must be a CustomNumber\\") if other.value == 0: raise ZeroDivisionError(\\"Division by zero\\") return CustomNumber(self.value % other.value) def negate(self): return CustomNumber(-self.value) def absolute(self): return CustomNumber(abs(self.value))"},{"question":"Objective: Write a Python script using the `ossaudiodev` module to play a series of audio samples. The script should: 1. Open the audio device for writing. 2. Set the audio parameters (format, number of channels, and sampling rate). 3. Write a small sequence of audio data to the device. 4. Handle any errors gracefully by displaying meaningful error messages. Requirements: 1. Use `AFMT_U8` as the audio format, mono channel (single channel), and a sampling rate of 8000 Hz. 2. The audio data to write is provided as a bytes-like object. 3. Use error handling to manage `OSSAudioError` and `OSError`. 4. Ensure the audio device is closed properly after the operation. Input: 1. A bytes-like object representing the audio data. Output: 1. The script should not have any explicit output but should print error messages if encountered. Constraints: 1. Assume the audio device file is `/dev/dsp`. 2. The audio data is provided and is small enough to be written in one `write()` call. 3. Handle the operation within a try-except block to manage possible errors. Example Usage: ```python audio_data = b\'x00x01x02x03x04\' # Example audio data play_audio(audio_data) ``` Function Signature: ```python def play_audio(data: bytes) -> None: pass ```","solution":"import ossaudiodev def play_audio(data: bytes) -> None: try: dsp = ossaudiodev.open(\'w\') dsp.setfmt(ossaudiodev.AFMT_U8) dsp.channels(1) # Mono dsp.speed(8000) # 8000 Hz dsp.write(data) except ossaudiodev.OSSAudioError as e: print(f\\"OSSAudioError: {e}\\") except OSError as e: print(f\\"OSError: {e}\\") finally: try: dsp.close() except NameError: pass"},{"question":"**Coding Assessment Question** **Title**: Optimize a Matrix Factorization Algorithm **Objective**: The goal of this assessment is to evaluate your ability to optimize Python code using Numpy\'s vectorized operations and, optionally, multi-processing. You will profile a given implementation and then apply optimizations to improve its performance. **Problem Statement**: A simple implementation of Non-negative Matrix Factorization (NMF) is provided below. Your task is to: 1. **Profile the code** to identify performance bottlenecks. 2. **Optimize the identified bottlenecks** using Numpy to avoid Python loops. 3. Optionally, apply **multi-processing using `joblib.Parallel`** to further enhance performance. **Provided Code**: ```python import numpy as np from sklearn.datasets import load_digits def nmf_simple(V, W, H, tol=1e-2, max_iter=200): Simple NMF implementation with Python loops. for n_iter in range(max_iter): H_old = H.copy() for i in range(H.shape[0]): for j in range(H.shape[1]): H[i, j] = H[i, j] * (np.dot(W.T, V)[i, j] / np.dot(W.T, np.dot(W, H))[i, j]) if np.linalg.norm(H - H_old) < tol: break return H if __name__ == \\"__main__\\": X, _ = load_digits(return_X_y=True) n_components = 16 W_init = np.abs(np.random.randn(X.shape[0], n_components)) H_init = np.abs(np.random.randn(n_components, X.shape[1])) V = X W = W_init H = H_init optimized_H = nmf_simple(V, W, H) print(f\\"Resulting Matrix H (shape: {optimized_H.shape})\\") # Note: Please ensure you have scikit-learn and joblib installed: # pip install scikit-learn joblib ``` **Tasks**: 1. **Profile the provided code** using the appropriate IPython magic commands or any other profiling tools mentioned in the documentation (e.g., `line_profiler`, `memory_profiler`). 2. **Optimize the bottleneck** in the function `nmf_simple` to use Numpy\'s vectorized operations instead of Python loops. 3. **Optional**: Utilize `joblib.Parallel` to add multi-processing for further optimization. **Submission Requirements**: - Provide the **profile analysis** output showing the identified bottleneck. - Submit the **optimized code** for the `nmf_simple` function. - Document **any additional multi-processing optimizations** implemented using `joblib.Parallel`. **Constraints**: - Aim for significant performance improvements. - Ensure the optimized implementation produces the same result as the original implementation. **Notes**: - Use the `digits` dataset from `sklearn.datasets` as provided in the code for testing and performance benchmarking. - Focus on clarity and maintainability in your code while achieving performance optimization. **Expected Input and Output**: - Input: Matrix `V` (digits dataset), initial matrices `W` and `H`. - Output: Optimized matrix `H`. **Performance Requirements**: - The optimized implementation should reduce the total execution time compared to the provided implementation. - Use profiling to demonstrate the performance gain. Good luck!","solution":"import numpy as np from sklearn.datasets import load_digits def nmf_optimized(V, W, H, tol=1e-2, max_iter=200): Optimized NMF implementation using Numpy vectorized operations. for n_iter in range(max_iter): H_old = H.copy() # Vectorize update for H numerator = np.dot(W.T, V) denominator = np.dot(W.T, np.dot(W, H)) + 1e-9 # Adding small value to avoid division by zero H *= numerator / denominator if np.linalg.norm(H - H_old) < tol: break return H if __name__ == \\"__main__\\": X, _ = load_digits(return_X_y=True) n_components = 16 W_init = np.abs(np.random.randn(X.shape[0], n_components)) H_init = np.abs(np.random.randn(n_components, X.shape[1])) V = X W = W_init H = H_init optimized_H = nmf_optimized(V, W, H) print(f\\"Resulting Matrix H (shape: {optimized_H.shape})\\")"},{"question":"**Title: Multivariate Linear Regression and Regularization Techniques in scikit-learn** **Objective:** Demonstrate your understanding of linear regression models and regularization techniques in scikit-learn by analyzing a given dataset, addressing multicollinearity, and comparing the performance of different models. **Task:** You are provided with a dataset containing information about various houses, including features like square footage, number of rooms, age, etc., along with the target variable \'price\'. Your task is to: 1. Implement different linear regression models using scikit-learn. 2. Address multicollinearity. 3. Compare the performance of these models using appropriate metrics. 4. Provide a brief report on your findings. **Dataset:** Download the dataset [here](https://example-dataset.com/house_prices.csv). **Steps:** 1. **Load and Explore the Data:** - Load the dataset using pandas. - Perform initial exploratory data analysis (EDA) to understand the data structure and summary statistics. - Identify and handle any missing values if present. 2. **Implement Linear Models:** - Implement Ordinary Least Squares (OLS) using `LinearRegression`. - Implement Ridge regression using `Ridge`. - Implement Lasso regression using `Lasso`. 3. **Address Multicollinearity:** - Calculate the correlation matrix to identify multicollinearity among features. - Apply Principal Component Analysis (PCA) to transform the features if multicollinearity is high. 4. **Model Evaluation:** - Split the data into training and testing sets. - Fit each model to the training data. - Evaluate the models on the testing set using metrics such as Mean Squared Error (MSE) and R-squared (R²). - Plot the actual vs. predicted prices for each model. 5. **Compare Model Performance:** - Print and compare the MSE and R² values for each model. - Discuss the impact of regularization techniques (Ridge and Lasso) on the performance compared to OLS. 6. **Report:** - Provide a brief report on your findings, including any insights drawn from the EDA, the handling of multicollinearity, and the performance comparison across models. **Expected Input and Output:** - **Input:** A CSV file containing the dataset with features and target variable \'price\'. - **Output:** A Jupyter notebook or a Python script containing: - EDA steps and findings. - Implemented linear models and their evaluations. - Plots showing actual vs. predicted prices. - A conclusion section summarizing the results and insights. **Constraints:** - Use scikit-learn for all model implementations. - Ensure reproducibility of results by setting random states where applicable. - Comment your code to explain the steps taken. **Performance Requirements:** - The code should be efficient and run within reasonable time for a dataset with 10,000 rows and 20 features. - The report should be clear, concise, and provide meaningful insights from the analysis. ```python import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.metrics import mean_squared_error, r2_score import matplotlib.pyplot as plt # Load dataset # df = pd.read_csv(\'house_prices.csv\') # Example implementation skeleton def load_and_preprocess_data(filepath): df = pd.read_csv(filepath) # Handle missing values df = df.dropna() return df def train_linear_models(df): X = df.drop(columns=[\'price\']) y = df[\'price\'] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Apply Standard Scaler scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Apply PCA if multicollinearity is high pca = PCA(n_components=min(X_train.shape[0], X_train.shape[1])) X_train_pca = pca.fit_transform(X_train) X_test_pca = pca.transform(X_test) # Train and evaluate Linear Regression ols = LinearRegression() ols.fit(X_train, y_train) y_pred_ols = ols.predict(X_test) ridge = Ridge(alpha=1.0) ridge.fit(X_train, y_train) y_pred_ridge = ridge.predict(X_test) lasso = Lasso(alpha=0.1) lasso.fit(X_train, y_train) y_pred_lasso = lasso.predict(X_test) # Calculate metrics metrics = { \'OLS\': (mean_squared_error(y_test, y_pred_ols), r2_score(y_test, y_pred_ols)), \'Ridge\': (mean_squared_error(y_test, y_pred_ridge), r2_score(y_test, y_pred_ridge)), \'Lasso\': (mean_squared_error(y_test, y_pred_lasso), r2_score(y_test, y_pred_lasso)) } return metrics def plot_results(y_test, y_pred, model_name): plt.figure(figsize=(10, 6)) plt.scatter(y_test, y_pred, alpha=0.3) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \'--r\', linewidth=2) plt.xlabel(\'Actual\') plt.ylabel(\'Predicted\') plt.title(f\'Actual vs Predicted Prices: {model_name}\') plt.show() # Example usage # df = load_and_preprocess_data(\'house_prices.csv\') # metrics = train_linear_models(df) # plot_results(y_test, y_pred_ols, \'OLS\') # plot_results(y_test, y_pred_ridge, \'Ridge\') # plot_results(y_test, y_pred_lasso, \'Lasso\') ```","solution":"import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.metrics import mean_squared_error, r2_score import matplotlib.pyplot as plt def load_and_preprocess_data(filepath): Load and preprocess the data. Args: filepath (str): Path to the CSV file. Returns: pd.DataFrame: Preprocessed DataFrame. # Load dataset df = pd.read_csv(filepath) # Handle missing values by dropping rows with any missing values df = df.dropna() return df def calculate_correlation_matrix(df): Calculate correlation matrix to identify multicollinearity. Args: df (pd.DataFrame): Input DataFrame. Returns: pd.DataFrame: Correlation matrix. return df.corr() def apply_pca(X, n_components): Apply Principal Component Analysis (PCA) to address multicollinearity. Args: X (pd.DataFrame or np.array): Feature matrix. n_components (int): Number of principal components. Returns: np.array: Transformed features. pca = PCA(n_components=n_components) return pca.fit_transform(X) def train_linear_models(df): Train and evaluate different linear regression models. Args: df (pd.DataFrame): Input DataFrame. Returns: dict: Performance metrics for each model. X = df.drop(columns=[\'price\']) y = df[\'price\'] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Apply PCA X_train_pca = apply_pca(X_train, n_components=min(X_train.shape[0], X_train.shape[1])) X_test_pca = apply_pca(X_test, n_components=min(X_test.shape[0], X_test.shape[1])) results = {} # Train and evaluate Linear Regression ols = LinearRegression() ols.fit(X_train_pca, y_train) y_pred_ols = ols.predict(X_test_pca) results[\'OLS\'] = (mean_squared_error(y_test, y_pred_ols), r2_score(y_test, y_pred_ols)) # Train and evaluate Ridge Regression ridge = Ridge(alpha=1.0) ridge.fit(X_train_pca, y_train) y_pred_ridge = ridge.predict(X_test_pca) results[\'Ridge\'] = (mean_squared_error(y_test, y_pred_ridge), r2_score(y_test, y_pred_ridge)) # Train and evaluate Lasso Regression lasso = Lasso(alpha=0.1) lasso.fit(X_train_pca, y_train) y_pred_lasso = lasso.predict(X_test_pca) results[\'Lasso\'] = (mean_squared_error(y_test, y_pred_lasso), r2_score(y_test, y_pred_lasso)) return results, y_test, y_pred_ols, y_pred_ridge, y_pred_lasso def plot_results(y_test, y_pred, model_name): Plot the actual vs predicted prices. Args: y_test (np.array): True prices. y_pred (np.array): Predicted prices. model_name (str): Name of the model. plt.figure(figsize=(10, 6)) plt.scatter(y_test, y_pred, alpha=0.3) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \'--r\', linewidth=2) plt.xlabel(\'Actual Prices\') plt.ylabel(\'Predicted Prices\') plt.title(f\'Actual vs Predicted Prices: {model_name}\') plt.show()"},{"question":"**Networking and Interprocess Communication — Asynchronous Web Server** **Question:** You are to implement a simple asynchronous web server using the `asyncio` and `socket` modules in Python. This server will accept incoming HTTP GET requests and respond with a simple HTML message. Ensure that the server can handle multiple simultaneous connections by leveraging the asynchronous capabilities of `asyncio`. # Specifications 1. **Function Definition**: - Implement a function `run_server(host: str, port: int) -> None:` 2. **Functionality**: - The server should listen on the specified `host` and `port`. - It should handle multiple client connections concurrently using `asyncio`. - For every incoming HTTP GET request, it should respond with an HTTP 200 status code and a body containing a simple HTML message, e.g., `<html><body><h1>Hello, World!</h1></body></html>`. 3. **Constraints**: - The server should only handle HTTP GET requests. Any other type of request (e.g., POST, PUT) should respond with HTTP 405 Method Not Allowed. - The server should correctly manage asynchronous operations and avoid blocking calls. 4. **Input**: - `host`: A string specifying the hostname or IP address the server will bind to, e.g., `\'127.0.0.1\'`. - `port`: An integer specifying the port number the server will listen on, e.g., `8080`. 5. **Output**: - The function doesn\'t return anything. It should run indefinitely until interrupted from outside (e.g., Ctrl+C in the terminal). # Example Usage ```python if __name__ == \\"__main__\\": run_server(\'127.0.0.1\', 8080) ``` # Additional Notes - Assume the `asyncio` and `socket` modules are already imported in your script. - Ensure your server can handle at least 10 simultaneous connections efficiently. - Properly manage the lifecycle of client connections to avoid resource leaks. Good luck and happy coding!","solution":"import asyncio import socket async def handle_client(client_socket): request = await asyncio.get_event_loop().sock_recv(client_socket, 1024) request_line = request.decode().splitlines()[0] method, path, _ = request_line.split() if method != \'GET\': response = \'HTTP/1.1 405 Method Not Allowedrnrn\' else: response = (\'HTTP/1.1 200 OKrn\' \'Content-Type: text/htmlrnrn\' \'<html><body><h1>Hello, World!</h1></body></html>\') await asyncio.get_event_loop().sock_sendall(client_socket, response.encode()) client_socket.close() async def run_server(host, port): server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_sock.setblocking(False) server_sock.bind((host, port)) server_sock.listen(5) loop = asyncio.get_event_loop() while True: client_socket, _ = await loop.sock_accept(server_sock) loop.create_task(handle_client(client_socket)) # Example usage: # if __name__ == \\"__main__\\": # asyncio.run(run_server(\'127.0.0.1\', 8080))"},{"question":"# File System Analyzer using the `stat` Module Problem Statement You are required to implement a function that performs recursive directory traversal and collects statistics about the types of files encountered. Using the `stat` module, you need to determine the count of each type of file (such as directories, regular files, symbolic links, etc.) within a given directory and its subdirectories. Function Signature ```python def analyze_file_system(directory_path: str) -> dict: Analyzes the given directory recursively to count the types of files present. Parameters: directory_path (str): The path to the directory to be analyzed. Returns: dict: A dictionary containing counts of each file type. The keys should be: - \'directories\' - \'regular_files\' - \'symbolic_links\' - \'character_devices\' - \'block_devices\' - \'fifos\' - \'sockets\' - \'doors\' (if the system supports it) - \'event_ports\' (if the system supports it) - \'whiteouts\' (if the system supports it) Each key maps to an integer representing the count of that file type. ``` Input - `directory_path` (string): The path to the directory that needs to be analyzed. Output - A dictionary containing counts of each file type defined in the keys mentioned above. Constraints - The function should handle directory structures that may contain nested directories. - The solution should be able to handle cases where the same type of file might be counted multiple times in different locations within the directory structure. - The system might not support certain file types (e.g., \'doors\', \'event_ports\', \'whiteouts\'); your code should handle this gracefully without errors. Example ```python # Example directory structure # /example_dir # ├── dir1 # │ ├── file1.txt # │ ├── file2.txt # │ └── link_to_file -> /example_dir/dir2/file3.txt # └── dir2 # ├── file3.txt # └── file4.txt # Calling the function on this structure: result = analyze_file_system(\'/example_dir\') print(result) # Output might be: # { # \'directories\': 2, # \'regular_files\': 4, # \'symbolic_links\': 1, # \'character_devices\': 0, # \'block_devices\': 0, # \'fifos\': 0, # \'sockets\': 0, # \'doors\': 0, # \'event_ports\': 0, # \'whiteouts\': 0 # } ``` # Additional Information - Use the `os.listdir()`, `os.lstat()`, and path manipulation tools from the `os` and `os.path` modules to traverse directories and analyze the file types. - Utilize the `stat` module functions such as `stat.S_ISDIR()`, `stat.S_ISREG()`, etc., to determine the file types. - Ensure your function handles exceptions gracefully, such as permission errors when accessing certain directories or files. # Hints - Consider defining helper functions for readability, for instance, a function to update the file type counts. - Start with a base count dictionary initialized with zeros for each key before recursively traversing the directory. This problem tests your understanding of: 1. File handling and directory traversal. 2. Application of the `stat` module to determine file types. 3. Recursive function implementation. 4. Error handling in file system operations.","solution":"import os import stat def analyze_file_system(directory_path: str) -> dict: file_type_counts = { \'directories\': 0, \'regular_files\': 0, \'symbolic_links\': 0, \'character_devices\': 0, \'block_devices\': 0, \'fifos\': 0, \'sockets\': 0, \'doors\': 0, \'event_ports\': 0, \'whiteouts\': 0 } def analyze_path(path): try: st = os.lstat(path) if stat.S_ISDIR(st.st_mode): file_type_counts[\'directories\'] += 1 for entry in os.listdir(path): analyze_path(os.path.join(path, entry)) elif stat.S_ISREG(st.st_mode): file_type_counts[\'regular_files\'] += 1 elif stat.S_ISLNK(st.st_mode): file_type_counts[\'symbolic_links\'] += 1 elif stat.S_ISCHR(st.st_mode): file_type_counts[\'character_devices\'] += 1 elif stat.S_ISBLK(st.st_mode): file_type_counts[\'block_devices\'] += 1 elif stat.S_ISFIFO(st.st_mode): file_type_counts[\'fifos\'] += 1 elif stat.S_ISSOCK(st.st_mode): file_type_counts[\'sockets\'] += 1 except FileNotFoundError: pass # Skip files that no longer exist except PermissionError: pass # Skip files/dirs without permission analyze_path(directory_path) return file_type_counts"},{"question":"Objective Implement a complete workflow to train a Multi-layer Perceptron (MLP) model for classification using scikit-learn, including data preprocessing, model training, hyperparameter tuning, and evaluation. Task 1. **Data Preprocessing:** - Load the provided dataset (for example, the iris dataset from scikit-learn). - Split the data into training and testing sets. - Standardize the features (mean=0 and variance=1). 2. **Model Training:** - Implement an MLPClassifier with a single hidden layer of size 10. - Use the \'adam\' solver and set `random_state=42` for reproducibility. - Train the model with the training data. 3. **Hyperparameter Tuning:** - Tune the `alpha` (regularization parameter) using a grid search. The values to consider are ([10^{-3}, 10^{-4}, 10^{-5}]). - Use cross-validation to determine the optimal regularization parameter. 4. **Evaluation:** - Evaluate the accuracy of the model on the test data. - Print the best `alpha` value and the corresponding accuracy. 5. **Implementation Details:** - Your implementation should follow good coding practices, including appropriate function decompositions and docstrings for each function. Example Input and Output **Input:** A dataset such as the iris dataset from scikit-learn. **Output:** - Best `alpha` from grid search. - Test accuracy of the model. Constraints - You must use `MLPClassifier` from scikit-learn. - You must use `StandardScaler` from scikit-learn for feature scaling. - Use `train_test_split` from scikit-learn for splitting the data. - Use `GridSearchCV` from scikit-learn for hyperparameter tuning. Code Template ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score def load_and_split_data(): data = load_iris() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def preprocess_data(X_train, X_test): scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) return X_train_scaled, X_test_scaled def train_mlp(X_train, y_train): mlp = MLPClassifier(hidden_layer_sizes=(10,), solver=\'adam\', random_state=42) mlp.fit(X_train, y_train) return mlp def tune_hyperparameters(X_train, y_train): param_grid = {\'alpha\': [1e-3, 1e-4, 1e-5]} grid_search = GridSearchCV(MLPClassifier(hidden_layer_sizes=(10,), solver=\'adam\', random_state=42), param_grid, cv=5) grid_search.fit(X_train, y_train) return grid_search.best_params_, grid_search.best_estimator_ def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy if __name__ == \\"__main__\\": X_train, X_test, y_train, y_test = load_and_split_data() X_train_scaled, X_test_scaled = preprocess_data(X_train, X_test) _, best_model = tune_hyperparameters(X_train_scaled, y_train) accuracy = evaluate_model(best_model, X_test_scaled, y_test) print(\\"Best alpha:\\", best_model.alpha) print(\\"Test accuracy:\\", accuracy) ```","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score def load_and_split_data(): Load the iris dataset and split it into training and testing sets. Returns: tuple: X_train, X_test, y_train, y_test data = load_iris() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def preprocess_data(X_train, X_test): Standardize the features (zero mean and unit variance). Parameters: X_train (np.ndarray): Training set features. X_test (np.ndarray): Testing set features. Returns: tuple: X_train_scaled, X_test_scaled scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) return X_train_scaled, X_test_scaled def tune_hyperparameters(X_train, y_train): Tune the hyperparameters of the MLPClassifier. Parameters: X_train (np.ndarray): Training set features. y_train (np.ndarray): Training set labels. Returns: tuple: Best parameters, best estimator param_grid = {\'alpha\': [1e-3, 1e-4, 1e-5]} grid_search = GridSearchCV(MLPClassifier(hidden_layer_sizes=(10,), solver=\'adam\', random_state=42), param_grid, cv=5) grid_search.fit(X_train, y_train) return grid_search.best_params_, grid_search.best_estimator_ def evaluate_model(model, X_test, y_test): Evaluate the model with the test data. Parameters: model: Trained MLP model. X_test (np.ndarray): Testing set features. y_test (np.ndarray): Testing set labels. Returns: float: Accuracy of the model on the test set. y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy if __name__ == \\"__main__\\": X_train, X_test, y_train, y_test = load_and_split_data() X_train_scaled, X_test_scaled = preprocess_data(X_train, X_test) best_params, best_model = tune_hyperparameters(X_train_scaled, y_train) accuracy = evaluate_model(best_model, X_test_scaled, y_test) print(\\"Best alpha:\\", best_params[\'alpha\']) print(\\"Test accuracy:\\", accuracy)"},{"question":"# PyTorch Logging Configuration PyTorch provides a configurable logging system where different components can have different log level settings. You are required to write a Python function that configures the logging levels of specific components and artifacts using both the environment variable and the provided Python API. Function Signature ```python def configure_pytorch_logging(env_log_settings: str, api_log_settings: dict) -> dict: pass ``` Inputs 1. `env_log_settings` (str): A comma-separated string to set log levels and artifacts using the environment variable format. 2. `api_log_settings` (dict): A dictionary where keys are component names and values are the desired log levels using the Python API. Outputs - `result_dict` (dict): A dictionary reflecting the final log levels of the components and the states of the artifacts after applying both configurations. Requirements 1. The function should first apply the configurations using the environment variable format. 2. It should then apply the configurations using the Python API to override any settings defined by the environment variable. 3. Assume the log levels follow standard Python logging levels such as `logging.DEBUG`, `logging.INFO`, `logging.WARN`, `logging.ERROR`. Constraints - Ensure to use the PyTorch `torch._logging.set_logs` function for the API settings. - Ignore any component or artifact not specified in the inputs. Example Usage ```python import torch._logging as logger env_log_settings = \\"+dynamo,aot_graphs\\" api_log_settings = { \\"dynamo\\": \\"logging.INFO\\", \\"some.random.module\\": \\"logging.DEBUG\\" } result = configure_pytorch_logging(env_log_settings, api_log_settings) print(result) # Expected output: # { # \\"dynamo\\": \\"logging.INFO\\", # \\"aot\\": \\"logging.WARN\\", # \\"inductor\\": \\"logging.WARN\\", # \\"some.random.module\\": \\"logging.DEBUG\\", # \\"bytecode\\": False, # \\"aot_graphs\\": True, # ... # } ``` Notes - This example combines settings from both the environment and the API, with the API taking precedence where conflicts arise. - Components and artifacts not mentioned in the inputs should retain their default log levels and states. Good luck!","solution":"import logging def configure_pytorch_logging(env_log_settings: str, api_log_settings: dict) -> dict: Configures PyTorch logging settings using both environment variable string and API settings. Args: - env_log_settings (str): Comma-separated string for setting log levels and states. - api_log_settings (dict): Dictionary with component names as keys and log levels as values. Returns: - result_dict (dict): Dictionary reflecting the final log levels and states of components. # Mock function to represent PyhToch\'s logging mechanism class MockLogging: _log_levels = {} _artifacts_state = {} @classmethod def set_logs(cls, settings: dict): for key, value in settings.items(): if value in (True, False): cls._artifacts_state[key] = value else: cls._log_levels[key] = value @classmethod def get_final_settings(cls): result = {key: value for key, value in cls._log_levels.items()} result.update(cls._artifacts_state) return result if env_log_settings: log_parts = env_log_settings.split(\',\') env_settings = {} for part in log_parts: part = part.strip() if part.startswith(\'+\'): env_settings[part[1:]] = True elif part.startswith(\'-\'): env_settings[part[1:]] = False else: component, level = part.split(\'=\') env_settings[component.strip()] = getattr(logging, level.strip().upper()) MockLogging.set_logs(env_settings) if api_log_settings: api_settings = {} for key, value in api_log_settings.items(): api_settings[key] = getattr(logging, value.strip().upper()) MockLogging.set_logs(api_settings) return MockLogging.get_final_settings()"},{"question":"# Question: Implementing and Optimizing a TorchScript Model You are provided with a PyTorch-based neural network model intended for image classification tasks. Your goal is to optimize this model using TorchScript, ensuring that it can be serialized and executed efficiently in a non-Python environment. # Instructions: Part 1: Define the Model 1. Define a PyTorch neural network model called `ImageClassifier` with the following architecture: - A convolutional layer `conv1` with 3 input channels, 32 output channels, and a kernel size of 3. - A ReLU activation function. - A max-pooling layer with a kernel size of 2. - A fully connected layer `fc` that takes input from the convolutional layer (after flattening) and outputs 10 classes (assuming CIFAR-10 dataset). Part 2: Script and Serialize the Model 2. Script the model using TorchScript’s `torch.jit.script`. 3. Serialize the scripted model to a file named `image_classifier.pt`. Part 3: Load and Optimize the Model 4. Load the serialized model from the file. 5. Optimize the loaded model for inference using TorchScript\'s optimization techniques. # Constraints: - Assume the input to the model is a tensor of size `[1, 3, 32, 32]` representing a batch of one 32x32 RGB image. - You must handle any attribute or module type annotations correctly to ensure smooth scripting. # Expected Output: - A PyTorch model definition. - Scripting and serialization code. - Inference optimization code after loading the model. - Example usage of the optimized model for a forward pass. # Example Code Structure: ```python import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define the ImageClassifier model class ImageClassifier(nn.Module): def __init__(self): super(ImageClassifier, self).__init__() self.conv1 = nn.Conv2d(3, 32, kernel_size=3) self.pool = nn.MaxPool2d(kernel_size=2) self.fc = nn.Linear(32 * 15 * 15, 10) # Assuming input size is (3, 32, 32) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = x.view(-1, self.num_flat_features(x)) x = self.fc(x) return x def num_flat_features(self, x): size = x.size()[1:] num_features = 1 for s in size: num_features *= s return num_features # Script the model model = ImageClassifier() scripted_model = torch.jit.script(model) # Serialize the scripted model torch.jit.save(scripted_model, \'image_classifier.pt\') # Load the serialized model loaded_model = torch.jit.load(\'image_classifier.pt\') # Optimize the loaded model for inference optimized_model = torch.jit.optimize_for_inference(loaded_model) # Example usage of the optimized model input_tensor = torch.rand(1, 3, 32, 32) # Example input tensor output = optimized_model(input_tensor) print(output) ``` # Notes: - Ensure you handle any potential issues that may arise due to the discrepancies between standard PyTorch and TorchScript, especially concerning data-dependent control flows and tensor operations. Good luck!","solution":"import torch import torch.nn as nn import torch.nn.functional as F # Define the ImageClassifier model class ImageClassifier(nn.Module): def __init__(self): super(ImageClassifier, self).__init__() self.conv1 = nn.Conv2d(3, 32, kernel_size=3) self.pool = nn.MaxPool2d(kernel_size=2) self.fc = nn.Linear(32 * 15 * 15, 10) # Assuming input size is (3, 32, 32) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = x.view(-1, self.num_flat_features(x)) x = self.fc(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features # Instantiate the model model = ImageClassifier() # Script the model scripted_model = torch.jit.script(model) # Serialize the scripted model torch.jit.save(scripted_model, \'image_classifier.pt\') # Load the serialized model loaded_model = torch.jit.load(\'image_classifier.pt\') # Optimize the loaded model for inference optimized_model = torch.jit.optimize_for_inference(loaded_model) # Example usage of the optimized model input_tensor = torch.rand(1, 3, 32, 32) # Example input tensor output = optimized_model(input_tensor) print(output)"},{"question":"# Coding Assessment: Custom Syslog Logger Objective Implement a Python program utilizing the `syslog` module to create a custom syslog logger. The logger should be capable of logging messages with varying priorities and facilities, handling initialization and closure of logging sessions, and managing priority masks. Task 1. **Initialization**: - Implement a function `initialize_logger(ident, logoption, facility)` that initializes the syslog with the given options. 2. **Logging messages**: - Implement a function `log_message(priority, message)` that logs a message with a specified priority. If the log is not already open, `initialize_logger` should be called with default parameters. 3. **Set priority mask**: - Implement a function `set_priority_mask(maskpri)` that sets the priority mask. Messages below the priority level set by the mask should be ignored. 4. **Closing the log**: - Implement a function `close_logger()` that closes the current log session. Constraints - **Ident**: A string that is prepended to each log message. - **Logoption**: An integer representing the log options (e.g., syslog.LOG_PID). - **Facility**: A constant representing the facility (e.g., syslog.LOG_MAIL). - **Priority**: One of the provided priority level constants (e.g., syslog.LOG_ERR). - **Maskpri**: A mask priority calculated from priority levels using `syslog.LOG_MASK` or `syslog.LOG_UPTO`. Example Usage ```python import syslog # Define functions def initialize_logger(ident, logoption, facility): syslog.openlog(ident, logoption, facility) def log_message(priority, message): syslog.syslog(priority, message) def set_priority_mask(maskpri): syslog.setlogmask(maskpri) def close_logger(): syslog.closelog() # Sample usage initialize_logger(\\"customIdent\\", syslog.LOG_PID, syslog.LOG_MAIL) log_message(syslog.LOG_INFO, \\"Informational message\\") log_message(syslog.LOG_ERR, \\"Error message encountered\\") set_priority_mask(syslog.LOG_MASK(syslog.LOG_ERR)) log_message(syslog.LOG_INFO, \\"This message should be ignored due to mask setting\\") log_message(syslog.LOG_CRIT, \\"Critical message\\") close_logger() ``` Expected Output The exact output depends on the syslog system, but it should log the informational and error messages appropriately, mask out the second informational message, and log the critical message. Performance Requirements - Ensure that the syslog is opened with appropriate options before logging messages. - Accurately recognize and apply priority masks to filter out messages.","solution":"import syslog def initialize_logger(ident, logoption, facility): Initializes the syslog with given identifier, log option, and facility. syslog.openlog(ident, logoption, facility) def log_message(priority, message): Logs a message with the specified priority. Calls `initialize_logger` with default parameters if the log is not already open. syslog.syslog(priority, message) def set_priority_mask(maskpri): Sets the syslog priority mask. syslog.setlogmask(maskpri) def close_logger(): Closes the current syslog session. syslog.closelog()"},{"question":"**Objective:** Write a Python program using the `ossaudiodev` module that records audio from the microphone and then plays it back through the speakers. Your task is to demonstrate the following functionalities: 1. Open the audio device for reading (recording) and writing (playback). 2. Set the appropriate audio parameters (format, channels, sample rate) for recording and playback. 3. Record audio data from the microphone and save it to a byte buffer. 4. Playback the recorded audio data through the speakers. 5. Handle any possible errors appropriately using `ossaudiodev.OSSAudioError` and `OSError`. **Requirements:** - Open the audio device for both reading and writing (`mode=\\"rw\\"`). - Set the audio format to `AFMT_S16_LE` (signed, 16-bit little-endian). - Set the number of channels to `1` (mono). - Set the sample rate to `44100` (CD quality). - Record audio for a duration of 5 seconds. - Ensure the audio is played back immediately after recording. - Close the audio device properly when finished. **Constraints:** - The recording and playback should be done without using any external libraries other than `ossaudiodev`. - Handle the recording and playback in a blocking manner for simplicity. **Input and Output:** - **Input**: No external input required. - **Output**: No external output required, just playback of recorded audio. **Hints:** - Use `ossaudiodev.open()` to open the audio device. - Use `setparameters()` to set the audio parameters in one call. - Use `read()` and `write()` methods for recording and playback, respectively. - Use `time.sleep()` to wait for the recording duration. Here is a skeleton code to get you started: ```python import ossaudiodev import time def record_and_playback(): try: # Open the audio device for read and write dsp = ossaudiodev.open(\'rw\') # Set audio parameters fmt = ossaudiodev.AFMT_S16_LE nchannels = 1 sampling_rate = 44100 dsp.setparameters(fmt, nchannels, sampling_rate) # Record audio for 5 seconds print(\\"Recording...\\") duration = 5 # seconds buffer_size = sampling_rate * duration * 2 # 2 bytes per sample for S16_LE audio_data = dsp.read(buffer_size) # Read the audio data from the device # Play the recorded audio print(\\"Playing back...\\") dsp.writeall(audio_data) # Write the audio data to the device # Close the device dsp.close() print(\\"Done.\\") except (ossaudiodev.OSSAudioError, OSError) as e: print(f\\"An error occurred: {e}\\") # Run the function record_and_playback() ``` Ensure your code is clear, robust, and handles various scenarios gracefully.","solution":"import ossaudiodev import time def record_and_playback(): try: # Open the audio device for read and write dsp = ossaudiodev.open(\'rw\') # Set audio parameters fmt = ossaudiodev.AFMT_S16_LE nchannels = 1 sampling_rate = 44100 dsp.setparameters(fmt, nchannels, sampling_rate) # Record audio for 5 seconds print(\\"Recording...\\") duration = 5 # seconds buffer_size = sampling_rate * duration * 2 # 2 bytes per sample for S16_LE audio_data = dsp.read(buffer_size) # Read the audio data from the device # Play the recorded audio print(\\"Playing back...\\") dsp.writeall(audio_data) # Write the audio data to the device # Close the device dsp.close() print(\\"Done.\\") except (ossaudiodev.OSSAudioError, OSError) as e: print(f\\"An error occurred: {e}\\") # Run the function record_and_playback()"},{"question":"**Question:** # Understanding Copy-on-Write (CoW) in pandas With the introduction of Copy-on-Write (CoW) in pandas, the behavior of data mutations has changed to prevent accidental modifications of shared data views. This question aims to test your understanding of CoW and how it affects commonly used pandas operations. Instructions: 1. **Dataset Preparation**: - Create a DataFrame `df` with the following data: ```plaintext foo bar 1 4 2 5 3 6 ``` 2. **Subsetting and Mutation with CoW**: - Create a subset `subset` of the `foo` column from `df`. - Attempt to modify the first element of `subset` to `100`. - Verify that the original DataFrame `df` remains unchanged. 3. **Rewriting Chained Assignment**: - Write a chained assignment statement that attempts to set values in the `foo` column of `df` to `100` where the `bar` column values are greater than `5`. - Rewrite this statement using `loc` to make it CoW compliant. 4. **Handling Read-only NumPy Array**: - Convert a single-column DataFrame from the `foo` column of `df` to a NumPy array. - Make the NumPy array writeable and modify its first element to `100` without affecting the DataFrame. Requirements: - The original DataFrame should not be modified by operations on subsets or views. - The rewritten assignment using `loc` should correctly update the DataFrame without causing a CoW violation. - Demonstrate making a NumPy array writeable and modifying it correctly. Expected Output: - The DataFrame `df` should remain: ```plaintext foo bar 1 4 2 5 3 6 ``` after attempting the mutation on `subset`. - The rewritten chained assignment using `loc` should update the DataFrame as: ```plaintext foo bar 1 4 2 5 100 6 ``` - The modification of the writeable NumPy array should reflect the updated value without affecting the original DataFrame. Example Code: ```python import pandas as pd # 1. Dataset Preparation df = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) # 2. Subsetting and Mutation with CoW subset = df[\\"foo\\"] subset.iloc[0] = 100 print(df) # df should remain unchanged # 3. Rewriting Chained Assignment df = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) try: df[\\"foo\\"][df[\\"bar\\"] > 5] = 100 # This will raise a ChainedAssignmentError except Exception as e: print(e) # CoW compliant assignment using loc df.loc[df[\\"bar\\"] > 5, \\"foo\\"] = 100 print(df) # Properly updated DataFrame # 4. Handling Read-only NumPy Array df = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) arr = df[\\"foo\\"].to_numpy() arr.flags.writeable = True arr[0] = 100 print(arr) print(df) # df should remain unchanged ```","solution":"import pandas as pd # 1. Dataset Preparation df = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) # 2. Subsetting and Mutation with CoW subset = df[\\"foo\\"].copy() subset.iloc[0] = 100 # df should remain unchanged # 3. Rewriting Chained Assignment df_chain = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) # CoW compliant assignment using loc df_chain.loc[df_chain[\\"bar\\"] > 5, \\"foo\\"] = 100 # Properly updated DataFrame # 4. Handling Read-only NumPy Array df_numpy = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) arr = df_numpy[\\"foo\\"].to_numpy().copy() # Convert to a NumPy array and make a copy arr.flags.writeable = True arr[0] = 100 # df should remain unchanged"},{"question":"**Question:** **Working with Timedeltas in Pandas** You are given a DataFrame `df` containing two columns: `start_time` and `end_time`. These columns represent the start and end times of certain events, and both columns are in `datetime` format. Your task is to write a function `process_time_deltas(df)` that performs the following operations: 1. Calculate the duration of each event as a `Timedelta` and store it in a new column `duration`. 2. Filter out any rows where the `duration` is negative or where either `start_time` or `end_time` is missing (NaT). 3. Replace any `NaT` values in the `duration` column with the average duration of the remaining events. 4. Return the modified DataFrame. **Function Signature:** ```python def process_time_deltas(df: pd.DataFrame) -> pd.DataFrame: pass ``` **Input:** - `df`: A pandas DataFrame with two columns: `start_time` and `end_time`. - Both columns are of `datetime64[ns]` dtype. **Output:** - A pandas DataFrame with three columns: `start_time`, `end_time`, and `duration`. - The `duration` column should be of `timedelta64[ns]` dtype, with negative durations and `NaT` values handled as described. **Constraints:** - The `duration` values should be calculated as `end_time - start_time`. - If all rows are filtered out due to invalid durations, return an empty DataFrame with the same columns. **Example:** Given the following DataFrame `df`: | start_time | end_time | |---------------------|---------------------| | 2023-01-01 08:00:00 | 2023-01-01 12:00:00 | | 2023-01-02 09:00:00 | 2023-01-02 11:00:00 | | NaT | 2023-01-03 10:00:00 | | 2023-01-04 10:00:00 | NaT | | 2023-01-05 12:00:00 | 2023-01-05 10:00:00 | Your function should return: | start_time | end_time | duration | |---------------------|---------------------|---------------| | 2023-01-01 08:00:00 | 2023-01-01 12:00:00 | 0 days 04:00:00 | | 2023-01-02 09:00:00 | 2023-01-02 11:00:00 | 0 days 02:00:00 | Explanation: - The third and fourth rows are filtered out because they contain `NaT` values. - The fifth row is filtered out because it has a negative duration. - Since there are no `NaT` values in the `duration` column after filtering, no replacements are needed.","solution":"import pandas as pd def process_time_deltas(df: pd.DataFrame) -> pd.DataFrame: # Calculate the duration for each event df[\'duration\'] = df[\'end_time\'] - df[\'start_time\'] # Filter out rows where the duration is negative or either start_time or end_time is NaT df = df[df[\'duration\'].notna() & (df[\'duration\'] >= pd.Timedelta(0))] # If all rows are filtered out, return an empty DataFrame with the same structure if df.empty: return pd.DataFrame(columns=[\'start_time\', \'end_time\', \'duration\']) # Calculate the average duration of the remaining events avg_duration = df[\'duration\'].mean() # Replace any NaT values in the duration column with the average duration df[\'duration\'].fillna(avg_duration, inplace=True) return df"},{"question":"# Question: Secure Temporary File and Directory Management You are required to implement a Python script that performs the following tasks using the `tempfile` module. The goal is to demonstrate your understanding of creating and managing temporary files and directories securely. 1. Create a temporary directory using `TemporaryDirectory`. Inside this directory: - Create a temporary file using `NamedTemporaryFile` with some specific requirements (e.g., a custom prefix and suffix). Write any data into this file. - Ensure this temporary file is readable, writeable, but not executable. - Read the data from the temporary file and print it. - Clean up the file manually before the directory context ends. 2. After the above context, create another temporary file using the lower-level function `mkstemp`. Ensure: - The returned file descriptor is converted into a file object for writing and reading. - Write some different data into this file and read from it. - Ensure manual cleanup after usage. 3. Finally, demonstrate the usage of `gettempdir()` to fetch and print the default directory used for temporary files. **Function Specifications:** 1. `manage_temp_files_and_dirs() -> None` - **Inputs:** None - **Outputs:** None - **Constraints:** - You only need to print the outputs as specified while ensuring all resources are properly cleaned up. **Example Output:** ``` Data read from NamedTemporaryFile: <Written data> Data read from mkstemp file: <Different written data> Default temporary directory: <System-specific temp directory> ``` Implement this function and ensure all cleanup activities are properly handled to avoid any resource leaks or security issues. **Notes:** - Utilize the `with` statement when working with context managers for `TemporaryDirectory` and `NamedTemporaryFile`. - Ensure the use of appropriate file modes to maintain cross-platform consistency. - Handle exceptions gracefully to avoid potential issues during cleanup.","solution":"import tempfile import os def manage_temp_files_and_dirs(): # First task: Temporary directory and NamedTemporaryFile with tempfile.TemporaryDirectory() as tempdir: temp_file_prefix = \\"tempfile_\\" temp_file_suffix = \\".txt\\" temp_file_path = None with tempfile.NamedTemporaryFile(dir=tempdir, prefix=temp_file_prefix, suffix=temp_file_suffix, delete=False) as temp_file: temp_file.write(b\\"Hello from NamedTemporaryFilen\\") temp_file_path = temp_file.name # Ensure file permissions: readable and writable, but not executable os.chmod(temp_file_path, 0o600) # Read the data from the temporary file with open(temp_file_path, \'r\') as temp_file: data = temp_file.read() print(f\\"Data read from NamedTemporaryFile: {data.strip()}\\") # Manual cleanup of the file os.remove(temp_file_path) # Second task: mkstemp temp_fd, temp_abs_path = tempfile.mkstemp() try: with os.fdopen(temp_fd, \'w+\') as temp_file: temp_file.write(\\"Hello from mkstemp filen\\") temp_file.seek(0) data = temp_file.read() print(f\\"Data read from mkstemp file: {data.strip()}\\") finally: os.remove(temp_abs_path) # Third task: gettempdir default_temp_dir = tempfile.gettempdir() print(f\\"Default temporary directory: {default_temp_dir}\\")"},{"question":"Coding Assessment Question # Scheduling and Managing Events using `sched` Module You are required to implement a Python function that simulates a scheduling system for different tasks using the `sched` module. Your function will perform the following: 1. Schedule a series of tasks with varying priorities and delays. 2. Demonstrate handling an event cancellation. 3. Display the task execution order and times. # Function Signature: ```python def schedule_and_manage_tasks(tasks: List[Tuple[float, int, Callable, Tuple, Dict]], event_to_cancel: int) -> List[str]: pass ``` # Input: 1. `tasks` (List[Tuple[float, int, Callable, Tuple, Dict]]): A list of tasks where each task is represented as a tuple containing: - `delay` (float): The delay time before the task should be executed. - `priority` (int): Priority of the task (lower number represents higher priority). - `action` (Callable): The function to execute when the task runs. - `argument` (Tuple): Positional arguments for the `action` function. - `kwargs` (Dict): Keyword arguments for the `action` function. 2. `event_to_cancel` (int): The index of the event in the `tasks` list that should be cancelled before running the scheduler. # Output: - `List[str]`: A list of strings where each string represents the execution details of a task, formatted as \\"Task executed at {time}: {details}\\", where `{time}` is the execution time and `{details}` is a string describing the action and arguments. # Constraints: - No two tasks will have the same delay and priority combination. - The `event_to_cancel` will always be a valid index within the `tasks` list. # Example: ```python import time def print_task(name): print(f\\"Task {name} executed at {time.time()}\\") tasks = [ (5, 1, print_task, (\\"Task1\\",), {}), (10, 2, print_task, (\\"Task2\\",), {}), (3, 1, print_task, (\\"Task3\\",), {}), (8, 1, print_task, (\\"Task4\\",), {}) ] event_to_cancel = 1 # Expected output: # [ # \\"Task executed at 1652342833.0: Task3\\", # \\"Task executed at 1652342835.0: Task1\\", # \\"Task executed at 1652342838.0: Task4\\" # ] print(schedule_and_manage_tasks(tasks, event_to_cancel)) ``` # Steps: 1. Initialize a `scheduler` instance using `time.time` and `time.sleep`. 2. Schedule each task in the `tasks` list using the `scheduler.enter` method. 3. Cancel the specified event using its index from the `tasks` list. 4. Run the scheduled tasks and capture their execution times and details. 5. Return the list of strings representing the execution details of the tasks. _NOTE: Make sure to handle the removal of the event gracefully and ensure correct scheduling of remaining tasks._","solution":"import sched import time from typing import List, Tuple, Callable, Dict def schedule_and_manage_tasks(tasks: List[Tuple[float, int, Callable, Tuple, Dict]], event_to_cancel: int) -> List[str]: # Initialize the scheduler using time.time and time.sleep scheduler = sched.scheduler(time.time, time.sleep) # List to hold the scheduled events events = [] # Function to capture the execution of tasks executed_tasks = [] def task_wrapper(task_name): def task(*args, **kwargs): current_time = time.time() executed_tasks.append(f\\"Task {task_name} executed at {current_time}\\") # Call the actual function args[0](*args[1:], **kwargs) return task # Schedule the tasks for idx, (delay, priority, action, args, kwargs) in enumerate(tasks): event = scheduler.enter(delay, priority, task_wrapper(f\\"Task{idx+1}\\"), (action, *args), kwargs) events.append(event) # Cancel the specified event scheduler.cancel(events[event_to_cancel]) # Run the scheduled events scheduler.run() return executed_tasks"}]'),z={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},q={class:"card-container"},R={key:0,class:"empty-state"},F=["disabled"],O={key:0},N={key:1};function M(n,e,l,m,s,r){const h=_("PoemCard");return a(),i("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>s.searchQuery=o),placeholder:"Search..."},null,512),[[y,s.searchQuery]]),s.searchQuery?(a(),i("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>s.searchQuery="")}," ✕ ")):d("",!0)]),t("div",q,[(a(!0),i(b,null,v(r.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),i("div",R,' No results found for "'+c(s.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),i("button",{key:0,class:"load-more-button",disabled:s.isLoading,onClick:e[2]||(e[2]=(...o)=>r.loadMore&&r.loadMore(...o))},[s.isLoading?(a(),i("span",N,"Loading...")):(a(),i("span",O,"See more"))],8,F)):d("",!0)])}const L=p(z,[["render",M],["__scopeId","data-v-0010b7c2"]]),X=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"quotes/59.md","filePath":"quotes/59.md"}'),j={name:"quotes/59.md"},Y=Object.assign(j,{setup(n){return(e,l)=>(a(),i("div",null,[x(L)]))}});export{X as __pageData,Y as default};
