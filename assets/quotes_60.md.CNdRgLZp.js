import{_ as p,o as a,c as n,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(s,e,l,m,o,i){return a(),n("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-84e3487d"]]),I=JSON.parse('[{"question":"**Problem Statement:** The Python `time` module offers various functions that assist in manipulating and converting times. Your task is to write a utility that takes a list of strings representing date-times in different formats and returns a list of date-times converted into a standardized format. Additionally, your utility should include a function to measure the time taken to process this list. # Requirements: 1. **`convert_date_times(date_time_strings: List[str], date_format: str) -> List[str]`:** - **Input:** - `date_time_strings`: A list of strings where each string represents a date-time in a different format. - `date_format`: A string that specifies the desired output format for the date-times (e.g., `\\"%Y-%m-%d %H:%M:%S\\"`). - **Output:** - A list of strings where each string represents a date-time converted to the `date_format`. - **Constraints:** - The input date-times could be in various formats, and the function should be flexible enough to parse them correctly. - If a string cannot be parsed, it should be skipped (not included in the output list). 2. **`measure_conversion_time(date_time_strings: List[str], date_format: str) -> float`:** - **Input:** - `date_time_strings`: A list of strings where each string represents a date-time in a different format. - `date_format`: A string that specifies the desired output format for the date-times. - **Output:** - A floating-point number representing the time taken (in seconds) to convert all date-times using the `convert_date_times` function. - **Constraints:** - This function should utilize high-resolution timers provided by the `time` module to measure the time taken for the conversion. # Example Usage: ```python from typing import List import time def convert_date_times(date_time_strings: List[str], date_format: str) -> List[str]: converted_dates = [] for dt_str in date_time_strings: for fmt in [\\"%Y-%m-%d %H:%M:%S\\", \\"%d-%m-%Y %H:%M:%S\\", \\"%m/%d/%Y %I:%M %p\\"]: try: dt = time.strptime(dt_str, fmt) converted_dates.append(time.strftime(date_format, dt)) break except ValueError: continue return converted_dates def measure_conversion_time(date_time_strings: List[str], date_format: str) -> float: start_time = time.perf_counter() convert_date_times(date_time_strings, date_format) end_time = time.perf_counter() return end_time - start_time # Example: date_time_strings = [\\"2023-10-01 13:45:30\\", \\"01-10-2023 14:50:15\\", \\"10/01/2023 01:55 PM\\"] date_format = \\"%Y-%m-%d %H:%M:%S\\" converted_dates = convert_date_times(date_time_strings, date_format) print(converted_dates) conversion_time = measure_conversion_time(date_time_strings, date_format) print(f\\"Time taken for conversion: {conversion_time} seconds\\") ``` # Note: - Include comprehensive error handling in your implementation. - Your solution should be efficient and make use of appropriate functions from the `time` module as described in the documentation provided.","solution":"from typing import List import time def convert_date_times(date_time_strings: List[str], date_format: str) -> List[str]: Converts a list of date-time strings into a standardized format. :param date_time_strings: List of strings representing date-times in various formats. :param date_format: The desired output format for the date-times. :return: A list of strings where each string represents a date-time converted to the `date_format`. converted_dates = [] for dt_str in date_time_strings: for fmt in [\\"%Y-%m-%d %H:%M:%S\\", \\"%d-%m-%Y %H:%M:%S\\", \\"%m/%d/%Y %I:%M %p\\"]: try: dt = time.strptime(dt_str, fmt) converted_dates.append(time.strftime(date_format, dt)) break except ValueError: continue return converted_dates def measure_conversion_time(date_time_strings: List[str], date_format: str) -> float: Measures the time taken to convert a list of date-time strings into a standardized format. :param date_time_strings: List of strings representing date-times in various formats. :param date_format: The desired output format for the date-times. :return: Time taken (in seconds) to convert all date-times using the `convert_date_times` function. start_time = time.perf_counter() convert_date_times(date_time_strings, date_format) end_time = time.perf_counter() return end_time - start_time"},{"question":"**Question: Optimize a Batch Processing System with String Templating, Binary Data Handling, and Multi-threading** You are tasked with designing and implementing a batch processing system that will handle large datasets. This system must: 1. Use templating to generate consistent file names for processed output files. 2. Efficiently manage binary data while reading and writing files. 3. Utilize multi-threading to process multiple datasets in parallel for improved performance. # Requirements: 1. **Data Handling**: Utilize the `struct` module to read and write binary data. Each dataset is represented as a binary file consisting of records with the format: - `record_id` (4-byte unsigned int) - `value` (4-byte float) 2. **Templating**: Implement a feature to generate output file names using template strings based on a format provided by the user. The template should support placeholders for: - Record ID (`record_id`) - Date (`date`) - Original file name (`orig_name`) 3. **Multi-threading**: Employ the `threading` module to process multiple datasets concurrently. Ensure that the main program continues to function while the datasets are being processed in the background. # Input and Output: - **Input**: - List of binary dataset file paths. - Output file naming template provided by user. - Directory to save processed files. - **Output**: - Processed binary files saved in the specified directory with names generated based on the provided template. # Constraints: - Each record\'s `value` is doubled before saving to the output file. - Ensure thread-safe operations when handling file reads/writes. - Use appropriate synchronization mechanisms to handle shared resources. # Performance: - The system should efficiently process large datasets with minimal delay, leveraging multi-threading for performance. - Ensure the system scales well with the increasing number of datasets and handles large binary files properly. # Example: 1. **Input**: - File paths: `[\\"dataset1.bin\\", \\"dataset2.bin\\"]` - Naming template: `\\"processed_%date%_%record_id%_%orig_name%\\"` - Output directory: `\\"output/\\"` 2. **Processing**: - Read each record from the input binary files. - Double the `value` for each record. - Write the modified record to a new binary file with the name generated from the template string (`date`, `record_id`, and `orig_name` included). 3. **Output**: Assuming today\'s date is \\"25Oct23\\", for a sample record from `dataset1.bin` with `record_id` 101 and `value` 25.5, the output file name could be `\\"processed_25Oct23_101_dataset1.bin\\"`. # Implementation: Implement the following functions: 1. **generate_filename**(template: str, record_id: int, date: str, orig_name: str) -> str: - Generates a file name based on the provided template and placeholders. 2. **process_dataset**(input_file: str, output_dir: str, template: str): - Reads records from the binary input file, processes each record, and writes to an output file with the name generated by `generate_filename`. 3. **batch_process**(files: List[str], output_dir: str, template: str): - Manages the multi-threading to process multiple datasets concurrently. # Notes: - Ensure that you handle exceptions properly, especially for file I/O operations. - Make use of synchronization primitives from `threading` module to ensure thread safety.","solution":"import os from datetime import datetime import threading import struct def generate_filename(template: str, record_id: int, date: str, orig_name: str) -> str: Generates a filename based on a given template. template: str - The template string containing placeholders for record_id, date, and orig_name. record_id: int - The ID of the record. date: str - The date string to be included in the filename. orig_name: str - The original file name. Returns: str - The generated file name. filename = template.replace(\\"%record_id%\\", str(record_id)) filename = filename.replace(\\"%date%\\", date) filename = filename.replace(\\"%orig_name%\\", orig_name) return filename def process_dataset(input_file: str, output_dir: str, template: str): Processes a single dataset file. input_file: str - The path to the input binary dataset file. output_dir: str - The directory where the output files should be saved. template: str - The template string for naming the output files. date_str = datetime.now().strftime(\\"%d%b%y\\") orig_name = os.path.basename(input_file) with open(input_file, \\"rb\\") as f: while chunk := f.read(8): # Each record is 8 bytes long record_id, value = struct.unpack(\'If\', chunk) value *= 2 # Double the value output_filename = generate_filename(template, record_id, date_str, orig_name) output_path = os.path.join(output_dir, output_filename) with open(output_path, \\"ab\\") as outf: outf.write(struct.pack(\'If\', record_id, value)) def batch_process(files, output_dir, template): Processes multiple dataset files concurrently using multi-threading. files: List[str] - The list of dataset file paths. output_dir: str - The directory where the output files should be saved. template: str - The template string for naming the output files. threads = [] for input_file in files: thread = threading.Thread(target=process_dataset, args=(input_file, output_dir, template)) threads.append(thread) thread.start() for thread in threads: thread.join()"},{"question":"**Objective**: Utilize the `atexit` module to manage resource cleanup and demonstrate understanding of function registration and argument passing. Problem Statement You are tasked with creating a logging system for a Python application that logs messages to a file. The log file should be closed properly upon the normal termination of the program. Additionally, provide functionality to dynamically register multiple cleanup functions with different priorities and arguments. Requirements 1. **Log Manager Class**: - Create a `LogManager` class with the following methods: - `__init__(self, filename)`: - Initializes the log manager with the given log file name. - Opens the file for writing. - `log(self, message)`: - Logs the provided message to the file. - Adds a timestamp to each log entry. - `close(self)`: - Closes the log file properly. - Handles any exceptions that might occur. 2. **Dynamic Cleanup Registration**: - Implement a function `register_cleanup(func, *args, **kwargs)` that registers cleanup functions using `atexit.register()` with optional arguments. - The function should allow registering the same function multiple times with different arguments. - Ensure all registered functions are called in the reverse order of registration at program termination. 3. **Usage**: - Create an instance of `LogManager` and register its `close` method as a cleanup function. - Register at least two additional cleanup functions with different priorities and arguments and demonstrate their execution order at interpreter termination. Input and Output - **Input**: No direct input from the user is needed. - **Output**: The log file should contain the logged messages with timestamps. The terminal should print messages indicating the cleanup functions\' execution order. Constraints - The log file operations should handle file-related exceptions gracefully. - Functions registered without arguments should be demonstrated using `atexit.register` as a decorator. Example Execution ```python # Sample usage demonstrating the expected functionality. if __name__ == \\"__main__\\": log_manager = LogManager(\\"app.log\\") log_manager.log(\\"Application started\\") register_cleanup(log_manager.close) # Register log file close method def goodbye(name): print(f\\"Goodbye, {name}!\\") register_cleanup(goodbye, \\"Alice\\") register_cleanup(goodbye, \\"Bob\\") @atexit.register def farewell(): print(\\"Farewell, the program is ending.\\") log_manager.log(\\"Application running\\") log_manager.log(\\"Application terminating\\") # Expected log file content: # [Timestamp] Application started # [Timestamp] Application running # [Timestamp] Application terminating # Expected terminal output at program termination: # Farewell, the program is ending. # Goodbye, Bob! # Goodbye, Alice! ``` Implement the `LogManager` class and `register_cleanup` function to meet the requirements.","solution":"import atexit import datetime class LogManager: def __init__(self, filename): self.filename = filename self.file = None try: self.file = open(self.filename, \'w\') except Exception as e: print(f\\"Failed to open log file {self.filename}: {e}\\") def log(self, message): if self.file: timestamp = datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') try: self.file.write(f\\"[{timestamp}] {message}n\\") except Exception as e: print(f\\"Failed to write to log file: {e}\\") def close(self): if self.file: try: self.file.close() self.file = None except Exception as e: print(f\\"Failed to close log file: {e}\\") def register_cleanup(func, *args, **kwargs): atexit.register(func, *args, **kwargs) if __name__ == \\"__main__\\": log_manager = LogManager(\\"app.log\\") log_manager.log(\\"Application started\\") register_cleanup(log_manager.close) # Register log file close method def goodbye(name): print(f\\"Goodbye, {name}!\\") register_cleanup(goodbye, \\"Alice\\") register_cleanup(goodbye, \\"Bob\\") @atexit.register def farewell(): print(\\"Farewell, the program is ending.\\") log_manager.log(\\"Application running\\") log_manager.log(\\"Application terminating\\")"},{"question":"# Assessing Unix Shadow Password Database Interaction using `spwd` Module Objective: You are required to demonstrate your understanding of the `spwd` module in Python to interact with the Unix shadow password database. Your task involves writing functions to retrieve specific information and handle potential exceptions or privilege issues. Problem Statement: 1. **Function: `get_user_shadow_info(username: str) -> dict`**: Implement a function that: - Takes a username as input. - Returns a dictionary with the user\'s shadow password information. The keys should correspond to the attribute names (`sp_namp`, `sp_pwdp`, `sp_lstchg`, etc.), and the values should be the respective data for those attributes. - Raises a `PermissionError` if the user does not have the necessary privileges to access the shadow password database. - Raises a `KeyError` if the username does not exist in the database. 2. **Function: `list_expired_accounts(expiration_date: int) -> List[str]`**: Implement a function that: - Takes an integer `expiration_date` representing the number of days since 1970-01-01 (Unix epoch). - Returns a list of usernames whose accounts have expired as of the given expiration date or earlier. - Handles any potential permission issues by raising a `PermissionError`. Input Constraints: - `username`: A string representing a valid Unix username. - `expiration_date`: An integer that denotes days since the Unix epoch. Output Format: - `get_user_shadow_info`: Returns a dictionary with keys `sp_namp`, `sp_pwdp`, `sp_lstchg`, `sp_min`, `sp_max`, `sp_warn`, `sp_inact`, `sp_expire`, `sp_flag`. - `list_expired_accounts`: Returns a list of usernames as strings. Example: ```python # Example usage: try: user_info = get_user_shadow_info(\\"john\\") print(user_info) except PermissionError: print(\\"Permission Denied. Please run as root.\\") except KeyError: print(\\"Username not found.\\") try: expired_users = list_expired_accounts(19000) # Jan 1, 2022 print(expired_users) except PermissionError: print(\\"Permission Denied. Please run as root.\\") ``` Notes: 1. Ensure your code handles permission and existence-related exceptions gracefully. 2. Consider edge cases where no accounts might be expired. 3. Properly document your code for clarity. ```python import spwd def get_user_shadow_info(username: str) -> dict: # Your implementation here pass def list_expired_accounts(expiration_date: int) -> List[str]: # Your implementation here pass ```","solution":"import spwd from typing import List def get_user_shadow_info(username: str) -> dict: try: user_info = spwd.getspnam(username) return { \'sp_namp\': user_info.sp_namp, \'sp_pwdp\': user_info.sp_pwdp, \'sp_lstchg\': user_info.sp_lstchg, \'sp_min\': user_info.sp_min, \'sp_max\': user_info.sp_max, \'sp_warn\': user_info.sp_warn, \'sp_inact\': user_info.sp_inact, \'sp_expire\': user_info.sp_expire, \'sp_flag\': user_info.sp_flag } except PermissionError: raise PermissionError(\\"Permission Denied. Please run as root.\\") except KeyError: raise KeyError(f\\"Username {username} not found.\\") def list_expired_accounts(expiration_date: int) -> List[str]: try: expired_users = [] all_users = spwd.getspall() for user in all_users: if user.sp_expire != -1 and user.sp_expire <= expiration_date: expired_users.append(user.sp_namp) return expired_users except PermissionError: raise PermissionError(\\"Permission Denied. Please run as root.\\")"},{"question":"# Question: Performance Comparison using `timeit` In performance-critical applications, identifying efficient code snippets is essential. In this task, you are to write a Python function utilizing the `timeit` module to compare the performance of two different implementations of a function. You will then return the execution times of each implementation and the faster one. Function Signature ```python def compare_performance(func1: str, func2: str, setup: str = \'pass\', number: int = 100000) -> dict: # Your code here ``` Input - `func1` (str): A string representation of the first code snippet to be timed. - `func2` (str): A string representation of the second code snippet to be timed. - `setup` (str): A string with setup code executed once before the timing (default is \'pass\'). - `number` (int): Number of executions for timing (default is 100000). Output - A dictionary containing: - `time1` (float): Execution time of `func1`. - `time2` (float): Execution time of `func2`. - `faster` (str): A string \'func1\' or \'func2\', representing the faster implementation. Constraints - The `setup` string, `func1`, and `func2` should be valid Python code snippets. - You are not allowed to use any external libraries except `timeit`. Example ```python func1 = \'\\"-\\".join(str(n) for n in range(100))\' func2 = \'\\"-\\".join([str(n) for n in range(100)])\' setup = \\"pass\\" number = 100000 result = compare_performance(func1, func2, setup, number) print(result) # Example Output: {\'time1\': 0.55, \'time2\': 0.37, \'faster\': \'func2\'} ``` Explanation In this example, the `compare_performance` function calculates the time it takes to execute `func1` and `func2` `number` times and returns a dictionary with the execution times and the faster implementation. Implement the function `compare_performance` in such a way that it adheres to these requirements.","solution":"import timeit def compare_performance(func1: str, func2: str, setup: str = \'pass\', number: int = 100000) -> dict: Compare the performance of two Python code snippets. Parameters: - func1 (str): The first code snippet to be timed. - func2 (str): The second code snippet to be timed. - setup (str): The setup code executed once before the timing (default is \'pass\'). - number (int): Number of executions for timing (default is 100000). Returns: - dict: Contains {\'time1\': float, \'time2\': float, \'faster\': str}. time1 = timeit.timeit(func1, setup=setup, number=number) time2 = timeit.timeit(func2, setup=setup, number=number) faster = \'func1\' if time1 < time2 else \'func2\' return { \'time1\': time1, \'time2\': time2, \'faster\': faster }"},{"question":"# Advanced Email Processing with Python You are tasked with developing a function that processes an `email.message.Message` object to extract and summarize information from its subparts. Specifically, you need to: 1. Print the structure of the message object, similar to the `_structure` function. 2. Retrieve all text content from subparts of the message that are of MIME type `text/plain` and return them as a list of strings. Function Definition ```python def summarize_email_contents(msg: email.message.Message) -> List[str]: Process an email.message.Message object to extract and summarize its information. Args: msg (email.message.Message): The email message object to be processed. Returns: List[str]: A list of strings containing text content from subparts with MIME type \'text/plain\'. pass ``` Requirements - Use the `email.iterators._structure` function to print the structure of the message object. - Use the `email.iterators.typed_subpart_iterator` function to iterate over subparts with MIME type `text/plain`. - Collect the payloads of these subparts into a list of strings and return this list. - Ensure that the function handles cases where there are no `text/plain` subparts gracefully by returning an empty list. Example Given a message object `msg`, your function should output: 1. The structure of the message object. 2. A list containing text content from subparts of type `text/plain`. ```python # Example usage: msg = email.message_from_string( MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\\"===============7316723738965507817==\\" --===============7316723738965507817== Content-Type: text/plain Hello, this is the first part. --===============7316723738965507817== Content-Type: text/html <html><body>This is the first <b>HTML</b> part.</body></html> --===============7316723738965507817== Content-Type: text/plain This is the second part. --===============7316723738965507817==-- ) result = summarize_email_contents(msg) print(result) ``` Output: ``` multipart/mixed text/plain text/html text/plain [\'Hello, this is the first part.\', \'This is the second part.\'] ``` Constraints - You may assume that the email message provided is well-formed. - Focus on correctly utilizing the provided iterators and handling string payloads appropriately.","solution":"import email from email.iterators import typed_subpart_iterator, _structure from typing import List def summarize_email_contents(msg: email.message.Message) -> List[str]: Process an email.message.Message object to extract and summarize its information. Args: msg (email.message.Message): The email message object to be processed. Returns: List[str]: A list of strings containing text content from subparts with MIME type \'text/plain\'. # Print the structure of the message object _structure(msg) # Initialize an empty list to hold the text/plain contents text_parts = [] # Iterate over the subparts of MIME type \'text/plain\' for part in typed_subpart_iterator(msg, \'text\', \'plain\'): text_parts.append(part.get_payload(decode=True).decode(part.get_content_charset() or \'utf-8\')) return text_parts"},{"question":"Objective Write a Python program that uses the `tempfile` module to perform secure and efficient operations with temporary files and directories. Task Your task is to implement a function `manage_temp_files` that performs the following: 1. Creates a temporary directory using `TemporaryDirectory` as a context manager. 2. Within this directory, creates three files using `NamedTemporaryFile` with the `delete=False` parameter and writes some data into each. 3. Lists all filenames created in the temporary directory. 4. Reads the content from each file and prints it. 5. Cleanly deletes all temporary files before the end of the context. Function Signature ```python def manage_temp_files() -> None: pass ``` Expected Input and Output - **Input**: The function does not take any input parameters. - **Output**: The function will print filenames and their respective contents. Example If the function creates three temporary files named `tmp1`, `tmp2`, and `tmp3` and writes \\"Hello\\", \\"World\\", and \\"!\\" into each respectively, the output should be: ``` created temporary directory /path/to/tmpdir /tmpdir/tmp1: Hello /tmpdir/tmp2: World /tmpdir/tmp3: ! All temporary files cleaned up. ``` Constraints - The function should handle any exceptions that may occur to ensure all temporary files and the directory are properly cleaned up. - The function should use context managers to manage resources effectively. Performance Requirements - The function should be efficient in terms of creating and deleting temporary files and directories. - Ensure there are no leftover temporary files or directories after the function execution. You are encouraged to refer to the `tempfile` module documentation for the specific methods and parameters needed to accomplish the task.","solution":"import tempfile import os def manage_temp_files() -> None: try: with tempfile.TemporaryDirectory() as tempdir: print(f\\"created temporary directory {tempdir}\\") # List of filenames to be created filenames = [tempfile.NamedTemporaryFile(dir=tempdir, delete=False) for _ in range(3)] # Writing some data into each file filenames[0].write(b\\"Hello\\") filenames[1].write(b\\"World\\") filenames[2].write(b\\"!\\") for f in filenames: f.close() # Listing all filenames for filename in filenames: file_path = filename.name with open(file_path, \'rb\') as f: content = f.read() print(f\\"{file_path}: {content.decode()}\\") # Cleanly deleting all temporary files for filename in filenames: os.remove(filename.name) except Exception as e: print(f\\"An error occurred: {e}\\") finally: print(\\"All temporary files cleaned up.\\")"},{"question":"You are tasked with creating a Python script that automates the distribution of a Python package using the information provided about creating source distributions with `sdist`. The goal is to demonstrate your understanding of the setup process, manifest creation, and distribution file formats. # Problem Statement: Write a Python script named `create_distribution.py` with the following functionalities: 1. **Generate Source Distribution**: - The script should generate a source distribution for a given package using the `sdist` command. - Accept command-line arguments to specify the distribution format(s). Default to `gztar`. 2. **Custom Manifest**: - Accept an optional `MANIFEST.in` file to customize the list of included/excluded files in the source distribution. - If `MANIFEST.in` is not provided, use the default file set. 3. **Owner and Group**: - Allow optional command-line arguments to set the owner and group for the distribution files. 4. **Manifest-only Option**: - Provide an option to generate only the `MANIFEST` file without creating the distribution. # Specifics: - The script should be invoked from the command line as follows: ```shell python create_distribution.py --formats=<format1,format2,...> --owner=<owner> --group=<group> --manifest-only <manifest_in_file> ``` Where: - `--formats` specifies one or more distribution formats (e.g., `zip`, `gztar`). This argument is optional and defaults to `gztar`. - `--owner` and `--group` specify the owner and group for the files in the distribution. These arguments are optional. - `--manifest-only` indicates that the script should only generate the `MANIFEST` file and not the distribution. This argument is optional. - `<manifest_in_file>` is the path to `MANIFEST.in`. This argument is optional and defaults to not using any custom `MANIFEST.in`. # Input: - `--formats`: Comma-separated list of distribution formats (optional) - `--owner`: Owner name for distribution files (optional) - `--group`: Group name for distribution files (optional) - `--manifest-only`: A flag to generate only the `MANIFEST` file (optional) - `<manifest_in_file>`: Path to a custom `MANIFEST.in` file (optional) # Output: - The script should create the source distribution files in the specified formats. - If `--manifest-only` is specified, only the `MANIFEST` file should be created. # Constraints: - Assume Python and necessary packages (`setuptools`, `distutils`) are installed. - The script should handle possible invalid inputs gracefully. # Example: ```shell python create_distribution.py --formats=zip,gztar --owner=root --group=root --manifest-only custom_manifest.in ``` # Hint: Use the `argparse` module for parsing command-line arguments and the `subprocess` module for executing shell commands.","solution":"import argparse import os import subprocess def create_distribution(formats=\'gztar\', owner=None, group=None, manifest_only=False, manifest_in_file=None): # Create the base command command = [\'python\', \'setup.py\', \'sdist\'] # Add formats to the command formats_list = formats.split(\',\') format_args = [f\'--formats={fmt}\' for fmt in formats_list] command.extend(format_args) # Add owner and group if specified if owner: command.append(f\'--owner={owner}\') if group: command.append(f\'--group={group}\') # Set the environment variable to use custom MANIFEST.in if provided if manifest_in_file: os.environ[\'MANIFEST\'] = manifest_in_file if manifest_only: command = [\'python\', \'setup.py\', \'sdist\', \'--manifest-only\'] # Execute the command subprocess.run(command) def main(): parser = argparse.ArgumentParser(description=\\"Create a Python package source distribution\\") parser.add_argument(\'--formats\', type=str, default=\'gztar\', help=\'Comma-separated list of distribution formats\') parser.add_argument(\'--owner\', type=str, help=\'Owner name for distribution files\') parser.add_argument(\'--group\', type=str, help=\'Group name for distribution files\') parser.add_argument(\'--manifest-only\', action=\'store_true\', help=\'Generate only the MANIFEST file\') parser.add_argument(\'manifest_in_file\', nargs=\'?\', help=\'Path to custom MANIFEST.in file\') args = parser.parse_args() create_distribution(args.formats, args.owner, args.group, args.manifest_only, args.manifest_in_file) if __name__ == \\"__main__\\": main()"},{"question":"Problem Statement You are required to implement a Python program that creates a simple echo server and client using the socket module. The server should be able to handle multiple clients using the `selectors` module to multiplex the I/O. 1. **Server Requirements**: - It should listen on a specified IP address and port. - It should use non-blocking sockets. - It should handle multiple clients concurrently without using multi-threading or multiprocessing (this is where the `selectors` module comes into play). - It should echo back any message it receives from a client. 2. **Client Requirements**: - It should connect to the server using the specified IP address and port. - It should send a message to the server and print the echoed response. Input and Output - The server does not take any direct input from the user. - The client will take a message string input from the user that it sends to the server. - The server will echo the received message back to the client, which the client should print out. Constraints - You must use the `socket` and `selectors` modules. - You should handle socket connections in a scalable way without thread or process based concurrency. - Handle partial messages and ensure the client receives the full message echo. Example Consider the following flow: 1. Start the server: - `python server.py` - The server starts listening on `localhost` and port `65432`. 2. Run the client: - `python client.py` - Input the message: `Hello, Server!` - Output: `Received from server: Hello, Server!` Implementation 1. **Server Implementation** (`server.py`): ```python import selectors import socket sel = selectors.DefaultSelector() def accept(sock, mask): conn, addr = sock.accept() print(\'accepted\', conn, \'from\', addr) conn.setblocking(False) sel.register(conn, selectors.EVENT_READ, read) def read(conn, mask): data = conn.recv(1000) if data: print(\'echoing\', repr(data), \'to\', conn) conn.send(data) else: print(\'closing\', conn) sel.unregister(conn) conn.close() def main(): sock = socket.socket() sock.bind((\'localhost\', 65432)) sock.listen(100) sock.setblocking(False) sel.register(sock, selectors.EVENT_READ, accept) while True: events = sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) if __name__ == \'__main__\': main() ``` 2. **Client Implementation** (`client.py`): ```python import socket def main(): server_address = (\'localhost\', 65432) sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.connect(server_address) try: message = input(\\"Input message: \\") sock.sendall(message.encode()) data = sock.recv(1000) print(\'Received from server:\', data.decode()) finally: sock.close() if __name__ == \'__main__\': main() ``` Your task is to complete the implementation for both the server and client using the `socket` and `selectors` modules, ensuring that they meet the requirements specified.","solution":"# server.py import selectors import socket sel = selectors.DefaultSelector() def accept(sock, mask): conn, addr = sock.accept() print(\'accepted\', conn, \'from\', addr) conn.setblocking(False) sel.register(conn, selectors.EVENT_READ, read) def read(conn, mask): data = conn.recv(1000) if data: print(\'echoing\', repr(data), \'to\', conn) conn.sendall(data) else: print(\'closing\', conn) sel.unregister(conn) conn.close() def main(): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.bind((\'localhost\', 65432)) sock.listen(100) sock.setblocking(False) sel.register(sock, selectors.EVENT_READ, accept) while True: events = sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) if __name__ == \'__main__\': main() # client.py import socket def main(): server_address = (\'localhost\', 65432) sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.connect(server_address) try: message = input(\\"Input message: \\") sock.sendall(message.encode()) data = sock.recv(1000) print(\'Received from server:\', data.decode()) finally: sock.close() if __name__ == \'__main__\': main()"},{"question":"# Custom Email Parser with Error Handling **Objective:** Implement a custom email parser that can extract and print headers from an email message. The parser should handle various exceptions and defects as described in the \\"email.errors\\" module. **Problem Statement:** Write a function `custom_email_parser(email_message: str) -> None` that takes an email message as input and attempts to parse its headers. If the message is malformed or contains any defects, the function should log the appropriate errors. The function should: 1. Parse the email headers. 2. Print each header in `key: value` format. 3. Catch and handle exceptions and defects defined in the \\"email.errors\\" module. 4. Print appropriate error messages when exceptions are encountered. 5. Log defects that are found during the parsing process. **Input:** - `email_message` (str): A string representing the raw email message. **Output:** - The function should print headers and error messages directly. **Constraints:** - Use only the standard Python libraries. - Handle all relevant exceptions and defects as defined in the \\"email.errors\\" module. **Example:** Suppose the input email message is: ``` From: sender@example.com To: recipient@example.com Subject: Test Email This is the body of the email. ``` Expected output: ``` From: sender@example.com To: recipient@example.com Subject: Test Email ``` If the email message is malformed (e.g., missing a colon in a header or invalid base64), the function should catch and print the relevant error message. **Performance Requirements:** - The function should handle reasonably large email messages efficiently. - The focus is on correct implementation of error handling, so performance requirements are secondary to correctness. Hints: - Use the `email` library and its submodules for parsing. - Utilize try-except blocks to handle exceptions. - Check for defects present in the `msg.defects` attribute. ```python import email from email import policy from email.parser import BytesParser from email.errors import MessageParseError, HeaderParseError, MultipartConversionError, HeaderWriteError def custom_email_parser(email_message: str) -> None: try: msg = BytesParser(policy=policy.default).parsebytes(email_message.encode()) for defect in msg.defects: print(f\\"Defect found: {defect}\\") for header, value in msg.items(): print(f\\"{header}: {value}\\") except (MessageParseError, HeaderParseError, MultipartConversionError, HeaderWriteError) as e: print(f\\"Error: {e}\\") # Example usage email_message = \\"From: sender@example.comnTo: recipient@example.comnSubject: Test EmailnnThis is the body of the email.\\" custom_email_parser(email_message) ```","solution":"import email from email import policy from email.parser import BytesParser from email.errors import MessageParseError, HeaderParseError, MultipartConversionError, HeaderWriteError def custom_email_parser(email_message: str) -> None: try: msg = BytesParser(policy=policy.default).parsebytes(email_message.encode()) for defect in msg.defects: print(f\\"Defect found: {defect}\\") for header, value in msg.items(): print(f\\"{header}: {value}\\") except (MessageParseError, HeaderParseError, MultipartConversionError, HeaderWriteError) as e: print(f\\"Error: {e}\\")"},{"question":"# Python Memory Profiling with `tracemalloc` **Objective:** Implement a function to monitor memory usage of a given function using Python\'s `tracemalloc` module. # Question You are required to write a function `profile_memory_usage()` that profiles the memory usage of a given function by taking memory snapshots before and after the function execution. The function should also return detailed statistics of memory allocations caused by the provided function. Here\'s the skeleton of the function: ```python import tracemalloc def profile_memory_usage(target_function, *args, **kwargs): Profiles memory usage of the target function using tracemalloc. Parameters: target_function (function): The function to profile. *args: Variable length argument list for the target function. **kwargs: Arbitrary keyword arguments for the target function. Returns: dict: Dictionary containing memory usage statistics. # Start tracing memory allocations tracemalloc.start() # Capture initial memory snapshot snapshot_before = tracemalloc.take_snapshot() # Execute the target function with provided arguments target_function(*args, **kwargs) # Capture memory snapshot after function execution snapshot_after = tracemalloc.take_snapshot() # Stop memory tracing tracemalloc.stop() # Compute differences between the two snapshots stats = snapshot_after.compare_to(snapshot_before, \'lineno\') # Prepare and return a dictionary with memory statistics result = { \'total_allocated_size\': sum(stat.size for stat in stats) / 1024, # in KiB \'top_stats\': [ { \'filename\': stat.traceback[0].filename, \'lineno\': stat.traceback[0].lineno, \'size\': stat.size / 1024 # in KiB } for stat in stats[:10] # Top 10 memory consuming locations ] } return result # Example usage of `profile_memory_usage` with a sample function if __name__ == \\"__main__\\": def sample_function(): # Example code that allocates memory return [x for x in range(10000)] memory_profile = profile_memory_usage(sample_function) print(memory_profile) ``` # Expected Input and Output The `profile_memory_usage()` function should take: - `target_function`: A reference to the function you want to profile. - `*args` and `**kwargs`: Arguments to pass to the target function. The function should return a dictionary containing: - `total_allocated_size` (in KiB): Total memory allocated by the target function. - `top_stats` (list of dictionaries): Details of the top 10 memory-consuming locations with keys: - `filename`: Name of the file where memory was allocated. - `lineno`: Line number in the file where memory was allocated. - `size`: Size of memory allocated in KiB. # Constraints and Limitations - Ensure that the `tracemalloc` module is properly started and stopped before and after tracing memory allocations. - Only include memory allocations related to the provided `target_function`. - Your function should handle possible exceptions that might occur during function execution and memory profiling. # Performance Requirements - The function should efficiently compute memory differences and provide detailed statistics within reasonable time limits. - Ensure the solution handles both small and large memory allocation scenarios without excessive overhead. This problem tests the students\' understanding of advanced debugging tools in Python, ability to handle function arguments, and manipulation of dictionaries and lists for structured data.","solution":"import tracemalloc def profile_memory_usage(target_function, *args, **kwargs): Profiles memory usage of the target function using tracemalloc. Parameters: target_function (function): The function to profile. *args: Variable length argument list for the target function. **kwargs: Arbitrary keyword arguments for the target function. Returns: dict: Dictionary containing memory usage statistics. # Start tracing memory allocations tracemalloc.start() # Capture initial memory snapshot snapshot_before = tracemalloc.take_snapshot() # Execute the target function with provided arguments target_function(*args, **kwargs) # Capture memory snapshot after function execution snapshot_after = tracemalloc.take_snapshot() # Stop memory tracing tracemalloc.stop() # Compute differences between the two snapshots stats = snapshot_after.compare_to(snapshot_before, \'lineno\') # Prepare and return a dictionary with memory statistics result = { \'total_allocated_size\': sum(stat.size for stat in stats) / 1024, # in KiB \'top_stats\': [ { \'filename\': stat.traceback[0].filename, \'lineno\': stat.traceback[0].lineno, \'size\': stat.size / 1024 # in KiB } for stat in stats[:10] # Top 10 memory consuming locations ] } return result # Example usage of `profile_memory_usage` with a sample function if __name__ == \\"__main__\\": def sample_function(): # Example code that allocates memory return [x for x in range(10000)] memory_profile = profile_memory_usage(sample_function) print(memory_profile)"},{"question":"You are required to implement a custom auto-completion mechanism for a simple interactive Python interpreter. This mechanism will be inspired by the `rlcompleter` module, focusing on basic and dotted name text completions from a predefined context. # Function Specification Class: `CustomCompleter` Method: `def complete(self, text: str, state: int) -> Optional[str]:` # Inputs: - **text** *(str)*: The text to be completed. - **state** *(int)*: The completion state (returns the *state*th completion). # Outputs: - Returns the *state*th completion for *text*. If there are no more completions, the method should return `None`. # Constraints: - You are required to handle both simple names (no period character) and dotted names as described in the rlcompleter. - Use the following predefined contexts for your completions: - **simple_names**: List of strings containing Python keywords. - **dotted_names**: Mock object structure defined for the completion. # Example: ```python simple_names = [\'def\', \'return\', \'if\', \'else\', \'import\', \'from\', \'class\'] class MockModule: class sub_module: @staticmethod def function_a(): pass @staticmethod def function_b(): pass dotted_names = { \'mod\': MockModule } # Example Usage completer = CustomCompleter() assert completer.complete(\'de\', 0) == \'def\' assert completer.complete(\'mod.sub-\', 0) == \'mod.sub_module\' assert completer.complete(\'mod.sub_module.f\', 0) == \'mod.sub_module.function_a\' assert completer.complete(\'mod.sub_module.f\', 1) == \'mod.sub_module.function_b\' ``` # Task: 1. Implement the `CustomCompleter` class with the `complete` method as described above. 2. Ensure that the `complete` method correctly returns completions based on the predefined contexts. **Note**: Handle any exceptions gracefully and return `None` if the expression evaluation could not be completed.","solution":"class CustomCompleter: def __init__(self): self.simple_names = [\'def\', \'return\', \'if\', \'else\', \'import\', \'from\', \'class\'] self.dotted_names = { \'mod\': self.MockModule } class MockModule: class sub_module: @staticmethod def function_a(): pass @staticmethod def function_b(): pass def complete(self, text: str, state: int) -> str: # Check if the text contains a dot, which means it\'s a dotted name if \'.\' in text: try: # Extract the base name and the starting text for the last fragment obj_name, _, attr_start = text.rpartition(\'.\') # Evaluate the current object obj = eval(obj_name, {}, self.dotted_names) # Collect all possible completions based on the current object completions = [f\\"{obj_name}.{attr}\\" for attr in dir(obj) if attr.startswith(attr_start)] except Exception: return None else: # Simple name completion (keywords) completions = [name for name in self.simple_names if name.startswith(text)] # Return the state-th completion or None try: return completions[state] except IndexError: return None"},{"question":"# Python Exception Handling and Resource Management Assessment **Objective**: Implement a function that reads from a file, processes its content, and demonstrates advanced exception handling techniques. Function Signature: ```python def process_file_data(filepath: str) -> dict: Reads from the given file, processes its content into a dictionary, and handles exceptions. Args: filepath: str - Path to the input file. Returns: A dictionary containing counts of each word in the file. pass ``` Problem Description: 1. **File Reading**: - Use a `with` statement to open the file and ensure it\'s properly closed afterward. - If the file does not exist, raise a `FileNotFoundError` with the message \\"File not found\\". 2. **Data Processing**: - Read the content of the file and count the occurrences of each word (case-insensitive). - If a line in the file cannot be processed due to a non-string value, raise a `ValueError` with the message \\"Invalid line in file\\". 3. **Exception Handling**: - Catch specific exceptions (`FileNotFoundError`, `ValueError`) and print appropriate error messages. - Use `finally` to print a message \\"Processing completed\\" whether an error occurred or not. 4. **Custom Exceptions**: - Define and raise a custom exception `EmptyFileError` if the file is empty. Constraints: - The file will contain one word per line. - All words are separated by spaces and may contain special characters or numbers. Example Usage: ```python filepath = \'words.txt\' # Assume this file contains several words, one per line. try: result = process_file_data(filepath) print(\\"Processed Data:\\", result) except Exception as e: print(\\"An error occurred:\\", e) ``` Sample File Content (`words.txt`): ``` Hello world hello Python world 3.9 hello 123 @# ``` Expected Output: ```python Processing completed ``` In case of file not found: ```python An error occurred: File not found Processing completed ``` In case of invalid line in file: ```python An error occurred: Invalid line in file Processing completed ``` In case of empty file: ```python An error occurred: File is empty Processing completed ``` **Notes**: - The function should be robust and handle various edge cases gracefully. - Provide clear and informative error messages. - Ensure all resources (like file handles) are properly managed and closed.","solution":"class EmptyFileError(Exception): pass def process_file_data(filepath: str) -> dict: Reads from the given file, processes its content into a dictionary, and handles exceptions. Args: filepath: str - Path to the input file. Returns: A dictionary containing counts of each word in the file. word_count = {} try: with open(filepath, \'r\') as file: content = file.readlines() if not content: raise EmptyFileError(\\"File is empty\\") for line in content: line = line.strip().lower() if not isinstance(line, str) or len(line) == 0: raise ValueError(\\"Invalid line in file\\") words = line.split() for word in words: if word in word_count: word_count[word] += 1 else: word_count[word] = 1 return word_count except FileNotFoundError: print(\\"An error occurred: File not found\\") except ValueError as ve: print(f\\"An error occurred: {ve}\\") except EmptyFileError as efe: print(f\\"An error occurred: {efe}\\") finally: print(\\"Processing completed\\")"},{"question":"# Custom Estimator Implementation in Scikit-learn Problem Statement You are required to create a custom estimator class in Scikit-learn that can be used in a pipeline or independently. This custom estimator should follow Scikit-learnâ€™s API and encapsulate a Machine Learning model. Your task is to implement a class `CustomLinearModel` that fits a linear regression using both ordinary least squares and ridge regularization. Function Signature ```python class CustomLinearModel(BaseEstimator, RegressorMixin): def __init__(self, alpha=1.0): CustomLinearModel constructor. Parameters: alpha (float): Regularization strength; must be a positive float. Defaults to 1.0. def fit(self, X, y): Fit linear model with training data. Parameters: X (numpy.ndarray or pandas.DataFrame): Training data of shape (n_samples, n_features) y (numpy.ndarray or pandas.Series): Target values of shape (n_samples,) Returns: self: object def predict(self, X): Predict using the linear model. Parameters: X (numpy.ndarray or pandas.DataFrame): Samples of shape (n_samples, n_features) Returns: numpy.ndarray: Predicted values of shape (n_samples,) def score(self, X, y): Returns the coefficient of determination R^2 of the prediction. Parameters: X (numpy.ndarray or pandas.DataFrame): Test data of shape (n_samples, n_features) y (numpy.ndarray or pandas.Series): True values of shape (n_samples,) Returns: float: R^2 score ``` Constraints 1. Implement the linear regression using closed-form solutions for both ordinary least squares and ridge regression. 2. You must handle the features and target values as `numpy.ndarray` or `pandas.DataFrame/Series`. 3. The `alpha` parameter is the regularization strength; when `alpha` is 0, the model should default to ordinary least squares. Example ```python import numpy as np # Example usage of the custom estimator model = CustomLinearModel(alpha=0.5) X_train = np.array([[1, 1], [1, 2], [2, 2], [2, 3]]) y_train = np.dot(X_train, np.array([1, 2])) + 3 model.fit(X_train, y_train) print(model.predict(np.array([[3, 5]]))) # Example prediction print(model.score(X_train, y_train)) # Evaluates R^2 score ``` Notes 1. **Ensure** that your implementation aligns with Scikit-learn\'s pipeline compatibility requirements. 2. **Include necessary imports** within your class/module. 3. Your class should pass **basic validations** and error checks for input data formats. 4. Assume `numpy` and `pandas` libraries are available during execution.","solution":"import numpy as np import pandas as pd from sklearn.base import BaseEstimator, RegressorMixin class CustomLinearModel(BaseEstimator, RegressorMixin): def __init__(self, alpha=1.0): CustomLinearModel constructor. Parameters: alpha (float): Regularization strength; must be a positive float. Defaults to 1.0. self.alpha = alpha self.coef_ = None self.intercept_ = None def fit(self, X, y): Fit linear model with training data. Parameters: X (numpy.ndarray or pandas.DataFrame): Training data of shape (n_samples, n_features) y (numpy.ndarray or pandas.Series): Target values of shape (n_samples,) Returns: self: object if isinstance(X, pd.DataFrame): X = X.values if isinstance(y, pd.Series): y = y.values # Add bias column to X X_b = np.hstack([np.ones((X.shape[0], 1)), X]) # Regularizer for least squares or ridge regression I = np.eye(X_b.shape[1]) I[0, 0] = 0 # Don\'t regularize the bias term self.coef_ = np.linalg.inv(X_b.T.dot(X_b) + self.alpha * I).dot(X_b.T).dot(y) self.intercept_ = self.coef_[0] self.coef_ = self.coef_[1:] return self def predict(self, X): Predict using the linear model. Parameters: X (numpy.ndarray or pandas.DataFrame): Samples of shape (n_samples, n_features) Returns: numpy.ndarray: Predicted values of shape (n_samples,) if isinstance(X, pd.DataFrame): X = X.values return X.dot(self.coef_) + self.intercept_ def score(self, X, y): Returns the coefficient of determination R^2 of the prediction. Parameters: X (numpy.ndarray or pandas.DataFrame): Test data of shape (n_samples, n_features) y (numpy.ndarray or pandas.Series): True values of shape (n_samples,) Returns: float: R^2 score from sklearn.metrics import r2_score y_pred = self.predict(X) return r2_score(y, y_pred)"},{"question":"**Question: Complex List and Dictionary Comprehensions with Custom Sorting and Filtering** **Objective:** Implement a Python function that processes a complex data structure utilizing list and dictionary comprehensions, comprehensively demonstrating your knowledge of expressions, comprehensions, managing sequences, and sorting operations. **Problem Statement:** Write a function `process_data(data)` that takes a list of dictionaries as input. Each dictionary represents a person with the following format: ```python { \'name\': str, \'age\': int, \'score\': float } ``` The function should return a dictionary where: 1. The keys are the age categories: `\'child\'` (0-12), `\'teen\'` (13-19), `\'adult\'` (20-64), and `\'senior\'` (65+). 2. The values are lists of names of the people in those categories. 3. The names should be sorted in descending order by their `score`. **Function Signature:** ```python def process_data(data: List[Dict[str, Union[str, int, float]]]) -> Dict[str, List[str]]: ``` **Input:** - `data`: A list of dictionaries, where each dictionary contains \'name\' (str), \'age\' (int), and \'score\' (float). Example: ```python [ {\'name\': \'Alice\', \'age\': 30, \'score\': 87.5}, {\'name\': \'Bob\', \'age\': 12, \'score\': 95.0}, {\'name\': \'Charlie\', \'age\': 17, \'score\': 79.0}, {\'name\': \'David\', \'age\': 65, \'score\': 85.4} ] ``` **Output:** - A dictionary with keys `\'child\'`, `\'teen\'`, `\'adult\'`, and `\'senior\'`, where each key maps to a list of names sorted by score in descending order. Example: ```python { \'child\': [\'Bob\'], \'teen\': [\'Charlie\'], \'adult\': [\'Alice\'], \'senior\': [\'David\'] } ``` **Constraints:** - The input list will contain at least one person and no more than 1000 people. - Names are unique. - Age will be between 0 and 120 inclusive. - Score will be between 0.0 and 100.0 inclusive. **Examples:** ```python data = [ {\'name\': \'Alice\', \'age\': 30, \'score\': 87.5}, {\'name\': \'Bob\', \'age\': 12, \'score\': 95.0}, {\'name\': \'Charlie\', \'age\': 17, \'score\': 79.0}, {\'name\': \'David\', \'age\': 65, \'score\': 85.4} ] print(process_data(data)) # Expected output: # {\'child\': [\'Bob\'], \'teen\': [\'Charlie\'], \'adult\': [\'Alice\'], \'senior\': [\'David\']} ``` **Hints:** 1. Use dictionary and list comprehensions to categorize and sort the data. 2. Remember to handle edge cases, like when no one belongs to a specific category.","solution":"from typing import List, Dict, Union def process_data(data: List[Dict[str, Union[str, int, float]]]) -> Dict[str, List[str]]: # Categorize age def categorize_age(age: int) -> str: if age <= 12: return \'child\' elif 13 <= age <= 19: return \'teen\' elif 20 <= age <= 64: return \'adult\' else: return \'senior\' # Initialize the result dictionary with keys and empty lists result = {\'child\': [], \'teen\': [], \'adult\': [], \'senior\': []} # Populate the dictionary with names for person in data: category = categorize_age(person[\'age\']) result[category].append((person[\'name\'], person[\'score\'])) # Sort the lists in descending order by score and retain only names for category in result.keys(): result[category] = [name for name, score in sorted(result[category], key=lambda x: x[1], reverse=True)] return result"},{"question":"**ConfigParser Advanced Usage** You are tasked with creating a Python script that reads a set of configuration settings and modifies them based on specified conditions. This script should showcase the ability to: 1. Handle reading from an INI file. 2. Manipulate data within the configuration. 3. Write the modified configuration back to an INI file. 4. Implement custom behaviors, such as non-default interpolation or custom handling of comments and delimiters. # Requirements: 1. Read a configuration file `settings.ini` with the following content: ``` [DEFAULT] log_path = /var/logs retries = 3 [Server] hostname = example.com port = 8080 [Database] user = admin password = secret dbname = app_db ``` 2. Modify the configuration based on these rules: - Change the `hostname` under `[Server]` to `server.example.com`. - Increment the `retries` value in `[DEFAULT]` by 1. - Add a new section `[Cache]` with entries `enabled` set to `yes` and `cache_size` set to `256MB`. 3. Enable extended interpolation and update the `Database` section to reference values from other sections: - Change `password` to be the same as `user`. - Add a new entry `backup_path` which should be `<log_path>/database_backup`. 4. Write the updated configuration back to a new file called `modified_settings.ini` with inline comments allowed using `#` as the prefix. # Implementation Details: 1. Use the `configparser.ConfigParser` class with any necessary custom settings. 2. Ensure `extended interpolation` is enabled. 3. Handle any potential errors that might arise during reading or writing the configuration file. 4. Follow the specified format and ensure all changes are correctly applied and saved in the output file. # Expected Output: The resulting `modified_settings.ini` file should look like this: ``` [DEFAULT] log_path = /var/logs retries = 4 [Server] hostname = server.example.com port = 8080 [Database] user = admin password = admin dbname = app_db backup_path = /var/logs/database_backup [Cache] enabled = yes cache_size = 256MB ``` ```python import configparser # Initialize the parser with extended interpolation and other custom settings config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation(), inline_comment_prefixes=(\'#\',), allow_no_value=True) # Read the configuration file config.read(\'settings.ini\') # Update the required values config[\'Server\'][\'hostname\'] = \'server.example.com\' config[\'DEFAULT\'][\'retries\'] = str(int(config[\'DEFAULT\'][\'retries\']) + 1) # Add a new section [Cache] config[\'Cache\'] = {\'enabled\': \'yes\', \'cache_size\': \'256MB\'} # Update Database section to use interpolation config[\'Database\'][\'password\'] = config[\'Database\'][\'user\'] config[\'Database\'][\'backup_path\'] = \'%(log_path)s/database_backup\' # Write the new configuration to a file with open(\'modified_settings.ini\', \'w\') as configfile: config.write(configfile) ```","solution":"import configparser def modify_config(input_file, output_file): Reads a configuration file, modifies it based on specific rules, and writes the modified configuration to a new file. Args: - input_file: str, the path to the input configuration file. - output_file: str, the path to the output configuration file. # Initialize the parser with extended interpolation and other custom settings config = configparser.ConfigParser(interpolation=configparser.ExtendedInterpolation(), inline_comment_prefixes=(\'#\',), allow_no_value=True) # Read the configuration file config.read(input_file) # Update the required values config[\'Server\'][\'hostname\'] = \'server.example.com\' config[\'DEFAULT\'][\'retries\'] = str(int(config[\'DEFAULT\'][\'retries\']) + 1) # Add a new section [Cache] config[\'Cache\'] = {\'enabled\': \'yes\', \'cache_size\': \'256MB\'} # Update Database section to use interpolation config[\'Database\'][\'password\'] = config[\'Database\'][\'user\'] config[\'Database\'][\'backup_path\'] = \'%(log_path)s/database_backup\' # Write the new configuration to a file with open(output_file, \'w\') as configfile: config.write(configfile) # Example usage: # modify_config(\'settings.ini\', \'modified_settings.ini\')"},{"question":"You are required to implement a Python program using the asyncio library to simulate a simplified multi-user chat server. The server will allow multiple clients to connect, send messages, and broadcast them to all connected clients simultaneously. # Objectives 1. **Create a Chat Server**: - Implement an asynchronous function, `start_server`, that initializes the server. - The function should handle multiple client connections concurrently. - When a client sends a message, the server should broadcast it to all connected clients. 2. **Manage Client Connections**: - Implement an asynchronous function, `handle_client`, to manage communication with each connected client. - The function should read messages from a client, and upon receiving a message, it should broadcast it to all other clients. 3. **Broadcast Messages**: - Implement an asynchronous function, `broadcast_message`, to send the message to all connected clients asynchronously. # Requirements - Use the `asyncio` library to handle asynchronous tasks. - Use asyncio synchronization primitives to manage shared resources (e.g., a list of connected clients). - Ensure the server can handle multiple clients connecting and disconnecting gracefully. - Provide appropriate error handling to manage client disconnections and other potential issues. # Constraints - The server should run indefinitely, and only stop if manually interrupted. - Assume messages sent by clients are short text strings. - Maximum number of clients: 100 - Clients may connect and disconnect at any time. # Input and Output Formats - **Input**: There isn\'t any direct input as the server should handle incoming connections and messages in real-time. - **Output**: Messages received from one client should be broadcasted to all other connected clients. # Example ```python import asyncio async def handle_client(reader, writer): # Function to handle individual client communication pass async def broadcast_message(message, clients): # Function to broadcast a message to all connected clients pass async def start_server(): # Function to initialize and start the chat server pass if __name__ == \\"__main__\\": asyncio.run(start_server()) ``` # Hints 1. Use `asyncio.start_server` to start the server and accept connections. 2. Use `asyncio.Queue` or other synchronization primitives to manage message broadcasting. 3. Use `writer`\'s transport to send data to clients and ensure proper exception handling.","solution":"import asyncio import traceback clients = set() async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"{addr} is connected\\") clients.add(writer) try: while True: data = await reader.read(100) # Customize the buffer size as needed if not data: break message = data.decode().strip() print(f\\"Received {message} from {addr}\\") await broadcast_message(f\\"{addr} says: {message}\\", clients) except Exception as e: print(f\\"Error handling client {addr}: {e}\\") traceback.print_exc() finally: print(f\\"{addr} is disconnected\\") clients.remove(writer) writer.close() await writer.wait_closed() async def broadcast_message(message, clients): for client in clients: try: client.write((message + \'n\').encode()) await client.drain() except Exception as e: print(f\\"Error sending message to client: {e}\\") traceback.print_exc() async def start_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\\"Serving on {addr}\\") async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(start_server())"},{"question":"# Question: Complex Number Operations You are required to implement a Python function that performs basic arithmetic operations on complex numbers. The function should take two complex numbers as arguments and a string representing the operation to be performed. The operations supported are addition, subtraction, multiplication, and division. Your function should also handle invalid operations with appropriate exception handling and use a context manager to ensure resource cleanup (even though there might be no physical resource to clean up, this will test your understanding of the `with` statement). In addition, you should appropriately handle cases where the division operation results in dividing by zero, which should raise an appropriate custom exception. # Instructions 1. Implement a custom exception called `DivisionByZeroError`. 2. Create a context manager class `OperationLogger` to simulate resource management (this can be as simple as printing entering and exiting logs). 3. Implement the function `complex_operations(c1: complex, c2: complex, operation: str) -> complex` that: - Takes two complex numbers, `c1` and `c2`. - Takes a string `operation` that specifies one of the operations: \\"add\\", \\"subtract\\", \\"multiply\\", \\"divide\\". - Uses your `OperationLogger` as a context manager to perform the operation. - Uses a try-except block to handle invalid operations and division by zero, raising the custom `DivisionByZeroError` where necessary. - Returns the result of the operation as a complex number. - Uses an `if-elif-else` structure to determine the operation to perform. - Includes a guard against invalid operation strings, raising a `ValueError` for any operation not specified above. # Example Usage ```python class DivisionByZeroError(Exception): pass class OperationLogger: def __enter__(self): print(\\"Entering the context\\") return self def __exit__(self, exc_type, exc_val, exc_tb): print(\\"Exiting the context\\") return False def complex_operations(c1: complex, c2: complex, operation: str) -> complex: if operation not in {\\"add\\", \\"subtract\\", \\"multiply\\", \\"divide\\"}: raise ValueError(f\\"Invalid operation {operation}\\") with OperationLogger(): try: if operation == \\"add\\": result = c1 + c2 elif operation == \\"subtract\\": result = c1 - c2 elif operation == \\"multiply\\": result = c1 * c2 elif operation == \\"divide\\": if c2 == 0: raise DivisionByZeroError(\\"Cannot divide by zero\\") result = c1 / c2 except DivisionByZeroError as e: print(e) return None except Exception as e: print(f\\"An error occurred: {e}\\") return None else: return result # Test cases print(complex_operations(1 + 2j, 3 + 4j, \\"add\\")) # Expected: (4 + 6j) print(complex_operations(1 + 2j, 3 + 4j, \\"subtract\\")) # Expected: (-2 - 2j) print(complex_operations(1 + 2j, 3 + 4j, \\"multiply\\")) # Expected: (-5 + 10j) print(complex_operations(1 + 2j, 3 + 0j, \\"divide\\")) # Expected: (0.5384615384615384 + 0.7692307692307693j) print(complex_operations(1 + 2j, 0 + 0j, \\"divide\\")) # Expected: None (with DivisionByZeroError) print(complex_operations(1 + 2j, 3 + 4j, \\"unknown\\")) # Expected: ValueError ``` # Constraints - You must use the given context manager and exception handling structures. - Ensure your code is clean, readable, and well-documented. - Your functionâ€™s performance should be acceptable for basic arithmetic operations on complex numbers.","solution":"class DivisionByZeroError(Exception): Custom exception for division by zero in complex number operations pass class OperationLogger: Context manager for logging the beginning and end of operations def __enter__(self): print(\\"Entering the context\\") return self def __exit__(self, exc_type, exc_val, exc_tb): print(\\"Exiting the context\\") # Returning False means that any exception that occurred will be propagated return False def complex_operations(c1: complex, c2: complex, operation: str) -> complex: Performs arithmetic operations on two complex numbers. Args: - c1: First complex number. - c2: Second complex number. - operation: A string specifying the operation (\\"add\\", \\"subtract\\", \\"multiply\\", \\"divide\\"). Returns: - The result of the operation as a complex number. Raises: - DivisionByZeroError: If attempting to divide by zero. - ValueError: If an invalid operation is specified. if operation not in {\\"add\\", \\"subtract\\", \\"multiply\\", \\"divide\\"}: raise ValueError(f\\"Invalid operation {operation}\\") with OperationLogger(): try: if operation == \\"add\\": result = c1 + c2 elif operation == \\"subtract\\": result = c1 - c2 elif operation == \\"multiply\\": result = c1 * c2 elif operation == \\"divide\\": if c2 == 0: raise DivisionByZeroError(\\"Cannot divide by zero\\") result = c1 / c2 except DivisionByZeroError as e: print(e) raise except Exception as e: print(f\\"An error occurred: {e}\\") raise return result"},{"question":"Objective: Implement a function that interacts with Python\'s integer type, focusing on key conversion methods between Python integers and C-like numeric types. Background: Python\'s integer type (`int`) is implemented as an arbitrary precision integer, `PyLongObject`, in the C layer. Several functions allow conversion between `PyLongObject` and various C numeric types for efficient computations. Your task is to implement functions that leverage this underlying API to perform specific integer manipulations and conversions. Requirements: 1. **Task 1:** Implement `convert_to_pylong` which takes an integer or float and converts it to a Python integer. 2. **Task 2:** Implement `get_long_value` which retrieves a C long representation from a Python integer. 3. **Task 3:** Implement `sum_pylongs` which takes a list of integers or floats, converts them all to Python integers, and returns their sum using Python\'s integer operations. Function Definitions: 1. `convert_to_pylong(value)` - **Input:** - `value` (int or float): A numeric value to be converted to a Python integer. - **Output:** - Returns a Python integer representation of the input value. - **Constraints:** - Raise `ValueError` if the input is not an int or float. 2. `get_long_value(pylong)` - **Input:** - `pylong` (int): A Python integer. - **Output:** - Returns the C long representation of the Python integer. - **Constraints:** - Raise `OverflowError` if the value cannot be represented as a C long. 3. `sum_pylongs(values)` - **Input:** - `values` (list of int or float): A list of numeric values. - **Output:** - Returns the sum of the values as a Python integer. - **Constraints:** - Raise `ValueError` if any value in the list is not an int or float. Example Usage: ```python def convert_to_pylong(value): if isinstance(value, (int, float)): if isinstance(value, int): return value return int(value) else: raise ValueError(\\"Input must be an int or float\\") def get_long_value(pylong): if isinstance(pylong, int): try: return pylong except OverflowError as e: raise e else: raise ValueError(\\"Input must be a Python integer\\") def sum_pylongs(values): if not all(isinstance(v, (int, float)) for v in values): raise ValueError(\\"All values must be int or float\\") return sum(int(v) for v in values) # Example print(convert_to_pylong(42.5)) # Expected: 42 print(get_long_value(1000000000000000)) # Expected: 1000000000000000 print(sum_pylongs([1, 2.5, 3])) # Expected: 6 ``` Notes: - Ensure your implementations raise appropriate errors when constraints are violated. - Test your functions with edge cases, such as large numbers that may cause overflow.","solution":"def convert_to_pylong(value): if isinstance(value, (int, float)): if isinstance(value, int): return value return int(value) else: raise ValueError(\\"Input must be an int or float\\") def get_long_value(pylong): if isinstance(pylong, int): try: if pylong > 2**63 - 1 or pylong < -2**63: raise OverflowError(\\"Value too large for C long representation\\") return pylong except OverflowError as e: raise e else: raise ValueError(\\"Input must be a Python integer\\") def sum_pylongs(values): if not all(isinstance(v, (int, float)) for v in values): raise ValueError(\\"All values must be int or float\\") return sum(int(v) for v in values)"},{"question":"**Objective:** The goal of this exercise is to evaluate your ability to manipulate and analyze data using the `pandas` Series object. You will be provided with a Series containing information about daily temperatures. Your task is to implement a function that processes this data in several steps to extract meaningful information. **Task:** Write a function `analyze_temperatures` that takes a pandas Series `temperatures` as input. This Series represents daily temperatures recorded over a year (365 values). The function should perform the following steps: 1. **Data Cleaning**: Replace any missing values (`NaN`) with the average temperature of the preceding and following days. If the missing value is at the start or end of the Series, use the closest non-missing value instead. 2. **Basic Statistics**: Compute the mean, median, and standard deviation of the cleaned temperatures. 3. **Temperature Ranges**: - Determine the number of days when the temperature was below 0Â°C (freezing) and above 30Â°C (hot). - Identify the month with the highest average temperature and the month with the lowest average temperature. 4. **Rolling Averages**: Calculate a rolling mean of the temperatures with a window size of 7 days. 5. **Visualization**: Generate a line plot of the cleaned daily temperatures along with the 7-day rolling average on the same plot. **Function Signature:** ```python import pandas as pd def analyze_temperatures(temperatures: pd.Series) -> None: pass ``` **Input:** - `temperatures`: A pandas Series representing daily temperatures over a year (length 365). The index of the Series is the date (format \'YYYY-MM-DD\'). **Output:** - The function should not return anything but should generate a line plot and print the following statistics to the console: - Mean temperature - Median temperature - Standard deviation of temperatures - Number of freezing days (temperature < 0Â°C) - Number of hot days (temperature > 30Â°C) - Month with the highest average temperature - Month with the lowest average temperature **Constraints:** - The input Series may contain missing values (`NaN`), and you must handle them as described. **Example:** ```python import pandas as pd import numpy as np # Generate a random temperature Series with missing values dates = pd.date_range(start=\'2022-01-01\', periods=365, freq=\'D\') temperatures = pd.Series(np.random.uniform(-5, 35, size=365), index=dates) temperatures.iloc[[50, 100, 150, 200]] = np.nan # Introducing missing values # Call the function analyze_temperatures(temperatures) ``` **Notes:** - Ensure your solution is efficient and leverages `pandas` operations effectively. - You may use additional helper functions if necessary. - The function should be able to handle edge cases, such as when there are missing values at the beginning or end of the Series.","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt def analyze_temperatures(temperatures: pd.Series) -> None: # Data Cleaning temperatures = temperatures.copy() temperatures[temperatures.isna()] = ( temperatures.fillna(method=\'ffill\').fillna(method=\'bfill\') )[temperatures.isna()] # Basic Statistics mean_temp = temperatures.mean() median_temp = temperatures.median() std_temp = temperatures.std() # Temperature Ranges freezing_days = temperatures[temperatures < 0].count() hot_days = temperatures[temperatures > 30].count() # Determine month with highest/lowest average temperature temperatures_monthly = temperatures.resample(\'M\').mean() highest_avg_temp_month = temperatures_monthly.idxmax().month lowest_avg_temp_month = temperatures_monthly.idxmin().month # Rolling Averages rolling_mean = temperatures.rolling(window=7).mean() # Print Statistics print(f\\"Mean temperature: {mean_temp:.2f}Â°C\\") print(f\\"Median temperature: {median_temp:.2f}Â°C\\") print(f\\"Standard deviation: {std_temp:.2f}Â°C\\") print(f\\"Number of freezing days: {freezing_days} days\\") print(f\\"Number of hot days: {hot_days} days\\") print(f\\"Month with highest average temperature: {highest_avg_temp_month}\\") print(f\\"Month with lowest average temperature: {lowest_avg_temp_month}\\") # Visualization plt.figure(figsize=(14, 7)) plt.plot(temperatures, label=\'Daily Temperatures\') plt.plot(rolling_mean, label=\'7-Day Rolling Mean\', color=\'orange\') plt.xlabel(\'Date\') plt.ylabel(\'Temperature (Â°C)\') plt.title(\'Daily Temperatures Over a Year\') plt.legend() plt.show()"},{"question":"**Question: Creating Advanced Faceted Plots with Seaborn** You are provided with a dataset containing information about different species of penguins. Your task is to utilize the Seaborn library to create a series of faceted plots that demonstrate your understanding of the library\'s advanced plotting capabilities. # Instructions: 1. **Load the Dataset:** Use the Seaborn `load_dataset` function to load the `penguins` dataset. 2. **Plot 1: Facet by Species** Create a scatter plot showing the relationship between `bill_length_mm` and `bill_depth_mm`, faceted by the `species` column. Each subplot should represent one species of penguin. 3. **Plot 2: Facet by Species and Sex** Create a grid of scatter plots showing the relationship between `bill_length_mm` and `bill_depth_mm`, faceted by both the `species` and `sex` columns. 4. **Plot 3: Ordered Facets** Create a scatter plot showing the relationship between `bill_length_mm` and `bill_depth_mm`, faceted by the `species` column. Order the subplots to display only the \\"Gentoo\\" and \\"Adelie\\" species. 5. **Plot 4: Wrapped Faceting** Create a scatter plot showing the relationship between `bill_length_mm` and `bill_depth_mm`, faceted by the `island` column. Wrap the subplots such that they are distributed across two rows. 6. **Plot 5: Customized Labels** Create a grid of scatter plots showing the relationship between `bill_length_mm` and `bill_depth_mm`, faceted by the `species` column. Customize the title of each subplot to append the string \\" species\\" to the species name (e.g., \\"Adelie species\\"). # Requirements: - Use the `seaborn.objects.Plot` class and its methods (`facet()`, `label()`, `add()`) effectively. - Ensure that each plot is correctly labeled and faceted as specified. - Include a brief comment for each plot explaining what the plot represents and any customization applied. # Example Output: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Plot 1: Facet by Species p1 = so.Plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\").add(so.Dots()) p1.facet(\\"species\\").label(title=\\"Penguin Species: {}\\") # Plot 2: Facet by Species and Sex p2 = so.Plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\").add(so.Dots()) p2.facet(\\"species\\", \\"sex\\").label(title=\\"{}\\") # Plot 3: Ordered Facets p3 = so.Plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\").add(so.Dots()) p3.facet(\\"species\\", order=[\\"Gentoo\\", \\"Adelie\\"]).label(title=\\"{}\\") # Plot 4: Wrapped Faceting p4 = so.Plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\").add(so.Dots()) p4.facet(\\"island\\", wrap=2).label(title=\\"Island: {}\\") # Plot 5: Customized Labels p5 = so.Plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\").add(so.Dots()) p5.facet(\\"species\\").label(title=\\"{} species\\") ``` Submit your code with comments to explain each plot and how it meets the specified requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Plot 1: Facet by Species # This plot faceted by species will show the scatter distribution of bill length and bill depth for each species. p1 = so.Plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\").add(so.Dots()) p1.facet(\\"species\\").label(title=\\"Penguin Species: {}\\").show() # Plot 2: Facet by Species and Sex # This plot creates a grid with facets by both species and sex, to explore the relationship between bill length and depth. p2 = so.Plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\").add(so.Dots()) p2.facet(\\"species\\", \\"sex\\").label(title=\\"{}\\").show() # Plot 3: Ordered Facets # This plot faceted by species, but only displays \\"Gentoo\\" and \\"Adelie\\" species in a specific order. p3 = so.Plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\").add(so.Dots()) p3.facet(\\"species\\", order=[\\"Gentoo\\", \\"Adelie\\"]).label(title=\\"{}\\").show() # Plot 4: Wrapped Faceting # This plot facets by island and wraps the subplots into two rows, showing how bill length and depth differ across islands. p4 = so.Plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\").add(so.Dots()) p4.facet(\\"island\\", wrap=2).label(title=\\"Island: {}\\").show() # Plot 5: Customized Labels # This plot faceted by species with customized titles for each subplot to add \\" species\\" after the species name. p5 = so.Plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\").add(so.Dots()) p5.facet(\\"species\\").label(title=\\"{} species\\").show()"},{"question":"**Python Coding Assessment Question** # Objective Implement a Python function that mimics part of the Argument Clinic\'s functionality as described in the provided documentation. Specifically, your function should process a string containing a representation of a function signature and generate the corresponding Python code. # Description Write a function `generate_python_function` that takes a string containing the function name, parameters, and a multiline docstring. Your function should parse this string and output a Python function definition with the corresponding docstring and parameters. # Input - A string `func_definition` containing the function definition, parameters, and a multiline docstring. - The input string format: ``` function_name param1: type1 = default_value1 param2: type2 ... Multiline Docstring ``` - Example: ``` my_function x: int = 10 name: str active: bool = True This is a sample function. This function demonstrates the use of Argument Clinic in Python. ``` # Output - A string containing the formatted Python function definition. - Should include parameters and their default values. - Should include the provided docstring. - Example output for the given input example: ```python def my_function(x: int = 10, name: str, active: bool = True): This is a sample function. This function demonstrates the use of Argument Clinic in Python. pass ``` # Constraints 1. Assume all types in the input are valid Python types. 2. Default values are properly formatted. 3. The input string will have a function name followed by valid Python parameter declarations and a standard multiline string for the docstring. # Function Signature ```python def generate_python_function(func_definition: str) -> str: pass ``` # Example ```python input_str = my_function x: int = 10 name: str active: bool = True \\"\\"\\"This is a sample function. This function demonstrates the use of Argument Clinic in Python.\\"\\"\\" print(generate_python_function(input_str)) ``` # Expected Output ``` def my_function(x: int = 10, name: str, active: bool = True): This is a sample function. This function demonstrates the use of Argument Clinic in Python. pass ``` Complete the function implementation to pass the sample test case and other edge cases.","solution":"def generate_python_function(func_definition: str) -> str: parts = func_definition.strip().split(\\"n\\") func_name = parts[0].strip() # Identify the line where the docstring starts doc_start_index = None for i, line in enumerate(parts[1:]): if line.strip().startswith(\'\'): doc_start_index = i + 1 break # Parameters and types (until docstring starts) param_lines = parts[1:doc_start_index] params = [p.strip() for p in param_lines if p.strip()] # Docstring extraction & formatting doc_lines = parts[doc_start_index:] docstring = \\"n\\".join(doc_lines).strip() param_str = \\", \\".join(params) function_def = f\\"def {func_name}({param_str}):n\\" function_def += f\\" {docstring}n\\" function_def += f\\" pass\\" return function_def"},{"question":"**Task**: Write a Python program implementing a server-client model using the `socket` module. The server should accept both IPv4 and IPv6 connections and process them in a way that demonstrates your understanding of the `socket` module. # Server Requirements: 1. **Socket Type**: Use a stream socket to handle TCP connections. 2. **Address Families**: Support both `AF_INET` (IPv4) and `AF_INET6` (IPv6). 3. **Exception Handling**: Your server should gracefully handle common socket exceptions. 4. **Concurrent Connections**: Use Python\'s `threading` module to handle multiple client connections concurrently. 5. **Functionality**: The server should echo any received data back to the client. # Client Requirements: 1. **Socket Type**: Also use a stream socket for TCP connections. 2. **Address Families**: The client should be able to connect to both IPv4 and IPv6 server addresses. 3. **Exception Handling**: Your client should gracefully handle socket exceptions. 4. **Functionality**: The client should send a message to the server and print the echoed reply. # Input/Output Specifications: - **Server Input**: None, the server listens on a specified port. - **Client Input**: A message string to be sent to the server. - **Output**: The server prints the client\'s address and the received message. The client prints the echoed message received from the server. # Constraints: - Use port 65432 (or any arbitrary non-privileged port). - Ensure proper cleanup of socket resources after usage. # Example: **Server Output (sample)**: ``` Server started on (\'::\', 65432) Connected to (\'::1\', 49490, 0, 0) Received message: \'Hello, Server!\' ``` **Client Output (sample)**: ``` Connected to (IPv6 server address) Sent: \'Hello, Server!\' Received: \'Hello, Server!\' ``` # Performance: - The server should respond to the client within 1 second upon sending a message. # Hints: - Use `socket.getaddrinfo()` to support dual-stack servers. - Use `threading.Thread` to handle each client connection in a separate thread. Implement the code for both the server and the client. Ensure comprehensive exception handling and resource management (like closing sockets after communication).","solution":"import socket import threading def handle_client(client_socket, client_address): print(f\'Connected to {client_address}\') try: while True: data = client_socket.recv(1024) if not data: break print(f\\"Received message: \'{data.decode()}\'\\") client_socket.sendall(data) except socket.error as e: print(f\\"Socket error: {e}\\") finally: client_socket.close() def start_server(host, port): server_sock = None for res in socket.getaddrinfo(host, port, socket.AF_UNSPEC, socket.SOCK_STREAM, 0, socket.AI_PASSIVE): af, socktype, proto, canonname, sa = res try: server_sock = socket.socket(af, socktype, proto) except socket.error as msg: server_sock = None continue try: server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_sock.bind(sa) server_sock.listen(5) except socket.error as msg: server_sock.close() server_sock = None continue break if server_sock is None: print(\'Could not open socket\') return print(f\'Server started on {sa}\') try: while True: client_socket, client_address = server_sock.accept() client_handler = threading.Thread(target=handle_client, args=(client_socket, client_address)) client_handler.start() except KeyboardInterrupt: print(\\"Server is shutting down.\\") finally: server_sock.close() def start_client(server_host, server_port, message): client_sock = None for res in socket.getaddrinfo(server_host, server_port, socket.AF_UNSPEC, socket.SOCK_STREAM): af, socktype, proto, canonname, sa = res try: client_sock = socket.socket(af, socktype, proto) client_sock.connect(sa) except socket.error as msg: if client_sock: client_sock.close() client_sock = None continue break if client_sock is None: print(\'Could not open socket\') return try: print(f\'Connected to {sa}\') client_sock.sendall(message.encode()) print(f\\"Sent: \'{message}\'\\") received = client_sock.recv(1024) print(f\\"Received: \'{received.decode()}\'\\") except socket.error as e: print(f\\"Socket error: {e}\\") finally: client_sock.close() # Example usage: # In one terminal, start the server: # start_server(\'\', 65432) # In another terminal, start the client: # start_client(\'localhost\', 65432, \'Hello, Server!\')"},{"question":"# User Authentication System with `getpass` Module You are required to implement a secure user authentication system utilizing the `getpass` module. Your task is to create a setup where users can input their usernames and passwords securely. Depending on the authenticated user\'s role, the system will grant access to different functionalities. Requirements: 1. **User Authentication**: - Prompt the user to enter their username and password using `getpass.getuser()` and `getpass.getpass()` respectively. - Simulate checking these credentials against a predefined list of users stored in a dictionary (see below for the dictionary structure). 2. **Role-Based Access Control**: - Each user has an associated role (e.g., \'admin\', \'user\'). - Implement two basic functionalities: `admin_function()` and `user_function()`. - `admin_function()`: Accessible only by users with the \'admin\' role. - `user_function()`: Accessible by users with either \'admin\' or \'user\' roles. 3. **Configuration**: - Assume the user data is hardcoded in your script as follows: ```python users = { \'admin_user\': {\'password\': \'admin_pass\', \'role\': \'admin\'}, \'regular_user\': {\'password\': \'user_pass\', \'role\': \'user\'} } ``` Implementation: 1. Write a function `authenticate_user()` that: - Prompts the user to enter their username and password. - Validates the entered credentials against the `users` dictionary. - Returns the role of the user if authentication is successful, otherwise returns `None`. 2. Write a function `main()` that: - Calls `authenticate_user()` to authenticate the user. - Depending on the returned role, calls `admin_function()` if the user is an admin, or `user_function()` if the user is a regular user. - Prints an appropriate message if authentication fails. 3. Implement the `admin_function()` and `user_function()`: - `admin_function()`: Prints \\"Access granted: Admin functionality\\" - `user_function()`: Prints \\"Access granted: User functionality\\" Constraints: - Do not use any external libraries for user input/output. - Ensure that password input is handled securely using `getpass.getpass()`. Example Usage: ``` Enter username: admin_user Password: (input hidden) Access granted: Admin functionality Enter username: regular_user Password: (input hidden) Access granted: User functionality Enter username: unknown_user Password: (input hidden) Authentication failed: Invalid credentials ``` Write the necessary functions and make sure the above requirements are met.","solution":"import getpass users = { \'admin_user\': {\'password\': \'admin_pass\', \'role\': \'admin\'}, \'regular_user\': {\'password\': \'user_pass\', \'role\': \'user\'} } def authenticate_user(): username = getpass.getuser(prompt=\'Enter username: \') password = getpass.getpass(prompt=\'Password: \') if username in users and users[username][\'password\'] == password: return users[username][\'role\'] else: return None def admin_function(): print(\\"Access granted: Admin functionality\\") def user_function(): print(\\"Access granted: User functionality\\") def main(): role = authenticate_user() if role == \'admin\': admin_function() elif role == \'user\': user_function() else: print(\\"Authentication failed: Invalid credentials\\")"},{"question":"# Python Coding Assessment Question: Secure Password Hashing and Verification Objective This task assesses your ability to use the `crypt` module for secure password hashing and verification. Instructions You are implementing a secure password handling system for a Unix-based application. You must write two functions: 1. `hash_password(password: str) -> str`: This function should hash the given password using the strongest available method and return the hashed password. It should use a randomly generated salt. 2. `check_password(password: str, hashed: str) -> bool`: This function should: - Take a plain-text password and a hashed password. - Verify if the plain-text password matches the hashed password. - Return `True` if they match, otherwise `False`. Function Signatures ```python def hash_password(password: str) -> str: pass def check_password(password: str, hashed: str) -> bool: pass ``` Constraints - The password input given to `hash_password` and `check_password` will only contain printable ASCII characters. - The length of the password will be between 1 and 128 characters. Requirements - Use the `crypt.crypt` function to perform hashing and verification. - Generate the salt using `crypt.mksalt()` within the `hash_password` function. Example ```python plaintext = \\"securepassword\\" hashed = hash_password(plaintext) assert check_password(plaintext, hashed) == True assert check_password(\\"wrongpassword\\", hashed) == False ``` Performance - Solutions should handle password hashing and checking efficiently within a reasonable time.","solution":"import crypt import random import string def hash_password(password: str) -> str: Hash the given password using the strongest available method and a random salt. salt = crypt.mksalt(crypt.METHOD_SHA512) hashed = crypt.crypt(password, salt) return hashed def check_password(password: str, hashed: str) -> bool: Verify if the plain-text password matches the hashed password. return crypt.crypt(password, hashed) == hashed"},{"question":"# Asynchronous Task Manager Objective You are required to implement a simple task manager using `asyncio.Future`. The task manager will manage asynchronous tasks, allow adding tasks to the queue, and handle their execution. Additionally, it should support setting callbacks to notify when tasks are done and retrieving results or exceptions. Requirements 1. **Function Signature**: ```python import asyncio from typing import List, Any, Callable class TaskManager: def __init__(self): pass def add_task(self, coroutine: Callable, *args) -> asyncio.Future: pass async def run_tasks(self) -> None: pass def set_done_callback(self, future: asyncio.Future, callback: Callable) -> None: pass def get_result(self, future: asyncio.Future) -> Any: pass def get_exception(self, future: asyncio.Future) -> Exception: pass ``` 2. **Functionality**: - `__init__(self)`: Initializes the TaskManager with an empty task list. - `add_task(self, coroutine: Callable, *args) -> asyncio.Future`: Adds a coroutine to the task list and returns the associated Future object which will be awaited. - `async def run_tasks(self) -> None`: Runs all tasks concurrently and waits for them to complete. - `set_done_callback(self, future: asyncio.Future, callback: Callable) -> None`: Sets a callback function to be invoked when the Future associated with a task is done. - `get_result(self, future: asyncio.Future) -> Any`: Returns the result of the Future if it is done, raises `asyncio.InvalidStateError` if the Future is not done. - `get_exception(self, future: asyncio.Future) -> Exception`: Returns the exception set on the Future if it is done, raises `asyncio.InvalidStateError` if the Future is not done. 3. **Example Usage**: ```python async def sample_task(delay, result): await asyncio.sleep(delay) return result async def main(): manager = TaskManager() future1 = manager.add_task(sample_task, 1, \'Task 1 Complete\') future2 = manager.add_task(sample_task, 2, \'Task 2 Complete\') def on_done(fut): print(f\'Task result: {fut.result()}\') manager.set_done_callback(future1, on_done) manager.set_done_callback(future2, on_done) await manager.run_tasks() asyncio.run(main()) ``` Constraints - All tasks added should be coroutines. - Ensure that the task manager can manage multiple tasks concurrently. - Handle exceptions within tasks appropriately and ensure that they can be retrieved later. Performance - Tasks should be managed efficiently with minimal overhead. - Ensure tasks run concurrently without blocking each other.","solution":"import asyncio from typing import List, Any, Callable class TaskManager: def __init__(self): self.tasks = [] def add_task(self, coroutine: Callable, *args) -> asyncio.Future: task = asyncio.ensure_future(coroutine(*args)) self.tasks.append(task) return task async def run_tasks(self) -> None: await asyncio.gather(*self.tasks) def set_done_callback(self, future: asyncio.Future, callback: Callable) -> None: future.add_done_callback(callback) def get_result(self, future: asyncio.Future) -> Any: if future.done(): return future.result() else: raise asyncio.InvalidStateError(\\"Future is not done yet\\") def get_exception(self, future: asyncio.Future) -> Exception: if future.done(): return future.exception() else: raise asyncio.InvalidStateError(\\"Future is not done yet\\")"},{"question":"You are required to implement a Python function `fetch_secure_data` that performs the following operations: 1. **Fetch Data with Authentication**: - Make a POST request to a given URL that requires basic authentication. - Send the necessary credentials (username and password) provided as arguments. 2. **Send Data**: - Encode and send the data provided as a dictionary argument using POST request. - Include headers with User-Agent specified as \'Python-urllib/3.10\'. 3. **Handle Response and Errors**: - Read the response content. If the response status code indicates an error (i.e., in the 400â€“599 range), raise an `HTTPError`. - If there\'s an issue with network connectivity, raise a `URLError`. 4. **Return Data**: - If the request is successful, return the decoded response content as a string. Function Signature: ```python def fetch_secure_data(url: str, username: str, password: str, data: dict) -> str: pass ``` # Input: - `url` (str): The URL to which the request is to be made. - `username` (str): The username for authentication. - `password` (str): The password for authentication. - `data` (dict): A dictionary containing data to be sent in the POST request. # Output: - (str): The decoded response content as a string. # Constraints: - Use the `urllib.request` module for making HTTP requests. - Handle `HTTPError` and `URLError` appropriately. - Set the User-Agent header to \'Python-urllib/3.10\'. - Ensure proper handling of HTTP status codes that indicate an error. # Example: ```python url = \\"http://example.com/api/data\\" username = \\"user1\\" password = \\"pass123\\" data = {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"} try: response = fetch_secure_data(url, username, password, data) print(\\"Response:\\", response) except Exception as e: print(\\"An error occurred:\\", e) ``` In the example above, upon successful data fetch, the function should print the decoded response, otherwise, it will handle and notify the type of error encountered.","solution":"import urllib.request import urllib.parse import urllib.error import base64 def fetch_secure_data(url: str, username: str, password: str, data: dict) -> str: Fetch secure data from the URL with given credentials and data. :param url: URL to which request is to be made :param username: Username for authentication :param password: Password for authentication :param data: Dictionary containing data to be sent in POST request :return: Decoded response content as a string # Encode the data to be sent in the POST request post_data = urllib.parse.urlencode(data).encode(\'utf-8\') # Create a request object with the given URL and encoded data req = urllib.request.Request(url, data=post_data) # Add the User-Agent header req.add_header(\'User-Agent\', \'Python-urllib/3.10\') # Add Basic Authentication header credentials = f\\"{username}:{password}\\" encoded_credentials = base64.b64encode(credentials.encode(\'ascii\')).decode(\'ascii\') req.add_header(\'Authorization\', f\'Basic {encoded_credentials}\') try: # Perform the request and handle the response with urllib.request.urlopen(req) as response: # Read and decode the response content return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: # Handle HTTP errors raise urllib.error.HTTPError(url, e.code, e.reason, e.headers, e.fp) except urllib.error.URLError as e: # Handle URL errors raise urllib.error.URLError(e.reason)"},{"question":"Objective Demonstrate your understanding of Python\'s import mechanisms by writing a function that imports a module from a given ZIP file and executes a specified function from that module. Problem Statement You need to write a function `import_and_run_from_zip(zip_path: str, module_name: str, func_name: str, *args, **kwargs) -> Any` that: 1. Takes the path of a ZIP file containing Python modules (`zip_path`). 2. Imports a specified module (`module_name`) from the ZIP file. 3. Invokes a specified function (`func_name`) from the imported module. 4. Passes any given positional (`*args`) and keyword arguments (`**kwargs`) to the function. 5. Returns the result of the function call. Input - `zip_path` (str): The file path to a ZIP archive containing Python modules. - `module_name` (str): The name of the module to import from the ZIP archive. - `func_name` (str): The name of the function to call from the imported module. - `*args`: Positional arguments to pass to the function. - `**kwargs`: Keyword arguments to pass to the function. Output - `Any`: The result of the function call. Constraints - Assume that the ZIP file and the module to be imported are available and correctly structured. - Handle import errors gracefully by raising an appropriate exception with a custom message. Example Suppose `example.zip` contains a module `mymodule.py` with the following function: ```python def greet(name): return f\\"Hello, {name}!\\" ``` Calling the following function: ```python result = import_and_run_from_zip(\'example.zip\', \'mymodule\', \'greet\', \'Alice\') ``` Should return: ```python \\"Hello, Alice!\\" ``` Guidelines 1. Use the `zipimport` module to import the module from the ZIP file. 2. Use the `getattr()` function to retrieve the function from the imported module. 3. Handle potential errors such as import errors or attribute errors with informative exception messages. Performance Requirements - The function should be efficient in loading and invoking the function but does not need to handle extremely large ZIP files or modules for this assessment. ```python import zipimport def import_and_run_from_zip(zip_path: str, module_name: str, func_name: str, *args, **kwargs) -> Any: try: # Create a zipimporter instance for the given zip file zip_importer = zipimport.zipimporter(zip_path) # Import the specified module using the zip importer module = zip_importer.load_module(module_name) # Retrieve the specified function from the imported module func = getattr(module, func_name) # Call the function with the provided arguments and return the result return func(*args, **kwargs) except ImportError as e: raise ImportError(f\\"Error importing module {module_name} from {zip_path}: {e}\\") except AttributeError as e: raise AttributeError(f\\"Module {module_name} does not have a function named {func_name}: {e}\\") # Example usage # result = import_and_run_from_zip(\'example.zip\', \'mymodule\', \'greet\', \'Alice\') # print(result) # Output: \\"Hello, Alice!\\" ```","solution":"import zipimport def import_and_run_from_zip(zip_path: str, module_name: str, func_name: str, *args, **kwargs): try: # Create a zipimporter instance for the given zip file zip_importer = zipimport.zipimporter(zip_path) # Import the specified module using the zip importer module = zip_importer.load_module(module_name) # Retrieve the specified function from the imported module func = getattr(module, func_name) # Call the function with the provided arguments and return the result return func(*args, **kwargs) except ImportError as e: raise ImportError(f\\"Error importing module {module_name} from {zip_path}: {e}\\") except AttributeError as e: raise AttributeError(f\\"Module {module_name} does not have a function named {func_name}: {e}\\")"},{"question":"**Question: Visualizing and Customizing Data with Seaborn** As a data analyst, you\'re tasked with creating an informative visualization for exploring a dataset using seaborn. Your goal is to demonstrate proficiency with multiple seaborn features to create a comprehensive plot. You will use the \'tips\' dataset that comes with seaborn. # Task 1. Load the \'tips\' dataset from seaborn. 2. Create a scatter plot with \'total_bill\' on the x-axis and \'tip\' on the y-axis. 3. Use color hue mapping to represent the \'time\' variable. 4. Add a rug plot along both axes (\'total_bill\' on x and \'tip\' on y) with height (0.05). 5. Adjust the appearance of the rug plot by setting `alpha` to 0.7 and using thinner lines (`lw=0.8`). 6. Add a KDE plot on the same scatter plot for the \'total_bill\' variable. # Input No input from the user is required; you will use the seaborn \'tips\' dataset. # Output A visualization plot that includes all the specified customization and combinations. # Constraints - Ensure all plot elements are visible and not overlapping excessively. - Maintain the integrity of the plot (e.g., don\'t hide important features). # Example solution: ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Set the seaborn theme sns.set_theme() # Create a scatter plot with hue mapping to time ax = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") # Add a custom rug plot with height and appearance adjustments sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", height=0.05, alpha=0.7, lw=0.8, ax=ax) # Add a KDE plot for the \'total_bill\' variable sns.kdeplot(data=tips, x=\\"total_bill\\", ax=ax) # Display the plot plt.show() # Call the function to create the plot create_custom_plot() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Set the seaborn theme sns.set_theme() # Create a scatter plot with hue mapping to time ax = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") # Add a custom rug plot with height and appearance adjustments sns.rugplot(data=tips, x=\\"total_bill\\", height=0.05, alpha=0.7, lw=0.8, ax=ax) sns.rugplot(data=tips, y=\\"tip\\", height=0.05, alpha=0.7, lw=0.8, ax=ax) # Add a KDE plot for the \'total_bill\' variable sns.kdeplot(data=tips, x=\\"total_bill\\", ax=ax) # Display the plot plt.show() # Call the function to create the plot create_custom_plot()"},{"question":"**Objective:** The goal of this task is to test your understanding of PyTorch, particularly how to work with tensor operations considering TorchScript\'s limitations and how to use `torch.jit.trace` for unsupported constructs. **Problem Statement:** You are required to implement a PyTorch model that can perform simple operations on tensors and ensure it can be compiled with TorchScript. The model should include: 1. A method to initialize tensors with given shape and value. 2. A method to perform element-wise addition on two tensors. 3. A method to return the mean and variance of elements in the tensor. 4. Handling and demonstrating the use of `torch.jit.trace` for any unsupported operations. **Implementation Requirements:** 1. Implement a class `PyTorchModel` with the following methods: - `__init__(self)`: Initializes the model. - `create_tensor(self, shape, value, requires_grad)`: Creates a tensor with the specified shape and value. The `requires_grad` argument should not be used directly if unsupported in TorchScript. - `add_tensors(self, tensor1, tensor2)`: Performs element-wise addition of `tensor1` and `tensor2`. - `compute_stats(self, tensor)`: Returns the mean and variance of the elements in the given `tensor`. 2. Create an instance of `PyTorchModel` and demonstrate: - Creating tensors with and without `requires_grad`. - Adding two tensors. - Computing the mean and variance of a tensor. - Using `torch.jit.trace` to handle any unsupported operations if necessary. **Constraints:** - You must ensure compatibility with TorchScript, so take care of unsupported functionalities and use tracing if needed. - Inputs to tensor creation function will always be valid. Assume non-empty tensor shapes. - Performance should be reasonable and handle typical tensor sizes used in practice. **Input:** - For tensor creation: `shape` (tuple), `value` (numeric), and `requires_grad` (boolean). - For tensor addition: Two tensors of the same shape. - For compute stats: A single tensor. **Output:** - Tensor creation: A PyTorch tensor. - Tensor addition: A PyTorch tensor resulting from element-wise addition. - Compute stats: A tuple containing the mean and variance of the tensor elements. --- **Example:** ```python model = PyTorchModel() tensor1 = model.create_tensor((2, 3), 1.0, requires_grad=True) tensor2 = model.create_tensor((2, 3), 2.0, requires_grad=False) result_tensor = model.add_tensors(tensor1, tensor2) mean, var = model.compute_stats(result_tensor) print(f\\"Result Tensor:n{result_tensor}\\") print(f\\"Mean: {mean}, Variance: {var}\\") ``` **Expected Output:** ``` Result Tensor: tensor([[3., 3., 3.], [3., 3., 3.]], grad_fn=<AddBackward0>) Mean: 3.0, Variance: 0.0 ``` **Note:** - When dealing with unsupported operations in TorchScript, you can refer to the PyTorch documentation and appropriately use tracing with `torch.jit.trace`.","solution":"import torch class PyTorchModel: def __init__(self): pass def create_tensor(self, shape, value, requires_grad): tensor = torch.full(shape, value, requires_grad=requires_grad) return tensor def add_tensors(self, tensor1, tensor2): return tensor1 + tensor2 def compute_stats(self, tensor): mean = torch.mean(tensor) var = torch.var(tensor) return mean, var # Example usage if __name__ == \\"__main__\\": model = PyTorchModel() tensor1 = model.create_tensor((2, 3), 1.0, requires_grad=True) tensor2 = model.create_tensor((2, 3), 2.0, requires_grad=False) result_tensor = model.add_tensors(tensor1, tensor2) mean, var = model.compute_stats(result_tensor) print(f\\"Result Tensor:n{result_tensor}\\") print(f\\"Mean: {mean}, Variance: {var}\\") # Demonstrating torch.jit.trace for unsupported operations traced_add_tensors = torch.jit.trace(model.add_tensors, (tensor1, tensor2)) traced_compute_stats = torch.jit.trace(model.compute_stats, (result_tensor,)) # Example output using traced functions result_tensor_traced = traced_add_tensors(tensor1, tensor2) mean_traced, var_traced = traced_compute_stats(result_tensor_traced) print(f\\"Traced Result Tensor:n{result_tensor_traced}\\") print(f\\"Traced Mean: {mean_traced}, Traced Variance: {var_traced}\\")"},{"question":"Coding Assessment Question # Objective Your task is to implement a function that creates a schedule of tasks based on their dependencies using `itertools` and `collections` from Python\'s standard library. # Problem Statement You are given a list of tasks and their prerequisites, where each task is represented by a unique ID. Implement a function `create_schedule` that generates an order in which tasks can be completed. If a task depends on another task, it should come after the prerequisite task in the schedule. # Input - A list of tuples `tasks`, where each tuple consists of two elements `(task, prerequisite)`. - `task`: an integer representing the ID of the task. - `prerequisite`: an integer representing the ID of the prerequisite task. # Output - A list of integers representing the task IDs in an order that adheres to their prerequisite constraints. If no valid order exists, return `None`. # Constraints - A task can have multiple prerequisites. - There are no cyclic dependencies. - Each task ID is a unique positive integer. # Example ```python def create_schedule(tasks: List[Tuple[int, int]]) -> Union[List[int], None]: # Your implementation here # Example usage: tasks = [(1, 2), (2, 3), (3, 4)] print(create_schedule(tasks)) # Output: [4, 3, 2, 1] tasks = [(1, 2), (2, 3), (3, 1)] print(create_schedule(tasks)) # Output: None ``` # Requirements 1. Use the `collections` module to manage the graph of tasks and their prerequisites. 2. Use the `itertools` module if appropriate to simplify your code. 3. Ensure your implementation adheres to the time complexity constraints suitable for large lists of tasks. # Hints 1. Consider using `collections.defaultdict` to create an adjacency list. 2. Perform a topological sort to determine the correct order of tasks. Good luck!","solution":"from collections import defaultdict, deque from typing import List, Tuple, Union def create_schedule(tasks: List[Tuple[int, int]]) -> Union[List[int], None]: Generates an order in which tasks can be completed based on their prerequisites. If no valid order exists, returns None. # Create graph and in-degree counter graph = defaultdict(list) in_degree = defaultdict(int) # Build the graph and in-degree of each node for task, prerequisite in tasks: graph[prerequisite].append(task) in_degree[task] += 1 if prerequisite not in in_degree: in_degree[prerequisite] = 0 # Initialize queue with nodes having 0 in-degree queue = deque([node for node in in_degree if in_degree[node] == 0]) ordered_tasks = [] while queue: current = queue.popleft() ordered_tasks.append(current) for neighbor in graph[current]: in_degree[neighbor] -= 1 if in_degree[neighbor] == 0: queue.append(neighbor) # Check if valid schedule if len(ordered_tasks) == len(in_degree): return ordered_tasks else: return None"},{"question":"# Question: Custom REPL Emulator In this exercise, you need to implement a custom REPL (Read-Eval-Print Loop) emulator using the `codeop` module from Python. Your REPL should be able to take multiple lines of input, determine if they form a complete Python statement, and if so, compile and execute them. Your Task: 1. Implement a function `custom_repl()` which: - Continuously reads input from the user. - Uses the `codeop.compile_command` function to determine if the input forms a complete Python statement. - If the input forms a complete statement, compile and execute it. - If the input does not form a complete statement, continue reading more input lines. - If there are any syntax errors or other exceptions during the compilation or execution, catch and handle them appropriately. 2. You should also keep track of `__future__` statements so that they are honored in subsequent inputs using the `codeop.Compile` or `codeop.CommandCompiler` classes. Example: Your REPL should function like this: ```python >>> custom_repl() >>> print(\\"Hello, World!\\") Hello, World! >>> from __future__ import division >>> 3/2 1.5 >>> incomplete_input = for i in range(5): ... print(i) 0 1 2 3 4 >>> ``` Constraints: - Do not use the built-in `exec()` or `eval()` functions; rely only on `codeop` utilities for compilation and execution. - Your REPL should continue running until the user manually exits, for example by inputting an EOF (End-Of-File) character or a specific keyboard interrupt. This task will test your understanding of the `codeop` module, as well as your ability to handle dynamic input and manage state across multiple executions. Function Signature: ```python def custom_repl(): pass ```","solution":"import codeop import sys def custom_repl(): compiler = codeop.CommandCompiler() source_code = \\"\\" while True: try: prompt = \\">>> \\" if not source_code else \\"... \\" source_code += input(prompt) + \\"n\\" code = compiler(source_code) if code: exec(code) source_code = \\"\\" except (EOFError, KeyboardInterrupt): print(\\"nExiting REPL.\\") break except Exception as e: print(f\\"Error: {e}\\") source_code = \\"\\""},{"question":"You have been tasked with creating a utility function to process a list of user data within an application. This list can include nested lists, tuples, and dictionaries, and your function should be able to handle it robustly. The requirements are as follows: 1. **Function Name**: `process_user_data` 2. **Input**: A list containing mixed data types including nested lists, tuples, and dictionaries. 3. **Output**: A dictionary containing: - `total_users`: Total count of users processed. - `successfully_processed`: Count of users that were successfully processed. - `failed_processing`: Count of users that failed processing. - `error_messages`: A list containing error messages for failed processing attempts. # Specific Requirements and Constraints: 1. Each user data should follow the format: ```python { \'name\': \'Name of the user\', \'age\': User\'s age, \'address\': { \'city\': \'City name\', \'zipcode\': \'Zip code\' } } ``` 2. Your function should: - Skip over and count any incomplete/incorrect user data. - Raise an appropriate custom exception if the age is missing or invalid. - Ensure that the function does not terminate on encountering an error; rather, it should collect the errors. 3. Use at least three different types of statements from the documented categories such as: - Assignment statements. - Assert statements for validation within the function. - Yield statements to demonstrate intermediate status if necessary. - Import statement to include any standard library functions if needed. 4. Ensure to include: - A clear input validation step using assert statements. - Handling of exceptions using try-except blocks and custom exceptions. - Use looping constructs to process each user. # Example Usage: ```python user_data = [ {\'name\': \'Alice\', \'age\': 30, \'address\': {\'city\': \'Wonderland\', \'zipcode\': \'12345\'}}, {\'name\': \'Bob\', \'age\': None, \'address\': {\'city\': \'Builderland\', \'zipcode\': \'67890\'}}, # Missing age should be handled {\'name\': \'Charlie\', \'address\': {\'city\': \'Chocolate Factory\', \'zipcode\': \'54321\'}}, # Missing age should be handled [12, \'incorrect\', \'format\'], # Incorrect format should be skipped (\'tuple_user\', 25, \'yet_another_invalid_format\'), # Incorrect format should be skipped ] result = process_user_data(user_data) print(result) # Example Output: # { # \'total_users\': 5, # \'successfully_processed\': 1, # \'failed_processing\': 4, # \'error_messages\': [ # \'Error for user Bob: Missing or invalid age\', # \'Error for user Charlie: Missing or invalid age\', # \'Skipping invalid data: [12, \'incorrect\', \'format\']\', # \'Skipping invalid data: (\'tuple_user\', 25, \'yet_another_invalid_format\')\' # ] # } ``` Implement the function `process_user_data` fulfilling the above requirements.","solution":"class InvalidUserDataException(Exception): pass def process_user_data(user_data): assert isinstance(user_data, list), \\"Input should be a list\\" total_users = 0 successfully_processed = 0 failed_processing = 0 error_messages = [] for user in user_data: total_users += 1 try: if not isinstance(user, dict): raise InvalidUserDataException(f\\"Skipping invalid data: {user}\\") if \'age\' not in user or not isinstance(user[\'age\'], int): raise InvalidUserDataException(f\\"Error for user {user.get(\'name\', \'unknown\')}: Missing or invalid age\\") successfully_processed += 1 except InvalidUserDataException as e: error_messages.append(str(e)) failed_processing += 1 return { \'total_users\': total_users, \'successfully_processed\': successfully_processed, \'failed_processing\': failed_processing, \'error_messages\': error_messages }"},{"question":"# Python Coding Assessment Question Problem Statement Using the `runpy` module, implement a script that dynamically runs Python modules or scripts located within a filesystem directory. This script should allow the user to specify a directory, search for all Python scripts (`*.py`) in that directory, and execute them in isolation, capturing their output and any exceptions raised. Requirements 1. The script should be implemented as a function, `execute_scripts_in_directory(directory: str) -> dict`. 2. This function should take a string parameter `directory` that specifies the path to the directory to be searched for `.py` files. 3. The function should execute each Python script in the provided directory using `runpy.run_path`. 4. The function should return a dictionary where: - The keys are the filenames of the executed scripts. - The values are a tuple containing two elements: - The first element is the output of the script\'s global variables after execution. - The second element is any exception message if an exception was raised during execution (otherwise, it should be `None`). Constraints - You should assume that the directory contains only valid Python scripts. - The execution environment should remain isolated for each script. - You cannot use multi-threading due to the thread-safety warnings in the `runpy` documentation. Example ```python import os def execute_scripts_in_directory(directory: str) -> dict: # Your implementation here # Example usage results = execute_scripts_in_directory(\\"/path/to/scripts\\") for filename, (globals_dict, error) in results.items(): if error is None: print(f\\"{filename} executed successfully with globals: {globals_dict}\\") else: print(f\\"{filename} failed with error: {error}\\") ``` Given the directory `/path/to/scripts` containing: 1. `script1.py` ```python x = 10 y = 20 result = x + y ``` 2. `script2.py` ```python raise ValueError(\\"An error occurred\\") ``` Your output might be: ```python { \'script1.py\': ({\'x\': 10, \'y\': 20, \'result\': 30, \'__name__\': \'<run_path>\', \'__file__\': \'/path/to/scripts/script1.py\'}, None), \'script2.py\': (None, \'ValueError: An error occurred\') } ``` Notes - Ensure proper error handling to capture and store any exceptions raised. - Use appropriate methods from `os` and `runpy` modules to find and execute scripts.","solution":"import os import runpy def execute_scripts_in_directory(directory: str) -> dict: results = {} for filename in os.listdir(directory): if filename.endswith(\\".py\\"): filepath = os.path.join(directory, filename) try: globals_dict = runpy.run_path(filepath) results[filename] = (globals_dict, None) except Exception as e: results[filename] = (None, f\\"{type(e).__name__}: {str(e)}\\") return results"},{"question":"**Objective:** Demonstrate your understanding of the `asyncio` event loop and related APIs by implementing a mini TCP server and client that communicate with each other asynchronously. **Requirements:** - Implement an asynchronous TCP server using `asyncio` that listens on a specific port, accepts connections, and handles multiple clients. - Implement an asynchronous TCP client using `asyncio` that connects to the server, sends messages, and receives responses. - Ensure that the server can handle multiple clients concurrently. - Ensure proper creation and management of the event loop, tasks, and connections on both the server and client sides. **Input:** For the server: - Port number to listen on. For the client: - Server address (localhost or IP). - Server port number. - A list of messages to send to the server. **Output:** For the server: - Log of connections from clients and messages received. For the client: - Responses received from the server. **Constraints:** - Python 3.8+ - Both client and server should handle network errors gracefully. - Implement a protocol for the communication between the client and the server where each message sent from the client is prefixed by its length (e.g., `len|message`). **Performance:** - Ensure the server can handle up to 10 concurrent clients. - Each client should send at least 5 messages to the server. **Skeleton Code:** ```python import asyncio # Server Implementation class TCPServer: def __init__(self, port): self.port = port async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Accepted connection from {addr}\\") while True: try: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {addr}\\") writer.write(data) await writer.drain() except ConnectionResetError: break print(f\\"Connection from {addr} closed\\") writer.close() await writer.wait_closed() async def start_server(self): server = await asyncio.start_server(self.handle_client, \\"127.0.0.1\\", self.port) async with server: await server.serve_forever() def run(self): asyncio.run(self.start_server()) # Client Implementation class TCPClient: def __init__(self, server_address, server_port, messages): self.server_address = server_address self.server_port = server_port self.messages = messages async def send_message(self, message, writer): writer.write(message.encode()) await writer.drain() async def start_client(self): reader, writer = await asyncio.open_connection(self.server_address, self.server_port) for message in self.messages: len_message = f\\"{len(message)}|{message}\\" await self.send_message(len_message, writer) data = await reader.read(100) print(f\\"Received: {data.decode()}\\") writer.close() await writer.wait_closed() def run(self): asyncio.run(self.start_client()) # Example Usage if __name__ == \'__main__\': server = TCPServer(port=8888) client = TCPClient(server_address=\'127.0.0.1\', server_port=8888, messages=[\'Message 1\', \'Message 2\', \'Message 3\']) server.run() # Run this in a separate process or thread client.run() ``` **Notes:** - You will need to adapt this example setup to meet the requirements of running both the server and the client in an appropriate asynchronous environment. - Consider extending error handling and logging as needed.","solution":"import asyncio # Server Implementation class TCPServer: def __init__(self, port): self.port = port async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Accepted connection from {addr}\\") try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {addr}\\") writer.write(data) await writer.drain() except (ConnectionResetError, asyncio.IncompleteReadError): pass print(f\\"Connection from {addr} closed\\") writer.close() await writer.wait_closed() async def start_server(self): server = await asyncio.start_server(self.handle_client, \\"127.0.0.1\\", self.port) async with server: await server.serve_forever() def run(self): asyncio.run(self.start_server()) # Client Implementation class TCPClient: def __init__(self, server_address, server_port, messages): self.server_address = server_address self.server_port = server_port self.messages = messages async def send_message(self, message, writer): writer.write(message.encode()) await writer.drain() async def start_client(self): reader, writer = await asyncio.open_connection(self.server_address, self.server_port) for message in self.messages: len_message = f\\"{len(message)}|{message}\\" await self.send_message(len_message, writer) data = await reader.read(100) print(f\\"Received: {data.decode()}\\") writer.close() await writer.wait_closed() def run(self): asyncio.run(self.start_client()) # Example Usage if __name__ == \'__main__\': server_port = 8888 server = TCPServer(port=server_port) client = TCPClient(server_address=\'127.0.0.1\', server_port=server_port, messages=[\'Message 1\', \'Message 2\', \'Message 3\']) import threading # Run the server in a separate thread server_thread = threading.Thread(target=server.run) server_thread.start() # Run the client client.run() # Join the server thread server_thread.join()"},{"question":"# HTML Entity Conversion You are tasked with implementing a function that converts strings containing HTML entities to their corresponding characters, and vice versa. You will use the dictionaries provided by the `html.entities` module to achieve this. Function 1: `convert_html_entities_to_chars` **Objective:** Convert all HTML entities in an input string to their corresponding characters using the `html5` dictionary. **Input:** - A string `input_str` that may contain HTML entities. **Output:** - A new string where all HTML entities have been converted to their corresponding characters. **Constraints:** - If an entity is not found in the `html5` dictionary, it should remain unchanged. **Example:** ```python input_str = \\"Hello, &gt; World &copy;\\" # Returns: \\"Hello, > World Â©\\" ``` ```python def convert_html_entities_to_chars(input_str): import html.entities # Fill in your code here ``` Function 2: `convert_chars_to_html_entities` **Objective:** Convert all characters in the input string that have a corresponding HTML entity (according to the `codepoint2name` dictionary) to their HTML entity form. **Input:** - A string `input_str` containing any characters. **Output:** - A new string where all characters with corresponding HTML entities have been converted to their HTML entity form. **Constraints:** - If a character does not have a corresponding HTML entity, it should remain unchanged. **Example:** ```python input_str = \\"Hello, > World Â©\\" # Returns: \\"Hello, &gt; World &copy;\\" ``` ```python def convert_chars_to_html_entities(input_str): import html.entities # Fill in your code here ``` # Testing You should test your implementation with various strings, including those that contain special characters and HTML entities. Ensure that all edge cases are considered.","solution":"def convert_html_entities_to_chars(input_str): import html.entities html5_dict = {f\\"&{name};\\": char for name, char in html.entities.html5.items()} for entity, char in html5_dict.items(): input_str = input_str.replace(entity, char) return input_str def convert_chars_to_html_entities(input_str): import html.entities codepoint2name_dict = html.entities.codepoint2name char2entity = {chr(codepoint): f\\"&{name};\\" for codepoint, name in codepoint2name_dict.items()} for char, entity in char2entity.items(): input_str = input_str.replace(char, entity) return input_str"},{"question":"Objective: Implement a function that manipulates a large binary data block using the `memoryview` class in Python. Your solution should demonstrate an understanding of efficient data manipulation using `memoryview`. Problem Statement: You are given a large binary data block in the form of a byte array. Implement a function `process_data(data: bytearray) -> bytearray` that performs the following operations using `memoryview`: 1. Access the first half of the data block and set all bytes to zero. 2. Access the second half of the data block and increment each byte by one (wrapping around at 255). 3. Return the modified byte array. Input: - `data` (bytearray): A byte array of length `n`, where `n` is even and `n >= 2`. Output: - (bytearray): The modified byte array after performing the specified operations. Constraints: - The length of the `data` array will always be even and at least 2. - The operations must be performed using `memoryview` for efficiency. Example: ```python data = bytearray([1, 2, 3, 4]) result = process_data(data) print(result) # Output: bytearray([0, 0, 4, 5]) ``` Requirements: - Implement the function efficiently using the `memoryview` class. - Do not use additional memory-intensive structures to hold intermediate data. # Function Signature: ```python def process_data(data: bytearray) -> bytearray: pass ```","solution":"def process_data(data: bytearray) -> bytearray: Manipulates the binary data block. The first half of the data block bytes are set to zero. The second half of the data block bytes are incremented by one (with wrap around). Args: data (bytearray): The input byte array of even length. Returns: bytearray: The modified byte array. mv = memoryview(data) half_length = len(data) // 2 # Set the first half to zero mv[:half_length] = bytearray(half_length) # Increment the second half by one, wrapping around at 255 for i in range(half_length, len(data)): mv[i] = (mv[i] + 1) % 256 return data"},{"question":"# Question: Transforming BatchNorm to GroupNorm for vmap Compatibility You are given a PyTorch neural network model that uses BatchNorm layers. Your task is to modify the model so that it is compatible with vmap by replacing all BatchNorm layers with GroupNorm layers. Additionally, ensure that the new GroupNorm layers follow the constraints mentioned in the provided documentation. Specifications: 1. **Input**: - A PyTorch neural network model that contains BatchNorm layers. - An integer `num_groups` representing the number of groups to use in GroupNorm, ensuring that the number of channels is divisible by `num_groups`. 2. **Output**: - The modified model, where all BatchNorm layers have been replaced with GroupNorm layers. Function Signature: ```python def convert_bn_to_gn(model: torch.nn.Module, num_groups: int) -> torch.nn.Module: ``` Constraints: * Assume the input model can have multiple BatchNorm layers. * Make sure the number of channels in each BatchNorm layer is divisible by `num_groups`. Example: Given a simple neural network model: ```python import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.bn2 = nn.BatchNorm2d(32) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.conv2(x) x = self.bn2(x) return x model = SimpleModel() ``` After running `convert_bn_to_gn(model, num_groups=8)`, the modified model should have GroupNorm layers instead of BatchNorm layers: ```python import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.gn1 = nn.GroupNorm(num_groups=8, num_channels=16) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.gn2 = nn.GroupNorm(num_groups=8, num_channels=32) def forward(self, x): x = self.conv1(x) x = self.gn1(x) x = self.conv2(x) x = self.gn2(x) return x ``` Hints: - Use the `torch.nn.Module` methods to iterate through layers and identify BatchNorm instances. - Replace the identified BatchNorm layers with GroupNorm layers of equivalent configurations.","solution":"import torch.nn as nn def convert_bn_to_gn(model: nn.Module, num_groups: int) -> nn.Module: Converts all BatchNorm layers in a PyTorch model to GroupNorm layers. Parameters: model (nn.Module): The input PyTorch model containing BatchNorm layers. num_groups (int): The number of groups to use in GroupNorm. Returns: nn.Module: The modified model with GroupNorm layers. for name, module in model.named_children(): if isinstance(module, nn.BatchNorm2d): num_channels = module.num_features # Ensure group wise normalization assert num_channels % num_groups == 0, f\\"The number of channels ({num_channels}) must be divisible by num_groups ({num_groups})\\" new_module = nn.GroupNorm(num_groups=num_groups, num_channels=num_channels) setattr(model, name, new_module) else: convert_bn_to_gn(module, num_groups) # Recurse for nested children return model"},{"question":"# Question: Custom Resource Log Context Manager You are required to implement a custom context manager using the `contextlib` module. This context manager should log the entry and exit of the context, and handle resources properly. You will need to follow these requirements: 1. Implement a custom context manager called `ResourceLogManager` that uses `contextlib`. 2. This context manager should: - Log the message \\"Entering context\\" when entering the context. - Acquire a hypothetical resource using a function `acquire_resource()`. - Log the message \\"Exiting context\\" when exiting the context. - Release the resource using a function `release_resource()`, ensuring this is done even if an exception occurs within the context. 3. The `acquire_resource` and `release_resource` functions are provided to you as follows: ```python def acquire_resource(): print(\\"Resource acquired\\") return \\"resource\\" def release_resource(resource): print(f\\"Resource {resource} released\\") ``` # Implementation Details - Define a class `ResourceLogManager`. - Utilize the `contextlib` module provided constructs to aid in context management, error handling, and cleanup. - Ensure that the context manager can handle exceptions and still ensure resource release. # Expected Behavior: The following code: ```python with ResourceLogManager() as resource: print(\\"Using the resource\\") ``` Should output: ``` Entering context Resource acquired Using the resource Exiting context Resource resource released ``` If an exception occurs inside the context, ensure that `release_resource()` is still called and the context exits gracefully. Here is the template for you to complete: ```python import contextlib def acquire_resource(): print(\\"Resource acquired\\") return \\"resource\\" def release_resource(resource): print(f\\"Resource {resource} released\\") class ResourceLogManager(contextlib.AbstractContextManager): def __enter__(self): print(\\"Entering context\\") self.resource = acquire_resource() return self.resource def __exit__(self, exc_type, exc_value, traceback): print(\\"Exiting context\\") release_resource(self.resource) ``` Implement the `ResourceLogManager` class according to the instructions. Test it with the provided example.","solution":"import contextlib def acquire_resource(): print(\\"Resource acquired\\") return \\"resource\\" def release_resource(resource): print(f\\"Resource {resource} released\\") class ResourceLogManager(contextlib.AbstractContextManager): def __enter__(self): print(\\"Entering context\\") self.resource = acquire_resource() return self.resource def __exit__(self, exc_type, exc_value, traceback): print(\\"Exiting context\\") release_resource(self.resource)"},{"question":"**Question: Analyzing Function Definitions in Python Code** You are tasked with writing a Python script that analyzes a given Python source code file and extracts all the function definitions from it using the Abstract Syntax Trees (AST) module. The goal is to output the name of each function along with the line number on which it is defined. # Requirements: 1. **Function Name**: `extract_functions` 2. **Input**: A string representing the Python source code. 3. **Output**: A list of tuples, where each tuple contains the name of a function (string) and the line number (integer) where it is defined. # Input Constraints: - The input string will be valid Python code. - The Python code may contain nested functions, class definitions, and other statements. # Example: ```python source_code = \'\'\' def foo(): print(\\"Hello, world!\\") class Bar: def bar_method(self): pass def baz(): return foo \'\'\' print(extract_functions(source_code)) # Output: [(\'foo\', 2), (\'bar_method\', 6), (\'baz\', 9)] ``` # Notes: - Assume the input code adheres to standard Python indentation rules. - Utilize the `ast` module to parse the source code and traverse the AST to identify function definitions. # Performance Requirements: - Efficiently handle Python source code up to 1000 lines. - Ensure the script runs within a reasonable time frame for the given input size.","solution":"import ast def extract_functions(source_code): Extracts function names and their line numbers from the given Python source code. Parameters: source_code (str): A string containing the Python source code. Returns: list: A list of tuples, where each tuple contains a function name (str) and its line number (int). class FunctionVisitor(ast.NodeVisitor): def __init__(self): self.functions = [] def visit_FunctionDef(self, node): self.functions.append((node.name, node.lineno)) self.generic_visit(node) tree = ast.parse(source_code) visitor = FunctionVisitor() visitor.visit(tree) return visitor.functions"},{"question":"# Question: XML Document Manipulation using `xml.dom` You are tasked with writing a Python function that takes an XML string as input, parses the XML into a DOM document, and performs a series of modifications to the document. Your function should then return the modified XML string. Specifically, you need to: 1. Parse the input XML string into a DOM document. 2. Find all elements with the tag name \\"item\\". 3. For each \\"item\\" element: - If it has an attribute `type=\\"fruit\\"`, change the text content of the element to \\"apple\\". - If it has an attribute `type=\\"vegetable\\"`, change the text content of the element to \\"carrot\\". 4. Add a new element `<item type=\\"fruit\\">banana</item>` at the end of the document. 5. Return the entire modified XML string. You must make use of the `xml.dom` module\'s classes and methods to achieve this. # Function Signature ```python def modify_xml(xml_string: str) -> str: pass ``` # Input - `xml_string` (str): A string containing the XML data. # Output - (str): The modified XML string. # Example ```python xml_input = <root> <item type=\\"fruit\\">orange</item> <item type=\\"vegetable\\">broccoli</item> <item type=\\"fruit\\">pear</item> </root> expected_output = <root> <item type=\\"fruit\\">apple</item> <item type=\\"vegetable\\">carrot</item> <item type=\\"fruit\\">apple</item> <item type=\\"fruit\\">banana</item> </root> assert modify_xml(xml_input) == expected_output ``` # Constraints: - Assume the input XML string is well-formed. - Do not use external libraries for XML parsing; only use the `xml.dom` module. **Note**: Ensure to handle attributes, text nodes, and element nodes appropriately using the `xml.dom` module functionalities.","solution":"from xml.dom import minidom def modify_xml(xml_string: str) -> str: # Parse the XML string dom = minidom.parseString(xml_string) root = dom.documentElement # Find all elements with the tag name \\"item\\" items = root.getElementsByTagName(\'item\') for item in items: item_type = item.getAttribute(\'type\') if item_type == \'fruit\': item.firstChild.nodeValue = \'apple\' elif item_type == \'vegetable\': item.firstChild.nodeValue = \'carrot\' # Add a new element <item type=\\"fruit\\">banana</item> at the end of the document new_item = dom.createElement(\'item\') new_item.setAttribute(\'type\', \'fruit\') new_text = dom.createTextNode(\'banana\') new_item.appendChild(new_text) root.appendChild(new_item) # Return the modified XML string return dom.toxml()"},{"question":"Objective Your task is to implement a function that generates a secure, random alphanumeric password using the `secrets` module. The password must satisfy certain complexity requirements to ensure it is cryptographically secure. Function Signature ```python def generate_secure_password(length: int) -> str: Generates a secure, random alphanumeric password of specified length. Parameters: length (int): The length of the password to be generated. Must be at least 8. Returns: str: A secure alphanumeric password of the specified length. Constraints: - The password must be at least 8 characters long. - The password must contain at least one lowercase letter, one uppercase letter, and one digit. ``` Input - `length` (int): An integer representing the desired length of the password. Value of length must be at least 8. Output - `str`: A string representing the generated secure alphanumeric password. Constraints - The generated password must contain at least one lowercase letter, one uppercase letter, and one digit. - The function should raise a `ValueError` if the length is less than 8. Examples ```python # Example 1: password = generate_secure_password(10) print(password) # Output: A random secure password of length 10, e.g., \\"aB3dEf1GHi\\" # Example 2: password = generate_secure_password(12) print(password) # Output: A random secure password of length 12, e.g., \\"aBcDeF4GhIjK\\" ``` Implementation Details 1. Use the `secrets` module to generate random characters ensuring cryptographic security. 2. Ensure the password meets the required complexity criteria by checking the presence of at least one lowercase letter, one uppercase letter, and one digit. 3. If the generated password does not meet the criteria, regenerate it until it does.","solution":"import secrets import string def generate_secure_password(length: int) -> str: Generates a secure, random alphanumeric password of specified length. Parameters: length (int): The length of the password to be generated. Must be at least 8. Returns: str: A secure alphanumeric password of the specified length. Constraints: - The password must be at least 8 characters long. - The password must contain at least one lowercase letter, one uppercase letter, and one digit. if length < 8: raise ValueError(\\"Password length must be at least 8\\") alphabet = string.ascii_letters + string.digits while True: password = \'\'.join(secrets.choice(alphabet) for _ in range(length)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and any(c.isdigit() for c in password)): return password"},{"question":"# Python Bytes Objects Interaction with C You have been provided with documentation for how Python handles bytes objects at the C level of its API. Utilizing the `ctypes` library, simulate the creation and manipulation of bytes objects as described in the documentation. Specifically, you need to implement a Python class `CBytes` that mimics the creation and concatenation functionalities of bytes objects using `ctypes`. Class Requirements - **Initialization** - The constructor should take a string or bytes and convert it to a bytes object using `ctypes`. - **Method `from_string`** - A static method that creates a `CBytes` instance from a given string. - **Method `concat`** - Instance method that concatenates the current bytes object with another `CBytes` instance\'s bytes object. Input format - Initialization: `CBytes(data)` - `data`: `str` or `bytes` - Method: `from_string(data: str) -> CBytes` - `data`: `str` - Method: `concat(new_part: CBytes) -> None` - `new_part`: `CBytes` Output format - Method: `from_string` returns a new `CBytes` instance. - Method: `concat` modifies the original object by appending the bytes from `new_part`. Example ```python # Creating instance from a string cbytes1 = CBytes.from_string(\\"Hello\\") # Creating instance from bytes cbytes2 = CBytes(b\\", World!\\") # Concatenating the two instances cbytes1.concat(cbytes2) # Expected internal representation is now b\\"Hello, World!\\" print(cbytes1.get_value()) # Outputs: b\\"Hello, World!\\" ``` # Constraints - The initial size of a bytes object created from a string should be at least the length of the string plus one for the null terminator. - The concatenation should handle memory reallocation appropriately. - You must use the `ctypes` library to interface with raw bytes at the C level. # Implementation Notes - You may assume all inputs are well-formed strings or bytes. - Handle memory management properly to avoid leaks, especially during concatenation. - You are encouraged to review the details from the provided documentation and replicate the core functionalities using `ctypes`. Write the class `CBytes` in Python with the functionality described.","solution":"import ctypes class CBytes: def __init__(self, data): if isinstance(data, str): data = data.encode(\'utf-8\') self.size = len(data) self._buffer = ctypes.create_string_buffer(data, self.size + 1) @staticmethod def from_string(data: str) -> \'CBytes\': return CBytes(data) def concat(self, new_part: \'CBytes\') -> None: new_size = self.size + new_part.size new_buffer = ctypes.create_string_buffer(new_size + 1) ctypes.memmove(new_buffer, self._buffer, self.size) ctypes.memmove(ctypes.byref(new_buffer, self.size), new_part._buffer, new_part.size) self.size = new_size self._buffer = new_buffer def get_value(self): return self._buffer.value"},{"question":"# Coding Challenge: Profiling and Optimizing a Machine Learning Algorithm Problem Description You have been provided with a skeleton of a non-negative matrix factorization (NMF) algorithm. Your task is to: 1. Implement the NMF algorithm. 2. Profile the implementation to identify performance bottlenecks. 3. Optimize the identified bottlenecks using Cython. Instructions 1. **Implement the NMF Algorithm**: - Implement the NMF algorithm using Python, Numpy, and Scipy. - Ensure to avoid using explicit loops where vectorized operations are possible. 2. **Profile the Implementation**: - Use the IPython integrated profiler to find the total execution time and identify bottlenecks. - Provide output for `%timeit` and `%prun` commands showing the performance profile of the NMF implementation. 3. **Optimize with Cython**: - Identify the most time-consuming function from your profiling analysis. - Rewrite this bottleneck function in Cython and integrate it back into the NMF implementation. - Ensure the optimized function provides the same output as the unoptimized function. Expected Input and Output Formats - **Input**: A matrix `V` of non-negative values. - **Output**: Two matrices `W` and `H` such that the product matrix `W @ H` approximates the input matrix `V`. Constraints - The input matrix `V` is a non-negative 2D numpy array with dimensions m x n. - The rank for the factorization is an integer k, such that 1 <= k <= min(m, n). Performance Requirements - Your initial Python implementation should handle a matrix of size 100x100 within 10 seconds. - The optimized Cython implementation should show a significant performance improvement. Example ```python import numpy as np def nmf(V, rank, tol=1e-4, max_iter=200): Perform non-negative matrix factorization on V. Parameters: V (2D ndarray): The non-negative input matrix to be factorized. rank (int): The number of latent features. tol (float): The tolerance for convergence. max_iter (int): The maximum number of iterations. Returns: W (2D ndarray): The factorized matrix of shape (m, rank). H (2D ndarray): The factorized matrix of shape (rank, n). # Your implementation here... pass # Example usage: V = np.random.rand(100, 100) W, H = nmf(V, 10) print(W @ H) ``` Submission Submit: 1. Your full NMF implementation in Python. 2. Profiling results using `%timeit` and `%prun`. 3. The identified bottleneck function optimized using Cython. 4. Verification that the optimized function matches the output of the unoptimized function.","solution":"import numpy as np def nmf(V, rank, tol=1e-4, max_iter=200): Perform non-negative matrix factorization on V. Parameters: V (2D ndarray): The non-negative input matrix to be factorized. rank (int): The number of latent features. tol (float): The tolerance for convergence. max_iter (int): The maximum number of iterations. Returns: W (2D ndarray): The factorized matrix of shape (m, rank). H (2D ndarray): The factorized matrix of shape (rank, n). m, n = V.shape W = np.random.rand(m, rank) H = np.random.rand(rank, n) for iteration in range(max_iter): V_approx = np.dot(W, H) loss = np.linalg.norm(V - V_approx, \'fro\') if loss < tol: break W *= (V @ H.T) / (W @ H @ H.T + 1e-9) H *= (W.T @ V) / (W.T @ W @ H + 1e-9) return W, H"},{"question":"# Python Programming Assessment **Problem Statement:** You are tasked with preparing a Python library distribution by ensuring that all the Python source files are precompiled into bytecode. This process will involve several steps and make use of the `compileall` module. Requirements: 1. You should compile all `.py` files in a given directory and its subdirectories. 2. Exclude any files that are located in directories whose names start with `test`. 3. Ensure that the compilation process is performed using multiple workers to speed up the process. The number of workers should be optimal based on the number of CPU cores available. 4. Target all optimization levels during the compilation (levels 0, 1, and 2). 5. Print errors if any occur during the compilation but suppress the list of compiled files. 6. Return `True` if all files compiled successfully, otherwise `False`. Function Signature: ```python def prepare_library_compilation(directory: str) -> bool: pass ``` Input: - `directory` (str): The path to the root directory of the library to be compiled. Output: - `bool`: `True` if all files compile successfully, otherwise `False`. Constraints: - You cannot use any external libraries other than those provided in the Python Standard Library. - The function should handle potential symlinks gracefully to avoid infinite loops. # Example: ```python result = prepare_library_compilation(\'path/to/library\') print(result) # The output should be either True or False based on the success of the compilation process. ``` # Notes: - Take advantage of regular expressions to handle directory exclusions. - Utilize the `compileall.compile_dir` function with appropriate arguments to meet the requirements. - Ensure error handling is robust to provide meaningful diagnostics if something goes wrong during the compilation.","solution":"import compileall import os import multiprocessing import re def prepare_library_compilation(directory: str) -> bool: Compiles all .py files in a given directory and its subdirectories. Excludes files in directories whose names start with \'test\'. Uses multiple workers and targets all optimization levels (0, 1, and 2). Args: directory (str): The path to the root directory of the library to be compiled. Returns: bool: True if all files compile successfully, otherwise False. def exclude_test_dirs(path, names): Exclude directories whose names start with \'test\'. return [name for name in names if name.startswith(\'test\')] num_workers = multiprocessing.cpu_count() try: result = compileall.compile_dir( directory, maxlevels=10, force=True, quiet=3, # Quiet mode (level 3) suppresses output but prints errors. legacy=False, workers=num_workers, rx=re.compile(r\'.*/test[^/]*/*\') # Regex to exclude any directory starting with \'test\'. ) return result except Exception as e: print(f\\"Error occurred during compilation: {e}\\") return False"},{"question":"Asynchronous Model Training using PyTorch Multiprocessing **Objective:** Implement a function to train a neural network model asynchronously using PyTorch\'s multiprocessing package. Ensure efficient use of resources and proper handling of potential pitfalls such as deadlocks and CPU oversubscription. **Problem Statement:** You are given a neural network model and a training dataset. Your task is to implement an asynchronous training function using PyTorch\'s multiprocessing module. Your implementation should include the necessary steps to avoid common issues such as deadlocks and CPU oversubscription. Follow the guidelines provided in the PyTorch documentation. **Function Signature:** ```python def asynchronous_train(model: torch.nn.Module, dataset: torch.utils.data.Dataset, num_processes: int, epochs: int, learning_rate: float, momentum: float) -> None: pass ``` **Input:** - `model`: A PyTorch neural network model (`torch.nn.Module`) that needs to be trained. - `dataset`: A PyTorch dataset (`torch.utils.data.Dataset`) for training. - `num_processes`: An integer representing the number of processes to be used for training. - `epochs`: An integer specifying the number of epochs to train the model. - `learning_rate`: A float specifying the learning rate for the optimizer. - `momentum`: A float specifying the momentum for the optimizer. **Output:** - The function returns `None`. The trained model should be updated in place. **Constraints:** - Use the `torch.multiprocessing` module to implement asynchronous training. - Avoid deadlocks and ensure efficient resource management as described in the provided documentation. - The implementation should work for both CPU and GPU training. **Notes:** - You may assume the data loader, loss function, and optimizer as defined below: ```python import torch import torch.optim as optim import torch.utils.data as data from torch.multiprocessing import Pool def data_loader(dataset): return torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True) def loss_fn(output, target): return torch.nn.functional.cross_entropy(output, target) def train_model(rank, model, epochs, dataset, learning_rate, momentum): torch.manual_seed(0) torch.set_num_threads(int(torch.get_num_threads() / num_processes)) loader = data_loader(dataset) optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum) for epoch in range(epochs): for data, target in loader: optimizer.zero_grad() output = model(data) loss = loss_fn(output, target) loss.backward() optimizer.step() ``` **Implementation:** 1. Implement the function `asynchronous_train` using the provided function signature. 2. Ensure that your implementation properly shares the model\'s memory across processes. 3. Use the appropriate start method to avoid issues with CUDA. 4. Avoid CPU oversubscription by setting the number of threads for each process appropriately. **Example:** ```python class MyModel(torch.nn.Module): def __init__(self): super(MyModel, self).__init__() self.linear = torch.nn.Linear(784, 10) def forward(self, x): return self.linear(x) if __name__ == \'__main__\': num_processes = 4 model = MyModel() dataset = torch.utils.data.TensorDataset(torch.randn(1000, 784), torch.randint(0, 10, (1000,))) asynchronous_train(model, dataset, num_processes, 5, 0.01, 0.9) ``` Ensure that your implementation is efficient and follows best practices as per the given documentation.","solution":"import torch import torch.optim as optim import torch.utils.data as data import torch.multiprocessing as mp from torch.multiprocessing import Pool def data_loader(dataset): return torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True) def loss_fn(output, target): return torch.nn.functional.cross_entropy(output, target) def train_model(rank, model_shared, epochs, dataset, learning_rate, momentum): torch.manual_seed(0) torch.set_num_threads(int(torch.get_num_threads() / mp.cpu_count())) loader = data_loader(dataset) optimizer = optim.SGD(model_shared.parameters(), lr=learning_rate, momentum=momentum) for epoch in range(epochs): for data, target in loader: optimizer.zero_grad() output = model_shared(data) loss = loss_fn(output, target) loss.backward() optimizer.step() def asynchronous_train(model: torch.nn.Module, dataset: torch.utils.data.Dataset, num_processes: int, epochs: int, learning_rate: float, momentum: float) -> None: model_shared = model.share_memory() processes = [] for rank in range(num_processes): p = mp.Process(target=train_model, args=(rank, model_shared, epochs, dataset, learning_rate, momentum)) p.start() processes.append(p) for p in processes: p.join()"},{"question":"# Custom Python Interpreter You are required to create a custom interactive Python interpreter using the `code` module. Your custom interpreter should have the following features: 1. **Basic Command Execution**: - The interpreter should accept any valid Python code and execute it. - It should display the output of the executed code or any errors that occurred during execution. 2. **Custom Command**: - Implement a special command called `!history` that, when entered, displays the list of all previously entered commands (excluding the `!history` command itself). 3. **Custom Exception Handling**: - Your interpreter should catch any exceptions raised during the execution of user commands and display a custom error message: \\"An error occurred: [error message]\\". # Requirements: - **Input and Output Format**: - The interpreter should continuously prompt the user to enter Python code (like a traditional Python shell). - On entering the `!history` command, it should print all previously executed commands chronologically. - On entering a command, it should display either the result of executing the command or the custom error message if an exception occurred. - **Constraints**: - You may assume the entered commands are syntactically valid Python code or the `!history` command. - The command history should exclude the `!history` command itself. - **Performance**: - The interpreter should efficiently handle a large number of commands without significant performance degradation. # Example Usage: ``` >>> print(\\"Hello, World!\\") Hello, World! >>> x = 5 >>> !history print(\\"Hello, World!\\") x = 5 >>> y = x / 0 An error occurred: division by zero >>> !history print(\\"Hello, World!\\") x = 5 y = x / 0 ``` # Implementation Skeleton: ```python import code class CustomInterpreter(code.InteractiveConsole): def __init__(self): super().__init__() self.history = [] def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): if source.strip() == \\"!history\\": self.show_history() return None else: self.history.append(source.strip()) try: return super().runsource(source, filename, symbol) except Exception as e: print(f\\"An error occurred: {e}\\") return False def show_history(self): for command in self.history: if command != \\"!history\\": print(command) if __name__ == \\"__main__\\": console = CustomInterpreter() console.interact(\\"Custom Python Interpreter. Type !history to see command history.\\") ``` Your task is to complete the implementation and ensure it works as specified. Test your interpreter thoroughly to make sure all requirements are met.","solution":"import code class CustomInterpreter(code.InteractiveConsole): def __init__(self): super().__init__() self.history = [] def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): if source.strip() == \\"!history\\": self.show_history() return None else: self.history.append(source.strip()) try: return super().runsource(source, filename, symbol) except Exception as e: print(f\\"An error occurred: {e}\\") return False def runcode(self, code): try: exec(code, self.locals) except Exception as e: print(f\\"An error occurred: {e}\\") def show_history(self): for command in self.history: print(command) if __name__ == \\"__main__\\": console = CustomInterpreter() console.interact(\\"Custom Python Interpreter. Type !history to see command history.\\")"},{"question":"**Topic**: Handling MIME Email Messages with Python\'s `email` package Objective: Write a function that constructs a MIME email message from given inputs and then parses the created email to extract specific information. Problem Description: You need to create and parse an email using the `email` package in Python. 1. **Construct an Email Message**: Write a function `create_mime_email` that constructs a MIME email message with the following details: - Sender: `from_email` (a string) - Recipient: `to_email` (a string) - Subject: `subject` (a string) - Plain text content: `text_content` (a string) - HTML content: `html_content` (a string) - Attachment: `attachment` (a dictionary with keys: `filename` and `file_content`) The function should return the string representation of the constructed email message. 2. **Parse an Email Message**: Write a function `parse_mime_email` that parses a MIME email message and extracts the following details: - Sender email - Recipient email - Subject - Plain text content - HTML content (if exists) - Filenames of any attachments The function should take the string representation of the email message and return a dictionary with the extracted details. Function Signatures: ```python def create_mime_email(from_email: str, to_email: str, subject: str, text_content: str, html_content: str, attachment: dict) -> str: Constructs a MIME email message and returns it as a string. pass def parse_mime_email(mime_email_str: str) -> dict: Parses a MIME email message string and returns extracted details. pass ``` Constraints: - You may assume the provided email inputs are well-formed. - Each attachment is represented as a dictionary with two keys: `filename` (a string) and `file_content` (a string, representing the content in binary form, assume it\'s base64 encoded). - The email may contain multiple parts, including both plain text and HTML. Example: ```python # Example Input from_email = \\"sender@example.com\\" to_email = \\"recipient@example.com\\" subject = \\"Sample Email\\" text_content = \\"This is a sample plain text message.\\" html_content = \\"<html>This is a sample HTML message.</html>\\" attachment = { \\"filename\\": \\"sample.txt\\", \\"file_content\\": \\"U2FtcGxlIGNvbnRlbnQ=\\" # Base64 encoded \\"Sample content\\" } # Example Usage email_str = create_mime_email(from_email, to_email, subject, text_content, html_content, attachment) parsed_details = parse_mime_email(email_str) # Expected Output print(parsed_details) # Should print a dictionary similar to: # { # \\"from_email\\": \\"sender@example.com\\", # \\"to_email\\": \\"recipient@example.com\\", # \\"subject\\": \\"Sample Email\\", # \\"text_content\\": \\"This is a sample plain text message.\\", # \\"html_content\\": \\"<html>This is a sample HTML message.</html>\\", # \\"attachments\\": [\\"sample.txt\\"] # } ``` > Note: Ensure you handle multi-part emails properly and use appropriate MIME types for the parts and attachments. Good luck!","solution":"from email.message import EmailMessage from email import policy from email.parser import BytesParser from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.base import MIMEBase from email import encoders import base64 def create_mime_email(from_email: str, to_email: str, subject: str, text_content: str, html_content: str, attachment: dict) -> str: Constructs a MIME email message and returns it as a string. msg = MIMEMultipart(\\"alternative\\") msg[\\"From\\"] = from_email msg[\\"To\\"] = to_email msg[\\"Subject\\"] = subject part1 = MIMEText(text_content, \\"plain\\") part2 = MIMEText(html_content, \\"html\\") msg.attach(part1) msg.attach(part2) if attachment: part = MIMEBase(\\"application\\", \\"octet-stream\\") part.set_payload(base64.b64decode(attachment[\'file_content\'])) encoders.encode_base64(part) part.add_header(\\"Content-Disposition\\", f\\"attachment; filename={attachment[\'filename\']}\\",) msg.attach(part) return msg.as_string() def parse_mime_email(mime_email_str: str) -> dict: Parses a MIME email message string and returns extracted details. msg = BytesParser(policy=policy.default).parsebytes(mime_email_str.encode()) extracted_details = { \\"from_email\\": msg[\\"From\\"], \\"to_email\\": msg[\\"To\\"], \\"subject\\": msg[\\"Subject\\"], \\"text_content\\": None, \\"html_content\\": None, \\"attachments\\": [] } for part in msg.walk(): content_type = part.get_content_type() if content_type == \\"text/plain\\": extracted_details[\\"text_content\\"] = part.get_payload(decode=True).decode(part.get_content_charset()) elif content_type == \\"text/html\\": extracted_details[\\"html_content\\"] = part.get_payload(decode=True).decode(part.get_content_charset()) elif part.get_content_disposition() == \'attachment\': extracted_details[\\"attachments\\"].append(part.get_filename()) return extracted_details"},{"question":"# Python Initialization Configuration Your task is to implement a set of functions that initialize Python using both standard and isolated configuration methods provided in the documentation. These functions will set up Python\'s state according to specific requirements and handle any potential errors. 1. Function: `initialize_python_standard` This function initializes Python using the standard configuration and sets some specific attributes. **Input:** - `program_name` (str): The program name to set. - `module_path` (str): Path to a custom module to be added to `sys.path`. **Output:** - Returns a dictionary with two keys: `status` containing \\"OK\\" if initialization succeeds, or an error message if it fails; `exit_code` with the corresponding exit code (0 for success, or non-zero for errors). **Example Usage:** ```python result = initialize_python_standard(\\"my_program\\", \\"/path/to/custom/module\\") print(result) # Expected Output: {\'status\': \'OK\', \'exit_code\': 0} ``` 2. Function: `initialize_python_isolated` This function initializes Python in isolated mode and configures it to ignore environment variables and command-line arguments. **Input:** - `string_list` (list of str): A list of strings to be managed by `PyWideStringList`. **Output:** - A dictionary containing two keys: `status` with \\"OK\\" if successfully initialized, or an error message if it fails; `exit_code` with the corresponding exit code (0 for success, or non-zero for errors). **Example Usage:** ```python result = initialize_python_isolated([\\"path1\\", \\"path2\\"]) print(result) # Expected Output: {\'status\': \'OK\', \'exit_code\': 0} ``` Specifications: - Make sure Python is preinitialized where necessary. - Use `PyStatus` functions to manage success and error states. - Manipulate wide string lists as needed. - Handle memory allocation failures explicitly and display an appropriate error message. - Ensure that initialization configurations are released and cleared appropriately. Constraints: - The solution must handle wide strings appropriately and ensure they are non-NULL. - Edge cases like empty lists or invalid paths should be handled gracefully. - Your solution should illustrate comprehension of both standard and isolated initialization methods as per the documented APIs. # Implementation Details: You may simulate the required Python C API functions and structures using pure Python for this exercise, translating the key behavior described. You don\'t have to use actual internal Python C API calls.","solution":"# We will simulate the Python C API function behavior using pure Python functions def initialize_python_standard(program_name, module_path): Initializes Python using the standard configuration. Args: program_name (str): The program name to set. module_path (str): Path to a custom module to be added to `sys.path`. Returns: dict: A dictionary with \'status\' and \'exit_code\' indicating success or failure. try: # Simulate setting the program name (this would be the equivalent of Py_SetProgramName in C API) if not isinstance(program_name, str): raise ValueError(\\"Program name must be a string.\\") # Simulate adding custom module path to sys.path (equivalent to manipulating sys.path in C API) import sys if not isinstance(module_path, str): raise ValueError(\\"Module path must be a string.\\") if module_path not in sys.path: sys.path.insert(0, module_path) # Simulated successful initialization return {\'status\': \'OK\', \'exit_code\': 0} except Exception as e: # In case of any exception, return the error message and a non-zero exit code return {\'status\': str(e), \'exit_code\': 1} def initialize_python_isolated(string_list): Initializes Python in isolated mode, ignoring environment variables and command-line arguments. Args: string_list (list of str): A list of strings to be managed by PyWideStringList (simulated). Returns: dict: A dictionary with \'status\' and \'exit_code\' indicating success or failure. try: # Simulate PyWideStringList behavior if not all(isinstance(s, str) for s in string_list): raise ValueError(\\"All elements of string_list must be strings.\\") # In isolated mode, we ignore environment variables and command-line args # For simulation, assuming we execute isolated behavior, setting necessary flags # Simulated successful isolated initialization return {\'status\': \'OK\', \'exit_code\': 0} except Exception as e: # In case of any exception, return the error message and a non-zero exit code return {\'status\': str(e), \'exit_code\': 1}"},{"question":"# Title: Implementing a Command-Line Tool Using `__name__` and `__main__` # Problem Statement: You are tasked with creating a Python module that functions as a command-line tool for basic arithmetic operations (addition, subtraction, multiplication, and division). You need to implement the module in such a way that it leverages the special `__name__ == \'__main__\'` construct to conditionally execute code if the module is run directly. The module should perform arithmetic operations based on user input and be executable both as a standalone script and as part of a package. # Requirements: 1. **Input Format**: - The module should accept command-line arguments for two operands (integers or floats) and an operation (add, subtract, multiply, or divide). For example: ``` python arithmetic.py 4 2 add ``` 2. **Output Format**: - The module should print the result of the operation to the standard output. - If operated as part of an interactive session, the same functions should be available for import without executing the script-specific code. 3. **Constraints**: - If the operation is not one of the specified ones, the module should print a meaningful message and exit. - Handle exceptional cases such as division by zero and provide appropriate error messaging. - Implement the primary functionality in a separate function outside `if __name__ == \'__main__\'` block to ensure importability. 4. **Package Execution**: - Create a `__main__.py` script within a package that allows the same arithmetic operations to be executed when the package is invoked directly. 5. **Performance**: - The operations should be efficient and handle invalid cases gracefully. # Example Usage: 1. As a standalone script: ```bash python arithmetic.py 10 5 add Result: 15.0 ``` 2. As part of a package: ```bash python -m mypackage 9 3 multiply Result: 27.0 ``` # Instructions: 1. Create a Python file named `arithmetic.py`. 2. Define a `main` function to handle command-line input and calls the appropriate arithmetic operation. 3. Implement functions for `add`, `subtract`, `multiply`, and `divide`. 4. Ensure that only the `main` function executes when the module is run directly. 5. Create a package structure with `__init__.py` and `__main__.py` files to support the package execution feature. # Submission: Submit a ZIP file containing: - `arithmetic.py` - Package directory (e.g., `mypackage`) with: - `__init__.py` - `__main__.py` - README with instructions on how to execute the script and the package.","solution":"import sys def add(a, b): return a + b def subtract(a, b): return a - b def multiply(a, b): return a * b def divide(a, b): if b == 0: raise ValueError(\\"Cannot divide by zero\\") return a / b def main(): if len(sys.argv) != 4: print(\\"Usage: python arithmetic.py <operand1> <operand2> <operation>\\") print(\\"Operations: add, subtract, multiply, divide\\") sys.exit(1) operand1 = float(sys.argv[1]) operand2 = float(sys.argv[2]) operation = sys.argv[3] if operation == \'add\': result = add(operand1, operand2) elif operation == \'subtract\': result = subtract(operand1, operand2) elif operation == \'multiply\': result = multiply(operand1, operand2) elif operation == \'divide\': try: result = divide(operand1, operand2) except ValueError as e: print(e) sys.exit(1) else: print(f\\"Invalid operation: {operation}\\") sys.exit(1) print(f\\"Result: {result}\\") if __name__ == \'__main__\': main()"},{"question":"**Pandas Data Analysis Task** You are given a CSV file named `students_grades.csv` that contains student grades for different subjects. Your task is to perform data manipulation and analysis using the pandas library to extract meaningful insights from the data. The CSV file follows this structure: ``` StudentID, Name, Math, Science, History, English, PhysicalEducation 1, Alice, 85, 91, 76, 88, 95 2, Bob, 78, 85, 80, 83, 90 ... ``` **Tasks:** 1. Load the CSV file into a pandas DataFrame. 2. Calculate the average grade for each student across all subjects. 3. Determine the overall average grade for each subject. 4. Identify the student with the highest average grade. 5. Create a new column \'Pass/Fail\' that marks students as \'Pass\' if their average grade is 60 or higher and \'Fail\' otherwise. 6. Sort the DataFrame by the students\' average grades in descending order. 7. Save the manipulated DataFrame to a new CSV file named `students_grades_analysis.csv`. **Function Signature:** ```python def analyze_student_grades(csv_file: str) -> None: # Your code here pass ``` **Input:** - `csv_file`: A string representing the path to the CSV file. **Output:** - The function does not return anything. Instead, it saves the resulting DataFrame to a CSV file named `students_grades_analysis.csv`. **Constraints:** - You should handle any potential missing data appropriately. - Assume the CSV file follows the structure mentioned above without corrupt data. **Example:** If the input CSV file `students_grades.csv` contains: ``` StudentID, Name, Math, Science, History, English, PhysicalEducation 1, Alice, 85, 91, 76, 88, 95 2, Bob, 78, 85, 80, 83, 90 3, Charlie, 95, 70, 65, 60, 75 4, David, 55, 58, 60, 50, 65 ``` The resulting `students_grades_analysis.csv` might look like: ``` StudentID, Name, Math, Science, History, English, PhysicalEducation, Average, Pass/Fail 1, Alice, 85, 91, 76, 88, 95, 87.0, Pass 2, Bob, 78, 85, 80, 83, 90, 83.2, Pass 3, Charlie, 95, 70, 65, 60, 75, 73.0, Pass 4, David, 55, 58, 60, 50, 65, 57.6, Fail ... ```","solution":"import pandas as pd def analyze_student_grades(csv_file: str) -> None: Analyze the student grades from the given CSV file and save the results to a new CSV file. Parameters: csv_file (str): The path to the CSV file containing student grades. Returns: None # Load the CSV file into a pandas DataFrame df = pd.read_csv(csv_file) # Calculate the average grade for each student across all subjects df[\'Average\'] = df[[\'Math\', \'Science\', \'History\', \'English\', \'PhysicalEducation\']].mean(axis=1) # Overall average grade for each subject (This will not be stored in the resulting DataFrame) subject_averages = df[[\'Math\', \'Science\', \'History\', \'English\', \'PhysicalEducation\']].mean() # Identify the student with the highest average grade top_student = df.loc[df[\'Average\'].idxmax(), \'Name\'] # Create a new column \'Pass/Fail\' df[\'Pass/Fail\'] = df[\'Average\'].apply(lambda x: \'Pass\' if x >= 60 else \'Fail\') # Sort the DataFrame by the students\' average grades in descending order df = df.sort_values(by=\'Average\', ascending=False) # Save the manipulated DataFrame to a new CSV file output_file = \'students_grades_analysis.csv\' df.to_csv(output_file, index=False)"},{"question":"# Email Generator Implementation with Policy Control **Context:** You have been tasked with implementing a script that can flatten and serialize email message objects into a textual format using the `email.generator` module. The serialization should support both `BytesGenerator` and `Generator` based on the content of the email message. Additionally, the script should be able to handle different policies dictating the message generation behavior such as header folding, content transfer encodings, etc. **Requirements:** 1. Implement a function `serialize_email` that takes the following parameters: - `message`: An email message object that conforms to the `email.message.Message` class. - `output_file_path`: The path where the serialized email content should be saved. - `as_bytes`: A boolean flag indicating whether the output needs to be in bytes (True) or text (False). - `mangle_from`: A boolean controlling whether lines starting with \\"From \\" should be mangled. - `maxheaderlen`: An integer specifying the maximum header line length (default: None). - `policy`: An optional policy object to control message generation; if None, default policies shall be used. 2. The function should use the appropriate generator class (`BytesGenerator` or `Generator`) based on the `as_bytes` flag to generate the serialized output. 3. Ensure that the function can handle both 7bit and 8bit content transfer encodings and adjust the output accordingly. 4. Implement error handling to ensure the function can gracefully handle invalid inputs or policies. 5. Provide a sample usage demonstrating the flexibility of different policy settings. **Function Signature:** ```python def serialize_email(message, output_file_path, as_bytes=True, mangle_from=False, maxheaderlen=None, policy=None): # Implementation here ``` # Constraints: - You can assume the message object conforms to the standard `email.message.Message` format. - The `output_file_path` should be a valid path, and the function should create the file if it does not exist. - The `policy` object, if provided, should comply with the standards mentioned in the `email.policy` module. **Example Usage:** ```python from email.message import EmailMessage from email.policy import default # Sample email message creation msg = EmailMessage() msg.set_content(\\"This is a test email.\\") msg[\'Subject\'] = \'Test\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' # Example usage of the serialize_email function serialize_email(msg, \'output.txt\', as_bytes=False, mangle_from=True, maxheaderlen=78, policy=default) ``` This task will test the students\' understanding of utilizing the `email.generator` module for serializing email messages, handling policy variations, and managing file outputs in a flexible manner.","solution":"from email.generator import BytesGenerator, Generator from email.message import Message from email.policy import default def serialize_email(message, output_file_path, as_bytes=True, mangle_from=False, maxheaderlen=None, policy=None): Serializes an email message to an output file. Parameters: - message: Message object conforming to email.message.Message - output_file_path: Path where the serialized email content should be saved - as_bytes: Boolean flag indicating whether the output should be in bytes (True) or text (False) - mangle_from: Boolean controlling whether lines starting with \'From \' should be mangled - maxheaderlen: Integer specifying the maximum header line length (default: None) - policy: Policy object to control message generation; if None, default policy is used if not isinstance(message, Message): raise ValueError(\\"message must be an instance of email.message.Message\\") try: with open(output_file_path, \'wb\' if as_bytes else \'w\') as f: policy = policy or default if as_bytes: gen = BytesGenerator(f, mangle_from_=mangle_from, maxheaderlen=maxheaderlen, policy=policy) else: gen = Generator(f, mangle_from_=mangle_from, maxheaderlen=maxheaderlen, policy=policy) gen.flatten(message) except Exception as e: raise ValueError(\\"An error occurred while serializing the email message\\") from e"},{"question":"# Question: Implementing a Custom Kernel SVM for Classification with Class Imbalance Handling You are provided with a dataset that you need to classify using a Support Vector Machine (SVM). However, this problem has certain nuances: 1. You need to implement a custom kernel function. 2. The dataset is imbalanced, and you will have to handle this imbalance. Task Given `X_train`, `y_train`, `X_test` datasets, where `X_train` and `X_test` are feature matrices and `y_train` is the target label vector, perform the following steps: 1. Implement a custom kernel function `polynomial_rbf_kernel(X, Y, d, gamma)` that combines a polynomial and RBF kernel. 2. Train an SVM model using `SVC` with this custom kernel. 3. Handle the class imbalance using class weights. 4. Evaluate the model on `X_test` and return the predicted labels. Input and Output Formats * **Function Signature:** ```python def svm_with_custom_kernel(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: ``` * **Input:** - `X_train`: A numpy array of shape (n_train_samples, n_features) containing the training samples. - `y_train`: A numpy array of shape (n_train_samples,) containing the training labels. The target labels are integers. - `X_test`: A numpy array of shape (n_test_samples, n_features) containing the test samples. * **Output:** - A numpy array of shape (n_test_samples,) containing the predicted labels for `X_test`. * **Constraints:** - The polynomial degree `d` for the custom kernel is 3. - The gamma parameter for the RBF kernel portion is 0.1. * **Implementation Details:** - Define the custom kernel function `polynomial_rbf_kernel(X, Y, d=3, gamma=0.1)`, which combines a polynomial and RBF kernel. - Use `SVC` from `sklearn.svm` with `class_weight=\'balanced\'` to handle class imbalance. - Train the model on `X_train`, `y_train`. - Predict the labels for `X_test`. Example ```python import numpy as np # Example custom kernel function def polynomial_rbf_kernel(X, Y, d=3, gamma=0.1): polynomial = (np.dot(X, Y.T) + 1) ** d rbf = np.exp(-gamma * np.linalg.norm(X[:, np.newaxis] - Y, axis=2) ** 2) return polynomial + rbf # Usage X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]]) y_train = np.array([0, 1, 0, 1]) X_test = np.array([[2, 2], [3, 3]]) predicted_labels = svm_with_custom_kernel(X_train, y_train, X_test) print(predicted_labels) # Output format for illustration ``` **Note:** The example data provided above is only for illustrative purposes. The actual training and test data will be different.","solution":"import numpy as np from sklearn.svm import SVC def polynomial_rbf_kernel(X, Y, d=3, gamma=0.1): Custom kernel function that combines polynomial and RBF kernels. Args: - X: np.ndarray, first input matrix of shape (n_samples_X, n_features) - Y: np.ndarray, second input matrix of shape (n_samples_Y, n_features) - d: int, degree for the polynomial kernel - gamma: float, parameter for the RBF kernel Returns: - K: np.ndarray, combined kernel matrix of shape (n_samples_X, n_samples_Y) polynomial = (np.dot(X, Y.T) + 1) ** d rbf = np.exp(-gamma * np.linalg.norm(X[:, np.newaxis] - Y, axis=2) ** 2) return polynomial + rbf def svm_with_custom_kernel(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: Train SVM model with custom kernel on imbalanced dataset and predict labels for test data. Args: - X_train: np.ndarray, feature matrix for training data of shape (n_train_samples, n_features) - y_train: np.ndarray, target labels for training data of shape (n_train_samples,) - X_test: np.ndarray, feature matrix for test data of shape (n_test_samples, n_features) Returns: - y_pred: np.ndarray, predicted labels for the test data of shape (n_test_samples,) def custom_kernel_wrapper(X, Y): return polynomial_rbf_kernel(X, Y, d=3, gamma=0.1) # Initialize SVM with the custom kernel and balanced class weights model = SVC(kernel=custom_kernel_wrapper, class_weight=\'balanced\') # Train the model model.fit(X_train, y_train) # Make predictions on the test set y_pred = model.predict(X_test) return y_pred"},{"question":"**Objective:** Assess students\' understanding of handling ZIP archives using the `zipfile` module in Python. **Task:** You are required to create a function that performs specific operations on a ZIP file. The function will read the contents of an existing ZIP file, apply transformations to some of its files, and then create a new ZIP file with the modified contents. **Function Signature:** ```python def transform_zip_file(input_zip_path: str, output_zip_path: str, transform_func: callable) -> None: Transforms files in a ZIP archive. :param input_zip_path: Path to the input ZIP file. :param output_zip_path: Path to save the transformed ZIP file. :param transform_func: A function that takes file data (bytes) and returns transformed data (bytes). ``` **Requirements:** 1. **Reading and Extracting:** - Open the input ZIP file in read mode. - Extract the content of each file in the ZIP archive. 2. **Transformation:** - Apply `transform_func` to the content of each file in the ZIP archive. The `transform_func` will take the file data as input and return modified data. 3. **Writing to New ZIP:** - Create a new ZIP file and write the transformed contents into it. - Maintain the original file structure within the new ZIP file. **Constraints:** - Do not modify directory structure or filenames within the ZIP archive. - The input ZIP file and output ZIP file should be handled as efficiently as possible. - Assume that `transform_func` is a valid callable that performs some transformation on file data. **Input Format:** - `input_zip_path` (str) â€“ Path to the input ZIP file. - `output_zip_path` (str) â€“ Path to save the transformed ZIP file. - `transform_func` (callable) â€“ A function that takes file data (bytes) and returns transformed data (bytes). **Example:** Suppose we have a ZIP file `input.zip` that contains three text files: `file1.txt`, `file2.txt`, and `file3.txt`. The transformation function `uppercase` simply converts all text to uppercase. ```python def uppercase(data: bytes) -> bytes: return data.upper() transform_zip_file(\'input.zip\', \'output.zip\', uppercase) ``` After running the function, `output.zip` should contain `file1.txt`, `file2.txt`, and `file3.txt` with all their contents in uppercase. # Evaluation Criteria - Correct usage of the `zipfile` module to read, modify, and write ZIP files. - Accurate application of the transformation function to the file contents. - Preservation of the original file structure in the new ZIP archive.","solution":"import zipfile def transform_zip_file(input_zip_path: str, output_zip_path: str, transform_func: callable) -> None: Transforms files in a ZIP archive. :param input_zip_path: Path to the input ZIP file. :param output_zip_path: Path to save the transformed ZIP file. :param transform_func: A function that takes file data (bytes) and returns transformed data (bytes). # Open the input ZIP file in read mode with zipfile.ZipFile(input_zip_path, \'r\') as input_zip: # Create a new ZIP file in write mode with zipfile.ZipFile(output_zip_path, \'w\') as output_zip: # Iterate over each file in the input ZIP for zip_info in input_zip.infolist(): # Read the file content with input_zip.open(zip_info.filename) as file: data = file.read() # Transform the file content using the transform function transformed_data = transform_func(data) # Write the transformed content to the output ZIP output_zip.writestr(zip_info, transformed_data)"},{"question":"**Programming Task: Work with Future and Task Objects in Asyncio** **Objective:** Write a Python program to create and manage `Future` and `Task` objects using the `asyncio` library. **Task Description:** 1. Create an asynchronous function `async compute_square(fut, n)` that takes a `Future` object `fut` and an integer `n`, calculates the square of `n` after sleeping for 2 seconds, and sets the result to the Future. 2. Write an asynchronous function `async gather_squares(numbers)` that: - Accepts a list of integers `numbers`. - Creates and schedules tasks to compute the square of each number. - Uses `asyncio.ensure_future` to ensure tasks are properly scheduled. - Waits for all Futures to be completed and returns a dictionary mapping each number to its computed square. 3. Implement the `main()` function to: - Get the current event loop. - Call `gather_squares` with the list `[1, 2, 3, 4, 5]`. - Print the resulting dictionary of squares to the console. 4. Run the event loop using `asyncio.run(main())`. **Constraints:** - You must use `asyncio.Future` and `loop.create_future()` for creating Future objects. - Use `asyncio.ensure_future` for scheduling tasks. **Example:** Given `numbers = [1, 2, 3, 4, 5]`, the output should be: ```python {1: 1, 2: 4, 3: 9, 4: 16, 5: 25} ``` **Function Signatures:** ```python import asyncio async def compute_square(fut, n): pass async def gather_squares(numbers): pass async def main(): pass # Start the event loop asyncio.run(main()) ``` **Note:** Remember to handle any possible exceptions and ensure that all tasks are awaited properly to avoid incomplete results or crashes.","solution":"import asyncio async def compute_square(fut, n): # Simulate a delay await asyncio.sleep(2) # Set the result of the Future fut.set_result(n * n) async def gather_squares(numbers): loop = asyncio.get_running_loop() futures = [] for number in numbers: fut = loop.create_future() task = asyncio.ensure_future(compute_square(fut, number)) futures.append((number, fut)) results = {} for number, fut in futures: value = await fut results[number] = value return results async def main(): numbers = [1, 2, 3, 4, 5] result = await gather_squares(numbers) print(result) # Start the event loop if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Automating Built Distribution Creation **Objective**: Write a Python script that automates the creation of built distributions using the `Distutils` package, based on user-specified options. Problem Statement: You need to implement a function `create_built_distribution` that accepts the following parameters: - `formats` (a list of strings): The distribution formats to create. Examples include `[\'gztar\', \'zip\']`. - `setup_path` (a string): The path to the directory containing the `setup.py` file. - `common_options` (a dictionary): Other common options that may need to be passed to the `bdist` command in the format `{option_name: value}`. The function should: 1. Change to the directory specified by `setup_path`. 2. Construct and execute the appropriate `bdist` commands with the specified formats and other options. 3. Print a success message indicating the types of distributions created. 4. Handle any possible exceptions and provide meaningful error messages. Function Signature: ```python def create_built_distribution(formats, setup_path, common_options): pass ``` Input: - `formats`: A list of strings specifying the distribution formats. - `setup_path`: A string specifying the path to the directory containing the `setup.py` file. - `common_options`: A dictionary containing additional options for the `bdist` command. Output: - Print statements indicating the success or failure of the built distribution creation. Example: ```python formats = [\'gztar\', \'zip\'] setup_path = \'/path/to/module\' common_options = { \'release\': \'2\', \'packager\': \'John Doe <jdoe@example.org>\' } create_built_distribution(formats, setup_path, common_options) ``` Constraints: - Ensure that invalid inputs (such as unsupported formats or incorrect paths) are handled gracefully with appropriate error messages. - Assume that the target machine has `Distutils` installed and configured properly. Notes: - You can utilize the `os` and `subprocess` modules for directory navigation and command execution. Good luck!","solution":"import os import subprocess def create_built_distribution(formats, setup_path, common_options): Create built distributions in the specified formats. :param formats: List of distribution formats to create. :param setup_path: Path to the directory containing setup.py. :param common_options: Dictionary of additional bdist options. try: # Change to the specified directory os.chdir(setup_path) # Construct the command command = [\'python\', \'setup.py\', \'bdist\'] + [f\'--formats={\\",\\".join(formats)}\'] # Add common options to the command for option, value in common_options.items(): command.append(f\'--{option}={value}\') # Execute the command subprocess.run(command, check=True) print(f\\"Successfully created distributions in formats: {\', \'.join(formats)}\\") except FileNotFoundError: print(f\\"Error: Directory \'{setup_path}\' not found.\\") except subprocess.CalledProcessError: print(\\"Error: Command execution failed.\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"# Custom Container Implement a custom container class named `CustomList` that emulates the behavior of Pythonâ€™s built-in list. Your implementation should support at least the following operations: 1. **Initialization**: The class should be initialized with an optional list of items. 2. **String Representation**: Implement the `__str__` method to return a string representation of the list in Python list format. 3. **Indexing**: Support indexing to get and set items. 4. **Length**: Implement the `__len__` method to return the number of items in the list. 5. **Containment**: Implement the `__contains__` method to support the `in` keyword. 6. **Iteration**: Implement the `__iter__` method to return an iterator. 7. **Addition**: Support the `+` operator to concatenate two `CustomList` objects. 8. **Equality**: Implement the `__eq__` method to compare two `CustomList` objects for equality. 9. **Append**: Implement an `append` method to add an element to the end of the list. Input and Output Formats: - The `CustomList` class should be initialized with a list or without any arguments. - The `__str__` method should return the list as a string similar to a standard Python list, e.g., `\\"[1, 2, 3]\\"`. - Indexing should work such that `custom_list[0]` returns the first element. - The `__len__` method should return the number of items in the list. - The `__contains__` method should return `True` if an item is in the list and `False` otherwise. - Iteration should allow the `CustomList` to be used in a `for` loop. - Using the `+` operator should concatenate two `CustomList` instances. - The `__eq__` method should return `True` if two `CustomList` instances have the same items in the same order. - The `append` method should add an element to the end of the list. Constraints: - No external libraries should be used. - Standard list operations should be used within the method implementations. # Example Usage: ```python # Creating and displaying a CustomList c1 = CustomList([1, 2, 3]) c2 = CustomList([4, 5]) print(c1) # Output: [1, 2, 3] print(len(c1)) # Output: 3 # Indexing print(c1[0]) # Output: 1 c1[0] = 10 print(c1) # Output: [10, 2, 3] # Containment print(2 in c1) # Output: True print(99 in c1) # Output: False # Iteration for item in c1: print(item) # Output: 10 2 3 # Addition c3 = c1 + c2 print(c3) # Output: [10, 2, 3, 4, 5] # Equality print(c1 == CustomList([10, 2, 3])) # Output: True print(c1 == CustomList([1, 2, 3])) # Output: False # Append c1.append(6) print(c1) # Output: [10, 2, 3, 6] ```","solution":"class CustomList: def __init__(self, items=None): if items is None: self._items = [] else: self._items = list(items) def __str__(self): return str(self._items) def __getitem__(self, index): return self._items[index] def __setitem__(self, index, value): self._items[index] = value def __len__(self): return len(self._items) def __contains__(self, item): return item in self._items def __iter__(self): return iter(self._items) def __add__(self, other): if isinstance(other, CustomList): return CustomList(self._items + other._items) return NotImplemented def __eq__(self, other): if isinstance(other, CustomList): return self._items == other._items return False def append(self, item): self._items.append(item)"},{"question":"**Coding Assessment Question** # Objective: Implement a function that combines manipulating packed binary data and encoding/decoding operations. Your task is to decode a given binary data stream, modify specific data within it, and then re-encode it back to a binary format. # Description: You are given a binary data stream that contains packed data including a sequence of integers and a UTF-8 encoded string. Write a function to: 1. Decode the binary data stream using the `struct` module to extract the sequence of integers and the string. 2. Replace every instance of a specific integer in the sequence with another integer. 3. Encode the modified sequence of integers and the original string back into a binary data stream. # Function Signature: ```python def modify_binary_data(data: bytes, target: int, replacement: int) -> bytes: ``` # Inputs: - `data`: A byte string containing packed integers and a UTF-8 encoded string. - The format is as follows: The first part of the byte string contains a fixed number of packed integers (4 bytes each) and is followed by a null-terminated UTF-8 encoded string. - `target`: The integer in the sequence that needs to be replaced. - `replacement`: The integer to replace the target with. # Outputs: - Return a byte string with the modified sequence of integers and the original string, following the same binary format as the input. # Constraints: - Assume that the byte string contains exactly 5 integers followed by a null-terminated UTF-8 string. - Integers are packed in standard big-endian format (\'>I\'). # Example: ```python # Assume the input data contains [1, 2, 3, 4, 5] + \\"hello\\" data = struct.pack(\'>5I\', 1, 2, 3, 4, 5) + b\'hellox00\' target = 3 replacement = 99 # Expected output should contain [1, 2, 99, 4, 5] + \\"hello\\" expected_output = struct.pack(\'>5I\', 1, 2, 99, 4, 5) + b\'hellox00\' assert modify_binary_data(data, target, replacement) == expected_output ``` # Notes: - Use the `struct` module to handle the packing and unpacking of integers. - Use the `codecs` module, if necessary, for handling the string part. Good luck!","solution":"import struct def modify_binary_data(data: bytes, target: int, replacement: int) -> bytes: Decodes a binary data stream containing 5 integers and a UTF-8 string, replaces target integer with a replacement integer, and re-encodes the data. :param data: Byte string containing packed integers and a UTF-8 string :param target: Integer value that needs to be replaced :param replacement: Integer value that replaces the target :return: Modified byte string # Unpack the first 20 bytes to get the five integers (each integer is 4 bytes) integers = struct.unpack(\'>5I\', data[:20]) # Locate and replace all instances of target with replacement modified_integers = tuple(replacement if x == target else x for x in integers) # Pack the modified integers back into binary format modified_data = struct.pack(\'>5I\', *modified_integers) # Append the original string part (everything after the first 20 bytes) modified_data += data[20:] return modified_data"},{"question":"# Binhex Encoder/Decoder **Objective:** Implement a Python class that can handle binhex4 encoding and decoding operations, similar to the deprecated `binhex` module. Class Definition Create a class `BinHexCodec` with the following methods: 1. `encode(input_file: str, output_file: str) -> None` - **Input:** - `input_file`: Path to the binary input file. - `output_file`: Path to the output file where binhex encoded data will be written. - **Output:** None 2. `decode(input_file: str, output_file: str) -> None` - **Input:** - `input_file`: Path to the binhex encoded input file. - `output_file`: Path to the binary output file to store the decoded data. - **Output:** None Details and Constraints - The encoding should convert a binary file into a binhex4 encoded file (ASCII text). - The decoding should convert a binhex4 encoded file back into its original binary format. - Handle all possible binhex encoding/decoding errors by raising a custom exception `BinHexError`. - You are not allowed to use the deprecated `binhex` module. You may use the `binascii` module as support for ASCII-binary conversions but implement the rest of the functionality yourself. - Ensure that your implementation can handle large files efficiently. Example Usage ```python codec = BinHexCodec() codec.encode(\'example.bin\', \'example.bh4\') codec.decode(\'example.bh4\', \'example_copy.bin\') ``` **Notes:** - The encoded file should be readable in a text editor as plain ASCII. - Make sure to handle different edge cases such as empty files, very large files, and files with non-typical characters.","solution":"import binascii class BinHexError(Exception): Custom exception for BinHex encoding/decoding errors. pass class BinHexCodec: @staticmethod def encode(input_file: str, output_file: str) -> None: try: with open(input_file, \'rb\') as f: binary_data = f.read() hex_data = binascii.hexlify(binary_data).decode(\'ascii\') with open(output_file, \'w\') as out_f: out_f.write(hex_data) except Exception as e: raise BinHexError(f\\"Encoding failed: {str(e)}\\") @staticmethod def decode(input_file: str, output_file: str) -> None: try: with open(input_file, \'r\') as f: hex_data = f.read() binary_data = binascii.unhexlify(hex_data.encode(\'ascii\')) with open(output_file, \'wb\') as out_f: out_f.write(binary_data) except Exception as e: raise BinHexError(f\\"Decoding failed: {str(e)}\\")"},{"question":"Objective To assess the understanding and ability to use Python\'s `concurrent.futures` module, specifically focusing on `ThreadPoolExecutor` and `ProcessPoolExecutor` for managing asynchronous tasks. Question You are tasked with processing a list of URLs to check whether they are reachable (i.e., return a status code of 200). However, the process of checking each URL should be done asynchronously to improve performance. Implement a function `check_urls(urls: List[str], max_workers: int, timeout: float) -> Dict[str, bool]`. This function should: 1. Use `ThreadPoolExecutor` to manage a pool of threads for asynchronous URL checking. 2. Each URL should be checked using a helper function `fetch_status(url: str, timeout: float) -> bool` provided below. 3. Return a dictionary where the keys are the URLs and the values are `True` if the URL is reachable (status code 200) or `False` otherwise. 4. Ensure proper handling of exceptions if a URL is unreachable within the specified timeout. The helper function `fetch_status` is as follows: ```python import urllib.request def fetch_status(url: str, timeout: float) -> bool: try: with urllib.request.urlopen(url, timeout=timeout) as response: return response.status == 200 except: return False ``` Input - `urls: List[str]`: A list of URLs to be checked. - `max_workers: int`: Maximum number of worker threads to use. - `timeout: float`: Timeout for each URL request in seconds. Output - `Dict[str, bool]`: A dictionary mapping each URL to a boolean indicating if the URL is reachable. Example ```python urls = [ \\"http://www.google.com\\", \\"http://www.nonexistentwebsite.com\\", \\"http://www.python.org\\" ] max_workers = 3 timeout = 5.0 result = check_urls(urls, max_workers, timeout) print(result) # Output: {\'http://www.google.com\': True, \'http://www.nonexistentwebsite.com\': False, \'http://www.python.org\': True} ``` Constraints - You may assume the list of URLs will not exceed 100. - The `timeout` value will always be a positive float. - The function should be efficient, utilizing the `ThreadPoolExecutor` correctly to maximize concurrency. Notes - Use the `concurrent.futures` module for managing asynchronous tasks. - Handle exceptions to ensure the program does not crash if a URL is unreachable within the timeout period. - Ensure the threads are properly cleaned up after completion by using the `with` statement when working with the `ThreadPoolExecutor`.","solution":"import concurrent.futures from typing import List, Dict import urllib.request def fetch_status(url: str, timeout: float) -> bool: try: with urllib.request.urlopen(url, timeout=timeout) as response: return response.status == 200 except: return False def check_urls(urls: List[str], max_workers: int, timeout: float) -> Dict[str, bool]: def fetch_status_helper(url: str) -> bool: return fetch_status(url, timeout) results = {} with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_url = {executor.submit(fetch_status_helper, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: results[url] = future.result() except Exception as e: results[url] = False return results"},{"question":"# Python Coding Assessment Question Objective Your task is to demonstrate your understanding of the `dbm` module in Python by creating a simple persistent database that stores and retrieves user information. Problem Statement Write a Python function `manage_user_db(filename: str, actions: list) -> dict:` that performs the following operations on a database: 1. **Open the database**: - Open the specified database file (`filename`) using `dbm.open()`. If the database does not exist, create it. 2. **Process actions**: - The `actions` parameter is a list of tuples where each tuple represents an action to be performed on the database. Each action can be one of three types: - `(\'add\', key, value)`: Add a new record with the given key and value. - `(\'delete\', key)`: Delete the record with the specified key. - `(\'get\', key)`: Retrieve the value associated with the given key. - Note: Keys and values should be handled as bytes internally. Ensure string inputs are converted to bytes before storing and vice versa when retrieving. 3. **Return Results**: - Return a dictionary containing the results of the \'get\' operations. The keys in this dictionary should be the original keys from the actions, and the values should be the corresponding retrieved values or `None` if the key was not found. Constraints - Use `dbm.dumb` for the implementation to ensure compatibility. - Keys and values are to be stored as strings but should be handled as bytes within the database. - Ensure that the database is properly closed after performing all actions. - Handle any exceptions that might occur during database operations and ensure they do not crash the program. Example ```python def manage_user_db(filename: str, actions: list) -> dict: # Your implementation here # Sample usage: actions = [ (\'add\', \'user1\', \'John Doe\'), (\'add\', \'user2\', \'Jane Smith\'), (\'get\', \'user1\'), (\'delete\', \'user2\'), (\'get\', \'user2\') ] result = manage_user_db(\\"userdb\\", actions) print(result) # Output: {\'user1\': \'John Doe\', \'user2\': None} ``` This function will help you practice creating and managing a simple database with Python\'s `dbm` module, specifically its `dumb` implementation. Notes - Make sure to read the documentation carefully and utilize the functions and methods provided by the `dbm` module accurately. - Your function will be tested with different `actions` lists and filenames to ensure its robustness and correctness.","solution":"import dbm.dumb def manage_user_db(filename: str, actions: list) -> dict: results = {} try: with dbm.dumb.open(filename, \'c\') as db: for action in actions: if action[0] == \'add\': key, value = action[1], action[2] db[key.encode(\'utf-8\')] = value.encode(\'utf-8\') elif action[0] == \'delete\': key = action[1] if key.encode(\'utf-8\') in db: del db[key.encode(\'utf-8\')] elif action[0] == \'get\': key = action[1] value = db.get(key.encode(\'utf-8\')) results[key] = value.decode(\'utf-8\') if value is not None else None except Exception as e: print(f\\"An error occurred: {e}\\") return results"},{"question":"**Objective:** Demonstrate your understanding of Python\'s Unicode handling by implementing a function that converts text between different Unicode encodings. **Problem Description:** You need to implement a Python function `convert_encoding(input_string: str, from_encoding: str, to_encoding: str) -> str`. This function should take a string (`input_string`) and convert it from the specified source encoding (`from_encoding`) to the target encoding (`to_encoding`). **Function Signature:** ```python def convert_encoding(input_string: str, from_encoding: str, to_encoding: str) -> str: pass ``` **Parameters:** - `input_string` (str): The original string that needs to be converted. - `from_encoding` (str): The name of the source encoding (e.g., \'utf-8\', \'utf-16\', \'latin-1\'). - `to_encoding` (str): The name of the target encoding (e.g., \'utf-8\', \'utf-16\', \'latin-1\'). **Returns:** - (str): The converted string, encoded in the target encoding. **Constraints:** - The input string is guaranteed to be a valid string in the specified source encoding. - Supported encodings are \'utf-8\', \'utf-16\', \'utf-32\', \'latin-1\', and \'ascii\'. You do not need to handle any encodings outside of these. - Raise a `ValueError` if an unsupported encoding is provided. **Example:** ```python # Convert from utf-8 to utf-16 input_string = \\"Hello, World!\\" from_encoding = \\"utf-8\\" to_encoding = \\"utf-16\\" result = convert_encoding(input_string, from_encoding, to_encoding) print(result) # Output will be a string encoded in utf-16 # Convert from latin-1 to utf-8 input_string = \\"Â¡Hola Mundo!\\" from_encoding = \\"latin-1\\" to_encoding = \\"utf-8\\" result = convert_encoding(input_string, from_encoding, to_encoding) print(result) # Output: \\"Â¡Hola Mundo!\\" ``` **Notes:** - You may use the `codecs` module in Python for encoding and decoding operations. - Be mindful of properly handling byte and string conversions. # Implementation Tips: 1. Start by validating the specified encodings. 2. Decode the `input_string` from the source encoding to a Python Unicode string. 3. Encode the resulting Unicode string to the target encoding. 4. Handle potential exceptions by raising `ValueError` for unsupported encodings.","solution":"import codecs def convert_encoding(input_string: str, from_encoding: str, to_encoding: str) -> str: Converts text from one encoding to another. Parameters: - input_string (str): The original string that needs to be converted. - from_encoding (str): The name of the source encoding. - to_encoding (str): The name of the target encoding. Returns: - str: The converted string, encoded in the target encoding. supported_encodings = [\'utf-8\', \'utf-16\', \'utf-32\', \'latin-1\', \'ascii\'] if from_encoding not in supported_encodings or to_encoding not in supported_encodings: raise ValueError(\\"Unsupported encoding provided.\\") # Decode the input string from the source encoding to Unicode unicode_string = codecs.decode(input_string.encode(from_encoding), from_encoding) # Encode the Unicode string into the target encoding converted_string = codecs.encode(unicode_string, to_encoding).decode(to_encoding) return converted_string"},{"question":"**Question: Analyzing Installed Python Packages Metadata** Your task is to create a Python script that utilizes the `importlib.metadata` module to analyze metadata of installed Python packages. Specifically, you need to write a function `analyze_installed_packages(requirement: str) -> dict` that takes the name of a required package as input and returns a dictionary containing the following information about all installed packages that require the given package as a dependency: 1. Package name 2. Package version 3. All entry points in the `console_scripts` group (names only) 4. Size of all files contained within the package (in bytes) **Function Signature:** ```python def analyze_installed_packages(requirement: str) -> dict: ``` **Input:** - `requirement` (str): A string representing the name of the package that other packages might depend on. **Output:** - A dictionary where keys are the names of packages that have `requirement` in their requirements, and values are sub-dictionaries with the following structure: ```python { \\"version\\": str, \\"entry_points\\": List[str], \\"total_file_size\\": int } ``` **Constraints:** - Your solution must make use of the `importlib.metadata` module. - Ensure to handle packages that do not have certain pieces of metadata. - Focus on performance; minimize the number of metadata queries if possible. **Example:** ```python >>> analyze_installed_packages(\'pytest\') { \'some_package\': { \'version\': \'1.2.3\', \'entry_points\': [\'script1\', \'script2\'], \'total_file_size\': 12345 }, \'another_package\': { \'version\': \'4.5.6\', \'entry_points\': [\'tool\'], \'total_file_size\': 67890 } } ``` In this example, `some_package` and `another_package` are installed packages that have `pytest` as a dependency. The details provided include their versions, any `console_scripts` entry points, and the cumulative size of all files installed by these packages. Consider edge cases such as packages without console scripts or missing file metadata.","solution":"import importlib.metadata from typing import List, Dict def analyze_installed_packages(requirement: str) -> dict: result = {} for dist in importlib.metadata.distributions(): try: dist_name = dist.metadata[\'Name\'] dist_version = dist.version requires = dist.metadata.get_all(\'Requires-Dist\', []) if any(requirement in req for req in requires): # Entry points entry_points = [ ep.name for ep in dist.entry_points if ep.group == \'console_scripts\' ] # Total file size total_file_size = sum( file.size for file in dist.files if file.size is not None ) result[dist_name] = { \\"version\\": dist_version, \\"entry_points\\": entry_points, \\"total_file_size\\": total_file_size } except Exception as e: # Handle the exception gracefully, which could occur in the distributed metadata load. continue return result"},{"question":"# Advanced Python Coding Assessment: Custom Auto-Completion Objective: Implement a custom command-line interface (CLI) auto-completion mechanism using the `rlcompleter` module. Task: You are to implement a custom `Completer` class that is tailored to complete commands for a simplified virtual machine (VM) CLI. The VM understands commands in the form of `COMMAND ARGUMENT`, where `COMMAND` can be one of the following: - `load`: Load a program into the VM. - `run`: Run the currently loaded program. - `status`: Get the current status of the VM. - `help`: Display available commands and their usage. `ARGUMENT` can vary depending on the `COMMAND`: - For `load`: A file name (string). - For `run`: No arguments required. - For `status`: No arguments required. - For `help`: No arguments required. Your task is to complete the following: 1. Implement a custom `Completer` class. 2. Configure it to complete the commands and their arguments as described. 3. Ensure your implementation handles incorrect inputs gracefully. # Input: The input should be a set of test cases where a partial command and its state are provided. For instance: ```python print(completer.complete(\\"lo\\", 0)) # Expected to complete to \\"load\\" print(completer.complete(\\"ru\\", 0)) # Expected to complete to \\"run\\" print(completer.complete(\\"st\\", 0)) # Expected to complete to \\"status\\" print(completer.complete(\\"he\\", 0)) # Expected to complete to \\"help\\" ``` # Output: Output should be the completion results corresponding to the given input. # Constraints: - `Completer.complete(text, state)` must handle all commands and argument completions. - Assume a fictitious list of available files: `[\\"program1.py\\", \\"program2.py\\"]` for usage with `load`. - Handle case-insensitive completions. # Performance Requirements: The completion mechanism should be efficient and responsive, with minimal delay in presenting completions. Example Usage: ```python # Example of binding the completer to readline import readline completer = CustomCompleter() readline.set_completer(completer.complete) readline.parse_and_bind(\\"tab: complete\\") # Example sessions >>> load <TAB> program1.py program2.py >>> run <TAB> >>> status <TAB> >>> help <TAB> ``` Implementation: Ensure your CustomCompleter class fully supports the described completion functionalities and integrates seamlessly with Python\'s readline. ```python import rlcompleter import readline class CustomCompleter(rlcompleter.Completer): commands = [\\"load\\", \\"run\\", \\"status\\", \\"help\\"] files = [\\"program1.py\\", \\"program2.py\\"] def complete(self, text, state): # Implementation goes here pass ``` Implement the logic inside the `complete` function to achieve the desired command and argument completions.","solution":"import rlcompleter import readline class CustomCompleter(rlcompleter.Completer): commands = [\\"load\\", \\"run\\", \\"status\\", \\"help\\"] files = [\\"program1.py\\", \\"program2.py\\"] def complete(self, text, state): options = [] if \\" \\" in text: # Command with argument cmd, arg = text.split(\\" \\", 1) if cmd == \\"load\\": options = [f for f in self.files if f.startswith(arg)] else: # Command completion options = [cmd for cmd in self.commands if cmd.startswith(text)] try: return options[state] except IndexError: return None # Example to bind the completer completer = CustomCompleter() readline.set_completer(completer.complete) readline.parse_and_bind(\\"tab: complete\\")"},{"question":"Objective: Assess the student\'s ability to preprocess a dataset, apply a regression model, and handle potential warnings or issues in scikit-learn. Problem Statement: You are provided with a small synthetic dataset and are asked to perform the following tasks: 1. Preprocess the dataset. 2. Train a `GradientBoostingRegressor` model. 3. Identify and handle any warnings or issues that arise during the model fitting. Provided Information: The dataset is provided in the following CSV format (Please create this dataset locally in the specified format): ``` feature_name_1,feature_name_2,feature_name_3,target -12.32,1.43,7.38,72 1.43,-3.55,12.65,55 30.01,-11.28,-2.12,32 22.17,9.14,11.37,43 -8.8,4.12,19.47,60 ``` Instructions: 1. Read the dataset into a Pandas DataFrame. 2. Split the dataset into features (`X`) and target (`y`). 3. Split the data into training and testing sets with a test size of 33% and a `random_state` of 42. 4. Scale the feature data using `StandardScaler` ensuring that the mean is removed. 5. Train a `GradientBoostingRegressor` model on the training set with the default parameters. 6. Retrain the `GradientBoostingRegressor` model with the `n_iter_no_change` parameter set to 5. 7. Note any warnings or errors that occur when fitting the model and handle them appropriately. Requirements: 1. The code should be written in Python. 2. Use `pandas`, `numpy`, and `scikit-learn` libraries. 3. Handle potential warnings using appropriate methods (i.e., warnings should not disrupt code execution). Expected Output: 1. Print out the first model\'s score on the test data. 2. Print out the second model\'s score on the test data. 3. Print any warnings or errors encountered and explain how they were handled. ```python # Solution Template import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor import warnings # Step 1: Read the dataset df = pd.read_csv(\'path/to/your_data.csv\') # Step 2: Split the dataset into features and target X = df.drop(columns=[\'target\']) y = df[\'target\'] # Step 3: Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Step 4: Scale the feature data scaler = StandardScaler(with_mean=True) X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 5: Train the GradientBoostingRegressor model on the training set gbdt_default = GradientBoostingRegressor(random_state=0) gbdt_default.fit(X_train_scaled, y_train) default_score = gbdt_default.score(X_test_scaled, y_test) print(\'Default model test score:\', default_score) # Step 6: Retrain the model with n_iter_no_change=5 # Handle warnings by capturing and printing them with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") gbdt_custom = GradientBoostingRegressor(random_state=0, n_iter_no_change=5) gbdt_custom.fit(X_train_scaled, y_train) custom_score = gbdt_custom.score(X_test_scaled, y_test) print(\'Custom model test score:\', custom_score) # Print warnings for warning in w: print(\\"Warning captured:\\", warning.message) ``` Evaluate the student\'s answer based on the correctness of preprocessing, model implementation, handling of warnings, and the interpretation of the outputs.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor import warnings # Step 1: Read the dataset into a Pandas DataFrame data = { \\"feature_name_1\\": [-12.32, 1.43, 30.01, 22.17, -8.8], \\"feature_name_2\\": [1.43, -3.55, -11.28, 9.14, 4.12], \\"feature_name_3\\": [7.38, 12.65, -2.12, 11.37, 19.47], \\"target\\": [72, 55, 32, 43, 60] } df = pd.DataFrame(data) # Step 2: Split the dataset into features and target X = df.drop(columns=[\'target\']) y = df[\'target\'] # Step 3: Split the data into training and testing sets with a test size of 33% X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Step 4: Scale the feature data scaler = StandardScaler(with_mean=True) X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 5: Train a GradientBoostingRegressor model on the training set gb_default = GradientBoostingRegressor(random_state=0) gb_default.fit(X_train_scaled, y_train) score_default = gb_default.score(X_test_scaled, y_test) print(\'Default model test score:\', score_default) # Step 6: Retrain the model with n_iter_no_change=5 # Handle warnings by capturing and printing them with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") gb_custom = GradientBoostingRegressor(random_state=0, n_iter_no_change=5) gb_custom.fit(X_train_scaled, y_train) score_custom = gb_custom.score(X_test_scaled, y_test) print(\'Custom model test score:\', score_custom) # Printing any captured warnings for warning in w: print(\\"Warning captured:\\", warning.message)"},{"question":"# Object Manipulation and Attribute Management You are tasked with creating a custom class that utilizes several functions from the Python Object Protocol to manipulate attributes and handle comparisons. The goal is to create a class `CustomObject` that demonstrates the following capabilities: 1. Getting and setting attributes dynamically. 2. Comparing two custom objects using rich comparison methods. 3. Providing a custom string representation of the object. # Task Implement the class `CustomObject` with the following methods: 1. `__init__(self, name, value, data)`: Constructor that initializes the object with a `name`, a `value`, and a dictionary `data`. 2. `get_attr(self, attr_name)`: Returns the value of the attribute `attr_name`. 3. `set_attr(self, attr_name, value)`: Sets the attribute `attr_name` to `value`. 4. `compare(self, other, op)`: Compares `self` with `other` using the comparison operator `op`. Supported operators are: \\"<\\", \\"<=\\", \\"==\\", \\"!=\\", \\">\\", \\">=\\". 5. `__str__(self)`: Returns a string representation of the object in the format: \\"CustomObject(name=<name>, value=<value>)\\". # Constraints - `attr_name` will always be a string. - `other` in `compare` method will always be an instance of `CustomObject`. - The `op` parameter will always be one of the six supported operators. # Example Usage ```python # Initialize objects obj1 = CustomObject(\\"object1\\", 10, {\'a\': 1, \'b\': 2}) obj2 = CustomObject(\\"object2\\", 20, {\'a\': 3, \'b\': 4}) # Get and set attributes print(obj1.get_attr(\'name\')) # Output: object1 obj1.set_attr(\'value\', 100) print(obj1.get_attr(\'value\')) # Output: 100 # Compare objects print(obj1.compare(obj2, \\"==\\")) # Output: False print(obj1.compare(obj2, \\"<\\")) # Output: True # String representation print(str(obj1)) # Output: CustomObject(name=object1, value=100) ``` # Implementation Implement the class `CustomObject` below: ```python class CustomObject: def __init__(self, name, value, data): self.name = name self.value = value self.data = data def get_attr(self, attr_name): pass def set_attr(self, attr_name, value): pass def compare(self, other, op): pass def __str__(self): pass ``` **Note:** Make sure to use attributes and functions from the Python Object Protocol documentation when applicable.","solution":"class CustomObject: def __init__(self, name, value, data): self.name = name self.value = value self.data = data def get_attr(self, attr_name): if hasattr(self, attr_name): return getattr(self, attr_name) else: raise AttributeError(f\\"\'CustomObject\' object has no attribute \'{attr_name}\'\\") def set_attr(self, attr_name, value): setattr(self, attr_name, value) def compare(self, other, op): if not isinstance(other, CustomObject): raise TypeError(\\"Comparisons should be performed with CustomObject instances only\\") if op == \\"<\\": return self.value < other.value elif op == \\"<=\\": return self.value <= other.value elif op == \\"==\\": return self.value == other.value elif op == \\"!=\\": return self.value != other.value elif op == \\">\\": return self.value > other.value elif op == \\">=\\": return self.value >= other.value else: raise ValueError(f\\"Unsupported comparison operator \'{op}\'\\") def __str__(self): return f\\"CustomObject(name={self.name}, value={self.value})\\""},{"question":"# Question: Encoding and Decoding Text with Base64 Variants and Validation You are tasked with implementing a function that will encode and decode data using various `base64` encoding schemes based on input parameters. You need to use the Python `base64` module to achieve this. Function Signature ```python def encode_decode_data(data: str, action: str, encoding_type: str, validate: bool = False) -> str: pass ``` Parameters - `data` (str): The input data to encode or decode. When encoding, this is a plain string. When decoding, this is a Base64 (or its variation) encoded string. - `action` (str): The action to perform; either `\\"encode\\"` or `\\"decode\\"`. - `encoding_type` (str): The type of Base64 encoding to use; one of `\\"standard\\"`, `\\"urlsafe\\"`, `\\"base32\\"`, `\\"base32hex\\"`, `\\"base16\\"`. - `validate` (bool): A flag for validation during decoding; only applicable for Base64 (`\\"standard\\"` and `\\"urlsafe\\"`) decoding. Default is `False`. Returns - `str`: The encoded string if action is `\\"encode\\"` or the decoded string if action is `\\"decode\\"`. Constraints - Only use the `base64` module for encoding/decoding. - For encoding, output must not have any newlines. - For decoding, raise a `ValueError` with the message \\"Invalid data\\" if decoding fails. - The function should only support valid encoding variants as specified in encoding_type. - If the input parameters are invalid (e.g., unrecognized encoding type or action), raise a `ValueError` with the message `\\"Invalid parameters\\"`. Example Usage ```python # Encoding and decoding using standard Base64 encoded = encode_decode_data(\\"hello world\\", \\"encode\\", \\"standard\\") print(encoded) # Output: aGVsbG8gd29ybGQ= decoded = encode_decode_data(\\"aGVsbG8gd29ybGQ=\\", \\"decode\\", \\"standard\\") print(decoded) # Output: hello world # Encoding and decoding using URL-safe Base64 encoded = encode_decode_data(\\"hello world\\", \\"encode\\", \\"urlsafe\\") print(encoded) # Output: aGVsbG8gd29ybGQ= decoded = encode_decode_data(\\"aGVsbG8gd29ybGQ=\\", \\"decode\\", \\"urlsafe\\") print(decoded) # Output: hello world # Error handling try: encode_decode_data(\\"Invalid data!!!\\", \\"decode\\", \\"standard\\") except ValueError as e: print(e) # Output: Invalid data ``` Implement this function to demonstrate your understanding of the base64 module.","solution":"import base64 def encode_decode_data(data: str, action: str, encoding_type: str, validate: bool = False) -> str: def handle_encode(bdata): if encoding_type == \\"standard\\": return base64.b64encode(bdata).decode(\'utf-8\') elif encoding_type == \\"urlsafe\\": return base64.urlsafe_b64encode(bdata).decode(\'utf-8\') elif encoding_type == \\"base32\\": return base64.b32encode(bdata).decode(\'utf-8\') elif encoding_type == \\"base32hex\\": return base64.b32hexencode(bdata).decode(\'utf-8\') elif encoding_type == \\"base16\\": return base64.b16encode(bdata).decode(\'utf-8\') else: raise ValueError(\\"Invalid parameters\\") def handle_decode(bdata): if encoding_type == \\"standard\\": validation_func = base64.b64decode elif encoding_type == \\"urlsafe\\": validation_func = base64.urlsafe_b64decode elif encoding_type == \\"base32\\": validation_func = base64.b32decode elif encoding_type == \\"base32hex\\": validation_func = base64.b32hexdecode elif encoding_type == \\"base16\\": validation_func = base64.b16decode else: raise ValueError(\\"Invalid parameters\\") try: if validate and encoding_type in [\\"standard\\", \\"urlsafe\\"]: base64.b64decode(data, validate=True) return validation_func(bdata).decode(\'utf-8\') except Exception as e: raise ValueError(\\"Invalid data\\") bdata = data.encode(\'utf-8\') if action == \\"encode\\" else data if action == \\"encode\\": return handle_encode(bdata) elif action == \\"decode\\": return handle_decode(bdata) else: raise ValueError(\\"Invalid parameters\\")"},{"question":"You are required to implement a Python script using the `webbrowser` module that takes a list of URLs and opens each of them in a new tab in the default web browser. Additionally, the script should provide an option to open the URLs either in new tabs or new windows based on user preference. # Function Signature ```python def open_urls_in_browser(urls: list, open_in_new_tab: bool = True) -> None: Opens the given list of URLs in a web browser. Args: urls (list): A list of URLs as strings to be opened. open_in_new_tab (bool): A boolean flag indicating whether to open URLs in new tabs (True) or new windows (False). Returns: None ``` # Input - `urls` (list): A list of URLs as strings. For example: `[\\"https://www.google.com\\", \\"https://www.python.org\\"]`. - `open_in_new_tab` (bool): A boolean flag that indicates whether to open URLs in new tabs (`True`) or new windows (`False`). Default is `True`. # Output The function does not return anything. It opens the provided URLs using the system\'s default web browser. # Constraints - Each URL in the `urls` list is a valid URL string. - The list `urls` will have at least one URL and can contain up to 100 URLs. - The function should handle exceptions raised by the `webbrowser` module gracefully, such as `webbrowser.Error`. # Example ```python urls = [\\"https://www.google.com\\", \\"https://www.python.org\\", \\"https://docs.python.org\\"] open_urls_in_browser(urls, open_in_new_tab=True) ``` * This should open all three URLs in new tabs of the default web browser. ```python urls = [\\"https://www.google.com\\", \\"https://www.python.org\\", \\"https://docs.python.org\\"] open_urls_in_browser(urls, open_in_new_tab=False) ``` * This should open all three URLs in new windows of the default web browser. # Notes - Make sure to handle browser opening errors gracefully using `webbrowser.Error`. - For the purpose of this assessment, assume that the runtime environment supports graphical browsers.","solution":"import webbrowser def open_urls_in_browser(urls: list, open_in_new_tab: bool = True) -> None: Opens the given list of URLs in a web browser. Args: urls (list): A list of URLs as strings to be opened. open_in_new_tab (bool): A boolean flag indicating whether to open URLs in new tabs (True) or new windows (False). Returns: None for url in urls: try: if open_in_new_tab: webbrowser.open_new_tab(url) else: webbrowser.open_new(url) except webbrowser.Error as e: print(f\\"Failed to open {url}: {e}\\")"},{"question":"# Seaborn Categorical Data Visualization Problem You are given a dataset related to passengers on the Titanic (`titanic` dataset available in seaborn). Your task is to visualize various relationships within this dataset using categorical plots from the `seaborn` library. This exercise will test your ability to create and customize these plots to derive meaningful insights. Instructions 1. **Load the Dataset**: Load the `titanic` dataset from the `seaborn` library. 2. **Plot 1**: Create a bar plot showing the average fare (`fare`) paid by different passenger classes (`class`), split by the point of embarkation (`embark_town`). Include error bars representing the 95% confidence interval. 3. **Plot 2**: Generate a violin plot that shows the distribution of passenger ages (`age`) in different passenger classes (`class`). Use different colors for males and females (`sex`), and split the plot by gender. 4. **Plot 3**: Display a box plot comparing the fare (`fare`) paid by different passenger classes (`class`) and embarkation points (`embark_town`). Visualize the plots on a single grid with row-wise faceting by embarkation town. 5. **Plot 4**: Create a point plot to visualize the survival rate (`survived`) across different passenger classes (`class`) and different genders (`sex`). Use different markers and line styles for each gender for better distinction. Requirements - Use the `catplot` function for creating these plots. - Customize the plots to include appropriate titles, axis labels, and legends wherever necessary. - Ensure your plots are not overly cluttered and are easy to interpret visually. - Output all plots in a single script. Notes - The `titanic` dataset can be loaded using `sns.load_dataset(\\"titanic\\")`. - Make sure to adjust parameters like `dodge`, `split`, `aspect`, `height`, etc., as necessary to improve the clarity and readability of your plots. - Remember to import necessary libraries (`seaborn`, `matplotlib.pyplot`, etc.). Example Output Here is an example of how your visualizations might look (note that these are just for illustration purposes, focus on the instructions above to create the plots): ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Plot 1: Bar plot for average fare by class and embarkation town sns.catplot( data=titanic, x=\\"class\\", y=\\"fare\\", hue=\\"embark_town\\", kind=\\"bar\\", ci=\\"sd\\" ) plt.title(\'Average Fare by Class and Embarkation Town\') # Plot 2: Violin plot for age distribution by class and gender sns.catplot( data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", kind=\\"violin\\", split=True ) plt.title(\'Age Distribution by Class and Gender\') # Plot 3: Box plot for fare by class and embarkation town with row faceting sns.catplot( data=titanic, x=\\"class\\", y=\\"fare\\", kind=\\"box\\", col=\\"embark_town\\" ) plt.subplots_adjust(top=0.9) plt.suptitle(\'Fare by Class and Embarkation Town\') # Plot 4: Point plot for survival rate by class and gender sns.catplot( data=titanic, x=\\"class\\", y=\\"survived\\", hue=\\"sex\\", markers=[\\"o\\", \\"x\\"], linestyles=[\\"-\\", \\"--\\"], kind=\\"point\\" ) plt.title(\'Survival Rate by Class and Gender\') plt.show() ``` This example provides a basic framework; ensure your final code meets all stated requirements and is well-commented.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_data(): # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Plot 1: Bar plot showing the average fare paid by different passenger classes, split by the point of embarkation sns.catplot( data=titanic, x=\\"class\\", y=\\"fare\\", hue=\\"embark_town\\", kind=\\"bar\\", ci=95 ).set(title=\'Average Fare by Class and Embarkation Town\') # Plot 2: Violin plot showing the distribution of passenger ages in different passenger classes, split by gender sns.catplot( data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", kind=\\"violin\\", split=True ).set(title=\'Age Distribution by Class and Gender\') # Plot 3: Box plot comparing the fare paid by different passenger classes and embarkation points with row-wise faceting by embarkation town sns.catplot( data=titanic, x=\\"class\\", y=\\"fare\\", kind=\\"box\\", col=\\"embark_town\\" ).set_titles(\'Fare by Class and Embarkation Town\') # Plot 4: Point plot to visualize the survival rate across different passenger classes and genders sns.catplot( data=titanic, x=\\"class\\", y=\\"survived\\", hue=\\"sex\\", markers=[\\"o\\", \\"x\\"], linestyles=[\\"-\\", \\"--\\"], kind=\\"point\\" ).set(title=\'Survival Rate by Class and Gender\') plt.show()"},{"question":"You are working on a project that requires dynamically importing and processing Python modules contained within a ZIP archive. To manage this efficiently, you\'ll use the `zipimport` module. Task 1. **Write a function `import_and_execute`**: - This function should accept the following parameters: - `zip_path`: The path to the ZIP file. - `module_name`: The fully qualified name of the module to be imported. - `function_name`: The name of the function within the module that needs to be executed. - `args`: A tuple of arguments to be passed to the function. - The function should import the specified module from the ZIP file and execute the specified function with the provided arguments. - The function should return the output of the executed function. 2. **Implement necessary error handling**: - If the ZIP archive does not exist or is not a valid ZIP archive, raise a `FileNotFoundError`. - If the module or function within the module does not exist in the ZIP archive, raise an `ImportError`. Input / Output format - **Input**: - `zip_path` (string): Path to the ZIP file. - `module_name` (string): Fully qualified module name to be imported. - `function_name` (string): Name of the function to execute from the imported module. - `args` (tuple): Arguments to pass to the function. - **Output**: - The output of the executed function, or an appropriate exception if any errors are encountered. Constraints - The ZIP file should contain valid Python modules (`.py` or `.pyc` files). - Ensure your solution can handle both `.py` and `.pyc` files in the ZIP archive. - Performance is not a primary concern, but your solution should be robust and handle typical exceptions and errors gracefully. Example: ```python # Assume `example.zip` contains a file `mymodule.py` with the following content: # def myfunction(x, y): # return x + y output = import_and_execute(\'example.zip\', \'mymodule\', \'myfunction\', (3, 5)) print(output) # Expected: 8 ``` ```python # Another example where a .pyc file might be involved instead of a .py file # For the purposes of this example, assume `compiled_example.zip` contains # `compiledmodule.pyc` with a function defined as: # def compiled_function(x, y): # return x * y output = import_and_execute(\'compiled_example.zip\', \'compiledmodule\', \'compiled_function\', (3, 5)) print(output) # Expected: 15 ``` Good luck!","solution":"import zipimport import os def import_and_execute(zip_path, module_name, function_name, args): Imports a module from a ZIP file and executes a specified function with provided arguments. Parameters: zip_path (str): The path to the ZIP file. module_name (str): The fully qualified name of the module to be imported. function_name (str): The name of the function within the module to be executed. args (tuple): Arguments to pass to the function. Returns: The output of the executed function. if not os.path.isfile(zip_path): raise FileNotFoundError(f\\"The file {zip_path} does not exist or is not a valid ZIP archive.\\") try: importer = zipimport.zipimporter(zip_path) module = importer.load_module(module_name) except zipimport.ZipImportError as e: raise ImportError(f\\"Could not import module {module_name} from {zip_path}: {e}\\") if not hasattr(module, function_name): raise ImportError(f\\"The module {module_name} does not have a function named {function_name}.\\") function_to_call = getattr(module, function_name) return function_to_call(*args)"},{"question":"# AsyncIO Custom Exception Handling You are tasked with writing an asynchronous function that reads data chunks from a simulated stream. The function should handle various exceptions from the asyncio package appropriately. Requirements: 1. Implement an asynchronous function `read_from_stream` that takes two parameters: - `stream` (an asynchronous generator that yields byte chunks) - `max_buffer_size` (an integer specifying the maximum buffer size before raising an error) 2. Your function should: - Read bytes from the stream. - Raise an `asyncio.TimeoutError` if the read operation times out (simulate this situation). - Raise an `asyncio.IncompleteReadError` if the stream ends before reading the expected bytes. - Raise an `asyncio.LimitOverrunError` if the buffer exceeds `max_buffer_size`. - Properly handle any `asyncio.CancelledError` by cleaning up resources and re-raising the exception. Expected Input: - `stream`: an asynchronous generator yielding byte chunks (e.g., `[b\'data1\', b\'data2\', b\'data3\']`) - `max_buffer_size`: an integer (e.g., `1024`) Expected Output: - A complete byte string read from the stream. Constraints: - Simulate scenarios where the exceptions can be raised for testing purposes. - Write unit tests to verify that your function handles these exceptions correctly. Example: ```python import asyncio class SimulatedStream: def __init__(self, data_chunks, delay=1): self.data_chunks = data_chunks self.delay = delay async def __aiter__(self): for chunk in self.data_chunks: await asyncio.sleep(self.delay) yield chunk async def read_from_stream(stream, max_buffer_size): # Implement the function based on the requirements pass async def main(): simulated_stream = SimulatedStream([b\'data1\', b\'data2\']) try: data = await read_from_stream(simulated_stream, 10) print(data) except asyncio.TimeoutError: print(\\"Operation timed out.\\") except asyncio.IncompleteReadError as e: print(f\\"Incomplete read: expected {e.expected}, got {e.partial}\\") except asyncio.LimitOverrunError as e: print(f\\"Buffer limit overrun: consumed {e.consumed}\\") except asyncio.CancelledError: print(\\"Operation cancelled.\\") # Run the test asyncio.run(main()) ``` **Note:** Make sure to simulate the exception scenarios within your testing framework.","solution":"import asyncio class SimulatedStream: def __init__(self, data_chunks, delay=0.1): self.data_chunks = data_chunks self.delay = delay async def __aiter__(self): for chunk in self.data_chunks: await asyncio.sleep(self.delay) yield chunk async def read_from_stream(stream, max_buffer_size): buffer = bytearray() try: async for chunk in stream: if len(buffer) + len(chunk) > max_buffer_size: raise asyncio.LimitOverrunError(\\"Buffer size limit exceeded\\", len(buffer) + len(chunk)) buffer.extend(chunk) # Simulate timeout await asyncio.sleep(0.02) if False: # Replace with actual condition to trigger TimeoutError raise asyncio.TimeoutError return bytes(buffer) except asyncio.CancelledError: # Clean up resources if needed raise"},{"question":"Custom Object Serialization with `copyreg` Problem Description You are required to create a custom class `Person` and use the `copyreg` module to define how instances of this class should be serialized (pickled) and deserialized (unpickled). The class should also include functionality that demonstrates your understanding of custom pickling and copying mechanisms. Task 1. **Implement the `Person` Class**: - The `Person` class should have the following attributes: - `name`: A string representing the person\'s name. - `age`: An integer representing the person\'s age. - Implement the `__init__`, `__repr__`, and `__eq__` methods for this class to handle initialization, string representation, and equality checks, respectively. 2. **Define Pickling Functions**: - Write a reduction function `pickle_person` for the `Person` class. This function should ensure that all necessary information of a `Person` instance is preserved for serialization. The function should return a tuple containing the class and a tuple of arguments needed to reconstruct the object. - Use `copyreg.pickle` to register this reduction function for the `Person` class. 3. **Test Serialization and Deserialization**: - Create an instance of the `Person` class. - Serialize (pickle) this instance using `pickle.dumps()`. - Deserialize (unpickle) the serialized data using `pickle.loads()`. - Verify and assert that the deserialized object is equal to the original instance. 4. **Test Copy Functionality**: - Create a copy of the `Person` instance using `copy.copy()`. - Verify and assert that the copied object is equal to the original instance. Input and Output Formats - **Input**: No explicit input; you are writing a class and functions as specified. - **Output**: - Ensure serialization and deserialization of `Person` instances work correctly. - Ensure copying of `Person` instances works correctly. Constraints - You must use the `copyreg` module for defining pickling behavior. - The `Person` class must be properly pickled and unpickled using the defined reduction function. - The copied instance should maintain the same attribute values as the original instance. Example ```python import copyreg, copy, pickle # Define the Person class class Person: def __init__(self, name, age): self.name = name self.age = age def __repr__(self): return f\'Person(name={self.name}, age={self.age})\' def __eq__(self, other): return isinstance(other, Person) and self.name == other.name and self.age == other.age # Define the pickling function def pickle_person(person): return Person, (person.name, person.age) # Register the pickling function copyreg.pickle(Person, pickle_person) # Testing serialization and deserialization original_person = Person(\\"Alice\\", 30) serialized_person = pickle.dumps(original_person) deserialized_person = pickle.loads(serialized_person) assert original_person == deserialized_person # Testing copying copied_person = copy.copy(original_person) assert original_person == copied_person ``` Implement the `Person` class and the necessary functions as described above. Verify serialization, deserialization, and copying functionality by running the provided example and ensuring all assertions pass.","solution":"import copyreg import pickle import copy class Person: def __init__(self, name, age): self.name = name self.age = age def __repr__(self): return f\'Person(name={self.name}, age={self.age})\' def __eq__(self, other): return isinstance(other, Person) and self.name == other.name and self.age == other.age def pickle_person(person): return Person, (person.name, person.age) copyreg.pickle(Person, pickle_person)"},{"question":"PyTorch Coding Assessment Question **Objective:** Implement a custom neural network in PyTorch and initialize the network\'s parameters using specific initialization methods from `torch.nn.init`. **Problem Statement:** You are tasked with creating a simple fully connected neural network in PyTorch designed to classify MNIST digits (0-9). Your network should consist of three linear layers with the following specifications: - Input layer: 784 input features (28x28 images flattened) to 128 hidden units - Hidden layer: 128 hidden units to 64 hidden units - Output layer: 64 hidden units to 10 output classes Implement the neural network and initialize its parameters using the following methods: 1. Initialize the weights of the input layer using `xavier_uniform_`. 2. Initialize the weights of the hidden layer using `kaiming_normal_`. 3. Initialize the weights of the output layer using `normal_` with a mean of 0 and standard deviation of 0.01. 4. Initialize all biases to `zeros_`. **Requirements:** - Your implementation should define the neural network architecture and include a method to apply the specific initialization schemes to each layer. - You must also include a forward method that defines the forward pass of the network. - Clearly specify how the weights and biases are initialized in your code. - Provide a small snippet of code to demonstrate the creation of your network and verify that the custom initialization has been applied. **Expected Input and Output Formats:** - **Input:** There are no direct inputs to the network class itself, but it should be designed to process inputs of shape `(batch_size, 784)` in its forward method. - **Output:** The forward method returns the class scores for each input sample in the shape `(batch_size, 10)`. **Constraints:** - Use only the functions available under `torch.nn.init` for initializing the weights and biases. **Example Usage:** ```python import torch class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, 10) # Initialize weights and biases here self._initialize_weights() def _initialize_weights(self): torch.nn.init.xavier_uniform_(self.fc1.weight) torch.nn.init.kaiming_normal_(self.fc2.weight) torch.nn.init.normal_(self.fc3.weight, mean=0, std=0.01) torch.nn.init.zeros_(self.fc1.bias) torch.nn.init.zeros_(self.fc2.bias) torch.nn.init.zeros_(self.fc3.bias) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x # Instantiate and check the network model = CustomNet() print(model) ``` Please ensure your implementation correctly applies the specified initializations.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, 10) # Initialize weights and biases here self._initialize_weights() def _initialize_weights(self): torch.nn.init.xavier_uniform_(self.fc1.weight) torch.nn.init.kaiming_normal_(self.fc2.weight) torch.nn.init.normal_(self.fc3.weight, mean=0, std=0.01) torch.nn.init.zeros_(self.fc1.bias) torch.nn.init.zeros_(self.fc2.bias) torch.nn.init.zeros_(self.fc3.bias) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x # Demonstration of network creation and initialization model = CustomNet() print(model)"},{"question":"You will create a function that converts a list of DLPack tensors into PyTorch tensors, processes them, and then converts them back to DLPack tensors. This will demonstrate your ability to work with the `torch.utils.dlpack` module and handle tensor data exchange. # Function Signature ```python def process_dlpack_tensors(dlpack_tensors: list) -> list: Convert a list of DLPack tensors to PyTorch tensors, apply a processing function, and convert them back to DLPack tensors. Args: - dlpack_tensors (list): A list of tensors in DLPack format. Returns: - list: A list of processed tensors in DLPack format. ``` # Detailed Requirements 1. **Input Format:** - `dlpack_tensors`: A list of N tensors in DLPack format. 2. **Output Format:** - A list of N tensors that have been processed and converted back to the DLPack format. 3. **Processing Details:** - For each tensor, apply element-wise multiplication by a factor of 2. 4. **Constraints:** - Assume the input DLPack tensors are valid and of compatible dimensions for element-wise operations. - You can use any in-built PyTorch functions needed to handle the conversions and operations. 5. **Library Usage:** - Utilize `torch.utils.dlpack.from_dlpack` to convert from DLPack to PyTorch tensors. - Utilize `torch.utils.dlpack.to_dlpack` to convert from PyTorch tensors back to DLPack format. # Example ```python # A mock example of how the function might be used: dlpack_tensor_1 = ... # Assume this is a valid DLPack tensor dlpack_tensor_2 = ... # Assume this is another valid DLPack tensor dlpack_tensors = [dlpack_tensor_1, dlpack_tensor_2] processed_tensors = process_dlpack_tensors(dlpack_tensors) # processed_tensors should contain the tensors with all elements multiplied by 2, back in DLPack format. ``` # Notes: - You do not need to create actual DLPack tensors for this assessment, but you must show the correct usage of `from_dlpack` and `to_dlpack` within your function definition. - Focus on handling the tensors within PyTorch once they are converted and demonstrate clear understanding of tensor operations. - Proper error handling and comments explaining your code are expected for evaluating your understanding and ability to write maintainable code.","solution":"import torch from torch.utils.dlpack import from_dlpack, to_dlpack def process_dlpack_tensors(dlpack_tensors: list) -> list: Convert a list of DLPack tensors to PyTorch tensors, apply a processing function, and convert them back to DLPack tensors. Args: - dlpack_tensors (list): A list of tensors in DLPack format. Returns: - list: A list of processed tensors in DLPack format. processed_dlpack_tensors = [] for dl_tensor in dlpack_tensors: # Convert DLPack tensor to PyTorch tensor torch_tensor = from_dlpack(dl_tensor) # Process the tensor (element-wise multiplication by 2) processed_tensor = torch_tensor * 2 # Convert the processed PyTorch tensor back to DLPack tensor processed_dlpack_tensor = to_dlpack(processed_tensor) # Append to the result list processed_dlpack_tensors.append(processed_dlpack_tensor) return processed_dlpack_tensors"},{"question":"# C Extension and Python Integration You are tasked with creating a Python package that includes a C extension. This exercise will test your understanding of both C and Python integration through the `distutils` package. The extension will include a simple C function that performs a basic mathematical operation, and you will write the necessary setup script to compile and build the extension. Part 1: Write the C Code Create a C source file named `my_math.c` with the following function: - **Function Name**: `power` - **Parameters**: - Two integers `base` and `exponent` - **Returns**: - An integer result of `base` raised to the power of `exponent`. Example: ```c // my_math.c #include <Python.h> static PyObject* power(PyObject* self, PyObject* args) { int base, exponent, result = 1; if (!PyArg_ParseTuple(args, \\"ii\\", &base, &exponent)) return NULL; for (int i = 0; i < exponent; ++i) { result *= base; } return Py_BuildValue(\\"i\\", result); } static PyMethodDef MyMathMethods[] = { {\\"power\\", power, METH_VARARGS, \\"Raise a number to a power\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef mymathmodule = { PyModuleDef_HEAD_INIT, \\"my_math\\", NULL, -1, MyMathMethods }; PyMODINIT_FUNC PyInit_my_math(void) { return PyModule_Create(&mymathmodule); } ``` Part 2: Write the Setup Script Create a Python setup script `setup.py` to build the C extension module. The setup script should: - Define an extension named `my_math` with the source file `my_math.c`. - Include appropriate metadata for the package, such as name, version, and description. Example: ```python # setup.py from distutils.core import setup, Extension module1 = Extension(\'my_math\', sources=[\'my_math.c\']) setup(name=\'MyMathPackage\', version=\'1.0\', description=\'This is a simple math package with a power function\', ext_modules=[module1]) ``` Part 3: Build and Test the Extension 1. Use the command `python setup.py build` to compile the extension. 2. Test the compiled extension in a Python script: ```python # test_my_math.py import my_math print(my_math.power(2, 3)) # Should output 8 print(my_math.power(5, 2)) # Should output 25 ``` # Submission Requirements - C source file: `my_math.c` - Setup script: `setup.py` - Test script: `test_my_math.py` Ensure your package builds correctly and performs as expected.","solution":"def power(base, exponent): Returns the result of base raised to the power of exponent. result = 1 for _ in range(exponent): result *= base return result"},{"question":"# Question: Implementing a Boolean Management Utility You are tasked with implementing a small utility that manages Boolean values and performs type checks, ensuring that the correct Boolean object is returned with proper reference count handling. The utility should provide functions to check if a given object is Boolean, create Boolean objects from integers, and return Boolean objects with accurate reference counts. Requirements: 1. Implement a function `is_boolean(obj)` that checks if the input `obj` is of Boolean type. The output should be `True` if `obj` is a Boolean, otherwise `False`. 2. Implement a function `long_to_boolean(value)` that returns a Boolean object (`True` or `False`) based on the truth value of the input integer `value`. This function should correctly handle the reference count for the returned Boolean object. 3. Implement functions `return_true()` and `return_false()` that return the `True` and `False` Boolean objects, respectively, ensuring proper reference count handling. Input: - `is_boolean(obj)`: a single input `obj` of any type. - `long_to_boolean(value)`: a single input `value` of integer type. - `return_true()` and `return_false()`: take no inputs. Output: - `is_boolean(obj)`: Boolean `True` or `False`. - `long_to_boolean(value)`: Boolean `True` or `False`. - `return_true()` and `return_false()`: Boolean `True` or `False`. Constraints: - You are required to use the Boolean type checking and reference handling concepts described. - Optimize for Python\'s reference count mechanism to ensure efficiency. Example Usage: ```python assert is_boolean(True) == True assert is_boolean(1) == False assert long_to_boolean(5) == True assert long_to_boolean(0) == False assert return_true() == True assert return_false() == False ``` Implement the above utility functions to demonstrate your understanding of Boolean object management in Python.","solution":"def is_boolean(obj): Returns True if the object is of Boolean type, False otherwise. return isinstance(obj, bool) def long_to_boolean(value): Converts an integer to a Boolean object. If the integer is non-zero, return True. Otherwise, return False. return bool(value) def return_true(): Returns the Boolean object True. return True def return_false(): Returns the Boolean object False. return False"},{"question":"**Objective:** Implement a Python function that performs a series of file and directory operations using the `pathlib`, `shutil`, and `tempfile` modules. **Question:** Design a function `manage_file_system()` that performs the following tasks: 1. Creates a temporary directory. 2. Within this directory, creates three text files named `file1.txt`, `file2.txt`, and `file3.txt`. Write some sample text into each file. 3. Copies `file1.txt` and `file2.txt` to a new directory named `backup` within the temporary directory. 4. Compares the copied files in the `backup` directory with the original files to ensure the content matches. 5. Deletes `file3.txt`. The function should return the paths of the created temporary directory and the `backup` directory as a tuple and confirm the results of the file comparison as a dictionary. **Function Signature:** ```python def manage_file_system() -> (str, str, dict): pass ``` **Constraints:** - The function should handle any exceptions that may arise during file and directory operations and print relevant error messages. - Use the `pathlib` module for handling filesystem paths. - Use the `shutil` module for copying files. - Use the `tempfile` module for creating the temporary directory. **Example Output:** Suppose the function creates a temporary directory at `/tmp/tmpabcd1234` and a `backup` directory at `/tmp/tmpabcd1234/backup`. ```python manage_file_system() ``` Output: ```python (\'/tmp/tmpabcd1234\', \'/tmp/tmpabcd1234/backup\', {\'file1.txt\': True, \'file2.txt\': True}) ``` **Notes:** - The paths provided in the example output are typical Unix-style paths. On Windows, they may differ. - The dictionary in the output confirms whether the files in the `backup` directory match the originals (True means they match).","solution":"import tempfile from pathlib import Path import shutil def manage_file_system() -> tuple: try: # Create a temporary directory tmp_dir = Path(tempfile.mkdtemp()) backup_dir = tmp_dir / \'backup\' backup_dir.mkdir() # Create text files in the temporary directory files_data = { \'file1.txt\': \'This is the content of file1.\', \'file2.txt\': \'This is the content of file2.\', \'file3.txt\': \'This is the content of file3.\' } for file_name, content in files_data.items(): file_path = tmp_dir / file_name with open(file_path, \'w\') as f: f.write(content) # Copy file1.txt and file2.txt to the backup directory shutil.copy(tmp_dir / \'file1.txt\', backup_dir / \'file1.txt\') shutil.copy(tmp_dir / \'file2.txt\', backup_dir / \'file2.txt\') # Verify the copied files comparison_results = {} for file_name in [\'file1.txt\', \'file2.txt\']: original_file = tmp_dir / file_name copied_file = backup_dir / file_name comparison_results[file_name] = original_file.read_text() == copied_file.read_text() # Delete file3.txt (tmp_dir / \'file3.txt\').unlink() return str(tmp_dir), str(backup_dir), comparison_results except Exception as e: print(f\\"An error occurred: {e}\\") return None"},{"question":"Objective: Implement and evaluate a semi-supervised learning model using scikit-learn\'s `LabelPropagation` method. You will be provided with a dataset containing a mix of labeled and unlabeled points. Your task is to accurately classify the unlabeled points and evaluate the overall performance of your model. Dataset: You are given a dataset with features `X` and target labels `y`, where `y` contains both labeled and unlabeled points. The unlabeled points are denoted by `-1`. ```python import numpy as np # Example dataset X = np.array([ [1.0, 2.1], [1.5, 1.8], [5.1, 6.2], [6.3, 5.8], [1.2, 1.5], [6.9, 6.2] ]) y = np.array([0, 0, 1, -1, -1, 1]) ``` Task: 1. **Load the dataset**: Use the provided dataset `X` and `y`. 2. **Implement Label Propagation**: - Configure the `LabelPropagation` model with an RBF kernel. - Fit the model to the dataset. - Predict the labels for all data points (including unlabeled points). 3. **Evaluate the model**: - Calculate the accuracy of the model on the labeled points. - Output the predicted labels for the entire dataset and the model accuracy on the labeled points. Constraints and Details: - Use `LabelPropagation` from `sklearn.semi_supervised`. - Assume `gamma=20` for the RBF kernel. - Calculate accuracy using only the originally labeled points (exclude the -1 labeled points). Expected Output: - A list of predicted labels for the entire dataset. - Accuracy of the model on the originally labeled points. Example Output: ```python Predicted Labels: [0, 0, 1, 1, 0, 1] Model Accuracy: 100.0% ``` # Submission: Submit your implementation as a Python script or Jupyter notebook.","solution":"import numpy as np from sklearn.semi_supervised import LabelPropagation from sklearn.metrics import accuracy_score def label_propagation(X, y): # Create and configure the LabelPropagation model label_prop_model = LabelPropagation(kernel=\'rbf\', gamma=20) # Fit the model to the dataset label_prop_model.fit(X, y) # Predict the labels for all data points y_pred = label_prop_model.predict(X) # Evaluate the model accuracy on the originally labeled points mask = y != -1 accuracy = accuracy_score(y[mask], y_pred[mask]) return y_pred, accuracy * 100 # Example dataset X = np.array([ [1.0, 2.1], [1.5, 1.8], [5.1, 6.2], [6.3, 5.8], [1.2, 1.5], [6.9, 6.2] ]) y = np.array([0, 0, 1, -1, -1, 1]) predicted_labels, model_accuracy = label_propagation(X, y) print(f\\"Predicted Labels: {predicted_labels}\\") print(f\\"Model Accuracy: {model_accuracy}%\\")"},{"question":"You are tasked with creating a Python module that mimics some of the functionalities provided by the `PyFunction` C API, specifically for creating and inspecting function objects. Your implementation will involve: 1. Verifying if an object is a function. 2. Creating a new function object. 3. Retrieving and setting the function\'s code object and global variables. # Requirements 1. **Function Verification**: Write a function `is_function(obj)` that checks if the provided `obj` is a Python function object. ```python def is_function(obj) -> bool: Check if the given object is a function object. :param obj: Object to check. :type obj: Any :return: True if object is a function, False otherwise. :rtype: bool ``` 2. **Function Creation**: Write a function `create_function(code, globals_)` that creates a new function object using the provided code object and globals dictionary. The function should have a default name \\"default_func\\" if not specified within the code object. ```python def create_function(code, globals_) -> \'function\': Create a new function object associated with the code object and globals. :param code: The code object to associate with the function. :type code: CodeType :param globals_: The global variables dictionary. :type globals_: dict :return: A new function object. :rtype: function ``` 3. **Retrieve and Set Function Attributes**: Write a function `get_function_details(func)` to retrieve the code object and globals dictionary associated with a function object. Additionally, write a `set_function_globals(func, globals_)` function to set the globals dictionary for a given function. ```python def get_function_details(func) -> dict: Retrieve the code object and globals dictionary from the function. :param func: The function object to inspect. :type func: function :return: A dictionary with \'code\' and \'globals\' keys. :rtype: dict def set_function_globals(func, globals_) -> None: Set the globals dictionary for the given function object. :param func: The function object to modify. :type func: function :param globals_: The global variables dictionary to set. :type globals_: dict :return: None ``` # Constraints 1. All input parameters and return values must adhere to the specified types. 2. You should handle any possible errors gracefully and raise appropriate exceptions when necessary. 3. You are not allowed to use any external libraries; only standard Python libraries and built-in functions are permitted. # Example Usage ```python # Example code object and globals dictionary code = compile(\'def generated_func(): return \\"Hello, World!\\"\', \'<string>\', \'exec\') globals_ = {\'__name__\': \'__main__\'} # Create a function object new_func = create_function(code, globals_) # Verify if object is a function print(is_function(new_func)) # Output: True # Retrieve function details details = get_function_details(new_func) print(details[\'code\']) # Output: <code object generated_func at ...> print(details[\'globals\']) # Output: {\'__name__\': \'__main__\'} # Modify function\'s globals new_globals = {\'__name__\': \'__main__\', \'extra_var\': 42} set_function_globals(new_func, new_globals) details = get_function_details(new_func) print(details[\'globals\']) # Output: {\'__name__\': \'__main__\', \'extra_var\': 42} ``` # Submission Submit your implementation as a single Python file with the name `function_inspector.py`, containing the functions `is_function`, `create_function`, `get_function_details`, and `set_function_globals`.","solution":"from types import FunctionType, CodeType def is_function(obj) -> bool: Check if the given object is a function object. :param obj: Object to check. :type obj: Any :return: True if object is a function, False otherwise. :rtype: bool return isinstance(obj, FunctionType) def create_function(code, globals_) -> FunctionType: Create a new function object associated with the code object and globals. :param code: The code object to associate with the function. :type code: CodeType :param globals_: The global variables dictionary. :type globals_: dict :return: A new function object. :rtype: function if not isinstance(code, CodeType): raise TypeError(\\"The \'code\' parameter must be a CodeType object.\\") if not isinstance(globals_, dict): raise TypeError(\\"The \'globals_\' parameter must be a dictionary.\\") # Create the function object exec(code, globals_) func_name = code.co_names[0] if code.co_names else \\"default_func\\" return globals_[func_name] def get_function_details(func) -> dict: Retrieve the code object and globals dictionary from the function. :param func: The function object to inspect. :type func: function :return: A dictionary with \'code\' and \'globals\' keys. :rtype: dict if not is_function(func): raise TypeError(\\"The provided object is not a function.\\") return { \'code\': func.__code__, \'globals\': func.__globals__ } def set_function_globals(func, globals_) -> None: Set the globals dictionary for the given function object. :param func: The function object to modify. :type func: function :param globals_: The global variables dictionary to set. :type globals_: dict :return: None if not is_function(func): raise TypeError(\\"The provided object is not a function.\\") if not isinstance(globals_, dict): raise TypeError(\\"The \'globals_\' parameter must be a dictionary.\\") func.__globals__.update(globals_)"},{"question":"**Objective**: This question will test your understanding of Python\'s Distutils package and your ability to work with project metadata. **Question**: You have been tasked with creating a Python script that reads the metadata of an installed Python package and prints out the package\'s name, version, and description. **Requirements**: 1. Your script should accept the path to the `.egg-info` directory as an argument. 2. It should read the `PKG-INFO` file located in the given `.egg-info` directory. 3. Finally, it should output the package name, version, and description in a readable format. Implement the function `print_package_info`. The function signature should be: ```python def print_package_info(egg_info_path: str) -> None: pass # Your implementation here ``` **Parameters**: - `egg_info_path` (str): The path to the directory containing the `PKG-INFO` file. **Constraints**: - You can assume that the `PKG-INFO` file always exists within the specified path and that it is correctly formatted. - You should handle any potential file reading errors gracefully. - The output should be printed to the console. **Example**: If the file at `example_package-1.0.0-py3.10.egg-info/PKG-INFO` contains: ``` Metadata-Version: 1.2 Name: example_package Version: 1.0.0 Summary: A simple example package Home-page: http://example.com Author: Example Author Author-email: author@example.com License: MIT Description: This package provides an example of using PKG-INFO. ``` Then running: ```python print_package_info(\'example_package-1.0.0-py3.10.egg-info\') ``` Should output: ``` Package Name: example_package Version: 1.0.0 Description: This package provides an example of using PKG-INFO. ``` **Hints**: - Review the `Distutils` documentation, specifically the section on reading metadata. - The `DistributionMetadata` class could be useful.","solution":"import os def print_package_info(egg_info_path: str) -> None: try: pkg_info_path = os.path.join(egg_info_path, \\"PKG-INFO\\") with open(pkg_info_path, \'r\') as file: name = version = description = None description_started = False description_lines = [] for line in file: if description_started: description_lines.append(line.strip()) elif line.startswith(\'Name: \'): name = line.split(\\"Name: \\")[1].strip() elif line.startswith(\'Version: \'): version = line.split(\\"Version: \\")[1].strip() elif line.startswith(\'Description:\'): description_started = True # The actual description starts on the next line description = \' \'.join(description_lines) print(f\\"Package Name: {name}\\") print(f\\"Version: {version}\\") print(f\\"Description: {description}\\") except FileNotFoundError: print(f\\"Error: The file {pkg_info_path} does not exist.\\") except IOError: print(f\\"Error: An IO error occurred while accessing {pkg_info_path}.\\")"},{"question":"Objective: Create a jittered scatter plot using seaborn\'s `so.Plot` class and the `so.Jitter` transformation to visualize the `penguins` dataset. Task: 1. Load the `penguins` dataset using `seaborn.load_dataset`. 2. Create a scatter plot that shows the relationship between `flipper_length_mm` (x-axis) and `body_mass_g` (y-axis). 3. Apply jitter to the plot using the `so.Jitter` transform with the following configurations: - Jitter the `x` axis by 200 units. - Jitter the `y` axis by 50 units. 4. Ensure the plot uses well-rounded values for clarity. Input: There is no specific input other than loading the dataset within the function. Expected Output: A jittered scatter plot that demonstrates a clear understanding of using `seaborn.objects` for creating and customizing visualizations. Constraints: - The configuration for jitter along the `x` and `y` axes must use the units defined (200 and 50 respectively). - The plot should be clearly labeled and readable. - Efficiency considerations are not emphasized for this task. Performance Requirements: - Properly load and use the `penguins` dataset. - Utilize `seaborn.objects.Plot` and `seaborn.objects.Jitter` effectively to achieve the desired visualization. Function Signature: ```python import seaborn.objects as so from seaborn import load_dataset def create_jittered_scatter_plot(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the scatter plot with jitter plot = ( so.Plot( penguins[\\"flipper_length_mm\\"].round(-1), penguins[\\"body_mass_g\\"].round(-3) ) .add(so.Dots(), so.Jitter(x=200, y=50)) ) # Show the plot plot.show() # Example usage to test the function create_jittered_scatter_plot() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def create_jittered_scatter_plot(): Creates and displays a jittered scatter plot using the `penguins` dataset. The plot shows the relationship between `flipper_length_mm` and `body_mass_g` with jitter applied to help visualize the data points clearly. # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the scatter plot with jitter plot = ( so.Plot( penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\" ) .add(so.Dots(), so.Jitter(x=200, y=50)) .label(x=\\"Flipper Length (mm)\\", y=\\"Body Mass (g)\\") ) # Show the plot plot.show() # Example usage to test the function create_jittered_scatter_plot()"},{"question":"UUID Manipulation and Validation Using the provided `uuid` module, write a function `generate_and_validate_uuids` that generates a set of UUIDs of different versions and validates their properties. The function should take no inputs and return a dictionary with the following structure: ```python { \\"uuid1\\": { \\"uuid\\": <UUID>, \\"is_safe\\": <SafeUUID>, \\"version\\": <int>, \\"node\\": <int>, \\"clock_seq\\": <int> }, \\"uuid3\\": { \\"uuid\\": <UUID>, \\"version\\": <int>, \\"valid\\": <bool> }, \\"uuid4\\": { \\"uuid\\": <UUID>, \\"version\\": <int>, \\"valid\\": <bool> }, \\"uuid5\\": { \\"uuid\\": <UUID>, \\"version\\": <int>, \\"valid\\": <bool> } } ``` # Specifications 1. **UUID Generation**: - Generate a version 1 UUID using `uuid.uuid1()`. - Generate a version 3 UUID using `uuid.uuid3()` with `uuid.NAMESPACE_DNS` and the name `\\"python.org\\"`. - Generate a version 4 UUID using `uuid.uuid4()`. - Generate a version 5 UUID using `uuid.uuid5()` with `uuid.NAMESPACE_DNS` and the name `\\"python.org\\"`. 2. **Validation**: - For UUID version 1: - Include the `is_safe` status. - Extract the `node` and `clock_seq` from the `UUID.fields` attribute. - For UUID versions 3, 4, and 5: - Verify if the UUID has the expected version. - Check if the UUID does not contain the `node` and `clock_seq` fields (since these fields are not relevant for these versions). # Constraints - Use the `uuid` module provided in Python. - Ensure the function adheres to the described dictionary structure. # Example Output ```python { \\"uuid1\\": { \\"uuid\\": UUID(\'a8098c1a-f86e-11da-bd1a-00112444be1e\'), \\"is_safe\\": uuid.SafeUUID.safe, \\"version\\": 1, \\"node\\": 123456789012345, \\"clock_seq\\": 5678 }, \\"uuid3\\": { \\"uuid\\": UUID(\'6fa459ea-ee8a-3ca4-894e-db77e160355e\'), \\"version\\": 3, \\"valid\\": True }, \\"uuid4\\": { \\"uuid\\": UUID(\'16fd2706-8baf-433b-82eb-8c7fada847da\'), \\"version\\": 4, \\"valid\\": True }, \\"uuid5\\": { \\"uuid\\": UUID(\'886313e1-3b8a-5372-9b90-0c9aee199e5d\'), \\"version\\": 5, \\"valid\\": True } } ``` # Function Signature ```python def generate_and_validate_uuids() -> dict: pass ```","solution":"import uuid def generate_and_validate_uuids(): uuid1 = uuid.uuid1() uuid3 = uuid.uuid3(uuid.NAMESPACE_DNS, \'python.org\') uuid4 = uuid.uuid4() uuid5 = uuid.uuid5(uuid.NAMESPACE_DNS, \'python.org\') result = { \\"uuid1\\": { \\"uuid\\": uuid1, \\"is_safe\\": uuid1.is_safe, \\"version\\": uuid1.version, \\"node\\": uuid1.node, \\"clock_seq\\": uuid1.clock_seq }, \\"uuid3\\": { \\"uuid\\": uuid3, \\"version\\": uuid3.version, \\"valid\\": (uuid3.version == 3) }, \\"uuid4\\": { \\"uuid\\": uuid4, \\"version\\": uuid4.version, \\"valid\\": (uuid4.version == 4) }, \\"uuid5\\": { \\"uuid\\": uuid5, \\"version\\": uuid5.version, \\"valid\\": (uuid5.version == 5) } } return result"},{"question":"# Objective You are to implement a Python program using `tkinter.ttk` that demonstrates the creation and manipulation of a `Treeview` widget. # Task Write a Python function `create_treeview_widget` that sets up a `tkinter` application with the following functionality: 1. Creates a `Treeview` widget. 2. Populates the `Treeview` with a hierarchical structure of at least three levels (e.g., Categories > Subcategories > Items). 3. Allows the user to dynamically add and remove nodes from the `Treeview`. 4. Implements event handling to print out the full path to a node when it is selected. Specific Requirements: - The `Treeview` should be displayed within a Tkinter window. - You should have at least one button to add a node and one button to remove the currently selected node. - Each node should be added with a random unique identifier. - When a node is selected, print its full path (e.g., \\"Root > Category1 > Subcategory1 > Item1\\") in the console. Expected Input - None. The functions should set up the GUI elements and event handling. Expected Output - A Tkinter window with an interactive `Treeview` widget. - Console output when nodes are selected. Constraints - You must use `ttk.Treeview` for the hierarchical display. - Use the methods provided by `ttk.Treeview` for item manipulation and event handling. - Ensure the program gracefully handles adding and removing nodes, including edge cases like removing a node with children or adding nodes to empty parents. Performance Requirements - The application should be responsive and properly manage Tkinter events. ```python import tkinter as tk from tkinter import ttk import random def create_treeview_widget(): # Your implementation here pass if __name__ == \\"__main__\\": create_treeview_widget() ``` # Example Upon running `create_treeview_widget`, a window should appear with a tree view structure. Adding nodes should work dynamically by pressing the \\"Add\\" button and removing nodes with the \\"Remove\\" button. Selecting any node should result in the console printing the full hierarchical path to the selected node.","solution":"import tkinter as tk from tkinter import ttk import random def create_treeview_widget(): def add_node(): selected_item = tree.selection() if selected_item: parent = selected_item[0] else: parent = \'\' # root if no selection new_id = tree.insert(parent, \'end\', text=f\\"Node_{random.randint(1000, 9999)}\\") tree.selection_set(new_id) def remove_node(): selected_item = tree.selection() if selected_item: tree.delete(selected_item[0]) def on_node_select(event): selected_item = tree.selection() if selected_item: item_id = selected_item[0] full_path = get_full_path(item_id) print(full_path) def get_full_path(item_id): hierarchy = [] while item_id: item = tree.item(item_id) hierarchy.insert(0, item[\'text\']) item_id = tree.parent(item_id) return \\" > \\".join(hierarchy) root = tk.Tk() root.title(\\"Treeview Example\\") tree = ttk.Treeview(root) tree.pack(expand=True, fill=\'both\') # Adding top-level nodes for i in range(3): parent_id = tree.insert(\'\', \'end\', text=f\\"Category_{i+1}\\") for j in range(3): subcat_id = tree.insert(parent_id, \'end\', text=f\\"Subcategory_{i+1}.{j+1}\\") for k in range(3): tree.insert(subcat_id, \'end\', text=f\\"Item_{i+1}.{j+1}.{k+1}\\") add_button = tk.Button(root, text=\\"Add Node\\", command=add_node) add_button.pack(side=\'left\') remove_button = tk.Button(root, text=\\"Remove Node\\", command=remove_node) remove_button.pack(side=\'right\') tree.bind(\\"<<TreeviewSelect>>\\", on_node_select) root.mainloop()"},{"question":"# Question: Model Evaluation using Validation and Learning Curves You are a Data Scientist in a company that deals with predicting product sales based on various features. You have decided to evaluate different Machine Learning models using the `scikit-learn` library to ensure that the models are performing optimally. Your task is to write code to evaluate the influence of hyperparameters and the impact of training data size on the models. **Task 1: Validation Curve** 1. Load the Iris dataset from `sklearn.datasets`. 2. Use the `validation_curve` function to generate training and validation scores for an SVM with a linear kernel. The hyperparameter to vary will be `C`, and the range should be `np.logspace(-7, 3, 10)`. 3. Plot the validation curve using these scores to determine whether the model is underfitting or overfitting. **Task 2: Learning Curve** 1. Use the same Iris dataset. 2. Choose an SVM with a linear kernel. 3. Use the `learning_curve` function to generate training and validation scores for different training sizes: `[50, 80, 110]`. 4. Plot the learning curve using these scores to determine whether increasing the training data size improves the model performance and if the model has a bias or variance problem. **Requirements:** - Use `matplotlib` for plotting the curves. - Provide comments in your code to describe what each part does. - Summarize the results from both plots explaining if the model is underfitting, overfitting, or performing well. **Input:** 1. Iris dataset will be loaded within the provided code using `load_iris()` from `sklearn.datasets`. **Output:** - Two plots: one for the validation curve and one for the learning curve. - A summary explaining the findings from the plots. # Constraints: - You must use only the `scikit-learn` functions and `matplotlib` for plotting. - Ensure the code is well-documented with comments. **Performance Requirements:** - The code should be efficiently written to handle the Iris dataset and generate plots within a reasonable time frame. ```python # Your implementation goes here. ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve def plot_validation_curve(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Define the range of parameter C param_range = np.logspace(-7, 3, 10) # Compute the validation curve train_scores, valid_scores = validation_curve( SVC(kernel=\'linear\'), X, y, param_name=\\"C\\", param_range=param_range, cv=5, scoring=\\"accuracy\\") # Calculate mean and standard deviation of training and validation scores train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) valid_scores_mean = np.mean(valid_scores, axis=1) valid_scores_std = np.std(valid_scores, axis=1) # Plot the validation curve plt.figure() plt.title(\\"Validation Curve with SVM (linear kernel) - Hyperparameter: C\\") plt.xlabel(\\"Parameter C\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) plt.semilogx(param_range, train_scores_mean, label=\\"Training score\\", color=\\"r\\") plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"r\\") plt.semilogx(param_range, valid_scores_mean, label=\\"Cross-validation score\\", color=\\"g\\") plt.fill_between(param_range, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.2, color=\\"g\\") plt.legend(loc=\\"best\\") plt.show() def plot_learning_curve(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Define the parameter for learning curves train_sizes = [50, 80, 110] # Compute the learning curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\', C=1.0), X, y, train_sizes=train_sizes, cv=5, scoring=\'accuracy\') # Calculate mean and standard deviation of training and validation scores train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) valid_scores_mean = np.mean(valid_scores, axis=1) valid_scores_std = np.std(valid_scores, axis=1) # Plot the learning curve plt.figure() plt.title(\\"Learning Curve with SVM (linear kernel)\\") plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) plt.plot(train_sizes, train_scores_mean, label=\\"Training score\\", color=\\"r\\") plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"r\\") plt.plot(train_sizes, valid_scores_mean, label=\\"Cross-validation score\\", color=\\"g\\") plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.2, color=\\"g\\") plt.legend(loc=\\"best\\") plt.show() # Generate the plots plot_validation_curve() plot_learning_curve()"},{"question":"You are given a hypothetical Python project with the following structure and files: ``` my_project/ |-- setup.py |-- src/ | |-- my_module.py | `-- c_extension.c |-- docs/ | |-- README.md | `-- CHANGELOG.md |-- examples/ | |-- example1.py | `-- example2.py `-- tests/ `-- test_my_module.py ``` The setup.py file contains all the necessary specifications except for the build and distribution configurations. Your task is to create a `setup.cfg` file that meets the following requirements: 1. The C extension `c_extension.c` should be built in place within the `src` directory. 2. When creating an RPM distribution, the package should be marked with a release number 2 and the packager should be \\"John Doe <john.doe@example.com>\\". 3. Documentation files `README.md` and `CHANGELOG.md` from the `docs` directory, as well as all files from the `examples` directory, should be included in the RPM distribution. # Input: No input is needed. You will create a file `setup.cfg` as described. # Output: The content of a valid `setup.cfg` file meeting the specified requirements. # Constraints: - You must use the correct syntax for the setup.cfg options and commands. - Ensure options for each command are correctly specified and adhere to the required format. Hereâ€™s an outline that describes how you would structure the `setup.cfg` file: ```plaintext [build_ext] # Your configuration for the build_ext command [bdist_rpm] # Your configuration for the bdist_rpm command ``` # Example: The correct `setup.cfg` file should be: ```ini [build_ext] inplace=1 [bdist_rpm] release=2 packager=John Doe <john.doe@example.com> doc_files=docs/README.md docs/CHANGELOG.md examples/ ``` Ensure your file captures all specified details and adheres to the provided guidelines.","solution":"def setup_cfg_content(): Returns the contents of the setup.cfg file as a string. return [build_ext] inplace=1 [bdist_rpm] release=2 packager=John Doe <john.doe@example.com> doc_files=docs/README.md docs/CHANGELOG.md examples/"},{"question":"# Question You have been tasked with creating a summary report of the Python installation on a given system using the `sysconfig` module. Write a Python function `generate_sysconfig_report()` that generates and returns a summary as a dictionary containing the following information: 1. **Platform**: This should be retrieved using `sysconfig.get_platform()`. 2. **Python Version**: This should be retrieved using `sysconfig.get_python_version()`. 3. **Default Installation Scheme**: This should be retrieved using `sysconfig.get_default_scheme()`. 4. **List of All Supported Schemes**: This should be retrieved using `sysconfig.get_scheme_names()`. 5. **Installation Paths for the Default Scheme**: Retrieve all installation paths for the default scheme using `sysconfig.get_paths()`. Your function should return a dictionary with the following structure: ```python { \\"Platform\\": \\"<platform>\\", \\"Python Version\\": \\"<python_version>\\", \\"Default Installation Scheme\\": \\"<default_scheme>\\", \\"Supported Schemes\\": [\\"<scheme1>\\", \\"<scheme2>\\", ...], \\"Installation Paths\\": { \\"<path_name1>\\": \\"<path_value1>\\", \\"<path_name2>\\": \\"<path_value2>\\", ... } } ``` # Constraints - You should handle any exceptions that could be raised if a value cannot be retrieved. - Ensure that the dictionary keys match the exact structure provided above. # Example Usage ```python result = generate_sysconfig_report() ``` Expected output (values will vary based on your system): ```python { \\"Platform\\": \\"win-amd64\\", \\"Python Version\\": \\"3.10\\", \\"Default Installation Scheme\\": \\"nt\\", \\"Supported Schemes\\": [\\"nt\\", \\"nt_user\\"], \\"Installation Paths\\": { \\"stdlib\\": \\"C:/Python310/Lib\\", \\"platstdlib\\": \\"C:/Python310/Lib\\", \\"purelib\\": \\"C:/Python310/Lib/site-packages\\", ... } } ``` # Notes - Use the `sysconfig` module functions to gather the required information. - Focus on correctly using the functions defined in the documentation to get each piece of information.","solution":"import sysconfig def generate_sysconfig_report(): Generates a summary report of the Python installation on the system. Returns: dict: A dictionary containing platform, python version, default installation scheme, list of supported schemes, and installation paths for the default scheme. try: platform = sysconfig.get_platform() except Exception: platform = None try: python_version = sysconfig.get_python_version() except Exception: python_version = None try: default_scheme = sysconfig.get_default_scheme() except Exception: default_scheme = None try: supported_schemes = sysconfig.get_scheme_names() except Exception: supported_schemes = [] try: installation_paths = sysconfig.get_paths() except Exception: installation_paths = {} return { \\"Platform\\": platform, \\"Python Version\\": python_version, \\"Default Installation Scheme\\": default_scheme, \\"Supported Schemes\\": supported_schemes, \\"Installation Paths\\": installation_paths }"},{"question":"Objective To assess the student\'s ability to work with the \\"zoneinfo\\" module and handle time zone-aware datetime manipulations in Python. Question You are given a list of events with their timestamps in UTC. Your task is to write a Python function that converts these timestamps to a specified time zone and returns the converted timestamps. Additionally, handle any ambiguous times due to daylight saving transitions by always using the offset after the transition (i.e., setting `fold=1`). Function Signature ```python def convert_to_timezone(events, timezone): Convert a list of events with UTC timestamps to a specified time zone. Args: events (list of tuples): A list of event tuples, where each tuple contains the event name (str) and timestamp in UTC (str in \'YYYY-MM-DD HH:MM:SS\' format). timezone (str): The time zone to which the timestamps should be converted (IANA time zone key). Returns: list of tuples: A list of event tuples with timestamps converted to the specified time zone, maintaining the original event name. ``` Input - `events`: A list of tuples, where each tuple contains: - `event_name` (str): The name of the event. - `timestamp` (str): The timestamp of the event in UTC, formatted as \'YYYY-MM-DD HH:MM:SS\'. - `timezone`: A string representing the IANA time zone key to which the timestamps should be converted. Output - A list of tuples, where each tuple contains: - `event_name` (str): The name of the event. - `converted_timestamp` (str): The timestamp of the event converted to the specified time zone, formatted as \'YYYY-MM-DD HH:MM:SS Â±hh:mm\'. Constraints - All UTC timestamps are assumed to be valid and in the correct format. - The specified time zone is a valid IANA time zone key. - Handle any ambiguous times during daylight saving transitions by using the offset after the transition (`fold=1`). Example ```python events = [ (\\"event1\\", \\"2023-03-12 02:30:00\\"), (\\"event2\\", \\"2023-11-05 01:30:00\\"), ] timezone = \\"America/Los_Angeles\\" converted_events = convert_to_timezone(events, timezone) print(converted_events) # Expected output: # [ # (\\"event1\\", \\"2023-03-11 18:30:00 -07:00\\"), # (\\"event2\\", \\"2023-11-04 18:30:00 -07:00\\") # ] ``` Note: 1. You need to handle the daylight saving transitions correctly. For example, \\"America/Los_Angeles\\" switches from \\"PDT\\" (UTC-07:00) to \\"PST\\" (UTC-08:00) in November and vice versa in March. 2. Use the `zoneinfo.ZoneInfo` class and the `datetime` module to manage time zones and timestamps.","solution":"from datetime import datetime from zoneinfo import ZoneInfo def convert_to_timezone(events, timezone): Convert a list of events with UTC timestamps to a specified time zone. Args: events (list of tuples): A list of event tuples, where each tuple contains the event name (str) and timestamp in UTC (str in \'YYYY-MM-DD HH:MM:SS\' format). timezone (str): The time zone to which the timestamps should be converted (IANA time zone key). Returns: list of tuples: A list of event tuples with timestamps converted to the specified time zone, maintaining the original event name. converted_events = [] for event in events: # Extract the event name and timestamp event_name, utc_timestamp = event # Convert the timestamp string to a datetime object utc_dt = datetime.strptime(utc_timestamp, \\"%Y-%m-%d %H:%M:%S\\").replace(tzinfo=ZoneInfo(\\"UTC\\")) # Convert the datetime to the specified timezone local_dt = utc_dt.astimezone(ZoneInfo(timezone)) # Handle ambiguous times due to daylight saving transitions by setting fold=1 if local_dt.fold == 0: fold_corrected_dt = local_dt.replace(fold=1) else: fold_corrected_dt = local_dt # Format the converted datetime back to string converted_timestamp = fold_corrected_dt.strftime(\\"%Y-%m-%d %H:%M:%S %z\\") # Append the converted event to the result list converted_events.append((event_name, converted_timestamp)) return converted_events"},{"question":"# Question: Handling System-Level Errors with `errno` As a developer, it is often important to handle system-level errors gracefully. Python provides the `errno` module to help map these errors to meaningful messages. In this task, you will create a function that simulates an error condition based on an error code and demonstrates how to handle and map these errors using the `errno` module. Function Specification Write a function `handle_system_error(error_code: int) -> str` that takes an integer `error_code` as an input and returns a string message describing the error. The function should: 1. Use the `errno` module to map the `error_code` to the corresponding error name. 2. Check if the `error_code` corresponds to a known error; if not, return \\"Unknown error code\\". 3. If the error code is known, construct and return an error message in the format: `\\"Error [error_code]: [error_name] - [error_description]\\"`. 4. Use the `os.strerror()` function to get the error description for the given error code. Example ```python import errno import os def handle_system_error(error_code: int) -> str: if error_code in errno.errorcode: error_name = errno.errorcode[error_code] error_description = os.strerror(error_code) return f\\"Error {error_code}: {error_name} - {error_description}\\" else: return \\"Unknown error code\\" # Example Usage print(handle_system_error(errno.EPERM)) # \\"Error 1: EPERM - Operation not permitted\\" print(handle_system_error(errno.ENOENT)) # \\"Error 2: ENOENT - No such file or directory\\" print(handle_system_error(99999)) # \\"Unknown error code\\" ``` Notes - You can assume the `errno` module is already imported, but importing other necessary modules is part of the task. - You may use any `errno` codes listed in the documentation to test the function. - Make sure your function handles both known and unknown error codes appropriately. Constraints - The function should handle all possible integer error codes. - Performance should be appropriate for the number of error codes typically defined in the `errno` module.","solution":"import errno import os def handle_system_error(error_code: int) -> str: if error_code in errno.errorcode: error_name = errno.errorcode[error_code] error_description = os.strerror(error_code) return f\\"Error {error_code}: {error_name} - {error_description}\\" else: return \\"Unknown error code\\""},{"question":"You are required to implement a custom Python function that mimics the behavior of a dictionary but extends its functionality to support the following operations: 1. Get the count of a specific key\'s occurrence. 2. List all keys that have been accessed more than a specified number of times. Your implementation should be efficient and should leverage the principles of Python\'s dictionary type and instance method objects. Function Signature: ```python class AdvancedDict: def __init__(self): # Initializes an empty dictionary and a dictionary to track access counts pass def set_item(self, key: str, value) -> None: # Sets the item in the dictionary with the given key and value pass def get_item(self, key: str): # Retrieves the value for the given key and increments the access count pass def count_key_access(self, key: str) -> int: # Returns the count of how many times the key has been accessed pass def keys_accessed_more_than(self, count: int) -> list: # Returns a list of keys that have been accessed more than the specified count pass ``` Guidelines: 1. **Input/Output Formats**: - `__init__()`: Initialize an empty dictionary to store key-value pairs and another to track access counts. - `set_item(key: str, value)`: Takes a key (string) and value (any type) and stores them in the dictionary. - `get_item(key: str)`: Takes a key (string) and returns the associated value. Also increments the access count for that key. - `count_key_access(key: str)`: Takes a key (string) and returns the count of how many times the key has been accessed. - `keys_accessed_more_than(count: int)`: Takes an integer count and returns a list of keys accessed more than that count. 2. **Constraints**: - Assume that all keys are unique strings. - The values can be of any type. - The dictionary should handle a maximum of 10^5 keys efficiently. 3. **Performance Requirements**: - The operations should be optimized for constant-time complexity where possible. - You should avoid recalculating the counts inefficiently; maintain a separate structure to keep track of access counts. Example Usage: ```python ad_dict = AdvancedDict() ad_dict.set_item(\\"a\\", 10) ad_dict.set_item(\\"b\\", 20) ad_dict.get_item(\\"a\\") ad_dict.get_item(\\"a\\") ad_dict.get_item(\\"b\\") print(ad_dict.count_key_access(\\"a\\")) # Output: 2 print(ad_dict.count_key_access(\\"b\\")) # Output: 1 print(ad_dict.keys_accessed_more_than(1)) # Output: [\\"a\\"] ``` Write code to implement the `AdvancedDict` class as specified.","solution":"class AdvancedDict: def __init__(self): self.dictionary = {} self.access_count = {} def set_item(self, key: str, value) -> None: self.dictionary[key] = value if key not in self.access_count: self.access_count[key] = 0 def get_item(self, key: str): if key in self.dictionary: self.access_count[key] += 1 return self.dictionary[key] return None def count_key_access(self, key: str) -> int: return self.access_count.get(key, 0) def keys_accessed_more_than(self, count: int) -> list: return [key for key, cnt in self.access_count.items() if cnt > count]"},{"question":"# Task You are asked to implement a function `extract_strings_from_code` that extracts all string literals from a given Python source code. These can include single-line or multi-line strings, both using single/double quotes. # Function Signature ```python def extract_strings_from_code(readline: callable) -> list: pass ``` # Input - `readline`: A callable object which provides the same interface as the `io.IOBase.readline()` method of file objects, returning one line of input as bytes. # Output - Returns a list of tuples. Each tuple contains the string literal and its starting position in the form `(string, (start_row, start_col))`. # Constraints - The provided Python source code is syntactically valid. - You should include both single-line and multi-line string literals. # Example Suppose we have the following Python code: ```python def example(): print(\\"Hello, World!\\") text = \'\'\'This is a multi-line string.\'\'\' string_literals = \\"single quotes\\" + \'double quotes\' ``` Your function should return: ```python [ (\'\\"Hello, World!\\"\', (1, 11)), (\\"\'\'This is an multi-line string.\'\'\\", (2, 11)), (\'\\"single quotes\\"\', (3, 23)), (\\"\'double quotes\'\\", (3, 40)) ] ``` # Additional Notes - Utilize the `tokenize.tokenize` function to break down the source code into tokens. - Ensure the function handles edge cases such as nested quotes correctly. - Do not assume strings appear only in certain contexts (e.g., as function arguments or variable assignments).","solution":"import tokenize from io import BytesIO def extract_strings_from_code(readline): Extracts all string literals from a given Python source code. :param readline: A callable object which provides the same interface as the io.IOBase.readline() method of file objects, returning one line of input as bytes. :return: A list of tuples containing the string literal and its starting position in the form (string, (start_row, start_col)). result = [] tokens = tokenize.tokenize(readline) try: for token in tokens: if token.type in (tokenize.STRING,): result.append((token.string, (token.start[0], token.start[1]))) except tokenize.TokenError: pass # Ignore token errors return result"},{"question":"**Incremental XML Parser Implementation** Create a class `MyIncrementalParser` that extends the `xml.sax.xmlreader.IncrementalParser` class. This parser should be able to handle XML data in chunks, process it, and maintain a record of elements encountered along with their attributes. Your implementation should include: 1. **Class Definition**: `MyIncrementalParser` extending `IncrementalParser`. 2. **Initialization**: Initialize necessary variables and settings. 3. **Methods Implementation**: - `feed(data)`: Processes a chunk of XML data. - `close()`: Completes the parsing and performs any end-of-document checks. - `reset()`: Resets the parser to handle new documents. 4. **Handler Methods**: - Override methods like `startElement()`, `endElement()`, `characters()` to track elements and attributes. **Input and Output Format**: - **Input**: XML data chunks fed using `feed(data)`. - **Output**: A summary of elements and their attributes stored internally. **Constraints**: - Handle well-formed XML data. - Ensure that methods `feed`, `close`, and `reset` function correctly without causing unexpected behavior. **Example Usage**: ```python parser = MyIncrementalParser() # Simulating feeding XML data in chunks parser.feed(\'<root>\') parser.feed(\'<child attr=\\"value\\">\') parser.feed(\'Text</child>\') parser.feed(\'</root>\') parser.close() # Assuming a method to get summary summary = parser.get_summary() print(summary) # Expected: {\'root\': {}, \'child\': {\'attr\': \'value\'}} ``` - Ensure appropriate exception handling where necessary. - You must use the `xml.sax.xmlreader.IncrementalParser` as the base class. **Performance Requirements**: - Efficiently handle XML data of sizes up to 10 MB. - Memory usage should be managed to avoid leaks. **Implementation Details**: - You can use the `xml.sax` module for parsing utilities. - Make sure the parser can handle and accurately report on nested elements and multiple attributes.","solution":"from xml.sax import handler, make_parser from xml.sax.xmlreader import IncrementalParser class MyIncrementalParser(IncrementalParser): def __init__(self): super().__init__() self.parser = make_parser() self.handler = MyContentHandler() self.parser.setContentHandler(self.handler) self.parser.setFeature(handler.feature_namespaces, 0) def feed(self, data): self.parser.feed(data) def close(self): self.parser.close() def reset(self): self.parser.reset() self.handler.reset() def get_summary(self): return self.handler.get_summary() class MyContentHandler(handler.ContentHandler): def __init__(self): self.elements = [] def startElement(self, name, attrs): attribute_dict = {} for k, v in attrs.items(): attribute_dict[k] = v self.elements.append({name: attribute_dict}) def endElement(self, name): pass def characters(self, content): pass def get_summary(self): summary = {} for element in self.elements: for key in element: summary[key] = element[key] return summary def reset(self): self.elements = []"},{"question":"# Coding Challenge: Advanced Directory Comparison and Report Generation Introduction: The `filecmp` module in Python is designed to compare files and directories, offering a variety of comparison functionalities. In this task, you will utilize the functionalities provided by `filecmp.cmpfiles()` and the `filecmp.dircmp` class to perform comprehensive directory comparisons and generate reports. Problem Statement: You are provided with two directories `dir1` and `dir2`. Your task is to: 1. Compare all files in these directories and generate reports of files that match, mismatch, and have errors. 2. Use the `filecmp.dircmp` class to recursively compare the directories and generate a summary report of the comparison. Input: - Two strings `dir1` and `dir2`, representing the paths to the directories to be compared. Output: - Three lists `match`, `mismatch`, `errors`, containing the names of files that match, differ, and have comparison errors respectively. - A detailed summary report of the directory comparison, which includes: - Files only in `dir1`. - Files only in `dir2`. - Common files. - Different files. - Unique names for which `os.stat()` reports an error. Constraints: - The directories may contain subdirectories. - Ensure all file operations handle permission issues gracefully. - Use the cache clearing functionality if you run into modified files during comparison. Performance Requirements: - The solution should efficiently handle directory structures with a large number of files and nested subdirectories. Function Signature: ```python def compare_directories(dir1: str, dir2: str) -> tuple: Compare the directories dir1 and dir2. Parameters: dir1 (str): Path to the first directory. dir2 (str): Path to the second directory. Returns: tuple: A tuple with three lists (match, mismatch, errors) and a string summary report. ``` Example: ```python dir1 = \\"path/to/dir1\\" dir2 = \\"path/to/dir2\\" match, mismatch, errors, summary_report = compare_directories(dir1, dir2) print(\\"Matched Files:\\", match) print(\\"Mismatched Files:\\", mismatch) print(\\"Errors:\\", errors) print(\\"Summary Report:\\") print(summary_report) ``` Additional Instructions: - Use the `filecmp.cmpfiles()` method to get the initial comparison of files. - For a detailed and recursive comparison, instantiate and utilize the `filecmp.dircmp` class. - Implement appropriate error handling for file access issues. - Clear the comparison cache at an appropriate point in your function. Your implementation should strictly follow these specifications and ensure that the results are accurate and well-structured.","solution":"import os import filecmp def compare_directories(dir1: str, dir2: str): def get_comparison_report(dcmp, match, mismatch, errors): summary_report = [] summary_report.append(f\\"Comparison between {dcmp.left} and {dcmp.right}:n\\") if dcmp.left_only: summary_report.append(f\\"Files only in {dcmp.left}:n\\" + \\"n\\".join(dcmp.left_only)) if dcmp.right_only: summary_report.append(f\\"Files only in {dcmp.right}:n\\" + \\"n\\".join(dcmp.right_only)) if dcmp.common: summary_report.append(f\\"Common files:n\\" + \\"n\\".join(dcmp.common)) m, mm, e = filecmp.cmpfiles(dcmp.left, dcmp.right, dcmp.common_files) match.extend(m) mismatch.extend(mm) errors.extend(e) if mm: summary_report.append(f\\"Different files:n\\" + \\"n\\".join(mm)) if e: summary_report.append(f\\"Errors:n\\" + \\"n\\".join(e)) for sub_dcmp in dcmp.subdirs.values(): get_comparison_report(sub_dcmp, match, mismatch, errors) return \\"n\\".join(summary_report) match, mismatch, errors = [], [], [] dcmp = filecmp.dircmp(dir1, dir2) summary_report = get_comparison_report(dcmp, match, mismatch, errors) return match, mismatch, errors, summary_report"},{"question":"# Question: You are tasked with implementing a function to analyze incomplete survey data stored in a pandas DataFrame. The survey data contains boolean responses (`True`/`False`/`NA`) for several questions. Your job is to evaluate the logical outcomes of these responses by applying certain conditions and transforming the DataFrame. Function Signature: ```python def analyze_survey_data(df: pd.DataFrame) -> pd.DataFrame: pass ``` Input: - `df` (pd.DataFrame): A DataFrame where each column represents a survey question and contains boolean values (`True`, `False`, `NA`). Output: - Returns a modified DataFrame with the following transformations: 1. Replace all `NA` values in the DataFrame with `False` for indexing purposes. 2. Add a new column `all_true` where each value is `True` if all the responses in that row are `True`, otherwise `False`. 3. Add a new column `any_true` where each value is `True` if any of the responses in that row is `True`, otherwise `False`. 4. Add a new column `none_true` where each value is `True` if none of the responses in that row are `True`, otherwise `False`. Constraints: - Only use the pandas library for manipulating the DataFrame. - Handle `NA` values correctly following the nullable boolean type specifics from pandas documentation provided. Example: ```python import pandas as pd import numpy as np # Sample Data data = { \'Q1\': [True, False, np.nan, True], \'Q2\': [True, np.nan, False, np.nan], \'Q3\': [np.nan, np.nan, np.nan, True] } df = pd.DataFrame(data, dtype=\\"boolean\\") # Expected Result: # Q1 Q2 Q3 all_true any_true none_true # 0 True True NA True True False # 1 False NA NA False False True # 2 NA False NA False False True # 3 True NA True True True False result = analyze_survey_data(df) print(result) ``` **Note:** Ensure that the column data types are handled correctly, and apply logical operations based on the nullable boolean behavior as described in the documentation.","solution":"import pandas as pd def analyze_survey_data(df: pd.DataFrame) -> pd.DataFrame: # Replace all NA values with False df_filled = df.fillna(False) # Add new column \'all_true\' df_filled[\'all_true\'] = df_filled.all(axis=1) # Add new column \'any_true\' df_filled[\'any_true\'] = df_filled.any(axis=1) # Add new column \'none_true\' df_filled[\'none_true\'] = ~df_filled.any(axis=1) return df_filled"},{"question":"# Question: Implementing and Managing Asynchronous Communication Server In this task, you are required to implement an asynchronous server using Python\'s asyncio module. The server will: 1. Accept multiple client connections, running concurrently. 2. Handle incoming client data and echo it back to the client. 3. Simulate a delay in responding by introducing artificial latency. 4. Manage and gracefully shut down using signal handling. # Requirements: 1. **Server Implementation**: - Create a TCP server that listens on a specified host and port. - For every client connection accepted, echo back the received data. - Simulate a 1-second delay before sending back the response. 2. **Signal Handling**: - Implement signal handlers for `SIGINT` and `SIGTERM` to gracefully shut down the server. 3. **Concurrency**: - Use asyncio\'s task management to handle multiple client connections concurrently. # Input and Output Specification: The server does not take any user input from stdin or command line. It will: - Log each connection acceptance and message receipt. - Echo back every message to the client after a 1-second delay. - Log shutdown messages when handling termination signals. # Constraints: - Implement using Python 3.10+ and asyncio module. - Do **not** use any blocking calls (`time.sleep`, etc.); rely purely on asyncio. # Example run: ```python import asyncio import signal import functools class EchoServerProtocol(asyncio.Protocol): def __init__(self, loop): self.loop = loop def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\'Connection from {peername}\') def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') # Simulate a delay before replying self.loop.create_task(self.delayed_echo(message)) async def delayed_echo(self, message): await asyncio.sleep(1) print(f\'Send: {message}\') self.transport.write(message.encode()) def connection_lost(self, exc): print(\'Connection closed\') async def main(): loop = asyncio.get_running_loop() # For handling shutdown signals for signame in {\'SIGINT\', \'SIGTERM\'}: loop.add_signal_handler(getattr(signal, signame), functools.partial(ask_exit, signame, loop)) server = await loop.create_server(lambda: EchoServerProtocol(loop), \'127.0.0.1\', 8888) print(\\"Server started at 127.0.0.1:8888\\") async with server: await server.serve_forever() def ask_exit(signame, loop): print(f\\"Received signal {signame}: shutting down\\") loop.stop() if __name__ == \\"__main__\\": asyncio.run(main()) ``` In the example, the task creates a simple echo server using asyncio that can handle multiple clients concurrently and responds with a delay. Make sure to properly close your loop and handle all resources gracefully when the termination signals (`SIGINT` or `SIGTERM`) are received. Good luck!","solution":"import asyncio import signal import functools class EchoServerProtocol(asyncio.Protocol): def __init__(self, loop): self.loop = loop def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\'Connection from {peername}\') def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') # Simulate a delay before replying self.loop.create_task(self.delayed_echo(message)) async def delayed_echo(self, message): await asyncio.sleep(1) print(f\'Send: {message}\') self.transport.write(message.encode()) def connection_lost(self, exc): print(\'Connection closed\') async def main(): loop = asyncio.get_running_loop() # For handling shutdown signals for signame in {\'SIGINT\', \'SIGTERM\'}: loop.add_signal_handler(getattr(signal, signame), functools.partial(ask_exit, signame, loop)) server = await loop.create_server(lambda: EchoServerProtocol(loop), \'127.0.0.1\', 8888) print(\\"Server started at 127.0.0.1:8888\\") async with server: await server.serve_forever() def ask_exit(signame, loop): print(f\\"Received signal {signame}: shutting down\\") loop.stop() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# PyTorch CUDA Stream Synchronization Objective: To test your understanding of CUDA stream management and synchronization in PyTorch, you need to write a script that detects and fixes a possible data race condition, similar to the example provided. Question: 1. Write a function `initialize_and_modify_tensor` that: - Takes an integer `n` as input. - Initializes a tensor with random values of shape `(n, n)` on the default CUDA stream. - Creates a new CUDA stream and within this stream, attempts to multiply the tensor by a factor of 10 in place, without initially synchronizing the streams. 2. Enable the CUDA Stream Sanitizer environment variable in your script to detect any synchronization errors when running your function. 3. Modify your function to correctly synchronize the streams to avoid any data race conditions as detected by the CUDA Stream Sanitizer. Constraints: - Do not use CPU tensors; all operations should be performed on GPU tensors. - Ensure that the function `initialize_and_modify_tensor` is properly defined and can be called with an integer argument. Expected Input and Output Formats: - Input: An integer `n`. - Output: None (The function should print any errors detected by the CUDA Stream Sanitizer, and then print \\"No errors reported\\" once fixed). Example Usage: ```python # Example usage initialize_and_modify_tensor(4) ``` When run with the CUDA Stream Sanitizer enabled, your script should: 1. Initially print the details of the detected data race. 2. After modifying the function to fix the synchronization error, print \\"No errors reported\\". Performance Requirements: - Ensure the function runs efficiently without unnecessary computations or data transfers. # Sample Solution Skeleton: ```python import torch def initialize_and_modify_tensor(n): a = torch.rand(n, n, device=\\"cuda\\") # Create a new CUDA stream new_stream = torch.cuda.Stream() # Add code below to introduce a synchronization error by not syncing streams with torch.cuda.stream(new_stream): # Your code to multiply the tensor by 10 goes here pass # Add code here to synchronize the streams correctly and avoid data race with torch.cuda.stream(new_stream): # Your code to synchronize the streams and then multiply the tensor by 10 goes here pass # Run your function with the sanitizer enabled if __name__ == \\"__main__\\": import os os.environ[\'TORCH_CUDA_SANITIZER\'] = \'1\' # Call the function with a sample input initialize_and_modify_tensor(4) ``` Ensure to follow similar structure and fix the synchronization errors as per the provided documentation.","solution":"import torch def initialize_and_modify_tensor(n): a = torch.rand(n, n, device=\\"cuda\\") # Create a new CUDA stream new_stream = torch.cuda.Stream() # Add code below to introduce a synchronization error by not syncing streams with torch.cuda.stream(new_stream): # Introduce the multiplication on the new stream without synchronization a.mul_(10) # Ensure stream synchronization torch.cuda.synchronize() # Correctly synchronize the streams to avoid data race with torch.cuda.stream(new_stream): # Synchronization is ensured with torch.cuda.synchronize() before and stream exits here after multiplication pass # Verify that synchronization avoids data race condition print(\\"No errors reported\\") # Run your function with the sanitizer enabled if __name__ == \\"__main__\\": import os os.environ[\'CUDA_LAUNCH_BLOCKING\'] = \'1\' # Enabling CUDA blocking for error detection # Call the function with a sample input initialize_and_modify_tensor(4)"},{"question":"You are required to create a multi-threaded program that simulates a bank account operation. The bank account will allow for concurrent deposits and withdrawals to demonstrate the correct use of threading and thread synchronization with locks. # Bank Account Operation Simulation Objective Create a `BankAccount` class that supports concurrent deposits and withdrawals using the `_thread` module for threading. Requirements - The `BankAccount` class should have the following methods: - `deposit(self, amount)`: Adds the amount to the balance. - `withdraw(self, amount)`: Subtracts the amount from the balance. If the amount is greater than the current balance, it should raise an exception. - `get_balance(self)`: Returns the current balance. - Ensure that deposits and withdrawals are thread-safe. - Simulate a scenario where multiple threads are performing deposits and withdrawals concurrently. Input and Output Formats - **Input**: No direct input is required. You will create a test case within the script demonstrating deposits and withdrawals by multiple threads. - **Output**: Print the final balance of the account to the console. Constraints - You must use the `_thread` module for creating and managing threads. - Ensure proper synchronization using locks to avoid race conditions. Example ```python import _thread import time class BankAccount: def __init__(self): self.balance = 0 self.lock = _thread.allocate_lock() def deposit(self, amount): with self.lock: self.balance += amount def withdraw(self, amount): with self.lock: if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount def get_balance(self): with self.lock: return self.balance def deposit_task(account, amount): account.deposit(amount) def withdraw_task(account, amount): account.withdraw(amount) # Test case account = BankAccount() # Starting threads for deposits and withdrawals try: _thread.start_new_thread(deposit_task, (account, 100)) _thread.start_new_thread(deposit_task, (account, 150)) _thread.start_new_thread(withdraw_task, (account, 50)) _thread.start_new_thread(withdraw_task, (account, 100)) except Exception as e: print(f\\"Error: unable to start thread: {e}\\") # Giving threads some time to finish time.sleep(1) # Printing the final balance print(f\\"Final Balance: {account.get_balance()}\\") ``` # Explanation 1. The `BankAccount` class contains a balance and a lock. 2. The `deposit` and `withdraw` methods use locks to ensure thread safety. 3. Multiple threads deposit and withdraw amounts concurrently. 4. The main thread waits for 1 second (`time.sleep(1)`) to allow all threads to complete their execution before printing the final balance. Implement the `BankAccount` class and demonstrate its usage with multiple threads as shown in the example.","solution":"import _thread import time class BankAccount: def __init__(self): self.balance = 0 self.lock = _thread.allocate_lock() def deposit(self, amount): with self.lock: self.balance += amount def withdraw(self, amount): with self.lock: if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount def get_balance(self): with self.lock: return self.balance def deposit_task(account, amount): account.deposit(amount) def withdraw_task(account, amount): try: account.withdraw(amount) except ValueError as e: print(e) # Demonstration case if __name__ == \\"__main__\\": account = BankAccount() # Starting threads for deposits and withdrawals try: _thread.start_new_thread(deposit_task, (account, 100)) _thread.start_new_thread(deposit_task, (account, 150)) _thread.start_new_thread(withdraw_task, (account, 50)) _thread.start_new_thread(withdraw_task, (account, 100)) except Exception as e: print(f\\"Error: unable to start thread: {e}\\") # Giving threads some time to finish (not ideal for real applications) time.sleep(1) # Printing the final balance print(f\\"Final Balance: {account.get_balance()}\\")"},{"question":"# ASCII String Manipulation and Testing You are tasked with writing a set of functions that perform various manipulations and checks on strings consisting of ASCII characters. Function 1: `filter_ascii_characters(input_string)` This function will take a string, `input_string`, as input and return a string containing only the ASCII characters from the `input_string`. # Input: - `input_string` (str): The original string which may contain both ASCII and non-ASCII characters. # Output: - (str): A string containing only the ASCII characters from the `input_string`. # Example: ```python filter_ascii_characters(\'Hello, World! ã“ã‚“ã«ã¡ã¯\') # Output: \'Hello, World! \' ``` Function 2: `control_char_to_display(char)` This function will convert an ASCII control character to its corresponding display representation using the `unctrl` function from `curses.ascii`. # Input: - `char` (str): A single ASCII control character. # Output: - (str): The display representation of the control character. # Example: ```python control_char_to_display(\'x00\') # Output: \'^@\' ``` Function 3: `substring_is_ascii_numeric(input_string, start, length)` This function checks if a substring of a given length, starting at a specified index from the `input_string`, consists solely of ASCII numeric characters (digits). # Input: - `input_string` (str): The original string. - `start` (int): The starting index of the substring. - `length` (int): The length of the substring. # Output: - (bool): `True` if the substring consists only of ASCII numeric characters, otherwise `False`. # Example: ```python substring_is_ascii_numeric(\'abc123def456\', 3, 3) # Output: True substring_is_ascii_numeric(\'abc123def456\', 0, 3) # Output: False ``` Using the provided utilities in the `curses.ascii` module, implement the three functions as described. Constraints and Performance Requirements: - You can assume that `input_string` will not exceed 1000 characters. - The index and lengths provided for substrings will always be valid given the length of `input_string`. - The implementation should focus on correctness rather than raw performance, but aim to avoid unnecessary iterations where possible. Hints: - Use appropriate functions from `curses.ascii` to check character types. - Handle special cases, such as empty substrings, appropriately.","solution":"import curses.ascii def filter_ascii_characters(input_string): Returns a string containing only the ASCII characters from the input_string. return \'\'.join(c for c in input_string if curses.ascii.isascii(c)) def control_char_to_display(char): Converts an ASCII control character to its display representation. if len(char) != 1: raise ValueError(\\"Input must be a single character.\\") if curses.ascii.iscntrl(char): return curses.ascii.unctrl(char) else: raise ValueError(\\"Input must be a control character.\\") def substring_is_ascii_numeric(input_string, start, length): Checks if a substring of a given length, starting at a specified index, consists solely of ASCII numeric characters (digits). substring = input_string[start:start + length] return all(curses.ascii.isdigit(c) for c in substring)"},{"question":"# Question: Implementing and Validating Logging Configuration using `logging.config.dictConfig()` Objective Your task is to configure and validate a logging setup using the `logging.config.dictConfig()` function from the `logging.config` module in Python. Problem Statement 1. Implement a function `configure_logging(config_dict: dict) -> None` that takes a dictionary representing the logging configuration and configures Python\'s logging module using `logging.config.dictConfig(config_dict)`. 2. Implement a function `validate_logging_configuration(config_dict: dict) -> bool` that takes a dictionary representing the logging configuration and validates it against the required schema format described below. 3. Write a function `test_logging_configuration()` that tests the `configure_logging` and `validate_logging_configuration` functions with various logging configuration scenarios to ensure they work correctly. Details 1. **Input Format:** - `config_dict` is a dictionary representing the logging configuration. The dictionary should follow the schema described below. 2. **Output Format:** - `configure_logging`: This function does not return any value. - `validate_logging_configuration`: This function returns a boolean indicating whether the given configuration dictionary is valid. - `test_logging_configuration`: This function does not return any value but should print the results of the tests. Configuration Dictionary Schema The configuration dictionary must contain the following keys: - `version` (int, required): Must be set to `1`. - `formatters` (dict, optional): A dictionary where each key is a formatter id and each value is a dictionary describing how to configure the corresponding `Formatter` instance. - Keys: `\\"format\\"`, `\\"datefmt\\"`, `\\"style\\"`, `\\"validate\\"`, `\\"class\\"` (optional). - `filters` (dict, optional): A dictionary where each key is a filter id and each value is a dictionary describing how to configure the corresponding `Filter` instance. - Key: `\\"name\\"` (optional). - `handlers` (dict, optional): A dictionary where each key is a handler id and each value is a dictionary describing how to configure the corresponding `Handler` instance. - Keys: `\\"class\\"` (required), `\\"level\\"`, `\\"formatter\\"`, `\\"filters\\"`, other keys based on the handler class. - `loggers` (dict, optional): A dictionary where each key is a logger name and each value is a dictionary describing how to configure the corresponding `Logger` instance. - Keys: `\\"level\\"`, `\\"propagate\\"`, `\\"filters\\"`, `\\"handlers\\"` (all optional). - `root` (dict, optional): Similar to `loggers` but applies to the root logger. - `incremental` (bool, optional): Defaults to `False`. If `True`, the configuration is processed as incremental to the existing configuration. - `disable_existing_loggers` (bool, optional): Defaults to `True`. When set to `False`, existing loggers are not disabled. Constraints - If the `config_dict` does not comply with the required schema or contains errors, `configure_logging` should raise an appropriate exception. - `validate_logging_configuration` should ensure all required keys and valid values/types are present in the configuration dictionary. Example ```python import logging import logging.config def configure_logging(config_dict): Configures logging using the provided configuration dictionary. logging.config.dictConfig(config_dict) def validate_logging_configuration(config_dict): Validates the provided logging configuration dictionary. Returns True if valid, False otherwise. try: assert config_dict.get(\'version\') == 1, \\"Invalid or missing \'version\' key. Must be set to 1.\\" if \'handlers\' in config_dict: for handler in config_dict[\'handlers\'].values(): assert \'class\' in handler, \\"Each handler must have a \'class\' key.\\" # Add other validation checks as needed return True except AssertionError as e: print(f\\"Validation error: {e}\\") return False def test_logging_configuration(): Tests the configure_logging and validate_logging_configuration functions. valid_config = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'simple\', \'level\': \'DEBUG\' } }, \'loggers\': { \'example_logger\': { \'level\': \'DEBUG\', \'handlers\': [\'console\'] } } } invalid_config = { \'version\': 1, \'handlers\': { \'console\': { \'formatter\': \'simple\', \'level\': \'DEBUG\' # Missing \'class\' key } } } print(\\"Testing valid configuration:\\") if validate_logging_configuration(valid_config): configure_logging(valid_config) logger = logging.getLogger(\'example_logger\') logger.debug(\\"This should appear in the console.\\") print(\\"nTesting invalid configuration:\\") if not validate_logging_configuration(invalid_config): print(\\"Invalid configuration detected.\\") test_logging_configuration() ``` Ensure your implementations of `configure_logging`, `validate_logging_configuration`, and `test_logging_configuration` functions adhere to the problem requirements and handle various configurations correctly.","solution":"import logging.config def configure_logging(config_dict): Configures logging using the provided configuration dictionary. logging.config.dictConfig(config_dict) def validate_logging_configuration(config_dict): Validates the provided logging configuration dictionary. Returns True if valid, False otherwise. if not isinstance(config_dict, dict): return False # Check mandatory \'version\' key if config_dict.get(\'version\') != 1: return False # Check \'handlers\' key requirements handlers = config_dict.get(\'handlers\', {}) for handler in handlers.values(): if \'class\' not in handler: return False # Add additional validation checks as needed return True def test_logging_configuration(): Tests the configure_logging and validate_logging_configuration functions with various scenarios. valid_config = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'simple\', \'level\': \'DEBUG\' } }, \'loggers\': { \'example_logger\': { \'level\': \'DEBUG\', \'handlers\': [\'console\'] } } } invalid_config = { \'version\': 1, \'handlers\': { \'console\': { \'formatter\': \'simple\', \'level\': \'DEBUG\' # Missing \'class\' key } } } print(\\"Testing valid configuration:\\") if validate_logging_configuration(valid_config): configure_logging(valid_config) logger = logging.getLogger(\'example_logger\') logger.debug(\\"This should appear in the console.\\") print(\\"nTesting invalid configuration:\\") if not validate_logging_configuration(invalid_config): print(\\"Invalid configuration detected.\\") test_logging_configuration()"},{"question":"# AIFF File Processing Challenge You are tasked with developing a utility function that processes AIFF audio files using the `aifc` module. Specifically, your function should perform the following tasks: 1. **Read** the audio properties of an input AIFF file. 2. **Extract** a segment of the audio from a specified start and end frame. 3. **Save** this audio segment into a new AIFF file with the same audio properties. Function Signature ```python def extract_audio_segment(input_file_path: str, output_file_path: str, start_frame: int, end_frame: int) -> None: Extracts a segment from an AIFF file and saves it to a new file. Parameters: - input_file_path: str - the path to the input AIFF file. - output_file_path: str - the path to the output AIFF file. - start_frame: int - the starting frame number for the segment to extract. - end_frame: int - the ending frame number for the segment to extract. Returns: - None: The function does not return any value. pass ``` Requirements 1. Use the `aifc` module to open and read the input AIFF file. 2. Use the `getparams()` method to retrieve the input file\'s audio properties. 3. Ensure that the `start_frame` is less than `end_frame` and both are valid within the file\'s total frame count. 4. Read the frames from `start_frame` to `end_frame` (inclusive) into memory. 5. Write these frames to the output file, ensuring to set the same audio properties as the input file. Example Usage ```python input_file = \'source.aiff\' output_file = \'segment.aiff\' start_frame = 44100 # Start at the 1-second mark assuming 44.1kHz sample rate end_frame = 88200 # End at the 2-second mark assuming 44.1kHz sample rate extract_audio_segment(input_file, output_file, start_frame, end_frame) ``` Constraints - The input file will always be a valid AIFF file. - Frame indices (`start_frame` and `end_frame`) will be within the bounds of the audio file. - The output file should correctly reflect the extracted audio segment\'s properties. Notes - The function should handle any necessary cleanup, such as closing files after processing. - Do not use any third-party libraries; rely solely on the built-in `aifc` module. This question is designed to assess your understanding of the `aifc` module, including file handling, audio properties manipulation, and frame extraction.","solution":"import aifc def extract_audio_segment(input_file_path: str, output_file_path: str, start_frame: int, end_frame: int) -> None: Extracts a segment from an AIFF file and saves it to a new file. Parameters: - input_file_path: str - the path to the input AIFF file. - output_file_path: str - the path to the output AIFF file. - start_frame: int - the starting frame number for the segment to extract. - end_frame: int - the ending frame number for the segment to extract. Returns: - None: The function does not return any value. # Open the input AIFF file with aifc.open(input_file_path, \'rb\') as infile: # Get audio properties params = infile.getparams() num_frames = infile.getnframes() # Validate frame indices if not (0 <= start_frame < end_frame <= num_frames): raise ValueError(\\"Frame indices are out of bounds or invalid.\\") # Set the input file\'s read position to start_frame infile.setpos(start_frame) # Read frames from start_frame to end_frame frames = infile.readframes(end_frame - start_frame) # Open the output AIFF file and write the extracted frames with the same parameters with aifc.open(output_file_path, \'wb\') as outfile: outfile.setparams(params) outfile.writeframes(frames)"},{"question":"**Coding Assessment Question** You are given an XML document that contains information about various books in a library. Each book has a unique identifier, title, author, and price. Using the `xml.dom.pulldom` module, write a Python function `extract_books_above_price(xml_content: str, price_threshold: float) -> List[Dict[str, str]]` that processes the given XML data and returns a list of dictionaries. Each dictionary should contain the details of books that have a price greater than the `price_threshold`. # Function Signature ```python def extract_books_above_price(xml_content: str, price_threshold: float) -> List[Dict[str, str]]: ``` # Input - `xml_content` (str): A string representation of the XML document. - `price_threshold` (float): The price threshold above which books should be included in the result. # Output - A list of dictionaries, where each dictionary has the following keys: `\'id\'`, `\'title\'`, `\'author\'`, and `\'price\'`. Only include books whose price is greater than the `price_threshold`. # Constraints - The XML structure is guaranteed to be well-formed. - The price values in the XML are always numeric. - The input XML document is provided as a string, not a file. # Example ```python xml_content = <library> <book id=\\"b1\\"> <title>Book One</title> <author>Author One</author> <price>45.00</price> </book> <book id=\\"b2\\"> <title>Book Two</title> <author>Author Two</author> <price>75.00</price> </book> <book id=\\"b3\\"> <title>Book Three</title> <author>Author Three</author> <price>55.00</price> </book> </library> price_threshold = 50.0 # Expected Output [ { \\"id\\": \\"b2\\", \\"title\\": \\"Book Two\\", \\"author\\": \\"Author Two\\", \\"price\\": \\"75.00\\" }, { \\"id\\": \\"b3\\", \\"title\\": \\"Book Three\\", \\"author\\": \\"Author Three\\", \\"price\\": \\"55.00\\" } ] ``` # Notes - Use the `parseString()` method from `xml.dom.pulldom` to parse the XML content. - Pull and process the events to extract relevant book information. - Ensure that the events are processed efficiently.","solution":"from xml.dom import pulldom from typing import List, Dict def extract_books_above_price(xml_content: str, price_threshold: float) -> List[Dict[str, str]]: Processes the given XML data and returns a list of books that have a price greater than the given threshold. Parameters: xml_content (str): The XML content as a string. price_threshold (float): The price threshold above which books should be included in the result. Returns: List[Dict[str, str]]: A list of dictionaries with details of books meeting the price condition. doc = pulldom.parseString(xml_content) books = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \\"book\\": book = {} book[\'id\'] = node.getAttribute(\\"id\\") doc.expandNode(node) for child in node.childNodes: if child.nodeType == child.ELEMENT_NODE: tag_name = child.tagName tag_value = child.firstChild.nodeValue if child.firstChild is not None else \\"\\" book[tag_name] = tag_value if \'price\' in book and float(book[\'price\']) > price_threshold: books.append({ \'id\': book[\'id\'], \'title\': book.get(\'title\', \'\'), \'author\': book.get(\'author\', \'\'), \'price\': book[\'price\'] }) return books"},{"question":"# Objective: Write a Python function that demonstrates the ability to use the `colorsys` module to perform a series of color space transformations and then reverse those transformations to verify consistency. This exercise will help verify your understanding of color coordinate systems and the use of the `colorsys` module for their inter-conversions. # Problem Statement: Create a function `verify_color_conversions` that takes a single argument `rgb` which is a tuple of three floating-point values representing an RGB color (each value between 0 and 1). This function should perform the following tasks in sequence: 1. Convert the RGB values to YIQ, HLS, and HSV color spaces using the appropriate functions from the `colorsys` module. 2. Convert the resulting YIQ, HLS, and HSV values back to RGB. 3. Check the consistency of the conversions by comparing the original RGB values with those obtained after converting back from YIQ, HLS, and HSV. Since floating point operations can introduce minor inaccuracies, a tolerance level of `1e-6` should be used for comparisons. 4. Return a dictionary with the following keys and boolean values: - `\\"RGB-YIQ-RGB\\"`: True if the original RGB matches the RGB after round-trip conversion via YIQ, otherwise False. - `\\"RGB-HLS-RGB\\"`: True if the original RGB matches the RGB after round-trip conversion via HLS, otherwise False. - `\\"RGB-HSV-RGB\\"`: True if the original RGB matches the RGB after round-trip conversion via HSV, otherwise False. # Function Signature: ```python def verify_color_conversions(rgb: tuple) -> dict: pass ``` # Example: ```python # Given RGB values rgb_example = (0.2, 0.4, 0.6) # Expected output format: # { # \\"RGB-YIQ-RGB\\": True or False, # \\"RGB-HLS-RGB\\": True or False, # \\"RGB-HSV-RGB\\": True or False # } # Actual function call result = verify_color_conversions(rgb_example) print(result) ``` # Constraints: - Each value in the `rgb` tuple is a floating-point number between 0 and 1. - Comparison tolerance: `1e-6` # Notes: - Use the `colorsys` module functions for the color conversions. - Ensure the comparisons of floating-point values account for possible minor inaccuracies arising from floating-point arithmetic.","solution":"import colorsys def verify_color_conversions(rgb): Verify color conversions for given RGB value by converting it to YIQ, HLS, and HSV and back to RGB. Args: rgb (tuple): a tuple containing three floating-point values representing RGB color. Returns: dict: A dictionary with boolean results for each color space conversion consistency. r, g, b = rgb # Convert RGB to YIQ and back to RGB yiq = colorsys.rgb_to_yiq(r, g, b) rgb_from_yiq = colorsys.yiq_to_rgb(*yiq) rgb_yiq_match = all(abs(a - b) < 1e-6 for a, b in zip(rgb, rgb_from_yiq)) # Convert RGB to HLS and back to RGB hls = colorsys.rgb_to_hls(r, g, b) rgb_from_hls = colorsys.hls_to_rgb(*hls) rgb_hls_match = all(abs(a - b) < 1e-6 for a, b in zip(rgb, rgb_from_hls)) # Convert RGB to HSV and back to RGB hsv = colorsys.rgb_to_hsv(r, g, b) rgb_from_hsv = colorsys.hsv_to_rgb(*hsv) rgb_hsv_match = all(abs(a - b) < 1e-6 for a, b in zip(rgb, rgb_from_hsv)) return { \\"RGB-YIQ-RGB\\": rgb_yiq_match, \\"RGB-HLS-RGB\\": rgb_hls_match, \\"RGB-HSV-RGB\\": rgb_hsv_match }"},{"question":"**Objective:** You are provided with a dataset containing information about wine quality. Your task is to implement a Python function that uses scikit-learn\'s `GridSearchCV` to find the best hyper-parameters for a Support Vector Classifier (SVC). You should also analyze the results and return the best parameters along with the best score. # Input: - A pandas DataFrame `X` containing the features of the wine dataset. - A pandas Series `y` containing the target variable (wine quality). - A dictionary `param_grid` specifying the hyper-parameters and corresponding values to be searched during the grid search. For example: ```python param_grid = [ {\'C\': [1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] ``` # Output: - A tuple containing: - The best hyper-parameters (a dictionary). - The best cross-validation score (a float). # Constraints: - You must use `GridSearchCV` from `sklearn.model_selection`. - The cross-validation should use 5 folds. - Use the default scoring method for classification. # Function Signature: ```python import pandas as pd from typing import Tuple, Dict def find_best_hyperparameters(X: pd.DataFrame, y: pd.Series, param_grid: Dict) -> Tuple[Dict, float]: pass ``` # Example: ```python # Example usage: from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split import pandas as pd # Load dataset data = load_wine() X = pd.DataFrame(data[\'data\'], columns=data[\'feature_names\']) y = pd.Series(data[\'target\']) # Define parameter grid param_grid = [ {\'C\': [1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] # Call the function best_params, best_score = find_best_hyperparameters(X, y, param_grid) print(\\"Best Parameters:\\", best_params) print(\\"Best Score:\\", best_score) ``` # Notes: - Ensure that scikit-learn and other dependencies are properly imported. - Take care of any data preprocessing steps if needed before fitting the model.","solution":"import pandas as pd from typing import Tuple, Dict from sklearn.model_selection import GridSearchCV from sklearn.svm import SVC def find_best_hyperparameters(X: pd.DataFrame, y: pd.Series, param_grid: Dict) -> Tuple[Dict, float]: Finds the best hyperparameters for a Support Vector Classifier using GridSearchCV. Parameters: - X: DataFrame containing the features of the wine dataset. - y: Series containing the target variable (wine quality). - param_grid: Dictionary specifying the hyper-parameters and corresponding values to be searched. Returns: - A tuple containing: - The best hyper-parameters (a dictionary). - The best cross-validation score (a float). # Create an SVC estimator svc = SVC() # Set up GridSearchCV with 5-fold cross-validation grid_search = GridSearchCV(svc, param_grid, cv=5) # Fit grid search grid_search.fit(X, y) # Extract the best parameters and best score best_params = grid_search.best_params_ best_score = grid_search.best_score_ return best_params, best_score"},{"question":"**Objective**: Demonstrate your understanding of `seaborn` color palettes and their integration into seaborn plots. **Task**: 1. Using seaborn, load a well-known dataset, such as the `tips` dataset. 2. Create three different plots using the `sns.scatterplot` function: a. One plot using a discrete colormap with 6 colors from the \'viridis\' colormap. b. One plot using the continuous \'viridis\' colormap without setting a fixed number of colors. c. One plot using a discrete colormap with 7 colors from the \'Set2\' qualitative colormap. 3. Customize the aesthetics and layout for better visualization, ensuring distinguishable colors for different data points. 4. Return a 3-tuple containing these three plots. **Input**: None. **Output**: A 3-tuple containing three seaborn scatterplot objects. **Constraints**: - Use the seaborn library for loading the dataset and plotting. - Ensure the plots are clear, properly labeled, and easy to differentiate. **Hints**: - Use `sns.load_dataset(\'tips\')` to load the dataset. - Use `sns.mpl_palette` to generate discrete color palettes. - Use appropriate plot customization methods to enhance plot aesthetics. ```python def create_custom_scatterplots(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\'tips\') # Set theme for visual consistency sns.set_theme() # Create discrete colormap with 6 colors from \'viridis\' viridis_6 = sns.mpl_palette(\\"viridis\\", 6) plot1 = sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\', palette=viridis_6) # Create continuous \'viridis\' colormap viridis_cmap = sns.mpl_palette(\\"viridis\\", as_cmap=True) plot2 = sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\', palette=viridis_cmap) # Create discrete colormap with 7 colors from \'Set2\' set2_7 = sns.mpl_palette(\\"Set2\\", 7) plot3 = sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\', palette=set2_7) plt.close(\'all\') # Close plots to prevent them from displaying automatically return plot1, plot2, plot3 ``` # Important Notes: - Make sure to run `plt.show()` if you want to visualize the plots while testing your function in a local environment. - The plots should be returned as objects, and they are expected to be visually evaluated.","solution":"def create_custom_scatterplots(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\'tips\') # Set theme for visual consistency sns.set_theme() # Create first plot with discrete colormap with 6 colors from \'viridis\' plt.figure() viridis_6 = sns.color_palette(\\"viridis\\", 6) plot1 = sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\', palette=viridis_6) plt.title(\\"Discrete \'viridis\' colormap with 6 colors\\") plt.close() # Create second plot with continuous \'viridis\' colormap plt.figure() viridis_cmap = sns.color_palette(\\"viridis\\", as_cmap=True) plot2 = sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\', palette=\'viridis\') plt.title(\\"Continuous \'viridis\' colormap\\") plt.close() # Create third plot with discrete colormap with 7 colors from \'Set2\' plt.figure() set2_7 = sns.color_palette(\\"Set2\\", 7) plot3 = sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\', palette=set2_7) plt.title(\\"Discrete \'Set2\' colormap with 7 colors\\") plt.close() return plot1, plot2, plot3"},{"question":"# Python Coding Assessment: Custom Module Importer In this assessment, you will be creating a function that simulates part of the Python module importing system using Python code. Your task is to implement a custom import function that mimics the behavior of `PyImport_ImportModule`. # Function Signature ```python def custom_import(module_name: str) -> object: This function simulates importing a module dynamically by its name, similar to the PyImport_ImportModule function. Args: - module_name (str): The name of the module to be imported. Returns: - object: The imported module object. Raises: - ImportError: If the module cannot be imported. ``` # Requirements: 1. **Dynamic Importing**: The function must take a module name as an argument and return the corresponding module object. 2. **Error Handling**: If the module does not exist, the function should raise an `ImportError` with a descriptive message. 3. **Absolute Imports**: The function should use absolute imports similar to `PyImport_ImportModule`. 4. **No Use of `import` Statement**: You are not allowed to use the `import` statement directly within this function. Use functions from the `importlib` library to perform the import. # Constraints: - **Module Name Format**: The module name can be a top-level module (e.g., `os`), a submodule (e.g., `xml.etree.ElementTree`), or any valid Python module. - **Performance**: The function should be efficient and avoid unnecessary operations or imports. # Example Usage: ```python # Importing a top-level module: os_module = custom_import(\'os\') print(os_module) # <module \'os\' from \'.../os.py\'> # Importing a submodule: etree_module = custom_import(\'xml.etree.ElementTree\') print(etree_module) # <module \'xml.etree.ElementTree\' from \'.../xml/etree/ElementTree.py\'> # Non-existent module: try: non_existent_module = custom_import(\'non_existent_module\') except ImportError as e: print(e) # Should print a message indicating the module cannot be imported ``` # Hint: Use the `importlib` module in Python, specifically `importlib.import_module` to perform the import. Ensure to handle exceptions appropriately to raise `ImportError` when the module cannot be imported. Write your function implementation below: ```python # Your implementation here def custom_import(module_name: str) -> object: import importlib try: module = importlib.import_module(module_name) return module except ModuleNotFoundError as e: raise ImportError(f\\"Module \'{module_name}\' cannot be imported.\\") from e ``` Test your implementation with various module names to ensure accuracy and correctness.","solution":"def custom_import(module_name: str) -> object: This function simulates importing a module dynamically by its name, similar to the PyImport_ImportModule function. Args: - module_name (str): The name of the module to be imported. Returns: - object: The imported module object. Raises: - ImportError: If the module cannot be imported. import importlib try: module = importlib.import_module(module_name) return module except (ModuleNotFoundError, ImportError) as e: raise ImportError(f\\"Module \'{module_name}\' cannot be imported.\\") from e"},{"question":"Managing Python Site Configuration Problem Statement As a Python developer, you need to configure the `sys.path` to add a new directory containing third-party packages for your project. The `site` module provides functionality to manage these configurations. Your task is to write a Python script that does the following: 1. **Create and Add a New Site Directory**: Create a new directory named `custom_packages`. 2. **Add Directory to sys.path**: Use the `site.addsitedir()` function to add this new directory to `sys.path`. 3. **Create and Process a .pth File**: - Inside `custom_packages`, create a `.pth` file named `custom_paths.pth`. - Add the following paths (relative to `custom_packages`) to the `.pth` file: ``` custom_module # Comment line another_module ``` 4. **Verify Custom Paths**: - Ensure that these paths are added to `sys.path` (ignoring the comment line). - Print the updated `sys.path` to verify the changes. Requirements - You should not manually modify `sys.path` except through the `site.addsitedir()` function. - Only existing paths should be added to `sys.path`. - Your script should run without any errors even if the directories do not exist initially. Make sure to create any required directories as part of your solution. Function Signature ```python def configure_site_packages(base_dir: str): Configures custom site packages directory and verifies the paths. Args: base_dir (str): The base directory where custom_packages will be created. Returns: None pass ``` Example Usage ```python base_dir = \\"/path/to/base/dir\\" configure_site_packages(base_dir) ``` Upon calling `configure_site_packages()`: 1. A new directory `/path/to/base/dir/custom_packages` should be created. 2. A `.pth` file with specified paths should be created inside this directory. 3. These paths should be added to `sys.path` and verified, ensuring the comment line is ignored. Notes - Ensure you handle file operations and directory creations safely. - Be aware of platform-specific path separators when handling file and directory paths.","solution":"import os import site import sys def configure_site_packages(base_dir: str): Configures custom site packages directory and verifies the paths. Args: base_dir (str): The base directory where custom_packages will be created. Returns: None # Step 1: Create custom_packages directory custom_packages_dir = os.path.join(base_dir, \'custom_packages\') os.makedirs(custom_packages_dir, exist_ok=True) # Step 2: Add Directory to sys.path using site.addsitedir() site.addsitedir(custom_packages_dir) # Step 3: Create and process a .pth file pth_file_path = os.path.join(custom_packages_dir, \'custom_paths.pth\') with open(pth_file_path, \'w\') as f: f.write(\\"custom_modulen\\") f.write(\\"# Comment linen\\") f.write(\\"another_modulen\\") # Step 4: Verify Custom Paths with open(pth_file_path, \'r\') as f: for line in f: cleaned_line = line.strip() if cleaned_line and not cleaned_line.startswith(\'#\'): path_to_add = os.path.join(custom_packages_dir, cleaned_line) if path_to_add not in sys.path: sys.path.append(path_to_add) # Print the updated sys.path to verify the changes print(sys.path)"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of binary data manipulation using the Python `struct` module. You are required to create a function that works with packed binary data to encode and decode structured information. # Problem Statement You are given information about a user in the following format: - `user_id` (4-byte integer, little-endian) - `age` (1-byte unsigned integer) - `height` (2-byte unsigned integer, little-endian) - `weight` (4-byte float, little-endian) - `name_length` (1-byte unsigned integer indicating the length of the name) - `name` (a sequence of ASCII characters, length specified by `name_length`) Write a function `pack_user_data` that takes user information as input and returns a bytes object representing the packed binary data. Also, write a complementary function `unpack_user_data` that takes the binary data produced by `pack_user_data` and returns the original user information. # Function Signatures ```python def pack_user_data(user_id: int, age: int, height: int, weight: float, name: str) -> bytes: # Your implementation here def unpack_user_data(packed_data: bytes) -> tuple: # Your implementation here ``` # Input 1. `pack_user_data` function: - `user_id` (integer): A 4-byte integer ID of the user. - `age` (integer): A 1-byte unsigned integer representing the user\'s age. - `height` (integer): A 2-byte unsigned integer representing the user\'s height in centimeters. - `weight` (float): A 4-byte float representing the user\'s weight in kilograms. - `name` (string): The user\'s name encoded in ASCII (maximum length of 255 characters). 2. `unpack_user_data` function: - `packed_data` (bytes): A bytes object representing the packed binary data of the user. # Output 1. `pack_user_data`: - Returns a bytes object representing the packed binary data of the user information. 2. `unpack_user_data`: - Returns a tuple with the original user information in the following order: ```python (user_id: int, age: int, height: int, weight: float, name: str) ``` # Constraints - `user_id` should be between 0 and 2^32-1. - `age` should be between 0 and 255. - `height` should be between 0 and 65535. - `weight` should be a positive float value. - `name` should be a non-empty ASCII string with a maximum length of 255 characters. # Example ```python user_data = (12345, 25, 180, 75.5, \\"John Doe\\") # Packing the user data packed_data = pack_user_data(*user_data) print(packed_data) # Should print a bytes object that represents the packed binary data # Unpacking the data unpacked_data = unpack_user_data(packed_data) print(unpacked_data) # Should print: (12345, 25, 180, 75.5, \\"John Doe\\") ``` **Hint**: Utilize the `struct` module\'s functions and format strings to accurately pack and unpack binary data.","solution":"import struct def pack_user_data(user_id: int, age: int, height: int, weight: float, name: str) -> bytes: Packs user information into a binary format. :param user_id: 4-byte integer ID of the user. :param age: 1-byte unsigned integer representing the user\'s age. :param height: 2-byte unsigned integer representing the user\'s height in centimeters. :param weight: 4-byte float representing the user\'s weight in kilograms. :param name: The user\'s name encoded in ASCII (maximum length of 255 characters). :return: A bytes object representing the packed binary data of the user information. name_bytes = name.encode(\'ascii\') name_length = len(name_bytes) format_string = f\'<I B H f B {name_length}s\' return struct.pack(format_string, user_id, age, height, weight, name_length, name_bytes) def unpack_user_data(packed_data: bytes) -> tuple: Unpacks binary data into original user information. :param packed_data: A bytes object representing the packed binary data of the user. :return: A tuple with the original user information (user_id, age, height, weight, name) user_id, age, height, weight, name_length = struct.unpack(\'<I B H f B\', packed_data[:12]) name = struct.unpack(f\'<{name_length}s\', packed_data[12:12+name_length])[0].decode(\'ascii\') return (user_id, age, height, weight, name)"},{"question":"Analyzing and Extracting Data Using Regular Expressions Objective: Demonstrate your understanding of regular expressions in Python by writing a function that identifies and extracts specific patterns from a given text. Problem Statement: You are provided with a text document containing multiple lines of data. This data includes dates, email addresses, and phone numbers, all mixed within other textual information. Your task is to write a Python function that extracts all the email addresses, phone numbers, and dates from the text. Input: - A string `text` containing multiple lines of data. Each line may have dates in the format `YYYY-MM-DD`, email addresses, and phone numbers in the format `(xxx) xxx-xxxx`. Output: - A dictionary with three keys: `emails`, `phone_numbers`, and `dates`. Each key should map to a list of strings containing the extracted data in the order they appear in the input text. Constraints: - The text can contain multiple lines. - Each line may contain zero or more email addresses, phone numbers, and dates. - Email addresses will follow the general format: `local_part@domain`. - Phone numbers will be in the format `(xxx) xxx-xxxx`. - Dates will be in the format `YYYY-MM-DD`. Example: ```python def extract_data(text: str) -> dict: # Your implementation here # Example Input text = Contact us at support@example.com or (123) 456-7890. Our meeting is scheduled on 2023-08-15. For more info, email info@company.org or call (987) 654-3210 on 2023-09-01. # Example Output { \\"emails\\": [\\"support@example.com\\", \\"info@company.org\\"], \\"phone_numbers\\": [\\"(123) 456-7890\\", \\"(987) 654-3210\\"], \\"dates\\": [\\"2023-08-15\\", \\"2023-09-01\\"] } ``` Requirements: - Use Python\'s built-in `re` module to perform all pattern matching and extraction. - Ensure your solution handles any edge cases and performs efficiently on large input texts.","solution":"import re def extract_data(text: str) -> dict: Extracts email addresses, phone numbers, and dates from the given text. Args: - text (str): The input text containing multiple lines of data. Returns: - dict: A dictionary with keys \'emails\', \'phone_numbers\', and \'dates\', mapping to lists of extracted strings. # Regular expressions for emails, phone numbers, and dates email_regex = r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\' phone_regex = r\'(d{3}) d{3}-d{4}\' date_regex = r\'d{4}-d{2}-d{2}\' # Using findall to extract all occurrences of the patterns emails = re.findall(email_regex, text) phone_numbers = re.findall(phone_regex, text) dates = re.findall(date_regex, text) return { \\"emails\\": emails, \\"phone_numbers\\": phone_numbers, \\"dates\\": dates }"},{"question":"# Formatted String Handling and Case-Insensitive Comparison You are required to implement functions that will read formatted data from a given string, convert it to numbers, perform specific operations, and then output the result as a formatted string. Additionally, you will perform case-insensitive comparisons between the original and the formatted strings. Task Details 1. **String to Number Conversion**: Implement a function named `convert_string_to_double` that utilizes the `PyOS_string_to_double` function to convert a given string representation of a floating-point number to a double. Ensure to handle exceptions appropriately. 2. **Number to String Conversion**: Implement a function named `convert_double_to_string` that utilizes the `PyOS_double_to_string` function to convert a double to a string with specified formatting parameters. 3. **Case-Insensitive String Comparison**: Implement a function named `case_insensitive_compare` that utilizes the `PyOS_stricmp` and `PyOS_strnicmp` functions to compare two strings for equality, ignoring any differences in case. 4. **Main Function**: Implement a function named `format_and_compare` which: - Takes an input string and checks if it is a valid representation of a floating-point number using `convert_string_to_double`. - Converts the number back to a string using `convert_double_to_string` with a specified format. - Compares the original input string with the newly formatted string using `case_insensitive_compare`. - Outputs the result of the comparison (True/False). Function Signatures and Behavior ```python def convert_string_to_double(s: str) -> float: Converts the given string `s` to a floating-point number using PyOS_string_to_double. Parameters: s (str): The string to convert. Returns: float: The converted floating point number. Raises: ValueError: If the string is not a valid floating-point number. def convert_double_to_string(val: float, format_code: str, precision: int, flags: int) -> str: Converts the given floating point number `val` to a string using PyOS_double_to_string. Parameters: val (float): The number to convert. format_code (str): The format code (e.g., \'f\', \'e\'). precision (int): The precision for the conversion. flags (int): The formatting flags. Returns: str: The formatted string representation of the number. def case_insensitive_compare(s1: str, s2: str) -> bool: Compares the two strings `s1` and `s2` in a case-insensitive manner using PyOS_stricmp. Parameters: s1 (str): The first string to compare. s2 (str): The second string to compare. Returns: bool: True if the strings are equal ignoring case, False otherwise. def format_and_compare(input_string: str, format_code: str, precision: int, flags: int) -> bool: Reads a number from the input string, converts it to a formatted string, and compares the original and formatted strings in a case-insensitive manner. Parameters: input_string (str): The input string to read the number from. format_code (str): The format code (e.g., \'f\', \'e\') for converting to a string. precision (int): The precision for the conversion. flags (int): The formatting flags. Returns: bool: The result of comparing the original and formatted strings for equality (case-insensitive). ``` Example Usage ```python try: result = format_and_compare(\\"123.456\\", \'f\', 2, 0) print(result) # Output should be True or False based on the comparison. except ValueError as e: print(f\\"Error: {e}\\") ``` Constraints - `convert_string_to_double` must ensure no leading or trailing whitespace is allowed. - Ensure appropriate error handling and input validation. - Performance should be efficient given practical constraints on floating-point number sizes and precisions.","solution":"def convert_string_to_double(s: str) -> float: Converts the given string `s` to a floating-point number. Parameters: s (str): The string to convert. Returns: float: The converted floating point number. Raises: ValueError: If the string is not a valid floating-point number. try: return float(s) except ValueError: raise ValueError(f\\"Invalid floating-point number: {s}\\") def convert_double_to_string(val: float, format_code: str, precision: int, flags: int) -> str: Converts the given floating point number `val` to a string using specified format parameters. Parameters: val (float): The number to convert. format_code (str): The format code (e.g., \'f\', \'e\'). precision (int): The precision for the conversion. flags (int): The formatting flags (currently not used). Returns: str: The formatted string representation of the number. if format_code not in [\'f\', \'e\', \'g\']: raise ValueError(f\\"Invalid format code: {format_code}\\") format_spec = f\\".{precision}{format_code}\\" return format(val, format_spec) def case_insensitive_compare(s1: str, s2: str) -> bool: Compares the two strings `s1` and `s2` in a case-insensitive manner. Parameters: s1 (str): The first string to compare. s2 (str): The second string to compare. Returns: bool: True if the strings are equal ignoring case, False otherwise. return s1.lower() == s2.lower() def format_and_compare(input_string: str, format_code: str, precision: int, flags: int) -> bool: Reads a number from the input string, converts it to a formatted string, and compares the original and formatted strings in a case-insensitive manner. Parameters: input_string (str): The input string to read the number from. format_code (str): The format code (e.g., \'f\', \'e\') for converting to a string. precision (int): The precision for the conversion. flags (int): The formatting flags. Returns: bool: The result of comparing the original and formatted strings for equality (case-insensitive). try: number = convert_string_to_double(input_string) formatted_string = convert_double_to_string(number, format_code, precision, flags) return case_insensitive_compare(input_string, formatted_string) except ValueError: return False"},{"question":"**XML Data Processing and Modification using `xml.etree.ElementTree`** In this task, you will be required to process and modify XML data using Python\'s `xml.etree.ElementTree` module. This problem will test your understanding of parsing XML documents, navigating and manipulating the XML tree, and generating modified XML documents. # Problem Statement You are given an XML document containing a list of books in a library. Each book has attributes such as title, author, year of publication, and genre. Your task is to: 1. Parse the XML document. 2. Find all books published after a given year. 3. Modify the title of each book in this list to append the year of publication to the title. 4. Create a new XML document with only these modified books and output it as a string. # Input Format 1. An XML string `xml_data` representing the library of books. 2. An integer `year` representing the year after which you need to find the books. # Output Format - A string representing the new XML containing only the modified books. # Example Given the following input: ```python xml_data = \'\'\' <library> <book> <title>Book One</title> <author>Author A</author> <year>2001</year> <genre>Fiction</genre> </book> <book> <title>Book Two</title> <author>Author B</author> <year>2005</year> <genre>Science Fiction</genre> </book> <book> <title>Book Three</title> <author>Author C</author> <year>2010</year> <genre>Non-fiction</genre> </book> </library> \'\'\' year = 2003 ``` Your function should output: ```python \'\'\' <library> <book> <title>Book Two (2005)</title> <author>Author B</author> <year>2005</year> <genre>Science Fiction</genre> </book> <book> <title>Book Three (2010)</title> <author>Author C</author> <year>2010</year> <genre>Non-fiction</genre> </book> </library> \'\'\' ``` # Constraints 1. The input XML is well-formed. 2. All books have the following mandatory fields: title, author, year, and genre. # Implementation Implement the function `process_books(xml_data: str, year: int) -> str` to solve the problem. ```python import xml.etree.ElementTree as ET def process_books(xml_data: str, year: int) -> str: # Your code here pass ``` # Notes - Ensure that your output XML string is properly formatted. - You may use utility functions as needed to keep your code modular and readable.","solution":"import xml.etree.ElementTree as ET def process_books(xml_data: str, year: int) -> str: root = ET.fromstring(xml_data) # Find all books published after the given year modified_books = [] for book in root.findall(\'book\'): book_year = int(book.find(\'year\').text) if book_year > year: title_element = book.find(\'title\') new_title = f\\"{title_element.text} ({book_year})\\" title_element.text = new_title modified_books.append(book) # Create a new XML tree with only the modified books new_library = ET.Element(\'library\') for book in modified_books: new_library.append(book) # Convert the new library Element to a string return ET.tostring(new_library, encoding=\'unicode\')"},{"question":"Objective: To assess the student\'s understanding of creating built distributions in Python, particularly focusing on custom RPM distributions using the **bdist_rpm** command. Question: You are required to create a Python module distribution and customize its packaging process to generate a specific RPM package. Your task involves writing scripts along with a setup configuration for the distribution. 1. Create a simple Python package named `mymodule` with a setup script (`setup.py`). The module should contain one file, `mymodule.py`, with the following content: ```python def greet(name): return f\\"Hello, {name}!\\" ``` 2. Write a setup script (`setup.py`) to build this module. Ensure your setup script includes metadata such as the name, version, author, and description. 3. Create a setup configuration file (`setup.cfg`) to customize the RPM packaging process with the following specifications: - **Release**: 2 - **Group**: Development/Utilities - **Vendor**: \\"Your Name <your.email@example.com>\\" - **Packager**: \\"Your Name <your.email@example.com>\\" - **Requires**: python310 4. Generate the `.spec` file and build the RPM package using the `bdist_rpm` command with appropriate options. Expected Input and Output: - The input will be the creation of necessary files (`mymodule.py`, `setup.py`, `setup.cfg`). - The output will be the creation of a custom RPM package for the `mymodule` library. Constraints: - You should assume the environment has all necessary tools and permissions to build RPMs and run distutils commands. - You should not hard code paths; use relative paths within your created package. Notes: - Test your module by creating and installing the RPM package to ensure it works correctly. - The output RPM package should reflect all specified metadata and dependencies. # Example Output: After building the RPM package, installing it, and running the following in Python: ```python from mymodule import greet print(greet(\\"World\\")) ``` The output should be: ``` Hello, World! ```","solution":"# Create the directory structure import os os.makedirs(\\"mymodule\\", exist_ok=True) # Create mymodule.py with open(\\"mymodule/mymodule.py\\", \\"w\\") as f: f.write(\'\'\' def greet(name): return f\\"Hello, {name}!\\" \'\'\') # Create setup.py with open(\\"setup.py\\", \\"w\\") as f: f.write(\'\'\' from setuptools import setup, find_packages setup( name=\\"mymodule\\", version=\\"0.1\\", author=\\"Your Name\\", author_email=\\"your.email@example.com\\", description=\\"A simple greeting module\\", packages=find_packages(), ) \'\'\') # Create setup.cfg with open(\\"setup.cfg\\", \\"w\\") as f: f.write(\'\'\' [bdist_rpm] release = 2 group = Development/Utilities vendor = Your Name <your.email@example.com> packager = Your Name <your.email@example.com> requires = python310 \'\'\') # Build the RPM package os.system(\\"python setup.py bdist_rpm\\")"},{"question":"Understanding and Using sns.husl_palette Problem Description You are tasked with generating and visualizing various color palettes using the `sns.husl_palette()` function from the seaborn library. Implement a function called `generate_color_palette` that experiments with different parameters of this function and returns a dictionary containing the generated palettes. Function Signature ```python def generate_color_palette(num_colors: int, lightness: float, saturation: float, hue: float, continuous: bool) -> dict: pass ``` Input - `num_colors` (int): The number of colors to include in the palette. Positive integer. - `lightness` (float): The lightness value for the palette. Must be between 0 and 1. - `saturation` (float): The saturation value for the palette. Must be between 0 and 1. - `hue` (float): The hue starting point for the palette. Must be between 0 and 1. - `continuous` (bool): Whether to return a continuous colormap. Output - The function should return a dictionary with the following keys and corresponding values: - `\\"discrete\\"`: The discrete color palette generated (if `continuous` is False). - `\\"continuous\\"`: The continuous colormap generated (if `continuous` is True). Constraints 1. `num_colors` should be a positive integer greater than 0. 2. `lightness`, `saturation`, and `hue` should be floating-point numbers between 0 and 1. 3. If `continuous` is True, generate and return a continuous colormap. Example ```python palette = generate_color_palette(8, 0.5, 0.5, 0.5, False) # Expected output: # { # \\"discrete\\": <List of 8 colors with specified properties>, # \\"continuous\\": None # } palette = generate_color_palette(6, 0.7, 0.4, 0.3, True) # Expected output: # { # \\"discrete\\": None, # \\"continuous\\": <Continuous colormap object> # } ``` Notes - Use the parameters effectively to manipulate the color palette. - Return `None` for `\\"discrete\\"` if a continuous palette is generated and vice versa. - Include import statements and required seaborn setup within your function implementation.","solution":"import seaborn as sns def generate_color_palette(num_colors: int, lightness: float, saturation: float, hue: float, continuous: bool) -> dict: Generates a color palette using the sns.husl_palette function. Parameters: - num_colors (int): Number of colors in the palette. - lightness (float): Lightness value (0 to 1). - saturation (float): Saturation value (0 to 1). - hue (float): Hue value (0 to 1). - continuous (bool): If True, a continuous colormap is generated. Returns: - dict: Dictionary containing the discrete or continuous color palette. if not (0 <= lightness <= 1 and 0 <= saturation <= 1 and 0 <= hue <= 1): raise ValueError(\\"lightness, saturation, and hue must be between 0 and 1\\") if num_colors <= 0: raise ValueError(\\"num_colors must be a positive integer\\") if continuous: palette = sns.husl_palette(num_colors, l=lightness, s=saturation, h=hue, as_cmap=True) return {\\"discrete\\": None, \\"continuous\\": palette} else: palette = sns.husl_palette(num_colors, l=lightness, s=saturation, h=hue) return {\\"discrete\\": palette, \\"continuous\\": None}"},{"question":"Objective: Design an inter-process communication system using Python\'s `socket` and `asyncio` modules. This system should allow multiple clients to connect to a server, send messages, and receive replies asynchronously. Requirements: 1. **Server Implementation**: - The server must handle multiple client connections simultaneously. - It should be capable of receiving messages from clients and sending appropriate responses. - Use the `asyncio` module to implement the asynchronous communication. 2. **Client Implementation**: - Clients should be able to connect to the server, send messages, and receive responses. - Use the `socket` module for networking and integrate with `asyncio` for asynchronous operations. # Specifics: Server: - The server should run on `localhost` and listen on port `12345`. - Upon receiving a message from a client, the server should respond with the message reversed. - The server should be able to handle at least 5 concurrent client connections. Client: - The client should connect to the server, send a user-specified message, and print the server\'s response. - Implement a simple command-line interface to allow the user to input messages. Input/Output Format: - Each client sends a single string message to the server. - The server receives the string, reverses it, and sends it back to the client. - The client prints the reversed string obtained from the server. # Constraints: - The solution should handle connection errors gracefully. - Ensure that all network resources (sockets) are properly closed after communication. # Example: ```python # Example server usage # Run the server in one terminal # >>> python server.py # Example client usage # Use another terminal to run the client # >>> python client.py # Enter message to send to the server: \\"Hello, Server!\\" # Server response: \\"!revreS ,olleH\\" ``` Server Implementation (server.py): ```python import asyncio async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") reversed_message = message[::-1] print(f\\"Send: {reversed_message}\\") writer.write(reversed_message.encode()) await writer.drain() print(\\"Close the connection\\") writer.close() async def main(): server = await asyncio.start_server(handle_client, \'localhost\', 12345) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() asyncio.run(main()) ``` Client Implementation (client.py): ```python import asyncio import socket async def tcp_client(message): reader, writer = await asyncio.open_connection(\'localhost\', 12345) print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Server response: {data.decode()}\') print(\'Close the connection\') writer.close() message = input(\\"Enter message to send to the server: \\") asyncio.run(tcp_client(message)) ```","solution":"# server.py import asyncio async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") reversed_message = message[::-1] print(f\\"Send: {reversed_message}\\") writer.write(reversed_message.encode()) await writer.drain() print(\\"Close the connection\\") writer.close() async def main(): server = await asyncio.start_server(handle_client, \'localhost\', 12345) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) # client.py import asyncio async def tcp_client(message): reader, writer = await asyncio.open_connection(\'localhost\', 12345) print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Server response: {data.decode()}\') print(\'Close the connection\') writer.close() if __name__ == \\"__main__\\": message = input(\\"Enter message to send to the server: \\") asyncio.run(tcp_client(message))"},{"question":"Question: Secure Login Simulation You are tasked with implementing a secure login simulation using the `getpass` module. In this exercise, you will write a function that prompts users for their username and password, validates the input, and grants access if the credentials are correct. Make sure to use the functions provided by the `getpass` module to handle the user input securely. # Function Signature ```python def secure_login(users_data: dict) -> str: ``` # Parameters - `users_data`: A dictionary where keys are usernames (strings) and values are their corresponding passwords (strings). # Expected Behavior 1. The function should first retrieve the current username using `getpass.getuser()`. 2. Prompt the user to enter their password using `getpass.getpass()`. 3. Validate the entered password against the provided `users_data` dictionary. 4. If the username does not exist in `users_data` or the password is incorrect, the function should return `\'Access Denied\'`. 5. If the credentials are correct, the function should return `\'Access Granted\'`. # Example ```python users_data = { \\"alice\\": \\"alice123\\", \\"bob\\": \\"bobpassword\\", \\"charlie\\": \\"charliepass\\" } # Assuming the current user\'s login name is \'alice\' and they correctly input \'alice123\' result = secure_login(users_data) print(result) # Output: \'Access Granted\' ``` # Constraints - Only valid, non-empty usernames and passwords should be present in the `users_data` dictionary. - The function should handle the input securely and should never print the password or any sensitive data. # Important Notes - You can assume that the script will be run on a terminal that supports the secure password input required by `getpass.getpass()`. - Handle any potential warnings using the appropriate exception handling techniques if necessary.","solution":"import getpass def secure_login(users_data: dict) -> str: Simulates a secure login process by authenticating a user based on a dictionary of usernames and passwords. Parameters: users_data (dict): A dictionary where keys are usernames and values are their passwords. Returns: str: \'Access Granted\' if credentials are correct, otherwise \'Access Denied\'. # Get the username of the current user current_user = getpass.getuser() # Check if the current user is in the provided users_data dictionary if current_user not in users_data: return \'Access Denied\' # Prompt the user to enter their password securely entered_password = getpass.getpass(\'Password: \') # Validate the entered password against the stored password if users_data[current_user] == entered_password: return \'Access Granted\' else: return \'Access Denied\'"},{"question":"# Persistent Diary using `shelve` You are required to implement a small application that manages a persistent diary using the `shelve` module. The diary should allow users to add, update, delete, and view diary entries. Each entry will have a date and content, stored as a dictionary where the key is the date (as a string in \'YYYY-MM-DD\' format) and the value is a list containing the diary contents for that date. Requirements: 1. Implement a `PersistentDiary` class that initializes a shelf with a given filename and provides methods to add, update, delete, and retrieve diary entries. 2. Ensure data integrity and handle potential errors (e.g., trying to delete an entry that does not exist). 3. Use context management to ensure proper opening and closing of the shelf. 4. The class should have the following methods: - `__init__(self, filename: str)`: Initialize the shelf with the given filename. - `add_entry(self, date: str, content: str)`: Add an entry on the specified date. If the date already has entries, append to the existing list. - `update_entry(self, date: str, index: int, new_content: str)`: Update the entry at the specified index on the given date. - `delete_entry(self, date: str, index: int)`: Delete the entry at the specified index on the given date. - `get_entries(self, date: str) -> list`: Retrieve all entries for the given date. - `list_dates(self) -> list`: List all dates that have diary entries. - `close(self)`: Close the shelf. Input and Output Format: - The `add_entry` method takes a `date` (string, \'YYYY-MM-DD\') and `content` (string). - The `update_entry` and `delete_entry` methods take a `date` (string, \'YYYY-MM-DD\') and an `index` (integer) and either new content (string) for updates or just the index for deletions. - The `get_entries` method takes a `date` (string, \'YYYY-MM-DD\') and returns a list of entries. - The `list_dates` method returns a list of strings representing dates with entries. Constraints: - Ensure date strings are in the proper \'YYYY-MM-DD\' format. - Handle errors gracefully, such as attempting to update or delete non-existent entries. Example Usage: ```python # Initialize the diary diary = PersistentDiary(\'my_diary_shelf.db\') # Add entries diary.add_entry(\'2023-10-10\', \'Went to the market.\') diary.add_entry(\'2023-10-10\', \'Cooked dinner.\') # Update an entry diary.update_entry(\'2023-10-10\', 1, \'Cooked a delicious dinner.\') # Get entries print(diary.get_entries(\'2023-10-10\')) # Output: [\'Went to the market.\', \'Cooked a delicious dinner.\'] # List all dates with entries print(diary.list_dates()) # Output: [\'2023-10-10\'] # Delete an entry diary.delete_entry(\'2023-10-10\', 0) # Get entries after deletion print(diary.get_entries(\'2023-10-10\')) # Output: [\'Cooked a delicious dinner.\'] # Close the diary diary.close() ``` Note: Make sure to close the shelf properly to save changes.","solution":"import shelve class PersistentDiary: def __init__(self, filename: str): self.filename = filename self.shelf = shelve.open(self.filename, writeback=True) def add_entry(self, date: str, content: str): if date not in self.shelf: self.shelf[date] = [] self.shelf[date].append(content) self.shelf.sync() def update_entry(self, date: str, index: int, new_content: str): if date in self.shelf and 0 <= index < len(self.shelf[date]): self.shelf[date][index] = new_content self.shelf.sync() else: raise IndexError(\\"Entry not found for the provided date and index.\\") def delete_entry(self, date: str, index: int): if date in self.shelf and 0 <= index < len(self.shelf[date]): del self.shelf[date][index] if not self.shelf[date]: del self.shelf[date] self.shelf.sync() else: raise IndexError(\\"Entry not found for the provided date and index.\\") def get_entries(self, date: str) -> list: return self.shelf.get(date, []) def list_dates(self) -> list: return list(self.shelf.keys()) def close(self): self.shelf.close()"},{"question":"**Question: Creating and Customizing Seaborn Color Palettes** You are tasked with creating a visual report that involves customizing color palettes using Seaborn. Your code should demonstrate a deep understanding of the `light_palette` function in Seaborn, including various ways to specify colors and control palette properties. # Requirements: 1. **Sequential Color Palette** - Create a sequential color palette that transitions from light gray to the color \\"teal\\". - Save this palette and use it to create a plot demonstrating the use of this palette with at least 10 colors. 2. **Hex Code Specification** - Create a sequential color palette from light gray to the color specified by the hex code \\"#FFA07A\\". - Create and display a heatmap using this palette. 3. **HUSL Color System** - Create a sequential color palette using HUSL with coordinates (50, 70, 40). - Use the palette to create a scatter plot with at least 50 points. 4. **Continuous Color Map** - Create a continuous colormap from light gray to the color \\"#BADA55\\". - Use this colormap to create and display a distribution plot of a dataset of your choice. # Inputs and Outputs - **Input:** You do not need specific input from the user; datasets can be randomly generated or imported via seaborn\'s built-in datasets. - **Output:** - Visualize each plot using matplotlib or seaborn\'s plotting functions. - Each plot must clearly demonstrate the use of the specified palette or colormap. # Constraints - You must use the Seaborn package for all palette and colormap creations. - Each plot should be clearly labeled and include a title. - Plots should be rendered in a single Jupyter notebook or Python script. # Performance Requirements - Ensure that your code runs efficiently and plots are rendered within a reasonable time frame. ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Your solution implementation goes here ``` This problem will test your ability to work with color palettes in Seaborn, demonstrate different ways to specify colors, and apply these palettes to different types of plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd def create_sequential_teal_palette(): # Create a sequential color palette from light gray to teal palette = sns.light_palette(\\"teal\\", n_colors=10) # Create a sample dataframe to visualize the palette df = pd.DataFrame({ \'x\': range(10), \'y\': np.random.rand(10) }) # Use the palette to create a plot sns.set(style=\\"whitegrid\\") sns.scatterplot(x=\'x\', y=\'y\', hue=\'x\', palette=palette, data=df) plt.title(\\"Sequential palette from Light Gray to Teal\\") plt.show() def create_sequential_hex_palette(): # Create a sequential color palette from light gray to hex color #FFA07A palette = sns.light_palette(\\"#FFA07A\\", n_colors=10) # Create a sample dataframe data = np.random.rand(10, 10) # Create a heatmap using the palette sns.heatmap(data, cmap=sns.color_palette(palette)) plt.title(\\"Heatmap with Palette from Light Gray to #FFA07A\\") plt.show() # Function to create sequential color palette using HUSL def create_husl_palette(): # Create a sequential color palette using HUSL with coordinates (50, 70, 40) palette = sns.light_palette((50, 70, 40), input=\\"husl\\", n_colors=10) # Create a sample dataframe for scatter plot df = pd.DataFrame({ \'x\': np.random.rand(50), \'y\': np.random.rand(50) }) # Plot a scatter plot using the palette sns.scatterplot(x=\'x\', y=\'y\', palette=palette, data=df) plt.title(\\"Scatter Plot with HUSL Palette (50, 70, 40)\\") plt.show() # Function to create continuous color map def create_continuous_cmap(): # Create a continuous colormap from light gray to #BADA55 cmap = sns.light_palette(\\"#BADA55\\", as_cmap=True) # Create a sample dataset data = np.random.randn(1000) # Plot a distribution plot using the colormap sns.displot(data, kind=\\"kde\\", fill=True, cmap=cmap) plt.title(\\"Distribution plot with continuous colormap #BADA55\\") plt.show() # Function calls create_sequential_teal_palette() create_sequential_hex_palette() create_husl_palette() create_continuous_cmap()"},{"question":"# Question: Advanced Cross-Validation Assessment You are provided with the historical sales dataset of a retail store. The dataset represents weekly sales data over several years and has the following columns: - `WeekOfYear` (int): The week number in a year (1 to 52/53). - `Year` (int): The year the sales data corresponds to. - `Store` (int): A unique identifier for the store. - `Sales` (float): The total sales recorded in that week. Your task is to implement a robust cross-validation strategy to evaluate a regression model that predicts weekly sales based on historical patterns. Implementation Details: 1. **Load the Dataset:** - Use an appropriate method to load the dataset into a pandas DataFrame. 2. **Data Preparation:** - Extract the features (`WeekOfYear`, `Year`, `Store`) and the target variable (`Sales`). - Standardize the features using `StandardScaler`. 3. **Cross-Validation Strategy:** - Implement three different cross-validation strategies: a. **KFold Cross-Validation** b. **TimeSeriesSplit Cross-Validation** c. **GroupKFold Cross-Validation** (using `Store` as the group identifier) 4. **Model Training and Evaluation:** - Use a linear regression model from scikit-learn. - Compute and print the cross-validated R-squared scores for each cross-validation strategy. 5. **Performance Comparison:** - Compare the outputs of the three cross-validation methods and discuss which method is more suitable for this dataset and why. Constraints: - Assume the dataset is loaded correctly from a CSV file named `sales_data.csv`. - Use relevant imports from `sklearn`, `pandas`, and `numpy`. - Ensure your code is modular and reusable with appropriate function definitions. Expected Input and Output: **Input:** None (Assume the file `sales_data.csv` exists in the current directory) **Output:** - Cross-validated R-squared scores for KFold. - Cross-validated R-squared scores for TimeSeriesSplit. - Cross-validated R-squared scores for GroupKFold. - A brief discussion on the most appropriate cross-validation strategy. Example Output: ``` KFold R-squared scores: [0.80, 0.82, 0.78, 0.85, 0.81] TimeSeriesSplit R-squared scores: [0.76, 0.78, 0.79, 0.80, 0.77] GroupKFold R-squared scores: [0.79, 0.81, 0.78, 0.84, 0.80] The most appropriate cross-validation strategy for this dataset is TimeSeriesSplit because it respects the temporal nature of the data and ensures that the model is evaluated on future data points, which is critical for time series forecasting tasks. ```","solution":"import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.preprocessing import StandardScaler from sklearn.model_selection import KFold, TimeSeriesSplit, GroupKFold, cross_val_score import numpy as np def load_dataset(filepath): Load the dataset from a CSV file. return pd.read_csv(filepath) def prepare_data(df): Extract features and target variable from the dataset. Standardize the features. X = df[[\'WeekOfYear\', \'Year\', \'Store\']] y = df[\'Sales\'] scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled, y def evaluate_model(model, X, y, cv_strategy): Evaluate the model using a given cross-validation strategy. Return the cross-validated R-squared scores. scores = cross_val_score(model, X, y, cv=cv_strategy, scoring=\'r2\') return scores def main(): # Load the dataset df = load_dataset(\'sales_data.csv\') # Prepare the data X, y = prepare_data(df) # Initialize the model model = LinearRegression() # Initialize Cross-Validation strategies kf = KFold(n_splits=5, shuffle=True, random_state=42) tscv = TimeSeriesSplit(n_splits=5) gkf = GroupKFold(n_splits=5) # Evaluate the model using different CV strategies kf_scores = evaluate_model(model, X, y, kf) tscv_scores = evaluate_model(model, X, y, tscv) gkf_scores = evaluate_model(model, X, y, gkf.split(X, y, groups=df[\'Store\'])) print(f\\"KFold R-squared scores: {kf_scores}\\") print(f\\"TimeSeriesSplit R-squared scores: {tscv_scores}\\") print(f\\"GroupKFold R-squared scores: {gkf_scores}\\") print() # Discuss the most appropriate cross-validation strategy print(\\"The most appropriate cross-validation strategy for this dataset is TimeSeriesSplit because it respects the temporal nature of the data and ensures that the model is evaluated on future data points, which is critical for time series forecasting tasks.\\") # Run the main function if __name__ == \\"__main__\\": main()"},{"question":"# Advanced File Operations in Python In this assessment, you will implement a class to handle file operations in a manner analogous to some of the C API functions but using Python\'s `io` module. This class, `AdvancedFileHandler`, should encapsulate file handling operations and error checking, providing methods similar to those described in the documentation. Class Specification: **`AdvancedFileHandler`** 1. **`__init__(self, file_path, mode=\'r\', encoding=None, newline=None)`** - Initializes the file handler with the given file path, mode, encoding, and newline parameters. - Raises appropriate exceptions if the file cannot be opened. 2. **`read_line(self, max_bytes=-1)`** - Reads a line from the file. - If `max_bytes` is `0`, exactly one line is read regardless of length. - If `max_bytes` is greater than `0`, no more than `max_bytes` bytes will be read from the file. - If `max_bytes` is negative (default), one line is read regardless of length. - Returns the read line or raises EOFError if the end of the file is reached. 3. **`write_object(self, obj, raw=False)`** - Writes `obj` to the file. - If `raw` is `True`, the `str()` of the object is written instead of the `repr()`. - Returns `None` on success or raises an appropriate exception on failure. 4. **`close(self)`** - Closes the file. Constraints: - Implement appropriate error handling. - Consider performance when handling large files. Example Usage: ```python handler = AdvancedFileHandler(\'example.txt\', \'w\') handler.write_object(\\"Hello, World!\\") handler.close() handler = AdvancedFileHandler(\'example.txt\', \'r\') print(handler.read_line()) handler.close() ``` Expected Input and Output - Input: - File path: String representing the path to the file. - Mode: String representing the mode in which the file should be opened (\'r\', \'w\', etc.). - Encoding: Optional string representing the file encoding. - Newline: Optional string representing newline handling. - max_bytes: Integer specifying maximum number of bytes to read from a file line. - obj: Any Python object to be written to the file. - raw: Boolean indicating whether the raw string of the object should be written. - Output: - For `read_line`: A string representing the read line. - For `write_object`: None (on success) or raises an exception (on failure). - For `close`: None. Implement the `AdvancedFileHandler` class according to the specification above.","solution":"import io class AdvancedFileHandler: def __init__(self, file_path, mode=\'r\', encoding=None, newline=None): try: self.file = open(file_path, mode, encoding=encoding, newline=newline) except Exception as e: raise IOError(f\\"Error opening file: {e}\\") def read_line(self, max_bytes=-1): if max_bytes == 0: return self.file.readline() elif max_bytes > 0: return self.file.read(max_bytes) else: line = self.file.readline() if line == \'\': raise EOFError(\\"End of file reached.\\") return line def write_object(self, obj, raw=False): try: if raw: self.file.write(str(obj)) else: self.file.write(repr(obj)) except Exception as e: raise IOError(f\\"Error writing to file: {e}\\") def close(self): self.file.close()"},{"question":"# PyTorch Optimization Check You are tasked with implementing a function that checks if the persistent algorithm optimization can be applied based on certain conditions described in PyTorch documentation. The function should be able to take a tensor and determine if all conditions for selecting the persistent algorithm are satisfied. # Function Signature ```python def can_use_persistent_algorithm(tensor: torch.Tensor, gpu_architecture: str, cudnn_enabled: bool) -> bool: pass ``` # Input - `tensor`: A PyTorch tensor - `gpu_architecture`: A string representing the GPU architecture (e.g., `\\"V100\\"`) - `cudnn_enabled`: A boolean flag indicating whether cudnn is enabled # Output - A boolean indicating whether the persistent algorithm optimization can be used. # Constraints - The `tensor` will be guaranteed to be a PyTorch tensor. - `gpu_architecture` will be a non-empty string. - The system check for GPU and cudnn status should be simulated within the function. # Example ```python import torch tensor = torch.randn((10, 10), dtype=torch.float16).cuda() gpu_architecture = \\"V100\\" cudnn_enabled = True assert can_use_persistent_algorithm(tensor, gpu_architecture, cudnn_enabled) == True tensor_float32 = torch.randn((10, 10), dtype=torch.float32).cuda() assert can_use_persistent_algorithm(tensor_float32, gpu_architecture, cudnn_enabled) == False tensor_cpu = torch.randn((10, 10), dtype=torch.float16) assert can_use_persistent_algorithm(tensor_cpu, gpu_architecture, cudnn_enabled) == False ``` # Performance Requirements - The function should perform all necessary checks efficiently. # Notes 1. The function is expected to handle the checking of input conditions, including data type, GPU usage, and correct GPU architecture. 2. The `PackedSequence` check can be assumed to be satisfied or can be extended based on additional domain-specific knowledge.","solution":"import torch def can_use_persistent_algorithm(tensor: torch.Tensor, gpu_architecture: str, cudnn_enabled: bool) -> bool: Check if the persistent algorithm optimization can be applied. Args: tensor (torch.Tensor): A PyTorch tensor. gpu_architecture (str): A string representing the GPU architecture (e.g., \\"V100\\"). cudnn_enabled (bool): A boolean flag indicating whether cudnn is enabled. Returns: bool: True if persistent algorithm optimization can be used, else False. if not tensor.is_cuda: return False if tensor.dtype not in [torch.float16, torch.bfloat16]: return False supported_architectures = [\\"V100\\", \\"A100\\", \\"T4\\"] if gpu_architecture not in supported_architectures: return False if not cudnn_enabled: return False return True"},{"question":"**Objective:** Demonstrate your understanding and ability to use the `configparser` module in Python to handle configuration files. **Problem Statement:** You are given a configuration file `settings.ini` with the following content: ``` [DEFAULT] PathToLogs = /var/logs Backup = true BackupFrequency = 30 [database] host = localhost port = 5432 user = admin password = admin [webserver] host = 192.168.1.1 port = 8080 document_root = %(PathToLogs)s/web ``` You need to write a Python script that: 1. Reads the configuration from `settings.ini` file. 2. Adds a new section `[email]` with the following options and values: - `smtp_server` = `smtp.example.com` - `smtp_port` = `587` - `use_tls` = `yes` 3. Modifies the `BackupFrequency` in the `DEFAULT` section to `15`. 4. Retrieves and prints the following values: - The host and port of the `webserver`. - The `use_tls` value in the new `email` section, converted to a boolean. - The document root of the `webserver`, ensuring that any interpolations are resolved. 5. Saves the updated configuration to a new file called `updated_settings.ini`. **Constraints:** - Ensure the script handles interpolation correctly. - Utilize appropriate `configparser` methods for reading, writing, and modifying the configuration. **Input:** - A file named `settings.ini` with the content provided above. **Output:** - Print statements for each required retrieved value. - A new file named `updated_settings.ini` with the updated configuration. **Example:** ```python # Expected output Webserver host: 192.168.1.1 Webserver port: 8080 Use TLS: True Document root: /var/logs/web # Contents of \'updated_settings.ini\' after execution [DEFAULT] PathToLogs = /var/logs Backup = true BackupFrequency = 15 [database] host = localhost port = 5432 user = admin password = admin [webserver] host = 192.168.1.1 port = 8080 document_root = /var/logs/web [email] smtp_server = smtp.example.com smtp_port = 587 use_tls = yes ``` **Code Implementation:** Write your implementation in the markdown code block below. ```python import configparser # Write your solution here ```","solution":"import configparser def manage_config(file_path, new_file_path): # Read the configuration file config = configparser.ConfigParser() config.read(file_path) # Add a new section [email] with the specified options and values config.add_section(\'email\') config.set(\'email\', \'smtp_server\', \'smtp.example.com\') config.set(\'email\', \'smtp_port\', \'587\') config.set(\'email\', \'use_tls\', \'yes\') # Modify the BackupFrequency in DEFAULT section config.set(\'DEFAULT\', \'BackupFrequency\', \'15\') # Retrieve and print the specified values webserver_host = config.get(\'webserver\', \'host\') webserver_port = config.get(\'webserver\', \'port\') use_tls = config.getboolean(\'email\', \'use_tls\') document_root = config.get(\'webserver\', \'document_root\') print(f\\"Webserver host: {webserver_host}\\") print(f\\"Webserver port: {webserver_port}\\") print(f\\"Use TLS: {use_tls}\\") print(f\\"Document root: {document_root}\\") # Save the updated configuration to a new file with open(new_file_path, \'w\') as configfile: config.write(configfile)"},{"question":"# Question: Implementing a Custom Import Module Function You are tasked with creating a function that imports a module using Python\'s C API, both handling errors gracefully and reloading the module if it is already imported. You need to write this in Python, using the `ctypes` library to interface with the Python C API. Here are the specific requirements: 1. **Function Name**: `custom_import_module` 2. **Input**: - `module_name` (string): The name of the module to be imported. - `reload` (boolean): If `True`, the module should be reloaded if it is already imported. 3. **Output**: The function should return the imported (or reloaded) module object. 4. **Constraints**: - If the module cannot be imported, return `None` and print an error message. - Use the `ctypes` library to call the necessary functions from the Python C API. 5. **Performance Requirements**: Write efficient error handling and ensure the module dictionary is properly managed. # Example Usage: ```python module = custom_import_module(\'os\', reload=False) if module: print(\\"Module imported successfully\\") else: print(\\"Failed to import module\\") ``` # Hints: 1. Use `ctypes` to interact with the Python C API functions like `PyImport_ImportModule`, `PyImport_ReloadModule`, etc. 2. Properly handle reference counting and exceptions in the C API using `ctypes`. # Solution Template: ```python import ctypes import sys def custom_import_module(module_name, reload=False): # Load the Python C API library libpython = ctypes.PyDLL(None) # Define proper types for C API functions we will use libpython.PyImport_ImportModule.restype = ctypes.py_object libpython.PyImport_ReloadModule.restype = ctypes.py_object try: if reload and module_name in sys.modules: module = libpython.PyImport_ReloadModule(sys.modules[module_name]) else: module = libpython.PyImport_ImportModule(module_name.encode(\'utf-8\')) if module is None: raise ImportError(f\\"Failed to import module {module_name}\\") return module except Exception as e: print(f\\"Error: {e}\\") return None # Example usage module = custom_import_module(\'os\', reload=False) if module: print(\\"Module imported successfully\\") else: print(\\"Failed to import module\\") ```","solution":"import ctypes import sys def custom_import_module(module_name, reload=False): # Load the Python C API library libpython = ctypes.PyDLL(None) # Define proper types for C API functions we will use libpython.PyImport_ImportModule.restype = ctypes.py_object libpython.PyImport_ImportModule.argtypes = [ctypes.c_char_p] libpython.PyImport_ReloadModule.restype = ctypes.py_object libpython.PyImport_ReloadModule.argtypes = [ctypes.py_object] try: if reload and module_name in sys.modules: module = libpython.PyImport_ReloadModule(sys.modules[module_name]) else: module = libpython.PyImport_ImportModule(module_name.encode(\'utf-8\')) if not module: raise ImportError(f\\"Failed to import module {module_name}\\") return module except Exception as e: print(f\\"Error: {e}\\") return None"},{"question":"# Pandas Coding Assessment Question **Problem Statement:** You are given two datasets represented as pandas DataFrames: 1. `students`: Contains information about students, - `student_id`: Unique identifier for each student. - `name`: Name of the student. - `age`: Age of the student. - `gender`: Gender of the student. 2. `scores`: Contains exam scores for various subjects, - `student_id`: Corresponds to the student identifier. - `subject`: Subject name. - `score`: Score obtained in the subject. You are required to: 1. Merge the `students` and `scores` DataFrames to create a consolidated DataFrame that includes student information along with their scores. 2. Compute the average score for each student and add it as a new column `average_score`. 3. Create a pivot table that shows the average score for each subject, categorized by gender. 4. Generate dummy variables for the `gender` column in the `students` DataFrame. 5. Use the dummy variables to calculate the correlation between student age and their average score. **Function Signature:** ```python import pandas as pd def analyze_student_data(students: pd.DataFrame, scores: pd.DataFrame) -> pd.DataFrame: This function takes in two pandas DataFrames - \'students\' and \'scores\' - and performs various data manipulation tasks to generate the required outputs. Parameters: students (pd.DataFrame): DataFrame containing student information. scores (pd.DataFrame): DataFrame containing exam scores for various subjects. Returns: pd.DataFrame: DataFrame containing correlation between student age and their average score. pass ``` **Expected Input Format:** - `students` DataFrame: ``` student_id name age gender 0 1 John 15 M 1 2 Alice 14 F 2 3 Bob 15 M ``` - `scores` DataFrame: ``` student_id subject score 0 1 Math 75 1 2 Math 85 2 3 Science 95 ``` **Expected Output:** - A DataFrame showing the correlation between student age and their average score. **Constraints:** 1. The `students` DataFrame can have up to `1,000` records. 2. The `scores` DataFrame can have up to `10,000` records. 3. Ensure your implementation is efficient with respect to both time and space complexity. **Instructions:** 1. Implement the function `analyze_student_data` as specified in the function signature. 2. Ensure to handle potential edge cases, such as missing scores for some students. 3. Use appropriate pandas functions to achieve the desired manipulations.","solution":"import pandas as pd def analyze_student_data(students: pd.DataFrame, scores: pd.DataFrame) -> pd.DataFrame: # 1. Merge the students and scores DataFrames merged_df = pd.merge(students, scores, on=\'student_id\') # 2. Compute the average score for each student and add it as a new column \'average_score\' avg_scores = merged_df.groupby(\'student_id\')[\'score\'].mean().reset_index() avg_scores = avg_scores.rename(columns={\'score\': \'average_score\'}) merged_df = pd.merge(merged_df, avg_scores, on=\'student_id\') # 3. Create a pivot table that shows the average score for each subject, categorized by gender pivot_table = merged_df.pivot_table(values=\'score\', index=\'subject\', columns=\'gender\', aggfunc=\'mean\') # 4. Generate dummy variables for the \'gender\' column in the students DataFrame students_with_dummies = pd.concat([students, pd.get_dummies(students[\'gender\'], prefix=\'gender\')], axis=1) # 5. Merge avg_scores with students_with_dummies on student_id final_df = pd.merge(students_with_dummies, avg_scores, on=\'student_id\') # Calculate the correlation between student age and their average score correlation = final_df[[\'age\', \'average_score\']].corr() return correlation"},{"question":"You are provided with a utility library `python310` that boasts several system-related capabilities. Your task is to implement a function that utilizes this library to perform a set of operations on system processes. # Function Signature ```python def analyze_processes(data: str) -> List[Dict[str, Any]]: ``` # Input - `data` (str): A string in JSON format that contains an array of process details. Each process is represented by a dictionary with at least the following keys: - `pid` (int): The process id. - `name` (str): The process name. - `memory` (int): The memory consumption of the process in MB. - `cpu` (float): The CPU utilization of the process in percentage. Example: ```json \'[ {\\"pid\\": 101, \\"name\\": \\"ProcessA\\", \\"memory\\": 120, \\"cpu\\": 15.5}, {\\"pid\\": 102, \\"name\\": \\"ProcessB\\", \\"memory\\": 130, \\"cpu\\": 10.0} ]\' ``` # Output - A list of dictionaries, where each dictionary represents a process and contains the following keys after applying the necessary operations: - `pid` (int): The process id. - `name` (str): The process name. - `memory_gb` (float): The memory consumption of the process converted to GB. - `high_cpu_usage` (bool): A boolean indicating whether the process\'s CPU usage is above 50%. - `terminated` (bool): A boolean indicating whether the process should be terminated (True if CPU usage is above 50%). # Constraints - You may assume that the input string is properly formatted JSON. - You must use appropriate utility functions from the `python310` library to handle process control operations where applicable. # Detailed Requirements 1. Parse the input JSON string to extract the list of process details. 2. Convert the memory consumption from MB to GB. 3. Determine if each process has a high CPU usage (greater than 50%). 4. Indicate whether each process would be terminated based on the CPU usage. 5. Return the updated list of process details including the new information. # Example ```python data = \'[ {\\"pid\\": 101, \\"name\\": \\"ProcessA\\", \\"memory\\": 120, \\"cpu\\": 15.5}, {\\"pid\\": 102, \\"name\\": \\"ProcessB\\", \\"memory\\": 130, \\"cpu\\": 60.0} ]\' output = analyze_processes(data) print(output) # Expected output [ {\\"pid\\": 101, \\"name\\": \\"ProcessA\\", \\"memory_gb\\": 0.117, \\"high_cpu_usage\\": False, \\"terminated\\": False}, {\\"pid\\": 102, \\"name\\": \\"ProcessB\\", \\"memory_gb\\": 0.127, \\"high_cpu_usage\\": True, \\"terminated\\": True} ] ``` Hint: You might find the following utility functions from the `python310` library useful: - Parsing arguments and building values functions. - System functions for process control.","solution":"import json from typing import List, Dict, Any def analyze_processes(data: str) -> List[Dict[str, Any]]: Analyzes process data and returns a list of processes with additional details. Args: - data (str): A string containing JSON formatted array of process details. Returns: - List[Dict[str, Any]]: A list of dictionaries with process details. # Convert MB to GB constant MB_TO_GB = 1 / 1024 # Parse the input JSON string process_list = json.loads(data) result = [] for process in process_list: process_details = { \'pid\': process[\'pid\'], \'name\': process[\'name\'], \'memory_gb\': round(process[\'memory\'] * MB_TO_GB, 3), \'high_cpu_usage\': process[\'cpu\'] > 50, \'terminated\': process[\'cpu\'] > 50 } result.append(process_details) return result"},{"question":"# Asynchronous Programming with asyncio Problem Statement You are tasked with implementing an asynchronous program that simulates tasks performed by a group of workers. Each worker performs several asynchronous operations, some of which may involve blocking I/O and CPU-bound tasks. Implement the following functionalities to meet the required specifications. Implement a function `async def worker(id: int) -> None` that will simulate a worker performing tasks. The worker should perform the following actions: 1. Print `\\"Worker {id} started\\"`. 2. Perform an asynchronous I/O-bound task that simulates reading from a file and takes approximately `2` seconds. 3. Perform a CPU-bound task that takes approximately `3` seconds. 4. Print `\\"Worker {id} completed I/O-bound task.\\"` after the I/O-bound task. 5. Print `\\"Worker {id} completed CPU-bound task.\\"` after the CPU-bound task. 6. Run these tasks using the `asyncio` framework such that no worker blocks the event loop when performing its tasks. 7. Handle any potential exceptions that may occur during the execution of the tasks, and log these exceptions appropriately. Additionally, implement a function `async def main()` which will: 1. Start a specified number of workers concurrently (e.g., 5 workers), each identified by its unique `id`. 2. Ensure that the event loop runs in debug mode to catch any potential issues. 3. Ensure all workers run and complete their tasks. Input - The main function does not take any inputs from the user. Output - The output should be the print statements indicating the progress of each worker. Additional Requirements 1. All logging should be set to DEBUG level. 2. Ensure the program runs without blocking the main event loop. 3. Use an executor to handle CPU-bound tasks. 4. Use the appropriate asyncio methods and practices to avoid common pitfalls as documented. Example Output ``` Worker 1 started Worker 2 started Worker 3 started Worker 4 started Worker 5 started Worker 1 completed I/O-bound task. Worker 2 completed I/O-bound task. Worker 3 completed I/O-bound task. Worker 4 completed I/O-bound task. Worker 5 completed I/O-bound task. Worker 1 completed CPU-bound task. Worker 2 completed CPU-bound task. Worker 3 completed CPU-bound task. Worker 4 completed CPU-bound task. Worker 5 completed CPU-bound task. ``` **Note**: The exact order of the output may vary due to the asynchronous nature of the program.","solution":"import asyncio import logging from concurrent.futures import ThreadPoolExecutor import time logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) async def io_bound_task(id): await asyncio.sleep(2) # Simulating an I/O-bound task, e.g., reading from a file logger.debug(f\\"Worker {id} completed I/O-bound task.\\") def cpu_bound_task(id): time.sleep(3) # Simulating a CPU-bound task logger.debug(f\\"Worker {id} completed CPU-bound task.\\") async def worker(id: int) -> None: logger.debug(f\\"Worker {id} started\\") try: await io_bound_task(id) loop = asyncio.get_running_loop() with ThreadPoolExecutor() as pool: await loop.run_in_executor(pool, cpu_bound_task, id) except Exception as e: logger.error(f\\"Worker {id} encountered an error: {e}\\") async def main() -> None: workers = [worker(i) for i in range(1, 6)] await asyncio.gather(*workers) if __name__ == \'__main__\': asyncio.run(main())"},{"question":"Background: In machine learning, we often need to create, manipulate, and perform operations on tensors. Moreover, calculating gradients is essential for optimization in neural networks. Objective: Write a function that simulates a simple linear regression model using PyTorch. The function will create tensors, perform tensor operations, and compute gradients. Requirements: 1. Create a tensor `X` of shape (100, 1) with values from a normal distribution, using `torch.randn`. 2. Create a tensor `Y` using the equation `Y = 3 * X + 2`, and add some noise using `torch.randn` to simulate data. 3. Initialize weights `W` and bias `b` as tensors with requires_grad=True. 4. Implement a simple linear regression model `Y_pred = W * X + b`. 5. Compute the Mean Squared Error (MSE) loss between `Y_pred` and `Y`. 6. Use the `backward` method to compute gradients. 7. Update the weights `W` and bias `b` using a learning rate of 0.01. 8. Return the updated weights and bias after one iteration. Function Signature: ```python import torch def simple_linear_regression(): # 1. Create tensors X and Y X = torch.randn(100, 1) Y = 3 * X + 2 + torch.randn(100, 1) # 2. Initialize weights W and bias b W = torch.tensor([0.0], requires_grad=True) b = torch.tensor([0.0], requires_grad=True) # 3. Simple linear regression model Y_pred = W * X + b # 4. Compute MSE loss loss = torch.mean((Y_pred - Y) ** 2) # 5. Compute gradients loss.backward() # 6. Update weights and bias learning_rate = 0.01 with torch.no_grad(): W -= learning_rate * W.grad b -= learning_rate * b.grad # Clear the gradients W.grad.zero_() b.grad.zero_() return W.item(), b.item() ``` # Constraints & Notes: - Use only PyTorch for tensor operations. - Ensure the function is clear, correct, and adheres to the requirements. - Performance is not the primary concern, so optimizations are not required. Expected Output: The function should return the updated weight `W` and bias `b` as floats after one gradient descent step. Prepare and submit your solution by implementing the function in the provided signature. Your function will be tested with different random seeds to ensure correctness and robustness.","solution":"import torch def simple_linear_regression(): # 1. Create tensors X and Y X = torch.randn(100, 1) Y = 3 * X + 2 + torch.randn(100, 1) # 2. Initialize weights W and bias b W = torch.tensor([0.0], requires_grad=True) b = torch.tensor([0.0], requires_grad=True) # 3. Simple linear regression model Y_pred = W * X + b # 4. Compute MSE loss loss = torch.mean((Y_pred - Y) ** 2) # 5. Compute gradients loss.backward() # 6. Update weights and bias learning_rate = 0.01 with torch.no_grad(): W -= learning_rate * W.grad b -= learning_rate * b.grad # Clear the gradients W.grad.zero_() b.grad.zero_() return W.item(), b.item()"},{"question":"# Python 310 Coding Assessment Question: You are tasked with developing a custom data structure that leverages Python\'s built-in constants to manage state and enforce immutability rules. Your objective is to create a `ConstantManager` class, which embodies the following specifications: 1. The class should accept an arbitrary list of values upon initialization. 2. It should provide capabilities to: - Retrieve an individual value (index lookup) and default to `None` if out of bounds. - Append a new value to the list, except for values `None`, `True`, `False`, `Ellipsis`, and `NotImplemented`. - Provide a boolean evaluation method `evaluate_value(value)` to explicitly return: - `True` for `True`. - `False` for `False`, `None`, and `NotImplemented`. - Raise a `ValueError` for values that do not match the above criteria. 3. Ensure that the class cannot accidentally overwrite its internal list by triggering a `SyntaxError` on attempts to reassign the list directly. Here is a template to get you started: ```python class ConstantManager: def __init__(self, values): self._values = values def get_value(self, index): Retrieves the value at the given index, returns None if index is out of bounds. try: return self._values[index] except IndexError: return None def append_value(self, value): Appends a value to the list, with the restriction that None, True, False, Ellipsis, and NotImplemented cannot be added. if value in [None, True, False, Ellipsis, NotImplemented]: raise ValueError(f\\"Cannot append {value}.\\") self._values.append(value) def evaluate_value(self, value): Evaluates the value against specific criteria: - Returns True for True. - Returns False for False, None, and NotImplemented. - Raises ValueError for any other value. if value is True: return True elif value in [False, None, NotImplemented]: return False else: raise ValueError(f\\"Unhandled value: {value}\\") # Example usage: cm = ConstantManager([1, 2, 3, None]) print(cm.get_value(2)) # Output: 3 print(cm.get_value(4)) # Output: None cm.append_value(5) # Works try: cm.append_value(None) # Raises ValueError except ValueError as ve: print(ve) print(cm.evaluate_value(True)) # Output: True print(cm.evaluate_value(False)) # Output: False try: cm._values = [10, 20] # Should raise a SyntaxError except SyntaxError as se: print(se) ``` Constraints: - Do not use any modules or functions that have not been explicitly defined within the class. - You must ensure immutability for specific constants and handle attempts at reassignment gracefully. - Your solution must work efficiently for typical list sizes (up to 100 elements). Tips: - Carefully consider Python\'s exception handling features. - Remember to leverage the documentation for accurate implementation of described behaviors. - Write additional tests to verify the robustness of your implementation. Good luck!","solution":"class ConstantManager: def __init__(self, values): self._values = values def get_value(self, index): Retrieves the value at the given index, returns None if index is out of bounds. try: return self._values[index] except IndexError: return None def append_value(self, value): Appends a value to the list, with the restriction that None, True, False, Ellipsis, and NotImplemented cannot be added. if value in [None, True, False, Ellipsis, NotImplemented]: raise ValueError(f\\"Cannot append {value}.\\") self._values.append(value) def evaluate_value(self, value): Evaluates the value against specific criteria: - Returns True for True. - Returns False for False, None, and NotImplemented. - Raises ValueError for any other value. if value is True: return True elif value in [False, None, NotImplemented]: return False else: raise ValueError(f\\"Unhandled value: {value}\\") def __setattr__(self, name, value): if name == \'_values\' and hasattr(self, \'_values\'): raise SyntaxError(\\"Reassignment of internal list \'_values\' is not allowed.\\") super().__setattr__(name, value)"},{"question":"Asyncio Task Management You are required to implement a function in Python that creates and manages multiple asynchronous tasks using the asyncio library. The goal is to simulate a scenario where multiple tasks are executed concurrently, and some tasks may face timeouts or cancellations. # Function Signature ```python import asyncio from typing import List, Tuple async def task_manager(task_delays: List[int], timeout: int) -> Tuple[List[str], List[str]]: This function takes a list of task delays and a timeout value, creates and manages tasks based on these parameters. Args: - task_delays (List[int]): A list of integers where each integer represents the delay time in seconds for a task. - timeout (int): The maximum amount of time (in seconds) to wait for each task to complete. Returns: - result (Tuple[List[str], List[str]]): A tuple containing two lists: - The first list contains the results of the tasks that completed successfully. - The second list contains messages for tasks that were cancelled or timed out. pass ``` # Detailed Requirements 1. **Task Creation**: - Create an asynchronous function `task_function` that takes an integer delay and returns a string message `\\"Task with delay X completed\\"` where `X` is the delay time. - Create and schedule a task for each delay given in the `task_delays` list using `asyncio.create_task()`. 2. **Concurrency Management**: - Use `asyncio.gather()` to run the created tasks concurrently. - Ensure that if a task does not complete within the specified `timeout`, it is cancelled. - Capture the results of each task. If a task completes successfully, its result should be appended to the first list in the returned tuple. - If a task is cancelled or times out, append an appropriate message (e.g., `\\"Task with delay X cancelled\\"`) to the second list in the returned tuple. 3. **Timeout Handling**: - Use `asyncio.wait_for()` to enforce the timeout for each task. # Example Usage ```python import asyncio async def task_manager(task_delays, timeout): async def task_function(delay): await asyncio.sleep(delay) return f\\"Task with delay {delay} completed\\" tasks = [asyncio.create_task(task_function(delay)) for delay in task_delays] completed, cancelled = [], [] for task in tasks: try: result = await asyncio.wait_for(task, timeout) completed.append(result) except asyncio.TimeoutError: cancelled.append(f\\"Task with delay {task.get_name()} timed out\\") except asyncio.CancelledError: cancelled.append(f\\"Task with delay {task.get_name()} cancelled\\") return completed, cancelled # Example: task_delays = [2, 4, 6] timeout = 3 result = asyncio.run(task_manager(task_delays, timeout)) print(result) # Expected output: ([\'Task with delay 2 completed\'], [\'Task with delay 4 timed out\', \'Task with delay 6 timed out\']) ``` # Constraints - The function should handle any reasonable length of `task_delays` list. - The code should be compatible with Python 3.7 and newer versions. - Proper exception handling should be implemented to handle task cancellations and timeouts. This exercise tests your understanding of asynchronous programming in Python using the asyncio library, specifically in creating, running, and managing tasks concurrently with timeouts.","solution":"import asyncio from typing import List, Tuple async def task_manager(task_delays: List[int], timeout: int) -> Tuple[List[str], List[str]]: This function takes a list of task delays and a timeout value, creates and manages tasks based on these parameters. Args: - task_delays (List[int]): A list of integers where each integer represents the delay time in seconds for a task. - timeout (int): The maximum amount of time (in seconds) to wait for each task to complete. Returns: - result (Tuple[List[str], List[str]]): A tuple containing two lists: - The first list contains the results of the tasks that completed successfully. - The second list contains messages for tasks that were cancelled or timed out. async def task_function(delay): await asyncio.sleep(delay) return f\\"Task with delay {delay} completed\\" tasks = [asyncio.create_task(task_function(delay)) for delay in task_delays] completed, cancelled = [], [] for delay, task in zip(task_delays, tasks): try: result = await asyncio.wait_for(task, timeout) completed.append(result) except asyncio.TimeoutError: cancelled.append(f\\"Task with delay {delay} timed out\\") except asyncio.CancelledError: cancelled.append(f\\"Task with delay {delay} cancelled\\") return completed, cancelled"},{"question":"Objective: Implement a custom pretty-printer for nested data structures, similar to the functionality provided by Pythonâ€™s `pprint` module. Your custom pretty-printer should follow certain guidelines and formatting rules. Task: Write a class `CustomPrettyPrinter` that formats nested lists and dictionaries into a human-readable string. Your class should support customizing the indentation and the maximum width of the output. Specifications: 1. **Class Definition:** ```python class CustomPrettyPrinter: def __init__(self, indent=1, width=80): pass def pformat(self, obj): pass ``` 2. **Constructor Parameters:** - `indent` (default 1): Specifies the amount of indentation for each nested level. - `width` (default 80): Specifies the maximum width of the output. 3. **Method:** - `pformat(self, obj)`: Accepts any arbitrary Python data structure (lists, dictionaries) and returns a string with the formatted representation. 4. **Formatting Rules:** - The formatted structure should include indentation for nested lists or dictionaries. - Ensure that lines do not exceed the specified width unless it is impossible (e.g., long strings or dictionary keys). - For lists, each item should be on a new line with the appropriate indentation. - For dictionaries, each key-value pair should be on a new line, and keys and values should be separated by a colon and a space `\\": \\"`. Example: ```python # Example usage printer = CustomPrettyPrinter(indent=2, width=40) nested_data = { \'a\': 1, \'b\': [1, 2, {\'c\': 3, \'d\': 4}], \'e\': \\"This is a long string that should wrap.\\" } formatted_output = printer.pformat(nested_data) print(formatted_output) ``` Expected Output: ```plaintext { \'a\': 1, \'b\': [ 1, 2, { \'c\': 3, \'d\': 4 } ], \'e\': \\"This is a long string that should wrap.\\" } ``` Constraints: - You can assume that the input data structure will not exceed a depth of 10. - Dictionary keys and values will be printable strings or nested lists/dictionaries. Performance Requirements: - Optimize for readability of the output. - The implementation should handle moderately large nested structures efficiently. Hints: - You might find it useful to implement a helper function to handle nested levels. - Consider edge cases such as empty lists or dictionaries and very long strings. Your task is to complete the `CustomPrettyPrinter` class to meet these specifications and pass the provided example scenario.","solution":"class CustomPrettyPrinter: def __init__(self, indent=1, width=80): self.indent = indent self.width = width def pformat(self, obj, _level=0): space = \' \' * (self.indent * _level) out_str = \\"\\" if isinstance(obj, dict): out_str += \\"{n\\" for i, (key, value) in enumerate(obj.items()): out_str += space + \' \' * self.indent + repr(key) + \\": \\" + self.pformat(value, _level + 1) if i < len(obj) - 1: out_str += \\",\\" out_str += \\"n\\" out_str += space + \\"}\\" elif isinstance(obj, list): out_str += \\"[n\\" for i, item in enumerate(obj): out_str += space + \' \' * self.indent + self.pformat(item, _level + 1) if i < len(obj) - 1: out_str += \\",\\" out_str += \\"n\\" out_str += space + \\"]\\" else: out_str += repr(obj) return out_str"},{"question":"# ASCII Character Operations You are tasked with implementing a function that processes a string to analyze and transform its ASCII characters based on specific criteria. Problem Write a function `process_ascii_string(s: str) -> dict` that takes a single string `s` as input and outputs a dictionary with the following keys and corresponding values: 1. **\\"alphanumeric_count\\"**: The count of alphanumeric characters in the input string. 2. **\\"control_count\\"**: The count of control characters in the input string. 3. **\\"transformed_string\\"**: A new string where: - All control characters are replaced with their `unctrl` representation. - All lowercase alphabetic characters are converted to uppercase. - Non-alphanumeric and non-control characters remain unchanged. Constraints - The input string `s` can contain any valid ASCII characters. - The output dictionary should have the specified keys with correct data types. Example ```python s = \\"Hellox01World!n\\" result = process_ascii_string(s) print(result) # Output should be: # { # \\"alphanumeric_count\\": 10, # \\"control_count\\": 2, # \\"transformed_string\\": \\"HELLO^A WORLD!^J\\" # } ``` Implementation Details - You should use functionalities provided by the `curses.ascii` module or implement equivalent logic. - You are not allowed to use any other modules outside of the standard library. - Pay attention to the handling of control characters and case conversion. ```python import curses.ascii def process_ascii_string(s: str) -> dict: # Your implementation here pass ``` # Notes - Ensure your function handles all edge cases, such as empty strings or strings with no alphanumeric characters. - Efficiency considerations are secondary here, but aim for clarity and correctness in your solution.","solution":"import curses.ascii def process_ascii_string(s: str) -> dict: alphanumeric_count = 0 control_count = 0 transformed_string = [] for char in s: if curses.ascii.isalnum(char): alphanumeric_count += 1 if curses.ascii.isctrl(char): control_count += 1 transformed_string.append(curses.ascii.unctrl(char)) elif curses.ascii.islower(char): transformed_string.append(char.upper()) else: transformed_string.append(char) return { \\"alphanumeric_count\\": alphanumeric_count, \\"control_count\\": control_count, \\"transformed_string\\": \\"\\".join(transformed_string) }"},{"question":"# BZIP2 Stream Compressor and Decompressor You are required to implement a class `BZIP2Stream` that provides methods for compressing and decompressing data streams using the bzip2 algorithm. The class should handle both one-shot and incremental (de)compression. Class Definition: ```python class BZIP2Stream: def __init__(self, compresslevel=9): Initializes the BZIP2Stream object. Args: compresslevel (int): The level of compression, which ranges from 1 (least compression) to 9 (most compression). Default is 9. pass def compress_data(self, data): Compresses the input data using one-shot compression. Args: data (bytes): The data to be compressed. Returns: bytes: The compressed data. pass def decompress_data(self, data): Decompresses the input data using one-shot decompression. Args: data (bytes): The data to be decompressed. Returns: bytes: The decompressed data. pass def compress_stream(self, data_generator): Compresses data incrementally from a generator of data chunks. Args: data_generator (generator): A generator that yields data chunks to be compressed. Returns: bytes: The compressed data. pass def decompress_stream(self, data_generator): Decompresses data incrementally from a generator of compressed data chunks. Args: data_generator (generator): A generator that yields compressed data chunks. Returns: bytes: The decompressed data. pass ``` Constraints: - Implement all methods, ensuring they handle both small and large datasets efficiently. - For `compress_stream` and `decompress_stream`, the methods should handle the data in chunks, allowing for stream processing. - Handle possible exceptions (e.g., invalid data for decompression) gracefully with appropriate error messages. Example Usage: ```python # Initialize the BZIP2Stream object bz2_stream = BZIP2Stream(compresslevel=5) # One-shot compression and decompression data = b\\"Example data for compression\\" compressed_data = bz2_stream.compress_data(data) decompressed_data = bz2_stream.decompress_data(compressed_data) assert data == decompressed_data, \\"One-shot decompression failed\\" # Incremental compression and decompression data_chunks = [b\\"chunk1\\", b\\"chunk2\\", b\\"chunk3\\"] compressed_stream = bz2_stream.compress_stream(iter(data_chunks)) decompressed_stream = bz2_stream.decompress_stream([compressed_stream]) assert b\\"\\".join(data_chunks) == decompressed_stream, \\"Incremental decompression failed\\" ``` Write your solution below:","solution":"import bz2 class BZIP2Stream: def __init__(self, compresslevel=9): Initializes the BZIP2Stream object. Args: compresslevel (int): The level of compression, which ranges from 1 (least compression) to 9 (most compression). Default is 9. self.compresslevel = compresslevel def compress_data(self, data): Compresses the input data using one-shot compression. Args: data (bytes): The data to be compressed. Returns: bytes: The compressed data. compressor = bz2.BZ2Compressor(self.compresslevel) compressed_data = compressor.compress(data) compressed_data += compressor.flush() return compressed_data def decompress_data(self, data): Decompresses the input data using one-shot decompression. Args: data (bytes): The data to be decompressed. Returns: bytes: The decompressed data. decompressor = bz2.BZ2Decompressor() return decompressor.decompress(data) def compress_stream(self, data_generator): Compresses data incrementally from a generator of data chunks. Args: data_generator (generator): A generator that yields data chunks to be compressed. Returns: bytes: The compressed data. compressor = bz2.BZ2Compressor(self.compresslevel) compressed_data = b\\"\\" for chunk in data_generator: compressed_data += compressor.compress(chunk) compressed_data += compressor.flush() return compressed_data def decompress_stream(self, data_generator): Decompresses data incrementally from a generator of compressed data chunks. Args: data_generator (generator): A generator that yields compressed data chunks. Returns: bytes: The decompressed data. decompressor = bz2.BZ2Decompressor() decompressed_data = b\\"\\" for chunk in data_generator: decompressed_data += decompressor.decompress(chunk) return decompressed_data"},{"question":"Coding Assessment Question # Objective You are required to create a Python application that interacts with an SQLite database to manage a simple book library system. The application should support adding, retrieving, updating, and deleting book records. Additionally, the application should save custom Python objects (e.g., a `Book` class) to the database and be able to retrieve them. # Requirements 1. **Book Class**: - Create a `Book` class with the following attributes: ```python class Book: def __init__(self, title: str, author: str, year: int, isbn: str): self.title = title self.author = author self.year = year self.isbn = isbn ``` 2. **Database Operations**: - Implement a function `init_db()` to initialize the SQLite database and create a table called `books` with the following columns: - `id` (INTEGER PRIMARY KEY AUTOINCREMENT) - `title` (TEXT) - `author` (TEXT) - `year` (INTEGER) - `isbn` (TEXT UNIQUE) - Implement the following CRUD functions for the `books` table: ```python def add_book(book: Book) -> None: Adds a book to the database. def get_books() -> list: Returns a list of all books in the database. def update_book(book_id: int, book: Book) -> None: Updates the book with the specified book_id. def delete_book(book_id: int) -> None: Deletes the book with the specified book_id. ``` 3. **Custom Type Adaptation**: - Implement custom adapters and converters for the `Book` class so that instances of `Book` can be directly stored in and retrieved from the SQLite database. 4. **Example Workflow**: - Demonstrate the use of the above functions in a script that: 1. Initializes the database. 2. Adds a few `Book` instances to the database. 3. Retrieves and prints all books. 4. Updates a book. 5. Deletes a book. 6. Retrieves and prints all books after the update and delete operations. # Constraints - Ensure that the `isbn` column in the `books` table is unique. - Handle any exceptions that may arise during database operations gracefully. # Input and Output Formats - **Input**: The input consists of Python function calls in the example workflow. - **Output**: The output should be the list of books printed at various stages of the workflow. # Example ```python if __name__ == \\"__main__\\": init_db() book1 = Book(\\"The Catcher in the Rye\\", \\"J.D. Salinger\\", 1951, \\"1234567890\\") book2 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960, \\"1111111111\\") add_book(book1) add_book(book2) print(\\"Books after addition:\\") print(get_books()) book1_new = Book(\\"The Catcher in the Rye\\", \\"J.D. Salinger\\", 1951, \\"1234567890\\") update_book(1, book1_new) print(\\"Books after update:\\") print(get_books()) delete_book(2) print(\\"Books after deletion:\\") print(get_books()) ``` # Submission Submit a single Python script file that contains the implementation of the `Book` class, the database operations functions, custom type adaptation functions, and the example workflow.","solution":"import sqlite3 class Book: def __init__(self, title: str, author: str, year: int, isbn: str): self.title = title self.author = author self.year = year self.isbn = isbn def init_db(): connection = sqlite3.connect(\'library.db\') cursor = connection.cursor() cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY AUTOINCREMENT, title TEXT, author TEXT, year INTEGER, isbn TEXT UNIQUE ) \'\'\') connection.commit() connection.close() def add_book(book: Book) -> None: connection = sqlite3.connect(\'library.db\') cursor = connection.cursor() cursor.execute(\'\'\' INSERT INTO books (title, author, year, isbn) VALUES (?, ?, ?, ?) \'\'\', (book.title, book.author, book.year, book.isbn)) connection.commit() connection.close() def get_books() -> list: connection = sqlite3.connect(\'library.db\') cursor = connection.cursor() cursor.execute(\'SELECT id, title, author, year, isbn FROM books\') books = cursor.fetchall() connection.close() return books def update_book(book_id: int, book: Book) -> None: connection = sqlite3.connect(\'library.db\') cursor = connection.cursor() cursor.execute(\'\'\' UPDATE books SET title = ?, author = ?, year = ?, isbn = ? WHERE id = ? \'\'\', (book.title, book.author, book.year, book.isbn, book_id)) connection.commit() connection.close() def delete_book(book_id: int) -> None: connection = sqlite3.connect(\'library.db\') cursor = connection.cursor() cursor.execute(\'DELETE FROM books WHERE id = ?\', (book_id,)) connection.commit() connection.close() def adapt_book(book: Book): return f\'{book.title};{book.author};{book.year};{book.isbn}\' def convert_book(s): title, author, year, isbn = s.split(\';\') return Book(title, author, int(year), isbn) sqlite3.register_adapter(Book, adapt_book) sqlite3.register_converter(\\"Book\\", convert_book) if __name__ == \\"__main__\\": init_db() book1 = Book(\\"The Catcher in the Rye\\", \\"J.D. Salinger\\", 1951, \\"1234567890\\") book2 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960, \\"1111111111\\") add_book(book1) add_book(book2) print(\\"Books after addition:\\") print(get_books()) book1_new = Book(\\"The Catcher in the Rye\\", \\"J.D. Salinger\\", 1951, \\"1234567890\\") update_book(1, book1_new) print(\\"Books after update:\\") print(get_books()) delete_book(2) print(\\"Books after deletion:\\") print(get_books())"},{"question":"# Coding Assessment: Parallel Processing with `multiprocessing` Objective: The aim of this exercise is to assess your understanding and utilization of the `multiprocessing` package in Python. You will be required to implement a function that processes a list of numbers in parallel and applies different mathematical operations using multiple processes. Problem Statement: You need to implement a function `parallel_process_operations` which takes a list of tuples as input. Each tuple will have the format `(operation, value)`, where `operation` is a string that can be one of \'square\', \'cube\', or \'double\', and `value` is an integer on which the operation needs to be performed. Your function should perform these operations in parallel using multiple processes and return the results in the same order as the input list. Requirements: 1. Use the `multiprocessing.Pool` class to create a pool of worker processes. 2. Implement three separate functions: - `square(x)`: Returns the square of `x`. - `cube(x)`: Returns the cube of `x`. - `double(x)`: Returns the double of `x`. 3. Use a dictionary to map the string operation (\'square\', \'cube\', \'double\') to the corresponding function. 4. Ensure that the operations are performed in parallel across multiple processes using the `Pool` class. 5. Your implementation should handle any number of input tuples efficiently. Function Signature: ```python def parallel_process_operations(operations: list[tuple[str, int]]) -> list[int]: pass ``` Input: - `operations`: A list of tuples where each tuple contains an operation (string) and a value (integer). The operation can be one of \'square\', \'cube\', or \'double\'. Output: - Returns a list of integers that are the result of applying the given operations on the values. The order of the results should match the order of the input list. Constraints: - The input list can contain up to `1000` tuples. - Each operation is valid and supported (\'square\', \'cube\', \'double\'). Example Usage: ```python # Example input operations = [(\'square\', 2), (\'cube\', 3), (\'double\', 4)] # Expected output: [4, 27, 8] # Example input operations = [(\'cube\', 5), (\'square\', 6), (\'double\', 7)] # Expected output: [125, 36, 14] ``` Notes: - Be sure to utilize the `multiprocessing.Pool` for parallel processing. - Take care of the order of resultsâ€”ensure they match the order of the input operations. Example Implementation Stub: ```python import multiprocessing def square(x): return x * x def cube(x): return x * x * x def double(x): return x * 2 def parallel_process_operations(operations): def process_operation(operation_value_tuple): operation, value = operation_value_tuple operation_map = { \'square\': square, \'cube\': cube, \'double\': double } return operation_map[operation](value) with multiprocessing.Pool() as pool: results = pool.map(process_operation, operations) return results ``` Implement the `parallel_process_operations` function and ensure it meets the requirements and constraints specified.","solution":"import multiprocessing def square(x): return x * x def cube(x): return x * x * x def double(x): return x * 2 def process_operation(operation_value_tuple): operation, value = operation_value_tuple operation_map = { \'square\': square, \'cube\': cube, \'double\': double } return operation_map[operation](value) def parallel_process_operations(operations): with multiprocessing.Pool() as pool: results = pool.map(process_operation, operations) return results"},{"question":"# Coding Assessment Question PyTorch Configuration and Parallelization Information **Objective:** Your task is to write a Python function utilizing PyTorch that makes use of the PyTorch configuration functions to retrieve and utilize the configuration information for further operations. This will demonstrate your understanding of integrating configuration settings and how PyTorch manages parallelism. **Problem Statement:** Write a Python function `get_and_use_pytorch_info()` that performs the following steps: 1. Retrieves the current PyTorch configuration and parallelization information using `torch.__config__.show()` and `torch.__config__.parallel_info()`. 2. Parses and extracts relevant details from the retrieved information. Specifically, extract: - The configuration settings as a dictionary. - The number of available and used CPU threads for parallel computation. 3. Use the extracted number of CPU threads to create a simple PyTorch tensor operation that demonstrates parallel computation. For example, performing a dot product of two large tensors. **Function Signature:** ```python def get_and_use_pytorch_info() -> dict: Retrieves PyTorch configuration and parallelization information, and uses this information to run a parallel computation. Returns: dict: A dictionary containing extracted configuration settings, number of available threads, number of used threads, and the result of a parallel computation. pass ``` **Input:** - No input parameters are required for this function. **Output:** - The function should return a dictionary with the following keys: - `config`: A dictionary containing configuration settings. - `available_threads`: An integer representing the number of available CPU threads. - `used_threads`: An integer representing the number of CPU threads currently used by PyTorch. - `computation_result`: The result of the PyTorch tensor operation demonstrating parallel computation. **Constraints:** 1. Assume that the system on which this function runs has multiple CPU cores available. 2. Ensure that the function runs efficiently and handles the retrieved information appropriately. **Example Output:** ```python { \\"config\\": {\\".... configuration settings ....}, \\"available_threads\\": 8, \\"used_threads\\": 4, \\"computation_result\\": the result of the tensor operation } ``` **Notes:** - Use the provided `torch.__config__.show()` and `torch.__config__.parallel_info()` functions for retrieving the configuration and parallelization information. - Make sure to handle any potential edge cases, such as missing or unavailable configuration settings. - The tensor operation can be any simple parallel computation (dot product is suggested for simplicity).","solution":"import torch def get_and_use_pytorch_info() -> dict: Retrieves PyTorch configuration and parallelization information, and uses this information to run a parallel computation. Returns: dict: A dictionary containing extracted configuration settings, number of available threads, number of used threads, and the result of a parallel computation. # Retrieve configuration info config_info = torch.__config__.show() # Retrieve parallel info parallel_info = torch.__config__.parallel_info() # Extracting relevant details config_lines = config_info.strip().split(\'n\') config_dict = {} for line in config_lines: parts = line.split(\':\', 1) if len(parts) == 2: key = parts[0].strip() value = parts[1].strip() config_dict[key] = value available_threads = int(torch.get_num_threads()) used_threads = int(torch.get_num_interop_threads()) # Demonstrating parallel computation with a dot product size = 10000 # Example size for large tensors tensor_a = torch.ones(size) tensor_b = torch.ones(size) computation_result = torch.dot(tensor_a, tensor_b).item() # Returning the compiled dictionary return { \\"config\\": config_dict, \\"available_threads\\": available_threads, \\"used_threads\\": used_threads, \\"computation_result\\": computation_result }"},{"question":"<|Analysis Begin|> The provided documentation outlines several environment variables specific to PyTorch\'s use of Apple\'s Metal Performance Shaders (MPS). These variables control various aspects such as logging, profiling, and memory allocation behaviors. Key aspects include setting high and low watermark ratios for memory allocation, enabling fast math for kernels, and fallback mechanisms to CPU when MPS is unsupported. To design a coding question based on this, students need to demonstrate their ability to manage and utilize these environment variables within a PyTorch context. This involves understanding how to set environment variables in Python, configuring PyTorch settings, and potentially managing memory use during tensor operations. To ensure the question is challenging while covering the fundamental and advanced concepts of PyTorch, we can create a question that requires students to simulate a specific scenario using these environment variables. <|Analysis End|> <|Question Begin|> **Question: Managing PyTorch MPS Environment for Efficient Computation** You have been tasked with optimizing a neural network\'s training process on a macOS system that utilizes Apple\'s Metal Performance Shaders (MPS) backend with PyTorch. Specifically, you need to manage memory allocation settings and enable specific profiling and fast math optimizations to ensure the training process is efficient and well-logged. Your task is to write a Python function `configure_pytorch_mps()` that performs the following configuration steps: 1. Sets the `PYTORCH_MPS_FAST_MATH` environment variable to `1` to enable fast math for kernels. 2. Adjusts the `PYTORCH_MPS_HIGH_WATERMARK_RATIO` to `0.95` to limit the maximum allowed allocation to 95% of the recommended maximum allocation size. 3. Sets `PYTORCH_MPS_LOW_WATERMARK_RATIO` to `0.85` to attempt to limit allocations to 85% of the recommended maximum allocation size. 4. Enables verbose logging for the MPS allocator by setting `PYTORCH_DEBUG_MPS_ALLOCATOR` to `1`. 5. Forces using metal kernels for matrix multiplication operations by setting `PYTORCH_MPS_PREFER_METAL` to `1`. Next, you need to demonstrate the effectiveness of your configuration by executing a sample tensor operation and logging the memory allocation information. Write a function `train_dummy_model()` that performs the following: 1. Creates a simple linear regression model using `torch.nn.Linear`. 2. Trains the model on random data for a few epochs, ensuring the configuration is applied. 3. Logs the memory allocation and profiling information during the training process. **Input:** - No input is required for the functions. **Output:** - Print logs demonstrating the configuration settings and profiling information. - The memory allocation updates during the dummy model training process. **Constraints:** - Ensure that you use the correct Python methods to set environment variables. - Utilize `torch` and `torch.nn` for creating and training the model. - Proper handling of PyTorch context to reflect the configured environment settings. ```python import os import torch def configure_pytorch_mps(): Configures the PyTorch MPS environment variables. os.environ[\'PYTORCH_MPS_FAST_MATH\'] = \'1\' os.environ[\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\'] = \'0.95\' os.environ[\'PYTORCH_MPS_LOW_WATERMARK_RATIO\'] = \'0.85\' os.environ[\'PYTORCH_DEBUG_MPS_ALLOCATOR\'] = \'1\' os.environ[\'PYTORCH_MPS_PREFER_METAL\'] = \'1\' # Print configuration to verify print(f\\"Configured PYTORCH_MPS_FAST_MATH: {os.getenv(\'PYTORCH_MPS_FAST_MATH\')}\\") print(f\\"Configured PYTORCH_MPS_HIGH_WATERMARK_RATIO: {os.getenv(\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\')}\\") print(f\\"Configured PYTORCH_MPS_LOW_WATERMARK_RATIO: {os.getenv(\'PYTORCH_MPS_LOW_WATERMARK_RATIO\')}\\") print(f\\"Configured PYTORCH_DEBUG_MPS_ALLOCATOR: {os.getenv(\'PYTORCH_DEBUG_MPS_ALLOCATOR\')}\\") print(f\\"Configured PYTORCH_MPS_PREFER_METAL: {os.getenv(\'PYTORCH_MPS_PREFER_METAL\')}\\") def train_dummy_model(): Creates and trains a dummy linear regression model to demonstrate the applied MPS configuration. configure_pytorch_mps() # Initialize a dummy dataset x = torch.randn(100, 1) y = 3 * x + torch.randn(100, 1) # Define a simple linear regression model model = torch.nn.Linear(1, 1) criterion = torch.nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Train the model for a few epochs for epoch in range(10): model.train() optimizer.zero_grad() outputs = model(x) loss = criterion(outputs, y) loss.backward() optimizer.step() print(f\\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\\") print(\\"Training complete. Check the logs for memory allocation details.\\") # Execute the training process train_dummy_model() ``` **Note:** Ensure you have the necessary environment to run MPS on macOS with PyTorch installed. This setup is particularly relevant for macOS users leveraging Apple\'s GPU capabilities for machine learning tasks. The example provided is simplistic to demonstrate environment configuration and logging.","solution":"import os import torch def configure_pytorch_mps(): Configures the PyTorch MPS environment variables. os.environ[\'PYTORCH_MPS_FAST_MATH\'] = \'1\' os.environ[\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\'] = \'0.95\' os.environ[\'PYTORCH_MPS_LOW_WATERMARK_RATIO\'] = \'0.85\' os.environ[\'PYTORCH_DEBUG_MPS_ALLOCATOR\'] = \'1\' os.environ[\'PYTORCH_MPS_PREFER_METAL\'] = \'1\' # Print configuration to verify print(f\\"Configured PYTORCH_MPS_FAST_MATH: {os.getenv(\'PYTORCH_MPS_FAST_MATH\')}\\") print(f\\"Configured PYTORCH_MPS_HIGH_WATERMARK_RATIO: {os.getenv(\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\')}\\") print(f\\"Configured PYTORCH_MPS_LOW_WATERMARK_RATIO: {os.getenv(\'PYTORCH_MPS_LOW_WATERMARK_RATIO\')}\\") print(f\\"Configured PYTORCH_DEBUG_MPS_ALLOCATOR: {os.getenv(\'PYTORCH_DEBUG_MPS_ALLOCATOR\')}\\") print(f\\"Configured PYTORCH_MPS_PREFER_METAL: {os.getenv(\'PYTORCH_MPS_PREFER_METAL\')}\\") def train_dummy_model(): Creates and trains a dummy linear regression model to demonstrate the applied MPS configuration. configure_pytorch_mps() # Check for MPS device if not torch.backends.mps.is_available(): print(\\"MPS is not available on this system.\\") return device = torch.device(\\"mps\\") # Initialize a dummy dataset x = torch.randn(100, 1, device=device) y = 3 * x + torch.randn(100, 1, device=device) # Define a simple linear regression model model = torch.nn.Linear(1, 1).to(device) criterion = torch.nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Train the model for a few epochs for epoch in range(10): model.train() optimizer.zero_grad() outputs = model(x) loss = criterion(outputs, y) loss.backward() optimizer.step() print(f\\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\\") print(\\"Training complete. Check the logs for memory allocation details.\\") # Execute the training process if __name__ == \\"__main__\\": train_dummy_model()"},{"question":"Custom PyTorch Autograd Function: Gradient Handling and In-place Operations # Background In PyTorch, you can define your own custom autograd functions by subclassing `torch.autograd.Function` and implementing the `forward` and `backward` static methods. This is useful when you need to define a new operation that PyTorch doesnâ€™t provide or if you need to ensure certain behaviors during forward and backward passes. # Task Create a custom autograd function to compute the element-wise reciprocal square root, ( f(x) = frac{1}{sqrt{x}} ), of input tensors. Your implementation should be efficient, handle in-place operations correctly, and make use of PyTorch\'s functionality to save intermediary tensors for the backward pass. You should also ensure that your custom function can deal with non-differentiable points appropriately (e.g., when ( x = 0 )). # Requirements: 1. Define a class `ReciprocalSqrt` that extends `torch.autograd.Function`. 2. Implement the `forward` method that computes ( f(x) = frac{1}{sqrt{x}} ) and saves necessary values for backward computation. 3. Implement the `backward` method to calculate the gradient of the loss with respect to the input. 4. Ensure that your implementation can handle in-place operations and raise proper errors when incorrect usage is detected. 5. Handle non-differentiable points by choosing an appropriate gradient value or using sub-gradients. # Constraints: - The input tensor will always have `requires_grad=True`. - Implementations should avoid creating reference cycles or memory leaks. - Handle tensors of various shapes in a matrix form. - Ensure that the backward pass works correctly if the forward pass was performed with `no_grad` or `inference_mode` enabled. # Input: - A PyTorch tensor ( x ) of any shape with `requires_grad=True`. # Output: - A tensor with the same shape as input, where each element is the reciprocal square root of the corresponding element in the input. # Performance Requirements: - The implementation should be efficient in terms of memory and computational complexity. # Example: ```python import torch from torch.autograd import Function class ReciprocalSqrt(Function): @staticmethod def forward(ctx, input): # Compute the forward pass and save necessary tensors # # YOUR CODE HERE pass @staticmethod def backward(ctx, grad_output): # Compute the backward pass # # YOUR CODE HERE pass # Test your custom function x = torch.tensor([4.0, 0.0, 16.0, 25.0], requires_grad=True) # Use the custom function y = ReciprocalSqrt.apply(x) y.sum().backward() print(x.grad) # Should print the gradients for each element of x ``` # Note: - Ensure to test the function on multiple input shapes and values to validate its correctness. - Document any assumptions or decisions made during the implementation.","solution":"import torch from torch.autograd import Function class ReciprocalSqrt(Function): @staticmethod def forward(ctx, input): # Save input for backward pass ctx.save_for_backward(input) # Compute the reciprocal square root result = 1.0 / torch.sqrt(input) return result @staticmethod def backward(ctx, grad_output): # Retrieve saved input input, = ctx.saved_tensors # Prevent division by zero in the backward pass grad_input = None if ctx.needs_input_grad[0]: grad_input = (-0.5) * grad_output / (input * torch.sqrt(input)) grad_input[input <= 0] = 0 return grad_input def reciprocal_sqrt(x): return ReciprocalSqrt.apply(x)"},{"question":"**Question: Implement a Custom Data Structure with Special Methods** # Objective You are required to implement a custom data structure in Python that behaves like a list but with additional constraints and functionalities. This custom list-like class should be able to handle numeric values only and provide certain custom behaviors using Python\'s special methods. # Requirements 1. **Class Definition**: Define a class `NumericList` that: - Inherits from Python\'s built-in `list` class. 2. **Initialization**: - Ensure that the list only takes numeric values (integers and floats). Raise a `TypeError` if any non-numeric value is added. 3. **Special Methods**: - Implement the `__setitem__` method to ensure that only numeric values can be set. - Implement the `__getitem__` method to allow slicing and return the sum of the sliced portion. - Implement the `__add__` method to allow concatenation with another `NumericList` and return a new `NumericList` with elements doubled. - Implement the `__sub__` method to remove all occurrences of elements from the list given another `NumericList`. - Implement the `__str__` method to return a string representation that indicates the class name and elements. # Constraints - Only numeric (int, float) values should be allowed in `NumericList`. - Implement proper error handling for non-numeric values. - Custom behaviors like addition and subtraction should follow the stipulated rules. # Function Signature ```python class NumericList(list): def __init__(self, *args): # Initialize with numeric values only pass def __setitem__(self, index, value): # Override to ensure only numeric values can be set pass def __getitem__(self, index): # Override to return sliced portion sum pass def __add__(self, other): # Override to concatenate and double elements pass def __sub__(self, other): # Override to remove occurrences of elements pass def __str__(self): # Override for string representation pass ``` # Test Cases Test your implementation with the following cases: 1. Initialization with numeric and non-numeric values. 2. Setting items with numeric and non-numeric values. 3. Slicing to obtain a sum. 4. Concatenation with another `NumericList`. 5. Subtraction of elements using another `NumericList`. 6. String representation check. **Example:** ```python try: nl = NumericList(1, 2, \'abc\') # Should raise TypeError except TypeError as e: print(e) # Output: Non-numeric value detected nl = NumericList(1, 2, 3.5, 4) nl[2] = 3.5 # No error try: nl[2] = \'a\' # Should raise TypeError except TypeError as e: print(e) # Output: Non-numeric value detected print(nl[:3]) # Output: 6.5 nl2 = NumericList(5, 6) nl3 = nl + nl2 print(nl3) # Output: NumericList([2, 4, 7.0, 8, 10, 12]) nl4 = NumericList(1, 3.5) nl -= nl4 print(nl) # Output: NumericList([2, 4]) ``` # Submission Submit your `NumericList` class implementation along with the test cases in a single Python (.py) file.","solution":"class NumericList(list): def __init__(self, *args): for arg in args: if not isinstance(arg, (int, float)): raise TypeError(\\"Non-numeric value detected\\") super().__init__(args) def __setitem__(self, index, value): if not isinstance(value, (int, float)): raise TypeError(\\"Non-numeric value detected\\") super().__setitem__(index, value) def __getitem__(self, index): if isinstance(index, slice): return sum(super().__getitem__(index)) return super().__getitem__(index) def __add__(self, other): if not isinstance(other, NumericList): raise TypeError(\\"Can only concatenate with another NumericList\\") return NumericList(*(2 * (x + y) for x, y in zip(self, other))) def __sub__(self, other): if not isinstance(other, NumericList): raise TypeError(\\"Can only subtract with another NumericList\\") result = NumericList(*self) for item in other: while item in result: result.remove(item) return result def __str__(self): return f\\"NumericList({super().__str__()})\\""},{"question":"# Pandas DataFrame Merging and Concatenation You are given two DataFrames containing sales and customer data for an e-commerce company. 1. **sales_df**: ```python sales_df = pd.DataFrame({ \'order_id\': [1, 2, 3, 4], \'product\': [\'A\', \'B\', \'A\', \'C\'], \'quantity\': [2, 1, 1, 3], \'price\': [100, 150, 100, 200] }) ``` 2. **customers_df**: ```python customers_df = pd.DataFrame({ \'customer_id\': [1, 2, 3], \'name\': [\'Alice\', \'Bob\', \'Charlie\'], \'location\': [\'New York\', \'California\', \'Texas\'] }) ``` Additionally, you have an order-customer mapping DataFrame called **order_customer_map**: ```python order_customer_map = pd.DataFrame({ \'order_id\': [1, 2, 3, 4], \'customer_id\': [1, 2, 3, 1] }) ``` # Tasks 1. **Merge sales_df and order_customer_map**: Combine `sales_df` with `order_customer_map` on `order_id` to associate each sale with its corresponding customer ID. 2. **Merge with customers_df**: Use the resulting DataFrame from task 1 to merge with `customers_df` on `customer_id` to include customer details for each sale. 3. **Add a total column**: Compute a `total` column in the resulting DataFrame which is the product of `quantity` and `price`. 4. **Generate a summary report**: Concatenate a summary report DataFrame that presents: - Total sales per product. - Total quantity sold per location. # Expected Input and Output Input None. The dataframes are predefined. Output The script should output: 1. The merged DataFrame after Task 1. 2. The merged DataFrame after Task 2. 3. The final DataFrame after adding the `total` column. 4. The summary report DataFrame. # Example Output ```python # After Task 1 order_id product quantity price customer_id 0 1 A 2 100 1 1 2 B 1 150 2 2 3 A 1 100 3 3 4 C 3 200 1 # After Task 2 order_id product quantity price customer_id name location 0 1 A 2 100 1 Alice New York 1 2 B 1 150 2 Bob California 2 3 A 1 100 3 Charlie Texas 3 4 C 3 200 1 Alice New York # After adding the total column order_id product quantity price customer_id name location total 0 1 A 2 100 1 Alice New York 200 1 2 B 1 150 2 Bob California 150 2 3 A 1 100 3 Charlie Texas 100 3 4 C 3 200 1 Alice New York 600 # Summary Report Total sales per product: product total_sales 0 A 300 1 B 150 2 C 600 Total quantity sold per location: location total_quantity 0 New York 5 1 California 1 2 Texas 1 ``` **Constraints and Performance Requirements**: - Use appropriate pandas functions and methods for merging, joining, and summarizing data. - Ensure that the operations are efficient and handle potential missing data gracefully (e.g., outer joins if necessary).","solution":"import pandas as pd # Define the dataframes sales_df = pd.DataFrame({ \'order_id\': [1, 2, 3, 4], \'product\': [\'A\', \'B\', \'A\', \'C\'], \'quantity\': [2, 1, 1, 3], \'price\': [100, 150, 100, 200] }) customers_df = pd.DataFrame({ \'customer_id\': [1, 2, 3], \'name\': [\'Alice\', \'Bob\', \'Charlie\'], \'location\': [\'New York\', \'California\', \'Texas\'] }) order_customer_map = pd.DataFrame({ \'order_id\': [1, 2, 3, 4], \'customer_id\': [1, 2, 3, 1] }) # Task 1: Merge sales_df and order_customer_map on order_id sales_with_customers = pd.merge(sales_df, order_customer_map, on=\'order_id\') print(sales_with_customers) # Task 2: Merge sales_with_customers with customers_df on customer_id sales_with_customer_details = pd.merge(sales_with_customers, customers_df, on=\'customer_id\') print(sales_with_customer_details) # Task 3: Add a total column which is the product of quantity and price sales_with_customer_details[\'total\'] = sales_with_customer_details[\'quantity\'] * sales_with_customer_details[\'price\'] print(sales_with_customer_details) # Task 4: Generate summary report # Total sales per product total_sales_per_product = sales_with_customer_details.groupby(\'product\')[\'total\'].sum().reset_index().rename(columns={\'total\': \'total_sales\'}) # Total quantity sold per location total_quantity_per_location = sales_with_customer_details.groupby(\'location\')[\'quantity\'].sum().reset_index().rename(columns={\'quantity\': \'total_quantity\'}) summary_report = { \'total_sales_per_product\': total_sales_per_product, \'total_quantity_per_location\': total_quantity_per_location } print(summary_report)"},{"question":"# Custom Matrix Object with PyTorch Operations You are required to create a custom matrix class `MyMatrix` that integrates seamlessly with PyTorch operations by overriding some PyTorch functions using the `__torch_function__` protocol. This matrix class should be able to support addition, multiplication (both element-wise and matrix multiplication), and mean operations. Specifications 1. **Class Definition**: - Define a class `MyMatrix` that takes an initial 2D list of numbers and converts it into a PyTorch tensor. - Implement the `__torch_function__` protocol in this class to override specific operations. 2. **Operations to Override**: - `torch.add` for element-wise addition. - `torch.mul` for element-wise multiplication. - `torch.matmul` for matrix multiplication. - `torch.mean` for calculating the mean of all elements. 3. **Expected Input and Output Formats**: - Constructor input: A 2D list of numbers. - Addition and Multiplication inputs: Another instance of `MyMatrix` or a PyTorch tensor. - Matrix multiplication inputs: Another instance of `MyMatrix` or a PyTorch tensor. - Mean input: No additional input required. - Outputs should be instances of `MyMatrix` for addition, multiplication, and matrix multiplication, and a float value for the mean. 4. **Constraints**: - Ensure that your implementation checks whether an operation involves valid `MyMatrix` instances or PyTorch tensors. - Raise an appropriate error for unsupported types. 5. **Performance Requirements**: - Ensure efficient handling for large matrices by utilizing the inherent operations of PyTorch. Example Usage ```python import torch class MyMatrix: def __init__(self, data): self.tensor = torch.tensor(data) def __torch_function__(self, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func not in {torch.add, torch.mul, torch.matmul, torch.mean}: return NotImplemented if not all(issubclass(t, (torch.Tensor, MyMatrix)) for t in types): return NotImplemented # Handle each function appropriately if func == torch.add: # Implement addition handling pass elif func == torch.mul: # Implement multiplication handling pass elif func == torch.matmul: # Implement matrix multiplication handling pass elif func == torch.mean: # Implement mean handling pass # Implementation of __repr__ for better visualization def __repr__(self): return repr(self.tensor) # Example usage matrix1 = MyMatrix([[1, 2], [3, 4]]) matrix2 = MyMatrix([[5, 6], [7, 8]]) result_add = torch.add(matrix1, matrix2) result_mul = torch.mul(matrix1, matrix2) result_matmul = torch.matmul(matrix1, matrix2) result_mean = torch.mean(matrix1) print(result_add) print(result_mul) print(result_matmul) print(result_mean) ``` Complete the `__torch_function__` method and ensure the operations are correctly overridden.","solution":"import torch class MyMatrix: def __init__(self, data): self.tensor = torch.tensor(data, dtype=torch.float32) def __torch_function__(self, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func not in {torch.add, torch.mul, torch.matmul, torch.mean}: return NotImplemented if not all(issubclass(t, (torch.Tensor, MyMatrix)) for t in types): return NotImplemented if func == torch.add: other = args[1] if isinstance(other, MyMatrix): other = other.tensor result = func(self.tensor, other, **kwargs) return MyMatrix(result.tolist()) elif func == torch.mul: other = args[1] if isinstance(other, MyMatrix): other = other.tensor result = func(self.tensor, other, **kwargs) return MyMatrix(result.tolist()) elif func == torch.matmul: other = args[1] if isinstance(other, MyMatrix): other = other.tensor result = func(self.tensor, other, **kwargs) return MyMatrix(result.tolist()) elif func == torch.mean: result = func(self.tensor, **kwargs) return result.item() # Implementation of __repr__ for better visualization def __repr__(self): return repr(self.tensor) # Example usage matrix1 = MyMatrix([[1, 2], [3, 4]]) matrix2 = MyMatrix([[5, 6], [7, 8]]) result_add = torch.add(matrix1, matrix2) result_mul = torch.mul(matrix1, matrix2) result_matmul = torch.matmul(matrix1, matrix2) result_mean = torch.mean(matrix1) print(result_add) print(result_mul) print(result_matmul) print(result_mean)"},{"question":"Problem: Plist Data Validator and Transformer # Objective You need to write a function `validate_and_transform_plist` that reads from a plist, validates the data, and transforms it according to given rules. # Input 1. A string `filename` that represents the path to the plist file. 2. A dictionary `validation_rules` where keys are strings representing keys in the plist, and values are types that the corresponding plist values should match. # Output A dictionary which is either: - The transformed plist if all validations pass, where every integer value is incremented by 1, or - A dictionary with a single key \\"error\\" whose value is a string describing the first validation error encountered. # Constraints - The plist file can be in either XML or binary format. - The plist can contain nested dictionaries and lists. - The validation should be recursive to accommodate nested dictionaries and lists. # Function Signature ```python def validate_and_transform_plist(filename: str, validation_rules: dict) -> dict: pass ``` # Example Given a plist file `example.plist` with content: ```xml <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>name</key> <string>John Doe</string> <key>age</key> <integer>30</integer> <key>details</key> <dict> <key>height</key> <real>5.9</real> <key>languages</key> <array> <string>English</string> <string>Spanish</string> </array> </dict> </dict> </plist> ``` And the validation rules: ```python validation_rules = { \\"name\\": str, \\"age\\": int, \\"details\\": dict, \\"details.height\\": float, \\"details.languages\\": list } ``` Your function should read the plist file, validate against the rules, and if valid, return: ```python { \\"name\\": \\"John Doe\\", \\"age\\": 31, \\"details\\": { \\"height\\": 5.9, \\"languages\\": [\\"English\\", \\"Spanish\\"] } } ``` If a validation error is encountered, for instance if `age` was a string: ```python { \\"error\\": \\"Validation error at key \'age\': expected type <class \'int\'>\\" } ``` # Note - Ensure to handle nested dictionary keys correctly in the validation_rules. - Increment every integer value by 1 if all validations pass.","solution":"import plistlib def validate_and_transform_plist(filename: str, validation_rules: dict) -> dict: def validate_and_transform(data, rules, parent_key=\'\'): for key, expected_type in rules.items(): current_key = parent_key + key if \'.\' in key: base_key, sub_key = key.split(\'.\', 1) if base_key in data: result = validate_and_transform(data[base_key], {sub_key: expected_type}, base_key + \'.\') if \'error\' in result: return result else: if key not in data: return {\\"error\\": f\\"Validation error at key \'{current_key}\': key not found in data\\"} if not isinstance(data[key], expected_type): return {\\"error\\": f\\"Validation error at key \'{current_key}\': expected type {expected_type} but found {type(data[key])}\\"} if expected_type == int: data[key] += 1 return data try: with open(filename, \'rb\') as f: plist_data = plistlib.load(f) except Exception as e: return {\\"error\\": f\\"Failed to read plist file: {e}\\"} result = validate_and_transform(plist_data, validation_rules) return result"},{"question":"# Support Vector Machine Classification with Custom Kernel Implementation Objective Implement a Support Vector Classifier (SVC) and demonstrate its use on a given dataset. Additionally, implement a custom kernel function and use it in the SVC. Evaluate and compare the performance of the classifier using the custom kernel and standard kernels (e.g., \'linear\', \'rbf\'). Problem Statement You are provided with a dataset containing features and labels. Your task is to: 1. Implement a Support Vector Classifier (SVC) using scikit-learn. 2. Create a custom kernel function. 3. Fit the classifier using both the custom kernel and standard kernels (\'linear\' and \'rbf\'). 4. Compare the performance of the classifiers using accuracy and confusion matrices. Input - `X`: A 2D array of shape `(n_samples, n_features)`, representing the feature vectors. - `y`: A 1D array of shape `(n_samples,)`, representing the class labels. Output For each kernel used (custom, linear, rbf), output the following: - `accuracy`: The accuracy of the classifier on the given dataset. - `confusion_matrix`: The confusion matrix for the classifier\'s predictions. Constraints - You must use scikit-learn\'s `SVC` for classification. - The custom kernel function must be implemented by you. - Use `accuracy_score` and `confusion_matrix` from scikit-learn for performance evaluation. Custom Kernel Function Implement a custom kernel function that computes the dot product and raises it to a power of 3. ```python def custom_kernel(X, Y): import numpy as np return np.dot(X, Y.T) ** 3 ``` Performance Requirements - The implementation should be efficient and capable of handling datasets with up to 10,000 samples and 100 features. Example Usage ```python from sklearn.metrics import accuracy_score, confusion_matrix # Example dataset X = [[0, 0], [1, 1], [2, 2], [3, 3]] y = [0, 1, 1, 0] def custom_kernel(X, Y): import numpy as np return np.dot(X, Y.T) ** 3 # Your implementation here # 1. Fit SVC with custom kernel svc_custom = SVC(kernel=custom_kernel) svc_custom.fit(X, y) y_pred_custom = svc_custom.predict(X) accuracy_custom = accuracy_score(y, y_pred_custom) conf_matrix_custom = confusion_matrix(y, y_pred_custom) # 2. Fit SVC with linear kernel svc_linear = SVC(kernel=\'linear\') svc_linear.fit(X, y) y_pred_linear = svc_linear.predict(X) accuracy_linear = accuracy_score(y, y_pred_linear) conf_matrix_linear = confusion_matrix(y, y_pred_linear) # 3. Fit SVC with rbf kernel svc_rbf = SVC(kernel=\'rbf\') svc_rbf.fit(X, y) y_pred_rbf = svc_rbf.predict(X) accuracy_rbf = accuracy_score(y, y_pred_rbf) conf_matrix_rbf = confusion_matrix(y, y_pred_rbf) # Output print(\\"Custom Kernel Accuracy:\\", accuracy_custom) print(\\"Custom Kernel Confusion Matrix:n\\", conf_matrix_custom) print(\\"Linear Kernel Accuracy:\\", accuracy_linear) print(\\"Linear Kernel Confusion Matrix:n\\", conf_matrix_linear) print(\\"RBF Kernel Accuracy:\\", accuracy_rbf) print(\\"RBF Kernel Confusion Matrix:n\\", conf_matrix_rbf) ``` Notes - Ensure that the custom kernel function is correct and optimized. - Comparisons should be made using the same dataset and evaluation metrics.","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.svm import SVC import numpy as np def custom_kernel(X, Y): Custom kernel function that computes the dot product and raises it to a power of 3. return np.dot(X, Y.T) ** 3 # Load a sample dataset for demonstration iris = datasets.load_iris() X, y = iris.data, iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit SVC with custom kernel svc_custom = SVC(kernel=custom_kernel) svc_custom.fit(X_train, y_train) y_pred_custom = svc_custom.predict(X_test) accuracy_custom = accuracy_score(y_test, y_pred_custom) conf_matrix_custom = confusion_matrix(y_test, y_pred_custom) # Fit SVC with linear kernel svc_linear = SVC(kernel=\'linear\') svc_linear.fit(X_train, y_train) y_pred_linear = svc_linear.predict(X_test) accuracy_linear = accuracy_score(y_test, y_pred_linear) conf_matrix_linear = confusion_matrix(y_test, y_pred_linear) # Fit SVC with rbf kernel svc_rbf = SVC(kernel=\'rbf\') svc_rbf.fit(X_train, y_train) y_pred_rbf = svc_rbf.predict(X_test) accuracy_rbf = accuracy_score(y_test, y_pred_rbf) conf_matrix_rbf = confusion_matrix(y_test, y_pred_rbf) def get_svc_performance(): return { \\"custom_kernel\\": { \\"accuracy\\": accuracy_custom, \\"confusion_matrix\\": conf_matrix_custom }, \\"linear_kernel\\": { \\"accuracy\\": accuracy_linear, \\"confusion_matrix\\": conf_matrix_linear }, \\"rbf_kernel\\": { \\"accuracy\\": accuracy_rbf, \\"confusion_matrix\\": conf_matrix_rbf } } # Output performance metrics svc_performance = get_svc_performance() for kernel, metrics in svc_performance.items(): print(f\\"{kernel.capitalize()} Accuracy: {metrics[\'accuracy\']}\\") print(f\\"{kernel.capitalize()} Confusion Matrix:n{metrics[\'confusion_matrix\']}n\\")"},{"question":"**Advanced Multipurpose Utility Function** Implement a Python function named `multi_utility_function` that performs a series of tasks, demonstrating proficiency in using utility functions in Python 3.10. # Specifications 1. The function should accept a string and an integer as arguments. 2. The string represents a path (it could be a file path or any system path), and the integer represents a mode of operation. 3. Depending on the mode of operation, perform one of the following tasks: - **Mode 1 (Path Check)**: Verify if the provided path exists in the file system. If it exists, return `True`; otherwise, return `False`. - **Mode 2 (Path Information)**: If the path exists, return a dictionary containing information about the path, including: - Absolute path - File size (if it is a file) - Number of files (if it is a directory) - **Mode 3 (String Manipulation)**: If the string is determined to be a valid path, convert the string to uppercase, return it, and log the result to a temporary log file. - **Mode 4 (Argument Reflection)**: Return a list containing the types of the provided arguments. # Input/Output - **Input**: `(path: str, mode: int)` - **Output**: - Depending on the mode, the output will be: - Mode 1: `bool` - Mode 2: `dict` - Mode 3: `str` - Mode 4: `list` # Constraints - Ensure the program handles invalid paths gracefully. - The function should be efficient in checking and handling paths. - For simplicity, consider only local paths (i.e., no network or remote paths). # Example Usage ```python def multi_utility_function(path: str, mode: int): import os import logging log_file = \\"temp_log.txt\\" logging.basicConfig(filename=log_file, level=logging.DEBUG) if mode == 1: return os.path.exists(path) elif mode == 2: if os.path.exists(path): path_info = { \'absolute_path\': os.path.abspath(path), } if os.path.isfile(path): path_info[\'size\'] = os.path.getsize(path) elif os.path.isdir(path): path_info[\'file_count\'] = len(os.listdir(path)) return path_info elif mode == 3: if os.path.exists(path): upper_path = path.upper() logging.debug(upper_path) return upper_path elif mode == 4: return [type(path), type(mode)] else: raise ValueError(\\"Invalid mode.\\") ``` # Performance Requirements - The function should efficiently handle path verifications and manipulations, even if the provided path is to a directory containing a large number of files. # Notes - You may use Python\'s standard libraries to implement this function. - Make sure to handle edge cases, such as invalid paths, and provide appropriate error messages or return values.","solution":"def multi_utility_function(path: str, mode: int): import os import logging log_file = \\"temp_log.txt\\" logging.basicConfig(filename=log_file, level=logging.DEBUG) if mode == 1: return os.path.exists(path) elif mode == 2: if os.path.exists(path): path_info = { \'absolute_path\': os.path.abspath(path), } if os.path.isfile(path): path_info[\'size\'] = os.path.getsize(path) elif os.path.isdir(path): path_info[\'file_count\'] = len(os.listdir(path)) return path_info return None elif mode == 3: if os.path.exists(path): upper_path = path.upper() logging.debug(upper_path) return upper_path return None elif mode == 4: return [type(path), type(mode)] else: raise ValueError(\\"Invalid mode.\\")"},{"question":"Objective Evaluate the students\' ability to use seaborn for data visualization, combining different plot types and utilizing advanced customization features. Problem Statement You are provided with the `tips` dataset. Your task is to perform the following steps using seaborn and matplotlib: 1. **Load the dataset** and inspect its structure. 2. **Preprocess the data** to filter out records where the total bill is less than 10. 3. Create a seaborn plot that **combines a KDE plot, scatter plot, and rug plot** to visualize the distribution of `total_bill` against `tip`. Ensure the combined plot meets the following criteria: * The KDE plot should be plotted for `total_bill`. * The scatter plot should use the `total_bill` and `tip` columns. * The rug plot should be added for both `total_bill` and `tip`. * Use `time` as the hue to distinguish between different times (Lunch/Dinner). * Customize the rug plot to have a taller height and be drawn outside the axes. * Apply alpha blending to the rug plot lines to manage density representation. Input - `tips` dataset from seaborn Expected Output A combined plot visualizing the distribution and relationship between `total_bill` and `tip` with appropriate customizations, including hue, height, and position of the rug plot. Coding Requirements - Your code must use seaborn and matplotlib. - You must utilize the following seaborn functions: `load_dataset`, `kdeplot`, `scatterplot`, and `rugplot`. - Include comments explaining key steps and choices made in your code. Example ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Preprocess the data tips_filtered = tips[tips[\'total_bill\'] >= 10] # Create the combined plot sns.kdeplot(data=tips_filtered, x=\\"total_bill\\") sns.scatterplot(data=tips_filtered, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") sns.rugplot(data=tips_filtered, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", height=.1, clip_on=False, alpha=0.5) # Display the plot plt.show() ``` **Note:** The example is provided for illustration purposes only. Your solution should include the necessary imports and detailed comments.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_combined_plot(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Preprocess the data: Filter out records where the total bill is less than 10 tips_filtered = tips[tips[\'total_bill\'] >= 10] # Create a KDE plot for total_bill sns.kdeplot(data=tips_filtered, x=\\"total_bill\\", fill=True) # Create a scatter plot for total_bill vs tip with hue based on time sns.scatterplot(data=tips_filtered, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\") # Add rug plots for total_bill and tip with customized attributes sns.rugplot(data=tips_filtered, x=\\"total_bill\\", height=-0.02, clip_on=False, alpha=0.7, color=\'red\') sns.rugplot(data=tips_filtered, y=\\"tip\\", height=-0.02, clip_on=False, alpha=0.7, color=\'red\') # Display the plot plt.show()"},{"question":"# Coding Assessment: Advanced Use of Error Bars in Seaborn Objective: Demonstrate your understanding of seaborn\'s error bar functionalities and how they can be used to represent data uncertainty and spread in visualizations. Task: Write a Python function named `plot_error_bars` that takes the following inputs and generates a plot with error bars using seaborn: 1. `data`: A Pandas DataFrame with at least two columns containing numeric data. 2. `x_col`: The name of the column in `data` representing the x-axis data. 3. `y_col`: The name of the column in `data` representing the y-axis data. 4. `errorbar_type`: A string specifying the type of error bar to use. This can be one of \\"sd\\", \\"se\\", \\"pi\\", or \\"ci\\". 5. `errorbar_param`: An optional parameter to adjust the error bar size. For \\"sd\\" and \\"se\\", this is a scalar factor. For \\"pi\\" and \\"ci\\", this is the width of the percentile/confidence interval. The function should: - Validate the inputs to ensure they are correct and meaningful. - Plot the data points with the specified error bars. - Include well-labeled axes, a title, and a legend. Constraints: - You may assume that `data` is not empty and contains numeric entries in the specified columns. - Handle any potential issues with non-numeric columns correctly by raising appropriate errors. - Use seaborn and matplotlib for all plotting activities. Example Usage: ```python import pandas as pd # Sample data data = pd.DataFrame({ \'x\': range(10), \'y\': [2, 3, 5, 7, 11, 13, 17, 19, 23, 29] }) # Generate the plot plot_error_bars(data, x_col=\'x\', y_col=\'y\', errorbar_type=\'ci\', errorbar_param=95) ``` This usage should produce a scatter plot of the `y` values against the `x` values with the specified type of error bars around the points. Evaluation Criteria: - Correct implementation and validation of inputs. - Appropriate application of seaborn functions to generate the desired plot. - Clarity and labeling of the plot for easy interpretation.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_error_bars(data, x_col, y_col, errorbar_type, errorbar_param=None): Plots the data with error bars using seaborn. Parameters: - data: pandas DataFrame containing the data - x_col: str, name of the column for the x-axis data - y_col: str, name of the column for the y-axis data - errorbar_type: str, the type of error bar (\'sd\', \'se\', \'pi\', \'ci\') - errorbar_param: parameter to adjust error bar size, required for \'pi\' and \'ci\' # Validate inputs if not isinstance(data, pd.DataFrame): raise ValueError(\\"data must be a pandas DataFrame.\\") if x_col not in data.columns or y_col not in data.columns: raise ValueError(\\"x_col and y_col must be columns in the data DataFrame.\\") if errorbar_type not in [\\"sd\\", \\"se\\", \\"pi\\", \\"ci\\"]: raise ValueError(\\"errorbar_type must be one of \'sd\', \'se\', \'pi\', \'ci\'.\\") if errorbar_type in [\\"pi\\", \\"ci\\"] and errorbar_param is None: raise ValueError(\\"errorbar_param must be provided for \'pi\' and \'ci\' error bars.\\") # Plot plt.figure(figsize=(10, 6)) sns.lineplot(data=data, x=x_col, y=y_col, marker=\'o\', err_style=\\"bars\\", ci=errorbar_param if errorbar_type in [\\"pi\\", \\"ci\\"] else None, estimator=\\"mean\\" if errorbar_type == \\"sd\\" else None, n_boot=1000 if errorbar_type == \\"ci\\" else None, err_kws={\\"linewidth\\": 1}) # Additional features for sd and se if errorbar_type in [\\"sd\\", \\"se\\"]: y_values = data[y_col] y_mean = y_values.mean() y_std = y_values.std() y_se = y_std / (len(y_values) ** 0.5) if errorbar_type == \\"sd\\": plt.errorbar(data[x_col], data[y_col], yerr=y_std * errorbar_param, fmt=\'o\', color=\'blue\') elif errorbar_type == \\"se\\": plt.errorbar(data[x_col], data[y_col], yerr=y_se * errorbar_param, fmt=\'o\', color=\'red\') plt.xlabel(x_col) plt.ylabel(y_col) plt.title(f\'{y_col} vs {x_col} with {errorbar_type} error bars\') plt.legend([\'Data\']) plt.show()"},{"question":"**Objective:** To test the student\'s ability to use the `doctest` module for testing Python code embedded in docstrings. **Instructions:** You are tasked with creating a Python module named `math_operations.py` that contains functions for basic math operations. Each function should include doctests in its docstring to validate its behavior. Additionally, you will implement a custom output checker to handle specific test scenarios and integrate the `doctest` with the `unittest` framework. You should implement the following: 1. **Basic Functions:** Implement functions for addition, subtraction, multiplication, and division. 2. **Docstrings with Doctests:** Write docstrings with doctests for each function. 3. **Custom Output Checker:** Implement a custom output checker that considers trailing newlines as acceptable in the output. 4. **Integration with `unittest`:** Create a `unittest` test suite to run the doctests. 5. **Script to Run Tests:** Ensure the module can be run as a script to execute all doctests. **Code Requirements:** 1. **Addition Function:** ```python def add(a, b): Returns the sum of `a` and `b`. >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(1.5, 2.5) 4.0 ``` 2. **Subtraction Function:** ```python def subtract(a, b): Returns the difference of `a` and `b`. >>> subtract(5, 3) 2 >>> subtract(2, 5) -3 >>> subtract(5.5, 2.0) 3.5 ``` 3. **Multiplication Function:** ```python def multiply(a, b): Returns the product of `a` and `b`. >>> multiply(3, 4) 12 >>> multiply(-2, 3) -6 >>> multiply(2.5, 2) 5.0 ``` 4. **Division Function:** ```python def divide(a, b): Returns the division of `a` by `b`. Raises ValueError if `b` is zero. >>> divide(6, 3) 2.0 >>> divide(5, 2) 2.5 >>> divide(1, 0) Traceback (most recent call last): ... ValueError: division by zero ``` 5. **Custom Output Checker:** Implement a custom `OutputChecker` that allows for trailing newlines in the expected output. 6. **Integration with `unittest`:** Create a `unittest` test suite to include all doctest tests. 7. **Script Execution:** Ensure that running `python math_operations.py` executes all doctests. **Example Implementation:** ```python import doctest import unittest def add(a, b): Returns the sum of `a` and `b`. >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(1.5, 2.5) 4.0 return a + b def subtract(a, b): Returns the difference of `a` and `b`. >>> subtract(5, 3) 2 >>> subtract(2, 5) -3 >>> subtract(5.5, 2.0) 3.5 return a - b def multiply(a, b): Returns the product of `a` and `b`. >>> multiply(3, 4) 12 >>> multiply(-2, 3) -6 >>> multiply(2.5, 2) 5.0 return a * b def divide(a, b): Returns the division of `a` by `b`. Raises ValueError if `b` is zero. >>> divide(6, 3) 2.0 >>> divide(5, 2) 2.5 >>> divide(1, 0) Traceback (most recent call last): ... ValueError: division by zero if b == 0: raise ValueError(\\"division by zero\\") return a / b class TrailingNewlineOutputChecker(doctest.OutputChecker): def check_output(self, want, got, optionflags): if want.rstrip() == got.rstrip(): return True return super().check_output(want, got, optionflags) def load_tests(loader, tests, ignore): tests.addTests(doctest.DocTestSuite(checker=TrailingNewlineOutputChecker())) return tests if __name__ == \\"__main__\\": import sys if len(sys.argv) > 1 and sys.argv[1] == \'unittest\': unittest.main() else: doctest.testmod(optionflags=doctest.REPORT_ONLY_FIRST_FAILURE) ``` **Constraints:** - Ensure compatibility with Python 3.10. - Adhere to proper coding standards and practices. - Make sure all examples provided in the docstrings are covered and correct. **Submission:** Submit the `math_operations.py` file with all the requirements implemented. Make sure the doctests can be run by executing the file, and the `unittest` integration runs correctly when specified.","solution":"import doctest import unittest def add(a, b): Returns the sum of `a` and `b`. >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(1.5, 2.5) 4.0 return a + b def subtract(a, b): Returns the difference of `a` and `b`. >>> subtract(5, 3) 2 >>> subtract(2, 5) -3 >>> subtract(5.5, 2.0) 3.5 return a - b def multiply(a, b): Returns the product of `a` and `b`. >>> multiply(3, 4) 12 >>> multiply(-2, 3) -6 >>> multiply(2.5, 2) 5.0 return a * b def divide(a, b): Returns the division of `a` by `b`. Raises ValueError if `b` is zero. >>> divide(6, 3) 2.0 >>> divide(5, 2) 2.5 >>> divide(1, 0) Traceback (most recent call last): ... ValueError: division by zero if b == 0: raise ValueError(\\"division by zero\\") return a / b class TrailingNewlineOutputChecker(doctest.OutputChecker): def check_output(self, want, got, optionflags): if want.rstrip() == got.rstrip(): return True return super().check_output(want, got, optionflags) def load_tests(loader, tests, ignore): tests.addTests(doctest.DocTestSuite(checker=TrailingNewlineOutputChecker())) return tests if __name__ == \\"__main__\\": import sys if len(sys.argv) > 1 and sys.argv[1] == \'unittest\': unittest.main() else: doctest.testmod(optionflags=doctest.REPORT_ONLY_FIRST_FAILURE)"},{"question":"**Objective:** Demonstrate your understanding of scikit-learn\'s dataset generation functions and apply relevant machine learning algorithms. **Question:** You are required to design a synthetic dataset for a classification problem using scikit-learn\'s dataset generation functions, and then evaluate the performance of a classification algorithm on this dataset. **Task:** 1. **Dataset Generation:** - Create a synthetic dataset using `make_classification` with the following parameters: - `n_samples=1000` (the number of samples) - `n_features=20` (the total number of features) - `n_informative=10` (the number of informative features) - `n_redundant=5` (the number of redundant features) - `n_classes=2` (the number of classes) - `random_state=42` (for reproducibility) 2. **Data Splitting:** - Split the generated dataset into training and test sets with a 70-30 split. 3. **Model Training and Evaluation:** - Train a logistic regression classifier on the training set. - Evaluate the model\'s performance on the test set using accuracy, precision, recall, and F1-score as metrics. 4. **Visualization:** - Plot the first two principal components of the dataset colored by their true class labels. - Plot a confusion matrix for the model\'s performance on the test set. **Input and Output Requirements:** - **Input:** No input required, as the dataset will be generated within the code. - **Output:** The code should output the following: 1. The accuracy, precision, recall, and F1-score of the logistic regression classifier on the test set. 2. A plot of the first two principal components of the dataset colored by their true class labels. 3. A confusion matrix plot for the model\'s performance on the test set. **Constraints and Limitations:** - Use only the libraries `scikit-learn`, `numpy`, `matplotlib`, and `seaborn`. - Ensure that the random state is set to 42 wherever applicable to guarantee reproducibility. **Performance Requirements:** - The code should execute within a reasonable time frame (under 10 seconds on a standard machine). ```python # Your implementation here import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix from sklearn.decomposition import PCA # Step 1: Dataset Generation X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, n_classes=2, random_state=42) # Step 2: Data Splitting X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Model Training and Evaluation model = LogisticRegression(random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) # Metrics Calculation accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) print(f\\"Accuracy: {accuracy:.2f}\\") print(f\\"Precision: {precision:.2f}\\") print(f\\"Recall: {recall:.2f}\\") print(f\\"F1 Score: {f1:.2f}\\") # Step 4: Visualization pca = PCA(n_components=2) X_pca = pca.fit_transform(X) plt.figure(figsize=(10, 5)) # Plotting PCA components plt.subplot(1, 2, 1) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=70) plt.xlabel(\'First Principal Component\') plt.ylabel(\'Second Principal Component\') plt.title(\'PCA - First two principal components\') # Plotting Confusion Matrix conf_mat = confusion_matrix(y_test, y_pred) sns.heatmap(conf_mat, annot=True, fmt=\'d\', cmap=\'YlGnBu\', xticklabels=[\'Class 0\', \'Class 1\'], yticklabels=[\'Class 0\', \'Class 1\']) plt.xlabel(\'Predicted\') plt.ylabel(\'Actual\') plt.title(\'Confusion Matrix\') plt.tight_layout() plt.show() ```","solution":"import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix from sklearn.decomposition import PCA def generate_dataset(): Generates a synthetic dataset for classification task. X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, n_classes=2, random_state=42) return X, y def split_dataset(X, y): Splits the dataset into training and testing sets. return train_test_split(X, y, test_size=0.3, random_state=42) def train_and_evaluate(X_train, y_train, X_test, y_test): Trains a logistic regression model and evaluates its performance on the test set. model = LogisticRegression(random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) # Metrics Calculation accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) return accuracy, precision, recall, f1, y_pred def plot_pca_and_confusion_matrix(X, y, y_test, y_pred): Plots PCA components and confusion matrix. pca = PCA(n_components=2) X_pca = pca.fit_transform(X) plt.figure(figsize=(15, 5)) # Plotting PCA components plt.subplot(1, 2, 1) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=70) plt.xlabel(\'First Principal Component\') plt.ylabel(\'Second Principal Component\') plt.title(\'PCA - First two principal components\') # Plotting Confusion Matrix plt.subplot(1, 2, 2) conf_mat = confusion_matrix(y_test, y_pred) sns.heatmap(conf_mat, annot=True, fmt=\'d\', cmap=\'YlGnBu\', xticklabels=[\'Class 0\', \'Class 1\'], yticklabels=[\'Class 0\', \'Class 1\']) plt.xlabel(\'Predicted\') plt.ylabel(\'Actual\') plt.title(\'Confusion Matrix\') plt.tight_layout() plt.show() # Main Execution X, y = generate_dataset() X_train, X_test, y_train, y_test = split_dataset(X, y) accuracy, precision, recall, f1, y_pred = train_and_evaluate(X_train, y_train, X_test, y_test) print(f\\"Accuracy: {accuracy:.2f}\\") print(f\\"Precision: {precision:.2f}\\") print(f\\"Recall: {recall:.2f}\\") print(f\\"F1 Score: {f1:.2f}\\") plot_pca_and_confusion_matrix(X, y, y_test, y_pred)"},{"question":"**Objective:** Demonstrate proficiency in using Pythonâ€™s `runpy` module to dynamically execute modules and scripts with customized execution contexts. **Problem Statement:** You are tasked with creating a utility function `execute_script_or_module` that can dynamically run a Python script or module, adjust its execution environment, and return the results of its global state. The function should be able to handle both module names and file paths and allow pre-defined global variables to be injected into the execution environment. **Function Signature:** ```python def execute_script_or_module(identifier: str, is_module: bool, init_globals: dict = None) -> dict: Executes a Python module or script, pre-populating its global dictionary and returning the resulting globals. :param identifier: The module name (if `is_module` is True) or the file path (if `is_module` is False) :param is_module: If True, `identifier` is treated as a module name; if False, as a file path. :param init_globals: Optional dictionary to pre-populate the module or script\'s global variables. :return: Dictionary representing the globals of the executed module or script. pass ``` **Input:** 1. `identifier`: A string representing the module name (if `is_module` is True) or the file path (if `is_module` is False). 2. `is_module`: A boolean specifying whether `identifier` is a module name or a file path. 3. `init_globals`: An optional dictionary with initial global variables for the execution environment. **Output:** - A dictionary representing the global variables of the executed module or script. **Constraints:** - The function must handle Python source files and compiled bytecode files. - The function must correctly adjust the `sys` module when executing. - Throw an appropriate exception if an error occurs in the execution. **Examples:** ```python # Example 1: Running a module globals_dict = execute_script_or_module(\'example_module\', is_module=True, init_globals={\'start_value\': 42}) print(globals_dict) # Example 2: Running a script globals_dict = execute_script_or_module(\'/path/to/example_script.py\', is_module=False, init_globals={\'start_value\': 42}) print(globals_dict) ``` *Example script or module (assume the same content for both purposes):* ```python # content of example_module.py or example_script.py result = start_value + 58 ``` *Expected output:* ```python # For both examples above: {\'start_value\': 42, \'result\': 100, \'__name__\': ..., \'__file__\': ..., ...} ``` **Note:** - The exact output will include additional standard global variables like `__name__`, `__file__`, etc., set by the `runpy` module functions. Test your function to ensure it behaves correctly for both modules and scripts. Handle any exceptions gracefully, providing informative error messages.","solution":"import runpy def execute_script_or_module(identifier: str, is_module: bool, init_globals: dict = None) -> dict: Executes a Python module or script, pre-populating its global dictionary and returning the resulting globals. :param identifier: The module name (if `is_module` is True) or the file path (if `is_module` is False) :param is_module: If True, `identifier` is treated as a module name; if False, as a file path. :param init_globals: Optional dictionary to pre-populate the module or script\'s global variables. :return: Dictionary representing the globals of the executed module or script. if init_globals is None: init_globals = {} try: if is_module: # Run the module and get the globals dictionary result_globals = runpy.run_module(identifier, init_globals=init_globals, run_name=\\"__main__\\") else: # Run the script and get the globals dictionary result_globals = runpy.run_path(identifier, init_globals=init_globals, run_name=\\"__main__\\") return result_globals except Exception as e: raise RuntimeError(f\\"Error executing {\'module\' if is_module else \'script\'} {identifier}: {e}\\") # Example usage: # globals_dict = execute_script_or_module(\'example_module\', is_module=True, init_globals={\'start_value\': 42}) # print(globals_dict) # globals_dict = execute_script_or_module(\'/path/to/example_script.py\', is_module=False, init_globals={\'start_value\': 42}) # print(globals_dict)"},{"question":"**Objective**: You are required to implement a small persistent database to manage user information using the `shelve` module in Python. Your implementation should provide both command-line and programmatic interfaces to interact with the database. **Instructions**: 1. Create a Python module called `userdb.py`. 2. Implement the following functionalities within the module: - **Add User**: Adds a new user with given details. - **Get User**: Retrieves user details by username. - **Update User**: Updates details of an existing user. - **Delete User**: Removes a user from the database. - **List Users**: Returns a list of all usernames in the database. 3. Ensure the database is gracefully closed after any operation using a context manager. 4. Implement basic concurrency control to ensure that no two processes can write to the database simultaneously. **Function Specifications**: ```python def add_user(db_path: str, username: str, user_data: dict) -> None: Adds a user to the database. Parameters: - db_path (str): Path to the database file. - username (str): Unique username for the user. - user_data (dict): Dictionary containing user details. Raises: - ValueError if the user already exists. pass def get_user(db_path: str, username: str) -> dict: Retrieves user details from the database. Parameters: - db_path (str): Path to the database file. - username (str): Username of the user to retrieve. Returns: - dict: User details. Raises: - KeyError if the user does not exist. pass def update_user(db_path: str, username: str, updated_data: dict) -> None: Updates details of an existing user in the database. Parameters: - db_path (str): Path to the database file. - username (str): Username of the user to update. - updated_data (dict): Updated user details. Raises: - KeyError if the user does not exist. pass def delete_user(db_path: str, username: str) -> None: Deletes a user from the database. Parameters: - db_path (str): Path to the database file. - username (str): Username of the user to delete. Raises: - KeyError if the user does not exist. pass def list_users(db_path: str) -> list: Returns a list of all usernames in the database. Parameters: - db_path (str): Path to the database file. Returns: - list: List of usernames. pass ``` **Constraints**: - The `user_data` and `updated_data` dictionaries can have arbitrary content but should at least contain keys like \'name\', \'email\', and \'age\'. - Operations should handle file locks to prevent concurrent write operations. **Example Usage**: ```python import userdb db_path = \'user_database\' # Add a user userdb.add_user(db_path, \'johndoe\', {\'name\': \'John Doe\', \'email\': \'john@example.com\', \'age\': 30}) # Get user details print(userdb.get_user(db_path, \'johndoe\')) # Output: {\'name\': \'John Doe\', \'email\': \'john@example.com\', \'age\': 30} # Update user information userdb.update_user(db_path, \'johndoe\', {\'age\': 31}) # Updated \'age\' key # List usernames print(userdb.list_users(db_path)) # Output: [\'johndoe\'] # Delete a user userdb.delete_user(db_path, \'johndoe\') ``` **Considerations**: - Ensure you handle file locks appropriately to prevent race conditions. - Use a context manager to guarantee that the database is closed properly after operations. - Thoroughly test your implementation, including edge cases like trying to get or delete a non-existent user.","solution":"import shelve import os from threading import Lock lock = Lock() def add_user(db_path: str, username: str, user_data: dict) -> None: Adds a user to the database. with lock, shelve.open(db_path, writeback=True) as db: if username in db: raise ValueError(\\"User already exists.\\") db[username] = user_data def get_user(db_path: str, username: str) -> dict: Retrieves user details from the database. with lock, shelve.open(db_path) as db: if username not in db: raise KeyError(\\"User does not exist.\\") return db[username] def update_user(db_path: str, username: str, updated_data: dict) -> None: Updates details of an existing user in the database. with lock, shelve.open(db_path, writeback=True) as db: if username not in db: raise KeyError(\\"User does not exist.\\") db[username].update(updated_data) def delete_user(db_path: str, username: str) -> None: Deletes a user from the database. with lock, shelve.open(db_path) as db: if username not in db: raise KeyError(\\"User does not exist.\\") del db[username] def list_users(db_path: str) -> list: Returns a list of all usernames in the database. with lock, shelve.open(db_path) as db: return list(db.keys())"},{"question":"# Coding Assessment: Custom Command Interpreter using `cmd` Module **Objective**: Create a custom command interpreter using the `cmd` module that can manage a simple to-do list. # Problem Statement In this task, you will create a command-line based To-Do List application using the `cmd` module. The application should support adding tasks, listing all tasks, marking tasks as completed, and deleting tasks. The user should be able to interact with the application using specific commands. # Requirements 1. **Class Definition**: Define a class `TodoShell` that extends the `cmd.Cmd` class. 2. **Commands**: - `add TASK_DESCRIPTION`: Adds a new task to the to-do list. - `list`: Lists all tasks with their status (completed or not). - `complete TASK_ID`: Marks a task as completed by its ID. - `delete TASK_ID`: Deletes a task by its ID. - `exit`: Exits the command interpreter. 3. **Command Method Implementation**: - Implement a method for each command using the `do_<command>()` naming convention. 4. **Hooks**: - Override the `precmd()` method to strip whitespace and convert the input to lowercase before processing. - Override the `emptyline()` method to prevent executing the last command when an empty line is entered. # Input and Output - **Input**: The user will interact with the application through the command-line. - **Output**: The application should print the current status of the to-do list based on the commands issued. # Constraints - Task IDs are zero-based indices based on the order of tasks as they are added. - Use instance variable `self.tasks` to maintain the list of tasks, each being a dictionary with keys \'description\' and \'completed\'. - Provide helpful messages for each action, e.g., `\\"Task added successfully.\\"`, `\\"Invalid Task ID.\\"`, etc. # Example Session ``` Welcome to the To-Do List app. Type help or ? to list commands. (to-do) add Finish homework Task added successfully. (to-do) add Buy groceries Task added successfully. (to-do) list 0: Finish homework [Incomplete] 1: Buy groceries [Incomplete] (to-do) complete 0 Task marked as completed. (to-do) list 0: Finish homework [Completed] 1: Buy groceries [Incomplete] (to-do) delete 1 Task deleted successfully. (to-do) list 0: Finish homework [Completed] (to-do) exit Thank you for using the To-Do List app. ``` # Implementation ```python import cmd class TodoShell(cmd.Cmd): intro = \'Welcome to the To-Do List app. Type help or ? to list commands.n\' prompt = \'(to-do) \' def __init__(self): super().__init__() self.tasks = [] def do_add(self, arg): \'Add a new task to the to-do list: add TASK_DESCRIPTION\' task = {\'description\': arg, \'completed\': False} self.tasks.append(task) print(\'Task added successfully.\') def do_list(self, arg): \'List all tasks with their status: list\' for idx, task in enumerate(self.tasks): status = \'[Completed]\' if task[\'completed\'] else \'[Incomplete]\' print(f\\"{idx}: {task[\'description\']} {status}\\") def do_complete(self, arg): \'Mark a task as completed by ID: complete TASK_ID\' try: task_id = int(arg) if 0 <= task_id < len(self.tasks): self.tasks[task_id][\'completed\'] = True print(\'Task marked as completed.\') else: print(\'Invalid Task ID.\') except ValueError: print(\'Invalid Task ID.\') def do_delete(self, arg): \'Delete a task by ID: delete TASK_ID\' try: task_id = int(arg) if 0 <= task_id < len(self.tasks): self.tasks.pop(task_id) print(\'Task deleted successfully.\') else: print(\'Invalid Task ID.\') except ValueError: print(\'Invalid Task ID.\') def do_exit(self, arg): \'Exit the To-Do List app: exit\' print(\'Thank you for using the To-Do List app.\') return True def precmd(self, line): return line.strip().lower() def emptyline(self): pass if __name__ == \'__main__\': TodoShell().cmdloop() ```","solution":"import cmd class TodoShell(cmd.Cmd): intro = \'Welcome to the To-Do List app. Type help or ? to list commands.n\' prompt = \'(to-do) \' def __init__(self): super().__init__() self.tasks = [] def do_add(self, arg): \'Add a new task to the to-do list: add TASK_DESCRIPTION\' task = {\'description\': arg, \'completed\': False} self.tasks.append(task) print(\'Task added successfully.\') def do_list(self, arg): \'List all tasks with their status: list\' for idx, task in enumerate(self.tasks): status = \'[Completed]\' if task[\'completed\'] else \'[Incomplete]\' print(f\\"{idx}: {task[\'description\']} {status}\\") def do_complete(self, arg): \'Mark a task as completed by ID: complete TASK_ID\' try: task_id = int(arg) if 0 <= task_id < len(self.tasks): self.tasks[task_id][\'completed\'] = True print(\'Task marked as completed.\') else: print(\'Invalid Task ID.\') except ValueError: print(\'Invalid Task ID.\') def do_delete(self, arg): \'Delete a task by ID: delete TASK_ID\' try: task_id = int(arg) if 0 <= task_id < len(self.tasks): self.tasks.pop(task_id) print(\'Task deleted successfully.\') else: print(\'Invalid Task ID.\') except ValueError: print(\'Invalid Task ID.\') def do_exit(self, arg): \'Exit the To-Do List app: exit\' print(\'Thank you for using the To-Do List app.\') return True def precmd(self, line): return line.strip().lower() def emptyline(self): pass if __name__ == \'__main__\': TodoShell().cmdloop()"},{"question":"# PyTorch Mobile Optimization Challenge # Objective: You are tasked with designing and optimizing a PyTorch model for mobile inference. Specifically, you will create a PyTorch `torch.jit.ScriptModule` containing some common neural network operations, and then use `torch.utils.mobile_optimizer.optimize_for_mobile` to optimize this model for mobile deployment. # Instructions: 1. **Create a PyTorch `torch.jit.ScriptModule`:** - Define a small neural network that includes layers such as `Conv2D`, `BatchNorm2D`, `ReLU`, `Linear`, `Dropout`, and `Hardtanh`. - Use the `torch.jit.script` decorator or other means to convert this model into a `torch.jit.ScriptModule`. 2. **Optimize the ScriptModule:** - Use the `torch.utils.mobile_optimizer.optimize_for_mobile` function to optimize the created ScriptModule. - Experiment with blocklisting different optimization types to observe how the model changes. 3. **Verify the Optimizations:** - After optimization, compare the original and optimized ScriptModules to verify that the optimizations have taken effect. You should look for changes such as fusion of operations and removal of dropout layers. # Guidelines: 1. **Model Definition:** - The model should include at least one `Conv2D + BatchNorm2d` sequence, followed by a `ReLU` or `Hardtanh`, and a `Dropout` layer. - Example input for testing the model: A tensor of shape (1, 3, 224, 224) representing a single image with 3 channels (RGB) and size 224x224. 2. **Optimization Application:** - Apply the default optimization, and then create a variation by blocklisting one or more optimizations. - Use both CPU and Vulkan backends and observe any differences. 3. **Output Verification:** - Print the model\'s properties before and after optimization. - You may use `assert` statements or other checks to confirm the optimizations, such as ensuring dropout layers are removed or verifying fused operations. # Expected Input and Output: - **Input:** ```python model = MyMobileOptimizedModel() optimized_model = optimize_model(model) ``` - **Output:** Print statements or assertions verifying the model optimizations. # Code Template: ```python import torch import torch.nn as nn import torch.utils.mobile_optimizer as mobile_optimizer class MyMobileOptimizedModel(nn.Module): def __init__(self): super(MyMobileOptimizedModel, self).__init__() self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) self.bn = nn.BatchNorm2d(16) self.relu = nn.ReLU() self.hardtanh = nn.Hardtanh() self.fc = nn.Linear(16 * 224 * 224, 10) self.dropout = nn.Dropout(0.5) def forward(self, x): x = self.conv(x) x = self.bn(x) x = self.relu(x) # Optionally: replace with self.hardtanh(x) x = self.dropout(x) x = x.view(x.size(0), -1) # Flatten x = self.fc(x) return x # Function to convert the model to ScriptModule and optimize it def optimize_model(model): scripted_model = torch.jit.script(model) optimized_model = mobile_optimizer.optimize_for_mobile(scripted_model) return optimized_model # Example usage if __name__ == \\"__main__\\": model = MyMobileOptimizedModel() optimized_model = optimize_model(model) print(\\"Original model:\\") print(model) print(\\"Optimized model:\\") print(optimized_model) ```","solution":"import torch import torch.nn as nn import torch.utils.mobile_optimizer as mobile_optimizer class MyMobileOptimizedModel(nn.Module): def __init__(self): super(MyMobileOptimizedModel, self).__init__() self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) self.bn = nn.BatchNorm2d(16) self.relu = nn.ReLU() self.fc = nn.Linear(16 * 224 * 224, 10) self.dropout = nn.Dropout(0.5) def forward(self, x): x = self.conv(x) x = self.bn(x) x = self.relu(x) x = self.dropout(x) x = x.view(x.size(0), -1) # Flatten x = self.fc(x) return x # Function to convert the model to ScriptModule and optimize it def optimize_model(model): scripted_model = torch.jit.script(model) optimized_model = mobile_optimizer.optimize_for_mobile(scripted_model) return optimized_model # Example usage if __name__ == \\"__main__\\": model = MyMobileOptimizedModel() optimized_model = optimize_model(model) print(\\"Original model:\\") print(model) print(\\"Optimized model:\\") print(optimized_model)"},{"question":"# Task: You are given functions to check and manipulate tuples, as well as to create and manage struct sequence objects in Python. Your task is to implement a function that performs the following operations and returns the results: 1. **Create a Tuple**: - Generate a tuple consisting of n elements, where each element is equal to its index (e.g., for n = 5, the tuple should be (0, 1, 2, 3, 4)). 2. **Slice the Tuple**: - Retrieve a slice from the tuple created in step 1, from index `low` to index `high`. 3. **Create a Struct Sequence**: - Define a struct sequence with the fields \\"field1\\", \\"field2\\", and \\"field3\\". - Create an instance of this struct sequence with values corresponding to the first three elements of the sliced tuple from step 2. # Implementation Specifications: Function Signature: ```python def tuple_and_struct_sequence_operations(n: int, low: int, high: int) -> dict: ``` Parameters: - `n` (int): The size of the tuple to create. - `low` (int): The starting index for slicing the tuple. - `high` (int): The ending index for slicing the tuple. Output: - A dictionary with the following keys: - `\\"created_tuple\\"`: The tuple created in step 1. - `\\"sliced_tuple\\"`: The slice of the tuple obtained in step 2. - `\\"struct_sequence\\"`: A dictionary representing the struct sequence created in step 3, with keys \\"field1\\", \\"field2\\", and \\"field3\\". Constraints: - Assume `0 <= low <= high <= n`. - If the slice operation in step 2 results in an empty slice, handle it gracefully by assigning `None` to the fields in the struct sequence. Example Usage: ```python result = tuple_and_struct_sequence_operations(5, 1, 4) # result should be: # { # \\"created_tuple\\": (0, 1, 2, 3, 4), # \\"sliced_tuple\\": (1, 2, 3), # \\"struct_sequence\\": { # \\"field1\\": 1, # \\"field2\\": 2, # \\"field3\\": 3 # } # } ``` # Note: This task emphasizes the understanding of tuples and struct sequences in Python, focusing on their creation and manipulation.","solution":"from collections import namedtuple def tuple_and_struct_sequence_operations(n: int, low: int, high: int) -> dict: # Step 1: Create a tuple created_tuple = tuple(range(n)) # Step 2: Slice the tuple sliced_tuple = created_tuple[low:high] # Step 3: Create a struct sequence StructSequence = namedtuple(\'StructSequence\', [\'field1\', \'field2\', \'field3\']) if len(sliced_tuple) >= 3: struct_sequence_instance = StructSequence(sliced_tuple[0], sliced_tuple[1], sliced_tuple[2]) else: # Handle the case with less than 3 elements in the sliced tuple values = [None, None, None] for i in range(min(3, len(sliced_tuple))): values[i] = sliced_tuple[i] struct_sequence_instance = StructSequence(*values) # Return the results as a dictionary return { \\"created_tuple\\": created_tuple, \\"sliced_tuple\\": sliced_tuple, \\"struct_sequence\\": struct_sequence_instance._asdict() # Convert named tuple to dictionary }"},{"question":"Objective Create an advanced interactive command-line prompt that uses features from the `readline` module to enhance user experience. The prompt should be able to handle command history, support custom tab completion, and save the session history to a file. Task 1. Implement an interactive command-line prompt that: - Loads previous command history from a file at startup. - Saves the command history to a file when exiting. - Supports custom word completion for commands. - Limits the saved history file to the most recent 500 commands. 2. Define two commands (`hello` and `exit`) to be supported by the prompt: - `hello`: When typed, the prompt should respond with \\"Hello, User!\\" - `exit`: Terminates the interactive session and saves the history. Requirements - Use the `readline` module to handle line buffer and history functionalities. - Use the `atexit` module to ensure the history is saved when the program exits. - Implement a custom completer function to suggest the `hello` and `exit` commands when the user presses tab. Function Skeleton ```python import readline import atexit import os def setup_readline(history_file, commands): # Initialize history try: readline.read_history_file(history_file) except FileNotFoundError: open(history_file, \'wb\').close() readline.set_history_length(500) readline.parse_and_bind(\\"tab: complete\\") def completer(text, state): options = [cmd for cmd in commands if cmd.startswith(text)] return options[state] if state < len(options) else None readline.set_completer(completer) atexit.register(readline.write_history_file, history_file) def run_prompt(): history_file = os.path.expanduser(\\"~/.my_prompt_history\\") commands = [\\"hello\\", \\"exit\\"] setup_readline(history_file, commands) while True: try: line = input(\\">>> \\") if line == \\"hello\\": print(\\"Hello, User!\\") elif line == \\"exit\\": break except EOFError: break except KeyboardInterrupt: print(\\"nSession interrupted. Use \'exit\' command to quit.\\") run_prompt() ``` Constraints - Make sure the prompt is user-friendly and handles potential errors gracefully. - Ensure the history file does not grow indefinitely by capping it to the latest 500 commands. Sample Interaction ```plaintext >>> he [press Tab] hello >>> hello Hello, User! >>> ex [press Tab] exit >>> exit ``` Develop the above interactive command-line prompt and test it thoroughly.","solution":"import readline import atexit import os def setup_readline(history_file, commands): # Initialize history try: readline.read_history_file(history_file) except FileNotFoundError: open(history_file, \'wb\').close() readline.set_history_length(500) readline.parse_and_bind(\\"tab: complete\\") def completer(text, state): options = [cmd for cmd in commands if cmd.startswith(text)] return options[state] if state < len(options) else None readline.set_completer(completer) atexit.register(readline.write_history_file, history_file) def run_prompt(): history_file = os.path.expanduser(\\"~/.my_prompt_history\\") commands = [\\"hello\\", \\"exit\\"] setup_readline(history_file, commands) while True: try: line = input(\\">>> \\") if line == \\"hello\\": print(\\"Hello, User!\\") elif line == \\"exit\\": break except EOFError: break except KeyboardInterrupt: print(\\"nSession interrupted. Use \'exit\' command to quit.\\")"},{"question":"Objective: Implement a command-line utility using the `argparse` module that provides functionalities to manage and display information about files in a directory. Problem Statement: Create a Python script named `file_manager.py` that performs the following operations based on the command-line arguments: 1. **List Files**: List all files in the specified directory. 2. **Count Files**: Display the count of files in the specified directory. 3. **File Details**: Display detailed information (name, size, and modification date) of each file in the specified directory. 4. **Search File**: Search for a file by name in the specified directory and display its details if found. Command-Line Arguments: 1. `--action` or `-a`: This argument should specify the action to be performed. It can have one of the following values: `list`, `count`, `details`, or `search`. 2. `--directory` or `-d`: This argument should specify the directory in which the action should be performed. It should default to the current working directory if not specified. 3. `--name` or `-n`: This argument should specify the name of the file to search for. It is only required if the action is `search`. Output Formats: - **List Files**: Print each file name on a new line. - **Count Files**: Print the count of files. - **File Details**: Print each file\'s information in the format: `Name: <file name>, Size: <file size> bytes, Modified: <modification date>`. - **Search File**: Print the file\'s information if found, otherwise print `File not found`. Constraints: - Only files should be considered, ignore directories within the specified directory. - The script should handle exceptions gracefully, such as permission errors or non-existent directories, by printing appropriate error messages and exiting. Example Usage: ```sh python file_manager.py --action list --directory /path/to/directory python file_manager.py -a count -d /path/to/directory python file_manager.py -a details -d /path/to/directory python file_manager.py -a search -d /path/to/directory -n filename.txt ``` Implementation Tips: - Use the `argparse` module to parse the command-line arguments. - Use the `os` and `time` modules to interact with the file system and retrieve file details. Expected Function Signature: ```python import argparse import os import time def main(): # Your code here if __name__ == \\"__main__\\": main() ```","solution":"import argparse import os import time def list_files(directory): try: files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))] for file in files: print(file) except Exception as e: print(f\\"Error: {e}\\") def count_files(directory): try: files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))] print(f\\"File count: {len(files)}\\") except Exception as e: print(f\\"Error: {e}\\") def file_details(directory): try: files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))] for file in files: file_path = os.path.join(directory, file) file_size = os.path.getsize(file_path) file_mtime = os.path.getmtime(file_path) mod_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(file_mtime)) print(f\\"Name: {file}, Size: {file_size} bytes, Modified: {mod_time}\\") except Exception as e: print(f\\"Error: {e}\\") def search_file(directory, name): try: files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))] for file in files: if file == name: file_path = os.path.join(directory, file) file_size = os.path.getsize(file_path) file_mtime = os.path.getmtime(file_path) mod_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(file_mtime)) print(f\\"Name: {file}, Size: {file_size} bytes, Modified: {mod_time}\\") return print(\\"File not found\\") except Exception as e: print(f\\"Error: {e}\\") def main(): parser = argparse.ArgumentParser(description=\\"Manage and display information about files in a directory.\\") parser.add_argument(\\"-a\\", \\"--action\\", required=True, choices=[\\"list\\", \\"count\\", \\"details\\", \\"search\\"], help=\\"Action to perform\\") parser.add_argument(\\"-d\\", \\"--directory\\", default=\\".\\", help=\\"Directory to perform the action in\\") parser.add_argument(\\"-n\\", \\"--name\\", help=\\"Name of the file to search for (required for search action)\\") args = parser.parse_args() if args.action == \\"list\\": list_files(args.directory) elif args.action == \\"count\\": count_files(args.directory) elif args.action == \\"details\\": file_details(args.directory) elif args.action == \\"search\\": if not args.name: print(\\"Error: --name is required when action is \'search\'\\") return search_file(args.directory, args.name) if __name__ == \\"__main__\\": main()"},{"question":"# Seaborn Coding Assessment Question **Objective:** Create a visualization that demonstrates your understanding of loading data, manipulating it, and creating complex plots using Seaborn. This task will assess your ability to work with Seaborn\'s advanced plotting functionalities, including creating plots with multiple layers. **Task:** 1. Load the `titanic` dataset using Seaborn. 2. Perform data manipulation to calculate the average fare paid by passengers for each class and each embarkation point. 3. Create a band plot that visualizes the range of average fares paid by passengers for each class across different embarkation points. **Input:** - The `titanic` dataset included in Seaborn. **Output:** - A Seaborn plot that contains: - X-axis representing the embarkation points (`embarked`). - Y-axis representing the range of average fares (`fare`). - Different colors for different passenger classes (`class`). - A band to show the range of average fares for each class at each embarkation point. - A line to show the average fare for each class at each embarkation point. **Constraints:** - You must use Seaborn\'s `so.Plot` and relevant methods to create the plot. - Do not use any other plotting libraries (e.g., Matplotlib directly). **Steps:** 1. Load the `titanic` dataset using `seaborn.load_dataset(\'titanic\')`. 2. Group and aggregate the dataset to find the average fare for each class and each embarkation point. 3. Use `so.Plot` to set up the plot with the manipulated data. 4. Add a band and a line to the plot to show the fare range and average fare distinctly. ```python # Your code import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the Titanic dataset titanic = load_dataset(\'titanic\') # Step 2: Manipulate the data to find average fares # Hint: Use groupby and aggregation functions # Step 3: Create the plot # Hint: Set x = \'embarked\', ymin and ymax for the band, # color = \'class\', and y for the line plot # Step 4: Add a band and a line to the plot # Hint: use add so.Band() and so.Line() ``` **Performance Requirements:** - The code should execute without errors and produce the desired plot. - Ensure data manipulations and plotting are handled efficiently.","solution":"import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the Titanic dataset titanic = load_dataset(\'titanic\') # Step 2: Manipulate the data to find average fares by class and embarkation point import pandas as pd # Group by class and embarkation point, then calculate the average fare avg_fare = (titanic .groupby([\'class\', \'embarked\'], as_index=False) .agg(average_fare=(\'fare\', \'mean\'), fare_min=(\'fare\', \'min\'), fare_max=(\'fare\', \'max\'))) # Step 3 & 4: Create the plot, set x = \'embarked\', create band and line for average fare plot = ( so.Plot(avg_fare, x=\'embarked\', color=\'class\') .add(so.Band(), ymin=\'fare_min\', ymax=\'fare_max\') .add(so.Line(), y=\'average_fare\') ) plot.show()"},{"question":"Question: Implement and Extend a Simple WSGI Server # Objective The goal of this exercise is to implement a simple WSGI server using the `wsgiref.simple_server` module, and to perform a series of operations utilizing utilities provided in `wsgiref.util` for handling requests. This will test your understanding of WSGI concepts and your ability to apply the provided functions to manipulate server behavior. # Task 1. **Implement a Basic WSGI Application**: - Create a simple WSGI application that responds with \\"Hello, World!\\" when accessed. 2. **Enhance the Application with Path Handling**: - Modify the application so that it extracts the first segment of the request path (using `wsgiref.util.shift_path_info`) and displays it in the response. For example, if the path is `/foo/bar`, the application should extract and respond with \\"First Path Segment: foo\\". 3. **Integrate Request URI Handling**: - Utilize `wsgiref.util.request_uri` to include the full request URI in the response. 4. **Set Up and Run the WSGI Server**: - Set up the WSGI server using `wsgiref.simple_server.make_server` and make your application accessible at `http://localhost:8000`. # Specifications - The WSGI application must be implemented in a function called `my_app`. - The server setup and application binding must be implemented within the `if __name__ == \'__main__\':` block. - The application must handle and respond to requests using both the path and full URI. # Hints - Use `wsgiref.util.shift_path_info` to extract the first path segment. - Use `wsgiref.util.request_uri` to get the full request URI. # Submission Requirements - Your submission should contain a single script file with all the required logic. - The script file should be executable and should start the server when run. # Example Usage If your script is named `simple_wsgi_server.py`, running `python simple_wsgi_server.py` should start the server, which will handle requests as described. Example output when accessing `http://localhost:8000/foo/bar`: ``` Hello, World! First Path Segment: foo Full Request URI: http://localhost:8000/foo/bar ``` # Constraints - You must handle exceptions gracefully. - The server should log any errors to the console. # Evaluation Criteria - Correctness of the WSGI application implementation. - Proper usage of `wsgiref.util.shift_path_info` and `wsgiref.util.request_uri`. - Successful server setup and request handling. - Clean and readable code.","solution":"from wsgiref.simple_server import make_server from wsgiref.util import shift_path_info, request_uri def my_app(environ, start_response): A simple WSGI application that responds with \'Hello, World!\', the first path segment, and the full request URI. status = \'200 OK\' headers = [(\'Content-Type\', \'text/plain\')] start_response(status, headers) first_path_segment = shift_path_info(environ) full_request_uri = request_uri(environ) response_body = f\\"Hello, World!nFirst Path Segment: {first_path_segment}nFull Request URI: {full_request_uri}\\" return [response_body.encode(\'utf-8\')] if __name__ == \'__main__\': try: port = 8000 httpd = make_server(\'\', port, my_app) print(f\\"Serving on port {port}...\\") httpd.serve_forever() except Exception as e: print(f\\"Error occurred: {e}\\")"},{"question":"**Objective:** You are tasked with designing a script that utilizes Python\'s `runpy` module to dynamically execute various Python modules and scripts. This task will test your understanding of locating and running Python modules using the `runpy` functions. **Question:** 1. Write a Python function `execute_module(module_name: str, init_globals: dict = None, run_name: str = None) -> dict` that: - Uses `runpy.run_module`. - Accepts the name of a module as a string. - Optionally accepts a dictionary of global variables to initialize the module\'s global namespace. - Optionally accepts a string to be used for the moduleâ€™s `__name__` attribute. - Returns the globals dictionary resulting from executing the module. 2. Write a second Python function `execute_script(path_name: str, init_globals: dict = None, run_name: str = None) -> dict` that: - Uses `runpy.run_path`. - Accepts a file path to the script as a string. - Optionally accepts a dictionary of global variables to initialize the module\'s global namespace. - Optionally accepts a string to be used for the moduleâ€™s `__name__` attribute. - Returns the globals dictionary resulting from executing the script. **Constraints:** - The module or script being executed should print \\"Execution Successful\\" to indicate successful execution. - Handle any exceptions that occur during the execution and print a user-friendly error message. - Ensure that the script or module being executed adheres to the constraints of not modifying the provided `init_globals`. **Input & Output Format:** 1. `execute_module` - Input: - `module_name`: A string representing the absolute name of the module to execute. - `init_globals`: An optional dictionary of initial global variables (default is `None`). - `run_name`: An optional string for the moduleâ€™s `__name__` attribute (default is `None`). - Output: - Returns the resulting module globals dictionary. 2. `execute_script` - Input: - `path_name`: A string representing the file path to the script. - `init_globals`: An optional dictionary of initial global variables (default is `None`). - `run_name`: An optional string for the scriptâ€™s `__name__` attribute (default is `None`). - Output: - Returns the resulting module globals dictionary. **Evaluation:** Your solution will be evaluated on: - Correct implementation of both functions. - Proper use of `runpy.run_module` and `runpy.run_path`. - Handling of optional arguments and defaults. - Exception handling and user-friendly error messages. - Adhering to the given constraints and properly returning the globals dictionary. **Example:** ```python # Example usage globals_dict = execute_module(\'example_module\') print(globals_dict[\'result\']) # Should output: \\"Execution Successful\\" if the module outputs it. globals_dict = execute_script(\'/path/to/script.py\') print(globals_dict[\'result\']) # Should output: \\"Execution Successful\\" if the script outputs it. ``` Create the necessary module or script that your functions will execute to provide a complete demonstration.","solution":"import runpy import sys def execute_module(module_name: str, init_globals: dict = None, run_name: str = None) -> dict: Execute a module using runpy.run_module and return the resulting globals dictionary. :param module_name: Name of the module to be executed. :param init_globals: Dictionary of initial global variables (default is None). :param run_name: String for the moduleâ€™s __name__ attribute (default is None). :return: Dictionary of resulting globals after execution. try: result_globals = runpy.run_module(module_name, init_globals=init_globals, run_name=run_name) print(\\"Execution Successful\\") return result_globals except Exception as e: print(f\\"Error occurred while executing module \'{module_name}\': {e}\\") def execute_script(path_name: str, init_globals: dict = None, run_name: str = None) -> dict: Execute a script using runpy.run_path and return the resulting globals dictionary. :param path_name: File path to the script to be executed. :param init_globals: Dictionary of initial global variables (default is None). :param run_name: String for the scriptâ€™s __name__ attribute (default is None). :return: Dictionary of resulting globals after execution. try: result_globals = runpy.run_path(path_name, init_globals=init_globals, run_name=run_name) print(\\"Execution Successful\\") return result_globals except Exception as e: print(f\\"Error occurred while executing script \'{path_name}\': {e}\\")"},{"question":"# Custom Random Number Generator **Problem Statement**: You are tasked with implementing a custom random number generator class in Python that supports the generation of random numbers from both standard and user-defined distributions. Additionally, you will extend the functionality of this class to simulate a specific real-world problem: determining customer wait times in a multi-server queue system. **Requirement 1**: Implement the `CustomRandom` class. - The class should include methods to generate random numbers akin to those provided by the `random` module: - `random()`: Generate a float uniformly in the range [0.0, 1.0). - `randint(a, b)`: Generate a random integer N such that a <= N <= b. - `uniform(a, b)`: Generate a random float N such that a <= N <= b. - `normalvariate(mu, sigma)`: Generate a random float based on the normal (Gaussian) distribution with mean `mu` and standard deviation `sigma`. - Include a method `custom_distribution()` that allows for user-defined distributions. **Requirement 2**: Create a simulation function using the `CustomRandom` class. - Implement the `simulate_queue` function to simulate arrival and service times in a multi-server queue system with the following parameters: - `average_arrival_interval` (float): Average interval between customer arrivals. - `average_service_time` (float): Average service time per customer. - `stdev_service_time` (float): Standard deviation of the service time. - `num_servers` (int): Number of available servers. - `num_simulations` (int): Number of customers to simulate. The function should: 1. Use the appropriate methods from `CustomRandom` to generate inter-arrival times and service times. 2. Maintain a heap to efficiently manage server availability and customer wait times. 3. Calculate and return key statistics: mean wait time, maximum wait time, and quartiles of the wait times. **Function Signatures**: ```python class CustomRandom: def __init__(self, seed=None): pass def random(self) -> float: pass def randint(self, a: int, b: int) -> int: pass def uniform(self, a: float, b: float) -> float: pass def normalvariate(self, mu: float, sigma: float) -> float: pass def custom_distribution(self, dist_func) -> float: pass def simulate_queue(average_arrival_interval: float, average_service_time: float, stdev_service_time: float, num_servers: int, num_simulations: int) -> dict: pass ``` **Constraints**: - Ensure reproducibility by allowing seeding of the random number generator in the `CustomRandom` class. - Efficient handling of large simulations (up to 1,000,000 customers). - Consider edge cases, such as zero or negative arrival/service times which must be handled appropriately in the context of the simulation. **Sample Output**: The output of `simulate_queue` should be a dictionary with the following keys: - `\'mean_wait\'`: Mean wait time for all simulated customers. - `\'max_wait\'`: Maximum wait time encountered. - `\'quartiles\'`: List containing the quartiles (25th, 50th, 75th percentile waits). ```json { \\"mean_wait\\": 12.3, \\"max_wait\\": 58.7, \\"quartiles\\": [0.5, 4.2, 15.8] } ``` **Example**: ```python sim_result = simulate_queue(5.6, 15.0, 3.5, 3, 1000000) print(sim_result) ``` **Note**: The provided documentation for the `random` module should be referred to for function implementations. Use the practical examples and recipes as guidance for handling random number generation and simulations effectively.","solution":"import random import heapq import statistics from typing import Callable class CustomRandom: def __init__(self, seed=None): self.random_gen = random.Random(seed) def random(self) -> float: return self.random_gen.random() def randint(self, a: int, b: int) -> int: return self.random_gen.randint(a, b) def uniform(self, a: float, b: float) -> float: return self.random_gen.uniform(a, b) def normalvariate(self, mu: float, sigma: float) -> float: return self.random_gen.normalvariate(mu, sigma) def custom_distribution(self, dist_func: Callable) -> float: return dist_func(self.random_gen) def simulate_queue(average_arrival_interval: float, average_service_time: float, stdev_service_time: float, num_servers: int, num_simulations: int) -> dict: random_gen = CustomRandom() event_heap = [] wait_times = [] # Initialize servers\' availability servers = [0] * num_servers # All servers are initially available at time 0 for i in range(num_simulations): # Generate arrival and service times if i == 0: arrival_time = random_gen.normalvariate(average_arrival_interval, average_arrival_interval / 3) else: arrival_time += random_gen.normalvariate(average_arrival_interval, average_arrival_interval / 3) arrival_time = max(0, arrival_time) # Ensure non-negative arrival time service_time = random_gen.normalvariate(average_service_time, stdev_service_time) service_time = max(0, service_time) # Ensure non-negative service time # Check for the next available server server_available_time = heapq.heappop(servers) wait_time = max(0, server_available_time - arrival_time) finish_time = arrival_time + wait_time + service_time heapq.heappush(servers, finish_time) wait_times.append(wait_time) # Calculate statistics mean_wait = statistics.mean(wait_times) max_wait = max(wait_times) quartiles = [statistics.quantiles(wait_times, n=4)[i] for i in range(3)] return { \'mean_wait\': mean_wait, \'max_wait\': max_wait, \'quartiles\': quartiles }"},{"question":"Problem Statement You are required to implement a multiprocessing system using PyTorch\'s `torch.distributed.elastic.multiprocessing` package. Your task is to simulate a distributed training setup where multiple worker processes perform computations on separate portions of input data, and then aggregate the results. # Task 1. **Initialization**: - Write a function `compute_worker` that takes a numerical list segment and performs a simple computation on each element (e.g., square each number and return the list of squared numbers). 2. **Process Management**: - Use the `start_processes` function to start multiple worker processes. - Each subprocess should execute the `compute_worker` function on different segments of the input data. 3. **Aggregation**: - Collect results from all worker processes and merge them into a single list. # Specifications - Implement the function `distributed_computation` with the following signature: ```python def distributed_computation(data: List[int], num_workers: int) -> List[int]: ``` - `data`: a list of integers to be processed. - `num_workers`: the number of worker processes to spawn. - The function should divide `data` into `num_workers` parts as evenly as possible and spawn one process for each part. Use the `start_processes` function to manage and execute these processes. - Return a single list containing all results from the worker processes (in the order corresponding to the input data). # Example ```python from multiprocessing import reduction # For compatibility with Jupyter Notebook (ignore this line if not using Jupyter) def compute_worker(data_segment): return [x ** 2 for x in data_segment] def distributed_computation(data, num_workers): # Your implementation here pass data = [1, 2, 3, 4, 5, 6] num_workers = 2 print(distributed_computation(data, num_workers)) ``` **Expected Output**: ``` [1, 4, 9, 16, 25, 36] ``` # Constraints - The `num_workers` parameter must be a positive integer less than or equal to the length of `data`. - Optimize the function to handle large lists with minimal overhead. # Notes - You are required to use the provided module `torch.distributed.elastic.multiprocessing` for process management. - Ensure the function is compatible with various environments, including Jupyter notebooks, by appropriately managing process reducers if necessary.","solution":"import torch.multiprocessing as mp def compute_worker(data_segment): return [x ** 2 for x in data_segment] def distributed_computation(data, num_workers): def worker_process(data_partition, return_list, index): result = compute_worker(data_partition) return_list[index] = result # Splitting data into nearly equal parts for the number of workers chunk_size = len(data) // num_workers chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)] # Ensure we have num_workers number of chunks if len(chunks) > num_workers: chunks[-2].extend(chunks.pop()) manager = mp.Manager() return_list = manager.list([None] * num_workers) processes = [] for i in range(num_workers): p = mp.Process(target=worker_process, args=(chunks[i], return_list, i)) processes.append(p) for p in processes: p.start() for p in processes: p.join() # Flatten the returned list of lists into a single list result = [] for sublist in return_list: result.extend(sublist) return result"},{"question":"**Dynamic Type Creation Using the `types` Module** **Objective:** Implement a dynamic class creation system using the `types` module that allows defining new classes with dynamically resolved base classes and a custom attribute. The goal is to ensure a solid grasp of dynamic type creation and metaclasses in Python. **Task:** You are required to create a function `create_dynamic_class` that: 1. Accepts the following parameters: - `name` (str): The name of the class. - `bases` (tuple): A tuple of base classes. - `attrs` (dict): A dictionary of attributes and methods to be added to the class. 2. Resolves the base classes dynamically. 3. Uses a custom metaclass that adds a class attribute `custom_attr` with a value of `\'custom\'`. **Function Signature:** ```python def create_dynamic_class(name: str, bases: tuple = (), attrs: dict = {}) -> type: pass ``` **Requirements:** 1. Utilize `types.new_class`, `types.prepare_class`, and `types.resolve_bases` methods to create the class dynamically. 2. Ensure that the dynamically created class has a metaclass that adds a `custom_attr` attribute to the class. 3. Verify the class behavior by ensuring: - The class can be instantiated. - The instance has the attributes defined in `attrs`. - The class has a class attribute `custom_attr` equal to `\'custom\'`. **Constraints:** - You cannot use the built-in `type` to create the class directly. - You must use the utility functions from the `types` module. **Example Usage:** ```python # Define a base class class Base: def base_method(self): return \\"base method\\" # Define class attributes and methods attributes = { \'attr1\': \'value1\', \'method1\': lambda self: \'method1\' } # Create the dynamic class MyClass = create_dynamic_class(\'MyClass\', (Base,), attributes) # Check class attribute assert MyClass.custom_attr == \'custom\' # Instantiate and check instance attributes instance = MyClass() assert instance.attr1 == \'value1\' assert instance.method1() == \'method1\' assert instance.base_method() == \'base method\' ``` **Notes:** - This exercise extensively uses the `types` module to evaluate your understanding of dynamic type creation and manipulation in Python. - Focus on properly utilizing the `prepare_class`, `new_class`, and `resolve_bases` functions to achieve the desired behavior.","solution":"import types class CustomMeta(type): def __new__(cls, name, bases, dct): dct[\'custom_attr\'] = \'custom\' return super().__new__(cls, name, bases, dct) def create_dynamic_class(name: str, bases: tuple = (), attrs: dict = {}) -> type: resolved_bases = types.resolve_bases(bases) metaclass = CustomMeta prepared = metaclass.__prepare__(name, resolved_bases) prepared.update(attrs) cls = types.new_class(name, resolved_bases, {\'metaclass\': metaclass}, lambda ns: ns.update(prepared)) return cls"},{"question":"# Question: HTTP Status Code Analyzer **Objective:** Write a function that categorizes a list of numeric HTTP status codes into groups based on their category (Informational, Success, Redirection, Client Error, Server Error) and provides a summary report. The summary report should include the counts of each category and the most frequent status code in each category. **Function Signature:** ```python def http_status_analyzer(status_codes: List[int]) -> Dict[str, Any]: pass ``` **Input:** - `status_codes` (List[int]): A list of numeric HTTP status codes (e.g., [200, 404, 500, 200, 503, 404, 302]). **Output:** - A dictionary with the following format: ```python { \\"Informational\\": { \\"count\\": int, # Number of informational status codes \\"most_frequent\\": int # Most frequent informational status code }, \\"Success\\": { \\"count\\": int, # Number of success status codes \\"most_frequent\\": int # Most frequent success status code }, \\"Redirection\\": { \\"count\\": int, # Number of redirection status codes \\"most_frequent\\": int # Most frequent redirection status code }, \\"Client Error\\": { \\"count\\": int, # Number of client error status codes \\"most_frequent\\": int # Most frequent client error status code }, \\"Server Error\\": { \\"count\\": int, # Number of server error status codes \\"most_frequent\\": int # Most frequent server error status code } } ``` **Constraints:** - The input list will have at least one status code. - If a category has no status codes, the count should be `0` and `most_frequent` should be `None`. **Example:** ```python status_codes = [200, 201, 404, 500, 200, 503, 404, 302] output = http_status_analyzer(status_codes) # Expected output: # { # \\"Informational\\": {\\"count\\": 0, \\"most_frequent\\": None}, # \\"Success\\": {\\"count\\": 3, \\"most_frequent\\": 200}, # \\"Redirection\\": {\\"count\\": 1, \\"most_frequent\\": 302}, # \\"Client Error\\": {\\"count\\": 2, \\"most_frequent\\": 404}, # \\"Server Error\\": {\\"count\\": 2, \\"most_frequent\\": 503} # } ``` **Advanced Considerations:** - Take into account the time complexity while categorizing and counting the status codes. - Consider how to handle situations where multiple status codes have the same frequency.","solution":"from typing import List, Dict, Any from collections import Counter def http_status_analyzer(status_codes: List[int]) -> Dict[str, Any]: categories = { \\"Informational\\": range(100, 200), \\"Success\\": range(200, 300), \\"Redirection\\": range(300, 400), \\"Client Error\\": range(400, 500), \\"Server Error\\": range(500, 600), } summary = {} for category in categories: filtered_codes = [code for code in status_codes if code in categories[category]] count = len(filtered_codes) most_frequent = None if count == 0 else Counter(filtered_codes).most_common(1)[0][0] summary[category] = { \\"count\\": count, \\"most_frequent\\": most_frequent } return summary"},{"question":"**Objective:** You are tasked with writing a Python function that archives a list of files and then extracts specific types of files from this archive. The specific types of files to be extracted will be specified using a file-extension filter. **Task:** 1. Write a function `archive_files(file_list, archive_name)` that: - Takes a list of file paths (`file_list`) and a string (`archive_name`) representing the name of the archive to be created. - Archives these files into a single tar file with gzip compression. - The archive should use the Pax POSIX.1-2001 `PAX_FORMAT`. 2. Write a function `extract_files_by_extension(archive_name, extension, output_dir)` that: - Takes the archive name (`archive_name`), a string representing the file extension (`extension`), and the output directory path (`output_dir`). - Extracts only the files with the specified extension from the archive and saves them in the provided output directory. - Use proper error handling and handle any exceptions that might occur during the extraction process. **Requirements:** - Use the `tarfile` module wherever necessary. - Use context managers (`with` statements) to ensure proper opening and closing of files. - Implement necessary error handling for file operations (e.g., file not exists, read/write errors). **Example Usage:** ```python # Given files: [\'file1.txt\', \'file2.py\', \'file3.md\'] archive_files([\'file1.txt\', \'file2.py\', \'file3.md\'], \'example.tar.gz\') # From the given archive, extract only Python files to \'output\' directory: extract_files_by_extension(\'example.tar.gz\', \'.py\', \'output/\') ``` **Expected Outputs:** - The tar archive \'example.tar.gz\' should be created containing the provided files. - After calling `extract_files_by_extension`, only `.py` files should be extracted into the \'output\' directory. **Constraints:** - Ensure that extracting files maintains the original directory structures. - Handle any errors gracefully, such as missing files or permission issues.","solution":"import tarfile import os def archive_files(file_list, archive_name): Archives the specified list of files into a tar.gz archive. :param file_list: List of file paths to be archived. :param archive_name: Name of the archive to be created. with tarfile.open(archive_name, \\"w:gz\\", format=tarfile.PAX_FORMAT) as tar: for file in file_list: if os.path.isfile(file): tar.add(file, arcname=os.path.basename(file)) def extract_files_by_extension(archive_name, extension, output_dir): Extracts files with a specific extension from a tar.gz archive into the specified directory. :param archive_name: Name of the archive to be extracted. :param extension: File extension to filter by (e.g. \'.py\'). :param output_dir: Directory where the filtered files will be extracted. if not os.path.exists(output_dir): os.makedirs(output_dir) with tarfile.open(archive_name, \\"r:gz\\") as tar: members = [m for m in tar.getmembers() if m.name.endswith(extension)] for member in members: # Extracting member to the output directory tar.extract(member, output_dir) # Moving the extracted file to maintain directory structure extracted_path = os.path.join(output_dir, member.name) final_path = os.path.join(output_dir, os.path.basename(member.name)) os.rename(extracted_path, final_path)"},{"question":"# Python Function Implementation: Custom Callable Object in CPython Objectives: For this assessment, you will create a Python extension using C that defines a new type of callable object. This object will support both the `tp_call` and vectorcall protocols. The goal is to ensure efficient and correct function/method calling compatible with CPython\'s internals. Requirements: 1. **Define a custom callable object**: - The new object should support both `tp_call` and vectorcall. - Positional and keyword arguments should be handled correctly. 2. **Implement `tp_call`**: - Ensure the implementation adheres to the provided protocol and can correctly manage the arguments supplied as `args` tuple and `kwargs` dictionary. 3. **Implement vectorcall**: - Optimize the calling process by correctly using the provided argument array (args) and keyword names array (kwnames). - Ensure that the callable can be invoked with or without keyword arguments. 4. **Ensure Recursion Safety**: - Use `Py_EnterRecursiveCall()` and `Py_LeaveRecursiveCall()` where appropriate to manage recursion control for vectorcall. 5. **Function Testing**: - Implement tests demonstrating the custom callable\'s usage from Python, ensuring it works correctly with both protocols and different argument types. Constraints: - Use the provided signatures and conventions strictly as described in the documentation. - Consider handling exceptions and edge cases, ensuring the callable handles incorrect or empty argument scenarios gracefully. - Focus on performance and correctness, leveraging vectorcall\'s efficiency. Example: ```c // Include necessary headers #include <Python.h> // Define the function signatures as per vectorcall and tp_call static PyObject *my_custom_call(PyObject *self, PyObject *args, PyObject *kwargs); static PyObject *my_vectorcall(PyObject *callable, PyObject *const *args, size_t nargsf, PyObject *kwnames); // Define the new type object with the necessary slots typedef struct { PyObject_HEAD } MyCustomObject; static PyTypeObject MyCustomType = { // Initialization and setting flags // ... .tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_HAVE_VECTORCALL, .tp_call = my_custom_call, .tp_vectorcall_offset = offsetof(MyCustomObject, vectorcall), // Additional settings... }; // Implement the `tp_call` and vectorcall functions static PyObject *my_custom_call(PyObject *self, PyObject *args, PyObject *kwargs) { // Function implementation... } static PyObject *my_vectorcall(PyObject *callable, PyObject *const *args, size_t nargsf, PyObject *kwnames) { // Function implementation... } // Module initialization code // Testing Examples in Python // def test_custom_callable(): // obj = MyCustomObject() // result = obj(...) // assert result == ... // Initialize and test the module. ``` Implement the above guidelines and requirements to create a new custom callable type in a C-Extension for Python. Thoroughly test the implementation using the described methods, ensuring compatibility with both `tp_call` and vectorcall protocols.","solution":"# As the question involves writing a C extension for Python, here\'s a high-level # Python simulation of what that C code would achieve. Note that an actual # implementation would require writing C code and compiling it as a Python extension module. class CustomCallable: def __init__(self, func): self.func = func def __call__(self, *args, **kwargs): return self.func(*args, **kwargs) def vector_call(self, args, kwargs): return self.func(*args, **kwargs) # Sample function to be used with CustomCallable def sample_function(a, b=0): return a + b # Creating an instance of CustomCallable using `sample_function` custom_callable = CustomCallable(sample_function)"},{"question":"# Causal Attention Implementation with PyTorch Context: The attention mechanism is a crucial component in transformer models, allowing the model to focus on different parts of the input sequence differently. In certain cases, like language modeling, we need causal (or autoregressive) attention, which ensures that each position in the sequence can only attend to previous positions and not future ones to maintain the autoregressive property. In PyTorch, the `torch.nn.attention.bias` module provides tools to handle causal biases used in such contexts. Task: You are required to implement a PyTorch module that applies causal attention masking using the functionalities available in the `torch.nn.attention.bias` module. Requirements: 1. Implement a custom `CausalAttention` class that inherits from `torch.nn.Module`. 2. The `CausalAttention` class should: - Initialize with parameters `embed_dim`, `num_heads`, `causal=True`. - Use `nn.MultiheadAttention` for the core attention mechanism. - Apply a causal mask using the `CausalBias` class from `torch.nn.attention.bias`. 3. The `forward` method of this class should: - Take inputs `query, key, value`. - Construct an appropriate causal mask using `CausalBias` or its associated functions. - Apply the attention mechanism with the mask. - Return the attention output. Constraints: - Assume that batch size is 1 for simplicity. - Your implementation should be efficient in terms of handling the sequences and using the mask. - You are expected to handle the `causal` parameter to switch between causal and non-causal attention. Input: - `query`: The query matrix of shape (sequence_length, batch_size, embed_dim). - `key`: The key matrix of shape (sequence_length, batch_size, embed_dim). - `value`: The value matrix of shape (sequence_length, batch_size, embed_dim). Output: - The attended output of shape (sequence_length, batch_size, embed_dim). Example: ```python import torch causal_attention = CausalAttention(embed_dim=64, num_heads=8, causal=True) query = torch.randn(10, 1, 64) # (sequence_length, batch_size, embed_dim) key = torch.randn(10, 1, 64) # (sequence_length, batch_size, embed_dim) value = torch.randn(10, 1, 64) # (sequence_length, batch_size, embed_dim) output = causal_attention(query, key, value) print(output.shape) # Should output torch.Size([10, 1, 64]) ``` Implementation: ```python import torch import torch.nn as nn from torch.nn.attention.bias import CausalBias class CausalAttention(nn.Module): def __init__(self, embed_dim, num_heads, causal=True): super(CausalAttention, self).__init__() self.embed_dim = embed_dim self.num_heads = num_heads self.causal = causal self.attention = nn.MultiheadAttention(embed_dim, num_heads) self.causal_bias = CausalBias(embed_dim, num_heads) if causal else None def forward(self, query, key, value): if self.causal: attn_mask = self.causal_bias(query.size(0), key.size(0)) else: attn_mask = None attn_output, _ = self.attention(query, key, value, attn_mask=attn_mask) return attn_output ``` Notes: - Ensure your model is tested with different input sizes and validate the causal masking through the outputs. - Include proper comments and document your code to explain the logic and usage of `CausalBias`.","solution":"import torch import torch.nn as nn class CausalAttention(nn.Module): def __init__(self, embed_dim, num_heads, causal=True): super(CausalAttention, self).__init__() self.embed_dim = embed_dim self.num_heads = num_heads self.causal = causal self.attention = nn.MultiheadAttention(embed_dim, num_heads) def forward(self, query, key, value): if self.causal: # Create a causal mask attn_mask = torch.tril(torch.ones(query.size(0), key.size(0))) attn_mask = attn_mask.masked_fill(attn_mask == 0, float(\'-inf\')) attn_mask = attn_mask.masked_fill(attn_mask == 1, float(0.0)) else: attn_mask = None attn_output, _ = self.attention(query, key, value, attn_mask=attn_mask) return attn_output"},{"question":"**Task: Implement and Use Custom Collection Class with Multiple ABCs** # Objective Create a custom collection class that inherits from multiple abstract base classes provided in the `collections.abc` module. This will test your understanding of abstract base classes, abstract methods, and mixin methods provided by the ABCs. # Description You need to implement a class `CustomList` that acts like a mutable sequence and set. Your `CustomList` should inherit from `MutableSequence` and `MutableSet`. It should retain unique elements only and provide list-like indexing. # Requirements 1. `CustomList` must inherit from `MutableSequence` and `MutableSet`. 2. Implement necessary abstract methods for both `MutableSequence` and `MutableSet`. 3. Ensure that the class retains only unique elements, resembling a set. 4. Implement indexing to allow accessing elements like a list. 5. Utilize mixin methods provided by the base ABCs. # Implementation Details - You must override `__getitem__`, `__setitem__`, `__delitem__`, `__len__`, `insert`, `add`, `discard`, `__contains__`, `__iter__` methods. - Override other methods where it is necessary to cater to the unique-property requirement. - Provide the following methods: - `def __init__(self, iterable=None):` Initialize the collection optionally with an initial iterable of elements. - `def __repr__(self):` Return a string representation of the collection. # Example Usage ```python clist = CustomList([1, 2, 3]) clist.append(4) # CustomList([1, 2, 3, 4]) clist.add(2) # CustomList([1, 2, 3, 4]), no duplicates clist.pop() # Returns 4, CustomList([1, 2, 3]) clist.discard(2) # CustomList([1, 3]) clist.remove(3) # CustomList([1]) print(clist[0]) # 1 clist[0] = 5 # CustomList([5]) print(len(clist)) # 1 print(clist) # CustomList([5]) ``` # Constraints - The collection must always consist of unique elements after any operation. - Indexing should be similar to lists, but retaining unique elements. - Your implementation should efficiently handle operations preserving both sequence and uniqueness constraints. Good luck, and make sure to review the `collections.abc` documentation carefully for accurate implementation of methods!","solution":"from collections.abc import MutableSequence, MutableSet class CustomList(MutableSequence, MutableSet): def __init__(self, iterable=None): self._items = [] self._set = set() if iterable: for item in iterable: self.add(item) def __repr__(self): return f\'CustomList({self._items})\' def __getitem__(self, index): return self._items[index] def __setitem__(self, index, value): if value in self._set: raise ValueError(\\"Duplicate values are not allowed\\") removed = self._items[index] self._items[index] = value self._set.remove(removed) self._set.add(value) def __delitem__(self, index): value = self._items.pop(index) self._set.remove(value) def __len__(self): return len(self._items) def insert(self, index, value): if value not in self._set: self._items.insert(index, value) self._set.add(value) def add(self, value): if value not in self._set: self._items.append(value) self._set.add(value) def discard(self, value): if value in self._set: self._items.remove(value) self._set.remove(value) def __contains__(self, value): return value in self._set def __iter__(self): return iter(self._items)"},{"question":"**Color Space Manipulation** You are provided with a list of colors represented in the RGB color space, and you need to convert each of these colors to one of the other color spaces and then back to RGB to verify the conversion process. The challenge is to implement a function that handles the color space conversion correctly, ensuring the resulting RGB colors match the original input colors as closely as possible after the round-trip conversion. # Task 1. Implement a function `validate_color_conversion(colors: List[Tuple[float, float, float]], target_space: str) -> List[bool]` that: - Takes a list of colors in the RGB color space and a target color space (\\"YIQ\\", \\"HLS\\", or \\"HSV\\"). - Converts each RGB color to the specified target color space and then converts it back to RGB. - Returns a list of booleans indicating for each color whether the conversion back to RGB closely matches the original color (with a tolerance of `1e-6` for each component). # Function Signature ```python from typing import List, Tuple def validate_color_conversion(colors: List[Tuple[float, float, float]], target_space: str) -> List[bool]: pass ``` # Input - `colors` (List[Tuple[float, float, float]]): A list of tuples where each tuple contains three floating-point numbers representing an RGB color. - `target_space` (str): A string indicating the target color space (\\"YIQ\\", \\"HLS\\", or \\"HSV\\"). # Output - (List[bool]): A list of booleans where each boolean indicates whether the original RGB color matches the round-trip converted RGB color within a tolerance of `1e-6`. # Constraints - Each RGB color component will be a floating-point number between 0 and 1, inclusive. - The target color space will always be one of \\"YIQ\\", \\"HLS\\", or \\"HSV\\". # Example ```python import colorsys colors = [(0.2, 0.4, 0.4), (0.5, 0.5, 0.5)] target_space = \\"HSV\\" result = validate_color_conversion(colors, target_space) print(result) # Expected output: [True, True] (depending on the precision handling of the conversions) ``` # Notes - Utilize the `colorsys` module functions for conversions between the color spaces. - Ensure the round-trip conversion falls within the tolerance specified. - Pay attention to the precision of floating-point arithmetic to ensure accurate comparisons.","solution":"from typing import List, Tuple import colorsys def validate_color_conversion(colors: List[Tuple[float, float, float]], target_space: str) -> List[bool]: def almost_equal(c1, c2, tol=1e-6): return all(abs(a - b) <= tol for a, b in zip(c1, c2)) results = [] for r, g, b in colors: if target_space == \\"YIQ\\": y, i, q = colorsys.rgb_to_yiq(r, g, b) r2, g2, b2 = colorsys.yiq_to_rgb(y, i, q) elif target_space == \\"HLS\\": h, l, s = colorsys.rgb_to_hls(r, g, b) r2, g2, b2 = colorsys.hls_to_rgb(h, l, s) elif target_space == \\"HSV\\": h, s, v = colorsys.rgb_to_hsv(r, g, b) r2, g2, b2 = colorsys.hsv_to_rgb(h, s, v) else: raise ValueError(\\"Unsupported target space\\") results.append(almost_equal((r, g, b), (r2, g2, b2))) return results"},{"question":"# Threaded Matrix Multiplication **Objective**: Write a Python function that performs matrix multiplication using threads. You should use the `_thread` module to create multiple threads, where each thread is responsible for computing a specific part of the resulting matrix. **Function Signature**: ```python def threaded_matrix_multiplication(A: List[List[int]], B: List[List[int]]) -> List[List[int]]: pass ``` **Input**: - `A`: A 2D list of integers representing matrix A (dimensions M x N). - `B`: A 2D list of integers representing matrix B (dimensions N x P). **Output**: - Returns a 2D list of integers representing the resulting matrix after multiplying A and B (dimensions M x P). **Constraints**: - The number of columns in A must be equal to the number of rows in B (i.e., N must be the same for both matrices). - You may assume that the dimensions of the matrices (M, N, P) are all positive integers and within a reasonable range that allows for threaded execution. - Your implementation should handle errors gracefully and utilize threading effectively to speed up the computation. **Performance Requirements**: - Use the `_thread` module for creating and managing threads. - Ensure that threads do not suffer from race conditions using locks. - Make efficient use of CPU cores by distributing the workload evenly among multiple threads. **Example**: ```python A = [ [1, 2, 3], [4, 5, 6] ] B = [ [7, 8], [9, 10], [11, 12] ] result = threaded_matrix_multiplication(A, B) print(result) # Expected output: [[58, 64], [139, 154]] ``` **Notes**: - Remember to initialize the result matrix with dimensions M x P. - Each thread should compute one or more elements of the result matrix. - Use `_thread.allocate_lock()` to create locks to ensure thread safety when updating shared data (i.e., the result matrix).","solution":"import _thread from typing import List def threaded_matrix_multiplication(A: List[List[int]], B: List[List[int]]) -> List[List[int]]: # Number of rows in A M = len(A) # Number of columns in A, which is also the number of rows in B N = len(A[0]) if A else 0 # Number of columns in B P = len(B[0]) if B else 0 # Initialize the result matrix with zeros result = [[0] * P for _ in range(M)] # Lock for thread safety lock = _thread.allocate_lock() def compute_element(i, j): nonlocal result sum_value = sum(A[i][k] * B[k][j] for k in range(N)) lock.acquire() result[i][j] = sum_value lock.release() # Create threads for each element in the result matrix for i in range(M): for j in range(P): _thread.start_new_thread(compute_element, (i, j)) # We need to make sure that all threads have finished before we return the result import time time.sleep(1) # Sleep for a bit to ensure all threads complete (not ideal, but works for simple cases) return result"},{"question":"# Custom Python Type Implementation Objective Implement a custom Python class that mimics some of the behaviors described for the `PyTypeObject` extension types in Python\'s C-API. Requirements 1. **Class Definition**: Create a class called `CustomType` in Python that simulates the following behaviors: - **Initialization**: It should take an initial value (an integer) and store it. - **Destruction**: It should have a deallocation method (`__del__`) that prints a message indicating the object is being destroyed. - **String Representation**: Implement both `__repr__` and `__str__` methods: - `__repr__` should return a string like `CustomType({value})`. - `__str__` should return a more human-readable string like `CustomType with value: {value}`. - **Attribute Management**: Implement special methods to manage attributes: - `__getattr__`: If trying to access a non-existing attribute, return a message `\\"Attribute \'{name}\' not found\\"`. - `__setattr__`: Log a message each time an attribute is set, indicating which attribute is being set and to what value. - **Equality Comparison**: Implement custom equals (`__eq__`) and not-equals (`__ne__`) methods that compare the stored values. Input and Output - **Input**: You are not required to read any input from the console. - **Output**: Ensure your class methods perform the specified behaviors. Example Usage ```python # Initialize a CustomType object with value 42 obj = CustomType(42) # Print string representations print(repr(obj)) # Output: CustomType(42) print(str(obj)) # Output: CustomType with value: 42 # Attribute management print(obj.some_attr) # Output: Attribute \'some_attr\' not found obj.new_attr = 10 # Output: Setting attribute \'new_attr\' to 10 print(obj.new_attr) # Output: 10 # Equality Comparison obj2 = CustomType(42) obj3 = CustomType(10) print(obj == obj2) # Output: True print(obj == obj3) # Output: False # Deallocation message (upon program termination or object deletion) del obj # Output: CustomType(42) object is being destroyed ``` Constraints - Your class\'s `__init__` should be able to handle only integer values for now. - Implement all methods within the `CustomType` class. - No additional libraries should be used; everything should be standard Python. Performance Requirements - Your implementation should efficiently handle attribute setting and getting without unnecessary overhead.","solution":"class CustomType: def __init__(self, value): if not isinstance(value, int): raise ValueError(\\"The initial value must be an integer\\") self.value = value def __del__(self): print(f\\"CustomType({self.value}) object is being destroyed\\") def __repr__(self): return f\\"CustomType({self.value})\\" def __str__(self): return f\\"CustomType with value: {self.value}\\" def __getattr__(self, name): return f\\"Attribute \'{name}\' not found\\" def __setattr__(self, name, value): print(f\\"Setting attribute \'{name}\' to {value}\\") super().__setattr__(name, value) def __eq__(self, other): if not isinstance(other, CustomType): return NotImplemented return self.value == other.value def __ne__(self, other): return not self.__eq__(other)"},{"question":"# Advanced Python Argument Parsing and Building Values You are tasked with creating a Python extension module using C that includes a function capable of parsing diverse Python argument types and converting them into their appropriate C equivalents. You will then output these values back into Python objects. Your task is to implement the following function: ```python def parse_and_build(args_tuple: tuple) -> tuple: Parses the provided argument tuple into respective C types and returns them constructed back into a Python tuple. Parameters: args_tuple (tuple): The tuple containing arguments of various types. The acceptable arguments include: - An integer - A string - A float - A boolean - A bytes object Returns: tuple: A tuple containing the parsed C values re-constructed back into Python objects. pass ``` Function Implementation Guidelines: 1. **Parsing Arguments**: Use `PyArg_ParseTuple()` to parse the provided argument tuple (`args_tuple`) into their respective C types. - Consider the following format units for acceptable types: - `\\"i\\"` for integers to C `int` - `\\"s\\"` for strings to C `char *` - `\\"d\\"` for floats to C `double` - `\\"p\\"` for booleans to C `int` - `\\"y\\"` for bytes to C `char *` 2. **Building Values**: Using `Py_BuildValue()`, convert these C types back into Python objects and return them in a new tuple. 3. **Constraints**: - The function should support up to 5 arguments in the input tuple. - The function should raise appropriate Python exceptions if the input does not match the expected formats or counts. 4. **Performance Requirements**: The function should handle error conditions and invalid inputs gracefully, raising appropriate exceptions without causing segmentation faults or memory leaks. Example Usage: ```python # Input tuple args = (42, \\"example string\\", 3.14, True, b\\"bytes data\\") # Expected output output = parse_and_build(args) # output would be a tuple akin to: (42, \\"example string\\", 3.14, True, b\\"bytes data\\") # Note: The output should demonstrate that the parsing and building processes are done correctly. ``` Implement this function in your Python C extension module and ensure it handles the specified argument types robustly.","solution":"def parse_and_build(args_tuple: tuple) -> tuple: Parses the provided argument tuple into respective C types and returns them constructed back into a Python tuple. Parameters: args_tuple (tuple): The tuple containing arguments of various types. The acceptable arguments include: - An integer - A string - A float - A boolean - A bytes object Returns: tuple: A tuple containing the parsed C values re-constructed back into Python objects. if not isinstance(args_tuple, tuple) or len(args_tuple) > 5: raise ValueError(\\"Input must be a tuple with at most 5 elements\\") for item in args_tuple: if not (isinstance(item, (int, str, float, bool, bytes))): raise TypeError(\\"Invalid argument type in tuple\\") return args_tuple"},{"question":"<|Analysis Begin|> The provided seaborn documentation focuses on visualizing categorical data with seaborn. It introduces various types of plots that are useful for categorical data, including: 1. **Categorical Scatterplots:** - `stripplot` - `swarmplot` 2. **Categorical Distribution Plots:** - `boxplot` - `violinplot` - `boxenplot` 3. **Categorical Estimate Plots:** - `pointplot` - `barplot` - `countplot` The documentation covers the basic functionalities of these plots, the parameters that can be adjusted (such as `hue`, `kind`, `order`, `native_scale`, `dodge`), and examples from seaborn\'s high-level `catplot` interface and individual plotting functions. Given this, an appropriate coding assessment question could focus on testing the student\'s ability to create various types of seaborn plots, customize them using different parameters, and interpret the resulting visualizations. <|Analysis End|> <|Question Begin|> Design a function that generates several categorical plots using seaborn and a provided dataset. The function should be capable of creating the following plots: 1. A `stripplot` showing total bill versus day. 2. A `swarmplot` with a hue semantic for smoker on total bill versus day. 3. A `boxplot` comparing day versus total bill with hue semantic for sex. 4. A `violinplot` comparing total bill versus day, with a split hue semantic on sex. Function signature: ```python def generate_categorical_plots(data): pass ``` # Expected Input: - `data`: A pandas DataFrame containing the dataset (specifically, the `tips` dataset from seaborn should be used for testing). # Expected Output: There is no return value. The function should generate and display the plots using matplotlib. # Constraints: - Use the seaborn `catplot` function where appropriate for higher-level interface plotting. - Utilize the specified hue semantics and plot kinds as described above. - Ensure that all plots are properly labeled and configured for readability. # Example: ```python import seaborn as sns import matplotlib.pyplot as plt tips = sns.load_dataset(\\"tips\\") generate_categorical_plots(tips) ``` This should generate and show 4 different plots: 1. Stripplot of total bill versus day. 2. Swarmplot of total bill versus day with hue for smoker. 3. Boxplot comparing total bill versus day, with hue for sex. 4. Violinplot comparing total bill versus day with split hue for sex. # Additional Notes: - Ensure that the plots are displayed in a single row or column for clarity. - Use seabornâ€™s default color palettes. - Include appropriate titles and axis labels to make the plots self-explanatory.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_categorical_plots(data): Generates and displays several categorical plots using the provided dataset. Parameters: data: pandas DataFrame containing the dataset (specifically, the `tips` dataset from seaborn). # Stripplot: Total bill vs day plt.figure(figsize=(12, 6)) plt.subplot(2, 2, 1) sns.stripplot(x=\\"day\\", y=\\"total_bill\\", data=data) plt.title(\\"Stripplot: Total Bill vs Day\\") # Swarmplot: Total bill vs day with hue semantic for smoker plt.subplot(2, 2, 2) sns.swarmplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", data=data) plt.title(\\"Swarmplot: Total Bill vs Day (Hue: Smoker)\\") # Boxplot: Day vs total bill with hue semantic for sex plt.subplot(2, 2, 3) sns.boxplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=data) plt.title(\\"Boxplot: Total Bill vs Day (Hue: Sex)\\") # Violinplot: Total bill vs day with split hue semantic on sex plt.subplot(2, 2, 4) sns.violinplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=data, split=True) plt.title(\\"Violinplot: Total Bill vs Day (Split Hue: Sex)\\") plt.tight_layout() plt.show()"},{"question":"# Custom Command-Line Interpreter with Recording and Playback In this task, you are required to create a custom command-line interpreter using the `cmd` module from the Python standard library. Your task is to create a simple command-driven calculator shell that performs basic mathematical operations and supports command recording and playback functionalities. Command Specifications 1. **Addition**: - Command: `add` - Description: Adds two numbers. - Usage: `add <number1> <number2>` - Example: `add 10 5` (Output: `15`) 2. **Subtraction**: - Command: `subtract` - Description: Subtracts the second number from the first number. - Usage: `subtract <number1> <number2>` - Example: `subtract 10 5` (Output: `5`) 3. **Multiplication**: - Command: `multiply` - Description: Multiplies two numbers. - Usage: `multiply <number1> <number2>` - Example: `multiply 10 5` (Output: `50`) 4. **Division**: - Command: `divide` - Description: Divides the first number by the second number. If the second number is zero, print an error message. - Usage: `divide <number1> <number2>` - Example: `divide 10 5` (Output: `2.0`) Record and Playback 1. **Record Commands**: - Command: `record` - Description: Records subsequent commands to a specified file. - Usage: `record <filename>` - Example: `record commands.txt` 2. **Playback Commands**: - Command: `playback` - Description: Executes commands from a specified file in order. - Usage: `playback <filename>` - Example: `playback commands.txt` 3. **Exit**: - Command: `exit` - Description: Exits the interpreter. - Usage: `exit` - Example: `exit` Implementation Details 1. Subclass the `cmd.Cmd` class to create your custom shell `CalculatorShell`. 2. Implement the described commands (`add`, `subtract`, `multiply`, `divide`) as methods in the `CalculatorShell` class. 3. Implement the `record` and `playback` functionalities to handle recording and executing commands from a file. 4. Override necessary methods (`precmd`, `postcmd`) to manage command recording and other tasks as required. 5. Ensure proper error handling for invalid commands and division by zero. Constraints 1. Input numbers are floating-point values or integers. 2. Recorded commands should be saved exactly as entered by the user. Example Session ```plaintext Welcome to the Calculator Shell. Type help or ? to list commands. (calculator) add 10 5 15 (calculator) subtract 10 5 5 (calculator) multiply 10 5 50 (calculator) divide 10 5 2.0 (calculator) divide 10 0 Error: Division by zero. (calculator) record commands.txt (calculator) add 1 2 3 (calculator) multiply 3 4 12 (calculator) playback commands.txt 3 12 (calculator) exit ``` Submission Create a Python script that implements the `CalculatorShell` class and executes the command loop. ```python import cmd class CalculatorShell(cmd.Cmd): intro = \'Welcome to the Calculator Shell. Type help or ? to list commands.n\' prompt = \'(calculator) \' file = None # Add your command implementations here def do_add(self, arg): # Implementation of add command pass def do_subtract(self, arg): # Implementation of subtract command pass def do_multiply(self, arg): # Implementation of multiply command pass def do_divide(self, arg): # Implementation of divide command pass def do_record(self, arg): # Implementation of record command pass def do_playback(self, arg): # Implementation of playback command pass def do_exit(self, arg): \'Exit the Calculator Shell: EXIT\' print(\'Exiting...\') return True def precmd(self, line): # Custom precmd implementation return line def postcmd(self, stop, line): # Custom postcmd implementation return stop def close(self): # Close any open file pass if __name__ == \'__main__\': CalculatorShell().cmdloop() ``` Fill in the implementation details for the commands and ensure the script fulfills the requirements specified.","solution":"import cmd class CalculatorShell(cmd.Cmd): intro = \'Welcome to the Calculator Shell. Type help or ? to list commands.n\' prompt = \'(calculator) \' file = None def do_add(self, arg): \'Add two numbers: add <number1> <number2>\' try: numbers = list(map(float, arg.split())) if len(numbers) != 2: raise ValueError(\\"Exactly two numbers are required\\") print(numbers[0] + numbers[1]) except ValueError as e: print(f\\"Error: {e}\\") def do_subtract(self, arg): \'Subtract the second number from the first number: subtract <number1> <number2>\' try: numbers = list(map(float, arg.split())) if len(numbers) != 2: raise ValueError(\\"Exactly two numbers are required\\") print(numbers[0] - numbers[1]) except ValueError as e: print(f\\"Error: {e}\\") def do_multiply(self, arg): \'Multiply two numbers: multiply <number1> <number2>\' try: numbers = list(map(float, arg.split())) if len(numbers) != 2: raise ValueError(\\"Exactly two numbers are required\\") print(numbers[0] * numbers[1]) except ValueError as e: print(f\\"Error: {e}\\") def do_divide(self, arg): \'Divide the first number by the second number: divide <number1> <number2>\' try: numbers = list(map(float, arg.split())) if len(numbers) != 2: raise ValueError(\\"Exactly two numbers are required\\") if numbers[1] == 0: print(\\"Error: Division by zero.\\") return print(numbers[0] / numbers[1]) except ValueError as e: print(f\\"Error: {e}\\") def do_record(self, arg): \'Record subsequent commands to a specified file: record <filename>\' self.file = open(arg, \'w\') self.stdout = self.file def do_playback(self, arg): \'Execute commands from a specified file: playback <filename>\' with open(arg) as f: for line in f: print(line.strip()) self.onecmd(line) def do_exit(self, arg): \'Exit the Calculator Shell: exit\' print(\'Exiting...\') return True def precmd(self, line): if self.stdout != self.file: return line print(line, file=self.file) return line def close(self): if self.file: self.file.close() self.file = None self.stdout = self.__class__.stdout if __name__ == \'__main__\': CalculatorShell().cmdloop()"},{"question":"**Advanced Coding Assessment Question: Working with ZIP Archives** **Objective**: Design a function that processes a given directory, compresses its contents into a ZIP file using a specified compression method, and verifies the integrity of the created ZIP file. **Function Signature**: ```python def create_and_verify_zip(input_dir: str, output_zip: str, compression_method: str) -> bool: pass ``` **Function Details**: 1. The function should take the following inputs: - `input_dir` (str): The path to the directory whose contents need to be compressed. - `output_zip` (str): The path to the output ZIP file. - `compression_method` (str): The compression method to be used. It can be one of the following values: \\"stored\\", \\"deflated\\", \\"bzip2\\", or \\"lzma\\". 2. The function should: - Compress all the contents of `input_dir` into a ZIP file specified by `output_zip` using the given `compression_method`. - Ensure that the created ZIP file is valid and that no files are corrupted. 3. The function should return a boolean value: - `True` if the ZIP file is created successfully and passes the integrity check using the `testzip()` method. - `False` if any error occurs during the creation or validation of the ZIP file. **Constraints**: - If the given `compression_method` is not one of the specified methods, raise a `ValueError`. - Assume that the `input_dir` exists and is a valid directory. - Handle any exceptions that may arise during file operations, and ensure that resources are properly closed. **Example**: ```python # Example usage result = create_and_verify_zip(\\"path/to/input_dir\\", \\"output.zip\\", \\"deflated\\") print(result) # Should print True if successful ``` **Notes**: - You will need to use the `zipfile` module, and possibly other modules to handle files and directories. - Ensure robust handling of file paths and consider different operating system environments. **Tips**: - Consider using context managers to handle file operations. - Utilize the `ZipFile` class and its methods for creating and validating ZIP files. - Use appropriate compression constants (`zipfile.ZIP_STORED`, `zipfile.ZIP_DEFLATED`, etc.) based on the `compression_method` input. **Grading Criteria**: - Correct implementation of the function. - Proper usage of `zipfile` module functionalities. - Handling of all specified constraints and edge cases. - Code readability and comments explaining key parts of the implementation.","solution":"import os import zipfile def create_and_verify_zip(input_dir: str, output_zip: str, compression_method: str) -> bool: Compresses the contents of input_dir into a ZIP file specified by output_zip using the given compression_method and verifies the integrity of the created ZIP file. Args: - input_dir (str): The path to the directory whose contents need to be compressed. - output_zip (str): The path to the output ZIP file. - compression_method (str): The compression method to be used (\\"stored\\", \\"deflated\\", \\"bzip2\\", \\"lzma\\"). Returns: - bool: True if the ZIP file is created successfully and passes the integrity check, False otherwise. Raises: - ValueError: If the provided compression_method is not valid. # Map compression method to zipfile constant compression_map = { \\"stored\\": zipfile.ZIP_STORED, \\"deflated\\": zipfile.ZIP_DEFLATED, \\"bzip2\\": zipfile.ZIP_BZIP2, \\"lzma\\": zipfile.ZIP_LZMA } if compression_method not in compression_map: raise ValueError(f\\"Invalid compression method: {compression_method}\\") try: compression = compression_map[compression_method] # Create the ZIP file with zipfile.ZipFile(output_zip, \'w\', compression=compression) as zipf: for root, _, files in os.walk(input_dir): for file in files: filepath = os.path.join(root, file) zipf.write(filepath, os.path.relpath(filepath, input_dir)) # Verify the created ZIP file with zipfile.ZipFile(output_zip, \'r\') as zipf: if zipf.testzip() is not None: return False # If testzip returns any file name, it means there\'s a corruption return True # Zip file is valid and intact except Exception as e: return False # In case of any exceptions during the process"},{"question":"**Coding Assessment Question** # Objective: Demonstrate understanding of the `compileall` module by implementing a custom function that compiles Python files in a specified directory with various constraints and outputs a summary of the compilation process. # Question: You are required to implement a function `custom_compile_dir` that will compile all `.py` files in a given directory according to specific constraints and generate a summary report. The summary report should include: - Total number of `.py` files found. - Number of files successfully compiled. - Number of files that failed to compile. The function should have the following specifications: Function Signature: ```python import compileall import re def custom_compile_dir(directory: str, max_depth: int, force_recompile: bool, exclude_pattern: str, quiet_level: int, optimization_levels: list): pass ``` Input: - `directory` (str): The root directory where the compilation should begin. - `max_depth` (int): The maximum depth to recurse into subdirectories. - `force_recompile` (bool): A flag indicating whether to force recompilation of files even if their timestamps are up-to-date. - `exclude_pattern` (str): A regex pattern used to exclude certain files from being compiled. - `quiet_level` (int): The level of verbosity for the compile process, where 0 is the default verbosity, 1 suppresses file names, and 2 suppresses all output. - `optimization_levels` (list): A list of optimization levels to be considered during compilation (allowed values are -1 for default, 0, 1, and 2). Output: - This function should print a summary to the console containing: - Total number of `.py` files found. - Number of files successfully compiled. - Number of files that failed to compile. Constraints and Assumptions: - The directory structure may be complex, and the function should handle deep recursion levels. - The exclude pattern should be valid regex. - The function should gracefully handle any I/O errors or exceptions during compilation. - Assume the function is called with valid inputs for the parameters. Example: ```python # Example usage: custom_compile_dir(\'my_project\', max_depth=5, force_recompile=True, exclude_pattern=r\'btest_\', quiet_level=1, optimization_levels=[0, 1, 2]) ``` In this example, the function should compile all Python files within `my_project` directory up to a depth of 5, force recompilation of all files, exclude files that start with \'test_\', and perform the compilation with optimization levels 0, 1, and 2 while suppressing the filenames of compiled files in the output. # Requirements: - Use the `compileall.compile_dir` function to perform the actual compilation. - Ensure that the `exclude_pattern` is properly used to skip files that match the pattern. - Handle different `quiet_level` settings appropriately. - Validate the optimization levels list before passing it to `compileall.compile_dir`.","solution":"import compileall import re import os def custom_compile_dir(directory: str, max_depth: int, force_recompile: bool, exclude_pattern: str, quiet_level: int, optimization_levels: list): Compiles Python files in the specified directory and its subdirectories. Args: - directory (str): The root directory where the compilation should begin. - max_depth (int): The maximum depth to recurse into subdirectories. - force_recompile (bool): Flag indicating whether to force recompilation. - exclude_pattern (str): Regex pattern to exclude certain files from compilation. - quiet_level (int): The verbosity level (0: default, 1: suppress file names, 2: suppress all output). - optimization_levels (list): A list of optimization levels to consider during compilation. Prints a summary report of the compilation process. py_files = [] exclude_regex = re.compile(exclude_pattern) # Traverse directory and collect .py files for root, _, files in os.walk(directory): # Calculate depth and skip if it exceeds max_depth relative_depth = len(os.path.relpath(root, directory).split(os.sep)) if relative_depth > max_depth: continue for file in files: if file.endswith(\'.py\') and not exclude_regex.search(file): py_files.append(os.path.join(root, file)) total_files = len(py_files) success_count = 0 failure_count = 0 # Run compilation for each file for py_file in py_files: result = all(compileall.compile_file(py_file, force=force_recompile, quiet=quiet_level, optimize=opt) for opt in optimization_levels) if result: success_count += 1 else: failure_count += 1 # Print summary print(f\\"Total .py files found: {total_files}\\") print(f\\"Files successfully compiled: {success_count}\\") print(f\\"Files failed to compile: {failure_count}\\")"},{"question":"**Coding Assessment Question: Porting Legacy Code for String Handling in Python 2 and Python 3** **Objective:** Your task is to implement a Python function that ensures compatibility of string handling logic between Python 2 and Python 3. Given a list of mixed text and binary literals representing file contents, your function should correctly identify and handle strings, encoding text as necessary and differentiating binary data. **Instructions:** 1. Implement a function `process_data(data)` that takes a list of literals (`data`). These literals can either be text (Python 2 unicode or Python 3 str) or binary data (Python 2 str or Python 3 bytes). 2. The function should: - Convert all text literals to unicode in Python 2 and str in Python 3. - Convert all binary literals to bytes in both Python 2 and Python 3. - Ensure the conversions are compatible with both Python 2 and Python 3. 3. Return a dictionary containing two lists: - `text_data`: List of processed text literals. - `binary_data`: List of processed binary literals. **Example:** ```python def process_data(data): # Your code here # Example input data = [\'hello\', b\'world\', u\'Python2\', \'Python3\', b\'2to3\'] # Call the function result = process_data(data) # Expected output for Python 2 and Python 3 expected_output = { \'text_data\': [u\'hello\', u\'Python2\', u\'Python3\'], \'binary_data\': [b\'world\', b\'2to3\'] } assert result == expected_output ``` **Input Format:** - A list of literals (strings), with some elements being text (Python 2 unicode or Python 3 str) and some being binary data (Python 2 str or Python 3 bytes). **Output Format:** - A dictionary with two keys: `text_data` and `binary_data`. Each key maps to a list containing the processed literals as specified. **Constraints:** - The input list can contain both text and binary literals. - Ensure the function runs correctly in both Python 2.7+ and Python 3.4+. **Performance Requirements:** - The function should handle at least 10^4 literals efficiently. Feel free to use any standard library imports necessary to achieve compatibility between Python 2 and Python 3. --- Test the function thoroughly to ensure it works correctly in both Python environments. Provide the test cases along with the implementation.","solution":"import sys def process_data(data): Process the input list to separate text and binary literals, ensuring compatibility between Python 2 and Python 3. Args: data (list): A list of literals which can be text or binary. Returns: dict: A dictionary with \'text_data\' and \'binary_data\' lists. text_data = [] binary_data = [] for item in data: if sys.version_info[0] < 3: # Python 2 if isinstance(item, unicode): text_data.append(item) elif isinstance(item, str): binary_data.append(item) else: # Python 3 if isinstance(item, str): text_data.append(item) elif isinstance(item, bytes): binary_data.append(item) return { \'text_data\': text_data, \'binary_data\': binary_data }"},{"question":"# XML Parsing with Custom SAX Handlers **Objective**: Implement a SAX-based XML parser with custom handlers to process specific XML elements and attributes. **Task**: Write a Python program that uses the **xml.sax.handler** module to parse an XML document. The program should implement custom handlers to achieve the following: 1. **ContentHandler**: - Print the name and attributes of each element when it starts. - Print the character data inside elements. - Print a message when the document begins and ends. 2. **ErrorHandler**: - Log any errors and warnings encountered during parsing. 3. **EntityResolver**: - Resolve any external entities by providing a URL or local file path. 4. **LexicalHandler** (optional): - Print comments found in the XML document. - Print the beginning and end of any CDATA sections. **Input**: - Provide a sample XML file containing various elements, attributes, character data, comments, and CDATA sections to test the handlers. **Output**: - Print statements should reflect the handling of XML elements, character data, comments, and other SAX events. **Constraints**: - Ensure the program handles nested elements and various SAX events correctly. - Implement necessary error handling for malformed or invalid XML content. **Performance Requirements**: - The custom handlers should efficiently process large XML documents without significant performance degradation. **Example XML**: ```xml <?xml version=\\"1.0\\"?> <catalog> <!-- This is a comment --> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> <![CDATA[Some unparsed data here]]> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies.</description> </book> </catalog> ``` **Implementation Requirements**: 1. Define custom handler classes inheriting from the **xml.sax.handler** base classes. 2. Use Python\'s **xml.sax** library to parse the XML document using your custom handlers. 3. Ensure proper setup and registration of handlers with the SAX parser. **Skeleton Code**: ```python import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler): def startDocument(self): print(\\"Starting document...\\") def endDocument(self): print(\\"Ending document...\\") def startElement(self, name, attrs): print(f\\"Start element: {name}, attributes: {dict(attrs)}\\") def endElement(self, name): print(f\\"End element: {name}\\") def characters(self, content): print(f\\"Character data: {content}\\") class MyErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal error: {exception}\\") def warning(self, exception): print(f\\"Warning: {exception}\\") class MyEntityResolver(xml.sax.handler.EntityResolver): def resolveEntity(self, publicId, systemId): # Custom resolution logic return systemId class MyLexicalHandler(xml.sax.handler.LexicalHandler): def comment(self, content): print(f\\"Comment: {content}\\") def startCDATA(self): print(\\"Starting CDATA section...\\") def endCDATA(self): print(\\"Ending CDATA section...\\") # XML Parsing logic if __name__ == \\"__main__\\": parser = xml.sax.make_parser() parser.setContentHandler(MyContentHandler()) parser.setErrorHandler(MyErrorHandler()) parser.setEntityResolver(MyEntityResolver()) # Optional: Set LexicalHandler if supported. # parser.setProperty(xml.sax.handler.property_lexical_handler, MyLexicalHandler()) with open(\\"sample.xml\\", \\"r\\") as xml_file: parser.parse(xml_file) ``` **Notes**: - Ensure to install necessary Python packages (if any) before running the program. - Test thoroughly with various XML configurations to validate the robustness of the solution.","solution":"import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler): def startDocument(self): print(\\"Starting document...\\") def endDocument(self): print(\\"Ending document...\\") def startElement(self, name, attrs): print(f\\"Start element: {name}, attributes: {dict(attrs)}\\") def endElement(self, name): print(f\\"End element: {name}\\") def characters(self, content): if content.strip(): print(f\\"Character data: {content.strip()}\\") class MyErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal error: {exception}\\") def warning(self, exception): print(f\\"Warning: {exception}\\") class MyEntityResolver(xml.sax.handler.EntityResolver): def resolveEntity(self, publicId, systemId): print(f\\"Resolving entity: publicId={publicId}, systemId={systemId}\\") return systemId class MyLexicalHandler(xml.sax.handler.LexicalHandler): def comment(self, content): print(f\\"Comment: {content}\\") def startCDATA(self): print(\\"Starting CDATA section...\\") def endCDATA(self): print(\\"Ending CDATA section...\\") # XML Parsing logic def parse_xml_file(file_path): parser = xml.sax.make_parser() parser.setContentHandler(MyContentHandler()) parser.setErrorHandler(MyErrorHandler()) parser.setEntityResolver(MyEntityResolver()) # Optional: Set LexicalHandler if supported. parser.setProperty(xml.sax.handler.property_lexical_handler, MyLexicalHandler()) with open(file_path, \\"r\\") as xml_file: parser.parse(xml_file)"},{"question":"# Unix User Information Fetcher You are required to implement a Python function that queries user information from a Unix system using the `pwd` module. The function will: 1. Retrieve user information based on both user ID and user name. 2. Handle cases where the user ID or user name does not exist. 3. Return a combined result of user information. Function Signature ```python import pwd def get_user_info(uid: int, username: str) -> dict: pass ``` Parameters - `uid` (int): The user ID to look up in the password database. - `username` (str): The user name to look up in the password database. Returns - A dictionary with the following structure: ```python { \\"uid_info\\": {\\"pw_name\\": ..., \\"pw_passwd\\": ..., \\"pw_uid\\": ..., \\"pw_gid\\": ..., \\"pw_gecos\\": ..., \\"pw_dir\\": ..., \\"pw_shell\\": ...}, \\"username_info\\": {\\"pw_name\\": ..., \\"pw_passwd\\": ..., \\"pw_uid\\": ..., \\"pw_gid\\": ..., \\"pw_gecos\\": ..., \\"pw_dir\\": ..., \\"pw_shell\\": ...} } ``` If a user ID or username does not exist, their corresponding info should be `None`. Constraints - The function should handle the potential `KeyError` if a user ID or username does not exist in the database. - Avoid loading the entire password database using `pwd.getpwall()` due to performance considerations. Example ```python # Example: Consider these are valid user information on the system print(get_user_info(1001, \\"testuser\\")) # Expected Output: { \\"uid_info\\": {\\"pw_name\\": \\"testuser\\", \\"pw_passwd\\": \\"x\\", \\"pw_uid\\": 1001, \\"pw_gid\\": 1001, \\"pw_gecos\\": \\"Test User\\", \\"pw_dir\\": \\"/home/testuser\\", \\"pw_shell\\": \\"/bin/bash\\"}, \\"username_info\\": {\\"pw_name\\": \\"testuser\\", \\"pw_passwd\\": \\"x\\", \\"pw_uid\\": 1001, \\"pw_gid\\": 1001, \\"pw_gecos\\": \\"Test User\\", \\"pw_dir\\": \\"/home/testuser\\", \\"pw_shell\\": \\"/bin/bash\\"} } # Example: User id does not exist but username does print(get_user_info(9999, \\"testuser\\")) # Expected Output: { \\"uid_info\\": None, \\"username_info\\": {\\"pw_name\\": \\"testuser\\", \\"pw_passwd\\": \\"x\\", \\"pw_uid\\": 1001, \\"pw_gid\\": 1001, \\"pw_gecos\\": \\"Test User\\", \\"pw_dir\\": \\"/home/testuser\\", \\"pw_shell\\": \\"/bin/bash\\"} } ``` Notes - Always verify user input and handle exceptions properly. - Ensure proper format of returned data to match the given structure.","solution":"import pwd def get_user_info(uid: int, username: str) -> dict: user_info = { \\"uid_info\\": None, \\"username_info\\": None } try: uid_info = pwd.getpwuid(uid) user_info[\\"uid_info\\"] = { \\"pw_name\\": uid_info.pw_name, \\"pw_passwd\\": uid_info.pw_passwd, \\"pw_uid\\": uid_info.pw_uid, \\"pw_gid\\": uid_info.pw_gid, \\"pw_gecos\\": uid_info.pw_gecos, \\"pw_dir\\": uid_info.pw_dir, \\"pw_shell\\": uid_info.pw_shell } except KeyError: user_info[\\"uid_info\\"] = None try: username_info = pwd.getpwnam(username) user_info[\\"username_info\\"] = { \\"pw_name\\": username_info.pw_name, \\"pw_passwd\\": username_info.pw_passwd, \\"pw_uid\\": username_info.pw_uid, \\"pw_gid\\": username_info.pw_gid, \\"pw_gecos\\": username_info.pw_gecos, \\"pw_dir\\": username_info.pw_dir, \\"pw_shell\\": username_info.pw_shell } except KeyError: user_info[\\"username_info\\"] = None return user_info"},{"question":"Objective: To assess your understanding of pandas, specifically in utilizing assertion functions for validating data frames and handling exceptions. Problem Statement: You are given two data frames that represent sales data for a company. Write a function `validate_sales_data` that performs the following tasks: 1. Check if the two given data frames are equal using the `pandas.testing.assert_frame_equal` function. 2. If the data frames are not equal, handle the exception by printing an appropriate error message. 3. Additionally, create a custom function `check_for_nulls` that checks whether any null values exist in the data frames and raises a `pandas.errors.DataError` if null values are found. Input: - `df1` (pandas.DataFrame): The first sales data frame. - `df2` (pandas.DataFrame): The second sales data frame. Output: - The function should return a boolean value `True` if both data frames are equal and no null values are found. - If the data frames are not equal or null values are found, the function should raise an appropriate exception and return `False`. Constraints: - Both data frames have the same structure but might not have the same data. - The data frames contain columns \'Date\', \'Product\', \'Sales\', and \'Revenue\'. Example: ```python import pandas as pd # Example data frames data1 = {\'Date\': [\'2023-01-01\', \'2023-01-02\'], \'Product\': [\'A\', \'B\'], \'Sales\': [10, 20], \'Revenue\': [100, 200]} data2 = {\'Date\': [\'2023-01-01\', \'2023-01-02\'], \'Product\': [\'A\', \'B\'], \'Sales\': [10, 20], \'Revenue\': [100, 250]} df1 = pd.DataFrame(data1) df2 = pd.DataFrame(data2) def validate_sales_data(df1, df2): try: import pandas.testing as pdt # Check for null values def check_for_nulls(df): if df.isnull().values.any(): raise pd.errors.DataError(\\"Data frame contains null values.\\") # Use the custom function to check for nulls check_for_nulls(df1) check_for_nulls(df2) # Assert if data frames are equal pdt.assert_frame_equal(df1, df2) return True except pd.errors.DataError as e: print(f\\"DataError: {e}\\") return False except AssertionError as e: print(f\\"AssertionError: Data frames are not equal. {e}\\") return False # Running the function print(validate_sales_data(df1, df2)) # Output should be False with AssertionError ``` The function `validate_sales_data` will use pandas assertion tools and custom error handling to ensure the integrity and correctness of the data frames.","solution":"import pandas as pd def validate_sales_data(df1, df2): try: import pandas.testing as pdt def check_for_nulls(df): if df.isnull().values.any(): raise pd.errors.DataError(\\"Data frame contains null values.\\") # Check for null values check_for_nulls(df1) check_for_nulls(df2) # Assert if data frames are equal pdt.assert_frame_equal(df1, df2) return True except pd.errors.DataError as e: print(f\\"DataError: {e}\\") return False except AssertionError as e: print(f\\"AssertionError: Data frames are not equal. {e}\\") return False"},{"question":"# Python Coding Assessment **Objective:** You will be required to implement a Python function that utilizes the Distutils package to generate a basic setup script for a given project structure. This function will demonstrate your understanding of creating and managing Python package distributions using Distutils. **Problem Statement:** Write a function `create_setup_script` that generates a `setup.py` file for a given project structure. The function should take a project structure as a dictionary containing package names as keys and a list of module names as values. The setup script should include the necessary metadata and other required configurations needed to build and distribute the package. **Function Signature:** ```python def create_setup_script(project_name: str, version: str, author: str, author_email: str, project_structure: dict) -> str: pass ``` **Input:** - `project_name` (str): The name of the project. - `version` (str): The version of the project. - `author` (str): The name of the author. - `author_email` (str): The email address of the author. - `project_structure` (dict): A dictionary where keys are package names (str) and values are lists of module names (str) within the package. **Output:** - Returns a string representing the content of a `setup.py` file. **Example Input:** ```python project_name = \\"MyProject\\" version = \\"1.0.0\\" author = \\"John Doe\\" author_email = \\"john.doe@example.com\\" project_structure = { \\"mypackage\\": [\\"module1\\", \\"module2\\"], \\"anotherpackage\\": [\\"module3\\"] } ``` **Example Output:** ```python from distutils.core import setup setup( name=\'MyProject\', version=\'1.0.0\', author=\'John Doe\', author_email=\'john.doe@example.com\', packages=[\'mypackage\', \'anotherpackage\'], py_modules=[\'mypackage.module1\', \'mypackage.module2\', \'anotherpackage.module3\'] ) ``` **Constraints:** - The function should handle any valid project structure as described above. - The output string should be properly formatted and suitable for use as a `setup.py` file. **Hint:** - The `setup` function from `distutils.core` will be used to create the setup script. - You need to flatten the project structure to list all individual modules correctly. **Performance Requirements:** - Ensure that the generated script handles large project structures efficiently.","solution":"def create_setup_script(project_name: str, version: str, author: str, author_email: str, project_structure: dict) -> str: Creates the content of a setup.py file for the given project structure. packages = list(project_structure.keys()) py_modules = [f\\"{pkg}.{mod}\\" for pkg, mods in project_structure.items() for mod in mods] setup_script = f from distutils.core import setup setup( name=\'{project_name}\', version=\'{version}\', author=\'{author}\', author_email=\'{author_email}\', packages={packages}, py_modules={py_modules} ) return setup_script.strip()"},{"question":"# Complex Number Operations and Conversions Problem Statement: You are required to implement and test a function to perform arithmetic operations on complex numbers and handle conversions between Python and C-style complex number representations. You will implement and use the provided functions/runtime details from the documentation. Requirements: 1. **Implement Function**: Implement a function called `complex_operations` that takes two complex numbers and an operation as input and returns the result in the desired representation. - **Function Signature**: `def complex_operations(c1, c2, operation, convert_to_python=True):` - **Parameters**: - `c1`: First complex number (a tuple of two floats representing the real and imaginary parts). - `c2`: Second complex number (a tuple of two floats representing the real and imaginary parts). - `operation`: A string indicating the operation (`\\"sum\\"`, `\\"diff\\"`, `\\"prod\\"`, `\\"quot\\"`, `\\"pow\\"`). - `convert_to_python`: If `True`, return a Python complex number object; otherwise, return a C structure (default is `True`). - **Returns**: Resulting complex number in the desired representation. 2. **Constraints**: - Implement conversions between Python complex type and C structure. - Handle division by zero and invalid exponentiation appropriately, returning a message or proper error handling. - Utilize available functions as per the provided documentation for operations and conversions. 3. **Example Usage**: ```python # Example usage: c1 = (1.0, 2.0) c2 = (3.0, 4.0) result_py = complex_operations(c1, c2, \\"sum\\", convert_to_python=True) result_c = complex_operations(c1, c2, \\"sum\\", convert_to_python=False) assert isinstance(result_py, complex) # Should be True print(result_c) # Should print Py_complex structure values ``` 4. **Performance**: - The function should complete all operations efficiently within a reasonable time frame. Additional Information: - Ensure the function is well-documented with proper coding standards. - Test your implementation with various edge cases to check for robustness and correctness.","solution":"import cmath def complex_operations(c1, c2, operation, convert_to_python=True): Perform operations on complex numbers and convert between Python and C-style representations. Parameters: c1: tuple of floats (representing a complex number) c2: tuple of floats (representing a complex number) operation: str (\\"sum\\", \\"diff\\", \\"prod\\", \\"quot\\", \\"pow\\") convert_to_python: bool (if True, return Python complex number object; otherwise, return C struct) Returns: complex number in the desired representation def to_python_complex(c): Convert C-style complex number to Python complex type. return complex(c[0], c[1]) def to_c_complex(z): Convert Python complex type to C-style complex number. return (z.real, z.imag) # Convert to Python complex types for easier arithmetic operation z1 = to_python_complex(c1) z2 = to_python_complex(c2) if operation == \\"sum\\": result = z1 + z2 elif operation == \\"diff\\": result = z1 - z2 elif operation == \\"prod\\": result = z1 * z2 elif operation == \\"quot\\": if z2 == 0: raise ZeroDivisionError(\\"Cannot divide by zero\\") result = z1 / z2 elif operation == \\"pow\\": result = z1 ** z2 else: raise ValueError(\\"Invalid operation\\") if convert_to_python: return result else: return to_c_complex(result)"},{"question":"# Question: Custom Set Operations In this assignment, you need to implement a custom class `CustomSet` that uses internal C API functions provided in the `python310` package for set manipulations. This class should encapsulate functionality for handling both `set` and `frozenset`, and expose a Pythonic interface for them. Requirements: 1. Implement the `CustomSet` class with the following methods: - `__init__(self, elements)`: Initializes a new `set` or `frozenset` with elements. - `add(self, element)`: Adds an element to the set. - `discard(self, element)`: Removes an element from the set if present, without raising an error if the element is not found. - `contains(self, element)`: Checks if the element is in the set. - `pop(self)`: Pops a random element from the set. - `size(self)`: Returns the size of the set. 2. Ensure that the class handles immutability of `frozenset` correctly by raising an appropriate error when attempting to modify it. Input/Output Specifications: - **Input**: Each input test case will consist of operations to be performed on an instance of `CustomSet`. - **Output**: For each operation, output the result as specified or handle the operation as required. Constraints: - The operations involving adding and discarding elements should only be allowed for mutable sets. - Handle exceptions as per instructions (e.g., attempting modifications on `frozenset`). Performance Requirements: - The methods should be efficient and make use of Python\'s in-built set operations wherever appropriate. Example: ```python class CustomSet: def __init__(self, elements): pass def add(self, element): pass def discard(self, element): pass def contains(self, element): pass def pop(self): pass def size(self): pass # Sample usage and expected behavior cs = CustomSet([1, 2, 3]) cs.add(4) # Adds 4 to the set cs.discard(2) # Removes 2 from the set print(cs.contains(3)) # Returns True if 3 is in the set element = cs.pop() # Removes and returns a random element print(cs.size()) # Returns the size of the set ``` Your task is to complete the implementation of the `CustomSet` class according to the specifications and ensure it works correctly for both `set` and `frozenset`.","solution":"class CustomSet: def __init__(self, elements, is_frozenset=False): Initializes a new set or frozenset with elements. :param elements: an iterable of initial elements for the set or frozenset :param is_frozenset: a boolean to decide whether to use frozenset, default is False self.is_frozenset = is_frozenset if is_frozenset: self.elements = frozenset(elements) else: self.elements = set(elements) def add(self, element): Adds an element to the set. Not allowed for frozenset. :param element: the element to add if self.is_frozenset: raise TypeError(\\"Cannot add elements to a frozenset\\") self.elements.add(element) def discard(self, element): Removes an element from the set if present. Not allowed for frozenset. :param element: the element to remove if self.is_frozenset: raise TypeError(\\"Cannot discard elements from a frozenset\\") self.elements.discard(element) def contains(self, element): Checks if the element is in the set. :param element: the element to check :return: True if element is in the set, else False return element in self.elements def pop(self): Pops a random element from the set. Not allowed for frozenset. :return: a random element from the set if self.is_frozenset: raise TypeError(\\"Cannot pop elements from a frozenset\\") return self.elements.pop() def size(self): Returns the size of the set. :return: the number of elements in the set return len(self.elements)"},{"question":"**Objective:** To assess your understanding of the `asyncio` exceptions in Python 3.10, you will write a function that performs asynchronous I/O operations and handles specific `asyncio` exceptions gracefully. **Problem Statement:** Implement an asynchronous function `perform_async_operations()` which performs the following tasks: 1. Asynchronously reads data from a given list of input file paths using `asyncio` stream APIs. 2. Simulates sending data over a network using an `asyncio` stream to an output file. Make sure to handle the following exceptions appropriately: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.InvalidStateError` - `asyncio.SendfileNotAvailableError` - `asyncio.IncompleteReadError` - `asyncio.LimitOverrunError` **Function Signature:** ```python async def perform_async_operations(input_files: List[str], output_file: str) -> None: pass ``` **Input:** - `input_files` (List[str]): A list of file paths from which data needs to be read. - `output_file` (str): A file path to which data will be written. **Output:** - None. The function should handle exceptions and ensure all operations are logged or handled properly. **Constraints:** - Use the asyncio streams for file I/O operations. - Properly manage and handle the specified exceptions. - Ensure that the operations are performed efficiently. - The function must be asynchronous and use `await` where necessary. **Example:** ```python import asyncio async def main(): await perform_async_operations([\'file1.txt\', \'file2.txt\'], \'output.txt\') asyncio.run(main()) ``` **Additional Information:** - **asyncio.TimeoutError:** Raised if an operation exceeds the specified timeout. - **asyncio.CancelledError:** Raised when a task is cancelled. - **asyncio.InvalidStateError:** Raised when an invalid state is encountered in a Task or Future. - **asyncio.SendfileNotAvailableError:** Raised when the `sendfile` syscall is not available. - **asyncio.IncompleteReadError:** Raised when a read operation does not complete successfully. - **asyncio.LimitOverrunError:** Raised when a buffer size limit is reached while looking for a separator. Handling these exceptions involves catching them using `try`/`except` blocks and taking appropriate action (such as logging the error, retrying the operation, or failing gracefully). **Performance Requirements:** - The function should efficiently handle file operations, manage resources appropriately, and minimize the impact of exceptions on the overall operation. Good luck, and happy coding!","solution":"import asyncio from typing import List async def read_file(path: str) -> str: try: reader = await asyncio.open_file(path, mode=\'r\') data = await reader.read() await reader.close() return data except asyncio.TimeoutError: print(f\'Timeout error reading {path}\') except asyncio.CancelledError: print(f\'Reading {path} was cancelled\') except asyncio.IncompleteReadError as e: print(f\'Incomplete read error for {path}: {e.partial}\') except asyncio.LimitOverrunError as e: print(f\'Limit overrun error for {path}: {e.consumed}\') except asyncio.InvalidStateError: print(f\'Invalid state encountered while reading {path}\') return \\"\\" async def write_file(path: str, data: str) -> None: try: writer = await asyncio.open_file(path, mode=\'w\') await writer.write(data) await writer.close() except asyncio.TimeoutError: print(f\'Timeout error writing to {path}\') except asyncio.CancelledError: print(f\'Writing to {path} was cancelled\') except asyncio.SendfileNotAvailableError: print(f\'Sendfile not available error writing to {path}\') except asyncio.InvalidStateError: print(f\'Invalid state encountered while writing to {path}\') async def perform_async_operations(input_files: List[str], output_file: str) -> None: data = \\"\\" for file in input_files: data += await read_file(file) await write_file(output_file, data)"},{"question":"# Advanced Boolean Operations in Python Objective Your task is to implement a function that performs advanced Boolean operations involving type checks, and logical manipulations, demonstrating a deep understanding of Boolean values in Python. Problem Statement You must implement the function `advanced_boolean_operations`. ```python def advanced_boolean_operations(values: list) -> bool: Given a list of values, return True if all values in the list are Python Boolean True. Implement it without using the built-in all() function or directly comparing to True. Args: - values (list): A list of values of any type. Returns: - bool: Returns True if all values in the list prove to be True after a type check. Example: - advanced_boolean_operations([1, \\"non-empty\\", [0]]) should return True - advanced_boolean_operations([0, \\"non-empty\\", [0]]) should return False pass ``` Requirements 1. The function should **not use** Python\'s built-in `all()` function. 2. Direct comparison to Python `True` or `False` is **not permitted**. 3. Perform type checking to determine if a value should be considered `True`. 4. Utilize Boolean operations to determine the overall truth of the list. Constraints - The length of the `values` list will be between 1 and 1000. - The elements in the list can be of any type. Explanation Python\'s Boolean interpretation implicitly considers the following: - Numbers are `True` if non-zero. - Strings are `True` if non-empty. - Lists, tuples, and dicts are `True` if they contain elements. - Custom objects are `True` if they have a __bool__() method returning True or no such method but are non-zero in length. Your function should thus consider these rules while evaluating the entire list\'s Boolean interpretation. Performance The function should be optimized to handle the upper constraint limits in O(n) time complexity.","solution":"def advanced_boolean_operations(values: list) -> bool: Given a list of values, return True if all values in the list are Python Boolean True. Implement it without using the built-in all() function or directly comparing to True. Args: - values (list): A list of values of any type. Returns: - bool: Returns True if all values in the list prove to be True after a type check. for value in values: # Utilize implicit Boolean checks, as directly checking `value is True` is not allowed if not value: return False return True"},{"question":"You are provided with a CSV file containing some raw data which includes duplicate row labels. Your task is to write a function in Python using pandas that will: 1. Read the raw data from the CSV file. 2. Identify and remove any duplicates in the row labels. 3. Ensure that no operation you perform reintroduces duplicates. 4. Return a cleaned DataFrame with duplicates permanently disallowed. Function Signature ```python def clean_and_deduplicate_csv(file_path: str) -> pd.DataFrame: pass ``` Parameters - `file_path` (str): The path to the CSV file containing the raw data. Returns - `pd.DataFrame`: A pandas DataFrame that contains the cleaned data with duplicates removed and any subsequent duplicate introduction disallowed. Example Input ```csv index,val1,val2 a,10,100 a,20,200 b,30,300 c,40,400 ``` Example Output ```python val1 val2 a 10 100 b 30 300 c 40 400 ``` Constraints - You must use `pandas` for data manipulation. - Duplicate row labels should be handled by retaining only the first occurrence. - Use the `allows_duplicate_labels` flag to disallow duplicates going forward. - The function should handle large datasets efficiently. Guidelines - Use `pandas.read_csv` to read the data. - Use `duplicated` and boolean indexing to remove duplicate row labels. - Set `flags.allows_duplicate_labels` to `False` after deduplication to ensure no duplicates are added later in the pipeline. Testing Your function will be tested with several CSV files containing various numbers of rows and duplicate row labels. Ensure the function works efficiently and correctly disallows duplicates after cleaning.","solution":"import pandas as pd def clean_and_deduplicate_csv(file_path: str) -> pd.DataFrame: # Read the raw data from the CSV file df = pd.read_csv(file_path, index_col=0) # Keep only the first occurrence of each duplicate index df = df[~df.index.duplicated(keep=\'first\')] # Ensure that no operation reintroduces duplicates df.flags.allows_duplicate_labels = False return df"},{"question":"Objective: You are required to demonstrate your understanding of the `sklearn.datasets` module by generating various types of datasets and applying appropriate machine learning algorithms to these datasets. The goal is to create, visualize, and analyze these datasets. Task: 1. **Dataset Generation**: - Generate the following datasets: 1. A multiclass dataset with 4 centers and a standard deviation of 0.6 using `make_blobs`. 2. A binary classification dataset with 3 informative features and 2 clusters per class using `make_classification`. 3. A dataset with 2D points divided into 2 interleaving half-circles using `make_moons`, with added Gaussian noise. 4. A regression dataset with 2 informative features using `make_regression`. 2. **Visualization**: - Visualize each dataset using `matplotlib` to provide a clear understanding of the generated data. Properly label each plot. 3. **Classification and Regression**: - Apply a suitable classifier (e.g., `sklearn.linear_model`\'s `LogisticRegression` or `sklearn.svm`\'s `SVC`) to the classification datasets and a regression model (e.g., `sklearn.linear_model`\'s `LinearRegression`) to the regression dataset. - Evaluate the performance of the models using appropriate metrics (e.g., accuracy for classification, mean squared error for regression). Expected Input and Output Formats: - No specific input format requirements. - Output should be a set of matplotlib plots for dataset visualization and printed performance metrics for the models. Constraints: - Use `random_state=0` where applicable for reproducibility. - The classifier and regression model are your choice, but you must justify your selection. Performance Requirements: - Ensure that the code runs efficiently and the visualizations are clear. Submission: - Submit your code in a Jupyter notebook format (.ipynb) with explanations and justifications for each step. Example Template: ```python import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_moons, make_regression from sklearn.linear_model import LogisticRegression, LinearRegression from sklearn.svm import SVC from sklearn.metrics import accuracy_score, mean_squared_error # Generate Datasets X_blobs, y_blobs = make_blobs(centers=4, cluster_std=0.6, random_state=0) X_classification, y_classification = make_classification(n_features=5, n_informative=3, n_clusters_per_class=2, random_state=0) X_moons, y_moons = make_moons(noise=0.1, random_state=0) X_regression, y_regression = make_regression(n_features=2, n_informative=2, noise=0.1, random_state=0) # Visualization plt.figure(figsize=(12, 10)) # Scatter plots for each dataset plt.subplot(2, 2, 1) plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs) plt.title(\\"make_blobs\\") plt.subplot(2, 2, 2) plt.scatter(X_classification[:, 0], X_classification[:, 1], c=y_classification) plt.title(\\"make_classification\\") plt.subplot(2, 2, 3) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons) plt.title(\\"make_moons\\") plt.subplot(2, 2, 4) plt.scatter(X_regression[:, 0], X_regression[:, 1], c=y_regression) plt.title(\\"make_regression\\") plt.tight_layout() plt.show() # Classification and Regression Models # Add your code here for model training, prediction, and performance evaluation ```","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_moons, make_regression from sklearn.linear_model import LogisticRegression, LinearRegression from sklearn.svm import SVC from sklearn.metrics import accuracy_score, mean_squared_error # Generate Datasets X_blobs, y_blobs = make_blobs(centers=4, cluster_std=0.6, random_state=0) X_classification, y_classification = make_classification(n_features=5, n_informative=3, n_clusters_per_class=2, random_state=0) X_moons, y_moons = make_moons(noise=0.1, random_state=0) X_regression, y_regression = make_regression(n_features=2, n_informative=2, noise=0.1, random_state=0) # Visualization plt.figure(figsize=(12, 10)) # Scatter plots for each dataset plt.subplot(2, 2, 1) plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs) plt.title(\\"make_blobs\\") plt.subplot(2, 2, 2) plt.scatter(X_classification[:, 0], X_classification[:, 1], c=y_classification) plt.title(\\"make_classification\\") plt.subplot(2, 2, 3) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons) plt.title(\\"make_moons\\") plt.subplot(2, 2, 4) plt.scatter(X_regression[:, 0], X_regression[:, 1], c=y_regression) plt.title(\\"make_regression\\") plt.tight_layout() plt.show() # Classification and Regression Models # Logistic Regression on make_blobs dataset clf_blobs = LogisticRegression(random_state=0) clf_blobs.fit(X_blobs, y_blobs) pred_blobs = clf_blobs.predict(X_blobs) acc_blobs = accuracy_score(y_blobs, pred_blobs) print(f\\"Accuracy on make_blobs dataset: {acc_blobs:.2f}\\") # Support Vector Classification on make_classification dataset svc_classification = SVC(random_state=0) svc_classification.fit(X_classification, y_classification) pred_classification = svc_classification.predict(X_classification) acc_classification = accuracy_score(y_classification, pred_classification) print(f\\"Accuracy on make_classification dataset: {acc_classification:.2f}\\") # Logistic Regression on make_moons dataset clf_moons = LogisticRegression(random_state=0) clf_moons.fit(X_moons, y_moons) pred_moons = clf_moons.predict(X_moons) acc_moons = accuracy_score(y_moons, pred_moons) print(f\\"Accuracy on make_moons dataset: {acc_moons:.2f}\\") # Linear Regression on make_regression dataset reg = LinearRegression() reg.fit(X_regression, y_regression) pred_regression = reg.predict(X_regression) mse_regression = mean_squared_error(y_regression, pred_regression) print(f\\"Mean Squared Error on make_regression dataset: {mse_regression:.2f}\\")"},{"question":"# Challenging Coding Assessment Question: Objective: You are required to write a function that processes a stereo audio fragment and generates a processed mono audio fragment. The processing involves converting the stereo fragment to mono, normalizing the audio to maximum peak value, and then reversing the fragment. Task: Write a function `process_audio(stereo_frag: bytes, width: int) -> bytes` that takes a stereo audio fragment and processes it as described below: 1. **Convert Stereo to Mono**: Use the `audioop.tomono()` function to convert the stereo fragment to mono. Assume the left and right channel factors (`lfactor` and `rfactor`) are both 0.5, as it equally weighs both channels in the mono conversion. 2. **Normalize Audio**: Adjust the audio to maximize its peak value using the `audioop.max()` function to find the current peak. Use `audioop.mul()` to scale the audio fragment so that its maximum value reaches the highest possible value for the given width. 3. **Reverse the Fragment**: Reverse the mono audio fragment. Requirements: - **Input**: - `stereo_frag` (bytes): The input stereo audio fragment. - `width` (int): The sample width in bytes (1, 2, 3, or 4). - **Output**: - The processed mono audio fragment (bytes). - **Constraints**: - Both channels of the stereo audio are of the same length. - Sample width (`width`) is provided correctly. - The expected sample width for stereo fragments should be actual and inherently accurate. Performance: - The function should efficiently handle audio fragments up to several megabytes in size. Sample Execution: ```python def process_audio(stereo_frag: bytes, width: int) -> bytes: import audioop # Step 1: Convert stereo to mono mono_frag = audioop.tomono(stereo_frag, width, 0.5, 0.5) # Step 2: Normalize the audio to the maximum peak value old_max = audioop.max(mono_frag, width) if old_max == 0: # To avoid division by zero return mono_frag max_val = (2 ** (8*width - 1)) - 1 # Maximum possible value for the sample width normalization_factor = max_val / old_max normalized_frag = audioop.mul(mono_frag, width, normalization_factor) # Step 3: Reverse the fragment reversed_frag = audioop.reverse(normalized_frag, width) return reversed_frag # Example usage stereo_frag = b\'x01x02x03x04\' # This is an example; real audio data will be more complex width = 2 processed_audio = process_audio(stereo_frag, width) print(processed_audio) ``` Feel free to test the function with actual audio data fragments to ensure correctness and performance efficiency.","solution":"def process_audio(stereo_frag: bytes, width: int) -> bytes: import audioop # Step 1: Convert stereo to mono mono_frag = audioop.tomono(stereo_frag, width, 0.5, 0.5) # Step 2: Normalize the audio to the maximum peak value old_max = audioop.max(mono_frag, width) if old_max == 0: return mono_frag max_val = (2 ** (8*width - 1)) - 1 # Maximum possible value for the sample width normalization_factor = max_val / old_max normalized_frag = audioop.mul(mono_frag, width, normalization_factor) # Step 3: Reverse the fragment reversed_frag = audioop.reverse(normalized_frag, width) return reversed_frag"},{"question":"# Advanced Set Implementation Objective Implement a custom set class in Python that mimics certain behaviors of Python\'s built-in set type, but with added constraints and functionalities. This class will provide a deeper understanding of Python\'s set operations and internal workings. Requirements You are required to implement a class `AdvancedSet` with the following methods: 1. **Initialization**: Initialize the set with an optional iterable. ```python def __init__(self, iterable=None): # Initialize the set with elements from the iterable (if provided) pass ``` 2. **`add` method**: Add an element to the set. ```python def add(self, element): # Add the element to the set pass ``` 3. **`remove` method**: Remove an element from the set. Raise a `KeyError` if the element is not present. ```python def remove(self, element): # Remove the element from the set, raising KeyError if not present pass ``` 4. **`contains` method**: Return `True` if the element is in the set, `False` otherwise. ```python def contains(self, element): # Check if the element is in the set pass ``` 5. **`union` method**: Return a new set that is the union of the current set and another set. ```python def union(self, other_set): # Return a new set which is the union of the two sets pass ``` 6. **`intersection` method**: Return a new set that is the intersection of the current set and another set. ```python def intersection(self, other_set): # Return a new set which is the intersection of the two sets pass ``` 7. **`difference` method**: Return a new set that is the difference of the current set and another set. ```python def difference(self, other_set): # Return a new set which is the difference of the two sets pass ``` 8. **`issubset` method**: Return `True` if the current set is a subset of another set, `False` otherwise. ```python def issubset(self, other_set): # Check if the current set is a subset of the other set pass ``` 9. **`issuperset` method**: Return `True` if the current set is a superset of another set, `False` otherwise. ```python def issuperset(self, other_set): # Check if the current set is a superset of the other set pass ``` Input and Output Formats - Initialization can optionally take an iterable (e.g., list, tuple) to initialize the set. If no iterable is provided, create an empty set. - Each method should handle its respective data format and return the appropriate types as specified above. - The methods `add`, `remove`, and `contains` should handle individual elements. `union`, `intersection`, `difference`, `issubset`, `issuperset` should handle sets. Constraints - Elements will always be hashable since we\'re simulating basic set operations. - Your implementation should aim for efficiency, mirroring the average time complexities of the built-in set operations whenever possible. Example Usage ```python s1 = AdvancedSet([1, 2, 3]) s2 = AdvancedSet([3, 4, 5]) s1.add(4) # s1 is now {1, 2, 3, 4} s1.remove(2) # s1 is now {1, 3, 4} print(s1.contains(3)) # True print(s1.contains(5)) # False s3 = s1.union(s2) # s3 is {1, 3, 4, 5} s4 = s1.intersection(s2) # s4 is {3, 4} s5 = s1.difference(s2) # s5 is {1} print(s1.issubset(s3)) # True print(s3.issuperset(s1)) # True ``` Your task is to implement the `AdvancedSet` class with the above methods ensuring that all functionalities work as expected.","solution":"class AdvancedSet: def __init__(self, iterable=None): Initialize the set with elements from the iterable (if provided). self.elements = set(iterable) if iterable else set() def add(self, element): Add the element to the set. self.elements.add(element) def remove(self, element): Remove the element from the set, raising KeyError if not present. self.elements.remove(element) def contains(self, element): Check if the element is in the set. return element in self.elements def union(self, other_set): Return a new set which is the union of the current set and another set. new_set = AdvancedSet() new_set.elements = self.elements.union(other_set.elements) return new_set def intersection(self, other_set): Return a new set which is the intersection of the two sets. new_set = AdvancedSet() new_set.elements = self.elements.intersection(other_set.elements) return new_set def difference(self, other_set): Return a new set which is the difference of the two sets. new_set = AdvancedSet() new_set.elements = self.elements.difference(other_set.elements) return new_set def issubset(self, other_set): Check if the current set is a subset of the other set. return self.elements.issubset(other_set.elements) def issuperset(self, other_set): Check if the current set is a superset of the other set. return self.elements.issuperset(other_set.elements)"},{"question":"# Complex Number Operations in Python You are required to implement a Python class to perform various operations on complex numbers. To verify your comprehension of Python\'s handling of complex numbers and the bridging between Python and C, you will implement methods to handle both Python object-based operations and C structure-based calculations. Task 1. Implement a Python class called `ComplexNumber` with the following methods: - `__init__(self, real: float, imag: float)`: Initializes a complex number object. - `add(self, other: \'ComplexNumber\') -> \'ComplexNumber\'`: Returns the sum of the current complex number and another complex number. - `subtract(self, other: \'ComplexNumber\') -> \'ComplexNumber\'`: Returns the difference between the current complex number and another complex number. - `multiply(self, other: \'ComplexNumber\') -> \'ComplexNumber\'`: Returns the product of the current complex number and another complex number. - `divide(self, other: \'ComplexNumber\') -> \'ComplexNumber\'`: Returns the quotient of the current complex number divided by another complex number. - `negate(self) -> \'ComplexNumber\'`: Returns the negation of the current complex number. - `to_c_structure(self) -> dict`: Returns a dictionary representation of the complex number as a C structure with keys `real` and `imag`. 2. Implement wrapper functions to execute the given C API functions within your class methods: - `c_add(left: dict, right: dict) -> dict` - `c_subtract(left: dict, right: dict) -> dict` - `c_multiply(left: dict, right: dict) -> dict` - `c_divide(left: dict, right: dict) -> dict` - `c_negate(num: dict) -> dict` Constraints - You must use the provided C structure-based API functions within your class methods to perform arithmetic operations. - The wrapper functions should convert the dictionary representations to C structures, call the respective C function, and convert the result back to a dictionary. Example ```python # Example code usage a = ComplexNumber(2, 3) b = ComplexNumber(1, 1) c = a.add(b) # Expected: ComplexNumber(3, 4) d = a.subtract(b) # Expected: ComplexNumber(1, 2) e = a.multiply(b) # Expected: ComplexNumber(-1, 5) f = a.divide(b) # Expected: ComplexNumber(2.5, 0.5) g = a.negate() # Expected: ComplexNumber(-2, -3) print(c.to_c_structure()) # Expected: {\'real\': 3, \'imag\': 4} print(d.to_c_structure()) # Expected: {\'real\': 1, \'imag\': 2} print(e.to_c_structure()) # Expected: {\'real\': -1, \'imag\': 5} print(f.to_c_structure()) # Expected: {\'real\': 2.5, \'imag\': 0.5} print(g.to_c_structure()) # Expected: {\'real\': -2, \'imag\': -3} ``` Additional Information For the purpose of this assessment, assume that the `Py_complex` C structure and its associated functions provided in the documentation are accessible and usable from within your Python environment.","solution":"class ComplexNumber: def __init__(self, real: float, imag: float): self.real = real self.imag = imag def add(self, other: \'ComplexNumber\') -> \'ComplexNumber\': result = self.c_add(self.to_c_structure(), other.to_c_structure()) return ComplexNumber(result[\'real\'], result[\'imag\']) def subtract(self, other: \'ComplexNumber\') -> \'ComplexNumber\': result = self.c_subtract(self.to_c_structure(), other.to_c_structure()) return ComplexNumber(result[\'real\'], result[\'imag\']) def multiply(self, other: \'ComplexNumber\') -> \'ComplexNumber\': result = self.c_multiply(self.to_c_structure(), other.to_c_structure()) return ComplexNumber(result[\'real\'], result[\'imag\']) def divide(self, other: \'ComplexNumber\') -> \'ComplexNumber\': result = self.c_divide(self.to_c_structure(), other.to_c_structure()) return ComplexNumber(result[\'real\'], result[\'imag\']) def negate(self) -> \'ComplexNumber\': result = self.c_negate(self.to_c_structure()) return ComplexNumber(result[\'real\'], result[\'imag\']) def to_c_structure(self) -> dict: return {\'real\': self.real, \'imag\': self.imag} @staticmethod def c_add(left: dict, right: dict) -> dict: return {\'real\': left[\'real\'] + right[\'real\'], \'imag\': left[\'imag\'] + right[\'imag\']} @staticmethod def c_subtract(left: dict, right: dict) -> dict: return {\'real\': left[\'real\'] - right[\'real\'], \'imag\': left[\'imag\'] - right[\'imag\']} @staticmethod def c_multiply(left: dict, right: dict) -> dict: real = left[\'real\'] * right[\'real\'] - left[\'imag\'] * right[\'imag\'] imag = left[\'real\'] * right[\'imag\'] + left[\'imag\'] * right[\'real\'] return {\'real\': real, \'imag\': imag} @staticmethod def c_divide(left: dict, right: dict) -> dict: denom = right[\'real\'] ** 2 + right[\'imag\'] ** 2 real = (left[\'real\'] * right[\'real\'] + left[\'imag\'] * right[\'imag\']) / denom imag = (left[\'imag\'] * right[\'real\'] - left[\'real\'] * right[\'imag\']) / denom return {\'real\': real, \'imag\': imag} @staticmethod def c_negate(num: dict) -> dict: return {\'real\': -num[\'real\'], \'imag\': -num[\'imag\']}"},{"question":"Question: Implement a Custom Sequence Class # Objective: Create a class `CustomSequence` that behaves like a standard sequence (similar to lists or tuples) using the `collections.abc.Sequence` abstract base class. Your custom sequence will store items and provide necessary methods to comply with the `Sequence` interface. # Requirements: 1. **Class Definition**: `CustomSequence` must inherit from `collections.abc.Sequence`. 2. **Methods to Implement**: - `__init__(self, data)`: Initialize with an iterable `data`. - `__getitem__(self, index)`: Retrieve the item at the specified index. - `__len__(self)`: Return the total number of items in the sequence. 3. **Features**: - Support for slicing. - Implement a method `find(self, value)` to return the index of the first occurrence of `value` in the sequence or `-1` if not present. 4. **Constraints**: - You cannot use any built-in data structures like lists or tuples to store the items internally. # Input and Output Format: - **Initialization**: Initializes with an iterable of items. - **`__getitem__(self, index)`**: - Input: integer index (can be negative or a slice object). - Output: element(s) at the specified position(s). - **`__len__(self)`**: - Output: total number of elements in the sequence. - **`find(self, value)`**: - Input: value to be searched. - Output: index of the value or `-1`. # Example: ```python from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, data): # Your code here def __getitem__(self, index): # Your code here def __len__(self): # Your code here def find(self, value): # Your code here # Usage cs = CustomSequence([10, 20, 30, 40, 50]) print(len(cs)) # Output: 5 print(cs[1]) # Output: 20 print(cs[-1]) # Output: 50 print(cs[1:3]) # Output: [20, 30] print(cs.find(30)) # Output: 2 print(cs.find(100)) # Output: -1 ``` # Note: Ensure that all methods adhere to the required interface specifications and handle edge cases appropriately.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, data): self._data = {i: value for i, value in enumerate(data)} def __getitem__(self, index): if isinstance(index, slice): start, stop, step = index.indices(len(self)) return [self._data[i] for i in range(start, stop, step)] if index < 0: index += len(self) if index >= len(self) or index < 0: raise IndexError(\'Index out of range\') return self._data[index] def __len__(self): return len(self._data) def find(self, value): for index, item in self._data.items(): if item == value: return index return -1"},{"question":"# Question: Data Encoding and Decoding with Base64 Module You are tasked to create a Python program that can encode and decode given binary data using different base encoding schemes (Base16, Base32, Base64, and Base85). The program should handle exceptions gracefully and print meaningful error messages when an encoding or decoding operation fails. Requirements: 1. **Function: `encode_data(data: bytes, encoding: str) -> bytes`** - Inputs: - `data` (bytes): The binary data to be encoded. - `encoding` (str): The encoding type to be used. Possible values are `\\"base16\\"`, `\\"base32\\"`, `\\"base64\\"`, or `\\"base85\\"`. - Outputs: - Returns the encoded data as bytes. - Exceptions: - Raises a `ValueError` if the encoding type is not supported. 2. **Function: `decode_data(encoded_data: bytes, encoding: str) -> bytes`** - Inputs: - `encoded_data` (bytes): The encoded binary data. - `encoding` (str): The encoding type that was used for encoding. Possible values are `\\"base16\\"`, `\\"base32\\"`, `\\"base64\\"`, or `\\"base85\\"`. - Outputs: - Returns the decoded data as bytes. - Exceptions: - Raises a `ValueError` if the encoding type is not supported. - Prints an error message if the decoded data is not properly padded or contains non-alphabet characters, without terminating the program. 3. **Function: `compare_encodings(data: bytes) -> None`** - Inputs: - `data` (bytes): The binary data to be encoded and decoded for comparison. - Behaviour: - Encodes and decodes the given data using `Base16`, `Base32`, `Base64`, and `Base85`. - Prints the original, encoded, and decoded data for each encoding type. Constraints: - Use the modern interface provided by the `base64` module. - Assume that the input data will always be a bytes-like object. Example Usage: ```python try: original_data = b\\"Python310_Base64_Test\\" # Example: Encode data with Base64 encoded = encode_data(original_data, \\"base64\\") print(f\\"Encoded Data (Base64): {encoded}\\") # Example: Decode data with Base64 decoded = decode_data(encoded, \\"base64\\") print(f\\"Decoded Data (Base64): {decoded}\\") # Compare encodings compare_encodings(original_data) except ValueError as e: print(e) ``` Implement the three required functions with proper error handling and meaningful output.","solution":"import base64 def encode_data(data: bytes, encoding: str) -> bytes: if encoding == \\"base16\\": return base64.b16encode(data) elif encoding == \\"base32\\": return base64.b32encode(data) elif encoding == \\"base64\\": return base64.b64encode(data) elif encoding == \\"base85\\": return base64.b85encode(data) else: raise ValueError(f\\"Unsupported encoding type: {encoding}\\") def decode_data(encoded_data: bytes, encoding: str) -> bytes: try: if encoding == \\"base16\\": return base64.b16decode(encoded_data, casefold=True) elif encoding == \\"base32\\": return base64.b32decode(encoded_data, casefold=True) elif encoding == \\"base64\\": return base64.b64decode(encoded_data) elif encoding == \\"base85\\": return base64.b85decode(encoded_data) else: raise ValueError(f\\"Unsupported encoding type: {encoding}\\") except (base64.binascii.Error, ValueError) as e: print(f\\"Decoding error: {e}\\") return b\\"\\" def compare_encodings(data: bytes) -> None: encodings = [\\"base16\\", \\"base32\\", \\"base64\\", \\"base85\\"] for encoding in encodings: try: encoded = encode_data(data, encoding) decoded = decode_data(encoded, encoding) print(f\\"Original data: {data}\\") print(f\\"Encoded data ({encoding}): {encoded}\\") print(f\\"Decoded data: {decoded}\\") print() except ValueError as e: print(f\\"Error: {e}\\") # Example usage if __name__ == \\"__main__\\": try: original_data = b\\"Python310_Base64_Test\\" # Example: Encode data with Base64 encoded = encode_data(original_data, \\"base64\\") print(f\\"Encoded Data (Base64): {encoded}\\") # Example: Decode data with Base64 decoded = decode_data(encoded, \\"base64\\") print(f\\"Decoded Data (Base64): {decoded}\\") # Compare encodings compare_encodings(original_data) except ValueError as e: print(e)"},{"question":"Problem Statement: You are given two 2D tensors `matrix1` and `matrix2` containing complex numbers. Your task is to perform the following operations using PyTorch complex tensor functionalities and return the results. 1. Convert the given real matrices (size Nx2) to complex matrices using `torch.view_as_complex`. 2. Compute the element-wise product of `matrix1` and `matrix2`. 3. Compute the matrix product of `matrix1` and `matrix2`. 4. Calculate the element-wise absolute value of the resulting tensor from step 2. 5. Calculate the angles of the resulting tensor from step 3. 6. Serialize the resulting tensor from step 4 to a file named `\'abs_tensor.pt\'`. 7. Deserialize this tensor from the file and return it. Input: - `matrix1` and `matrix2` are 2D tensors of shape (N, 2), representing the real and imaginary parts of complex numbers respectively. Output: - A tensor that is the deserialized version of the tensor containing element-wise absolute values from step 4. Constraints: 1. All input tensors will have appropriate values such that no complex values exceed the range of the respective dtype. 2. You may assume the matrix dimensions are compatible for matrix multiplication. Example: ```python import torch matrix1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32) matrix2 = torch.tensor([[5.0, 6.0], [7.0, 8.0]], dtype=torch.float32) output = complex_operations(matrix1, matrix2) print(output) # Expected output tensor after deserialization ``` Function Signature: ```python def complex_operations(matrix1: torch.Tensor, matrix2: torch.Tensor) -> torch.Tensor: # Your code here ```","solution":"import torch def complex_operations(matrix1: torch.Tensor, matrix2: torch.Tensor) -> torch.Tensor: # Step 1: Convert real matrices to complex matrices complex_matrix1 = torch.view_as_complex(matrix1) complex_matrix2 = torch.view_as_complex(matrix2) # Step 2: Element-wise product of complex matrices element_wise_product = complex_matrix1 * complex_matrix2 # Step 3: Matrix product of complex matrices matrix_product = torch.matmul(complex_matrix1, complex_matrix2) # Step 4: Element-wise absolute value of the element-wise product tensor element_wise_abs = torch.abs(element_wise_product) # Step 5: Angles of the resulting tensor from the matrix product angles = torch.angle(matrix_product) # Step 6: Serialize the tensor containing element-wise absolute values torch.save(element_wise_abs, \'abs_tensor.pt\') # Step 7: Deserialize the tensor from the file and return it deserialized_tensor = torch.load(\'abs_tensor.pt\') return deserialized_tensor"},{"question":"# Advanced Python310 Coding Assessment **Objective**: Demonstrate your understanding of Python\'s `contextvars` module by creating a complex multi-threaded application that utilizes context variables to maintain state. **Problem Statement**: You are required to write a Python program that manages the execution of tasks across multiple threads using the `contextvars` module. Your solution should: 1. Create a context variable to store a unique identifier for each task. 2. Implement functions to start and stop tasks, ensuring that the unique identifier is correctly managed within each thread\'s context. 3. Use context variables to pass and retrieve unique identifiers between threads. 4. Ensure that the context variable for a task is isolated and does not interfere with context variables of other tasks executed in parallel. **Requirements**: 1. **Context Variable Creation**: - Create a context variable named `task_id` with no default value. - Implement a function `set_task_id()` that sets the unique identifier for the current task. 2. **Task Management**: - Implement a function `start_task(task_number)` that performs the following: - Sets the `task_id` context variable to the `task_number`. - Simulates a task by sleeping for a random duration between 1 to 3 seconds. - Prints the current thread name and the `task_id` context variable. - Implement a function `run_tasks_in_threads(task_numbers)` that performs the following: - Takes a list of unique task numbers. - Starts a new thread for each task number in the list, executing the `start_task` function. - Ensures proper synchronization and isolation of context variables across threads. **Input**: - A list of unique task numbers as integers. **Output**: - Printed statements of the form: `\\"Thread-<N>: Task ID - <task_id>\\"`, where `<N>` is the thread number and `<task_id>` is the context variable value. **Constraints**: - Use the `concurrent.futures.ThreadPoolExecutor` to manage threads. - The functions must handle any potential exceptions and ensure proper cleanup. - The solution must work for at least 10 concurrent tasks without interference. **Example**: ```python from concurrent.futures import ThreadPoolExecutor import contextvars import threading import time import random # Step 1: Create a context variable named task_id task_id = contextvars.ContextVar(\'task_id\') def set_task_id(id): task_id.set(id) # Step 2: Implement the start_task function def start_task(task_number): set_task_id(task_number) time.sleep(random.randint(1, 3)) print(f\\"{threading.current_thread().name}: Task ID - {task_id.get()}\\") # Step 3: Implement the run_tasks_in_threads function def run_tasks_in_threads(task_numbers): with ThreadPoolExecutor() as executor: futures = [executor.submit(start_task, number) for number in task_numbers] for future in futures: future.result() # waits for the task to complete # Example usage if __name__ == \\"__main__\\": task_numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] run_tasks_in_threads(task_numbers) ``` In this example, you\'ll see multiple task IDs printed by various threads, demonstrating the proper use of context variables to manage isolated task states. **Evaluation Criteria**: - Correctness: Ensure the functions correctly set and retrieve context variables within multi-threaded contexts. - Isolation: Each thread should have its task ID without interfering with others. - Robustness: Handle exceptions gracefully and ensure proper cleanup. - Performance: Efficiently manage and execute multiple tasks concurrently.","solution":"from concurrent.futures import ThreadPoolExecutor import contextvars import threading import time import random # Step 1: Create a context variable named task_id task_id = contextvars.ContextVar(\'task_id\') def set_task_id(id): task_id.set(id) # Step 2: Implement the start_task function def start_task(task_number): set_task_id(task_number) # Simulate a task by sleeping for a random duration between 1 to 3 seconds. time.sleep(random.randint(1, 3)) # Print the current thread name and the task_id. print(f\\"{threading.current_thread().name}: Task ID - {task_id.get()}\\") return task_id.get() # Step 3: Implement the run_tasks_in_threads function def run_tasks_in_threads(task_numbers): results = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(start_task, number) for number in task_numbers] for future in futures: results.append(future.result()) # waits for the task to complete return results # Example usage if __name__ == \\"__main__\\": task_numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] run_tasks_in_threads(task_numbers)"},{"question":"# Dynamic Module Importer and Manager You are required to develop a `ModuleManager` class in Python that provides the following capabilities using the `PyImport_*` functions described in the provided documentation: 1. **Dynamic Import**: - A method `import_module` which takes a module name as its parameter and attempts to import it using `PyImport_ImportModule`. - If the module is successfully imported, store the module in the class attribute `loaded_modules` (a dictionary) with the module name as the key. - Return the module object. - Handle any exceptions that occur during the import and return `None`. 2. **Reload Module**: - A method `reload_module` which takes a module name that has already been imported and reloads it using `PyImport_ReloadModule`. - Update the module reference in `loaded_modules` upon successful reload. - Return the reloaded module object. - Handle cases where the module isn\'t found in `loaded_modules` and return `None`. 3. **Get Imported Modules Dictionary**: - A method `get_module_dict` which returns the dictionary of all imported modules using `PyImport_GetModuleDict`. 4. **Exec Code in Module**: - A method `exec_code_module` which takes a module name and a code object, and executes the code within the module context using `PyImport_ExecCodeModule`. - Return the module object with the executed code. - Handle any exceptions and return `None`. # Constraints: - Assure that the module names provided are valid and the code objects passed are compilable. # Expected Input and Output: - `import_module` method: - Input: `\\"math\\"` - Output: `<module \'math\' (built-in)>` (or similar module object) - `reload_module` method: - Input: `\\"sys\\"` - Output: `<module \'sys\' (built-in)>` (or similar module object) - `get_module_dict` method: - Input: None - Output: Dict of loaded module references from `sys.modules`. - `exec_code_module` method: - Input: `\\"example\\"`, `compile(\\"a = 10\\", \\"<string>\\", \\"exec\\")` - Output: `<module \'example\'>`, where `a = 10` is now defined in the module. # Example Usage: ```python manager = ModuleManager() math_module = manager.import_module(\'math\') print(math_module) reloaded_sys = manager.reload_module(\'sys\') print(reloaded_sys) modules_dict = manager.get_module_dict() print(modules_dict) code = compile(\\"a = 10\\", \\"<string>\\", \\"exec\\") example_module = manager.exec_code_module(\'example\', code) print(example_module) ``` Implement the `ModuleManager` class according to the specification above.","solution":"import importlib import sys class ModuleManager: def __init__(self): self.loaded_modules = {} def import_module(self, module_name): try: module = importlib.import_module(module_name) self.loaded_modules[module_name] = module return module except ImportError: return None def reload_module(self, module_name): if module_name in self.loaded_modules: module = importlib.reload(self.loaded_modules[module_name]) self.loaded_modules[module_name] = module return module else: return None def get_module_dict(self): return sys.modules def exec_code_module(self, module_name, code_object): try: if module_name in self.loaded_modules: module = self.loaded_modules[module_name] else: module = importlib.util.module_from_spec(importlib.util.spec_from_loader(module_name, loader=None)) self.loaded_modules[module_name] = module exec(code_object, module.__dict__) return module except Exception: return None"},{"question":"Task Using the Python `functools` module, implement a class that demonstrates various functionalities of caching calculations and single-dispatch generic functions to perform different operations based on the input type. The class should fulfill the following requirements: 1. **Class Name**: `Calculator` 2. **Attributes**: - `data`: A list of numerical values, which is initialized via the `__init__` method. 3. **Methods**: - `@functools.cached_property`: Method to calculate and cache the standard deviation of the `data` list. - `@functools.lru_cache(maxsize=None)`: Method to calculate the nth Fibonacci number. - `@functools.singledispatchmethod`: Method `calculate` that performs different operations depending on the type of the first argument: - When the argument is an `int`, return its factorial. - When the argument is a `float`, return its square. - When the argument is a `list`, return the sum of all elements. - For other types, raise a `NotImplementedError`. # Input and Output Specifications - The `__init__` method takes a single argument, a list of numerical values. - The `cached_property` method computes the standard deviation of the data list. - The `lru_cache` method computes the nth Fibonacci number. - The `singledispatchmethod` determines the operation based on the argument type and returns the result. # Constraints - The list provided to the class should contain at least one numerical value. - The methods should handle typical edge cases gracefully, such as ensuring non-negative inputs for the Fibonacci calculation and proper type checks for single-dispatch operations. # Example Usage ```python from functools import cached_property, lru_cache, singledispatchmethod from statistics import stdev from math import factorial class Calculator: def __init__(self, data): self.data = data @cached_property def compute_stdev(self): return stdev(self.data) @lru_cache(maxsize=None) def fibonacci(self, n): if n < 0: raise ValueError(\\"Fibonacci number cannot be negative\\") if n == 0: return 0 elif n == 1: return 1 return self.fibonacci(n-1) + self.fibonacci(n-2) @singledispatchmethod def calculate(self, arg): raise NotImplementedError(\\"Cannot calculate value for the given type\\") @calculate.register def _(self, arg: int): return factorial(arg) @calculate.register def _(self, arg: float): return arg * arg @calculate.register def _(self, arg: list): return sum(arg) # Example usage calc = Calculator([1, 2, 3, 4, 5]) print(calc.compute_stdev) # Outputs the standard deviation of the data list print(calc.fibonacci(10)) # Outputs the 10th Fibonacci number print(calc.calculate(5)) # Outputs the factorial of 5 print(calc.calculate(4.5)) # Outputs the square of 4.5 print(calc.calculate([1, 2, 3])) # Outputs the sum of the list [1, 2, 3] ``` # Additional Notes - Use proper error handling to manage invalid inputs. - Make sure to include comments and docstrings for each method to explain its functionality.","solution":"from functools import cached_property, lru_cache, singledispatchmethod from statistics import stdev from math import factorial class Calculator: def __init__(self, data): if not isinstance(data, list) or not data: raise ValueError(\\"Data should be a non-empty list of numerical values\\") self.data = data @cached_property def compute_stdev(self): Calculate and cache the standard deviation of the data list. if len(self.data) < 2: raise ValueError(\\"At least two data points are required to compute standard deviation\\") return stdev(self.data) @lru_cache(maxsize=None) def fibonacci(self, n): Calculate the nth Fibonacci number using LRU cache. if n < 0: raise ValueError(\\"Fibonacci number cannot be negative\\") if n == 0: return 0 elif n == 1: return 1 return self.fibonacci(n - 1) + self.fibonacci(n - 2) @singledispatchmethod def calculate(self, arg): Perform different calculations based on the type of the argument. raise NotImplementedError(\\"Cannot calculate value for the given type\\") @calculate.register def _(self, arg: int): Calculate the factorial of the int argument. return factorial(arg) @calculate.register def _(self, arg: float): Calculate the square of the float argument. return arg * arg @calculate.register def _(self, arg: list): Calculate the sum of the elements in the list argument. return sum(arg)"},{"question":"You are tasked with implementing a C extension module in Python that utilizes the `PyCapsule` type to handle and share C pointers among different Python modules. In this assessment, you will create a Python script that uses an existing C extension module to verify the functionality and integration of `PyCapsule`. **Requirements:** 1. Implement a C extension module named `capsulemod` that: - Provides a C function `create_capsule` to create a new `PyCapsule` with a given pointer and name. - Provides a C function `get_capsule_pointer` to retrieve the pointer from a `PyCapsule` given its name. - Provides a C function `set_capsule_name` to set a new name to an existing `PyCapsule`. - Provides a C function `is_capsule_valid` to check the validity of a `PyCapsule`. 2. Implement a Python script that: - Imports the `capsulemod` module. - Creates a new `PyCapsule` using `create_capsule`. - Retrieves the pointer from the capsule using `get_capsule_pointer` and verifies it. - Sets a new name to the capsule using `set_capsule_name`. - Checks the validity of the capsule using `is_capsule_valid`. - Prints informative messages at each step to demonstrate the capsule\'s state and attributes. Here is the function signature for the C extension module: ```c PyObject *create_capsule(PyObject *self, PyObject *args); PyObject *get_capsule_pointer(PyObject *self, PyObject *args); PyObject *set_capsule_name(PyObject *self, PyObject *args); PyObject *is_capsule_valid(PyObject *self, PyObject *args); ``` **Constraints:** - The pointer value for `create_capsule` should be non-NULL and cast to `(void*)1` for simplicity. - The capsule name should be passed as a C string (char*). - Ensure to handle errors, raise appropriate exceptions, and manage memory properly. # Expected Input and Output Python script interaction: ```python import capsulemod # Create a capsule capsule = capsulemod.create_capsule((void*)1, \\"initial.name\\") print(f\\"Capsule created with pointer: {capsulemod.get_capsule_pointer(capsule, \'initial.name\')}\\") # Set a new name to the capsule result = capsulemod.set_capsule_name(capsule, \\"new.name\\") print(f\\"Setting new name result: {result}\\") # Validate the capsule is_valid = capsulemod.is_capsule_valid(capsule, \\"new.name\\") print(f\\"Is the capsule valid? {is_valid}\\") ``` **Expected Output:** ```plaintext Capsule created with pointer: 1 Setting new name result: 0 Is the capsule valid? 1 ``` Ensure to package the C extension module properly and test it with the provided Python script. Performance Requirements: - Functions should execute efficiently with minimal overhead. - Handle exceptions gracefully without crashing the interpreter.","solution":"# Since we need to implement a C extension module, below is a stub of the required python functions # In real-world usage, these would be implemented in a `.c` file, compiled with Python C API and imported as a Python module. # Python file to simulate usage of the C extension module # Mock implementation in Python for demonstrating unit testing class Capsule: def __init__(self, pointer, name): self.pointer = pointer self.name = name capsules = {} def create_capsule(pointer, name): if not pointer or not name: raise ValueError(\\"Pointer and name must be non-NULL\\") if name in capsules: raise ValueError(f\\"Capsule with the name \'{name}\' already exists\\") capsules[name] = Capsule(pointer, name) return capsules[name] def get_capsule_pointer(capsule, name): if name in capsules and capsules[name] == capsule: return capsule.pointer raise ValueError(\\"Invalid capsule or name\\") def set_capsule_name(capsule, new_name): if not new_name: raise ValueError(\\"New name must be non-NULL\\") if capsule.name not in capsules or capsules[capsule.name] != capsule: raise ValueError(\\"Capsule not found\\") if new_name in capsules: raise ValueError(f\\"Capsule with the name \'{new_name}\' already exists\\") old_name = capsule.name capsule.name = new_name capsules[new_name] = capsules.pop(old_name) return 0 def is_capsule_valid(capsule, name): return name in capsules and capsules[name] == capsule"},{"question":"**Coding Assessment Question** You are tasked with implementing a logging function in Python using the Unix `syslog` library routines. Your function should be versatile, accepting different message types, priorities, and facilities. Here are the requirements: # Function Specification **Function Name:** `custom_syslog` **Parameters:** 1. `messages` (list of tuples): Each tuple contains two elements: - A `string` (the log message). - An `integer` (the log priority level, e.g., `syslog.LOG_ERR`). 2. `ident` (string, optional): The identity string to prepend to each message. Defaults to \'custom\'. 3. `logoption` (integer, optional): Log options for `openlog()`. Defaults to `syslog.LOG_PID`. 4. `facility` (integer, optional): The log source facility. Defaults to `syslog.LOG_USER`. **Output:** - No return value. The function will log messages as a side effect. # Requirements: 1. Use `syslog.openlog()` to configure the log options. 2. Ensure proper logging of each message with its associated priority. 3. Use `syslog.closelog()` to reset the `syslog` module after logging all messages. 4. Handle any exceptions, printing an appropriate error message in case of failure. # Example Usage ```python import syslog def custom_syslog(messages, ident=\'custom\', logoption=syslog.LOG_PID, facility=syslog.LOG_USER): try: # Open the log with specified options syslog.openlog(ident=ident, logoption=logoption, facility=facility) # Iterate through the messages and log each with the specified priority for message, priority in messages: syslog.syslog(priority, message) except Exception as e: print(f\\"An error occurred: {e}\\") finally: # Close the log syslog.closelog() # Example list of messages messages = [(\\"Server started\\", syslog.LOG_INFO), (\\"Disk space low\\", syslog.LOG_WARNING), (\\"Server error occurred\\", syslog.LOG_ERR)] # Call the function custom_syslog(messages, ident=\'myServer\', logoption=syslog.LOG_PID | syslog.LOG_CONS, facility=syslog.LOG_DAEMON) ``` In this example, `custom_syslog()` logs three messages with specified priorities and facilities, using custom ident and log option settings. # Constraints: - Ensure that the `ident` is a non-empty string. - The `messages` list should contain at least one message. Design your function to handle these constraints. Use the Unix `syslog` routines adequately to demonstrate your comprehension of the package.","solution":"import syslog def custom_syslog(messages, ident=\'custom\', logoption=syslog.LOG_PID, facility=syslog.LOG_USER): if not ident: raise ValueError(\\"Ident must be a non-empty string\\") if not messages or not isinstance(messages, list) or not all(isinstance(m, tuple) and len(m) == 2 for m in messages): raise ValueError(\\"Messages must be a list of tuples, and each tuple must contain exactly two elements\\") try: # Open the log with specified options syslog.openlog(ident=ident, logoption=logoption, facility=facility) # Iterate through the messages and log each with the specified priority for message, priority in messages: syslog.syslog(priority, message) except Exception as e: print(f\\"An error occurred: {e}\\") finally: # Close the log syslog.closelog()"},{"question":"# Detailed Exception Handling and Custom Exception Creation In this exercise, you are to demonstrate comprehensive exception handling. Specifically, you are asked to create a function that processes a list of elements and create custom exceptions to handle specific error scenarios that are not covered by the built-in exceptions. # Function: `process_elements` Task: Implement the function `process_elements(elements: List[Union[int, None]]) -> List[int]`: 1. Iterate through the list `elements` to perform specific operations: - If the element is `None`, raise a `CustomNoneError` with a clear message. - If the element is not an integer, raise a `CustomTypeError` with a clear message. - If the element is a negative integer, raise a `CustomValueError` with a clear message. - If the element is greater than 1000, raise an `OverflowError`. - Otherwise, multiply the element by 2 and add it to the result list. 2. Handle multiple exception scenarios: - Use a try-except block to handle each specific exception type. - For `CustomTypeError`, `CustomNoneError`, `CustomValueError`, record a log message or handle it as required but do not stop the iteration. - For `OverflowError`, print a message and exit the loop. - Ensure all other uncaught exceptions raise proper messages. 3. The return value should be a list of processed elements (doubled values) that did not raise any exceptions. Custom Exceptions: Define the following custom exceptions: - `CustomNoneError`: Raised when an element is `None`. - `CustomTypeError`: Raised when an element is not of type integer. - `CustomValueError`: Raised when an element is a negative integer. Inputs: - elements: A list of elements that can be integers or `None`. Output: - A list of integers where each valid integer from the input list is multiplied by 2. Constraints: - Use proper exception chaining where applicable. - Do not use any library-specific exception-handling methods but only the built-in ones and the custom exceptions as defined. - The length of the list will not exceed 1000 items. - Each integer in the list will be between -100000 and 100000 (inclusive), and None values are also included. # Example Usage: ```python from typing import List, Union class CustomNoneError(Exception): pass class CustomTypeError(Exception): pass class CustomValueError(Exception): pass def process_elements(elements: List[Union[int, None]]) -> List[int]: result = [] for element in elements: try: if element is None: raise CustomNoneError(\\"Element is None\\") elif not isinstance(element, int): raise CustomTypeError(f\\"Element {element} is not an integer\\") elif element < 0: raise CustomValueError(f\\"Element {element} is a negative integer\\") elif element > 1000: raise OverflowError(f\\"Element {element} is too large\\") result.append(element * 2) except CustomNoneError as e: print(f\\"Warning: {e}\\") except CustomTypeError as e: print(f\\"Warning: {e}\\") except CustomValueError as e: print(f\\"Warning: {e}\\") except OverflowError as e: print(f\\"Error: {e}. Stopping processing.\\") break except Exception as e: raise RuntimeError(f\\"Unexpected error: {e}\\") return result # Example test: elements = [1, -2, 3, None, \\"text\\", 500, 1500, 1000] print(process_elements(elements)) ``` Expected Output: ``` Warning: Element -2 is a negative integer Warning: Element is None Warning: Element text is not an integer Error: Element 1500 is too large. Stopping processing. [2, 6, 1000, 2000] ``` In this question, students are tested on their ability to understand and implement custom exception handling, as well as on their capability to distinguish between different error types and respond accordingly.","solution":"from typing import List, Union class CustomNoneError(Exception): pass class CustomTypeError(Exception): pass class CustomValueError(Exception): pass def process_elements(elements: List[Union[int, None]]) -> List[int]: result = [] for element in elements: try: if element is None: raise CustomNoneError(\\"Element is None\\") elif not isinstance(element, int): raise CustomTypeError(f\\"Element {element} is not an integer\\") elif element < 0: raise CustomValueError(f\\"Element {element} is a negative integer\\") elif element > 1000: raise OverflowError(f\\"Element {element} is too large\\") result.append(element * 2) except CustomNoneError as e: print(f\\"Warning: {e}\\") except CustomTypeError as e: print(f\\"Warning: {e}\\") except CustomValueError as e: print(f\\"Warning: {e}\\") except OverflowError as e: print(f\\"Error: {e}. Stopping processing.\\") break except Exception as e: raise RuntimeError(f\\"Unexpected error: {e}\\") return result"},{"question":"<|Analysis Begin|> The provided documentation explains the `torch.utils.mobile_optimizer.optimize_for_mobile` utility in PyTorch. This utility is designed to optimize models for mobile execution. It includes several optimization passes aimed at reducing model size and improving inference speed on mobile devices. The default optimizations include: - Conv2D + BatchNorm fusion - Insert and Fold prepacked ops - ReLU/Hardtanh fusion - Dropout removal - Conv packed params hoisting - Add/ReLU fusion for CPU backend - Automatic GPU Transfer for Vulkan backend The function works by taking a `torch.jit.ScriptModule` object and applying the optimizations while preserving specific methods if required. Some of these optimizations involve rewriting parts of the computation graph to enhance efficiency on mobile hardware (e.g., ARM CPUs). Given this detailed functionality, a good assessment question would be to have students apply this `optimize_for_mobile` function to a simple model and analyze the changes. This will test their understanding of the optimization process and their ability to apply and verify it using PyTorch. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Implement and optimize a PyTorch model for mobile deployment using the `torch.utils.mobile_optimizer.optimize_for_mobile` function. You will need to: 1. Create a simple neural network model. 2. Convert this model to a `torch.jit.ScriptModule`. 3. Apply the mobile optimization. 4. Verify the optimizations have been applied. **Instructions:** 1. **Model Definition:** Define a simple neural network model in PyTorch that includes: - A `Conv2d` layer followed by a `BatchNorm2d` layer. - A `ReLU` activation. - A `Dropout` layer. - Another `Conv2d` layer. Example model: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.bn1 = nn.BatchNorm2d(10) self.relu = nn.ReLU() self.dropout = nn.Dropout(p=0.5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.dropout(x) x = self.conv2(x) return x ``` 2. **ScriptModule Conversion:** Convert the model to a `torch.jit.ScriptModule`. ```python model = SimpleModel() scripted_model = torch.jit.script(model) ``` 3. **Optimize for Mobile:** Apply the `optimize_for_mobile` function with default parameters. ```python from torch.utils.mobile_optimizer import optimize_for_mobile optimized_model = optimize_for_mobile(scripted_model) ``` 4. **Verification:** Print and compare the graph of the original and optimized model to verify that the mobile optimizations have been applied. ```python print(\\"Original model graph:\\") print(scripted_model.graph) print(\\"Optimized model graph:\\") print(optimized_model.graph) ``` **Expected Output:** Your submission should include: - The full implementation of the model definition. - The conversion to `torch.jit.ScriptModule`. - The optimization code. - Output showing the original and optimized computational graphs. **Constraints:** - Ensure that the model is in evaluation mode before applying the optimizations. - Focus on readability and clarity of your code.","solution":"import torch import torch.nn as nn from torch.utils.mobile_optimizer import optimize_for_mobile # Define a simple neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.bn1 = nn.BatchNorm2d(10) self.relu = nn.ReLU() self.dropout = nn.Dropout(p=0.5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.dropout(x) x = self.conv2(x) return x # Instantiate the model and put it in evaluation mode model = SimpleModel() model.eval() # Convert the model to a torch.jit.ScriptModule scripted_model = torch.jit.script(model) # Optimize the model for mobile optimized_model = optimize_for_mobile(scripted_model) # Print the original and optimized model graphs for comparison print(\\"Original model graph:\\") print(scripted_model.graph) print(\\"Optimized model graph:\\") print(optimized_model.graph)"},{"question":"Objective The goal is to write a Python script that utilizes the Unix `syslog` library to log messages with different levels of priority and facilities. Your code should demonstrate an understanding of configuring syslog, logging messages, and filtering them based on priority. Problem Statement You are required to implement a class `Logger` which uses the Unix `syslog` library for logging messages. The `Logger` class should support the following methods: 1. **`__init__(self, ident: str, logoption: int, facility: int)`**: - Initializes the logger with the specified `ident`, `logoption`, and `facility` using the `openlog` function. 2. **`log_message(self, priority: int, message: str)`**: - Logs a message with a specific priority. 3. **`set_mask(self, maskpri: int)`**: - Sets the log mask to filter messages based on priority using the `setlogmask` function. 4. **`close(self)`**: - Resets the logger values and closes the log using the `closelog` function. Below is a template for the `Logger` class: ```python import syslog class Logger: def __init__(self, ident: str, logoption: int, facility: int): Initialize the logger. :param ident: Identifier string for log entries. :param logoption: Log options. :param facility: Log facility. syslog.openlog(ident, logoption, facility) def log_message(self, priority: int, message: str): Log a message with a specific priority. :param priority: Log priority. :param message: Message to log. syslog.syslog(priority, message) def set_mask(self, maskpri: int): Set the log mask to filter messages by priority. :param maskpri: Priority mask. syslog.setlogmask(maskpri) def close(self): Close the logger. syslog.closelog() ``` Constraints - You must use the `syslog` module for logging. - Priorities and facilities should be utilized from the constants provided in the `syslog` module. - Ensure proper use of `openlog`, `syslog`, `setlogmask`, and `closelog` functions. Example Usage ```python # Initialize the logger logger = Logger(ident=\\"myapp\\", logoption=syslog.LOG_PID, facility=syslog.LOG_USER) # Log messages with different priorities logger.log_message(syslog.LOG_INFO, \\"This is an info message\\") logger.log_message(syslog.LOG_ERR, \\"This is an error message\\") # Set a log mask to filter out messages with lower priority than LOG_ERR logger.set_mask(syslog.LOG_UPTO(syslog.LOG_ERR)) # This message will not be logged due to the log mask logger.log_message(syslog.LOG_INFO, \\"This info message will be filtered out\\") # This message will be logged logger.log_message(syslog.LOG_CRIT, \\"This is a critical message\\") # Close the logger logger.close() ``` Notes - Test your implementation thoroughly to ensure all logging functionalities work as expected. - Handle any edge cases and ensure your code is robust and clean.","solution":"import syslog class Logger: def __init__(self, ident: str, logoption: int, facility: int): Initialize the logger. :param ident: Identifier string for log entries. :param logoption: Log options. :param facility: Log facility. syslog.openlog(ident, logoption, facility) def log_message(self, priority: int, message: str): Log a message with a specific priority. :param priority: Log priority. :param message: Message to log. syslog.syslog(priority, message) def set_mask(self, maskpri: int): Set the log mask to filter messages by priority. :param maskpri: Priority mask. syslog.setlogmask(maskpri) def close(self): Close the logger. syslog.closelog()"},{"question":"Objective: Demonstrate your understanding of MIME quoted-printable encoding and decoding using the `quopri` module. Problem Statement: You are given the task to develop a Python class named `QuopriHandler` that provides methods to encode and decode strings using the quoted-printable standard. Implement this class with the following methods: 1. **encode_string(self, input_str: str, encode_spaces: bool = False, encode_headers: bool = False) -> str** - **Parameters:** - `input_str` (str): The input string to be encoded. - `encode_spaces` (bool, optional): If True, all spaces and tabs are encoded. Defaults to False. - `encode_headers` (bool, optional): If True, spaces are encoded as underscores. Defaults to False. - **Returns:** - str: The encoded quoted-printable string. 2. **decode_string(self, input_str: str, decode_headers: bool = False) -> str** - **Parameters:** - `input_str` (str): The quoted-printable encoded string to be decoded. - `decode_headers` (bool, optional): If True, underscores will be decoded as spaces. Defaults to False. - **Returns:** - str: The decoded original string. Constraints: - You must utilize `quopri.encodestring` and `quopri.decodestring` for encoding and decoding respectively. - Do not use any external library other than `quopri`. - Consider edge cases such as empty strings, strings with special characters, and strings with embedded spaces and tabs. Example Usage: ```python # Create an instance of QuopriHandler handler = QuopriHandler() # Example 1: Encoding and decoding a string without special flags encoded_str = handler.encode_string(\\"Hello World!\\") print(encoded_str) # Encoded result decoded_str = handler.decode_string(encoded_str) print(decoded_str) # \\"Hello World!\\" # Example 2: Encoding string with spaces encoded as underscores (useful for headers) encoded_str = handler.encode_string(\\"Hello World!\\", encode_headers=True) print(encoded_str) # Encoded result with underscores for spaces decoded_str = handler.decode_string(encoded_str, decode_headers=True) print(decoded_str) # \\"Hello World!\\" ``` Develop the required class `QuopriHandler` to succeed in this task. Note: Ensure your implementation handles edge cases efficiently and adheres strictly to the method signatures provided.","solution":"import quopri class QuopriHandler: def encode_string(self, input_str: str, encode_spaces: bool = False, encode_headers: bool = False) -> str: encoded_bytes = quopri.encodestring(input_str.encode(\'utf-8\'), header=encode_headers) encoded_str = encoded_bytes.decode(\'utf-8\') if not encode_headers and encode_spaces: encoded_str = encoded_str.replace(\' \', \'=20\').replace(\'t\', \'=09\') return encoded_str def decode_string(self, input_str: str, decode_headers: bool = False) -> str: if decode_headers: input_str = input_str.replace(\'_\', \' \') decoded_bytes = quopri.decodestring(input_str.encode(\'utf-8\')) decoded_str = decoded_bytes.decode(\'utf-8\') return decoded_str"},{"question":"**Email Analyzer Function Implementation** You are required to implement an Email Analyzer function in Python that demonstrates comprehensive knowledge of email parsing using the `email.parser` module: # Problem Statement Write a function, `analyze_email_content`, which takes in an email message in various formats (bytes, string, file) and performs the following tasks: 1. Parses the email to construct the `EmailMessage` object. 2. Extracts and returns the following details: - Subject of the email. - The sender\'s email address. - The recipient\'s email address(es). - A boolean indicating whether the email is a multipart email. - Text content of the email (handles both simple and multipart emails). # Function Signature ```python def analyze_email_content(email_input: Any, input_format: str) -> Dict[str, Any]: pass ``` # Parameters - `email_input` (Any): The email message input which can be of the following types: - `bytes`: A bytes object containing the raw email data. - `str`: A string containing the raw email data. - `file`: A file object opened in binary or text mode. - `input_format` (str): A string indicating the format of `email_input`. It can take values `\'bytes\'`, `\'string\'`, or `\'file\'`. # Returns - A dictionary with the following keys: - `\'subject\'` (str): The subject of the email. - `\'from\'` (str): The sender\'s email address. - `\'to\'` (str or List[str]): The recipient\'s email address(es). - `\'is_multipart\'` (bool): Whether the email is multipart. - `\'content\'` (str): The text content of the email. # Example ```python email_data = b\\"From: example@example.comnTo: test@example.comnSubject: Test EmailnnThis is a test email.\\" result = analyze_email_content(email_data, \'bytes\') print(result) # Output: {\'subject\': \'Test Email\', \'from\': \'example@example.com\', \'to\': \'test@example.com\', \'is_multipart\': False, \'content\': \'This is a test email.\'} ``` # Constraints - Ensure the function can handle both MIME and non-MIME emails. - Correctly parse and handle multipart email messages. - Manage different line endings and partial lines in the input data. - Handle potential parsing errors gracefully and indicate parsing issues in the output dictionary. # Note Use the appropriate classes and methods from the `email.parser` module to implement the function.","solution":"from typing import Any, Dict, Union from email import message_from_bytes, message_from_string, message_from_file from email.message import EmailMessage import io def analyze_email_content(email_input: Any, input_format: str) -> Dict[str, Any]: Analyzes email content and extracts relevant details. Parameters: email_input (Any): The input email data in bytes, string, or file object. input_format (str): The format of the input data. Can be \'bytes\', \'string\', or \'file\'. Returns: Dict[str, Any]: A dictionary containing the extracted details. try: if input_format == \'bytes\': email_message = message_from_bytes(email_input) elif input_format == \'string\': email_message = message_from_string(email_input) elif input_format == \'file\': email_message = message_from_file(email_input) else: raise ValueError(\\"Invalid input format\\") subject = email_message.get(\'Subject\', \'\') from_addr = email_message.get(\'From\', \'\') to_addrs = email_message.get_all(\'To\', []) # Convert to_addrs into a single string if it is a singleton list if isinstance(to_addrs, list) and len(to_addrs) == 1: to_addrs = to_addrs[0] is_multipart = email_message.is_multipart() content = \'\' if is_multipart: for part in email_message.walk(): content_type = part.get_content_type() if part.get_content_maintype() == \'text\': content = part.get_payload(decode=True).decode(part.get_content_charset() or \'utf-8\') break else: content = email_message.get_payload(decode=True).decode(email_message.get_content_charset() or \'utf-8\') return { \'subject\': subject, \'from\': from_addr, \'to\': to_addrs, \'is_multipart\': is_multipart, \'content\': content } except Exception as e: return { \'subject\': \'\', \'from\': \'\', \'to\': \'\', \'is_multipart\': False, \'content\': \'\', \'error\': str(e) }"},{"question":"**Objective**: Implement a simplified Python interpreter that can execute different types of input: complete programs, interactive statements, and expressions. # Task Write a Python class `SimpleInterpreter` with the following methods: 1. `execute_complete_program(code: str) -> None`: Executes a complete Python program from the given string `code`. 2. `execute_interactive_statement(statement: str) -> None`: Executes a single interactive statement from the given string `statement`. 3. `evaluate_expression(expression: str) -> any`: Evaluates an expression from the given string `expression` and returns the result. In addition, the class should maintain a shared execution environment (namespace) such that variables declared and modified in one method call affect subsequent calls. # Requirements - For `execute_complete_program`, the input string can contain multiple lines of Python code, simulating a complete script. - For `execute_interactive_statement`, the input string will contain a single statement or a compound statement, similar to an interactive shell. - For `evaluate_expression`, the input string will contain a single expression whose result should be returned. - The environment should be persisted across all method calls to reflect a continued state of the interpreter. # Constraints 1. Your solution should handle basic built-in functions and operations as supported by standard Python. 2. Assume all provided code strings are valid Python code. 3. Performance: The methods should handle input strings up to 10,000 characters efficiently. # Example Usage ```python interpreter = SimpleInterpreter() # Execute a complete program interpreter.execute_complete_program( x = 10 y = 20 z = x + y ) # Interactive statement execution interpreter.execute_interactive_statement(\\"print(z)\\") # Should print 30 # Evaluate an expression result = interpreter.evaluate_expression(\\"z * 2\\") assert result == 60 ``` # Implementation Notes - Use the `exec()` function to handle complete program and interactive statement execution. - Use the `eval()` function for expression evaluation. - Maintain a dictionary for the execution environment that should be updated across method calls. Happy coding!","solution":"class SimpleInterpreter: def __init__(self): self.env = {} def execute_complete_program(self, code: str) -> None: exec(code, self.env) def execute_interactive_statement(self, statement: str) -> None: exec(statement, self.env) def evaluate_expression(self, expression: str) -> any: return eval(expression, self.env)"},{"question":"**Objective:** Implement an example usage of the `IsotonicRegression` class from the scikit-learn library. The task involves fitting the model to given training data, making predictions on test data, and evaluating the model\'s performance. **Problem Statement:** Given the following training data: - `X_train` (input features): `[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]` - `y_train` (targets): `[2.3, 2.5, 3.0, 3.5, 4.1, 4.3, 5.1, 5.4, 6.0, 6.8]` And the following test data: - `X_test`: `[2.5, 3.5, 6.5, 9.5]` 1. Fit an isotonic regression model to the training data. 2. Predict the output for the test data points. 3. Evaluate the performance of the model by calculating the mean squared error (MSE) between the predicted values and the following true values for the test set: `[2.6, 3.2, 4.8, 6.5]` **Function Signature:** ```python def isotonic_regression_example(X_train: list, y_train: list, X_test: list, y_test: list) -> float: Fits an isotonic regression model to the training data and evaluates it on the test data. Parameters: - X_train (list of float): Training input features. - y_train (list of float): Training targets. - X_test (list of float): Test input features. - y_test (list of float): True values for test data to compute MSE. Returns: - mse (float): The mean squared error of predictions on the test dataset. ``` **Constraints:** - You must use the `IsotonicRegression` class from the `sklearn.isotonic` module. - The function should calculate and return the mean squared error (MSE) of the predictions on the test data. **Example:** ```python X_train = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] y_train = [2.3, 2.5, 3.0, 3.5, 4.1, 4.3, 5.1, 5.4, 6.0, 6.8] X_test = [2.5, 3.5, 6.5, 9.5] y_test = [2.6, 3.2, 4.8, 6.5] mse = isotonic_regression_example(X_train, y_train, X_test, y_test) print(mse) # Expected Output: Some float value representing the MSE ``` Ensure your implementation is efficient and correctly utilizes the `IsotonicRegression` class. The calculated MSE on the test data will serve as a metric for the model\'s performance.","solution":"from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error def isotonic_regression_example(X_train, y_train, X_test, y_test): Fits an isotonic regression model to the training data and evaluates it on the test data. Parameters: - X_train (list of float): Training input features. - y_train (list of float): Training targets. - X_test (list of float): Test input features. - y_test (list of float): True values for test data to compute MSE. Returns: - mse (float): The mean squared error of predictions on the test dataset. # Initialize the isotonic regression model iso_reg = IsotonicRegression() # Fit the model with the training data iso_reg.fit(X_train, y_train) # Predict the output for the test data points y_pred = iso_reg.transform(X_test) # Calculate and return the mean squared error mse = mean_squared_error(y_test, y_pred) return mse"},{"question":"Coding Assessment Question # Overview In Python, variables referenced by multiple scopes (e.g., closures, nested functions) are managed using \\"Cell\\" objects. In this question, you are required to implement a Python class that replicates the behavior of these cell objects. # Task Implement a Python class named `Cell` with the following methods: 1. `__init__(self, value=None)`: Initializes a new cell object with an optional initial value. The value can be `None`. 2. `get(self)`: Returns the current value stored in the cell object. 3. `set(self, value)`: Sets a new value in the cell object, replacing any existing value. 4. `__call__(self)`: An alias for the `get` method, allowing the cell object to be called as a function to retrieve its value. 5. `__repr__(self)`: Provides a string representation of the cell object in the format `Cell(value)`. # Constraints - The class should handle `None` as a valid value. - Do not use any Python C API functions. The implementation should be purely in Python. # Example ```python # Initialize a cell with a value of 10 cell = Cell(10) print(cell) # Output: Cell(10) # Get the current value print(cell.get()) # Output: 10 # Set a new value cell.set(20) print(cell) # Output: Cell(20) # Call to get the current value print(cell()) # Output: 20 # Initialize a cell with no initial value empty_cell = Cell() print(empty_cell) # Output: Cell(None) print(empty_cell.get()) # Output: None empty_cell.set(30) print(empty_cell.get()) # Output: 30 ``` # Performance Requirements - The methods should have a constant time complexity, O(1). Implement the `Cell` class in Python fulfilling the above specifications and constraints. # Submission Please write the complete implementation of the `Cell` class below: ```python class Cell: def __init__(self, value=None): self._value = value def get(self): return self._value def set(self, value): self._value = value def __call__(self): return self.get() def __repr__(self): return f\\"Cell({self._value})\\" ```","solution":"class Cell: def __init__(self, value=None): Initializes a new cell object with an optional initial value. self._value = value def get(self): Returns the current value stored in the cell object. return self._value def set(self, value): Sets a new value in the cell object, replacing any existing value. self._value = value def __call__(self): Alias for the get method, allowing the cell object to be called as a function to retrieve its value. return self.get() def __repr__(self): Provides a string representation of the cell object in the format Cell(value). return f\\"Cell({self._value})\\""},{"question":"You are tasked with creating a simple Python application using the `tkinter` module that simulates a series of prompts to guide a user through a sequence of decisions. The application should demonstrate the use of different types of `tkinter.messagebox` message prompts. **Requirements:** 1. Create a `tkinter` application with the following specifications: - A main window with a title \\"Decision Simulator\\". - A button labeled \\"Start\\" which initiates the sequence of prompts when clicked. 2. When the \\"Start\\" button is clicked, display the following sequence of message boxes: - An informational message box with the title \\"Welcome\\" and the message \\"This is a decision simulation application.\\". - A warning message box with the title \\"Warning\\" and the message \\"Make sure to read all prompts carefully.\\". - An error message box with the title \\"Error\\" and the message \\"This is just a test error.\\". - A yes/no question message box with the title \\"Proceed?\\" and the message \\"Would you like to continue?\\". If the user selects \\"No\\", the sequence stops; if the user selects \\"Yes\\", the sequence continues. - An ok/cancel question message box with the title \\"Confirm\\" and the message \\"Do you wish to confirm your action?\\". If the user selects \\"Cancel\\", the sequence stops; if the user selects \\"OK\\", the sequence continues. - A retry/cancel question message box with the title \\"Retry\\" and the message \\"Would you like to retry?\\". If the user selects \\"Cancel\\", the sequence stops; if the user selects \\"Retry\\", the sequence continues. - A yes/no/cancel question message box with the title \\"Finalize\\" and the message \\"Do you wish to finalize the simulation?\\". The outcome should be handled as follows: - \\"Yes\\" shows a message box with the title \\"Finalized\\" and the message \\"You have finalized the simulation.\\". - \\"No\\" shows a message box with the title \\"Not Finalized\\" and the message \\"You have chosen not to finalize the simulation.\\". - \\"Cancel\\" ends the sequence without any further action. 3. Ensure the GUI stays responsive throughout the sequence. **Input/Output:** - Input: User interaction with the message boxes. - Output: Responses and messages displayed in message boxes as described above. **Constraints:** - Use only the `tkinter` and `tkinter.messagebox` modules from the standard library. - The application should be implemented as a single Python script. **Example Illustration:** When the \\"Start\\" button is clicked, the application should show the series of message boxes as described. Depending on the user\'s choices in the message boxes, the application may stop or proceed to display further prompts. The final message box\'s content will depend on user choices throughout the sequence. **Performance Considerations:** - The application should handle user interactions smoothly without freezing. - Ensure that appropriate actions are taken based on user inputs in message boxes. **Deliverable:** Submit a Python script file (`decision_simulator.py`) containing the implementation of the described application.","solution":"import tkinter as tk from tkinter import messagebox def start_simulation(): if not messagebox.showinfo(\\"Welcome\\", \\"This is a decision simulation application.\\"): if not messagebox.showwarning(\\"Warning\\", \\"Make sure to read all prompts carefully.\\"): if not messagebox.showerror(\\"Error\\", \\"This is just a test error.\\"): if messagebox.askyesno(\\"Proceed?\\", \\"Would you like to continue?\\"): if messagebox.askokcancel(\\"Confirm\\", \\"Do you wish to confirm your action?\\"): if messagebox.askretrycancel(\\"Retry\\", \\"Would you like to retry?\\"): decision = messagebox.askyesnocancel(\\"Finalize\\", \\"Do you wish to finalize the simulation?\\") if decision is None: return elif decision: messagebox.showinfo(\\"Finalized\\", \\"You have finalized the simulation.\\") else: messagebox.showinfo(\\"Not Finalized\\", \\"You have chosen not to finalize the simulation.\\") else: return else: return else: return def create_gui(): root = tk.Tk() root.title(\\"Decision Simulator\\") start_button = tk.Button(root, text=\\"Start\\", command=start_simulation) start_button.pack(pady=20) root.mainloop() if __name__ == \\"__main__\\": create_gui()"},{"question":"Problem Statement: You are given a dataset containing information about protein expression levels in mice (`miceprotein` dataset from OpenML). The task is to preprocess this data to make it suitable for a classification machine learning model using the `sklearn.datasets` utilities. You will later train and evaluate a Support Vector Machine (SVM) model using this preprocessed data. Instructions: 1. **Data Loading**: Write a function `load_data() -> (pd.DataFrame, pd.Series)` that loads the `miceprotein` dataset using the `fetch_openml` function, and returns the features as a DataFrame and the target labels as a Series. 2. **Data Preprocessing**: - Normalize the numeric features in the range [0,1]. - Convert categorical features to numerical values using one-hot encoding. 3. **Model Training**: Implement a function `train_and_evaluate_svm(X: pd.DataFrame, y: pd.Series) -> float` to train an SVM classifier with the preprocessed data and return the mean cross-validated accuracy. 4. **Putting It All Together**: Implement a main function to execute the entire process. 5. **Code Requirements**: - Use `StandardScaler` to normalize numeric features. - Use `OneHotEncoder` for one-hot encoding categorical features. - Use `SVC` from `sklearn.svm` for the SVM classifier. - Use `cross_val_score` with 5-fold cross-validation to evaluate the model\'s accuracy. Constraints: - The dataset must be loaded from OpenML as specified. - The code should handle missing values by filling them with the mean of the respective columns. - Ensure that your solution is efficient and follows scikit-learn\'s best practices for preprocessing and model evaluation. Input / Output Formats: - **Input**: None (the dataset path is fixed within the code). - **Output**: The mean accuracy of the cross-validation as a float. Example Output: ```python Mean cross-validated accuracy: 0.75 ``` Solution Template: ```python import pandas as pd from sklearn.datasets import fetch_openml from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.model_selection import cross_val_score def load_data(): # Code to fetch and load the data mice = fetch_openml(name=\'miceprotein\', version=4) X = mice.data y = mice.target return X, y def preprocess_data(X, y): # Code to preprocess the data numeric_features = X.select_dtypes(include=[\'float64\', \'int\']) categorical_features = X.select_dtypes(include=[\'object\']) numeric_transformer = Pipeline(steps=[ (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features.columns), (\'cat\', categorical_transformer, categorical_features.columns) ]) X_preprocessed = preprocessor.fit_transform(X) return X_preprocessed, y def train_and_evaluate_svm(X, y): # Code to train the SVM and evaluate accuracy svm = SVC() scores = cross_val_score(svm, X, y, cv=5) return scores.mean() def main(): X, y = load_data() X_preprocessed, y = preprocess_data(X, y) accuracy = train_and_evaluate_svm(X_preprocessed, y) print(f\\"Mean cross-validated accuracy: {accuracy:.2f}\\") if __name__ == \\"__main__\\": main() ``` **Note**: Ensure you complete the function definitions `load_data`, `preprocess_data`, and `train_and_evaluate_svm`.","solution":"import pandas as pd from sklearn.datasets import fetch_openml from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.svm import SVC from sklearn.model_selection import cross_val_score def load_data(): Loads the miceprotein dataset from OpenML and returns the features as a DataFrame and target as a Series. mice = fetch_openml(name=\'miceprotein\', version=4, as_frame=True) X = mice.data y = mice.target return X, y def preprocess_data(X, y): Preprocesses the data by normalizing numeric features and one-hot encoding categorical features. # Fill missing values with the mean of the respective column X = X.fillna(X.mean()) numeric_features = X.select_dtypes(include=[\'float64\', \'int\']).columns categorical_features = X.select_dtypes(include=[\'category\', \'object\']).columns numeric_transformer = Pipeline(steps=[ (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features) ]) X_preprocessed = preprocessor.fit_transform(X) return X_preprocessed, y def train_and_evaluate_svm(X, y): Trains an SVM classifier with the preprocessed data and returns the mean cross-validated accuracy. svm = SVC() scores = cross_val_score(svm, X, y, cv=5) return scores.mean() def main(): Executes the entire process: loading data, preprocessing, training and evaluating the SVM model. X, y = load_data() X_preprocessed, y = preprocess_data(X, y) accuracy = train_and_evaluate_svm(X_preprocessed, y) print(f\\"Mean cross-validated accuracy: {accuracy:.2f}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Objective Create a new custom Python type in a C extension module using the `PyTypeObject` structure. Your type will represent a simple immutable \\"Point\\" with `x` and `y` coordinates, and should include appropriate methods and representations as specified below. # Requirements 1. **Attributes**: - The `Point` type should have two attributes, `x` and `y`, both being integers. - These attributes should be set during initialization and immutable afterwards. 2. **Methods**: - `__init__(self, x: int, y: int)`: Initialize attributes `x` and `y`. - `__repr__(self)`: Return a string representation of the point in the format `Point(x, y)`. - `__add__(self, other)`: Add two points together and return a new point with the sum of their coordinates. 3. **Special Slots**: - Implement the necessary slots in `PyTypeObject` to achieve the above behaviors. 4. **Constraints**: - Ensure the `x` and `y` attributes are immutable after the object is created. - The type should inherit necessary protocols and slots from `PyBaseObject_Type`. # Input and Output Specifications 1. **Initialization**: - Input: Two integers `x` and `y`. - Output: An instance of the `Point` type. 2. **String Representation**: - Input: A `Point` instance. - Output: A string in the format `Point(x, y)`. 3. **Addition Operation**: - Input: Two `Point` instances. - Output: A new `Point` instance with coordinates being the sum of the input points\' coordinates. # Deliverables 1. C Code (`point.c`): - Define the `Point` type using `PyTypeObject`. - Implement memory management and method functions as required. - Ensure compatibility with Python\'s memory management and exception-handling conventions. 2. Setup Script (`setup.py`): - A `setup.py` script to build the extension module. # Example Below is an example of what the usage should look like in Python: ```python # Assuming the module is named \'geometry\' from geometry import Point # Create instances p1 = Point(2, 3) p2 = Point(4, 1) # Add points p3 = p1 + p2 # Point(6, 4) # Print points print(p1) # Output: Point(2, 3) print(p3) # Output: Point(6, 4) ``` # Instructions 1. **Write the C code** to define the `Point` type with the required attributes and methods. 2. **Compile the extension module** using `setup.py`. 3. **Test** your implementation with a sample Python script to ensure it meets the requirements. Hints: - Use `PyModuleDef` and `PyModule_Create` for module definition. - Use `PyTypeObject` structure for defining the `Point` type. - Make sure to handle reference counting correctly to avoid memory leaks.","solution":"def point_init(self, x, y): Initialize the Point instance with x and y coordinates. self._x = x self._y = y def point_repr(self): Return the string representation of the Point instance. return f\\"Point({self._x}, {self._y})\\" def point_add(self, other): Add two Point instances and return a new Point with the sum of their coordinates. return Point(self._x + other._x, self._y + other._y) class Point: A simple immutable Point class with x and y integer coordinates. def __init__(self, x, y): point_init(self, x, y) def __repr__(self): return point_repr(self) def __add__(self, other): return point_add(self, other) @property def x(self): return self._x @property def y(self): return self._y"},{"question":"# PyTorch NCCL ProcessGroup Environment Configuration Objective Create a class in PyTorch that configures and manages a `ProcessGroupNCCL` according to specific environment variables. This class should: - Initialize the ProcessGroupNCCL with customizable configuration using environment variables. - Provide facilities to modify certain configurations at runtime. - Handle errors and timeouts gracefully. Input 1. A dictionary `config` where keys represent the environment variable names and values represent the desired settings. Output A configured `ProcessGroupNCCL` instance and a method to modify, fetch and print the current configurations. Constraints - Only the environment variables listed in the provided documentation should be considered. - The class should have a hard limit of 10 seconds for any wait operation to prevent indefinite hanging. - Error handling should take into account nan checks and asynchronous errors. - You can use Python built-in modules and PyTorch. Solution Requirements 1. **Class Definition**: Create a `DistributedTrainerConfig` class that accepts a configuration dictionary during initialization. 2. **Initialization**: Set the environment variables based on provided configurations. 3. **Runtime Modification**: Methods to modify, fetch and print the current configuration settings. 4. **Error Handling**: Gracefully handle `NaN` errors and watchdog timeouts with clear messages. 5. **Performance**: Ensure configurations do not cause significant slowdowns in the run-time performance of the ProcessGroupNCCL. Here\'s some starter code: ```python import os import torch from torch import distributed as dist class DistributedTrainerConfig: def __init__(self, config): self.config = config self.apply_configurations() def apply_configurations(self): for key, value in self.config.items(): os.environ[key] = str(value) # Initialize ProcessGroupNCCL dist.init_process_group(backend=\'nccl\') def modify_configuration(self, key, value): self.config[key] = value os.environ[key] = str(value) def fetch_configuration(self, key): return self.config.get(key, None) def print_current_config(self): for key, value in self.config.items(): print(f\\"{key}: {value}\\") def handle_errors(self): # Implement error handling based on TORCH_NCCL_NAN_CHECK & TORCH_NCCL_ASYNC_ERROR_HANDLING pass def monitor_timeout(self): # Implement watchdog monitoring with TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC pass # Example of usage: config = { \\"TORCH_NCCL_ASYNC_ERROR_HANDLING\\": 1, \\"TORCH_NCCL_ENABLE_TIMING\\": 1, \\"TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\\": 5, \\"TORCH_NCCL_NAN_CHECK\\": 1 } trainer_config = DistributedTrainerConfig(config) trainer_config.print_current_config() trainer_config.modify_configuration(\\"TORCH_NCCL_ENABLE_TIMING\\", 0) trainer_config.print_current_config() ``` Complete the methods `handle_errors` and `monitor_timeout` to ensure the DistributedTrainerConfig operates according to the specifications and handles errors gracefully.","solution":"import os import torch from torch import distributed as dist class DistributedTrainerConfig: def __init__(self, config): self.config = config self.apply_configurations() self.init_process_group() def apply_configurations(self): for key, value in self.config.items(): os.environ[key] = str(value) def init_process_group(self): try: dist.init_process_group(backend=\'nccl\') except Exception as e: print(f\\"Error initializing ProcessGroupNCCL: {e}\\") def modify_configuration(self, key, value): self.config[key] = value os.environ[key] = str(value) def fetch_configuration(self, key): return self.config.get(key, None) def print_current_config(self): for key, value in self.config.items(): print(f\\"{key}: {value}\\") def handle_errors(self): nan_check = int(self.fetch_configuration(\\"TORCH_NCCL_NAN_CHECK\\") or 0) async_error_handling = int(self.fetch_configuration(\\"TORCH_NCCL_ASYNC_ERROR_HANDLING\\") or 0) if nan_check: # This is a placeholder for actual NaN detection logic. print(\\"NaN check enabled. Monitoring for NaNs...\\") if async_error_handling: # This is a placeholder for actual async error handling logic. print(\\"Async error handling enabled. Monitoring for async errors...\\") def monitor_timeout(self): heartbeat_timeout = int(self.fetch_configuration(\\"TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\\") or 10) # Placeholder logic for timeout monitoring print(f\\"Heartbeat timeout set to {heartbeat_timeout} seconds. Monitoring...\\") # Example of usage: # Commented out as actual execution requires the appropriate distributed environment setup. # config = { # \\"TORCH_NCCL_ASYNC_ERROR_HANDLING\\": 1, # \\"TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\\": 5, # \\"TORCH_NCCL_NAN_CHECK\\": 1 # } # trainer_config = DistributedTrainerConfig(config) # trainer_config.print_current_config() # trainer_config.modify_configuration(\\"TORCH_NCCL_ASYNC_ERROR_HANDLING\\", 0) # trainer_config.print_current_config() # trainer_config.handle_errors() # trainer_config.monitor_timeout()"},{"question":"# Coding Assessment: Implementing Conditional Control Flow with `torch.cond` **Objective:** Demonstrate your understanding of using `torch.cond` in PyTorch for implementing data-dependent control flow in tensor operations. **Problem Statement:** Write a PyTorch module that uses `torch.cond` to implement a function that behaves differently based on the sum of the input tensor elements and the shape of the tensor. The module should satisfy the following conditions: 1. If the sum of all elements in the tensor is greater than a given threshold `t`, apply the function `torch.exp` to all elements of the tensor. 2. If the sum of all elements in the tensor is less than or equal to `t` but the number of elements in the tensor is more than `n`, then apply `torch.sqrt` to all elements of the tensor. 3. Otherwise, apply `torch.log1p` to all elements of the tensor. **Function Signature:** ```python import torch class ConditionalTensorOperations(torch.nn.Module): def __init__(self, threshold: float, n: int): super().__init__() self.threshold = threshold self.n = n def forward(self, x: torch.Tensor) -> torch.Tensor: # Define the true and false functions for the inner condition def true_fn_exp(x: torch.Tensor): return torch.exp(x) def false_fn_sqrt_or_log1p(x: torch.Tensor): # Nested condition if x.numel() > self.n: return torch.sqrt(x) else: return torch.log1p(x) # Use torch.cond for the primary condition result = torch.cond(x.sum() > self.threshold, true_fn_exp, false_fn_sqrt_or_log1p, (x,)) return result ``` **Examples:** ```python # Example usage model = ConditionalTensorOperations(threshold=5.0, n=10) # Case when sum > threshold input_tensor_1 = torch.tensor([[1.0, 2.0, 3.0], [2.0, 0.5, 1.5]]) output_tensor_1 = model(input_tensor_1) # Expecting output tensor with torch.exp applied # Case when sum <= threshold and numel > n (inner true case) input_tensor_2 = torch.randn(11, 1) * 0.1 output_tensor_2 = model(input_tensor_2) # Expecting output tensor with torch.sqrt applied # Case when sum <= threshold and numel <= n (inner false case) input_tensor_3 = torch.randn(2, 2) * 0.1 output_tensor_3 = model(input_tensor_3) # Expecting output tensor with torch.log1p applied ``` **Constraints:** - Input tensor `x` can be of any shape and dtype `torch.float32`. - The computation should handle both small and large tensors efficiently. - The threshold `t` and the number of elements `n` are positive constants provided during the module initialization. Note: Training of this module is not required or supported as per the current limitations of `torch.cond`. **Evaluation Criteria:** - Correctness: The implementation must satisfy all specified conditions correctly. - Efficiency: The solution must perform efficiently for various tensor sizes. - Code Readability: The code should be clean, well-commented, and easy to understand.","solution":"import torch class ConditionalTensorOperations(torch.nn.Module): def __init__(self, threshold: float, n: int): super().__init__() self.threshold = threshold self.n = n def forward(self, x: torch.Tensor) -> torch.Tensor: # Define the true and false functions for the inner condition def true_fn_exp(x: torch.Tensor): return torch.exp(x) def false_fn_sqrt_or_log1p(x: torch.Tensor): # Nested condition if x.numel() > self.n: return torch.sqrt(x) else: return torch.log1p(x) # Check if sum > threshold if x.sum().item() > self.threshold: result = true_fn_exp(x) else: result = false_fn_sqrt_or_log1p(x) return result"},{"question":"**Objective:** Write a Python C extension that uses `PyCapsule` to manage and pass around pointers to C objects/functions. Implement functions to create a capsule, validate its contents, manipulate its contents, and import the capsule. **Question:** 1. Write a C function `create_capsule` that creates a `PyCapsule` encapsulating a given non-`NULL` pointer along with a specified name and destructor. The name should follow the format `module.attribute`. 2. Implement a function `is_valid_capsule` that checks if a given `PyObject` is a valid capsule with a specified name. 3. Implement a function `manipulate_capsule` that: - Retrieves the stored pointer, name, and destructor from an existing capsule. - Sets a new pointer, name, and context in the capsule. - Ensures the previous name is properly freed if it\'s not `NULL`. 4. Write a Python-exposed function `import_capsule` that: - Imports a pointer from a capsule attribute within a specified module. - Verifies the imported pointer. **Requirements:** - `create_capsule` should accept: - `void *pointer`: A non-NULL pointer that the capsule will encapsulate. - `const char *name`: The name of the capsule following the format `module.attribute`. - `PyCapsule_Destructor destructor`: (Optional) Destructor function for the capsule. - `is_valid_capsule` should accept: - `PyObject *obj`: The object to check. - `const char *name`: The expected name of the capsule. - `manipulate_capsule` should: - Get and set the respective values from the capsule using the appropriate `PyCapsule` functions. - `import_capsule` should: - Use the `PyCapsule_Import` function to import and verify a pointer. **Constraints:** - Ensure proper error handling for all functions. - Use stable ABI functions as mentioned. - Ensure `create_capsule` correctly initializes the `PyCapsule` and associates the given `name` and `destructor`. **Expected Output:** - `create_capsule` should return a new capsule object. - `is_valid_capsule` should return a boolean indicating if the capsule is valid. - `manipulate_capsule` should return a modified capsule with updated values. - `import_capsule` should return the imported pointer if successful. Provide appropriate testing in Python to demonstrate the functionality of your C functions.","solution":"# Note: Python does not support writing actual C extensions directly within this Python script. # Below is a simulated version of the solution provided as C code comments within Python, # and how it would look like as Python functions using the ctypes library. from ctypes import * # Simulated C Code examples # C # /* create_capsule function */ # PyObject* create_capsule(void *pointer, const char *name, PyCapsule_Destructor destructor) { # if (!pointer) { # PyErr_SetString(PyExc_ValueError, \\"pointer must be non-NULL\\"); # return NULL; # } # return PyCapsule_New(pointer, name, destructor); # } # # /* is_valid_capsule function */ # int is_valid_capsule(PyObject *obj, const char *name) { # if (PyCapsule_CheckExact(obj) && PyCapsule_GetPointer(obj, name)) { # return 1; # } # return 0; # } # # /* manipulate_capsule function */ # void manipulate_capsule(PyObject *capsule, void *new_pointer, const char *new_name, void *new_context) { # PyCapsule_SetPointer(capsule, new_pointer); # PyCapsule_SetName(capsule, new_name); # PyCapsule_SetContext(capsule, new_context); # } # # /* import_capsule function */ # void* import_capsule(const char *fullname) { # return PyCapsule_Import(fullname, 0); # } # # Explanation in Python-like pseudocode (since ctypes cannot mimic exactly) def create_capsule(pointer, name, destructor=None): Creates a PyCapsule enclosing the given pointer, name, and optional destructor. if pointer is None: raise ValueError(\\"Pointer must be non-NULL\\") # Simulating PyCapsule creation and setting using ctypes capsule = { \\"pointer\\": pointer, \\"name\\": name, \\"destructor\\": destructor } return capsule def is_valid_capsule(obj, name): Checks if the given object is a valid PyCapsule with the given name. if isinstance(obj, dict) and obj.get(\\"name\\") == name: return True return False def manipulate_capsule(capsule, new_pointer, new_name, new_context): Manipulates the existing PyCapsule by setting a new pointer, name, and context. if isinstance(capsule, dict): capsule[\\"pointer\\"] = new_pointer capsule[\\"name\\"] = new_name capsule[\\"context\\"] = new_context def import_capsule(fullname): Imports a pointer from a capsule attribute within a specified module. # Simulating PyCapsule_Import capsules = { \\"module.attribute\\": \\"pointer_value\\" # Mock value } if fullname in capsules: return capsules[fullname] else: raise ImportError(f\\"No capsule found for {fullname}\\")"},{"question":"Objective: Demonstrate your understanding of low-level threading in Python using the `_thread` module. Problem Statement: You are required to implement a `Counter` class that incrementally counts up to a certain number using multiple threads. The class should allow users to define how many threads to use for counting and should ensure thread-safe access to the shared counter. Requirements: 1. Implement the `Counter` class with the following methods: - `__init__(self, max_count, num_threads)`: Initializes the counter to 0, sets the maximum count value, and the number of threads to use. - `increment(self)`: Increments the counter by 1 in a thread-safe manner. - `run(self)`: Creates the specified number of threads to increment the counter until it reaches the maximum count. 2. Use the `_thread` module\'s features to handle threading and synchronization: - Use `_thread.start_new_thread()` to create new threads. - Use `_thread.allocate_lock()` to acquire and release locks as needed. Constraints: - The counter must be incremented exactly `max_count` times, regardless of how many threads are used. - Ensure that race conditions do not occur by properly managing access to the shared counter value. - The `num_threads` parameter will not exceed 10, and `max_count` will not exceed 1,000,000. Input and Output: - **Input**: `max_count` (int), `num_threads` (int) - **Output**: The final value of the counter should be equal to `max_count`. Here is the skeleton of the `Counter` class to get you started: ```python import _thread class Counter: def __init__(self, max_count, num_threads): self.max_count = max_count self.num_threads = num_threads self.current_count = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: if self.current_count < self.max_count: self.current_count += 1 def run(self): def worker(): while True: with self.lock: if self.current_count >= self.max_count: break self.increment() threads = [] for _ in range(self.num_threads): threads.append(_thread.start_new_thread(worker, ())) # Wait for all threads to complete (add your waiting mechanism here) # Hint: Use a busy-wait or any other mechanism you deem appropriate # Example Usage: if __name__ == \\"__main__\\": counter = Counter(max_count=100000, num_threads=5) counter.run() print(f\\"Final counter value: {counter.current_count}\\") ``` Performance Considerations: - The solution should efficiently handle synchronization using locks without unnecessary blocking. - Avoid busy-waiting wherever possible; aim for a clean and efficient thread management approach. Good luck!","solution":"import _thread import time class Counter: def __init__(self, max_count, num_threads): self.max_count = max_count self.num_threads = num_threads self.current_count = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: if self.current_count < self.max_count: self.current_count += 1 def run(self): def worker(): while True: with self.lock: if self.current_count >= self.max_count: break self.increment() for _ in range(self.num_threads): _thread.start_new_thread(worker, ()) # Busy-wait until the counter reaches the maximum count while True: with self.lock: if self.current_count >= self.max_count: break time.sleep(0.01) # Sleep to prevent high CPU usage # Example Usage: if __name__ == \\"__main__\\": counter = Counter(max_count=100000, num_threads=5) counter.run() print(f\\"Final counter value: {counter.current_count}\\")"},{"question":"# Complex Number Operations and Conversions You are required to write a Python function named `complex_operations` that performs a series of mathematical operations on complex numbers using the â€œcmathâ€ module. Function Signature ```python def complex_operations(z: complex) -> dict: pass ``` Input - `z`: A complex number (i.e., a number with a real and an imaginary part, such as `3 + 4j`). Output - A dictionary with the following keys and corresponding values: - `\'rectangular\'`: A tuple representing the complex number in rectangular coordinates (real part, imaginary part). - `\'polar\'`: A tuple representing the complex number in polar coordinates (modulus, phase angle in radians). - `\'exp\'`: The result of raising the constant `e` to the power of `z`. - `\'log\'`: The natural logarithm of `z`. - `\'sqrt\'`: The square root of `z`. - `\'trig_functions\'`: A tuple containing cosine, sine, and tangent of `z`. Constraints - For complex numbers where branch cuts exist, ensure that the output reflects the correct side of the branch cut. - You are **not** allowed to use any other library apart from `cmath` and built-in Python functions. Example ```python result = complex_operations(1 + 1j) print(result) # Expected output: { \'rectangular\': (1.0, 1.0), \'polar\': (1.4142135623730951, 0.7853981633974483), \'exp\': (1.4686939399158851+2.2873552871788423j), \'log\': (0.34657359027997264+0.7853981633974483j), \'sqrt\': (1.0986841134678098+0.4550898605622273j), \'trig_functions\': ((0.8337300251311491-0.9888977057628651j), (1.2984575814159773+0.6349639147847361j), (0.27175258531951174-1.0839233273386946j)) } ``` # Requirements 1. Use appropriate `cmath` functions to perform each of the operations. 2. Ensure the results abide by the mathematical properties and branch cuts discussed in the \\"cmath\\" documentation. 3. Verify your implementation with various test cases, especially edge cases like negative real parts, zero imaginary parts, etc., to ensure correctness. # Notes - The `rect` function should be used to convert polar coordinates back to the rectangular form, mainly for verification within the `polar` conversion process. - Use the `cmath` constants and functions wherever appropriate.","solution":"import cmath def complex_operations(z: complex) -> dict: return { \'rectangular\': (z.real, z.imag), \'polar\': cmath.polar(z), \'exp\': cmath.exp(z), \'log\': cmath.log(z), \'sqrt\': cmath.sqrt(z), \'trig_functions\': (cmath.cos(z), cmath.sin(z), cmath.tan(z)) }"},{"question":"<|Analysis Begin|> The provided documentation for the `__future__` module gives us an overview of its purpose and usage. The `__future__` module is significant for enabling new language features that may alter the behavior of Python in a way that isn\'t compatible with previous versions. The key details provided include: 1. Definition of the `__future__` module and its primary purposes. 2. Structure and components of future statements (`FeatureName = _Feature(OptionalRelease, MandatoryRelease, CompilerFlag)`). 3. Explanation of the `OptionalRelease`, `MandatoryRelease`, and `CompilerFlag` components. 4. Examples of language features that were introduced with the help of `__future__` and their respective PEP references, optional and mandatory release versions. Given this information, we can design a question that requires understanding these components and how they interact, possibly including a practical coding application. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective To demonstrate comprehension of the `__future__` module in Python and to apply it practically in a coding context. Problem Statement Write a Python function `future_features_info` that processes the list of features introduced by the `__future__` module. The function should return a summary of the features, including the name of the feature, the first version where it was optional, the version where it became (or is expected to become) mandatory, and the corresponding PEP. The list of features and their information are provided in a dictionary format as shown below: ```python future_features = { \\"nested_scopes\\": {\\"optional\\": (2, 1, 0, \\"beta\\", 1), \\"mandatory\\": (2, 2, 0, \\"final\\", 0), \\"pep\\": 227}, \\"generators\\": {\\"optional\\": (2, 2, 0, \\"alpha\\", 1), \\"mandatory\\": (2, 3, 0, \\"final\\", 0), \\"pep\\": 255}, \\"division\\": {\\"optional\\": (2, 2, 0, \\"alpha\\", 2), \\"mandatory\\": (3, 0, 0, \\"final\\", 0), \\"pep\\": 238}, ... } ``` Function Signature ```python def future_features_info(features: dict) -> list: ``` Input - `features` (dict): A dictionary containing information about future features in the `__future__` module. Each key is a feature name, and its value is another dictionary with keys `optional`, `mandatory`, and `pep`. Output - Returns a list of tuples, each containing information about a feature. Each tuple is of the form: `(feature_name, optional_version, mandatory_version, pep_number)`. Example Input: ```python future_features = { \\"nested_scopes\\": {\\"optional\\": (2, 1, 0, \\"beta\\", 1), \\"mandatory\\": (2, 2, 0, \\"final\\", 0), \\"pep\\": 227}, \\"generators\\": {\\"optional\\": (2, 2, 0, \\"alpha\\", 1), \\"mandatory\\": (2, 3, 0, \\"final\\", 0), \\"pep\\": 255}, ... } ``` Output: ```python [ (\'nested_scopes\', (2, 1, 0, \'beta\', 1), (2, 2, 0, \'final\', 0), 227), (\'generators\', (2, 2, 0, \'alpha\', 1), (2, 3, 0, \'final\', 0), 255), ... ] ``` Constraints - Python version should be 3.10 or later. - The function should handle the version tuples correctly and ignore any `mandatory` versions that are `None`. Notes - Use the given structure for the output format. - Focus on appropriately handling tuple data and dictionary lookups. Good luck!","solution":"def future_features_info(features): Processes a dictionary of future features and returns a summary. Args: - features (dict): a dictionary where keys are feature names and values are dictionaries with \'optional\', \'mandatory\', and \'pep\' data. Returns: - list: A list of tuples, each containing (feature_name, optional_version, mandatory_version, pep_number). summary = [] for feature, info in features.items(): optional_version = info[\'optional\'] mandatory_version = info[\'mandatory\'] pep_number = info[\'pep\'] summary.append((feature, optional_version, mandatory_version, pep_number)) return summary"},{"question":"# Multiclass and Multioutput Classification Assessment You are given a dataset of images of fruits. Each image can either be of an apple, a pear, or an orange, and each fruit can also be labeled with one or more colors: green, red, yellow, or orange. Your task is to classify the type of fruit and its colors using a `OneVsRestClassifier` for multiclass classification and `MultiOutputClassifier` (using the `RandomForestClassifier` as the base estimators) for multilabel classification. **Requirements:** 1. Implement a function `classify_fruits` which: - Takes in `X` (features of the images) and `y_type` (type of fruit) for multiclass classification. - Takes in `X` (features of the images) and `y_colors` (binary matrix indicating the presence of each color) for multilabel classification. - Returns the trained multiclass `OneVsRestClassifier` and multilabel `MultiOutputClassifier` models. 2. Implement a function `predict_fruits` which: - Takes in the trained models and new data `X_new` and returns the predicted fruit type and the predicted colors for each sample. **Input:** - `X`: A (n_samples, n_features) numpy array representing the features. - `y_type`: A (n_samples, ) numpy array representing the type of fruit with possible values [\'apple\', \'pear\', \'orange\']. - `y_colors`: A (n_samples, n_colors) binary numpy array where each column represents a color, and each entry is either 1 (color present) or 0 (color absent). **Output:** - `classify_fruits`: - Returns the trained `OneVsRestClassifier` model for the fruit type. - Returns the trained `MultiOutputClassifier` model for the fruit colors. - `predict_fruits`: - Returns two numpy arrays: predicted fruit types and predicted color indicators. **Constraints:** - Use `RandomForestClassifier` as the base estimator for the models. - Assume `n_classes=3` for the fruit types and `n_colors=4` for the colors. - Ensure the classifiers are trained within a reasonable computational time. # Example: ```python import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.multiclass import OneVsRestClassifier from sklearn.multioutput import MultiOutputClassifier # Sample Data X_train = ... # Your features data for training y_type_train = np.array([\'apple\', \'pear\', \'apple\', \'orange\']) # Example target labels for types y_colors_train = np.array([[1, 0, 0, 1], [0, 0, 1, 1], [1, 0, 0, 1], [0, 1, 0, 0]]) # Example target labels for colors # Train models classifier_type, classifier_colors = classify_fruits(X_train, y_type_train, y_colors_train) # Sample New Data X_new = ... # Your features data for prediction # Predict new data predicted_type, predicted_colors = predict_fruits(classifier_type, classifier_colors, X_new) ``` **Note:** - Properly preprocess the target data to fit it into the required format for sklearn classifiers. - Implement necessary import statements and assume the use of numpy and sklearn packages.","solution":"import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.multiclass import OneVsRestClassifier from sklearn.multioutput import MultiOutputClassifier from sklearn.preprocessing import LabelBinarizer def classify_fruits(X, y_type, y_colors): Trains classifiers to predict fruit types and associated colors. Parameters: - X: np.ndarray, shape (n_samples, n_features) - y_type: np.ndarray, shape (n_samples, ) - y_colors: np.ndarray, shape (n_samples, n_colors) Returns: - OneVsRestClassifier model for fruit type prediction. - MultiOutputClassifier model for fruit color prediction. # Encode fruit types into a binary format lb = LabelBinarizer() y_type_encoded = lb.fit_transform(y_type) # Create and train a OneVsRestClassifier for fruit type type_classifier = OneVsRestClassifier(RandomForestClassifier()) type_classifier.fit(X, y_type_encoded) # Create and train a MultiOutputClassifier for fruit colors color_classifier = MultiOutputClassifier(RandomForestClassifier()) color_classifier.fit(X, y_colors) return type_classifier, color_classifier def predict_fruits(type_classifier, color_classifier, X_new): Predicts the type and colors of fruits for given features. Parameters: - type_classifier: OneVsRestClassifier model for type prediction - color_classifier: MultiOutputClassifier model for color prediction - X_new: np.ndarray, shape (n_samples, n_features) Returns: - np.ndarray: Predicted fruit types, shape (n_samples, ) - np.ndarray: Predicted fruit colors, shape (n_samples, n_colors) # Predict the fruit types predicted_type_encoded = type_classifier.predict(X_new) predicted_type = type_classifier.classes_[predicted_type_encoded.argmax(axis=1)] # Predict the fruit colors predicted_colors = color_classifier.predict(X_new) return predicted_type, predicted_colors"},{"question":"You have been tasked with writing a Python program that interacts with the NIS (Yellow Pages) system using the deprecated `nis` module. Your program should demonstrate the ability to query NIS maps, handle potential errors gracefully, and return meaningful information to the user. Function Specifications 1. **Function Name**: `get_nis_map_value` 2. **Input**: - `key` (string): The key to look for in the NIS map. - `mapname` (string): The name of the NIS map. - `domain` (string, optional): The NIS domain to use for the lookup. Default is the system\'s default NIS domain. 3. **Output**: - Returns the value corresponding to the `key` in the specified `mapname` as a string. - If the `key` is not found, returns `\\"Key not found\\"`. 4. **Constraints**: - Ensure the program works only on Unix systems. - Handle potential `nis.error` exceptions when the key is not found or any other NIS-related error occurs. Additionally, write a function to retrieve and display all maps in the current domain. 1. **Function Name**: `list_nis_maps` 2. **Input**: None 3. **Output**: - Returns a list of all valid NIS maps in the current NIS domain. - If there is an error retrieving the maps, returns `\\"Error retrieving maps\\"`. Example Usage ```python try: # Example 1: Get the value for a specific key in a specific map value = get_nis_map_value(\\"example_key\\", \\"example_map\\") print(f\\"Value found: {value}\\") # Example 2: List all valid NIS maps maps = list_nis_maps() print(f\\"Available maps: {maps}\\") except nis.error as e: print(f\\"NIS error occurred: {e}\\") ``` Hints - Make use of `nis.match()` to find the value for a given key in a map. - Utilize `nis.maps()` to retrieve a list of valid NIS maps. - Use `nis.get_default_domain()` to obtain the default domain when the domain parameter is not provided. Your task is to implement these functions in a robust manner, ensuring proper error handling and clear output.","solution":"import nis import os def get_nis_map_value(key, mapname, domain=None): Retrieve the value corresponding to the `key` in the specified `mapname` in the NIS domain. Parameters: key (str): The key to look up in the NIS map. mapname (str): The name of the NIS map. domain (str, optional): The NIS domain to use for the lookup. Defaults to the system\'s default NIS domain. Returns: str: The value corresponding to the `key` or \\"Key not found\\" if the key is not present. if domain is None: domain = nis.get_default_domain() try: return nis.match(key, mapname, domain).decode(\'utf-8\') except nis.error: return \\"Key not found\\" def list_nis_maps(): Retrieve and return a list of all valid NIS maps in the current NIS domain. Returns: list: List of all valid NIS maps or \\"Error retrieving maps\\" if there is an error. try: maps = nis.maps() return list(maps) except nis.error: return \\"Error retrieving maps\\""},{"question":"# Question: Sparse Data Structures in Pandas You are given a large dataset mostly filled with zeros. For this exercise, you will: 1. Create a DataFrame with mostly zeros. 2. Convert this DataFrame into a sparse DataFrame. 3. Compute basic statistical operations to show the efficiency and behavior of sparse data structures. 4. Convert a SciPy sparse matrix into a pandas sparse DataFrame and manipulate it. Part 1: DataFrame Creation Create a DataFrame with dimensions 10,000 x 4, filled with random floats between 0 and 1. Set 99% of the values to zero. Part 2: Convert to Sparse DataFrame Convert the DataFrame created in Part 1 to a sparse DataFrame using `NaN` as the fill value. Part 3: Analyze Efficiency 1. Compute the memory usage of both the dense and sparse DataFrames. 2. Calculate the density of the sparse DataFrame. Part 4: Convert SciPy Sparse Matrix Create a SciPy sparse matrix of shape 1000 x 100 filled with random integers between 0 and 10, with 90% of the values being zero. Convert this SciPy sparse matrix into a pandas sparse DataFrame. Constraints - Ensure the pandas DataFrame and SciPy matrix have the required characteristics. - Perform operations efficiently and use pandas and SciPy functionalities as much as possible. Expected **Input and Output** Formats - **Input:** There are no explicit inputs as the operations should be coded directly. - **Output:** Print statements showing memory usage before and after conversion, density of the sparse DataFrame, and a preview of the SciPy sparse matrix and resulting pandas sparse DataFrame. ```python import pandas as pd import numpy as np from scipy.sparse import csr_matrix # Part 1: Create a DataFrame with mostly zeros dense_df = pd.DataFrame(np.random.choice([0, 1], p=[0.99, 0.01], size=(10000, 4))) # Part 2: Convert to Sparse DataFrame sparse_df = dense_df.astype(pd.SparseDtype(\\"float\\", np.nan)) # Part 3: Analyze Efficiency dense_memory_usage = dense_df.memory_usage().sum() sparse_memory_usage = sparse_df.memory_usage().sum() sparse_density = sparse_df.sparse.density print(f\\"Dense DataFrame memory usage: {dense_memory_usage} bytes\\") print(f\\"Sparse DataFrame memory usage: {sparse_memory_usage} bytes\\") print(f\\"Sparse DataFrame density: {sparse_density}\\") # Part 4: Convert SciPy Sparse Matrix scipy_sparse_matrix = csr_matrix(np.random.choice([0, 1], p=[0.9, 0.1], size=(1000, 100))) pandas_sparse_df = pd.DataFrame.sparse.from_spmatrix(scipy_sparse_matrix) print(pandas_sparse_df.head()) print(pandas_sparse_df.dtypes) ```","solution":"import pandas as pd import numpy as np from scipy.sparse import csr_matrix def create_dense_dataframe(): Creates a DataFrame with dimensions 10,000 x 4, filled with random floats between 0 and 1. Sets 99% of the values to zero. data = np.random.choice([0, np.random.rand()], size=(10000, 4), p=[0.99, 0.01]) return pd.DataFrame(data) def convert_to_sparse_dataframe(dense_df): Converts a dense DataFrame to a sparse DataFrame with NaN as the fill value. return dense_df.astype(pd.SparseDtype(\\"float\\", np.nan)) def analyze_efficiency(dense_df, sparse_df): Computes and prints memory usage and density of DataFrames. dense_memory_usage = dense_df.memory_usage().sum() sparse_memory_usage = sparse_df.memory_usage().sum() sparse_density = sparse_df.sparse.density print(f\\"Dense DataFrame memory usage: {dense_memory_usage} bytes\\") print(f\\"Sparse DataFrame memory usage: {sparse_memory_usage} bytes\\") print(f\\"Sparse DataFrame density: {sparse_density}\\") def convert_scipy_sparse_matrix(): Creates a SciPy sparse matrix and converts it to a pandas sparse DataFrame. scipy_sparse_matrix = csr_matrix(np.random.choice([0, np.random.randint(1, 11)], p=[0.9, 0.1], size=(1000, 100))) pandas_sparse_df = pd.DataFrame.sparse.from_spmatrix(scipy_sparse_matrix) print(pandas_sparse_df.head()) print(pandas_sparse_df.dtypes) # Create and manipulate data dense_df = create_dense_dataframe() sparse_df = convert_to_sparse_dataframe(dense_df) analyze_efficiency(dense_df, sparse_df) convert_scipy_sparse_matrix()"},{"question":"Objective You are provided with a dataset containing information about various species of flowers. Your task is to use the `seaborn` library to create a visually appealing and informative plot. Your plot should utilize the `hls_palette` function to customize the color palette according to the specified requirements. Instructions 1. **Load the dataset:** The dataset is a .csv file named `flowers.csv` and contains the following columns: - `species`: The species name of the flower. - `sepal_length`: The length of the sepal in cm. - `sepal_width`: The width of the sepal in cm. - `petal_length`: The length of the petal in cm. - `petal_width`: The width of the petal in cm. 2. **Create a `seaborn` scatter plot:** The scatter plot should plot `sepal_length` against `sepal_width`. 3. **Customize the color palette:** - Use the `hls_palette` function to create a palette with 5 colors. - Set the lightness to 0.5. - Set the saturation to 0.7. - Use these colors to color the points according to their species. 4. **Add plot elements:** - Add a title to the plot: \\"Sepal Dimensions by Species\\". - Label the x-axis as \\"Sepal Length (cm)\\". - Label the y-axis as \\"Sepal Width (cm)\\". - Add a legend to differentiate the species. Constraints - You must use the `seaborn` library for plotting. - Ensure that the plot is clearly labeled and easy to read. Expected Output Your output should be a scatter plot with the custom palette applied, with points colored by species, proper labeling, and a legend. Example Input ```csv species,sepal_length,sepal_width,petal_length,petal_width setosa,5.1,3.5,1.4,0.2 versicolor,6.0,2.2,4.0,1.0 virginica,5.9,3.0,5.1,1.8 ... ``` Example Code ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'flowers.csv\') # Create the custom palette custom_palette = sns.hls_palette(n_colors=5, l=0.5, s=0.7) # Create the scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', data=df, palette=custom_palette) # Customize the plot scatter_plot.set_title(\'Sepal Dimensions by Species\') scatter_plot.set_xlabel(\'Sepal Length (cm)\') scatter_plot.set_ylabel(\'Sepal Width (cm)\') # Add the legend plt.legend(title=\'Species\') plt.show() ``` **Use the above example to guide your implementation.**","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_sepal_scatterplot(csv_path): # Load the dataset df = pd.read_csv(csv_path) # Create the custom palette custom_palette = sns.hls_palette(n_colors=5, l=0.5, s=0.7) # Create the scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', data=df, palette=custom_palette) # Customize the plot scatter_plot.set_title(\'Sepal Dimensions by Species\') scatter_plot.set_xlabel(\'Sepal Length (cm)\') scatter_plot.set_ylabel(\'Sepal Width (cm)\') # Add the legend plt.legend(title=\'Species\') plt.show()"},{"question":"Coding Assessment Question # Objective Your task is to create a bar plot using the Seaborn `objects` interface, applying various transformations to visualize a dataset in an informative way. # Dataset Use the `tips` dataset available in Seaborn. You can load it using the following command: ```python from seaborn import load_dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) ``` # Instructions 1. Create a bar plot showing the count of tips given on each day of the week, differentiated by the time of day (Lunch/Dinner). 2. Ensure that: - Bars are dodged based on the `time` variable. - Any empty spaces due to missing combinations of `days` and `time` are filled. 3. Add a second plot layer with a transformed dot plot showing the same `day` and `time` variables. - Apply a dodge transformation to the dots. - Add a jitter transformation to the dots. 4. The bar plot should have a semantic variable for color, set to `sex`. # Expected Output Your output should be a single plot with the following characteristics: - A bar plot with dodged bars, using different colors for `time`, and filling empty spaces. - Overlaid dots, also dodged and jittered, showing the distribution of tips given, differentiated by `sex`. # Example Snippet to Help You Get Started ```python import seaborn.objects as so from seaborn import load_dataset # Load the data tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Create the plot p = so.Plot(tips, \\"day\\", color=\\"time\\") # Add a dodged bar plot with filled spaces p.add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\")) # Add a dodged and jittered dot plot, with color differentiated by sex p.add(so.Dot(), so.Dodge(), so.Jitter()) # Show the plot p.show() ``` # Constraints - Use only the Seaborn `objects` module for this task. - Ensure the plot is well-labeled, with appropriate titles and legends where necessary.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_bar_dot_plot(): # Load the data tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Create the plot p = so.Plot(tips, \\"day\\", color=\\"time\\") # Add a dodged bar plot with filled spaces and color by sex p.add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\")).facet(\\"sex\\") # Add a dodged and jittered dot plot p.add(so.Dot(), so.Dodge(), so.Jitter()) # Show the plot p.show()"},{"question":"# Question 1: Function Object Manipulation In this task, you are required to write a Python C-extension module that creates, inspects, and manipulates Python functions using the `PyFunction_*` set of functions described in the provided documentation. Part 1: Create and Return a Simple Function Object Write a C function `create_simple_function` that creates and returns a simple function object which takes a single integer argument and returns its square. Part 2: Retrieve Function Details Write a C function `get_function_details` that takes a function object as input and returns a dictionary containing the following details about the function: - The function\'s code object - The function\'s globals dictionary - The function\'s default arguments - The function\'s annotations Part 3: Modify Function Defaults Write a C function `set_function_defaults` that takes a function object and a tuple of default arguments as input, and sets the function\'s default arguments to the provided tuple. Part 4: Python Integration Create a Python wrapper for your C-extension module that: 1. Exposes the functions `create_simple_function`, `get_function_details`, and `set_function_defaults` to Python. 2. Provides Python unit tests for each of these functionalities. # Input and Output Formats - Part 1: `create_simple_function` does not take any arguments and returns a callable Python function. - Part 2: `get_function_details` takes a callable Python function and returns a dictionary with keys `\\"code\\"`, `\\"globals\\"`, `\\"defaults\\"`, and `\\"annotations\\"`. - Part 3: `set_function_defaults` takes a callable Python function and a tuple of default arguments, it modifies the function in place and returns `None`. # Constraints - Your implementation must use the provided functions from the documentation. - All operations should handle error cases gracefully, e.g., by returning appropriate error messages if an invalid function object is provided. - Performance considerations are secondary but aim to keep the solution efficient for standard use cases. # Example Python Code Example Using the C-extension ```python # Assuming the extension module is named \'function_ext\' from function_ext import create_simple_function, get_function_details, set_function_defaults func = create_simple_function() print(func(5)) # Output: 25 details = get_function_details(func) print(details) set_function_defaults(func, (10,)) print(func()) # Calls func with default, expected output: 100 ```","solution":"def create_simple_function(): Creates and returns a simple function that takes a single integer argument and returns its square. def simple_function(x): return x * x return simple_function def get_function_details(func): Returns details of a given function including code object, globals, defaults, and annotations. details = { \'code\': func.__code__, \'globals\': func.__globals__, \'defaults\': func.__defaults__, \'annotations\': func.__annotations__ } return details def set_function_defaults(func, defaults): Sets the default arguments for a given function. func.__defaults__ = defaults"},{"question":"# Seaborn Residual Plot Analysis You are tasked with analyzing a dataset using Seaborn to create and interpret residual plots to diagnose linear regression models. Follow the steps and instructions below to complete this assignment. Dataset Use the `mpg` dataset from Seaborn. Steps and Instructions 1. **Data Loading and Preparation**: - Load the `mpg` dataset using Seaborn. 2. **Basic Residual Plot**: - Generate and display a residual plot for `weight` vs. `displacement`. - Interpret any visible patterns and discuss what they suggest about the linear relationship between `weight` and `displacement`. 3. **Check Regression Assumptions**: - Generate a residual plot for `horsepower` vs. `mpg`. - Identify any patterns in the residuals and discuss their implications for the assumptions of the linear regression model between `horsepower` and `mpg`. 4. **Higher-order Trends**: - Generate a residual plot for `horsepower` vs. `mpg` by including a higher-order term (`order=2`). - Analyze the changes in the residuals and explain how the higher-order term affects the model\'s performance. 5. **LOWESS Smoothing**: - Create a residual plot for `horsepower` vs. `mpg` with a LOWESS smoothing curve and highlight the curve in red. - Discuss how the addition of the smoothing curve impacts your understanding of the relationship between `horsepower` and `mpg`. Submission Submit a Jupyter notebook containing: - The code needed for each step. - Plots generated. - A thorough explanation and interpretation of each plot. Example Below is an example code snippet to load the dataset and create a basic residual plot: ```python import seaborn as sns sns.set_theme() mpg = sns.load_dataset(\\"mpg\\") # Basic Residual Plot sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset mpg = sns.load_dataset(\\"mpg\\") def basic_residual_plot(): Generates and displays a residual plot for weight vs. displacement. sns.set_theme() plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual Plot: Weight vs. Displacement\\") plt.show() def residual_plot_with_horsepower(): Generates and displays a residual plot for horsepower vs. mpg. sns.set_theme() plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Residual Plot: Horsepower vs. MPG\\") plt.show() def higher_order_residual_plot(): Generates and displays a residual plot for horsepower vs. mpg with higher order term. sns.set_theme() plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Residual Plot (Order 2): Horsepower vs. MPG\\") plt.show() def lowess_residual_plot(): Generates and displays a residual plot for horsepower vs. mpg with LOWESS smoothing. sns.set_theme() plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws={\'color\': \'red\'}) plt.title(\\"Residual Plot with LOWESS: Horsepower vs. MPG\\") plt.show()"},{"question":"# Seaborn Advanced Plotting Assessment **Objective:** You need to demonstrate your understanding of seaborn\'s `seaborn.objects` module by creating a customized scatter plot with multiple visual properties. **Dataset:** We will use the `mpg` dataset provided by seaborn. This dataset contains information on car fuel efficiency. **Task:** 1. Load the `mpg` dataset from seaborn. 2. Create a scatter plot of `horsepower` (x-axis) vs `mpg` (y-axis) using `seaborn.objects`: - Use dots as the marker (`so.Dots`). - Assign the `origin` column to the color of the dots. - Set the fill color of the dots based on the `weight` of the cars, and use a binary color scale. - Use a jitter of `0.25` on the x-axis to visualize density, enabling partial opacity to better visualize overlaps. 3. Configure the plot to use different shapes for markers based on the `origin` of the cars, using variations like circles (`o`), crosses (`x`), and a custom marker. **Requirements:** - Your code should be clear and well-documented. - The plot should be informative and visually appealing. - Use appropriate seaborn functions and classes as demonstrated. **Constraints:** - You must use seaborn\'s `objects` module (import as `so`). - Ensure that the plot is annotated and titled appropriately to make it clear what the data represents. **Performance:** - The plot should be generated efficiently without redundant computations. **Input:** No input required; the code should load the `mpg` dataset directly within the script. **Output:** A matplotlib figure showing the scatter plot with all the specified customizations. ```python # Your solution here: import seaborn.objects as so from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the plot with specified customizations ( so.Plot(mpg, \\"horsepower\\", \\"mpg\\") .add(so.Dots(), color=\\"origin\\", fillcolor=\\"weight\\") .scale(fillcolor=\\"binary\\") .add(so.Dots(stroke=1), marker=\\"origin\\").scale(marker=[\\"o\\", \\"x\\", (6, 2, 1)]) .add(so.Jitter(.25)) ) ``` **Note:** You may include additional customizations to improve the readability and aesthetics of the plot if you see fit.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the plot with specified customizations ( so.Plot(mpg, x=\\"horsepower\\", y=\\"mpg\\") .add(so.Dots(), color=\\"origin\\", fillcolor=\\"weight\\", jitter=.25, opacity=0.7) .scale(fillcolor=\\"binary\\", marker=[\\"o\\", \\"x\\", \\"^\\"]) .plot() )"},{"question":"**Context Management in Asynchronous Programming** You have been provided with a `contextvars` module to manage and access context-local states in Python. This task will evaluate your understanding of the `ContextVar` class, its methods, and its usage in an asynchronous programming context that requires handling state efficiently and safely. # Problem Statement Create an asynchronous service that manages user sessions using `contextvars.ContextVar`. Each session should store the user ID and maintain a counter of the number of requests made by that user in the current session. Specifically, you will implement the following functionalities: 1. **Create Context Variables**: - `session_user_id`: Stores the current user\'s ID. - `request_count`: Stores the count of requests made in the current session. 2. **Asynchronous Request Handler**: - Increment the user\'s request count. - Retrieve the current user ID and request count, and return them in a formatted string. 3. **Main Function**: - Set up an asynchronous server that handles multiple users and demonstrates the isolated context for each user\'s session. # Function Definitions 1. **initialize_session**: ```python from contextvars import ContextVar session_user_id = ContextVar(\\"session_user_id\\", default=None) request_count = ContextVar(\\"request_count\\", default=0) async def initialize_session(user_id: int): Initialize a session for a user by setting the session_user_id and resetting the request_count. :param user_id: int - Unique identifier for the user # Your code here ``` 2. **handle_request**: ```python async def handle_request(): Handle an incoming request by incrementing the request count and returning a formatted string containing the user ID and request count. :return: str - Formatted string with user ID and request count # Your code here ``` 3. **main**: ```python async def main(): Set up an asynchronous server to manage user requests. The server should handle requests concurrently for multiple users and demonstrate isolated context for each user\'s session. # Simulate handling requests for multiple users await initialize_session(1) print(await handle_request()) # Expected: User 1 - Request 1 await initialize_session(2) print(await handle_request()) # Expected: User 2 - Request 1 # Handling additional requests print(await handle_request()) # Expected: User 2 - Request 2 await initialize_session(1) print(await handle_request()) # Expected: User 1 - Request 2 ``` # Constraints - User IDs are positive integers. - `initialize_session` should reset the `request_count` for a new session. - The `handle_request` function should operate in the context of the current session. - Handle requests asynchronously to simulate real-world use cases. # Expected Output Running the `main` function should demonstrate the isolated context for each session with the following output: ``` User 1 - Request 1 User 2 - Request 1 User 2 - Request 2 User 1 - Request 2 ``` # Notes - Your implementation should utilize `ContextVar` for storing and managing session variables. - Ensure asynchronous behavior for handling requests. Good luck!","solution":"from contextvars import ContextVar import asyncio session_user_id = ContextVar(\\"session_user_id\\", default=None) request_count = ContextVar(\\"request_count\\", default=0) async def initialize_session(user_id: int): Initialize a session for a user by setting the session_user_id and resetting the request_count. :param user_id: int - Unique identifier for the user session_user_id.set(user_id) request_count.set(0) async def handle_request(): Handle an incoming request by incrementing the request count and returning a formatted string containing the user ID and request count. :return: str - Formatted string with user ID and request count current_count = request_count.get() + 1 request_count.set(current_count) user_id = session_user_id.get() return f\\"User {user_id} - Request {current_count}\\" async def main(): Set up an asynchronous server to manage user requests. The server should handle requests concurrently for multiple users and demonstrate isolated context for each user\'s session. await initialize_session(1) print(await handle_request()) # Expected: User 1 - Request 1 await initialize_session(2) print(await handle_request()) # Expected: User 2 - Request 1 print(await handle_request()) # Expected: User 2 - Request 2 await initialize_session(1) print(await handle_request()) # Expected: User 1 - Request 2 # To actually run the main function if executed directly if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Asynchronous Web Scraper **Objective**: The goal of this exercise is to demonstrate your understanding of Python\'s `asyncio` library, including creating and running coroutines, managing tasks, handling concurrency, and implementing timeouts and cancellations. **Problem Statement**: You are required to create an asynchronous web scraper that fetches data from multiple URLs concurrently. The web scraper should perform the following operations: 1. **Fetch Data**: - Define a coroutine `fetch_data(url: str) -> str` that performs an HTTP GET request to the given URL and returns the response text. - Implement a timeout of 3 seconds for each fetch operation. If the fetch operation takes longer than 3 seconds, it should be cancelled, and an appropriate message should be logged. 2. **Scrape URLs Concurrently**: - Define a coroutine `scrape_urls(urls: List[str]) -> List[str]` that takes a list of URLs, fetches their data concurrently using `fetch_data`, and returns a list of response texts. - Use `asyncio.create_task()` to create tasks for each fetch operation. - Use `asyncio.gather()` to run these tasks concurrently and collect their results. 3. **Main Coroutine**: - Implement the `main()` coroutine that initializes a list of URLs, calls `scrape_urls` with this list, and prints the results. # Input - A list of URLs (for the purpose of this task, you can use dummy URLs or mock HTTP responses). # Constraints - Use a timeout of 3 seconds for each fetch request. - Handle possible `asyncio.TimeoutError` exceptions. # Expected Output - A list of response texts from the URLs. - Appropriate logging messages for URLs that time out. # Example Usage ```python import asyncio from typing import List async def fetch_data(url: str) -> str: # Implement the fetch logic with timeout handling pass async def scrape_urls(urls: List[str]) -> List[str]: # Implement the logic to fetch data from multiple URLs concurrently pass async def main(): urls = [\\"https://example.com\\", \\"https://httpbin.org/delay/5\\", \\"https://jsonplaceholder.typicode.com/posts\\"] results = await scrape_urls(urls) for result in results: print(result) if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Notes: 1. You can use the `aiohttp` library to perform HTTP GET requests asynchronously. 2. Ensure to handle all exceptions that may occur during the fetch operation. **Performance Requirements**: - The solution should be efficient in terms of handling multiple concurrent HTTP requests. - Properly manage the asyncio event loop to avoid blocking operations.","solution":"import asyncio import aiohttp from typing import List async def fetch_data(url: str) -> str: Performs an HTTP GET request to the given URL and returns the response text. Implements a timeout of 3 seconds. async with aiohttp.ClientSession() as session: try: async with session.get(url, timeout=3) as response: return await response.text() except asyncio.TimeoutError: return f\\"Timeout error for URL: {url}\\" except Exception as e: return f\\"Exception {str(e)} occurred for URL: {url}\\" async def scrape_urls(urls: List[str]) -> List[str]: Takes a list of URLs, fetches their data concurrently using fetch_data, and returns a list of response texts. tasks = [asyncio.create_task(fetch_data(url)) for url in urls] return await asyncio.gather(*tasks) async def main(): urls = [\\"https://example.com\\", \\"https://httpbin.org/delay/5\\", \\"https://jsonplaceholder.typicode.com/posts\\"] results = await scrape_urls(urls) for result in results: print(result) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Question: Handling and Manipulating Bytes Objects in Python You are required to implement a Python function that performs several operations on bytes objects. This will test your understanding of creating, accessing, and manipulating bytes objects using various techniques. Function Specification: **Function Name:** `process_bytes` **Input:** - `input_string`: A string representing the initial data to create the bytes object. - `concat_string`: A string which needs to be concatenated to the previous bytes object. - `resize_length`: An integer specifying the new size of the bytes object after concatenation (It may be smaller than the concatenated bytes length). **Output:** - A dictionary with the following keys and corresponding values: - `\'initial_bytes\'`: The initial bytes object created from `input_string`. - `\'concatenated_bytes\'`: The bytes object after concatenation with `concat_string`. - `\'resized_bytes\'`: The bytes object after resizing to `resize_length`. **Constraints:** - `input_string` and `concat_string` are non-empty strings. - `resize_length` is a non-negative integer. **Example:** ```python def process_bytes(input_string: str, concat_string: str, resize_length: int) -> dict: # Your implementation here # Example Usage result = process_bytes(\\"hello\\", \\"world\\", 8) print(result) # Expected Output: # { # \'initial_bytes\': b\'hello\', # \'concatenated_bytes\': b\'helloworld\', # \'resized_bytes\': b\'hellowor\' # } ``` # Steps to Implement: 1. **Create the initial bytes object** from the given `input_string`. 2. **Concatenate** the initial bytes object with `concat_string`. 3. **Resize** the concatenated bytes object to `resize_length`. # Notes: - Use the appropriate functions from the provided documentation to handle bytes objects. - Ensure the keys in the output dictionary are exactly as specified. - Include error handling for invalid types or values as per the constraints.","solution":"def process_bytes(input_string: str, concat_string: str, resize_length: int) -> dict: Function to process bytes objects by creating, concatenating, and resizing. Parameters: input_string (str): Initial string to create a bytes object. concat_string (str): String to concatenate to initial bytes object. resize_length (int): New size for the concatenated bytes object. Returns: dict: A dictionary containing the initial bytes, concatenated bytes, and resized bytes. # Create initial bytes object from input_string initial_bytes = bytes(input_string, \'utf-8\') # Concatenate with concat_string concatenated_bytes = initial_bytes + bytes(concat_string, \'utf-8\') # Resize the concatenated bytes object resized_bytes = concatenated_bytes[:resize_length] return { \'initial_bytes\': initial_bytes, \'concatenated_bytes\': concatenated_bytes, \'resized_bytes\': resized_bytes }"},{"question":"**Objective:** Implement a Python function that interacts with an HTTP server using the `urllib.request` module and demonstrates comprehension of handling HTTP requests, responses, and basic HTTP authentication. **Task:** Write a Python function `fetch_and_authenticate` that performs the following steps: 1. Opens an HTTP URL and retrieves its content. 2. If the server responds with a 401 status code (Unauthorized), it retries the request with HTTP Basic Authentication. 3. Returns the content of the URL if the request is successful; otherwise, it raises an appropriate error. **Function Signature:** ```python def fetch_and_authenticate(url: str, user: str = None, password: str = None) -> str: pass ``` **Input:** - `url` (str): The URL to be fetched. - `user` (str, optional): The username for HTTP Basic Authentication. Defaults to None. - `password` (str, optional): The password for HTTP Basic Authentication. Defaults to None. **Output:** - (str): The content of the URL as a string. **Constraints:** - If `user` and `password` are not provided, the function should attempt to access the URL without authentication. - If the server responds with a 401 error and `user` and `password` are provided, the function should retry the request with the provided credentials. - If the server responds with any other status code, the function should raise an `URLError` or `HTTPError` as appropriate. **Example Usage:** ```python content = fetch_and_authenticate(\\"http://example.com/protected\\", \\"username\\", \\"password\\") print(content) ``` **Requirements:** - Use the `urllib.request` module exclusively to handle HTTP requests and responses. - Handle HTTP Basic Authentication using `HTTPBasicAuthHandler`. - Provide meaningful error handling for various server responses. **Hints:** - Use `urllib.request.urlopen` to open the URL. - Use `urllib.request.HTTPBasicAuthHandler` to handle authentication. - Use context managers to manage the lifecycle of the HTTP request.","solution":"import urllib.request from urllib.error import URLError, HTTPError from base64 import b64encode def fetch_and_authenticate(url: str, user: str = None, password: str = None) -> str: Fetch a URL and retry with HTTP Basic Authentication if needed. :param url: The URL to be fetched. :param user: The username for HTTP Basic Authentication. :param password: The password for HTTP Basic Authentication. :return: The content of the URL as a string. request = urllib.request.Request(url) try: response = urllib.request.urlopen(request) return response.read().decode(\'utf-8\') except HTTPError as e: if e.code == 401 and user is not None and password is not None: # Retry with authentication. credentials = f\'{user}:{password}\' encoded_credentials = b64encode(credentials.encode(\'utf-8\')).decode(\'utf-8\') request.add_header(\'Authorization\', f\'Basic {encoded_credentials}\') # Try again with credentials try: response = urllib.request.urlopen(request) return response.read().decode(\'utf-8\') except HTTPError as auth_err: raise auth_err else: raise e except URLError as e: raise e"},{"question":"# Question: Create a Custom Python Zip Archive with Filters and Compression You are tasked with creating a custom Python zip archive using the `zipapp` module. Your goal is to compress a directory of Python files into a `.pyz` archive, while applying specific filters and ensuring the archive is executable on a POSIX system. **Requirements:** 1. Implement a function `create_custom_archive(source_dir, target_file, interpreter_path, main_module_callable, excluded_files, compressed)`. 2. The function parameters are: - `source_dir` (str): Path to the source directory containing Python files. - `target_file` (str): Path where the output `.pyz` file should be saved. - `interpreter_path` (str): Path to the interpreter to be used in the shebang line of the archive. - `main_module_callable` (str): The main entry point in the format `pkg.module:callable`. - `excluded_files` (List[str]): A list of filenames or patterns to exclude from the archive. - `compressed` (bool): Whether to compress the files in the archive. 3. The function should apply the filters to exclude specified files or patterns. 4. Ensure the resulting archive is compressed if the `compressed` parameter is `True`. 5. Include the specified interpreter path in the shebang line. **Constraints:** - The `source_dir` should not already contain a `__main__.py` file, as the main entry point will be added dynamically. - The specified `interpreter_path` must be a valid path to a Python interpreter. **Expected Function Signature:** ```python def create_custom_archive(source_dir: str, target_file: str, interpreter_path: str, main_module_callable: str, excluded_files: List[str], compressed: bool) -> None: pass ``` # Example Usage: ```python source_directory = \'myapp\' output_archive = \'myapp.pyz\' interpreter = \'/usr/bin/env python3\' main_callable = \'myapp:main\' exclude = [\'test_*\', \'*.txt\'] compress = True create_custom_archive(source_directory, output_archive, interpreter, main_callable, exclude, compress) ``` In this example, the directory `myapp` is archived into `myapp.pyz` with the specified interpreter, main entry point, excluding files that match `\'test_*\'` and `\'*.txt\'`, and the files are compressed. # Evaluation Criteria: - Correct implementation of the function according to specifications. - Proper handling of file and path manipulations. - Effective utilization of the `zipapp` module and its API. - Demonstration of skills in filtering and compressing files. - Robustness and error handling.","solution":"import os import zipapp import fnmatch import sys def create_custom_archive(source_dir, target_file, interpreter_path, main_module_callable, excluded_files, compressed): Create a custom .pyz archive from a source directory with specified filters and compression. Parameters: - source_dir (str): Path to the source directory containing Python files. - target_file (str): Path where the output .pyz file should be saved. - interpreter_path (str): Path to the interpreter to be used in the shebang line of the archive. - main_module_callable (str): The main entry point in the format `pkg.module:callable`. - excluded_files (List[str]): A list of filenames or patterns to exclude from the archive. - compressed (bool): Whether to compress the files in the archive. def filter_fn(path): Filter function to exclude specified files. filename = os.path.basename(path) for pattern in excluded_files: if fnmatch.fnmatch(filename, pattern): return False return True # Convert the main_module_callable into __main__.py content main_module, callable_name = main_module_callable.split(\':\') main_py_content = f if __name__ == \'__main__\': from {main_module} import {callable_name} {callable_name}() # Create a temporary __main__.py file tmp_main_file = os.path.join(source_dir, \'__main__.py\') with open(tmp_main_file, \'w\') as main_file: main_file.write(main_py_content) # Using the zipapp module to create the archive try: zipapp.create_archive( source=source_dir, target=target_file, interpreter=interpreter_path, main=None, # main is None because we manually added __main__.py filter=filter_fn, compressed=compressed ) finally: # Remove the temporary __main__.py file os.remove(tmp_main_file)"},{"question":"# Unix-Specific Terminal Mode Handling You are required to implement a Python program that changes the terminal mode of a given file descriptor between \'raw\' and \'cbreak\' modes to enable certain types of input handling. This exercise will help you understand how to work with low-level terminal control in a Unix environment using Python. **Task**: 1. Write a function `set_terminal_mode(fd: int, mode: str, when: int = termios.TCSAFLUSH) -> None` that changes the terminal mode of the specified file descriptor `fd`. - `fd`: the file descriptor of the terminal. - `mode`: a string that can be either `\'raw\'` or `\'cbreak\'`. - `when`: an optional integer parameter (default is `termios.TCSAFLUSH`). The function should: - Use `tty.setraw` when the mode is `\'raw\'`. - Use `tty.setcbreak` when the mode is `\'cbreak\'`. - Raise an appropriate exception for any other mode. 2. Demonstrate the function by: - Opening `/dev/tty` for reading the terminal file descriptor. - Switching between \'raw\' and \'cbreak\' modes and printing a message confirming the change. **Constraints**: - Only Unix-based systems are supported as the `tty` module is dependent on `termios` which is Unix-specific. - Ensure to handle any potential errors gracefully, including invalid file descriptors or unsupported modes. **Expected Output**: - The function does not return any value but should print appropriate messages indicating the mode changes and any errors encountered. # Example ```python import os import tty import termios def set_terminal_mode(fd: int, mode: str, when: int = termios.TCSAFLUSH) -> None: if mode == \'raw\': tty.setraw(fd, when) print(f\\"Terminal mode set to \'raw\' for fd {fd}.\\") elif mode == \'cbreak\': tty.setcbreak(fd, when) print(f\\"Terminal mode set to \'cbreak\' for fd {fd}.\\") else: raise ValueError(\\"Mode must be either \'raw\' or \'cbreak\'.\\") # Demonstration try: with open(\'/dev/tty\', \'rb\') as tty_fd: fd = tty_fd.fileno() set_terminal_mode(fd, \'raw\') # you can perform some read operations here in raw mode if needed set_terminal_mode(fd, \'cbreak\') # you can perform some read operations here in cbreak mode if needed except Exception as e: print(f\\"Error: {e}\\") ``` Make sure to test the implemented function in a Unix environment and validate its behavior by observing the program\'s output when switching terminal modes.","solution":"import os import tty import termios def set_terminal_mode(fd: int, mode: str, when: int = termios.TCSAFLUSH) -> None: Changes the terminal mode of the specified file descriptor. Parameters: fd (int): The file descriptor of the terminal. mode (str): The terminal mode (\'raw\' or \'cbreak\'). when (int): An optional parameter that determines when the mode change occurs. Defaults to termios.TCSAFLUSH. Raises: ValueError: If the mode is not \'raw\' or \'cbreak\'. if mode == \'raw\': tty.setraw(fd, when) print(f\\"Terminal mode set to \'raw\' for fd {fd}.\\") elif mode == \'cbreak\': tty.setcbreak(fd, when) print(f\\"Terminal mode set to \'cbreak\' for fd {fd}.\\") else: raise ValueError(\\"Mode must be either \'raw\' or \'cbreak\'.\\")"},{"question":"Coding Assessment Question: Thread Pool Implementation with Timeout # Objective Create a basic thread pool executor using the `_thread` module. The executor should be able to manage a pool of threads, assign tasks to them, and enforce a timeout for each task. # Requirements 1. **Thread Pool Class**: Implement a `ThreadPool` class with the following methods: - `__init__(self, size)`: Initialize the pool with a fixed number of threads. - `submit(self, fn, *args, **kwargs)`: Assign a new task to the pool. Tasks should be executed as threads become available. - `shutdown(self, wait=True)`: Clean up the pool, ensuring all threads have completed if `wait=True`. 2. **Task Management**: - Use `threading.Lock` or `_thread.allocate_lock()` for managing access to shared resources. - Implement a mechanism to enforce a timeout for task execution. - Use a queue to manage tasks awaiting execution. 3. **Thread Behavior**: - Threads should pick up tasks from the queue and execute them. - If a task exceeds the timeout, the thread should be terminated and resources cleaned up. # Constraints - Only use the `_thread` module for thread management and locks. - Do not use the `threading` module or higher-level threading abstractions. # Input and Output Format - **Input**: Initialization of the `ThreadPool` class with an integer indicating the number of threads in the pool. - **Output**: Print statements within the task functions to indicate when tasks start and end. # Example Usage ```python def example_task(duration): import time print(f\\"Task started for {duration} seconds\\") time.sleep(duration) print(f\\"Task ended after {duration} seconds\\") if __name__ == \\"__main__\\": pool = ThreadPool(3) pool.submit(example_task, 2) pool.submit(example_task, 5) pool.submit(example_task, 1) pool.shutdown(wait=True) ``` # Implementation Notes - Ensure that deadlocks are avoided and that the pool is properly cleaned up when shutting down. - Handle exceptions and errors gracefully, ensuring that threads which fail do not impact the overall functionality. Happy Coding!","solution":"import _thread import time import queue class ThreadPool: def __init__(self, size): self.size = size self.tasks = queue.Queue() self.lock = _thread.allocate_lock() self.threads = [] self.shutdown_flag = False for _ in range(size): thread = _thread.start_new_thread(self.worker, ()) self.threads.append(thread) def worker(self): while not self.shutdown_flag: try: fn, args, kwargs, timeout = self.tasks.get(timeout=1) self.execute_with_timeout(fn, args, kwargs, timeout) except queue.Empty: continue def execute_with_timeout(self, fn, args, kwargs, timeout): def wrapper(): fn(*args, **kwargs) worker_thread = _thread.start_new_thread(wrapper, ()) timeout_time = time.time() + timeout while time.time() < timeout_time and _thread.get_ident() == worker_thread: time.sleep(0.1) if _thread.get_ident() == worker_thread: _thread.exit() def submit(self, fn, *args, **kwargs): timeout = kwargs.pop(\'timeout\', 5) self.tasks.put((fn, args, kwargs, timeout)) def shutdown(self, wait=True): self.shutdown_flag = True if wait: for thread in self.threads: try: while _thread.get_ident() == thread: time.sleep(0.1) except Exception: pass"},{"question":"# Command Line Argument Parser with `getopt` You are required to implement a Python script that simulates a simple command-line tool using the `getopt` module. Your tool should accept and correctly process the following command-line options: Short Options: - `-h`: Display a help message and exit. - `-v`: Enable verbose mode. - `-l <logfile>`: Specify a logfile. Long Options: - `--help`: Display a help message and exit. - `--verbose`: Enable verbose mode. - `--logfile=<file>`: Specify a logfile. # Requirements: 1. Create a Python function named `parse_command_line_arguments(args)` which takes a list of arguments (excluding the script name) as input and returns a dictionary with the keys `\'help\'`, `\'verbose\'`, and `\'logfile\'`. 2. The `help` key should be set to `True` if `-h` or `--help` is present in the arguments. 3. The `verbose` key should be set to `True` if `-v` or `--verbose` is present in the arguments. 4. The `logfile` key should hold the string value of the logfile specified either by `-l <logfile>` or `--logfile=<file>`. If no logfile is specified, it should be `None`. 5. If an invalid option or missing required argument is encountered, the function should raise a `getopt.GetoptError`. # Input Format: - A list of command-line arguments (excluding the script name). # Output Format: - A dictionary with the processed options and their values. # Constraints: - Use the `getopt` module for parsing. - Handle errors gracefully by raising a `getopt.GetoptError` with an appropriate message. # Example: ```python import getopt def parse_command_line_arguments(args): options = { \'help\': False, \'verbose\': False, \'logfile\': None } try: opts, _ = getopt.getopt(args, \'hvl:\', [\'help\', \'verbose\', \'logfile=\']) for opt, arg in opts: if opt in (\'-h\', \'--help\'): options[\'help\'] = True elif opt in (\'-v\', \'--verbose\'): options[\'verbose\'] = True elif opt in (\'-l\', \'--logfile\'): options[\'logfile\'] = arg except getopt.GetoptError as err: raise getopt.GetoptError(f\\"Error parsing arguments: {err}\\") return options # Example usage: args = [\\"-v\\", \\"--logfile=output.log\\"] print(parse_command_line_arguments(args)) # Output: {\'help\': False, \'verbose\': True, \'logfile\': \'output.log\'} ``` Write and test your implementation to ensure it handles the specified options and arguments correctly.","solution":"import getopt def parse_command_line_arguments(args): options = { \'help\': False, \'verbose\': False, \'logfile\': None } try: opts, _ = getopt.getopt(args, \'hvl:\', [\'help\', \'verbose\', \'logfile=\']) for opt, arg in opts: if opt in (\'-h\', \'--help\'): options[\'help\'] = True elif opt in (\'-v\', \'--verbose\'): options[\'verbose\'] = True elif opt in (\'-l\', \'--logfile\'): options[\'logfile\'] = arg except getopt.GetoptError as err: raise getopt.GetoptError(f\\"Error parsing arguments: {err}\\") return options"},{"question":"Coding Assessment Question **Objective:** Implement a custom dictionary class in Python that mimics part of the functionality provided by the C API functions in Python dictionaries. **Task:** Create a class `CustomDict` with the following methods: 1. `__init__(self)`: Initializes an empty dictionary. 2. `clear(self)`: Empties all key-value pairs from the dictionary. 3. `contains(self, key)`: Returns `True` if the dictionary contains the specified key, otherwise `False`. 4. `set_item(self, key, value)`: Inserts the specified key-value pair into the dictionary. 5. `del_item(self, key)`: Removes the key-value pair with the specified key from the dictionary. 6. `get_item(self, key)`: Returns the value corresponding to the specified key, or `None` if the key is not in the dictionary. 7. `size(self)`: Returns the number of items in the dictionary. 8. `items(self)`: Returns a list of tuples containing the key-value pairs in the dictionary. 9. `keys(self)`: Returns a list of keys in the dictionary. 10. `values(self)`: Returns a list of values in the dictionary. 11. `copy(self)`: Returns a copy of the dictionary. 12. `merge(self, other_dict, override=True)`: Merges another dictionary into this one. If `override` is `True`, existing keys will be overwritten. **Constraints:** - Your implementation should handle typical Python dictionary constraints, such as keys being hashable. - Optimize for performance where possible. **Example Usage:** ```python # Initialize cd = CustomDict() # Add items cd.set_item(\\"a\\", 1) cd.set_item(\\"b\\", 2) print(cd.items()) # Output: [(\'a\', 1), (\'b\', 2)] # Check size print(cd.size()) # Output: 2 # Check contains print(cd.contains(\\"a\\")) # Output: True print(cd.contains(\\"z\\")) # Output: False # Get item print(cd.get_item(\\"b\\")) # Output: 2 # Delete item cd.del_item(\\"a\\") print(cd.items()) # Output: [(\'b\', 2)] # Merge another dict cd2 = CustomDict() cd2.set_item(\\"c\\", 3) cd.merge(cd2) print(cd.items()) # Output: [(\'b\', 2), (\'c\', 3)] # Get keys and values print(cd.keys()) # Output: [\'b\', \'c\'] print(cd.values()) # Output: [2, 3] # Copy dictionary cd3 = cd.copy() print(cd3.items()) # Output: [(\'b\', 2), (\'c\', 3)] # Clear dictionary cd.clear() print(cd.items()) # Output: [] ``` **Note:** Pay attention to error handling and edge cases, such as non-hashable keys and attempts to get or delete non-existing keys.","solution":"class CustomDict: def __init__(self): self._dict = {} def clear(self): self._dict.clear() def contains(self, key): return key in self._dict def set_item(self, key, value): self._dict[key] = value def del_item(self, key): if key in self._dict: del self._dict[key] def get_item(self, key): return self._dict.get(key, None) def size(self): return len(self._dict) def items(self): return list(self._dict.items()) def keys(self): return list(self._dict.keys()) def values(self): return list(self._dict.values()) def copy(self): new_dict = CustomDict() new_dict._dict = self._dict.copy() return new_dict def merge(self, other_dict, override=True): for key, value in other_dict.items(): if override or key not in self._dict: self._dict[key] = value"},{"question":"You are required to create an asynchronous TCP server using the `socketserver` module from Python. The server should process incoming messages from clients, reverse the received message string, and send the reversed string back to the client. Requirements: 1. Define a request handler class by subclassing `socketserver.StreamRequestHandler` that reverses the incoming message from the client. 2. Create an asynchronous TCP server using `ThreadingMixIn`. 3. The server should handle multiple clients concurrently, and each client\'s request should be processed in a separate thread. 4. Demonstrate how the server handles multiple client requests and how each client can send a message and receive a reversed message in response. Input & Output Format: - The client will send a string message to the server. - The server will receive the string, reverse it, and send it back to the client. Example: **Client sends:** \\"hello\\" **Server responses:** \\"olleh\\" **Client sends:** \\"Python\\" **Server responses:** \\"nohtyP\\" Constraints: - The message strings will have a maximum length of 1024 characters. - The server should be able to handle at least 5 simultaneous client requests. - Ensure thread safety in the handling of client requests. Implementation Constraints: 1. Use `ThreadingMixIn` to achieve asynchronous behavior. 2. Implement the server using classes and methods as described in the provided documentation. Detailed Steps: 1. Implement the `ReversingTCPHandler` class inherited from `socketserver.StreamRequestHandler` and override the `handle` method. 2. Implement the `ReversingTCPServer` class inherited from both `socketserver.ThreadingMixIn` and `socketserver.TCPServer`. 3. Write the main logic to start the server and handle incoming clients. 4. Demonstrate the server and client communication with at least 3 example client requests. # Your Task: Implement the described server and provide a demonstration script showing at least 3 client-server interactions. # Example: Here is an outline of the approach you might take: ```python import socketserver import threading class ReversingTCPHandler(socketserver.StreamRequestHandler): def handle(self): self.data = self.rfile.readline().strip() print(f\\"{self.client_address} wrote: {self.data}\\") reversed_data = self.data[::-1] self.wfile.write(reversed_data + b\\"n\\") class ReversingTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass def main(): HOST, PORT = \\"localhost\\", 9999 with ReversingTCPServer((HOST, PORT), ReversingTCPHandler) as server: server_thread = threading.Thread(target=server.serve_forever) server_thread.daemon = True server_thread.start() print(\\"Server loop running in thread:\\", server_thread.name) server_thread.join() if __name__ == \\"__main__\\": main() ``` Provide the complete implementation according to these guidelines and run a few client requests to test your server.","solution":"import socketserver class ReversingTCPHandler(socketserver.StreamRequestHandler): def handle(self): # Read a line of input from the client self.data = self.rfile.readline().strip() print(f\\"Received from {self.client_address}: {self.data}\\") # Reverse the data reversed_data = self.data[::-1] # Send the reversed data back to the client self.wfile.write(reversed_data + b\\"n\\") class ReversingTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass def main(): HOST, PORT = \\"localhost\\", 9999 # Create the server, binding to localhost on port 9999 with ReversingTCPServer((HOST, PORT), ReversingTCPHandler) as server: # Activate the server; this will keep running until you # interrupt the program with Ctrl-C server.serve_forever() if __name__ == \\"__main__\\": main()"},{"question":"Objective: To assess your understanding of using scikit-learn for building and evaluating machine learning models, including data preprocessing and handling. Problem Statement: You are provided with a synthetic dataset. The dataset is designed for a regression task with some noise. Your task is to: 1. Generate a synthetic dataset using `make_regression` with the following parameters: - `n_samples=1000` - `n_features=10` - `noise=0.1` 2. Split the generated dataset into training and testing sets with a ratio of 75/25 using `train_test_split`. 3. Preprocess the data by standardizing the features using `StandardScaler`. 4. Fit a `GradientBoostingRegressor` model to the training set without any hyperparameter tuning, and calculate the R^2 score on the test set. 5. Modify the `n_iter_no_change` parameter of the `GradientBoostingRegressor` to 5 and 10, fit the model again to the training set each time, and calculate the R^2 score for each case on the test set. 6. Report the three R^2 scores obtained. Input and Output Formats: **Input:** - None. You need to generate the dataset within the function. **Output:** - A dictionary with keys `default`, `n_iter_no_change_5`, and `n_iter_no_change_10`, containing the corresponding R^2 scores for each model. Constraints: - Use `random_state=42` wherever applicable to ensure reproducibility. - Ensure the code follows good practices for readability and reproducibility. Function Signature: ```python def evaluate_regression_models(): # Your code here return {\\"default\\": default_score, \\"n_iter_no_change_5\\": score_5, \\"n_iter_no_change_10\\": score_10} ``` Example: ```python scores = evaluate_regression_models() print(scores) # Expected Output (Example): # { # \\"default\\": 0.85, # \\"n_iter_no_change_5\\": 0.84, # \\"n_iter_no_change_10\\": 0.83 # } ``` **Note:** The actual values of R^2 scores may vary based on the random generation of the dataset.","solution":"from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import r2_score def evaluate_regression_models(): # Generate synthetic dataset X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42) # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Fit and evaluate default GradientBoostingRegressor gbr_default = GradientBoostingRegressor(random_state=42) gbr_default.fit(X_train, y_train) y_pred_default = gbr_default.predict(X_test) default_score = r2_score(y_test, y_pred_default) # Fit and evaluate GradientBoostingRegressor with n_iter_no_change=5 gbr_5 = GradientBoostingRegressor(n_iter_no_change=5, random_state=42) gbr_5.fit(X_train, y_train) y_pred_5 = gbr_5.predict(X_test) score_5 = r2_score(y_test, y_pred_5) # Fit and evaluate GradientBoostingRegressor with n_iter_no_change=10 gbr_10 = GradientBoostingRegressor(n_iter_no_change=10, random_state=42) gbr_10.fit(X_train, y_train) y_pred_10 = gbr_10.predict(X_test) score_10 = r2_score(y_test, y_pred_10) return {\\"default\\": default_score, \\"n_iter_no_change_5\\": score_5, \\"n_iter_no_change_10\\": score_10}"},{"question":"# Custom Operator Implementation in PyTorch Objective Implement a custom operator in PyTorch that computes the element-wise exponential of a tensor, registers a backend kernel to perform the computation, and validates it using PyTorch\'s testing utilities. Tasks 1. **Define a Custom Operation**: - Use `torch.library.custom_op` to define a custom operation named `exponential_op`. 2. **Implement the Operation\'s Kernel**: - Register a backend kernel using `torch.library.register_kernel` that implements the `exponential_op` to compute the element-wise exponential of a tensor. 3. **Validate the Custom Operation**: - Use `torch.library.opcheck` to verify the correct usage of the API. - Use `torch.autograd.gradcheck` if your operator supports autograd to test that the gradients are mathematically correct. # Requirements: - The custom operator should be named `exponential_op`. - The operator should take a single input tensor and return a tensor of the same shape, where each element is the exponential of the corresponding input element. - Your code should include unit tests to verify the operation\'s correctness. # Input and Output Formats: - **Input**: A single input tensor of any shape. - **Output**: A tensor of the same shape where each element is the exponential of the corresponding input element. # Example Input: ```python input_tensor = torch.tensor([1.0, 2.0, 3.0]) ``` Output: ```python output_tensor = torch.tensor([2.7183, 7.3891, 20.0855]) # Approximate values of exp(1), exp(2), exp(3) ``` # Constraints: - Your implementation should handle both CPU and CUDA tensors. - Ensure the implementation is efficient. - Demonstrate testing for both function correctness and gradient correctness. # Performance Requirements: - The operation should be efficient and match the performance characteristics of built-in PyTorch operations whenever possible. Submission Submit your implementation and test code in a single Python script or a Jupyter notebook.","solution":"import torch from torch import library import math # Define the custom operation my_lib = library.Library(\\"my_ops\\", \\"DEF\\") my_lib.define(\\"exponential_op(Tensor input) -> Tensor\\") # Register the kernel for the operation @torch.library.impl(my_lib, \\"exponential_op\\", \\"CPU\\") def exponential_op_cpu(input): return torch.exp(input) # Register the kernel for CUDA if torch.cuda.is_available(): @torch.library.impl(my_lib, \\"exponential_op\\", \\"CUDA\\") def exponential_op_cuda(input): return torch.exp(input)"},{"question":"**Objective**: To test your understanding and ability to work with the `subprocess` module in Python. **Problem Statement**: You are tasked with writing a function `execute_commands` that takes a list of shell commands and executes them sequentially. The function should ensure the following: 1. Each command is run in a new process. 2. Capture both `stdout` and `stderr` of each command. 3. Handle cases where commands do not execute successfully by capturing their error messages. 4. Return a dictionary with the command as the key and its output/error message as the value. **Function Signature**: ```python def execute_commands(commands: List[str], timeout: int = 10) -> Dict[str, str]: ``` **Inputs**: - `commands`: A list of shell commands to be executed. - Example: `[\\"ls -l\\", \\"echo Hello World\\", \\"invalid_command\\"]` - `timeout`: An optional argument specifying the maximum time (in seconds) to wait for each command to complete. Default is 10 seconds. **Outputs**: - A dictionary where: - Keys are the commands as strings. - Values are the captured `stdout` if the command succeeded or the captured `stderr` if the command failed. **Constraints**: - Ensure the function does not hang indefinitely by using the `timeout` parameter. - Use `subprocess.run()` for executing commands. **Example**: ```python commands = [\\"ls -l\\", \\"echo Hello World\\", \\"invalid_command\\"] output = execute_commands(commands) ``` The `output` could be: ```python { \\"ls -l\\": \\"total 0ndrwxr-xr-x 3 user staff 96 Oct 5 12:34 foldern\\", \\"echo Hello World\\": \\"Hello Worldn\\", \\"invalid_command\\": \\"/bin/sh: invalid_command: command not foundn\\" } ``` **Starter Code**: ```python from typing import List, Dict import subprocess def execute_commands(commands: List[str], timeout: int = 10) -> Dict[str, str]: result = {} for cmd in commands: try: completed_process = subprocess.run(cmd, capture_output=True, text=True, shell=True, timeout=timeout) if completed_process.returncode == 0: result[cmd] = completed_process.stdout else: result[cmd] = completed_process.stderr except subprocess.TimeoutExpired: result[cmd] = \\"Command timed out\\" except subprocess.SubprocessError as e: result[cmd] = str(e) return result ``` In the given function, you are expected to: 1. Run each command using `subprocess.run()`. 2. Use `capture_output=True` to get both `stdout` and `stderr`. 3. Check `returncode` to determine if the command was successful or if it failed, and record the appropriate output. 4. Handle `TimeoutExpired` and other `SubprocessError` exceptions properly. Good luck!","solution":"from typing import List, Dict import subprocess def execute_commands(commands: List[str], timeout: int = 10) -> Dict[str, str]: result = {} for cmd in commands: try: completed_process = subprocess.run(cmd, capture_output=True, text=True, shell=True, timeout=timeout) if completed_process.returncode == 0: result[cmd] = completed_process.stdout else: result[cmd] = completed_process.stderr except subprocess.TimeoutExpired: result[cmd] = \\"Command timed out\\" except subprocess.SubprocessError as e: result[cmd] = str(e) return result"},{"question":"# PyTorch Gradient Check for Complex Functions Objective Your task is to implement a complex mathematical function using PyTorch and verify its gradients using `torch.autograd.gradcheck`. Description 1. **Function Implementation**: Implement a PyTorch function `complex_function` that takes a complex tensor `z` as input and computes a real-valued output `y`. The function is defined as follows: [ y = sum_{i=1}^{N} (Re(z_i)^2 - Im(z_i)^2) ] where ( z ) is a complex tensor, ( Re(z_i) ) is the real part of the i-th element, and ( Im(z_i) ) is the imaginary part of the i-th element. 2. **Gradient Check**: Use `torch.autograd.gradcheck` to verify the gradients of the `complex_function` with respect to the input tensor `z`. 3. **Input and Output**: - Input: - `z`: A PyTorch tensor of complex numbers with shape `(N,)`. Requires gradient. - Output: - `y`: A single real-valued scalar tensor. 4. **Constraints**: - You should ensure that the input tensor `z` has `requires_grad=True`. - Use a small perturbation `eps=1e-6` for the numerical differences in gradient checking. - The input tensor `z` should have both real and imaginary components set randomly between specific ranges to ensure diverse testing. Requirements 1. Implement the `complex_function` in PyTorch. 2. Verify the gradients using `torch.autograd.gradcheck`. Example Usage ```python import torch def complex_function(z): return torch.sum(torch.real(z) ** 2 - torch.imag(z) ** 2) # Define the complex input tensor N = 5 z = torch.rand(N, dtype=torch.cdouble, requires_grad=True) # Perform gradient check eps = 1e-6 assert torch.autograd.gradcheck(complex_function, (z,), eps=eps), \\"GradCheck failed!\\" print(\\"GradCheck passed!\\") ``` Implement the `complex_function` and ensure it passes the gradcheck for given constraints.","solution":"import torch def complex_function(z): Computes the sum of the squares of the real parts minus the squares of the imaginary parts of a complex tensor. Args: - z (torch.Tensor): A complex tensor with shape (N,). Returns: - torch.Tensor: A real-valued scalar tensor. return torch.sum(torch.real(z) ** 2 - torch.imag(z) ** 2)"},{"question":"Background You are given a collection of files and directories in different formats (text files, directories, symbolic links, etc.). Your task is to create a tar archive of these files, list its contents, and extract files based on certain criteria using the Python `tarfile` module. Task 1. Implement a function `create_tar_archive(input_paths, archive_name)` that takes a list of file/directory paths and creates a tar archive (`archive_name.tar.gz`). 2. Implement a function `list_tar_contents(archive_name, verbose=True)` that lists the contents of the tar archive. If `verbose` is `True`, it should display details similar to the `ls -l` command. 3. Implement a function `extract_python_files(archive_name, output_dir)` that extracts only `.py` files from the tar archive into the given output directory. 4. Implement a custom extraction filter that prevents extracting files with names containing the string \\"unsafe\\". Use this filter in the extraction of `.py` files. Requirements: - The tar archive should be compressed using gzip. - Utilize the `tarfile` module in Python. - Ensure that your solution includes proper error handling. Constraints: - You can assume all paths provided in `input_paths` exist and are valid. Input and Output Formats: - Functions Overview: ```python def create_tar_archive(input_paths: List[str], archive_name: str) -> None: pass def list_tar_contents(archive_name: str, verbose: bool = True) -> None: pass def extract_python_files(archive_name: str, output_dir: str) -> None: pass ``` - `create_tar_archive`: Takes a list of paths (files/directories) and the desired name of the tar archive (excluding the extension). This function will not return anything. - `list_tar_contents`: Takes the name of the tar archive and an optional verbose flag. It prints the contents of the archive. - `extract_python_files`: Takes the name of the tar archive and the output directory. It extracts `.py` files to the specified directory using a custom filter. Below is an example usage of the functions: ```python input_paths = [\'file1.py\', \'directory1\', \'link1\'] archive_name = \'example_archive\' create_tar_archive(input_paths, archive_name) list_tar_contents(archive_name) output_dir = \'extracted_files\' extract_python_files(archive_name, output_dir) ``` The above code should create a gzip compressed tar archive, list its contents, and extract only the `.py` files to the `extracted_files` directory, ignoring files with names containing \\"unsafe\\".","solution":"import os import tarfile from typing import List def create_tar_archive(input_paths: List[str], archive_name: str) -> None: Creates a tar.gz archive from the given list of file and directory paths. :param input_paths: List of paths to files and directories to include in the archive :param archive_name: The name of the tar.gz archive to create (without extension) with tarfile.open(f\\"{archive_name}.tar.gz\\", \\"w:gz\\") as tar: for path in input_paths: tar.add(path) def list_tar_contents(archive_name: str, verbose: bool = True) -> None: Lists the contents of the tar.gz archive. :param archive_name: The name of the tar.gz archive to list contents :param verbose: If True, display details similar to \'ls -l\' command with tarfile.open(f\\"{archive_name}.tar.gz\\", \\"r:gz\\") as tar: if verbose: for tarinfo in tar.getmembers(): print(f\\"{tarinfo.mode:06o} {tarinfo.uid}/{tarinfo.gid} {tarinfo.size} {tarinfo.mtime} {tarinfo.name}\\") else: for tarinfo in tar.getmembers(): print(tarinfo.name) def extract_python_files(archive_name: str, output_dir: str) -> None: Extracts .py files from the tar.gz archive into the given output directory, ignoring files with \\"unsafe\\" in their names. :param archive_name: The name of the tar.gz archive to extract .py files from :param output_dir: The directory to extract the .py files to def is_safe(tarinfo): return tarinfo.name.endswith(\'.py\') and \'unsafe\' not in tarinfo.name with tarfile.open(f\\"{archive_name}.tar.gz\\", \\"r:gz\\") as tar: for member in tar.getmembers(): if is_safe(member): tar.extract(member, path=output_dir)"},{"question":"You are tasked with creating a function that provides summarized metadata information about a list of installed packages. This function will leverage the `importlib.metadata` module to retrieve details such as the version, entry points, and specific metadata fields of given packages. This function should handle errors gracefully and return collected information in a structured format. # Function Signature ```python def get_packages_summary(package_names: list) -> dict: pass ``` # Expected Input - `package_names`: A list of strings where each string is a name of an installed package. Example: `[\'wheel\', \'setuptools\', \'pip\']`. # Expected Output - A dictionary where keys are the package names and values are another dictionary containing the package\'s version, entry points, metadata fields (at least \'Name\', \'Version\', \'Summary\'), and any requirements (if available). # Constraints - You should handle cases where a package might not be found. - You should handle errors if metadata fields or entry points are not available for a package. - The function should be performant and fetch data in a single call (or minimum API calls) per package. # Example Usage ```python >>> get_packages_summary([\'wheel\', \'setuptools\']) { \'wheel\': { \'version\': \'0.32.3\', \'entry_points\': { \'console_scripts\': [\'wheel\'], \'distutils.commands\': [], \'distutils.setup_keywords\': [], \'egg_info.writers\': [], \'setuptools.installation\': [] }, \'metadata\': { \'Name\': \'wheel\', \'Version\': \'0.32.3\', \'Summary\': \'A built-package format for Python\' }, \'requirements\': [\\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\"] }, \'setuptools\': { \'version\': \'50.3.2\', \'entry_points\': { \'console_scripts\': [\'easy_install\'], \'distutils.commands\': [], \'distutils.setup_keywords\': [], \'egg_info.writers\': [], \'setuptools.installation\': [] }, \'metadata\': { \'Name\': \'setuptools\', \'Version\': \'50.3.2\', \'Summary\': \'Easily download, build, install, upgrade, and uninstall Python packages\' }, \'requirements\': None } } ``` # Notes - Use the `importlib.metadata.version()`, `importlib.metadata.entry_points()`, `importlib.metadata.metadata()`, and `importlib.metadata.requires()` functions as needed to fetch this information. - Ensure to handle exceptions and missing data gracefully, providing clear output for each package.","solution":"import importlib.metadata from typing import List, Dict, Any def get_packages_summary(package_names: List[str]) -> Dict[str, Any]: summary = {} for package in package_names: try: version = importlib.metadata.version(package) except importlib.metadata.PackageNotFoundError: summary[package] = { \'error\': \'Package not found\' } continue try: entry_points = importlib.metadata.entry_points(group=None).select(name=package) except Exception as e: entry_points = {\'error\': str(e)} try: metadata = importlib.metadata.metadata(package) metadata_fields = { \'Name\': metadata.get(\'Name\', \'Unknown\'), \'Version\': metadata.get(\'Version\', \'Unknown\'), \'Summary\': metadata.get(\'Summary\', \'No summary available\') } except Exception as e: metadata_fields = {\'error\': str(e)} try: requirements = importlib.metadata.requires(package) or [] except Exception as e: requirements = {\'error\': str(e)} summary[package] = { \'version\': version, \'entry_points\': entry_points, \'metadata\': metadata_fields, \'requirements\': requirements } return summary"},{"question":"**Coding Assessment Question:** # Function Implementation: `analyze_path` Objective Create a function `analyze_path` that takes a list of file paths as input and returns a detailed analysis of each path. The function should utilize various functionalities of the `os.path` module to provide insights about the input paths. Function Signature ```python def analyze_path(paths: list) -> dict: pass ``` Input - `paths`: A list of strings, where each string represents a file path. Example: `[\\"/usr/bin/python3\\", \\"C:WindowsSystem32\\", \\"~/Desktop/notes.txt\\"]` Output - A dictionary where keys are the input paths, and values are dictionaries containing insights about the paths. Each inner dictionary should contain the following keys: - `is_absolute`: Boolean indicating if the path is absolute. - `exists`: Boolean indicating if the path exists. - `is_file`: Boolean indicating if the path is a file. - `is_dir`: Boolean indicating if the path is a directory. - `abspath`: The absolute path of the file. - `basename`: The base name of the file. - `dirname`: The directory name of the file. - `normpath`: The normalized version of the path. - `realpath`: The canonical path, eliminating any symbolic links. Constraints 1. The input list will have at least one path and at most 100 paths. 2. Path strings may be in UNIX or Windows formats. Performance Requirements - Your function should handle the maximum input size efficiently. - Consider filesystem access costs while implementing the solution. Example ```python input_paths = [\\"/usr/bin/python3\\", \\"C:WindowsSystem32\\", \\"~/Desktop/notes.txt\\"] expected_output = { \\"/usr/bin/python3\\": { \\"is_absolute\\": True, \\"exists\\": True, \\"is_file\\": False, \\"is_dir\\": True, \\"abspath\\": \\"/usr/bin/python3\\", \\"basename\\": \\"python3\\", \\"dirname\\": \\"/usr/bin\\", \\"normpath\\": \\"/usr/bin/python3\\", \\"realpath\\": \\"/usr/bin/python3\\" }, \\"C:WindowsSystem32\\": { \\"is_absolute\\": True, \\"exists\\": True, \\"is_file\\": False, \\"is_dir\\": True, \\"abspath\\": \\"C:WindowsSystem32\\", \\"basename\\": \\"System32\\", \\"dirname\\": \\"C:Windows\\", \\"normpath\\": \\"C:WindowsSystem32\\", \\"realpath\\": \\"C:WindowsSystem32\\" }, \\"~/Desktop/notes.txt\\": { \\"is_absolute\\": False, \\"exists\\": True, \\"is_file\\": True, \\"is_dir\\": False, \\"abspath\\": \\"/home/user/Desktop/notes.txt\\", \\"basename\\": \\"notes.txt\\", \\"dirname\\": \\"/home/user/Desktop\\", \\"normpath\\": \\"/home/user/Desktop/notes.txt\\", \\"realpath\\": \\"/home/user/Desktop/notes.txt\\" } } output_paths = analyze_path(input_paths) assert output_paths == expected_output ``` Notes: - Use the `os.path` functions to implement this solution. - Ensure the function handles both Windows and UNIX path formats. - Make sure to handle paths that include environment variables and user home directory symbols.","solution":"import os def analyze_path(paths): Analyzes the given list of paths and returns a dictionary of path insights. analysis = {} for path in paths: expanded_path = os.path.expanduser(os.path.expandvars(path)) analysis[path] = { \\"is_absolute\\": os.path.isabs(expanded_path), \\"exists\\": os.path.exists(expanded_path), \\"is_file\\": os.path.isfile(expanded_path), \\"is_dir\\": os.path.isdir(expanded_path), \\"abspath\\": os.path.abspath(expanded_path), \\"basename\\": os.path.basename(expanded_path), \\"dirname\\": os.path.dirname(expanded_path), \\"normpath\\": os.path.normpath(expanded_path), \\"realpath\\": os.path.realpath(expanded_path) } return analysis"},{"question":"# Advanced PyTorch Profiling and Performance Optimization You are tasked with profiling and optimizing the performance of a deep learning model using PyTorch\'s TorchInductor. Specifically, you need to analyze the model at a granular kernel level and identify the kernels that take the most time. Instructions: 1. **Environment Setup:** - Set up the necessary environment variables to ensure meaningful kernel names and benchmark individual kernels. 2. **Benchmark Script:** - Write a script to run a given PyTorch model (`resnet18`, available from the `torchvision.models` package) with the necessary TorchInductor configurations. - Ensure that the model runs in a way that enables profiling and produces an output log with the performance details of the different kernels. 3. **Performance Analysis:** - Parse the output log to extract information about the overall GPU usage and the percentage of time taken by individual kernel categories (e.g., pointwise, reduction). - Identify the top three most time-consuming kernels and generate a summary report. 4. **Kernel Benchmarking:** - For the most expensive kernel, generate a standalone python file similar to `k.py` in the provided documentation. - This file should contain the kernel code and a function to benchmark its execution time and bandwidth. - Optionally, include a configuration to enable maximum autotuning and re-run the benchmark to observe improvements. 5. **Output:** - Provide the summarized performance report extracted from the profiling output. - Submit the standalone Python file used for individual kernel benchmarking. Note: - Your solution should be efficient and handle any edge cases with appropriate exceptions. - Ensure that your scripts are well-documented and include comments for clarity. - Pay special attention to the readability and organization of the code. Example: ```python import os import subprocess # Step 1: Setup Environment Variables os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' # Step 2: Run Benchmark Script command = \'python -u benchmarks/dynamo/timm_models.py --backend inductor --amp --performance --dashboard --only resnet18 --disable-cudagraphs --training\' subprocess.run(command, shell=True) # Step 3: Parse Output Log and Generate Summary Report def parse_log(log_file): # Implementation to parse log and extract required information pass log_file = \\"/path/to/profile_output.log\\" report = parse_log(log_file) print(report) # Step 4: Generate Python File for Kernel Benchmarking def generate_kernel_benchmark(kernel_name, kernel_code): # Implementation to generate k.py for kernel benchmarking pass most_expensive_kernel = \\"triton_red_fused_example_kernel\\" kernel_code = \\"...\\" # Actual kernel code extracted generate_kernel_benchmark(most_expensive_kernel, kernel_code) # Step 5: Autotune and Re-benchmark the Kernel os.environ[\'TORCHINDUCTOR_MAX_AUTOTUNE\'] = \'1\' subprocess.run(f\'python /tmp/{most_expensive_kernel}.py\', shell=True) ``` The above outline provides a guide on how to structure your solution. Adapt the functions and steps to fully meet the requirements of the task.","solution":"import os import subprocess import re # Step 1: Setup Environment Variables def setup_environment(): os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' # Step 2: Run Benchmark Script def run_benchmark(): command = \'python -u benchmarks/dynamo/timm_models.py --backend inductor --amp --performance --dashboard --only resnet18 --disable-cudagraphs --training\' subprocess.run(command, shell=True) # Step 3: Parse Output Log and Generate Summary Report def parse_log(log_file): kernel_times = {} kernel_total_time = 0.0 with open(log_file, \'r\') as file: for line in file: match = re.search(r\'([^s]+)s+:s+(d+.d+) ms\', line) if match: kernel_name, time_taken = match.groups() time_taken = float(time_taken) kernel_total_time += time_taken if kernel_name in kernel_times: kernel_times[kernel_name] += time_taken else: kernel_times[kernel_name] = time_taken sorted_kernels = sorted(kernel_times.items(), key=lambda item: item[1], reverse=True)[:3] report = { \\"total_gpu_time\\": kernel_total_time, \\"top_kernels\\": [{\\"name\\": k, \\"time\\": t, \\"percentage\\": (t / kernel_total_time) * 100} for k, t in sorted_kernels] } return report # Step 4: Generate Python File for Kernel Benchmarking def generate_kernel_benchmark(kernel_name, kernel_code, output_path=\'/tmp/k.py\'): with open(output_path, \'w\') as file: file.write(f import torch import time import triton {kernel_code} def benchmark_kernel(): # Setup code start = time.time() # Kernel execution end = time.time() print(f\\"Execution time: {{end - start}} seconds\\") if __name__ == \\"__main__\\": benchmark_kernel() ) # Main Execution def main(): setup_environment() run_benchmark() log_file = \\"/path/to/profile_output.log\\" report = parse_log(log_file) # Print the report print(\\"Performance Summary Report\\") print(f\\"Total GPU Time: {report[\'total_gpu_time\']} ms\\") for kernel in report[\\"top_kernels\\"]: print(f\\"Kernel Name: {kernel[\'name\']}, Time: {kernel[\'time\']} ms, Percentage: {kernel[\'percentage\']:.2f}%\\") # Assuming we have identified the most expensive kernel code (mocked here) most_expensive_kernel_code = # Placeholder kernel code @triton.jit def example_kernel(X, Y): pass most_expensive_kernel_name = report[\'top_kernels\'][0][\'name\'] generate_kernel_benchmark(most_expensive_kernel_name, most_expensive_kernel_code) if __name__ == \'__main__\': main()"},{"question":"Objective: To assess the understanding of asyncio synchronization primitives, particularly focusing on the `Lock` and `Event` classes, and their appropriate usage in an asynchronous Python program. Task: You are required to implement a simplified version of a hotel booking system using asyncio primitives. The system should allow multiple customers to try to book a room asynchronously. An `asyncio.Lock` should be used to ensure that only one customer can book a single room at any given moment. An `asyncio.Event` should be used to notify customers when a room becomes available. Requirements: 1. Implement a class `Hotel` with the following methods: - `__init__(self, total_rooms: int)`: Initializes the hotel with a given number of rooms. - `async book_room(self, customer_id: int)`: Tries to book a room for the customer. If no rooms are available, the customer should wait until a room becomes available. - `async release_room(self, customer_id: int)`: Releases a room, making it available for booking. 2. The `Hotel` class must use `asyncio.Lock` to ensure that room booking and releasing operations are performed exclusively. 3. The `Hotel` class must use `asyncio.Event` to notify waiting customers when a room becomes available. 4. Implement an `async main()` function to simulate multiple customers attempting to book rooms. Use `asyncio.gather()` to manage the concurrency. Constraints: - Assume the number of rooms initialized will be a positive integer. - Customer IDs will be unique integers. - The maximum number of customers will not exceed 1000. Example: ```python import asyncio class Hotel: def __init__(self, total_rooms: int): # initialize here async def book_room(self, customer_id: int): # implement room booking logic here async def release_room(self, customer_id: int): # implement room releasing logic here async def customer_action(hotel: Hotel, customer_id: int): # simulate customer trying to book and release a room await hotel.book_room(customer_id) await asyncio.sleep(2) # simulate the duration of room occupancy await hotel.release_room(customer_id) async def main(): hotel = Hotel(total_rooms=3) customers = [customer_action(hotel, i) for i in range(5)] await asyncio.gather(*customers) if __name__ == \\"__main__\\": asyncio.run(main()) ``` Expected Output: The output order may vary due to the asynchronous nature, but it must show that at most three customers at a time are occupying rooms, and others are waiting for their turn. No two customers should book the same room simultaneously.","solution":"import asyncio class Hotel: def __init__(self, total_rooms: int): self.total_rooms = total_rooms self.available_rooms = total_rooms self.lock = asyncio.Lock() self.room_available_event = asyncio.Event() self.room_available_event.set() async def book_room(self, customer_id: int): async with self.lock: while self.available_rooms == 0: await self.room_available_event.wait() self.available_rooms -= 1 if self.available_rooms == 0: self.room_available_event.clear() print(f\\"Customer {customer_id} booked a room. Available rooms: {self.available_rooms}\\") async def release_room(self, customer_id: int): async with self.lock: self.available_rooms += 1 if self.available_rooms == 1: self.room_available_event.set() print(f\\"Customer {customer_id} released a room. Available rooms: {self.available_rooms}\\") async def customer_action(hotel: Hotel, customer_id: int): await hotel.book_room(customer_id) await asyncio.sleep(2) # Simulate the duration of room occupancy await hotel.release_room(customer_id) async def main(): hotel = Hotel(total_rooms=3) customers = [customer_action(hotel, i) for i in range(5)] await asyncio.gather(*customers) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"You are required to implement a custom content manager for Python\'s `email` package, extending its capabilities to handle `json` content types. This custom content manager should be able to get and set JSON content in an email message. Requirements: 1. **Class Definition**: - Define a class `JsonContentManager` derived from `email.contentmanager.ContentManager`. 2. **Methods to Implement**: - `get_content(msg, **kwargs)`: This should extract the JSON payload from the message and return it as a Python dictionary. If the message is not of JSON type, raise a `KeyError`. - `set_content(msg, obj, **kwargs)`: This should take a Python dictionary, convert it into a JSON string, and set it as the payload of the message. Add the appropriate `Content-Type` header (`application/json`). 3. **Handler Registration**: - Register appropriate handlers for getting and setting JSON content using the `add_get_handler` and `add_set_handler` methods. Input/Output Format: - **Input**: - For `get_content`: A message object with JSON content. - For `set_content`: A message object and a Python dictionary to be set as JSON content. - **Output**: - For `get_content`: Return the JSON content as a Python dictionary. - For `set_content`: The message object should be modified to include the JSON content and appropriate headers. - **Constraints**: - Ensure proper MIME type handling for `application/json`. - Handle errors gracefully, such as when a non-JSON message is passed. Example: ```python from email.message import EmailMessage import json from email.contentmanager import ContentManager class JsonContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'application/json\', self._get_json_content) self.add_set_handler(dict, self._set_json_content) def _get_json_content(self, msg, **kwargs): if msg.get_content_type() != \'application/json\': raise KeyError(f\\"No handler for MIME type {msg.get_content_type()}\\") return json.loads(msg.get_payload()) def _set_json_content(self, msg, obj, **kwargs): msg.set_payload(json.dumps(obj)) msg.set_type(\'application/json\') # Example usage email_msg = EmailMessage() json_manager = JsonContentManager() data = {\\"name\\": \\"John Doe\\", \\"email\\": \\"john.doe@example.com\\"} json_manager.set_content(email_msg, data) print(email_msg.get_payload()) retrieved_data = json_manager.get_content(email_msg) print(retrieved_data) ``` In the example above: - The `JsonContentManager` is used to set a JSON dictionary as the payload of an `EmailMessage`. - The `get_content` method is used to retrieve the JSON content from the message. Make sure your implementation correctly manages MIME types and handles errors as specified.","solution":"from email.message import EmailMessage import json from email.contentmanager import ContentManager class JsonContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'application/json\', self._get_json_content) self.add_set_handler(dict, self._set_json_content) def _get_json_content(self, msg, **kwargs): if msg.get_content_type() != \'application/json\': raise KeyError(f\\"No handler for MIME type {msg.get_content_type()}\\") return json.loads(msg.get_payload()) def _set_json_content(self, msg, obj, **kwargs): msg.set_payload(json.dumps(obj)) msg.set_type(\'application/json\') # Example usage email_msg = EmailMessage() json_manager = JsonContentManager() data = {\\"name\\": \\"John Doe\\", \\"email\\": \\"john.doe@example.com\\"} json_manager.set_content(email_msg, data) print(email_msg.get_payload()) # This should output the JSON string: {\\"name\\": \\"John Doe\\", \\"email\\": \\"john.doe@example.com\\"} retrieved_data = json_manager.get_content(email_msg) print(retrieved_data) # This should output the dictionary: {\'name\': \'John Doe\', \'email\': \'john.doe@example.com\'}"},{"question":"**Question: Implement a File Compressor/Decompressor using `bz2`** You are tasked with creating a tool in Python that compresses and decompresses text files using the bzip2 compression algorithm from the `bz2` module. This tool should handle both single-pass and incremental operations effectively. # Requirements: 1. **Function Definitions**: - `compress_file(input_filename: str, output_filename: str, compresslevel: int = 9) -> None`: - Opens and compresses the file located at `input_filename`. - Writes the compressed data to `output_filename`. - `compresslevel` determines the level of compression (1-9). - `decompress_file(input_filename: str, output_filename: str) -> None`: - Opens and decompresses the file located at `input_filename`. - Writes the decompressed data to `output_filename`. 2. **Incremental Compression and Decompression**: - `incremental_compress_file(input_filename: str, output_filename: str, chunk_size: int = 1024, compresslevel: int = 9) -> None`: - Compresses the file in chunks defined by `chunk_size`. - Suitable for large files that may not fit into memory for single-pass operations. - `incremental_decompress_file(input_filename: str, output_filename: str, chunk_size: int = 1024) -> None`: - Decompresses the file in chunks similar to incremental compression. 3. **Constraints**: - Ensure the input file exists and can be read. - The output file should correctly reflect the content of the decompressed or compressed data. # Example: ```python # Sample usage of the tool # Single-pass operations compress_file(\'example.txt\', \'example.txt.bz2\') decompress_file(\'example.txt.bz2\', \'decompressed_example.txt\') # Incremental operations incremental_compress_file(\'large_example.txt\', \'large_example.txt.bz2\') incremental_decompress_file(\'large_example.txt.bz2\', \'decompressed_large_example.txt\') ``` **Note**: Use proper exception handling to manage file operations and invalid compression levels. Ensure that compression levels are set between 1 and 9, and chunk sizes are reasonable for memory usage considerations. The output file names must differ from the input file names to avoid overwriting.","solution":"import bz2 def compress_file(input_filename: str, output_filename: str, compresslevel: int = 9) -> None: if compresslevel < 1 or compresslevel > 9: raise ValueError(\\"compresslevel must be between 1 and 9\\") with open(input_filename, \'rb\') as input_file, bz2.BZ2File(output_filename, \'wb\', compresslevel=compresslevel) as output_file: input_data = input_file.read() output_file.write(input_data) def decompress_file(input_filename: str, output_filename: str) -> None: with bz2.BZ2File(input_filename, \'rb\') as input_file, open(output_filename, \'wb\') as output_file: input_data = input_file.read() output_file.write(input_data) def incremental_compress_file(input_filename: str, output_filename: str, chunk_size: int = 1024, compresslevel: int = 9) -> None: if compresslevel < 1 or compresslevel > 9: raise ValueError(\\"compresslevel must be between 1 and 9\\") with open(input_filename, \'rb\') as input_file, bz2.BZ2File(output_filename, \'wb\', compresslevel=compresslevel) as output_file: while chunk := input_file.read(chunk_size): output_file.write(chunk) def incremental_decompress_file(input_filename: str, output_filename: str, chunk_size: int = 1024) -> None: with bz2.BZ2File(input_filename, \'rb\') as input_file, open(output_filename, \'wb\') as output_file: while chunk := input_file.read(chunk_size): output_file.write(chunk)"},{"question":"**Objective**: Demonstrate a deep understanding of asyncio\'s `Future` class and related functionalities in Python. **Requirements**: Implement a function that coordinates multiple asynchronous operations using `Future` objects. Specifically, you will write a function `process_tasks` that: 1. Schedules three asynchronous operations. 2. Each operation waits for different durations before setting a result. 3. Collects the results into a list once all operations are complete. # Function Signature ```python import asyncio async def process_tasks() -> list: pass ``` # Detailed Instructions: 1. **Create Three Futures**: - Create three `Future` objects using the event loop\'s `create_future()` method. 2. **Set Up Tasks**: - Define three asynchronous tasks that: - Sleep for a given number of seconds. - Set a predefined value on the corresponding `Future` once the sleep period is over. - Schedule these tasks to run concurrently. 3. **Collect Results**: - Await the completion of all `Future` objects. - Ensure the results of the futures are collected into a list. 4. **Return the List of Results**: - The function should return a list containing the results set on the three futures in the order they were created. # Example Output: ```python results = asyncio.run(process_tasks()) print(results) # Output could be something like: [\'result1\', \'result2\', \'result3\'] ``` # Constraints: - You must use `asyncio` and the `Future` class specifically. - Handle any exceptions that occur during the tasks and ensure they do not crash the function. - Demonstrate proper use of callbacks and setting results in `Future` objects. **Hint:** Utilize `asyncio.gather` to await multiple futures if needed. Implement the `process_tasks` function as described above.","solution":"import asyncio async def set_future_after_delay(future, result, delay): await asyncio.sleep(delay) future.set_result(result) async def process_tasks() -> list: loop = asyncio.get_event_loop() future1 = loop.create_future() future2 = loop.create_future() future3 = loop.create_future() task1 = set_future_after_delay(future1, \'result1\', 1) task2 = set_future_after_delay(future2, \'result2\', 2) task3 = set_future_after_delay(future3, \'result3\', 3) await asyncio.gather(task1, task2, task3) return [future1.result(), future2.result(), future3.result()]"},{"question":"**Objective:** Assess the students\' understanding and application of Seaborn\'s color palette and colormap functionalities. # Problem Statement You are provided with a dataset containing information about different species of flowers, their sepal and petal dimensions, and their species type. Your task is to write a function `visualize_species_distribution` that uses Seaborn to visualize the distribution of sepal lengths across different species using customized color palettes. The function should perform the following steps: 1. Create a light color palette using the color \\"skyblue\\". 2. Increase the number of colors in the palette to match the number of species. 3. Plot a violin plot of the sepal length for each species using the created palette. 4. Return the figure object. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_species_distribution(data: pd.DataFrame) -> plt.Figure: pass # Your implementation here ``` # Input - `data`: A pandas DataFrame containing at least the columns `sepal_length` and `species`. # Output - A Matplotlib Figure object containing the violin plot. # Constraints - The DataFrame will have at most 10,000 rows. - The column `species` will have no more than 10 unique values. - You should use the color palette functionalities from Seaborn as demonstrated in the provided documentation. # Example Usage ```python import seaborn as sns import pandas as pd # Sample data data = pd.DataFrame({ \'sepal_length\': [5.1, 4.9, 6.7, 5.6, 5.0, 6.1], \'species\': [\'setosa\', \'setosa\', \'versicolor\', \'virginica\', \'setosa\', \'versicolor\'] }) # Visualize the data fig = visualize_species_distribution(data) plt.show() ``` **Note:** Ensure that your plot is clear and visually distinguishable.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_species_distribution(data: pd.DataFrame) -> plt.Figure: Visualizes the distribution of sepal lengths across different species using a violin plot with a custom color palette. Parameters: data (pd.DataFrame): DataFrame containing at least the columns \'sepal_length\' and \'species\'. Returns: plt.Figure: A Matplotlib Figure object containing the violin plot. # Create a light color palette using the color \\"skyblue\\" unique_species = data[\'species\'].unique() n_colors = len(unique_species) palette = sns.light_palette(\\"skyblue\\", n_colors=n_colors) # Create the violin plot plt.figure(figsize=(10, 6)) violin_plot = sns.violinplot(x=\'species\', y=\'sepal_length\', data=data, palette=palette) # Customize the plot (optional) plt.title(\'Distribution of Sepal Lengths by Species\') plt.xlabel(\'Species\') plt.ylabel(\'Sepal Length\') # Retrieve the figure object fig = violin_plot.get_figure() return fig"},{"question":"**Coding Assessment Question:** # Logging System Implementation You are tasked with developing a logging system for a fictional web application that performs several concurrent operations and requires detailed logging for troubleshooting and auditing purposes. Requirements: 1. **Create a Logger:** - Name the logger \\"webApp\\". - Set the logging level to `DEBUG`. 2. **Add Handlers:** - Add a `StreamHandler` to the logger, so logs are displayed in the console. - Add a `FileHandler` to write logs to a file named \\"webapp.log\\". Open the file in append mode. 3. **Custom Formatter:** - Define a custom format for log messages: `\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'`. - Apply this format to both handlers. 4. **Logging Levels:** - Log messages at different levels (DEBUG, INFO, WARNING, ERROR, CRITICAL) demonstrating the logger\'s functionality. 5. **Contextual Information:** - Add contextual information (e.g., user ID and request ID) to log messages. 6. **Logger Hierarchy:** - Create a child logger named \\"webApp.database\\" and another child logger named \\"webApp.network\\". - Set the logging level of the child loggers to `ERROR` and ensure they propagate events to the parent \\"webApp\\". 7. **Filters:** - Implement a custom filter that blocks all log messages containing the word \\"sensitive\\". - Apply this filter to the \\"webApp\\" logger. Constraints: - The `FileHandler` should only log messages of levels `WARNING` and above. - The `StreamHandler` should log all messages. # Expected Output: Write a script that initializes the logging system as described above and logs a few messages at various levels to demonstrate the setup. Ensure to showcase the filtering of \\"sensitive\\" log messages. ```python import logging # Create the main logger logger = logging.getLogger(\'webApp\') logger.setLevel(logging.DEBUG) # Create console handler and set level to debug ch = logging.StreamHandler() ch.setLevel(logging.DEBUG) # Create file handler and set level to warning fh = logging.FileHandler(\'webapp.log\', mode=\'a\') fh.setLevel(logging.WARNING) # Create custom formatter formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') # Add formatter to handlers ch.setFormatter(formatter) fh.setFormatter(formatter) # Add handlers to logger logger.addHandler(ch) logger.addHandler(fh) # Custom filter to block \\"sensitive\\" class SensitiveFilter(logging.Filter): def filter(self, record): return \'sensitive\' not in record.getMessage() # Add filter to logger logger.addFilter(SensitiveFilter()) # Create child loggers db_logger = logging.getLogger(\'webApp.database\') db_logger.setLevel(logging.ERROR) network_logger = logging.getLogger(\'webApp.network\') network_logger.setLevel(logging.ERROR) # Logging messages for demonstration logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') # Log a message with sensitive content logger.info(\'This message contains sensitive information\') # Additional contextual information in log messages extra = {\'userId\': \'12345\', \'requestId\': \'67890\'} logger.info(\'User operation performed\', extra=extra) # Child logger messages db_logger.error(\'Database connection failed\') network_logger.error(\'Network timeout occurred\') ``` Ensure your script adheres to the requirements and constraints. The expected behavior should involve both console and file logging, appropriate message levels, contextual logging, and the sensitive message filter effectiveness.","solution":"import logging # Create the main logger logger = logging.getLogger(\'webApp\') logger.setLevel(logging.DEBUG) # Create console handler and set level to debug ch = logging.StreamHandler() ch.setLevel(logging.DEBUG) # Create file handler and set level to warning fh = logging.FileHandler(\'webapp.log\', mode=\'a\') fh.setLevel(logging.WARNING) # Create custom formatter formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') # Add formatter to handlers ch.setFormatter(formatter) fh.setFormatter(formatter) # Add handlers to logger logger.addHandler(ch) logger.addHandler(fh) # Custom filter to block \\"sensitive\\" class SensitiveFilter(logging.Filter): def filter(self, record): return \'sensitive\' not in record.getMessage() # Add filter to logger logger.addFilter(SensitiveFilter()) # Create child loggers db_logger = logging.getLogger(\'webApp.database\') db_logger.setLevel(logging.ERROR) network_logger = logging.getLogger(\'webApp.network\') network_logger.setLevel(logging.ERROR) # Logging messages for demonstration logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') # Log a message with sensitive content logger.info(\'This message contains sensitive information\') # Additional contextual information in log messages extra = {\'userId\': \'12345\', \'requestId\': \'67890\'} logger.info(\'User operation performed\', extra=extra) # Child logger messages db_logger.error(\'Database connection failed\') network_logger.error(\'Network timeout occurred\')"},{"question":"# Task: Implement a TorchScript Class with Type Annotations and Methods Objective In this task, you will create a custom TorchScript class that models a simple neural network module. The class must include: 1. Instance attributes with proper type annotations. 2. Methods annotated with type hints. 3. Use of TorchScript-specific features such as `torch.jit.script`. Problem Statement Implement a TorchScript class `MyNeuralNet` using the `torch.nn.Module` as the base class. The `MyNeuralNet` class should meet the following requirements: 1. **Initialization**: - The class should accept two parameters: `input_dim` (an integer) and `output_dim` (an integer). - It should initialize two linear layers, `layer1` and `layer2`, using `torch.nn.Linear`. 2. **Forward Method**: - Implement the `forward` method that takes a `torch.Tensor` as input and passes it through `layer1` followed by `layer2`. - The method should return the output of `layer2`. 3. **Additional Method**: - Implement a method `get_layer_weights` that accepts a string parameter `layer_name` and returns the weights of the specified layer (`layer1` or `layer2`) as a `torch.Tensor`. - Use the `torch.jit.export` decorator to allow this method to be called from TorchScript. The class definition must include type annotations for all attributes and methods. Input - `input_dim`: an integer specifying the input dimension. - `output_dim`: an integer specifying the output dimension. Output - `forward(x: torch.Tensor) -> torch.Tensor`: a method that returns the output of the neural network. - `get_layer_weights(layer_name: str) -> torch.Tensor`: a method that returns the weights of the specified layer. Constraints - `input_dim` and `output_dim` are positive integers. - `layer_name` must be either `\\"layer1\\"` or `\\"layer2\\"`. Example Usage ```python import torch from torch.jit import script class MyNeuralNet(torch.nn.Module): def __init__(self, input_dim: int, output_dim: int): super().__init__() self.layer1 = torch.nn.Linear(input_dim, 10) # Hidden layer with 10 units self.layer2 = torch.nn.Linear(10, output_dim) # Output layer def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) return x @torch.jit.export def get_layer_weights(self, layer_name: str) -> torch.Tensor: if layer_name == \\"layer1\\": return self.layer1.weight elif layer_name == \\"layer2\\": return self.layer2.weight else: raise ValueError(f\\"Layer name {layer_name} is not valid. Use \'layer1\' or \'layer2\'.\\") # Example Script input_dim = 5 output_dim = 2 model = MyNeuralNet(input_dim, output_dim) scripted_model = torch.jit.script(model) # Test forward method x = torch.randn(1, input_dim) output = scripted_model(x) print(output) # Test get_layer_weights method weights_layer1 = scripted_model.get_layer_weights(\\"layer1\\") print(weights_layer1) weights_layer2 = scripted_model.get_layer_weights(\\"layer2\\") print(weights_layer2) ``` Ensure your implemented class passes the above example usage. Notes - Use the provided example usage to verify the correctness of your implementation. - Handle any errors or invalid inputs gracefully.","solution":"import torch from torch.jit import script class MyNeuralNet(torch.nn.Module): def __init__(self, input_dim: int, output_dim: int): super(MyNeuralNet, self).__init__() self.layer1 = torch.nn.Linear(input_dim, 10) # Hidden layer with 10 units self.layer2 = torch.nn.Linear(10, output_dim) # Output layer def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) return x @torch.jit.export def get_layer_weights(self, layer_name: str) -> torch.Tensor: if layer_name == \\"layer1\\": return self.layer1.weight elif layer_name == \\"layer2\\": return self.layer2.weight else: raise ValueError(f\\"Layer name {layer_name} is not valid. Use \'layer1\' or \'layer2\'.\\") # Example Script input_dim = 5 output_dim = 2 model = MyNeuralNet(input_dim, output_dim) scripted_model = torch.jit.script(model) # Test forward method x = torch.randn(1, input_dim) output = scripted_model(x) print(output) # Test get_layer_weights method weights_layer1 = scripted_model.get_layer_weights(\\"layer1\\") print(weights_layer1) weights_layer2 = scripted_model.get_layer_weights(\\"layer2\\") print(weights_layer2)"},{"question":"Objective The goal of this task is to assess your understanding of PyTorchâ€™s TorchInductor GPU profiling tools and your ability to effectively utilize them to analyze and optimize the performance of a model. Description You are provided with a PyTorch model and are required to perform GPU profiling using TorchInductor to identify performance bottlenecks. Follow the steps outlined below to set up the environment, run benchmarks, and generate profiling logs. Finally, analyze the performance data and identify which kernels are taking the most GPU time. Steps 1. **Setting up the Environment:** - Enable the following environment variables: - `TORCHINDUCTOR_UNIQUE_KERNEL_NAMES` - `TORCHINDUCTOR_BENCHMARK_KERNEL` - These environment variables are crucial for generating meaningful kernel names and benchmarking individual triton kernels. 2. **Running the Benchmark Script:** - Use the provided model script (`model_script.py`) which defines and trains a PyTorch model. The script should be run with the following command: ```bash TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1 python -u model_script.py --backend inductor --amp --performance --dashboard --disable-cudagraphs --training ``` 3. **Analyzing the Output Logs:** - After running the benchmark, the output log will contain lines indicating the compiled module paths. Extract these paths and identify the ones related to the forward and backward graphs. - Further, run the compiled module for the forward graph using the `-p` argument: ```bash python <compiled_module_path> -p ``` 4. **Detailed Analysis:** - The profiling log will contain details about GPU time breakdown for various kernel categories. Identify the kernels that take the most GPU time. - For the most expensive kernel, extract the corresponding kernel file and run it to get individual execution time and bandwidth: ```bash TORCHINDUCTOR_MAX_AUTOTUNE=1 python <kernel_file_path> ``` Expected Outputs You are required to submit the following: 1. **Environment Setup Code:** A script to set the appropriate environment variables. 2. **Benchmarking Output:** The console output of running the benchmark, showing the GPU profiling log. 3. **Performance Analysis Report:** A brief report identifying which kernels take the most GPU time, including the execution time and bandwidth of the most expensive kernel. Constraints - Ensure you use the provided environment variable settings to generate meaningful kernel names and to benchmark individual kernels. - The output analysis must summarize kernel performance and identify potential areas for optimization. Example Here is a basic example of the expected output: **Environment Setup Code:** ```python import os os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' # Now continue with the script execution... ``` **Benchmarking Output:** ``` Compiled module path: /tmp/torchinductor_shunting/qz/cqz7hvhood7y3psp7fy6msjxsxyli7qiwiybizdwtjw6ffyq5wwd.py ... Percent of time when GPU is busy: 82.45% ... pointwise kernel takes 25.14% reduction kernel takes 18.56% ... ``` **Performance Analysis Report:** ``` - The pointwise kernel takes the most GPU time at 25.14%. - The most expensive kernel is: triton_red_fused__native_batch_norm_legit_functional_16 - Execution time: 0.23 ms - Bandwidth: 45.6 GB/s - Potential for optimization: Focus on optimizing the pointwise kernel to improve overall performance. ``` Complete the task as outlined and submit the required outputs for evaluation.","solution":"import os def setup_environment(): Set the appropriate environment variables for GPU profiling with TorchInductor. os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' print(\\"Environment variables set for TorchInductor GPU profiling.\\") setup_environment() # Assuming model_script.py is present and imports are correctly defined # Also assuming an available GPU for running the following shell command def run_benchmark(): Function to execute the benchmark script with necessary arguments. command = (\\"TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1 \\" \\"python -u model_script.py --backend inductor --amp --performance \\" \\"--dashboard --disable-cudagraphs --training\\") os.system(command) print(\\"Benchmark script executed with the required environment variables set.\\") run_benchmark() # Output Analysis function to be used after the actual outputs are obtained (Placeholder) def analyze_output(output_log): Parse benchmark log to identify which kernels take most GPU time. compiled_module_path = None most_exensive_kernel_info = None for line in output_log.split(\\"n\\"): if \\"Compiled module path:\\" in line: compiled_module_path = line.split(\\":\\")[1].strip() if \\"takes\\" in line: # Example: pointwise kernel takes 25.14% kernel_type, percentage = line.split(\\"kernel takes\\") percentage = float(percentage.strip().strip(\'%\')) if most_exensive_kernel_info is None or percentage > most_exensive_kernel_info[\'percentage\']: most_exensive_kernel_info = {\'kernel_type\': kernel_type.strip(), \'percentage\': percentage} if compiled_module_path: # Analyze the compiled module for detailed profiling profiling_command = f\\"python {compiled_module_path} -p\\" os.system(profiling_command) print(\\"Profiled the compiled module to get detailed GPU time breakdown.\\") return most_exensive_kernel_info # Placeholder output log for unit test purposes example_output_log = Compiled module path: /tmp/torchinductor_shunting/qz/cqz7hvhood7y3psp7fy6msjxsxyli7qiwiybizdwtjw6ffyq5wwd.py ... Percent of time when GPU is busy: 82.45% ... pointwise kernel takes 25.14% reduction kernel takes 18.56% ... # Analyze example output log (For real use, replace example_output_log with actual log content) analysis_result = analyze_output(example_output_log) print(f\\"Most expensive kernel details: {analysis_result}\\")"},{"question":"# Asynchronous Web Scraper with asyncio Problem Statement: You are tasked with creating an asynchronous web scraper using the `asyncio` package. Your scraper should perform the following tasks: 1. Fetch the HTML content from multiple URLs concurrently. 2. Parse the HTML to extract specific information (for this example, let\'s assume you need to fetch the title of the webpage). 3. Store the extracted information in a thread-safe data structure. Requirements: 1. **URL Fetching:** - Implement an asynchronous function `fetch_url` which takes a URL and uses an HTTP library (such as aiohttp) to fetch the content of the URL. - Handle potential timeouts and exceptions gracefully, utilizing `asyncio`\'s exception handling. 2. **HTML Parsing:** - Implement a function `parse_html` which takes the HTML content and extracts the title of the webpage using an HTML parser (such as BeautifulSoup). 3. **Concurrency:** - Use `asyncio.gather` to fetch multiple URLs concurrently. - Implement a thread-safe queue using `asyncio.Queue` to manage the URLs to be fetched. 4. **Thread-Safe Storage:** - Store the titles in a thread-safe data structure, such as an `asyncio.Queue`, `asyncio.Lock`, or any other suitable synchronization mechanism. Input Format: - A list of URLs (strings) that you need to fetch. Output Format: - A dictionary where the keys are the URLs and the values are the titles of the corresponding webpages. Constraints: 1. You should handle a maximum of 10 URLs concurrently. 2. Each fetch request should timeout after 10 seconds if the URL is not responding. Example: ```python urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\", ] result = await scrape_titles(urls) ``` Output: ```python { \\"http://example.com\\": \\"Example Domain\\", \\"http://example.org\\": \\"Example Domain\\", \\"http://example.net\\": \\"Example Domain\\" } ``` Solution Template: ```python import aiohttp import asyncio from bs4 import BeautifulSoup async def fetch_url(session, url): try: async with session.get(url, timeout=10) as response: return await response.text() except asyncio.TimeoutError: print(f\\"Timeout while fetching {url}\\") except Exception as e: print(f\\"Error while fetching {url}: {e}\\") return None def parse_html(html): # Implement your HTML parsing logic to extract the title soup = BeautifulSoup(html, \'html.parser\') return soup.title.string if soup.title else \'No title found\' async def scrape_titles(urls): async with aiohttp.ClientSession() as session: tasks = [] for url in urls: tasks.append(fetch_url(session, url)) html_contents = await asyncio.gather(*tasks) result = {} for url, html in zip(urls, html_contents): if html: result[url] = parse_html(html) return result # Example usage urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\", ] # Note: This code should be run in an event loop to work properly # asyncio.run(scrape_titles(urls)) ``` **Note**: This template provides a starting point. Further implementation is required to handle concurrency, timeouts, and storing results in a thread-safe manner.","solution":"import aiohttp import asyncio from bs4 import BeautifulSoup async def fetch_url(session, url): try: async with session.get(url, timeout=10) as response: return await response.text() except asyncio.TimeoutError: print(f\\"Timeout while fetching {url}\\") except Exception as e: print(f\\"Error while fetching {url}: {e}\\") return None def parse_html(html): soup = BeautifulSoup(html, \'html.parser\') return soup.title.string if soup.title else \'No title found\' async def scrape_titles(urls): async with aiohttp.ClientSession() as session: tasks = [] for url in urls: tasks.append(fetch_url(session, url)) html_contents = await asyncio.gather(*tasks) result = {} for url, html in zip(urls, html_contents): if html: result[url] = parse_html(html) return result # Example usage urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\", ] # Note: This code should be run in an event loop to work properly # asyncio.run(scrape_titles(urls))"},{"question":"# Question: Implement a Python Module to Manage System Commands You are required to create a Python module named `sys_manager.py` that provides utility functions to execute system commands and handle their outputs and errors effectively. Your task is to implement the following functions using the `subprocess` module, ensuring you handle different scenarios and exceptions appropriately. Function 1: `run_command` This function will run a given system command and return its output. **Signature:** ```python def run_command(command: str) -> str: Runs the specified command using the system shell and returns the standard output (stdout). Args: command (str): The system command to be executed. Returns: str: The standard output (stdout) from the command execution. Raises: subprocess.CalledProcessError: If the command exits with a non-zero status. subprocess.TimeoutExpired: If the command execution exceeds the timeout period. ``` **Requirements:** - Use `subprocess.run()` to execute the command. - Capture and return the command\'s standard output (`stdout`). - Raise `subprocess.CalledProcessError` if the command exits with a non-zero status. - Raise `subprocess.TimeoutExpired` if the command execution exceeds 10 seconds. Function 2: `run_command_with_input` This function will run a given system command, send specific input to it, and return its output. **Signature:** ```python def run_command_with_input(command: str, input_data: str) -> str: Runs the specified command, sends input data to it, and returns the standard output (stdout). Args: command (str): The system command to be executed. input_data (str): The input data to be sent to the command. Returns: str: The standard output (stdout) from the command execution. Raises: subprocess.CalledProcessError: If the command exits with a non-zero status. subprocess.TimeoutExpired: If the command execution exceeds the timeout period. ``` **Requirements:** - Use `subprocess.run()` to execute the command. - Send the provided `input_data` to the command. - Capture and return the command\'s standard output (`stdout`). - Raise `subprocess.CalledProcessError` if the command exits with a non-zero status. - Raise `subprocess.TimeoutExpired` if the command execution exceeds 5 seconds. Function 3: `run_command_with_pipe` This function will run two given system commands and pipe the output of the first command to the input of the second command, returning the final output. **Signature:** ```python def run_command_with_pipe(command1: str, command2: str) -> str: Runs two specified commands and pipes the output of the first command to the input of the second command. Args: command1 (str): The first system command to be executed. command2 (str): The second system command to be executed. Returns: str: The standard output (stdout) from the second command execution. Raises: subprocess.CalledProcessError: If either command exits with a non-zero status. subprocess.TimeoutExpired: If either command execution exceeds the timeout period. ``` **Requirements:** - Use `subprocess.Popen` to execute `command1` and `command2`. - Pipe the output of `command1` to the input of `command2`. - Capture and return the standard output (`stdout`) of `command2`. - Raise `subprocess.CalledProcessError` if either command exits with a non-zero status. - Raise `subprocess.TimeoutExpired` if either command execution exceeds 10 seconds in total. Example Usage: ```python if __name__ == \\"__main__\\": print(run_command(\\"echo \'Hello World\'\\")) print(run_command_with_input(\\"grep \'Hello\'\\", \\"Hello WorldnGoodbye World\\")) print(run_command_with_pipe(\\"echo \'Hello World\'\\", \\"grep \'Hello\'\\")) ``` # Constraints: - Only use the `subprocess` module for system command executions. - Ensure your implementations are efficient and handle exceptions gracefully. - Write clean and readable code with appropriate comments and docstrings.","solution":"import subprocess def run_command(command: str) -> str: Runs the specified command using the system shell and returns the standard output (stdout). Args: command (str): The system command to be executed. Returns: str: The standard output (stdout) from the command execution. Raises: subprocess.CalledProcessError: If the command exits with a non-zero status. subprocess.TimeoutExpired: If the command execution exceeds the timeout period. try: result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True, timeout=10) return result.stdout except subprocess.CalledProcessError as e: raise e except subprocess.TimeoutExpired as e: raise e def run_command_with_input(command: str, input_data: str) -> str: Runs the specified command, sends input data to it, and returns the standard output (stdout). Args: command (str): The system command to be executed. input_data (str): The input data to be sent to the command. Returns: str: The standard output (stdout) from the command execution. Raises: subprocess.CalledProcessError: If the command exits with a non-zero status. subprocess.TimeoutExpired: If the command execution exceeds the timeout period. try: result = subprocess.run(command, input=input_data, shell=True, check=True, capture_output=True, text=True, timeout=5) return result.stdout except subprocess.CalledProcessError as e: raise e except subprocess.TimeoutExpired as e: raise e def run_command_with_pipe(command1: str, command2: str) -> str: Runs two specified commands and pipes the output of the first command to the input of the second command. Args: command1 (str): The first system command to be executed. command2 (str): The second system command to be executed. Returns: str: The standard output (stdout) from the second command execution. Raises: subprocess.CalledProcessError: If either command exits with a non-zero status. subprocess.TimeoutExpired: If either command execution exceeds the timeout period. try: p1 = subprocess.Popen(command1, shell=True, stdout=subprocess.PIPE, text=True) p2 = subprocess.Popen(command2, shell=True, stdin=p1.stdout, stdout=subprocess.PIPE, text=True) p1.stdout.close() # Allow p1 to receive a SIGPIPE if p2 exits. output, _ = p2.communicate(timeout=10) p1.wait() if p1.returncode != 0: raise subprocess.CalledProcessError(p1.returncode, command1) if p2.returncode != 0: raise subprocess.CalledProcessError(p2.returncode, command2) return output except subprocess.CalledProcessError as e: raise e except subprocess.TimeoutExpired as e: raise e"},{"question":"Cross-Platform Asynchronous Task Manager You are tasked with creating a cross-platform asynchronous task manager using Python\'s `asyncio` module. The task manager must be able to: 1. Schedule and wait for completion of multiple asynchronous tasks. 2. Handle cross-platform differences in the event loop implementations. # Requirements: 1. **Task Scheduling and Execution**: - Implement a function `schedule_tasks(task_list)` that receives a list of tasks (coroutines) and runs them concurrently. - Each task should simply wait for a random amount of time between 1 and 5 seconds and then return the elapsed time. 2. **Cross-Platform Compatibility**: - On Windows, use the default event loop `ProactorEventLoop`. - On macOS (modern versions), use the default event loop. - For older macOS versions (<= 10.8), configure the `SelectorEventLoop` with `SelectSelector`. 3. **Error Handling**: - Implement appropriate error handling for unsupported operations on both platforms. - Use try-except blocks to catch and handle any exceptions that might arise due to platform-specific limitations. 4. **Output**: - The function should return a list of the results from the completed tasks. # Input & Output: - **Input**: A list of `task_list` where each task is an asyncio coroutine. - **Output**: A list of integers representing the elapsed time for each task. # Constraints: - You cannot use any methods not supported on the respective platforms. - Ensure the solution handles at least 5 tasks concurrently. # Example Usage: ```python import asyncio import random async def sample_task(): await asyncio.sleep(random.randint(1, 5)) return random.randint(1, 5) tasks = [sample_task() for _ in range(5)] results = asyncio.run(schedule_tasks(tasks)) print(results) ``` *Note*: Use the provided documentation snippet to ensure your solution adheres to platform-specific constraints and capabilities. # Implementation ```python import asyncio import random import selectors import platform async def sample_task(): await asyncio.sleep(random.randint(1, 5)) return random.randint(1, 5) def configure_event_loop(): sys_platform = platform.system() if sys_platform == \'Windows\': asyncio.set_event_loop(asyncio.ProactorEventLoop()) elif sys_platform == \'Darwin\': major_version = int(platform.release().split(\'.\')[0]) if major_version <= 12: selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) asyncio.set_event_loop(loop) # For all other platforms, default event loop is used async def schedule_tasks(task_list): configure_event_loop() results = await asyncio.gather(*task_list, return_exceptions=True) return results tasks = [sample_task() for _ in range(5)] if __name__ == \\"__main__\\": results = asyncio.run(schedule_tasks(tasks)) print(results) ``` Ensure your implementation adheres to the platform constraints to avoid unsupported operation errors.","solution":"import asyncio import random import selectors import platform async def sample_task(): await asyncio.sleep(random.randint(1, 5)) return random.randint(1, 5) def configure_event_loop(): sys_platform = platform.system() if sys_platform == \'Windows\': asyncio.set_event_loop(asyncio.ProactorEventLoop()) elif sys_platform == \'Darwin\': major_version = int(platform.release().split(\'.\')[0]) if major_version <= 12: selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) asyncio.set_event_loop(loop) # For all other platforms, default event loop is used async def schedule_tasks(task_list): configure_event_loop() results = await asyncio.gather(*task_list, return_exceptions=True) return results # Sample usage if __name__ == \\"__main__\\": tasks = [sample_task() for _ in range(5)] results = asyncio.run(schedule_tasks(tasks)) print(results)"},{"question":"# Signal Processing with PyTorch In this programming task, you will utilize the `torch.signal.windows` submodule to implement a function that preprocesses a signal for spectral analysis. Your function will apply a specified window function to the input signal, compute the Discrete Fourier Transform (DFT) of the windowed signal, and return the magnitude spectrum. Function Signature ```python import torch def windowed_signal_spectrum(signal: torch.Tensor, window_type: str, *args) -> torch.Tensor: Applies a window function to the input signal, computes the DFT, and returns the magnitude spectrum. Parameters: - signal (torch.Tensor): A 1D tensor representing the input signal. - window_type (str): The type of window function to apply. One of [\\"bartlett\\", \\"blackman\\", \\"cosine\\", \\"exponential\\", \\"gaussian\\", \\"general_cosine\\", \\"general_hamming\\", \\"hamming\\", \\"hann\\", \\"kaiser\\", \\"nuttall\\"]. - *args: Additional arguments required by the specified window function (optional). Returns: - torch.Tensor: A 1D tensor representing the magnitude spectrum of the windowed signal. pass ``` Constraints 1. The input signal will always be a one-dimensional tensor with at least 32 elements. 2. The `window_type` parameter will always be a valid window function name. 3. The `*args` parameters are optional and specific to the window function chosen. For example, the Kaiser window requires an additional beta parameter. Example ```python signal = torch.tensor([0.0, 0.1, 0.3, 0.4, 0.0, -0.1, -0.3, -0.4, 0.0, 0.1, 0.3, 0.4, 0.0, -0.1, -0.3, -0.4, 0.0, 0.1, 0.3, 0.4, 0.0, -0.1, -0.3, -0.4, 0.0, 0.1, 0.3, 0.4, 0.0, -0.1, -0.3, -0.4]) # Applying Hamming window result = windowed_signal_spectrum(signal, \\"hamming\\") # Applying Gaussian window with std=7 result2 = windowed_signal_spectrum(signal, \\"gaussian\\", 7) print(result) print(result2) ``` Notes - Use the appropriate window function from the `torch.signal.windows` submodule based on the `window_type` parameter. - Compute the DFT using PyTorch\'s FFT functions. - The magnitude spectrum is obtained by taking the absolute value of the complex result from the DFT. Here is the skeleton of the function you need to complete: ```python import torch from torch.fft import fft import torch.signal.windows as windows def windowed_signal_spectrum(signal: torch.Tensor, window_type: str, *args) -> torch.Tensor: window_function = getattr(windows, window_type) window = window_function(len(signal), *args) windowed_signal = signal * window spectrum = fft(windowed_signal) magnitude_spectrum = torch.abs(spectrum) return magnitude_spectrum ``` Ensure your implementation correctly handles different window functions and any additional arguments required by them.","solution":"import torch from torch.fft import fft import scipy.signal.windows as windows def windowed_signal_spectrum(signal: torch.Tensor, window_type: str, *args) -> torch.Tensor: Applies a window function to the input signal, computes the DFT, and returns the magnitude spectrum. Parameters: - signal (torch.Tensor): A 1D tensor representing the input signal. - window_type (str): The type of window function to apply. One of [\\"bartlett\\", \\"blackman\\", \\"cosine\\", \\"exponential\\", \\"gaussian\\", \\"general_cosine\\", \\"general_hamming\\", \\"hamming\\", \\"hann\\", \\"kaiser\\", \\"nuttall\\"]. - *args: Additional arguments required by the specified window function (optional). Returns: - torch.Tensor: A 1D tensor representing the magnitude spectrum of the windowed signal. # Get the desired window function with the appropriate arguments window_function = getattr(windows, window_type) window = torch.tensor(window_function(len(signal), *args), dtype=torch.float32) windowed_signal = signal * window spectrum = fft(windowed_signal) magnitude_spectrum = torch.abs(spectrum) return magnitude_spectrum"},{"question":"**Question: Seaborn Error Bar Analysis** You are provided with a dataset containing students\' marks in three different subjects (Mathematics, Physics, and Chemistry). Your task is to visualize the data using seaborn by creating a series of plots that display the following: # Dataset The dataset is a CSV file `students_marks.csv` with the following columns: - `id`: Unique identifier for each student. - `Mathematics`: Marks obtained in Mathematics. - `Physics`: Marks obtained in Physics. - `Chemistry`: Marks obtained in Chemistry. # Tasks 1. **Data Preparation** - Load the dataset into a pandas DataFrame. - Display the first five rows of the DataFrame to understand its structure. 2. **Summary Statistics with Error Bars** - Create a plot that shows the mean and standard deviation of marks for each subject. Use different colors for each subject. 3. **Estimate Uncertainty with Confidence Intervals** - Create a plot that displays the mean marks for each subject with 95% confidence intervals using bootstrapping. 4. **Percentile Intervals for Data Spread** - Create a plot showing the median marks for each subject with a 50% percentile interval. 5. **Regression Analysis with Error Bands** - Perform regression analysis to visualize the relationship between marks in Mathematics and Physics. Display a regression line with a confidence interval band. # Requirements - Use seaborn for plotting. - Clearly label the axes and add titles to each plot. - Ensure that the figures are well-organized and properly formatted. # Example Code Structure ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset df = pd.read_csv(\'students_marks.csv\') # Display the first five rows of the DataFrame print(df.head()) # Task 2: Mean and Standard Deviation plt.figure(figsize=(10, 6)) sns.pointplot(data=df[[\'Mathematics\', \'Physics\', \'Chemistry\']], errorbar=\'sd\') plt.title(\'Mean and Standard Deviation of Marks\') plt.show() # Task 3: Mean with 95% Confidence Intervals plt.figure(figsize=(10, 6)) sns.pointplot(data=df[[\'Mathematics\', \'Physics\', \'Chemistry\']], errorbar=\'ci\', n_boot=1000) plt.title(\'Mean Marks with 95% Confidence Intervals\') plt.show() # Task 4: Median with 50% Percentile Interval plt.figure(figsize=(10, 6)) sns.pointplot(data=df[[\'Mathematics\', \'Physics\', \'Chemistry\']], estimator=\'median\', errorbar=(\'pi\', 50)) plt.title(\'Median Marks with 50% Percentile Interval\') plt.show() # Task 5: Regression Analysis plt.figure(figsize=(10, 6)) sns.regplot(x=\'Mathematics\', y=\'Physics\', data=df, ci=95) plt.title(\'Regression Analysis: Mathematics vs Physics\') plt.show() ``` # Input - A CSV file named `students_marks.csv` with columns `id`, `Mathematics`, `Physics`, and `Chemistry`. # Output - Four plots displaying the required analysis. **Constraints** - Use the seaborn library for all visualizations. - Ensure that the error bars are properly formatted and display the required statistical measures. - The dataset should be handled properly, ensuring any necessary data cleaning steps are taken. Good luck, and happy coding!","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_data(file_path): Load the dataset into a pandas DataFrame. Parameters: file_path (str): The path to the CSV file. Returns: pd.DataFrame: The loaded DataFrame. return pd.read_csv(file_path) def display_first_five_rows(df): Display the first five rows of the DataFrame. Parameters: df (pd.DataFrame): The DataFrame to display. print(df.head()) def plot_mean_sd(df): Create a plot that shows the mean and standard deviation of marks for each subject. Parameters: df (pd.DataFrame): The DataFrame containing the marks. plt.figure(figsize=(10, 6)) sns.pointplot(data=df[[\'Mathematics\', \'Physics\', \'Chemistry\']].melt(), x=\'variable\', y=\'value\', errorbar=\'sd\') plt.title(\'Mean and Standard Deviation of Marks\') plt.xlabel(\'Subjects\') plt.ylabel(\'Marks\') plt.show() def plot_mean_ci(df): Create a plot that displays the mean marks for each subject with 95% confidence intervals using bootstrapping. Parameters: df (pd.DataFrame): The DataFrame containing the marks. plt.figure(figsize=(10, 6)) sns.pointplot(data=df[[\'Mathematics\', \'Physics\', \'Chemistry\']].melt(), x=\'variable\', y=\'value\', errorbar=\'ci\', n_boot=1000) plt.title(\'Mean Marks with 95% Confidence Intervals\') plt.xlabel(\'Subjects\') plt.ylabel(\'Marks\') plt.show() def plot_median_percentile(df): Create a plot showing the median marks for each subject with a 50% percentile interval. Parameters: df (pd.DataFrame): The DataFrame containing the marks. plt.figure(figsize=(10, 6)) sns.pointplot(data=df[[\'Mathematics\', \'Physics\', \'Chemistry\']].melt(), x=\'variable\', y=\'value\', estimator=\'median\', errorbar=(\'pi\', 50)) plt.title(\'Median Marks with 50% Percentile Interval\') plt.xlabel(\'Subjects\') plt.ylabel(\'Marks\') plt.show() def plot_regression_analysis(df): Perform regression analysis to visualize the relationship between marks in Mathematics and Physics. Parameters: df (pd.DataFrame): The DataFrame containing the marks. plt.figure(figsize=(10, 6)) sns.regplot(x=\'Mathematics\', y=\'Physics\', data=df, ci=95) plt.title(\'Regression Analysis: Mathematics vs Physics\') plt.xlabel(\'Mathematics Marks\') plt.ylabel(\'Physics Marks\') plt.show() # Example usage: if __name__ == \\"__main__\\": df = load_data(\'students_marks.csv\') display_first_five_rows(df) plot_mean_sd(df) plot_mean_ci(df) plot_median_percentile(df) plot_regression_analysis(df)"},{"question":"# Distributed Training with PyTorch: Implementing a Simple Distributed Data-Parallel Model **Objective:** Your task is to implement a basic distributed data-parallel model using the `torch.distributed` package. This will involve initializing the process group, defining a simple neural network model, applying a collective operation, and ensuring proper synchronization and cleanup. **Problem Statement:** You are given a simple feed-forward neural network. The neural network will be trained in a distributed manner across multiple processes using the NCCL backend for GPU training. You need to implement the following: 1. **Initialization:** - Initialize the process group using the NCCL backend. - Set up the environment variables (`MASTER_ADDR`, `MASTER_PORT`, `WORLD_SIZE`, `RANK`). 2. **Model Definition:** - Define a simple feed-forward neural network using `torch.nn.Module`. 3. **Training Loop:** - Implement a basic training loop that performs forward and backward passes and updates model parameters. - Use the `all_reduce` collective operation to average the gradients across all processes. 4. **Synchronization and Cleanup:** - Ensure that all processes synchronize at the end of each epoch using `barrier`. - Properly clean up resources by calling `destroy_process_group`. **Constraints:** - The training should be performed on GPUs. - The code should be able to run with any specified number of processes. **Input:** - `rank` (int): The rank of the current process. - `world_size` (int): Total number of processes. - `master_addr` (str): Address of the master node. - `master_port` (str): Port of the master node. **Output:** - No explicit output required. The correctness will be evaluated based on successful distributed training and proper synchronization. **Sample Code Structure:** ```python import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.multiprocessing import spawn # 1. Environment variables setup def setup(rank, world_size, master_addr=\'localhost\', master_port=\'12355\'): os.environ[\'MASTER_ADDR\'] = master_addr os.environ[\'MASTER_PORT\'] = master_port dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) torch.cuda.set_device(rank) # 2. Define a simple feed-forward neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(784, 128) self.relu = nn.ReLU() self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # 3. Training loop def train_model(rank, world_size): setup(rank, world_size) # Model, loss, and optimizer model = SimpleNN().cuda(rank) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Dummy data (for example purposes, you can change this as needed) inputs = torch.randn(32, 784).cuda(rank) targets = torch.randint(0, 10, (32,)).cuda(rank) model.train() for epoch in range(5): # Example for 5 epochs optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() # All reduce operation to average gradients for param in model.parameters(): dist.all_reduce(param.grad.data) param.grad.data /= world_size optimizer.step() # Synchronize processes dist.barrier() # Cleanup dist.destroy_process_group() # Function to spawn multiple processes def main(): world_size = 2 # Number of processes spawn(train_model, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main() ``` **Instructions:** 1. Complete the code for setting up the environment, initializing the process group, and defining the neural network. 2. Implement the training loop as described, ensuring the use of collective operations for gradient averaging. 3. Ensure proper synchronization and cleanup at the end of the training. Remember to use the NCCL backend and ensure the training runs on GPUs.","solution":"import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.multiprocessing import spawn from torch.utils.data import DataLoader, TensorDataset # 1. Environment variables setup def setup(rank, world_size, master_addr=\'localhost\', master_port=\'12355\'): os.environ[\'MASTER_ADDR\'] = master_addr os.environ[\'MASTER_PORT\'] = master_port dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) torch.cuda.set_device(rank) # 2. Define a simple feed-forward neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(784, 128) self.relu = nn.ReLU() self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # 3. Training loop def train_model(rank, world_size): setup(rank, world_size) # Model, loss, and optimizer model = SimpleNN().cuda(rank) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Dummy data (for example purposes, you can change this as needed) inputs = torch.randn(32, 784).cuda(rank) targets = torch.randint(0, 10, (32,)).cuda(rank) dataset = TensorDataset(inputs, targets) dataloader = DataLoader(dataset, batch_size=32) model.train() for epoch in range(5): # Example for 5 epochs for batch_inputs, batch_targets in dataloader: optimizer.zero_grad() outputs = model(batch_inputs) loss = criterion(outputs, batch_targets) loss.backward() # All reduce operation to average gradients for param in model.parameters(): dist.all_reduce(param.grad.data) param.grad.data /= world_size optimizer.step() # Synchronize processes dist.barrier() # Cleanup dist.destroy_process_group() # Function to spawn multiple processes def main(): world_size = 2 # Number of processes spawn(train_model, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main()"},{"question":"Coding Assessment Question # Objective The objective of this assessment is to test your understanding of the `hashlib` module and its various functionalities such as secure hashing, key derivation, and other specialized uses like personalized and salted hashing. # Problem Statement You are required to implement a function `generate_secure_hashes` that takes an input string and a dictionary of hashing configurations. The function should return a dictionary where each key is the name of the hashing algorithm and each value is the corresponding hash of the input string according to the given configurations. # Requirements 1. The function must generate hashes using at least the following algorithms: `sha256`, `sha512`, `blake2b`, `blake2s`. 2. Each algorithm\'s hash output should be according to specified parameters within the configuration, such as `digest_size`, `salt`, `key`, `personalization`, etc. 3. The function must handle both simple and more complex configurations (keyed hashing, salted hashing, etc.) # Input Format 1. **input_string**: A non-empty string that needs to be hashed. 2. **hash_configs**: A dictionary where each key is the name of a hashing algorithm (one of `sha256`, `sha512`, `blake2b`, `blake2s`) and each value is a dictionary of parameters for that algorithm. Each parameter dictionary can contain: - `digest_size`: An integer specifying the size of the output hash. - `salt`: A bytes object to be used as salt. - `key`: A bytes object to be used as a key for keyed hashing. - `person`: A bytes object for personalization. # Output Format A dictionary where each key is a hash algorithm name and each value is the corresponding hexadecimal hash string of the input data. # Constraints - The input string should be within ASCII characters. - The function should handle exceptions gracefully and return an appropriate message if an error is encountered. # Performance Requirements The code should be efficient and capable of handling reasonably large input strings and configurations without significant performance degradation. # Example ```python def generate_secure_hashes(input_string, hash_configs): # Your implementation here # Example usage configs = { \\"sha256\\": {}, \\"blake2b\\": {\\"digest_size\\": 16, \\"salt\\": b\\"random_salt\\", \\"person\\": b\\"example_person\\"}, \\"blake2s\\": {\\"digest_size\\": 10}, } input_str = \\"Hello, hashlib!\\" result = generate_secure_hashes(input_str, configs) # Example result (actual hash values will differ) # { # \\"sha256\\": \\"2c8ec2cb92223429b3b7d888c1a886f244e876cdef72e60d7c6a7ad9d4bf8a7f\\", # \\"blake2b\\": \\"96a296e1a2c37d8a5e6e3c35d79a\\", # \\"blake2s\\": \\"43567ace1a90bd\\" # } print(result) ``` **Note**: Make sure to handle different configurations and return the hexadecimal representation of the hash values.","solution":"import hashlib def generate_secure_hashes(input_string, hash_configs): Generate secure hashes for the given input string according to the specified configurations. Args: input_string (str): The input string to be hashed. hash_configs (dict): A dictionary with hashing algorithm names as keys and configuration dictionaries as values. Returns: dict: A dictionary with hashing algorithm names as keys and their corresponding hash values as hexadecimal strings. input_bytes = input_string.encode(\'ascii\') result_hashes = {} for algo, config in hash_configs.items(): if algo in {\'sha256\', \'sha512\'}: hash_func = getattr(hashlib, algo)() elif algo in {\'blake2b\', \'blake2s\'}: params = {k: v for k, v in config.items() if k in {\'digest_size\', \'key\', \'salt\', \'person\'}} hash_func = getattr(hashlib, algo)(**params) else: raise ValueError(f\\"Unsupported hashing algorithm: {algo}\\") hash_func.update(input_bytes) result_hashes[algo] = hash_func.hexdigest() return result_hashes"},{"question":"# Seaborn Advanced Jitter Plot You are provided with a dataset of penguin measurements (`penguins`). Your task is to write a function to create a jitter plot using Seaborn\'s object-oriented interface that shows the relationship between the penguin species and their body mass, while also customizing the jitter effect. **Function Signature:** ```python def advanced_jitter_plot(penguins): pass ``` **Input:** - `penguins`: A pandas DataFrame containing at least the columns `species` and `body_mass_g`. **Output:** - The function should return a Seaborn plot object with the following requirements: 1. Use Seaborn\'s object-oriented interface to create a jitter plot. 2. The x-axis should represent the `species`. 3. The y-axis should represent the `body_mass_g`. 4. Apply a jitter transform to the dots with the following specifications: - Width of jitter (relative to orientation axis spacing) should be `0.4`. - Custom jitter values on both axes with `x=100` and `y=200`. **Example Usage:** ```python import seaborn.objects as so from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") plot = advanced_jitter_plot(penguins) plot.show() ``` **Constraints:** - Assume that the `penguins` DataFrame will always contain non-null and valid data for the given columns. **Additional Information:** - You do not need to install or import Seaborn or Pandas within the function. You can assume they are already available in the environment where this function will run. - Focus on utilizing the `seaborn.objects` module, specifically the `so.Plot` and `so.Jitter` classes. **Evaluation Criteria:** - Correct use of Seaborn\'s object-oriented interface. - Proper implementation of jittering on the specified axes. - The plot should be clear and visually correct as per the specifications.","solution":"import seaborn.objects as so import pandas as pd def advanced_jitter_plot(penguins): Creates a jitter plot using Seaborn\'s object-oriented interface that shows the relationship between the penguin species and their body mass, while also customizing the jitter effect. Parameters: penguins (pd.DataFrame): DataFrame containing at least the columns `species` and `body_mass_g`. Returns: seaborn.objects.Plot: Seaborn Plot object. plot = ( so.Plot(penguins, x=\'species\', y=\'body_mass_g\') .add(so.Dot(), so.Jitter(width=0.4, x=100, y=200)) ) return plot"},{"question":"**Question: Custom Distribution Implementation in PyTorch** # Objective Implement a custom distribution class in PyTorch by extending the `torch.distributions.Distribution` class. # Description You need to create a custom probability distribution called `MyCustomDistribution`, which generates samples from a mixture of two Gaussian distributions. The distribution can be described as follows: P(X) = frac{1}{2} mathcal{N}(X; mu_1, sigma_1^2) + frac{1}{2} mathcal{N}(X; mu_2, sigma_2^2) where mathcal{N}(X; mu, sigma^2) is a normal distribution with mean mu and variance sigma^2. # Requirements 1. **Constructor:** ```python def __init__(self, mu1, sigma1, mu2, sigma2): pass ``` - Parameters: - `mu1` (float): Mean of the first Gaussian. - `sigma1` (float): Standard deviation of the first Gaussian. - `mu2` (float): Mean of the second Gaussian. - `sigma2` (float): Standard deviation of the second Gaussian. 2. **Sample Generation:** Implement a method to generate samples from this distribution. ```python def sample(self, sample_shape=torch.Size()): pass ``` - Parameters: - `sample_shape` (torch.Size): Shape of the sample to be generated. - Returns: - `sample` (torch.Tensor): Samples drawn from the custom distribution. 3. **Log Probability:** Implement a method to compute the log-probability of a given sample. ```python def log_prob(self, value): pass ``` - Parameters: - `value` (torch.Tensor): Samples for which the log-probability is to be computed. - Returns: - `log_prob` (torch.Tensor): The log-probability of each component in the value tensor. 4. **Mean:** Implement a method to compute the mean of the distribution. ```python @property def mean(self): pass ``` 5. **Variance:** Implement a method to compute the variance of the distribution. ```python @property def variance(self): pass ``` # Constraints - You must use the `torch.distributions.Normal` class to create the two Gaussian distributions. - Assume all input parameters are valid and well-formed. # Example Usage ```python dist = MyCustomDistribution(mu1=0.0, sigma1=1.0, mu2=5.0, sigma2=2.0) samples = dist.sample((1000,)) log_probs = dist.log_prob(samples) mean = dist.mean variance = dist.variance ``` # Notes: - This question tests your understanding of probability distributions in PyTorch, inheritance in Python, and the use of PyTorch\'s tensor operations. - Pay attention to efficient tensor operations to ensure your implementation is performant.","solution":"import torch from torch.distributions import Normal, Distribution class MyCustomDistribution(Distribution): def __init__(self, mu1, sigma1, mu2, sigma2): self.normal1 = Normal(mu1, sigma1) self.normal2 = Normal(mu2, sigma2) def sample(self, sample_shape=torch.Size()): # Create a mask to sample from either normal1 or normal2 mask = torch.randint(0, 2, sample_shape).bool() samples1 = self.normal1.sample(sample_shape) samples2 = self.normal2.sample(sample_shape) samples = torch.where(mask, samples1, samples2) return samples def log_prob(self, value): log_prob1 = self.normal1.log_prob(value) log_prob2 = self.normal2.log_prob(value) # Since it\'s a mixture with weights 0.5, we average the probabilities return torch.log(0.5 * torch.exp(log_prob1) + 0.5 * torch.exp(log_prob2)) @property def mean(self): return 0.5 * self.normal1.mean + 0.5 * self.normal2.mean @property def variance(self): # For two independent variables X and Y: # Var(X+Y) = Var(X) + Var(Y) + 2*Cov(X,Y) # Here Cov(X,Y) is 0 and expected for sum of mixture: # Var = mean(Var) + Var(mean) mean1, var1 = self.normal1.mean, self.normal1.variance mean2, var2 = self.normal2.mean, self.normal2.variance mean_mix = self.mean return 0.5 * (var1 + (mean1 - mean_mix)**2) + 0.5 * (var2 + (mean2 - mean_mix)**2)"},{"question":"# Question You are provided with a list of dictionaries, each representing a studentâ€™s record. Each record contains the student\'s name, age, and their grades in three subjects: Mathematics, Science, and English. Your task is to perform the following operations: 1. Sort the list of students based on their average grades in descending order. 2. Filter out students who have any subject grade below 50. 3. Extract and return a list of names of the remaining students. You must use the `operator` module functions to achieve this. Specifically, the usage of `itemgetter`, `attrgetter`, `methodcaller`, and sequence operations is required. Implement the function `process_student_records(students: List[Dict[str, Union[str, int]]]) -> List[str]` that performs the necessary operations. Input: - `students`: A list of dictionaries. Each dictionary has the following structure: ```python {\'name\': str, \'age\': int, \'Mathematics\': int, \'Science\': int, \'English\': int} ``` Output: - Returns a list of names (strings) of students who meet the criteria sorted by their average grade in descending order. Example: ```python students = [ {\'name\': \'Alice\', \'age\': 20, \'Mathematics\': 90, \'Science\': 85, \'English\': 88}, {\'name\': \'Bob\', \'age\': 22, \'Mathematics\': 45, \'Science\': 50, \'English\': 78}, {\'name\': \'Charlie\', \'age\': 23, \'Mathematics\': 70, \'Science\': 60, \'English\': 65} ] print(process_student_records(students)) ``` Output: ``` [\'Alice\', \'Charlie\'] ``` Constraints: - All values for grades are integers between 0 and 100. - The number of students, N, is such that 1 <= N <= 10^3. Notes: - You must use functions from the `operator` module where applicable. - Ensure proper error handling and validation within the function.","solution":"from typing import List, Dict, Union from operator import itemgetter def process_student_records(students: List[Dict[str, Union[str, int]]]) -> List[str]: # Filter out students with any grade below 50 in any subject filtered_students = [ student for student in students if student[\'Mathematics\'] >= 50 and student[\'Science\'] >= 50 and student[\'English\'] >= 50 ] # Calculate average grade for each student for student in filtered_students: student[\'Average\'] = (student[\'Mathematics\'] + student[\'Science\'] + student[\'English\']) / 3 # Sort the students based on their average grade in descending order sorted_students = sorted(filtered_students, key=itemgetter(\'Average\'), reverse=True) # Extract and return the list of names return [student[\'name\'] for student in sorted_students]"},{"question":"# Question: Advanced Use of Seaborn\'s `rugplot` You are provided with a dataset that contains information about various diamonds, including their characteristics and prices. Your task is to visualize insights from the dataset using Seaborn\'s plotting capabilities. Specifically, your visualization should adhere to the following requirements: 1. Load the `diamonds` dataset from Seaborn. 2. Create a scatter plot displaying the relationship between the `carat` and `price` of the diamonds. The `carat` should be on the x-axis, and `price` should be on the y-axis. 3. Overlay a `rugplot` on both the x and y axes to showcase the distribution of the points along these axes. 4. Use the `hue` parameter in both the scatter plot and rug plot to represent the `clarity` of the diamonds, thus encoding the `clarity` as a third variable. 5. Adjust the `height` of the rug dashes to 0.05 to make them taller. 6. Ensure that the rug dashes are positioned outside the axes boundaries by using the appropriate parameters. Write a function `visualize_diamond_data()` that implements the above requirements. Your function should not return anything but should display the plot. Input - None (the function should load the `diamonds` dataset internally) Output - None (the function should just display the plot) Constraints - Ensure that you use seaborn for creating the plot. - The plot should be neat and visually informative. Performance - The code should be efficient and load and handle the dataset without unnecessary overhead. ```python import seaborn as sns def visualize_diamond_data(): # Implement the function to meet the requirements stated above pass # Example usage: # visualize_diamond_data() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_diamond_data(): # Load the diamonds dataset from Seaborn diamonds = sns.load_dataset(\\"diamonds\\") # Create a scatter plot displaying the relationship between carat and price plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(x=\'carat\', y=\'price\', hue=\'clarity\', data=diamonds, palette=\\"viridis\\", legend=\'full\') # Overlay rugplot on both the x and y axes to show distribution sns.rugplot(x=\'carat\', data=diamonds, hue=\'clarity\', palette=\\"viridis\\", height=0.05, alpha=0.7, ax=scatter_plot, clip_on=False) sns.rugplot(y=\'price\', data=diamonds, hue=\'clarity\', palette=\\"viridis\\", height=0.05, alpha=0.7, ax=scatter_plot, clip_on=False) # Adjust plot aesthetics scatter_plot.margins(x=0.01, y=0.01) scatter_plot.set_title(\'Scatter plot of Carat vs Price with Clarity as Hue\') # Show the plot plt.show()"},{"question":"**Question: Managing Randomness and Ensuring Determinism in PyTorch** You are required to implement a PyTorch function that sets up an environment for reproducible results, taking into account various sources of randomness and ensuring deterministic operations wherever possible. Your function should configure the PyTorch random number generator, Python and NumPy random seeds, CUDA convolution settings, and DataLoader worker initialization. # Function Signature ```python import torch import numpy as np import random from torch.utils.data import DataLoader def setup_reproducible_environment(seed: int): Configures the environment to ensure deterministic behavior in PyTorch operations. Args: seed (int): The seed value to set for controlling randomness. Returns: DataLoader: A configured DataLoader with reproducible behavior. pass ``` # Requirements: 1. **Set PyTorch Random Seed**: Use `torch.manual_seed()` to set the seed across both CPU and CUDA devices. 2. **Set Python Random Seed**: Use `random.seed()` to set the seed for Pythonâ€™s random number generator. 3. **Set NumPy Random Seed**: Use `np.random.seed()` to set the seed for NumPy\'s random generator. 4. **Configure PyTorch for Deterministic Algorithms**: Use `torch.use_deterministic_algorithms(True)` to enforce deterministic operations. 5. **Disable CUDA Convolution Benchmarking**: Set `torch.backends.cudnn.benchmark` to `False` to ensure deterministic algorithm selection for CUDA convolutions. 6. **Set up DataLoader**: Implement a DataLoader with `worker_init_fn` to seed each worker properly, ensuring reproducibility when loading data. # Examples: ```python if __name__ == \\"__main__\\": seed = 42 data_loader = setup_reproducible_environment(seed) for batch in data_loader: # Process the batch pass ``` # Performance Constraints: - Ensure that the implementation adheres to reproducibility without significantly compromising performance. - Efficiently manage data loading with minimal overhead from random seeding. # Notes: 1. You do not need to provide any custom dataset or model within the solution. Focus on setting up the environment and configuration. 2. Use dummy data for creating the DataLoader just to validate reproducibility settings.","solution":"import torch import numpy as np import random from torch.utils.data import DataLoader, TensorDataset def setup_reproducible_environment(seed: int): Configures the environment to ensure deterministic behavior in PyTorch operations. Args: seed (int): The seed value to set for controlling randomness. Returns: DataLoader: A configured DataLoader with reproducible behavior. # Set the seed for PyTorch random number generator torch.manual_seed(seed) # If using CUDA, set the seed for all GPUs if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed) # Set the seed for Python\'s built-in random number generator random.seed(seed) # Set the seed for NumPy\'s random number generator np.random.seed(seed) # Use deterministic algorithms in PyTorch torch.use_deterministic_algorithms(True) # Disable CUDA convolved benchmarking for deterministic behavior torch.backends.cudnn.benchmark = False # Dummy dataset for creating DataLoader data = torch.randn(100, 10) # 100 samples, 10 features each targets = torch.randint(0, 2, (100, )) # 100 binary labels dataset = TensorDataset(data, targets) # Worker initialization function to ensure reproducibility with DataLoader workers def worker_init_fn(worker_id): np.random.seed(seed + worker_id) # DataLoader configuration data_loader = DataLoader(dataset, batch_size=32, shuffle=True, worker_init_fn=worker_init_fn) return data_loader"},{"question":"# Advanced Context Management with `contextvars` **Problem Statement:** You are tasked with building a robust logging system for an asynchronous web server. This system should keep track of request-specific data such as request ID and current user information without explicitly passing this data through function arguments. Instead, you will use Python\'s `contextvars` module to manage this context information. You need to implement two functions: 1. `initialize_context(request_id: str, user: str) -> None`: Initializes the context for a new request with a given request ID and user information. 2. `log_message(message: str) -> str`: Logs a message while including the current request ID and user information from the context. Additionally, you need to handle contexts properly in an asynchronous environment to ensure that different requests do not interfere with each other. **Expected Input and Output:** `initialize_context(request_id: str, user: str) -> None` - Input: - `request_id`: A string representing the unique identifier for the current request. - `user`: A string representing the username of the current user. `log_message(message: str) -> str` - Input: - `message`: A string message to be logged. - Output: - Returns a formatted string incorporating the message, request ID, and user. **Constraints:** - The functions will be tested in an asynchronous environment. Ensure that context variables do not bleed across different requests. - Use `contextvars.ContextVar` to manage the request-specific context data. **Performance Requirements:** - The implementation should be efficient and must not have significant performance overhead. **Example:** ```python import asyncio import contextvars # Context variable declarations request_id_var = contextvars.ContextVar(\'request_id\') user_var = contextvars.ContextVar(\'user\') async def example_request_workflow(request_id, user): initialize_context(request_id, user) log1 = log_message(\\"User login attempt\\") log2 = log_message(\\"User successfully logged in\\") return log1, log2 async def main(): workflows = await asyncio.gather( example_request_workflow(\\"req-123\\", \\"alice\\"), example_request_workflow(\\"req-456\\", \\"bob\\") ) for logs in workflows: print(logs) asyncio.run(main()) ``` **Expected Output:** ``` (\'User login attempt [request_id: req-123, user: alice]\', \'User successfully logged in [request_id: req-123, user: alice]\') (\'User login attempt [request_id: req-456, user: bob]\', \'User successfully logged in [request_id: req-456, user: bob]\') ``` **Note:** - Use `ContextVar.set()` to set the context variables. - Use `ContextVar.get()` to retrieve the context variables. - Handle cases where context variables are not set. In such cases, raise a `LookupError`. Implement the `initialize_context` and `log_message` functions below: ```python # Context variable declarations request_id_var = contextvars.ContextVar(\'request_id\') user_var = contextvars.ContextVar(\'user\') def initialize_context(request_id: str, user: str) -> None: # Implement initialization of context variables here def log_message(message: str) -> str: # Implement logging a message with context variables here ```","solution":"import contextvars # Context variable declarations request_id_var = contextvars.ContextVar(\'request_id\') user_var = contextvars.ContextVar(\'user\') def initialize_context(request_id: str, user: str) -> None: request_id_var.set(request_id) user_var.set(user) def log_message(message: str) -> str: request_id = request_id_var.get() user = user_var.get() return f\\"{message} [request_id: {request_id}, user: {user}]\\""},{"question":"# Question: You are provided with a dataset named `tips`, which contains the following columns: - `total_bill`: Numeric value representing the total bill amount. - `tip`: Numeric value representing the tip amount. - `sex`: Categorical value (Male/Female) representing the sex of the bill payer. - `smoker`: Categorical value (Yes/No) indicating if the payer is a smoker. - `day`: Categorical value representing the day of the week when the bill was paid. - `time`: Categorical value (Lunch/Dinner) representing the time when the bill was paid. - `size`: Numeric value representing the size of the group that paid the bill. Using seaborn and the `sns.stripplot` function, you are required to accomplish the following: 1. **Basic Visualization:** - Create a strip plot to visualize the distribution of total bill amounts (`total_bill`) across different days (`day`). 2. **Multi-dimensional Relationships:** - Modify the strip plot such that it includes the `sex` variable for additional differentiation using different colors for Male and Female. Ensure that a legend is included in the plot. 3. **Categorical Mapping and Styling:** - Create another strip plot where you visualize the total bill amounts (`total_bill`) against `day`, but include the `size` of the groups as a hue with a custom palette of your choice. Ensure that the legend is appropriate and clear. 4. **Customization and Enhancement:** - Style this plot further by setting the marker to a diamond shape, setting the jitter to `False`, using a size (`s`) parameter of 15 for the markers, and adding linewidths of 0.5. Also, set the transparency (`alpha`) of the markers to 0.2. 5. **Advanced Analysis:** - Finally, use `sns.catplot` to create a multi-faceted strip plot that shows the relationship between the total bill amounts (`total_bill`), the time of the meal (`time`), and the sex of the payer (`sex`). Each facet should represent a different day. Set the aspect ratio of the plots to 0.5 for a more compact display. # Expectations: - **Input:** The input data is preloaded from the seaborn library using `tips = sns.load_dataset(\\"tips\\")`. - **Output:** The output should be visualizations created using seaborn\'s plotting functions. - The code should be efficient and make use of seaborn\'s advanced customization features where applicable. # Constraints: - You must use seaborn functions to create the plots. - Ensure matplotlib\'s `show()` is called to render the plots. - The solution should be implemented within a Jupyter notebook or similar environment that supports inline plotting. # Bonus: - Explain briefly (in markdown cells) the choices you made for customizing the plots. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Basic Visualization def plot_basic_visualization(): plt.figure(figsize=(10, 6)) sns.stripplot(x=\\"day\\", y=\\"total_bill\\", data=tips) plt.title(\'Total Bill Amounts by Day\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Total Bill Amount\') plt.show() # 2. Multi-dimensional Relationships def plot_multidimensional_relationships(): plt.figure(figsize=(10, 6)) sns.stripplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips, dodge=True) plt.title(\'Total Bill Amounts by Day with Sex Differentiation\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Total Bill Amount\') plt.legend(title=\'Sex\') plt.show() # 3. Categorical Mapping and Styling def plot_categorical_mapping_and_styling(): plt.figure(figsize=(10, 6)) sns.stripplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"size\\", data=tips, palette=\\"coolwarm\\") plt.title(\'Total Bill Amounts by Day with Group Size Differentiation\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Total Bill Amount\') plt.legend(title=\'Group Size\') plt.show() # 4. Customization and Enhancement def plot_customization_and_enhancement(): plt.figure(figsize=(10, 6)) sns.stripplot(x=\\"day\\", y=\\"total_bill\\", data=tips, marker=\\"d\\", jitter=False, size=15, linewidth=0.5, alpha=0.2) plt.title(\'Total Bill Amounts by Day with Custom Styling\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Total Bill Amount\') plt.show() # 5. Advanced Analysis using catplot def plot_advanced_analysis(): g = sns.catplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"time\\", data=tips, kind=\\"strip\\", aspect=0.5) g.set_axis_labels(\\"Day of the Week\\", \\"Total Bill Amount\\") g.add_legend(title=\'Sex\') g.fig.suptitle(\'Total Bill Amounts with Time and Sex Differentiation\', y=1.05) plt.show()"},{"question":"# Question You are provided with a dataset and are required to implement several covariance estimation techniques using scikit-learn\'s `sklearn.covariance` module. Your task is to estimate the covariance matrix using different methods and compare their performances. Follow the steps below to complete the assignment: 1. **Load the Dataset:** Use any publicly available dataset with at least 10 features and 200 samples. For this task, you can use the `wine` dataset from `sklearn.datasets`. 2. **Preprocess the Data:** Center the data by subtracting the mean of each feature. 3. **Estimate Covariance Matrices:** - Compute the empirical covariance matrix using the `EmpiricalCovariance` class. - Compute the shrunk covariance matrix using a user-defined shrinkage coefficient (e.g., 0.1) with the `ShrunkCovariance` class. - Compute the Ledoit-Wolf shrinkage estimator using the `LedoitWolf` class. - Compute the Oracle Approximating Shrinkage estimator using the `OAS` class. - Compute the robust covariance estimator using the `MinCovDet` class. 4. **Performance Comparison:** - Display the covariance matrices computed in step 3. - Compute and display the log-likelihood of the data under each model. - Visualize the difference between the empirical covariance and other covariance matrices using a heatmap. # Requirements 1. Use appropriate functions and classes from the `sklearn.covariance` module. 2. Center the data before applying any covariance estimation technique. 3. Compare the performance of different estimators by computing the log-likelihood of the data. # Input - Load the `wine` dataset from `sklearn.datasets`. - Preprocess the data by centering it. # Output - Covariance matrices of each estimation technique. - Log-likelihood values for each covariance estimation technique. - Heatmap visualization of the covariance matrices. # Constraints - Use only the functions and classes from the `sklearn.covariance` module for covariance estimation. - Ensure the code is efficient and well-documented. ```python import numpy as np from sklearn.datasets import load_wine from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset data = load_wine() X = data.data # Step 2: Preprocess the data X_centered = X - X.mean(axis=0) # Step 3: Estimate Covariance Matrices # Empirical Covariance emp_cov = EmpiricalCovariance().fit(X_centered).covariance_ # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(X_centered).covariance_ # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf().fit(X_centered).covariance_ # Oracle Approximating Shrinkage oas_cov = OAS().fit(X_centered).covariance_ # Minimum Covariance Determinant mcd_cov = MinCovDet().fit(X_centered).covariance_ # Step 4: Performance Comparison # Log-likelihoods ll_emp = EmpiricalCovariance().fit(X_centered).score(X_centered) ll_shrunk = ShrunkCovariance(shrinkage=0.1).fit(X_centered).score(X_centered) ll_lw = LedoitWolf().fit(X_centered).score(X_centered) ll_oas = OAS().fit(X_centered).score(X_centered) ll_mcd = MinCovDet().fit(X_centered).score(X_centered) print(\\"Log-likelihoods:\\") print(\\"Empirical Covariance:\\", ll_emp) print(\\"Shrunk Covariance:\\", ll_shrunk) print(\\"Ledoit-Wolf Shrinkage:\\", ll_lw) print(\\"OAS Shrinkage:\\", ll_oas) print(\\"Minimum Covariance Determinant:\\", ll_mcd) # Heatmap visualization plt.figure(figsize=(12, 8)) sns.heatmap(emp_cov, annot=True, fmt=\'.2f\', cmap=\'viridis\', cbar=True) plt.title(\\"Empirical Covariance Matrix\\") plt.show() plt.figure(figsize=(12, 8)) sns.heatmap(shrunk_cov, annot=True, fmt=\'.2f\', cmap=\'viridis\', cbar=True) plt.title(\\"Shrunk Covariance Matrix\\") plt.show() plt.figure(figsize=(12, 8)) sns.heatmap(lw_cov, annot=True, fmt=\'.2f\', cmap=\'viridis\', cbar=True) plt.title(\\"Ledoit-Wolf Covariance Matrix\\") plt.show() plt.figure(figsize=(12, 8)) sns.heatmap(oas_cov, annot=True, fmt=\'.2f\', cmap=\'viridis\', cbar=True) plt.title(\\"OAS Covariance Matrix\\") plt.show() plt.figure(figsize=(12, 8)) sns.heatmap(mcd_cov, annot=True, fmt=\'.2f\', cmap=\'viridis\', cbar=True) plt.title(\\"MCD Covariance Matrix\\") plt.show() ``` This coding assessment tests the student\'s ability to use the scikit-learn library for advanced covariance matrix estimation techniques and to compare their performance using log-likelihood and visualizations.","solution":"import numpy as np from sklearn.datasets import load_wine from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet import seaborn as sns import matplotlib.pyplot as plt def load_and_center_data(): Loads the wine dataset and centers the features by subtracting their mean. Returns: np.ndarray: Centered dataset. data = load_wine() X = data.data X_centered = X - X.mean(axis=0) return X_centered def compute_covariance_matrices(X_centered): Computes various covariance matrices for the given centered data. Args: X_centered (np.ndarray): Centered dataset. Returns: dict: A dictionary with keys as the method names and values as the computed covariance matrices. cov_matrices = {} cov_matrices[\'Empirical\'] = EmpiricalCovariance().fit(X_centered).covariance_ cov_matrices[\'Shrunk\'] = ShrunkCovariance(shrinkage=0.1).fit(X_centered).covariance_ cov_matrices[\'LedoitWolf\'] = LedoitWolf().fit(X_centered).covariance_ cov_matrices[\'OAS\'] = OAS().fit(X_centered).covariance_ cov_matrices[\'MCD\'] = MinCovDet().fit(X_centered).covariance_ return cov_matrices def compute_log_likelihoods(X_centered, cov_matrices): Computes the log-likelihoods of the data under each covariance model. Args: X_centered (np.ndarray): Centered dataset. cov_matrices (dict): A dictionary with keys as the method names and values as the computed covariance matrices. Returns: dict: A dictionary with keys as the method names and values as the log-likelihoods. log_likelihoods = {} log_likelihoods[\'Empirical\'] = EmpiricalCovariance().fit(X_centered).score(X_centered) log_likelihoods[\'Shrunk\'] = ShrunkCovariance(shrinkage=0.1).fit(X_centered).score(X_centered) log_likelihoods[\'LedoitWolf\'] = LedoitWolf().fit(X_centered).score(X_centered) log_likelihoods[\'OAS\'] = OAS().fit(X_centered).score(X_centered) log_likelihoods[\'MCD\'] = MinCovDet().fit(X_centered).score(X_centered) return log_likelihoods def visualize_covariance_matrices(cov_matrices): Visualizes the covariance matrices using heatmaps. Args: cov_matrices (dict): A dictionary with keys as the method names and values as the computed covariance matrices. for method, cov in cov_matrices.items(): plt.figure(figsize=(12, 8)) sns.heatmap(cov, annot=False, fmt=\'.2f\', cmap=\'viridis\', cbar=True) plt.title(f\\"{method} Covariance Matrix\\") plt.show() # Execute the steps X_centered = load_and_center_data() cov_matrices = compute_covariance_matrices(X_centered) log_likelihoods = compute_log_likelihoods(X_centered, cov_matrices) visualize_covariance_matrices(cov_matrices) # Display Log-likelihood values print(\\"Log-likelihoods:\\") for method, ll in log_likelihoods.items(): print(f\\"{method} Covariance:\\", ll)"},{"question":"# Question: Implement and Utilize ModuleTracker in PyTorch Problem Statement You are given the task of implementing a simple neural network in PyTorch and leveraging the `torch.utils.module_tracker.ModuleTracker` utility to track layers within the model. Then you will use this tracking mechanism to print a user-friendly representation of the model\'s layer hierarchies. Requirements 1. Implement a simple feedforward neural network with at least three layers using PyTorch\'s `torch.nn.Module`. 2. Utilize the `torch.utils.module_tracker.ModuleTracker` to track the current position inside the module hierarchy. 3. Implement a function `track_model(model: torch.nn.Module) -> None` which: - Takes a PyTorch model as input. - Uses `ModuleTracker` to print out a user-friendly representation of the model\'s hierarchical structure. Input - Instantiate and pass the implemented neural network model to the `track_model` function. For example: ```python model = SimpleFeedForwardNet() track_model(model) ``` Output - The `track_model` function should print a user-friendly hierarchical structure of the model. Example If your model is structured as follows: ```plaintext SimpleFeedForwardNet( (fc1): Linear(in_features=784, out_features=128, bias=True) (relu1): ReLU() (fc2): Linear(in_features=128, out_features=64, bias=True) (relu2): ReLU() (fc3): Linear(in_features=64, out_features=10, bias=True) ) ``` Your `track_model` function should print: ```plaintext Layer hierarchy: 1. SimpleFeedForwardNet â”œâ”€â”€ fc1: Linear(in_features=784, out_features=128, bias=True) â”œâ”€â”€ relu1: ReLU() â”œâ”€â”€ fc2: Linear(in_features=128, out_features=64, bias=True) â”œâ”€â”€ relu2: ReLU() â””â”€â”€ fc3: Linear(in_features=64, out_features=10, bias=True) ``` Constraints - The implementation of `track_model` must use `torch.utils.module_tracker.ModuleTracker`. Guidelines Make sure your function is well-documented, structured, and adheres to the PyTorch conventions. # Additional Notes - If any imports are necessary, ensure they are included. - Maintain clear and consistent naming conventions in your code. - Test your code to ensure the hierarchical structure is printed correctly.","solution":"import torch import torch.nn as nn class SimpleFeedForwardNet(nn.Module): def __init__(self): super(SimpleFeedForwardNet, self).__init__() self.fc1 = nn.Linear(784, 128) self.relu1 = nn.ReLU() self.fc2 = nn.Linear(128, 64) self.relu2 = nn.ReLU() self.fc3 = nn.Linear(64, 10) def forward(self, x): x = self.fc1(x) x = self.relu1(x) x = self.fc2(x) x = self.relu2(x) x = self.fc3(x) return x def track_model(model: nn.Module) -> None: Takes a PyTorch model as input and prints out a user-friendly representation of the model\'s hierarchical structure. module_str = repr(model) print(\\"Layer hierarchy:\\") print(module_str)"},{"question":"# Email Management with `imaplib` You are required to implement an email management utility using the Python `imaplib` module. This utility should provide functions to log in to an email server, list emails, fetch email content, and delete emails based on specific criteria. # Specifications 1. **Login Function**: - **Function Name**: `login_to_email` - **Input**: `host (str)`, `username (str)`, `password (str)` - **Output**: `IMAP4 object` if login is successful, otherwise raise an appropriate exception. - Use `IMAP4_SSL` for secure connections on port 993. 2. **List Emails Function**: - **Function Name**: `list_emails` - **Input**: `imap (IMAP4 object)`, `mailbox (str)` (default to \'INBOX\') - **Output**: List of email IDs (str). Return all email IDs in the selected mailbox. - Should handle exceptions and return an appropriate message if the mailbox doesnâ€™t exist. 3. **Fetch Email Function**: - **Function Name**: `fetch_email_content` - **Input**: `imap (IMAP4 object)`, `email_id (str)` - **Output**: `str` content of the email (main body text). - Should fetch and return the content of the specified email. 4. **Delete Emails Function**: - **Function Name**: `delete_emails` - **Input**: `imap (IMAP4 object)`, `criteria (str)` (IMAP search criteria compatible string) - **Output**: `None`. This function should delete emails that match the given criteria. # Constraints - You must use `imaplib.IMAP4_SSL` for secure connection. - Handle errors appropriately and provide meaningful messages in case of any failure (`try..except` blocks). - Maintain the connection (login) throughout the operations until explicitly logged out. # Example ```python import getpass from imaplib import IMAP4_SSL def login_to_email(host: str, username: str, password: str) -> IMAP4_SSL: try: imap = IMAP4_SSL(host) imap.login(username, password) return imap except IMAP4_SSL.error as e: raise Exception(f\\"Failed to login: {e}\\") def list_emails(imap: IMAP4_SSL, mailbox: str = \'INBOX\') -> list: try: imap.select(mailbox) result, data = imap.search(None, \'ALL\') if result != \'OK\': raise Exception(\\"Failed to retrieve emails\\") email_ids = data[0].split() return email_ids except IMAP4_SSL.error as e: raise Exception(f\\"Failed to retrieve emails: {e}\\") def fetch_email_content(imap: IMAP4_SSL, email_id: str) -> str: try: result, data = imap.fetch(email_id, \'(RFC822)\') if result != \'OK\': raise Exception(f\\"Failed to fetch email content\\") return data[0][1].decode() except IMAP4_SSL.error as e: raise Exception(f\\"Failed to fetch email content: {e}\\") def delete_emails(imap: IMAP4_SSL, criterion: str) -> None: try: imap.select(\'INBOX\') result, data = imap.search(None, criterion) if result != \'OK\': raise Exception(f\\"Failed to search emails\\") for num in data[0].split(): imap.store(num, \'+FLAGS\', \'Deleted\') imap.expunge() except IMAP4_SSL.error as e: raise Exception(f\\"Failed to delete emails: {e}\\") ``` Write the missing parts of each function leveraging the imaplib module and ensure to handle potential exceptions.","solution":"import imaplib def login_to_email(host: str, username: str, password: str) -> imaplib.IMAP4_SSL: Logs into an email server using the given credentials. try: imap = imaplib.IMAP4_SSL(host) imap.login(username, password) return imap except imaplib.IMAP4.error as e: raise Exception(f\\"Failed to login: {e}\\") def list_emails(imap: imaplib.IMAP4_SSL, mailbox: str = \'INBOX\') -> list: Lists email IDs in the specified mailbox. try: imap.select(mailbox) result, data = imap.search(None, \'ALL\') if result != \'OK\': raise Exception(\\"Failed to retrieve emails\\") email_ids = data[0].split() return email_ids except imaplib.IMAP4.error as e: raise Exception(f\\"Failed to retrieve emails: {e}\\") def fetch_email_content(imap: imaplib.IMAP4_SSL, email_id: str) -> str: Fetches the content of the specified email. try: result, data = imap.fetch(email_id, \'(RFC822)\') if result != \'OK\': raise Exception(f\\"Failed to fetch email content\\") return data[0][1].decode() except imaplib.IMAP4.error as e: raise Exception(f\\"Failed to fetch email content: {e}\\") def delete_emails(imap: imaplib.IMAP4_SSL, criteria: str) -> None: Deletes emails matching the specified criteria. try: imap.select(\'INBOX\') result, data = imap.search(None, criteria) if result != \'OK\': raise Exception(f\\"Failed to search emails\\") for num in data[0].split(): imap.store(num, \'+FLAGS\', \'Deleted\') imap.expunge() except imaplib.IMAP4.error as e: raise Exception(f\\"Failed to delete emails: {e}\\")"},{"question":"Title: Custom Multiclass Classification with OneVsRest Strategy **Objective**: Implement a multiclass classifier using the OneVsRestClassifier strategy in scikit-learn. You will load a dataset, preprocess it, fit a OneVsRestClassifier using a base estimator, and evaluate the model\'s performance. **Problem Statement**: 1. Load the Iris dataset from `sklearn.datasets`. 2. Preprocess the dataset by splitting it into training and test sets with an 80-20 ratio. 3. Implement the `create_and_evaluate_ovr_classifier` function. 4. The function `create_and_evaluate_ovr_classifier` should: - Take no input parameters. - Load the Iris dataset. - Split the dataset into training and test sets. - Initialize a `OneVsRestClassifier` with a `LinearSVC` as the base estimator. - Fit the classifier on the training set. - Predict the output labels for the test set. - Return the classification report as a string. **Function Signature**: ```python def create_and_evaluate_ovr_classifier() -> str: pass ``` **Constraints**: - The classifier should use `LinearSVC` and not any other classifier. - Use an 80-20 train-test split. - Return the classification report using `sklearn.metrics.classification_report`. **Evaluation**: Your solution will be evaluated on: - Correctness of implementation. - Proper use of `OneVsRestClassifier`. - Accuracy and clarity of the returned classification report. **Hints**: - Use `train_test_split` from `sklearn.model_selection` for splitting the dataset. - Use `classification_report` from `sklearn.metrics` to generate the evaluation report.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.svm import LinearSVC from sklearn.multiclass import OneVsRestClassifier from sklearn.metrics import classification_report def create_and_evaluate_ovr_classifier() -> str: # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training and test sets with an 80-20 ratio X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize a OneVsRestClassifier with a LinearSVC as the base estimator ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=42)) # Fit the classifier on the training set ovr_classifier.fit(X_train, y_train) # Predict the output labels for the test set y_pred = ovr_classifier.predict(X_test) # Return the classification report as a string return classification_report(y_test, y_pred)"},{"question":"# Weak Reference Management in Python Python provides weak references to optimize memory usage by allowing objects to be referenced without preventing their garbage collection. Your task is to implement a Python class that uses weak references to manage a set of objects and provides utilities based on weak reference functionalities. Task 1. **Create the class `WeakSetManager`**: - The class should manage a collection of objects using weak references. - Use `weakref.WeakSet` from Python\'s `weakref` module. 2. **Implement the following methods**: - `__init__(self)`: Initializes an empty weak reference set. - `add_object(self, obj)`: Adds a new object to the weak reference set. - `remove_object(self, obj)`: Removes an object from the weak reference set. - `get_all_objects(self)`: Returns a list of all objects currently in the weak reference set. Since weak references can be garbage collected, ensure that the output does not include objects that have been collected. - `clear(self)`: Clears all weak references. Constraints - You are required to use weak references for managing the set of objects. - Ensure that the class handles garbage collection correctly, i.e., objects that are no longer in use should not appear in the result of `get_all_objects`. Example ```python import weakref class WeakSetManager: def __init__(self): # Your implementation here def add_object(self, obj): # Your implementation here def remove_object(self, obj): # Your implementation here def get_all_objects(self): # Your implementation here def clear(self): # Your implementation here # Usage example if __name__ == \\"__main__\\": manager = WeakSetManager() class A: def __init__(self, name): self.name = name a1 = A(\\"obj1\\") a2 = A(\\"obj2\\") manager.add_object(a1) manager.add_object(a2) print(manager.get_all_objects()) # Should print a list with a1 and a2 in it del a1 import gc; gc.collect() # Force garbage collection print(manager.get_all_objects()) # Should not include a1 now ``` Note: - Your solution should use the WeakSet to handle weak references efficiently. - You do not need to handle any threading issues. - You can assume that objects added to the `WeakSetManager` will have a `__repr__` method that uniquely identifies them during debugging.","solution":"import weakref class WeakSetManager: def __init__(self): self._weak_set = weakref.WeakSet() def add_object(self, obj): self._weak_set.add(obj) def remove_object(self, obj): self._weak_set.discard(obj) def get_all_objects(self): return list(self._weak_set) def clear(self): self._weak_set.clear()"},{"question":"# Question: Advanced Logging Configuration with dictConfig() Objective Demonstrate your understanding of the `logging.config` module in Python 3.10, particularly the `dictConfig` method for configuring the logging system using a dictionary schema. Problem Statement You are tasked with configuring a logging system for a Python application using the `dictConfig` function from the `logging.config` module. The configuration should include different types of loggers, handlers, and formatters, as described below. Requirements 1. **Loggers**: - A root logger with level `WARNING` and a handler called `console`. - A logger named `app.debug` with level `DEBUG`, a handler called `file`, and `propagate` set to `True`. 2. **Handlers**: - `console`: a `StreamHandler` that outputs to `sys.stdout` with the formatter `detailed`. - `file`: a `FileHandler` that writes logs to `app.log` with the formatter `simple`. 3. **Formatters**: - `detailed`: format string `\'%(asctime)s %(name)-12s %(levelname)-8s %(message)s\'`. - `simple`: format string `\'%(levelname)-8s %(message)s\'`. 4. Ensure that the `dictConfig` raises an appropriate error if the configuration is invalid. Constraints - All configurations must be done using the `dictConfig` function. - Do not use external configuration files. - Your configuration dictionary should follow the schema provided in the documentation. Input - None (You will create the configuration dictionary within the function). Output - Successfully configured logging as per the requirements. Print a debug message to demonstrate the `app.debug` logger is correctly configured. Function Signature ```python def setup_logging(): import logging import logging.config import sys # Create and configure the dictionary here config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'detailed\': { \'format\': \'%(asctime)s %(name)-12s %(levelname)-8s %(message)s\', }, \'simple\': { \'format\': \'%(levelname)-8s %(message)s\', } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'detailed\', \'stream\': \'ext://sys.stdout\', }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'filename\': \'app.log\', \'mode\': \'a\', \'encoding\': \'utf-8\', } }, \'loggers\': { \'app.debug\': { \'level\': \'DEBUG\', \'handlers\': [\'file\'], \'propagate\': True, } }, \'root\': { \'level\': \'WARNING\', \'handlers\': [\'console\'] }, } try: logging.config.dictConfig(config) logger = logging.getLogger(\'app.debug\') logger.debug(\'Logging system is configured correctly.\') except (ValueError, TypeError, AttributeError, ImportError) as e: raise e ``` Explanation and Implementation Details 1. **Define Configuration Dictionary**: Follow the schema structure to define `formatters`, `handlers`, `loggers`, and `root` configurations. 2. **Use dictConfig**: Apply the configuration with the `dictConfig` function. 3. **Logging Example**: Instantiate the logger `app.debug` and print a debug message to verify the setup. 4. **Error Handling**: Ensure that appropriate errors are raised for invalid configurations.","solution":"def setup_logging(): import logging import logging.config import sys # Create and configure the dictionary here config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'detailed\': { \'format\': \'%(asctime)s %(name)-12s %(levelname)-8s %(message)s\', }, \'simple\': { \'format\': \'%(levelname)-8s %(message)s\', } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'detailed\', \'stream\': \'ext://sys.stdout\', }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'filename\': \'app.log\', \'mode\': \'a\', \'encoding\': \'utf-8\', } }, \'loggers\': { \'app.debug\': { \'level\': \'DEBUG\', \'handlers\': [\'file\'], \'propagate\': True, } }, \'root\': { \'level\': \'WARNING\', \'handlers\': [\'console\'] }, } try: logging.config.dictConfig(config) logger = logging.getLogger(\'app.debug\') logger.debug(\'Logging system is configured correctly.\') except (ValueError, TypeError, AttributeError, ImportError) as e: raise e"},{"question":"# Pandas Coding Assessment: Copy-on-Write (CoW) Objective: Design a function `modify_dataframe` that demonstrates a deep understanding of pandas\' Copy-on-Write (CoW) mechanism introduced in pandas 3.0. This exercise will assess your ability to manipulate DataFrame objects without inadvertently modifying other objects that share data. Problem Statement: Implement a function `modify_dataframe` that takes two inputs: 1. A pandas DataFrame `df` with at least two columns and several rows. 2. A list of modifications to be applied on `df`, where each modification is a dictionary containing: - \'action\': A string that specifies the action to perform. Actions can be \'set_value\', \'replace\', or \'reset_index\'. - \'column\': A string specifying the column to modify (for \'set_value\' and \'replace\' actions). - \'index\': An integer specifying the row index to modify (for \'set_value\' action). - \'value\': The new value to be set (for \'set_value\' and \'replace\' actions). - \'drop\': A boolean specifying whether to drop the index column during reset_index (for \'reset_index\' action). Your function should: 1. Implement the specified modifications in a way that adheres to Copy-on-Write principles. 2. Return the original DataFrame `df` and the modified DataFrame `df_modified`. **Function Signature:** ```python import pandas as pd from typing import Tuple, List, Dict def modify_dataframe(df: pd.DataFrame, modifications: List[Dict[str, any]]) -> Tuple[pd.DataFrame, pd.DataFrame]: pass ``` Example: ```python # Sample DataFrame df = pd.DataFrame({ \'foo\': [1, 2, 3], \'bar\': [4, 5, 6] }) # List of modifications modifications = [ {\'action\': \'set_value\', \'column\': \'foo\', \'index\': 0, \'value\': 100}, {\'action\': \'replace\', \'column\': \'foo\', \'value\': 2, \'new_value\': 200}, {\'action\': \'reset_index\', \'drop\': True} ] # Expected output original_df, modified_df = modify_dataframe(df, modifications) print(original_df) # Output: # foo bar # 0 1 4 # 1 2 5 # 2 3 6 print(modified_df) # Output: # foo bar # 0 100 4 # 1 200 5 # 2 3 6 ``` Constraints: - You may assume the DataFrame has at least two columns and five rows. - You should NOT mutate the original DataFrame `df`. Any modifications should affect only `df_modified`. - You should handle invalid actions gracefully by ignoring them.","solution":"import pandas as pd from typing import Tuple, List, Dict def modify_dataframe(df: pd.DataFrame, modifications: List[Dict[str, any]]) -> Tuple[pd.DataFrame, pd.DataFrame]: Returns the original dataframe and a modified dataframe based on the specified list of modifications. Modifications should adhere to pandas\' Copy-on-Write principles. # Create a deep copy of the original DataFrame to avoid modifying it directly df_modified = df.copy(deep=True) for mod in modifications: action = mod.get(\'action\') if action == \'set_value\': column = mod.get(\'column\') index = mod.get(\'index\') value = mod.get(\'value\') if column in df_modified.columns and index in df_modified.index: df_modified.at[index, column] = value elif action == \'replace\': column = mod.get(\'column\') value = mod.get(\'value\') new_value = mod.get(\'new_value\', None) if column in df_modified.columns: df_modified[column] = df_modified[column].replace(value, new_value) elif action == \'reset_index\': drop = mod.get(\'drop\', False) df_modified.reset_index(drop=drop, inplace=True) return df, df_modified"},{"question":"**Problem: Frame Introspection and Evaluation in Python** You are tasked with writing a Python function that performs introspection on the current call stack and prints detailed information about each active frame. Specifically, the function should: 1. Retrieve the current thread\'s top frame. 2. Traverse the call stack from the top frame to the outermost frame. 3. For each frame, gather the following information: - Frame\'s code object and its associated filename. - The line number being executed in that frame. - All local variables and their values. - All global variables and their values. 4. Print this information in a readable format. # Function Signature ```python def introspect_call_stack(): pass ``` # Output Format The output should be a series of printed statements formatted as follows for each frame: ``` Frame at <filename>, line <line_number>: Locals: <local_vars_dict> Globals: <global_vars_dict> ``` # Example Output ``` Frame at script.py, line 42: Locals: {\'x\': 10, \'y\': 20} Globals: {\'a\': 1, \'b\': 2} Frame at script.py, line 38: Locals: {\'foo\': \'bar\'} Globals: {\'a\': 1, \'b\': 2} ``` # Constraints - You must use the functions provided in the `python310` documentation for accessing frame information. - If a frame or variable dictionary is `NULL`, handle gracefully by printing `None`. # Performance Requirements - The function should efficiently traverse and introspect the call stack. Ensure that your implementation does not introduce significant overhead to the execution. # Hints - You may find the functions `PyEval_GetFrame()`, `PyFrame_GetBack()`, `PyFrame_GetCode()`, `PyFrame_GetLineNumber()`, `PyEval_GetLocals()`, and `PyEval_GetGlobals()` particularly useful. - To access the C API in Python, consider using the `ctypes` or `cffi` library to call these functions. Implement the `introspect_call_stack` function to meet the described requirements.","solution":"import inspect def introspect_call_stack(): Introspects the current call stack and prints detailed information about each active frame. current_frame = inspect.currentframe() while current_frame is not None: # Get frame info frame_info = inspect.getframeinfo(current_frame) code_object = current_frame.f_code filename = code_object.co_filename line_number = frame_info.lineno local_vars = current_frame.f_locals global_vars = current_frame.f_globals # Print frame info print(f\\"Frame at {filename}, line {line_number}:\\") print(f\\"Locals: {local_vars}\\") print(f\\"Globals: {global_vars}\\") # Move to the outer frame current_frame = current_frame.f_back"},{"question":"Question: Using the Seaborn library, you will visualize the relationship between the `year`, `species`, and `pounds_caught` from a hypothetical dataset of fish caught over multiple years. 1. **Data Load and Preparation**: - Load the dataset using the following dictionary to simulate the dataset: ```python fish_data = { \'year\': [2000, 2000, 2000, 2001, 2001, 2001, 2002, 2002, 2002, 2003, 2003, 2003], \'species\': [\'salmon\', \'tuna\', \'trout\', \'salmon\', \'tuna\', \'trout\', \'salmon\', \'tuna\', \'trout\', \'salmon\', \'tuna\', \'trout\'], \'pounds_caught\': [500, 600, 700, 550, 620, 710, 580, 640, 730, 590, 660, 740] } ``` - Convert this dictionary to a Pandas DataFrame. 2. **Visualization**: - Create a plot using the `seaborn.objects` module to visualize the total `pounds_caught` per year for each fish species. - Your plot should contain: - A line plot for each species showing the trend over the years. - A band plot to show a 95% confidence interval for the `pounds_caught` per year for each species. Constraints: - Utilize `seaborn.objects.Plot` to create the visualization. - Use appropriate methods for computing the confidence interval. Expected Function Signature: ```python def visualize_fish_catch(): # Implementation here pass ``` Example: For the given dataset, your function should generate a plot with: - X-axis labeled as \'Year\' - Y-axis labeled as \'Pounds Caught\' - Different line colors representing different species - Bands representing the confidence interval around the lines. **Note**: The generated plot should be clear and informative, demonstrating the comprehension of Seaborn\'s object-oriented plotting system.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from seaborn import objects as so def visualize_fish_catch(): # Load the data fish_data = { \'year\': [2000, 2000, 2000, 2001, 2001, 2001, 2002, 2002, 2002, 2003, 2003, 2003], \'species\': [\'salmon\', \'tuna\', \'trout\', \'salmon\', \'tuna\', \'trout\', \'salmon\', \'tuna\', \'trout\', \'salmon\', \'tuna\', \'trout\'], \'pounds_caught\': [500, 600, 700, 550, 620, 710, 580, 640, 730, 590, 660, 740] } df = pd.DataFrame(fish_data) # Create the plot p = so.Plot(df, x=\'year\', y=\'pounds_caught\', color=\'species\').add( so.Line(), so.Agg() ).add( so.Band(), so.Est(errorbar=\'se\') ).label( x=\'Year\', y=\'Pounds Caught\', color=\'Species\' ) p.show() # Run the function to produce the plot visualize_fish_catch()"},{"question":"# Asynchronous Network Communication with Python\'s `asyncio` Streams Objective Write a Python program using the `asyncio` streams module to implement a custom server-client architecture. You need to demonstrate skills in network connection management, asynchronous data transmission, and graceful connection termination. Problem Statement You will create both a server and a client. The server should perform the following tasks: 1. Listen for incoming connections on a specified port. 2. For each connected client, it should read a message, reply with a confirmation message, and log the activity. The client should: 1. Connect to the server. 2. Send a predefined message to the server. 3. Await and print the server\'s confirmation message. Detailed Requirements: 1. **Server Implementation:** * Use `asyncio.start_server` to start a TCP server that listens on `localhost` and port `8888`. * Accept incoming connections using an asynchronous callback function. * For each connection, read a message from the client. * Respond with the message: \\"Received `<client_message>`\\", where `<client_message>` is the message received from the client. * Log the message received from the client and the confirmation message sent to the console. 2. **Client Implementation:** * Use `asyncio.open_connection` to connect to the server on `localhost` and port `8888`. * Send a message `\\"Hello Server!\\"` to the server. * Await and print the confirmation message from the server. Constraints and Limitations: * Both the server and client should be implemented using `asyncio` and `async/await` syntax to maintain non-blocking behavior. * Ensure proper closure of streams and sockets by using `writer.close()` and `await writer.wait_closed()` for both server and client. * You are allowed to run server and client in a sequence or concurrently for testing based on your setup. Code Template: ```python import asyncio # Server Code async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') log_message = f\\"Received {message!r} from {addr!r}\\" print(log_message) response = f\\"Received {message!r}\\" writer.write(response.encode()) await writer.drain() writer.close() await writer.wait_closed() print(\\"Connection closed\\") async def start_server(): server = await asyncio.start_server( handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() # Client Code async def send_message(): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) message = \\"Hello Server!\\" print(f\'Send: {message!r}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()!r}\') writer.close() await writer.wait_closed() # Main function to run server and client for testing async def main(): # Start server server_task = asyncio.create_task(start_server()) # Allow server some time to start up await asyncio.sleep(1) # Start client await send_message() # Give some time to complete connection and processing await asyncio.sleep(1) # Stop server task server_task.cancel() # Running the main function asyncio.run(main()) ``` # Notes: * You can run these functions in different scripts/files for better clarity while testing in a practical scenario. * Use logging and try-except blocks for handling errors gracefully in real-world projects. Evaluation Criteria: * Correctness and readability of code. * Proper use of asynchronous constructs and maintaining non-blocking behavior. * Efficient management of network connections and data handling.","solution":"import asyncio # Server Code async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') log_message = f\\"Received {message!r} from {addr!r}\\" print(log_message) response = f\\"Received {message!r}\\" writer.write(response.encode()) await writer.drain() writer.close() await writer.wait_closed() print(\\"Connection closed\\") async def start_server(): server = await asyncio.start_server( handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() # Client Code async def send_message(): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) message = \\"Hello Server!\\" print(f\'Send: {message!r}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()!r}\') writer.close() await writer.wait_closed() # Main function to run server and client for testing async def main(): # Start server server_task = asyncio.create_task(start_server()) # Allow server some time to start up await asyncio.sleep(1) # Start client await send_message() # Give some time to complete connection and processing await asyncio.sleep(1) # Stop server task server_task.cancel() # Running the main function asyncio.run(main())"},{"question":"# Logging System Implementation with Multiple Handlers Objective: Design and implement a logging system in Python that utilizes the `StreamHandler`, `RotatingFileHandler`, and `SMTPHandler` from the `logging` and `logging.handlers` modules. The system should demonstrate comprehensive usage and configuration of these handlers. Requirements: 1. **StreamHandler Configuration**: - Send log messages to `sys.stdout`. - Format the log messages with the format: `%(asctime)s - %(levelname)s - %(message)s`. - The log level should be set to `DEBUG`. 2. **RotatingFileHandler Configuration**: - Write log messages to a file named `app.log`. - Rotate the log file when it reaches 1 MB size. - Keep up to 5 backup log files. - The log level for this handler should be `INFO`. 3. **SMTPHandler Configuration**: - Send an email when a log message with `ERROR` or higher level is logged. - The email should be sent from `\\"logger@example.com\\"` to `\\"admin@example.com\\"`. - The subject line of the email should be `\\"Application Error Log\\"`. - Assume the SMTP server is `\\"localhost\\"` and does not require authentication. 4. **Customization and Usage**: - Create a custom logger named `\'appLogger\'`. - Add all the handlers to this logger. - Write a function `setup_custom_logger()` that configures and returns this custom logger. - Demonstrate the logging system by logging messages of various levels (`DEBUG`, `INFO`, `ERROR`, etc.) and show how the messages are handled by the different handlers. Constraints: - Do not use any external libraries besides `logging` and `logging.handlers`. - Ensure the code is clear and well-documented. Expected Input and Output: - **No Direct Input**: The function `setup_custom_logger()` does not take any parameters. - **Output**: Demonstrates logger configuration and logs messages to stdout, a rotating file, and email. - **Example Demonstration**: ```python logger = setup_custom_logger() logger.debug(\\"This is a debug message.\\") logger.info(\\"This is an info message.\\") logger.error(\\"This is an error message.\\") ``` Performance: - Ensure the logging configuration is efficient and does not introduce unnecessary overhead.","solution":"import logging import logging.handlers import sys def setup_custom_logger(): Set up a custom logger with StreamHandler, RotatingFileHandler, and SMTPHandler. # Create a custom logger logger = logging.getLogger(\'appLogger\') logger.setLevel(logging.DEBUG) # Create handlers stream_handler = logging.StreamHandler(sys.stdout) rotating_file_handler = logging.handlers.RotatingFileHandler( \'app.log\', maxBytes=1*1024*1024, backupCount=5 ) smtp_handler = logging.handlers.SMTPHandler( mailhost=\'localhost\', fromaddr=\'logger@example.com\', toaddrs=[\'admin@example.com\'], subject=\'Application Error Log\', ) # Set levels for handlers stream_handler.setLevel(logging.DEBUG) rotating_file_handler.setLevel(logging.INFO) smtp_handler.setLevel(logging.ERROR) # Create formatters and add them to the handlers formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') stream_handler.setFormatter(formatter) rotating_file_handler.setFormatter(formatter) smtp_handler.setFormatter(formatter) # Add handlers to the logger logger.addHandler(stream_handler) logger.addHandler(rotating_file_handler) logger.addHandler(smtp_handler) return logger # Example usage of the logger if __name__ == \\"__main__\\": logger = setup_custom_logger() logger.debug(\\"This is a debug message.\\") logger.info(\\"This is an info message.\\") logger.error(\\"This is an error message.\\")"},{"question":"# Coding Assessment: Audit Event Logger Objective: You are to implement a Python function that sets up an audit hook, logs specific audit events to a file, and then provides a summary of these events. Requirements: 1. Implement a function `setup_audit_logger(events_to_log: List[str], log_file: str) -> None` which sets up an audit hook, listens for specified events, and logs their data to a provided file. 2. Implement a function `summarize_events(log_file: str) -> Dict[str, int]` which reads the logged events from the file and returns a summary dictionary where keys are event names and values are the number of times each event occurred. Specifics: 1. The `setup_audit_logger` function: - Should accept a list of event names to log. - Should add an audit hook that logs these events to the specified file. Each log entry should include the event name and its arguments. 2. The `summarize_events` function: - Should read the file used for logging and produce a dictionary summary of the events. Input/Output: **Function: `setup_audit_logger(events_to_log: List[str], log_file: str) -> None`** - **Input:** - `events_to_log`: A list of strings representing the audit events to log. - `log_file`: A string representing the path of the file where audit events should be logged. - **Output:** None **Function: `summarize_events(log_file: str) -> Dict[str, int]`** - **Input:** - `log_file`: A string representing the path of the file where audit events were logged. - **Output:** - A dictionary where keys are event names and values are integers representing the count of each event. Example: ```python def setup_audit_logger(events_to_log, log_file): # Your implementation here def summarize_events(log_file): # Your implementation here # Example events = [\\"os.remove\\", \\"os.rmdir\\"] log_file_path = \\"audit_log.txt\\" setup_audit_logger(events, log_file_path) import os os.remove(\\"example.txt\\") os.rmdir(\\"example_dir\\") summary = summarize_events(log_file_path) print(summary) # Output example: {\'os.remove\': 1, \'os.rmdir\': 1} ``` Constraints: - Consider proper handling of file operations. - Ensure performance efficiency with large log files. - Standard error handling for non-existent events.","solution":"import sys import os from typing import List, Dict def audit_hook(event, args): This function will be used as the audit hook to log events. if event in sys.audit_events_to_log: with open(sys.audit_log_file, \\"a\\") as f: f.write(f\\"{event}: {args}n\\") def setup_audit_logger(events_to_log: List[str], log_file: str) -> None: Sets up the audit logger to listen for specific events and logs them to a file. Parameters: events_to_log (List[str]): List of event names to log. log_file (str): Path to the file for logging events. sys.audit_events_to_log = events_to_log sys.audit_log_file = log_file sys.addaudithook(audit_hook) def summarize_events(log_file: str) -> Dict[str, int]: Summarizes the events logged in the file. Parameters: log_file (str): Path to the file with logged events. Returns: Dict[str, int]: A dictionary with event names as keys and their occurrence counts as values. event_count = {} with open(log_file, \\"r\\") as f: for line in f: event_name = line.split(\\":\\")[0] if event_name in event_count: event_count[event_name] += 1 else: event_count[event_name] = 1 return event_count"},{"question":"You are given a dataset that tracks several features of different species of plants. Your task is to create a series of plots using `seaborn.objects` to analyze these features in a well-structured and visually appealing manner. **Dataset Description:** The dataset `plant_dataset.csv` has the following columns: - `species`: Name of the plant species. - `height`: Height of the plant in cm. - `width`: Width of the plant in cm. - `petal_length`: Length of the petals in cm. - `petal_width`: Width of the petals in cm. - `color`: Color category of the petals. *Note:* Ensure `seaborn.objects` and other necessary packages are installed in your environment. Requirements: 1. **Create the Main Plot**: - Load the dataset. - Initialize a `seaborn.objects.Plot`. 2. **Facet the Plot**: - Use `facet` to create a grid of plots where rows correspond to different `species` and columns are based on the `color` of the petals. 3. **Customize Size and Layout**: - Adjust the overall dimensions of the figure to be square, specifically `size=(10, 10)`. - Choose a layout engine that best fits your grid of plots. 4. **Save the Plot**: - Adjust the plot size relative to the figure to focus more on the content. - Save the plot to a file named `plant_analysis_plot.png`. Constraints: - You must use methods from `seaborn.objects`. - Your code should be modular, and each step should be clearly defined in its function. - Ensure that the final plot is clear, well-labeled, and visually structured. **Input**: - The path to the dataset file (`plant_dataset.csv`) is given. **Output**: - A saved plot image file `plant_analysis_plot.png`. Example Usage: ```python import seaborn.objects as so import pandas as pd def create_plant_analysis_plot(file_path): # Step 1: Load the dataset df = pd.read_csv(file_path) # Step 2: Initialize the Plot p = so.Plot(df) # Step 3: Facet the plot by species and color p = p.facet([\'species\', \'color\'], [\'species\', \'color\']) # Step 4: Adjust overall dimensions and layout engine p = p.layout(size=(10, 10), engine=\\"constrained\\") # Step 5: Adjust the plot size relative to the figure and save p = p.layout(extent=[0, 0, 0.85, 1]) p.show() p.save(\\"plant_analysis_plot.png\\") # Example function call create_plant_analysis_plot(\\"path/to/plant_dataset.csv\\") ``` Make sure that your solution adheres to the above requirements and correctly implements the necessary functionality using the seaborn objects interface.","solution":"import seaborn.objects as so import pandas as pd def create_plant_analysis_plot(file_path): Creates and saves a plot analyzing plant species and their features faceted by species and color using seaborn.objects. Parameters: file_path (str): The path to the plant dataset CSV file. # Step 1: Load the dataset df = pd.read_csv(file_path) # Step 2: Initialize the Plot p = so.Plot(df, x=\'height\', y=\'petal_length\') # Step 3: Facet the plot by species and color p = p.facet(col=\'color\', row=\'species\') # Step 4: Customize Size and Layout p = p.layout(size=(10, 10)).scale(extent=[0, 0, 0.85, 1], scale_type=\\"figure\\") # Save the plot p.save(\\"plant_analysis_plot.png\\") # Example function call # create_plant_analysis_plot(\\"path/to/plant_dataset.csv\\")"},{"question":"**Question: Package Metadata Analyzer** You are tasked with creating a Python function `analyze_package_metadata(package_name: str) -> dict` that uses the `importlib.metadata` module to gather and analyze metadata for a given installed package. The function should return a dictionary with the following information: 1. `\\"version\\"`: The version of the package. 2. `\\"author\\"`: The author of the package. 3. `\\"requires_python\\"`: The required Python version(s) for the package. 4. `\\"entry_points\\"`: A list of all entry points defined by the package, categorized by their groups (e.g., `\'console_scripts\'`, etc.). Each entry point should be represented as a dictionary with its name, module, and the callable attribute. 5. `\\"installation_files\\"`: A list of all the files installed by the package, with their sizes in kilobytes and their hash values. **Input:** - `package_name` (string): The name of the installed package to analyze. **Output:** - A dictionary with the aforementioned keys and corresponding values. **Constraints:** - Assume that the package specified by `package_name` is already installed. - The function should handle the absence of certain metadata gracefully, by using `None` if a value is not available (e.g., no entry points or missing author information). **Example:** ```python result = analyze_package_metadata(\\"wheel\\") print(result) ``` Expected example structure of the output dictionary (actual values will depend on the package metadata): ```python { \\"version\\": \\"0.35.1\\", \\"author\\": \\"Daniel Holth\\", \\"requires_python\\": \\">=3.6\\", \\"entry_points\\": { \\"console_scripts\\": [ {\\"name\\": \\"wheel\\", \\"module\\": \\"wheel.cli\\", \\"attr\\": \\"main\\"} ] }, \\"installation_files\\": [ {\\"path\\": \\"wheel/__init__.py\\", \\"size_kb\\": 2.5, \\"hash\\": \\"abc123\\"}, {\\"path\\": \\"wheel/util.py\\", \\"size_kb\\": 0.9, \\"hash\\": \\"def456\\"} ] } ``` Implement this function. Your solution will be evaluated on correctness, handling of edge cases, and code readability. **Notes:** - Use the `importlib.metadata` module as needed to fetch the package metadata. - Ensure to catch and handle any exceptions that might occur due to missing metadata gracefully.","solution":"import importlib.metadata import hashlib import os def analyze_package_metadata(package_name: str) -> dict: metadata = {} try: dist = importlib.metadata.distribution(package_name) except importlib.metadata.PackageNotFoundError: return {\\"error\\": f\\"Package {package_name} not found.\\"} # Version Information metadata[\'version\'] = dist.version # Author Information metadata[\'author\'] = dist.metadata.get(\'Author\', None) # Requires Python metadata[\'requires_python\'] = dist.metadata.get(\'Requires-Python\', None) # Entry Points entry_points = {} if dist.entry_points: for ep in dist.entry_points: if ep.group not in entry_points: entry_points[ep.group] = [] entry_points[ep.group].append({ \\"name\\": ep.name, \\"module\\": ep.module, \\"attr\\": ep.attr }) metadata[\'entry_points\'] = entry_points # Installation files with size and hash installation_files = [] for file_info in dist.files: file_path = file_info.locate().as_posix() size_kb = round(os.path.getsize(file_path) / 1024, 2) hash_val = hashlib.md5(open(file_path, \'rb\').read()).hexdigest() installation_files.append({ \\"path\\": file_path, \\"size_kb\\": size_kb, \\"hash\\": hash_val }) metadata[\'installation_files\'] = installation_files return metadata"},{"question":"# Covariance Estimation with Scikit-Learn You are given a dataset containing multiple observations of various features. Your task is to implement a function to estimate the covariance matrix using different methods provided by the `sklearn.covariance` module in scikit-learn. Specifically, you need to implement the following estimation methods and compare their outputs: 1. Empirical Covariance 2. Shrunk Covariance 3. Ledoit-Wolf Shrinkage 4. Oracle Approximating Shrinkage (OAS) 5. Sparse Inverse Covariance 6. Robust Covariance Estimation # Function Signature ```python import numpy as np def estimate_covariance_methods(data: np.ndarray) -> dict: Estimate the covariance matrix using various methods. Parameters: - data: np.ndarray of shape (n_samples, n_features), the input data. Returns: - dict: A dictionary containing the covariance matrices for each method. Keys should be the method names as strings. pass ``` # Detailed Requirements 1. **Empirical Covariance**: - Use `EmpiricalCovariance` to fit and obtain the empirical covariance matrix. 2. **Shrunk Covariance**: - Use `ShrunkCovariance` with a shrinkage parameter of 0.1 to fit and obtain the covariance matrix. 3. **Ledoit-Wolf Shrinkage**: - Use `LedoitWolf` to fit and obtain the Ledoit-Wolf shrinkage covariance matrix. 4. **Oracle Approximating Shrinkage (OAS)**: - Use `OAS` to fit and obtain the OAS shrinkage covariance matrix. 5. **Sparse Inverse Covariance**: - Use `GraphicalLassoCV` to fit and obtain the sparse inverse covariance (precision matrix). Convert this precision matrix to a covariance matrix. 6. **Robust Covariance Estimation**: - Use `MinCovDet` to fit and obtain the Minimum Covariance Determinant (MCD) matrix. # Example Usage ```python # Example data array data = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) # Perform covariance estimation using various methods covariances = estimate_covariance_methods(data) # Display the estimated covariance matrices for method, cov_matrix in covariances.items(): print(f\\"{method} covariance matrix:n{cov_matrix}n\\") ``` # Constraints - The input data matrix `data` will have at most 1000 samples and 50 features. - The estimation should handle cases where the number of samples is less than the number of features. # Performance Requirements - The function should be efficient enough to handle the upper limit of the input size within reasonable time. Note: Import necessary modules from `sklearn.covariance` and other required libraries as needed to implement the task.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLassoCV, MinCovDet def estimate_covariance_methods(data: np.ndarray) -> dict: Estimate the covariance matrix using various methods. Parameters: - data: np.ndarray of shape (n_samples, n_features), the input data. Returns: - dict: A dictionary containing the covariance matrices for each method. Keys should be the method names as strings. covariances = {} # Empirical Covariance emp_cov = EmpiricalCovariance().fit(data) covariances[\'Empirical Covariance\'] = emp_cov.covariance_ # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(data) covariances[\'Shrunk Covariance\'] = shrunk_cov.covariance_ # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf().fit(data) covariances[\'Ledoit-Wolf\'] = lw_cov.covariance_ # Oracle Approximating Shrinkage (OAS) oas_cov = OAS().fit(data) covariances[\'OAS\'] = oas_cov.covariance_ # Sparse Inverse Covariance sparse_inv_cov = GraphicalLassoCV().fit(data) covariances[\'Sparse Inv Cov\'] = np.linalg.inv(sparse_inv_cov.precision_) # Robust Covariance Estimation robust_cov = MinCovDet().fit(data) covariances[\'Robust Cov\'] = robust_cov.covariance_ return covariances"},{"question":"**Objective:** Implement a TCP echo server and client using the asyncio streams functionality provided in Python 3.10. This exercise will test your understanding of asynchronous programming, handling network connections, and using the StreamReader and StreamWriter classes. **Task 1: TCP Echo Server** 1. Define an asynchronous function named `handle_client(reader, writer)` that reads data from the client, processes it, and sends it back to the client. 2. Create an asynchronous function named `start_server()` that starts the server on `localhost` and port `8888`, using the `asyncio.start_server` function and the `handle_client` function as the callback for client connections. 3. Ensure the server is started and running indefinitely. **Task 2: TCP Echo Client** 1. Define an asynchronous function named `tcp_echo_client(message)` that: - Opens a connection to `localhost` on port `8888` using `asyncio.open_connection`. - Sends the given `message` to the server and waits for the response. - Prints the received data. 2. Call the `tcp_echo_client` function to send and receive a message (for example, \\"Hello World!\\") from the server. **Requirements and Constraints:** - Your solution should include proper coroutine definitions and use await expressions appropriately. - Ensure the server can handle multiple client connections concurrently. - The client must print the sent message and the received echo message. **Sample Code Structure:** ```python import asyncio async def handle_client(reader, writer): # Your implementation here pass async def start_server(): # Your implementation here pass async def tcp_echo_client(message): # Your implementation here pass # To run the server and client if __name__ == \\"__main__\\": server_task = asyncio.run(start_server()) message = \\"Hello World!\\" asyncio.run(tcp_echo_client(message)) ``` The provided structure is a guideline. You may include additional helper functions if necessary. Ensure your solution is robust, handling potential edge cases like client disconnections gracefully. **Performance Considerations:** - The server should be capable of handling multiple clients without significant delays. - Clients should receive their echoed messages in a timely manner. Good luck!","solution":"import asyncio async def handle_client(reader, writer): Handles communication with the client. Reads a message from the client, echoes it back, and then closes the connection. data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") print(f\\"Send: {message}\\") writer.write(data) await writer.drain() print(\\"Close the connection\\") writer.close() await writer.wait_closed() async def start_server(): Starts the TCP echo server on localhost:8888. server = await asyncio.start_server(handle_client, \'localhost\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def tcp_echo_client(message): Opens a connection to the echo server, sends a message, and prints the response. reader, writer = await asyncio.open_connection(\'localhost\', 8888) print(f\'Send: {message}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') print(\'Close the connection\') writer.close() await writer.wait_closed()"},{"question":"# Coding Challenge Question **Objective:** Design a function that demonstrates proficiency with the `lzma` module from the Python Standard Library. **Problem Statement:** You are required to implement a function `compress_and_decompress_file(input_file_path, output_file_path, custom_filters=None)` that reads plain text data from an input file, compresses it using LZMA compression, writes the compressed data to another file, then reads back the compressed data, decompresses it, and outputs the decompressed text. **Function Signature:** ```python def compress_and_decompress_file(input_file_path: str, output_file_path: str, custom_filters: list = None) -> str: pass ``` **Parameters:** - `input_file_path (str)`: Path to the input file containing plain text to be compressed. - `output_file_path (str)`: Path where the compressed data shall be written. - `custom_filters (list, optional)`: A custom filter chain specifier for compressing data. If not provided, use the default compression settings. **Returns:** - `str`: The decompressed text that you read back from the compressed file. **Constraints:** 1. You may assume that the input file will always exist and contain valid plain text data. 2. If `custom_filters` is provided, it should adhere to valid filter chain specifications as described in the `lzma` documentation. **Requirements:** 1. Use `lzma.open` for handling file operations of reading and writing compressed data. 2. Optional usage of a custom filter chain. 3. Ensure exception handling for potential IO operations and `lzma.LZMAError`. **Example Usage:** ```python # Content of input.txt: \\"Hello, this is a test data for compression.\\" # Path to store compressed data: \\"compressed_file.xz\\" filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 2}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 5} ] result = compress_and_decompress_file(\\"input.txt\\", \\"compressed_file.xz\\", filters) print(result) # Output should be: \\"Hello, this is a test data for compression.\\" ``` **Notes:** 1. To implement custom filter chains, refer to the `Specifying custom filter chains` section of the lzma documentation. 2. Consider edge cases such as the input file being empty. Implement the function `compress_and_decompress_file` to solve this problem.","solution":"import lzma def compress_and_decompress_file(input_file_path: str, output_file_path: str, custom_filters: list = None) -> str: try: # Read the input file content with open(input_file_path, \'r\') as f: data = f.read().encode() # Compress the data with lzma.open(output_file_path, \'wb\', filters=custom_filters) as f: f.write(data) # Decompress the data with lzma.open(output_file_path, \'rb\') as f: decompressed_data = f.read().decode() return decompressed_data except (IOError, lzma.LZMAError) as e: return str(e)"},{"question":"# Binary Data Conversion with Encoding and Decoding **Objective:** Write a Python function that reads a binary file, interprets the data using the `struct` module, and then encodes the interpreted data into a specified encoding format using the `codecs` module. Your function should demonstrate the use of both modules in combination with each other. **Function Signature:** ```python def process_binary_file(file_path: str, struct_format: str, encoding: str) -> str: Reads a binary file, interprets it using the given struct format, and encodes the interpreted data into the specified encoding format. Parameters: - file_path (str): The path to the binary file to read. - struct_format (str): The format string to use for interpreting the binary data. - encoding (str): The encoding format to use for encoding the interpreted data. Returns: - str: The encoded string of the interpreted binary data. pass ``` **Input:** - `file_path` (str): A string representing the path to the binary file to read. - `struct_format` (str): A struct format string to specify how to interpret the binary data. - `encoding` (str): A string representing the encoding format (e.g., \'utf-8\', \'ascii\') to use for encoding the interpreted data. **Output:** - The function should return a string that represents the encoded data after interpreting the binary file. **Constraints:** - The binary file specified by `file_path` can be assumed to exist and be readable. - The `struct_format` provided will be a valid format string for the `struct` module. - The `encoding` provided will be a valid encoding supported by the `codecs` module. **Example:** ```python # Assume we have a binary file `data.bin` with the following bytes: `b\'x00x01x00x02\'` # And the struct format string is \'4B\', which means 4 unsigned bytes # We want to encode the interpreted data using \'utf-8\' encoded_data = process_binary_file(\'data.bin\', \'4B\', \'utf-8\') print(encoded_data) # Should output a UTF-8 encoded string of the interpreted bytes ``` **Instructions:** 1. Open and read the binary file specified by `file_path`. 2. Use the `struct` module to interpret the binary data according to the provided `struct_format`. 3. Convert the interpreted data to a string format. 4. Use the `codecs` module to encode the string into the specified `encoding`. 5. Return the encoded string. **Notes:** - You may need to handle cases where the interpreted data is not directly encodeable and convert it appropriately. - Consider how to handle multi-byte characters and ensure the encoding process preserves the data correctly.","solution":"import struct import codecs def process_binary_file(file_path: str, struct_format: str, encoding: str) -> str: Reads a binary file, interprets it using the given struct format, and encodes the interpreted data into the specified encoding format. Parameters: - file_path (str): The path to the binary file to read. - struct_format (str): The format string to use for interpreting the binary data. - encoding (str): The encoding format to use for encoding the interpreted data. Returns: - str: The encoded string of the interpreted binary data. # Read the binary file with open(file_path, \'rb\') as binary_file: binary_data = binary_file.read() # Unpack binary data according to the struct format unpacked_data = struct.unpack(struct_format, binary_data) # Convert the unpacked data to a string representation data_str = \'\'.join(map(str, unpacked_data)) # Encode the string with the specified encoding format encoded_data = codecs.encode(data_str, encoding) return encoded_data"},{"question":"Objective Your task is to use the Seaborn package to create a layered plot visualizing the `tips` dataset. This dataset contains information about the tips received by a waiter in a restaurant over a period of time. Dataset You should use the `tips` dataset from the Seaborn library. You can load this dataset using the following code: ```python import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\") ``` Requirements 1. Create a function `plot_tips_data()` that: - Takes no parameters. - Returns a Seaborn plot object. 2. The function should utilize the `so.Plot` object with the following specifications: - X-axis represents `day`. - Y-axis represents `total_bill`. - Add Dots with a Jitter transformation to avoid overlap. - Add a Range to show interquartile range (25th to 75th percentile) with a horizontal shift. 3. Ensure that your code includes: - Import statements for required libraries. - A properly defined function as described. - Steps to generate the layered plot according to the outlined specifications. Example Output Your function should return an object that when displayed, resembles the following visual: ```python import matplotlib.pyplot as plt plot = plot_tips_data() plot.show() plt.show() ``` Constraints - Use only the `seaborn.objects` module to create the plot. - The plot should properly handle the data and transformations as specified. - Ensure that all steps are performed within the `plot_tips_data()` function. Submission Submit your complete code including: - Function definition. - Any necessary import statements. - No additional script or file inputs are required. The evaluation will be based on correctness, functionality, and adherence to the provided constraints.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_tips_data(): Loads the tips dataset from Seaborn and generates a plot using seaborn.objects that visualizes the distribution of total bills over days of the week. Returns: seaborn.objects.Plot: The generated Seaborn plot object. # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the plot using seaborn.objects.Plot plot = ( so.Plot(tips, x=\\"day\\", y=\\"total_bill\\") .add(so.Dot(), so.Jitter()) .add(so.Range(), so.Dodge(\'h\')) ) return plot"},{"question":"# Problem: You are tasked with implementing a task scheduler that works based on priorities. The scheduler should be able to handle tasks with various priorities, allowing you to efficiently add tasks, remove tasks, and fetch the highest priority task. You need to implement the following functions using the `heapq` module: 1. **add_task(task_list, task, priority)**: Adds a new task with a given priority to the task list. 2. **remove_task(task_list, task, entry_finder, removed_marker)**: Removes a specific task from the task list. 3. **pop_task(task_list, entry_finder, removed_marker)**: Fetches the task with the highest priority from the task list and removes it. 4. **get_n_highest_priority_tasks(task_list, entry_finder, removed_marker, n)**: Returns the `n` highest priority tasks without modifying the task list. Function Details: 1. **add_task(task_list, task, priority)**: - **Input**: - `task_list` (list): A list used as the heap to store tasks. - `task` (str): A string representing the task. - `priority` (int): An integer representing the priority of the task. - **Output**: None 2. **remove_task(task_list, task, entry_finder, removed_marker)**: - **Input**: - `task_list` (list): A list used as the heap to store tasks. - `task` (str): A string representing the task. - `entry_finder` (dict): A dictionary that maps tasks to their corresponding heap entries. - `removed_marker` (str): A string used to mark removed tasks. - **Output**: None 3. **pop_task(task_list, entry_finder, removed_marker)**: - **Input**: - `task_list` (list): A list used as the heap to store tasks. - `entry_finder` (dict): A dictionary that maps tasks to their corresponding heap entries. - `removed_marker` (str): A string used to mark removed tasks. - **Output**: - Returns the task with the highest priority. 4. **get_n_highest_priority_tasks(task_list, entry_finder, removed_marker, n)**: - **Input**: - `task_list` (list): A list used as the heap to store tasks. - `entry_finder` (dict): A dictionary that maps tasks to their corresponding heap entries. - `removed_marker` (str): A string used to mark removed tasks. - `n` (int): The number of highest priority tasks to return. - **Output**: - Returns a list of `n` highest priority tasks. # Constraints: - `task_list` can contain up to 10^6 tasks. - Each task is represented as a string with a maximum length of 100 characters. - The priority is an integer and can range from -10^6 to 10^6. - Up to 10^5 operations can be performed in total. - You may assume that the number of unique tasks is significantly smaller than the number of operations. # Example Usage: ```python task_list = [] entry_finder = {} removed_marker = \'<removed-task>\' add_task(task_list, \'task1\', 2) add_task(task_list, \'task2\', 3) add_task(task_list, \'task3\', 1) print(pop_task(task_list, entry_finder, removed_marker)) # Output: task3 remove_task(task_list, \'task2\', entry_finder, removed_marker) print(get_n_highest_priority_tasks(task_list, entry_finder, removed_marker, 2)) # Output: [\'task1\'] ``` **Note:** Ensure that the functions are efficient and handle edge cases such as adding a task that already exists or trying to remove a task that does not exist.","solution":"import heapq def add_task(task_list, task, priority, entry_finder): Adds a new task with a given priority to the task list. if task in entry_finder: remove_task(task_list, task, entry_finder, \'<removed-task>\') entry = [priority, task] entry_finder[task] = entry heapq.heappush(task_list, entry) def remove_task(task_list, task, entry_finder, removed_marker): Removes a specific task from the task list. Marks the entry as removed. entry = entry_finder.pop(task) entry[-1] = removed_marker def pop_task(task_list, entry_finder, removed_marker): Fetches the task with the highest priority from the task list and removes it. while task_list: priority, task = heapq.heappop(task_list) if task != removed_marker: del entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') def get_n_highest_priority_tasks(task_list, entry_finder, removed_marker, n): Returns the n highest priority tasks without modifying the task list. result = [] temp_heap = [] temp_entries = {} # Clone task_list and entry_finder to temp_heap and temp_entries for entry in task_list: heapq.heappush(temp_heap, entry) for task, entry in entry_finder.items(): temp_entries[task] = temp_heap[temp_heap.index(entry)] while len(result) < n and temp_heap: priority, task = heapq.heappop(temp_heap) if task != removed_marker: result.append(task) return result"},{"question":"**Coding Question: Seaborn Percentile Visualization** # Objective: Use the seaborn library to visualize the distribution of diamond prices across different cuts using percentile calculations. # Task: 1. Load the diamonds dataset using seaborn\'s `load_dataset` function. 2. Create a `Plot` object to visualize the relationship between the `cut` and `price` of diamonds. 3. Scale the y-axis of the plot using a logarithmic scale. 4. Add a `Dot` layer to the plot with percentile calculations representing: - The default quartiles and min/max. - Ten evenly spaced percentiles. - Custom percentiles at 10, 25, 50, 75, and 90. 5. Create a separate plot combining dot and range visualizations to show a percentile interval for the `price` of diamonds based on their `cut`. Adjust visual aesthetics as needed. # Input and Output: - **Input**: None required. The dataset is loaded within the function. - **Output**: The function should display the two seaborn visualizations as described. # Constraints: - Utilize seaborn\'s `objects` module to create the visualizations. - Ensure the plots are clearly labeled and aesthetically pleasing. # Function Signature: ```python def visualize_diamond_prices(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load dataset diamonds = load_dataset(\\"diamonds\\") # Plot 1: Quartiles and Percentiles p1 = ( so.Plot(diamonds, \\"cut\\", \\"price\\") .scale(y=\\"log\\") ) p1.add(so.Dot(), so.Perc()) p1.add(so.Dot(), so.Perc(10)) p1.add(so.Dot(), so.Perc([10, 25, 50, 75, 90])) p1.show() # Plot 2: Combined Dot and Range visualization p2 = ( so.Plot(diamonds, \\"price\\", \\"cut\\") .add(so.Dots(pointsize=1, alpha=.2), so.Jitter(.3)) .add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=.2)) .scale(x=\\"log\\") ) p2.show() # Show plots plt.show() ``` Implement the `visualize_diamond_prices` function according to the specifications above.","solution":"def visualize_diamond_prices(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load dataset diamonds = load_dataset(\\"diamonds\\") # Plot 1: Quartiles and Percentiles p1 = ( so.Plot(diamonds, \\"cut\\", \\"price\\") .scale(y=\\"log\\") ) p1.add(so.Dot(), so.Perc()) p1.add(so.Dot(), so.Perc(10)) p1.add(so.Dot(), so.Perc([10, 25, 50, 75, 90])) p1.show() # Plot 2: Combined Dot and Range visualization p2 = ( so.Plot(diamonds, \\"price\\", \\"cut\\") .add(so.Dots(pointsize=1, alpha=.2), so.Jitter(.3)) .add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=.2)) .scale(x=\\"log\\") ) p2.show() # Show plots plt.show()"},{"question":"You are tasked with implementing a machine learning pipeline that utilizes one of the toy datasets from scikit-learn. Specifically, you will focus on the wine dataset. # Objective Write a Python function that: 1. Loads the wine dataset using scikit-learn. 2. Splits the dataset into training and testing sets. 3. Trains a Random Forest Classifier on the training set. 4. Evaluates the model on the test set and returns the accuracy. # Function Signature ```python def wine_classifier_evaluation() -> float: pass ``` # Constraints - Use an 80-20 split for training and testing sets. - Use `random_state=42` for reproducibility in data splitting and model training. - You should use default hyperparameters for the Random Forest Classifier. # Expected Input and Output - **Input:** None - **Output:** A float representing the accuracy of the classifier on the test set. # Example ```python accuracy = wine_classifier_evaluation() print(accuracy) # Output might vary but should be a float value between 0 and 1, indicating the model\'s accuracy. ``` # Instructions 1. Ensure that you import the necessary modules: ```python from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score ``` 2. Load the wine dataset: ```python data = load_wine() X, y = data.data, data.target ``` 3. Split the dataset into training and testing sets using an 80-20 split: ```python X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) ``` 4. Train a Random Forest Classifier on the training set: ```python clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) ``` 5. Evaluate the model on the test set and return the accuracy: ```python y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy ``` Implement the `wine_classifier_evaluation` function based on the instructions provided. Your submission should only include the function implementation.","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def wine_classifier_evaluation() -> float: Loads the wine dataset, splits it into training and testing sets, trains a Random Forest Classifier, and evaluates the accuracy on the test set. Returns: float: Accuracy of the classifier on the test set. # Load the wine dataset data = load_wine() X, y = data.data, data.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a Random Forest Classifier clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Evaluate the model on the test set y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Python Versioning System You are tasked with implementing a set of functions to handle and manipulate Python version numbers based on the documentation provided. **Function 1:** `decode_version_hex(version_hex)` This function should accept an integer representing the `PY_VERSION_HEX` and return a dictionary with the following components: - `major` - `minor` - `micro` - `release_level` - `release_serial` **Input:** - An integer `version_hex` representing the encoded version number. **Output:** - A dictionary containing the decoded components. ```python def decode_version_hex(version_hex): \'\'\' Decode the Python version hex number into its respective components. Parameters: version_hex (int): The Python version number encoded in a single integer. Returns: dict: A dictionary containing major, minor, micro, release_level, and release_serial. \'\'\' pass ``` **Function 2:** `encode_version(major, minor, micro, release_level, release_serial)` This function should accept the individual components of a version number and return the corresponding encoded hexadecimal integer which follows the `PY_VERSION_HEX` format. **Input:** - Integers representing `major`, `minor`, `micro`, `release_level`, and `release_serial`. **Output:** - A single integer representing the encoded version number. ```python def encode_version(major, minor, micro, release_level, release_serial): \'\'\' Encode the given Python version components into a single hex number. Parameters: major (int): The major version number. minor (int): The minor version number. micro (int): The micro version number. release_level (int): The release level (0xA for alpha, 0xB for beta, 0xC for release candidate, 0xF for final). release_serial (int): The release serial number. Returns: int: A single integer representing the encoded version number. \'\'\' pass ``` # Constraints and Assumptions: 1. `major`, `minor`, `micro` are non-negative integers. 2. `release_level` can only take values from the set {0xA, 0xB, 0xC, 0xF}. 3. `release_serial` is a non-negative integer. 4. Ensure the encoded number fits within a 32-bit integer. # Example Usage and Testing To help clarify the implementation, here is an example usage of the functions: ```python version_hex = 0x030401a2 components = decode_version_hex(version_hex) print(components) # Output should be {\'major\': 3, \'minor\': 4, \'micro\': 1, \'release_level\': 0xA, \'release_serial\': 2} encoded_version = encode_version(3, 4, 1, 0xA, 2) print(encoded_version) # Output should be 0x030401a2 ``` Your task involves implementing the two functions as specified. Ensure your code handles potential edge cases and provides correct results as per the example usage shown.","solution":"def decode_version_hex(version_hex): \'\'\' Decode the Python version hex number into its respective components. Parameters: version_hex (int): The Python version number encoded in a single integer. Returns: dict: A dictionary containing major, minor, micro, release_level, and release_serial. \'\'\' major = (version_hex >> 24) & 0xFF minor = (version_hex >> 16) & 0xFF micro = (version_hex >> 8) & 0xFF release_level = (version_hex >> 4) & 0xF release_serial = version_hex & 0xF return { \'major\': major, \'minor\': minor, \'micro\': micro, \'release_level\': release_level, \'release_serial\': release_serial } def encode_version(major, minor, micro, release_level, release_serial): \'\'\' Encode the given Python version components into a single hex number. Parameters: major (int): The major version number. minor (int): The minor version number. micro (int): The micro version number. release_level (int): The release level (0xA for alpha, 0xB for beta, 0xC for release candidate, 0xF for final). release_serial (int): The release serial number. Returns: int: A single integer representing the encoded version number. \'\'\' version_hex = (major << 24) | (minor << 16) | (micro << 8) | (release_level << 4) | release_serial return version_hex"},{"question":"Objective: You are required to implement a function that performs various common pathname manipulations using the `os.path` module. This task will assess your understanding of different `os.path` functions and how to use them together to achieve the desired outcome. Problem Statement: Write a Python function `organize_files(base_path: str, files: List[str]) -> Dict[str, List[str]]` that takes a base directory path and a list of file paths. Your task is to: 1. Group the files by their extensions. 2. For each group, maintain an entry in the dictionary where the key is the extension and the value is a list of file names with that extension. 3. The file names should be relative to the given `base_path`. Input: - `base_path`: A string representing the base directory path. This path will be absolute. - `files`: A list of strings where each string is an absolute path to a file. Output: - A dictionary where keys are file extensions (including the dot, e.g., \\".txt\\") and values are lists of file names (relative to `base_path`) with those extensions. Constraints: - You can assume that all paths provided in `files` exist and are valid. - The `base_path` and `files` will be absolute paths. - The function should handle paths on both Unix and Windows systems. Example: ```python base_path = \\"/home/user/documents\\" files = [ \\"/home/user/documents/report.doc\\", \\"/home/user/documents/sheet.xlsx\\", \\"/home/user/documents/presentation.pptx\\", \\"/home/user/documents/subdir/notes.txt\\" ] result = organize_files(base_path, files) # Expected Output: # { # \\".doc\\": [\\"report.doc\\"], # \\".xlsx\\": [\\"sheet.xlsx\\"], # \\".pptx\\": [\\"presentation.pptx\\"], # \\".txt\\": [\\"subdir/notes.txt\\"] # } ``` Implementation: You are required to use appropriate functions from the `os.path` module to achieve the result. Do not use any other third-party library for path manipulations. Make sure your code is readable, well-commented, and handles edge cases appropriately. ```python from typing import List, Dict import os def organize_files(base_path: str, files: List[str]) -> Dict[str, List[str]]: # Your implementation here pass ``` Notes: - Make sure to test your function with paths containing different drive names on Windows, symbolic links, and directory structures. - Consider edge cases such as empty file lists and paths without extensions.","solution":"from typing import List, Dict import os def organize_files(base_path: str, files: List[str]) -> Dict[str, List[str]]: organized_files = {} for file_path in files: # Get file extension _, extension = os.path.splitext(file_path) # Get relative path to base path relative_path = os.path.relpath(file_path, base_path) # Add to the dictionary if extension not in organized_files: organized_files[extension] = [] organized_files[extension].append(relative_path) return organized_files"},{"question":"# Windowing Operations in pandas Suppose you are given daily stock prices for multiple companies in a DataFrame where each column represents a company\'s stock price. Calculate the following for each stock: 1. **Rolling Mean**: Compute the rolling mean over a 7-day window. 2. **Exponential Moving Average (EMA)**: Compute the 10-day exponential moving average. 3. **Rolling Standard Deviation**: Calculate the rolling standard deviation over a 7-day window. 4. **Weighted Rolling Mean**: Compute the weighted rolling mean over a 7-day window with a triangular window type. However, there are some constraints: - Any calculation should skip NaN values. - You should use a `BaseIndexer` subclass if you choose to implement additional custom window operations. Implement the following function: ```python import pandas as pd def compute_stock_indicators(df): Computes various stock indicators for the given DataFrame Args: df (pd.DataFrame): DataFrame containing stock prices with each column representing a stock. Returns: pd.DataFrame: DataFrame with multi-level columns where the top level is the original stock name and the second level includes the computed indicators (\'RollingMean\', \'EMA\', \'RollingStd\', \'WeightedRollingMean\'). # Your code here ``` # Input - `df`: A pandas DataFrame where each column represents the daily stock price for a company. # Output - A pandas DataFrame with multi-level columns consisting of: - Original column names (stock names) - The computed indicators (\'RollingMean\', \'EMA\', \'RollingStd\', \'WeightedRollingMean\') # Example Given the input DataFrame: ```python data = { \'Stock_A\': [10, 11, 12, 13, 12, 11, 12, 14, 15, 16], \'Stock_B\': [20, 21, 19, 18, 17, 16, 15, 17, 18, 19] } df = pd.DataFrame(data) ``` The expected output could be: ```plaintext Stock_A Stock_B RollingMean EMA RollingStd WeightedRollingMean RollingMean EMA RollingStd WeightedRollingMean 0 NaN NaN NaN NaN NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN NaN NaN NaN 2 NaN NaN NaN NaN NaN NaN NaN NaN 3 NaN NaN NaN NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN NaN NaN NaN NaN 5 NaN NaN NaN NaN NaN NaN NaN NaN 6 11.571429 11.0 0.786796 11.571429 17.571429 19.0 0.786796 17.571429 7 11.428571 11.4 0.976224 11.428571 16.571429 18.4 0.976224 16.571429 8 12.000000 11.92 1.066003 12.000000 17.000000 18.92 1.066003 17.000000 9 12.428571 12.736 1.090488 12.428571 17.428571 19.736 1.090488 17.428571 ``` Your task is to implement this function such that it handles the computations efficiently and correctly. Be sure to handle edge cases where there may be NaN values in the input DataFrame.","solution":"import pandas as pd def compute_stock_indicators(df): Computes various stock indicators for the given DataFrame Args: df (pd.DataFrame): DataFrame containing stock prices with each column representing a stock. Returns: pd.DataFrame: DataFrame with multi-level columns where the top level is the original stock name and the second level includes the computed indicators (\'RollingMean\', \'EMA\', \'RollingStd\', \'WeightedRollingMean\'). indicators = [\'RollingMean\', \'EMA\', \'RollingStd\', \'WeightedRollingMean\'] result = pd.DataFrame() for stock in df.columns: stock_data = df[stock] # Compute rolling mean over 7-day window rolling_mean = stock_data.rolling(window=7, min_periods=1).mean() # Compute 10-day Exponential Moving Average (EMA) ema = stock_data.ewm(span=10, adjust=False).mean() # Compute rolling standard deviation over 7-day window rolling_std = stock_data.rolling(window=7, min_periods=1).std() # Compute weighted rolling mean with triangular window type over 7-day window weighted_rolling_mean = stock_data.rolling(window=7, min_periods=1, win_type=\'triang\').mean() # Combine all indicators into a dictionary indicator_data = { (stock, \'RollingMean\'): rolling_mean, (stock, \'EMA\'): ema, (stock, \'RollingStd\'): rolling_std, (stock, \'WeightedRollingMean\'): weighted_rolling_mean } stock_indicators_df = pd.DataFrame(indicator_data) result = pd.concat([result, stock_indicators_df], axis=1) return result"},{"question":"**Advanced Visualization with Seaborn: Customizing Jitter Effects** **Objective:** Evaluate the student\'s ability to use Seaborn for creating advanced plots with jitter effects and understand the impact of various jitter parameters. **Problem Statement:** You are provided with the `penguins` dataset available through the Seaborn library. Your task is to create various visualizations using `seaborn.objects` module to understand and demonstrate the effects of jittering. **Instructions:** 1. **Load the Dataset:** Load the `penguins` dataset using the `seaborn.load_dataset()` method. 2. **Visualization Tasks:** a. Create a plot that displays the relationship between the species (x-axis) and their body mass (y-axis) using `seaborn.objects`: - Use default jitter effect. b. Modify the jitter effect by setting the `width` parameter to 0.5 and observe the impact on the plot. c. Create another plot to depict the relationship between body mass (x-axis) and flipper length (y-axis): - Use jitter effect with the `width` parameter set to a default value. - Next, tweak the jitter effect by setting the `x` jitter to 100. d. Combine `x` and `y` jitter parameters to create an advanced visualization, set `x` to 200 and `y` to 5, and display the relationship between rounded body mass (to the nearest 1000) and rounded flipper length (to the nearest 10). **Expected Input and Output:** - **Input:** Seaborn\'s `penguins` dataset. - **Output:** 1. A visual plot with default jitter. 2. A visual plot with `width` jitter set to 0.5. 3. Two visual plots showing the impact of `width`, `x`, and `y` jitter parameters individually. 4. A final visual plot showing a combination of `x` and `y` jitter at 200 and 5 respectively. **Constraints and Requirements:** 1. Use the Seaborn library for plotting. 2. Demonstrate a clear understanding of the impact of various jitter parameters by comparing the different plots visually. **Performance Requirements:** - The code should execute efficiently. - Ensure plots are clearly labelled and visually distinguishable. **Example Code Format:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Task a: Default jitter default_jitter_plot = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) ) # Task b: Width jitter width_05_jitter_plot = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.jitter(0.5)) ) # Task c: Width and x jitter default_width_x_jitter_plot = ( so.Plot(penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"]) .add(so.Dots(), so.jitter()) ) x_jitter_100_plot = ( so.Plot(penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"]) .add(so.Dots(), so.Jitter(x=100)) ) # Task d: Combined x and y jitter combined_jitter_plot = ( so.Plot( penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"].round(-1), ) .add(so.Dots(), so.Jitter(x=200, y=5)) ) # Display plots (choose your method of displaying like plt.show() etc.) ``` **Note:** Ensure to label all plots appropriately for clarity.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Task a: Default jitter def create_default_jitter_plot(): plot = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) ) return plot # Task b: Width jitter def create_width_jitter_plot(): plot = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.5)) ) return plot # Task c: Width and x jitter def create_default_width_x_jitter_plot(): plot = ( so.Plot(penguins, \\"body_mass_g\\", \\"flipper_length_mm\\") .add(so.Dots(), so.Jitter()) ) return plot def create_x_jitter_100_plot(): plot = ( so.Plot(penguins, \\"body_mass_g\\", \\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(x=100)) ) return plot # Task d: Combined x and y jitter def create_combined_jitter_plot(): plot = ( so.Plot( penguins.assign( body_mass_g_rounded=penguins[\\"body_mass_g\\"].round(-3), flipper_length_mm_rounded=penguins[\\"flipper_length_mm\\"].round(-1) ), \\"body_mass_g_rounded\\", \\"flipper_length_mm_rounded\\" ) .add(so.Dots(), so.Jitter(x=200, y=5)) ) return plot # To display the plots, use a method like plot.show() or plt.show()"},{"question":"Objective: Write a Python function that utilizes the `importlib.metadata` package to gather specific metadata about an installed Python package and format this information for reporting. Task: 1. Implement a function `package_report(package_name: str) -> str` that accepts the name of an installed package and returns a formatted string containing: - The package version. - A list of files included in the package. - A list of package requirements. - Any entry points within the `console_scripts` group. 2. If the package is not installed or any metadata information is missing, the function should handle these cases gracefully and return an appropriate message. Expected Input and Output: **Input:** - `package_name`: A string representing the name of an installed package. **Output:** - A string containing the formatted package metadata information. Example: ```python package_name = \\"example_package\\" # Assume this package is installed print(package_report(package_name)) ``` **Expected Output:** ``` Package: example_package Version: 1.0.0 Files: - example_package/__init__.py - example_package/module.py Requirements: - numpy (>=1.18.0) - requests Entry Points (console_scripts): - example: example_package.cli:main ``` Constraints and Limitations: - The function should handle cases where a package name does not exist or metadata information is not accessible. - The function should utilize the `importlib.metadata` package functionalities, such as `version`, `files`, `requires`, and `entry_points`. - Performance consideration: The function should be efficient and avoid unnecessary computations. Implementation Hints: - Use `importlib.metadata.version` to get the package version. - Use `importlib.metadata.files` to get the list of package files. - Use `importlib.metadata.requires` to get the list of package requirements. - Use `importlib.metadata.entry_points` to get entry points and filter for `console_scripts`.","solution":"import importlib.metadata from typing import List, Optional def package_report(package_name: str) -> str: try: # Get package version version = importlib.metadata.version(package_name) except importlib.metadata.PackageNotFoundError: return f\\"Package \'{package_name}\' is not installed.\\" # Get package files try: files = importlib.metadata.files(package_name) file_list = \\"n- \\".join(str(file) for file in files) if files else \\"None\\" except Exception: file_list = \\"Could not retrieve files.\\" # Get package requirements try: requirements = importlib.metadata.requires(package_name) requirements_list = \\"n- \\".join(requirements) if requirements else \\"None\\" except Exception: requirements_list = \\"Could not retrieve requirements.\\" # Get entry points try: entry_points = importlib.metadata.entry_points() console_scripts = [ f\\"{ep.name}: {ep.value}\\" for ep in entry_points.get(\\"console_scripts\\", []) if ep.group == \\"console_scripts\\" ] entry_points_list = \\"n- \\".join(console_scripts) if console_scripts else \\"None\\" except Exception: entry_points_list = \\"Could not retrieve entry points.\\" return (f\\"Package: {package_name}n\\" f\\"Version: {version}n\\" f\\"Files:n- {file_list}n\\" f\\"Requirements:n- {requirements_list}n\\" f\\"Entry Points (console_scripts):n- {entry_points_list}\\")"},{"question":"# Resource Management and Monitoring in Python Given the following scenario: You are developing a Python application that performs heavy computations and file processing. To prevent the application from consuming too many system resources (which could affect other applications running on the same system), you need to set and monitor various resource limits. Your task is to write a set of functions to perform the following: 1. **Set Resource Limits**: Write a function to set resource limits for the application. 2. **Monitor Resource Usage**: Write a function to get the current resource usage. 3. **Safe Execution**: Create a decorator that will apply the resource limits and monitor resource usage. Requirements 1. **Function to Set Resource Limits**: ```python def set_resource_limits(cpu_time_limit, file_size_limit): Sets the resource limits for CPU time and file size. Parameters: cpu_time_limit (int): Maximum CPU time in seconds. file_size_limit (int): Maximum file size in bytes. Returns: None ``` 2. **Function to Get Resource Usage**: ```python def get_resource_usage(): Retrieves the current resource usage of the process. Returns: dict: A dictionary with resource usage metrics such as user time, system time, maximum resident set size, page faults, etc. ``` 3. **Decorator for Safe Execution**: ```python import functools def resource_limited_execution(cpu_time_limit, file_size_limit): Decorator to apply resource limits and monitor usage. Parameters: cpu_time_limit (int): Maximum CPU time in seconds. file_size_limit (int): Maximum file size in bytes. Returns: function: The decorated function with applied resource limits and usage monitoring. def decorator(func): @functools.wraps(func) def wrapper(*args, **kwargs): set_resource_limits(cpu_time_limit, file_size_limit) try: result = func(*args, **kwargs) finally: usage = get_resource_usage() print(f\\"Resource usage: {usage}\\") return result return wrapper return decorator ``` # Examples 1. **Setting Resource Limits**: ```python set_resource_limits(10, 10485760) # 10 seconds CPU time limit, 10 MB file size limit ``` 2. **Monitoring Resource Usage**: ```python usage = get_resource_usage() print(usage) ``` 3. **Using the Decorator**: ```python @resource_limited_execution(10, 10485760) def heavy_computation_task(): # Your heavy computation logic here pass heavy_computation_task() ``` Constraints: - Assume the limits provided to `set_resource_limits` should be positive integers. - The decorator should ensure resource limits are enforced and resource usage is printed upon completion or exception in the decorated function. # Notes: - Handle exceptions properly where needed, especially when setting limits. - Be mindful of the platform-specific availability of certain resources and use appropriate fallbacks where necessary.","solution":"import resource import functools import os def set_resource_limits(cpu_time_limit, file_size_limit): Sets the resource limits for CPU time and file size. Parameters: cpu_time_limit (int): Maximum CPU time in seconds. file_size_limit (int): Maximum file size in bytes. Returns: None # Set CPU time limit resource.setrlimit(resource.RLIMIT_CPU, (cpu_time_limit, cpu_time_limit)) # Set file size limit resource.setrlimit(resource.RLIMIT_FSIZE, (file_size_limit, file_size_limit)) def get_resource_usage(): Retrieves the current resource usage of the process. Returns: dict: A dictionary with resource usage metrics such as user time, system time, maximum resident set size, page faults, etc. usage = resource.getrusage(resource.RUSAGE_SELF) return { \'user_time\': usage.ru_utime, \'system_time\': usage.ru_stime, \'max_resident_set_size\': usage.ru_maxrss, \'page_faults\': usage.ru_majflt, # Add other relevant usage metrics if needed } def resource_limited_execution(cpu_time_limit, file_size_limit): Decorator to apply resource limits and monitor usage. Parameters: cpu_time_limit (int): Maximum CPU time in seconds. file_size_limit (int): Maximum file size in bytes. Returns: function: The decorated function with applied resource limits and usage monitoring. def decorator(func): @functools.wraps(func) def wrapper(*args, **kwargs): set_resource_limits(cpu_time_limit, file_size_limit) try: result = func(*args, **kwargs) finally: usage = get_resource_usage() print(f\\"Resource usage: {usage}\\") return result return wrapper return decorator"},{"question":"Coding Assessment Question # Problem Statement You are given a log file containing multiple raw email messages, concatenated one after another. Each message is separated by a special delimiter string `\\"=====\\"`. You need to implement a function `parse_emails_from_log(log_file)`, which reads the log file, parses each individual email message, and returns a list of parsed email subjects. Use the appropriate parser class for handling potential non-blocking I/O operations, as the log file might be very large. # Function Signature ```python def parse_emails_from_log(log_file: str) -> list[str]: pass ``` # Input - `log_file`: A string representing the path to a log file. The log file contains multiple email messages separated by the delimiter string `\\"=====\\"`. # Output - A list of strings where each string is the subject of one of the parsed email messages. # Constraints - Each email message in the log file is complete. - The log file may contain a large number of email messages (up to 10,000), and each email may have a size up to a few MB. Therefore, memory efficiency is crucial. - Each email message is standards-compliant but may have minor deviations. - If an email does not contain a subject, it should return an empty string for that email. # Example Suppose the log file content is as follows: ``` From: user1@example.com Subject: Meeting Today To: user2@example.com Please join the meeting at 10 AM. ===== From: user3@example.com To: user4@example.com Reminder to submit the report. ===== From: user5@example.com Subject: Party Invitation To: user6@example.com You are invited to the party tomorrow! ``` After parsing, the function should return: ```python [\\"Meeting Today\\", \\"\\", \\"Party Invitation\\"] ``` # Notes - Use the `BytesFeedParser` class to handle the message parsing in an incremental manner. - Make sure to handle any potential defects in the email formatting gracefully, noting that each email parsing should be as robust as possible.","solution":"import email from email.parser import BytesFeedParser def parse_emails_from_log(log_file: str) -> list[str]: subjects = [] delimiter = b\\"=====n\\" with open(log_file, \'rb\') as f: message_data = [] for line in f: if line == delimiter: if message_data: subjects.append(parse_email_subject(b\'\'.join(message_data))) message_data = [] else: message_data.append(line) # Capture the last email if there\'s no delimiter at the end if message_data: subjects.append(parse_email_subject(b\'\'.join(message_data))) return subjects def parse_email_subject(raw_message: bytes) -> str: parser = BytesFeedParser() parser.feed(raw_message) message = parser.close() return message.get(\'subject\', \'\')"},{"question":"**K-Nearest Neighbors Classification with Custom Distance Metrics** **Objective:** Implement a K-Nearest Neighbors (KNN) classifier for a synthetic dataset. The classifier should support multiple distance metrics and allow the user to choose different algorithms to find the nearest neighbors. **Description:** You are required to write a function `custom_knn_classifier` that fits a KNN model to a given dataset and predicts the class labels for a test set. Your function should allow the user to specify the distance metric and the algorithm used for finding the nearest neighbors. **Function Signature:** ```python def custom_knn_classifier(X_train, y_train, X_test, n_neighbors, metric, algorithm): Fit a KNN model and predict class labels for the test set. Parameters: - X_train (numpy.ndarray): Training feature set. - y_train (numpy.ndarray): Training labels. - X_test (numpy.ndarray): Test feature set. - n_neighbors (int): Number of neighbors to use. - metric (str): The distance metric to use. Supported metrics: [\'euclidean\', \'manhattan\', \'chebyshev\', \'minkowski\']. - algorithm (str): Algorithm to compute the nearest neighbors. Supported algorithms: [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\']. Returns: - y_pred (numpy.ndarray): Predicted class labels for the test set. pass ``` **Input Format:** - `X_train`: A 2D NumPy array of shape (n_samples, n_features) representing the training feature set. - `y_train`: A 1D NumPy array of shape (n_samples,) containing the class labels for the training set. Class labels are integers starting from 0. - `X_test`: A 2D NumPy array of shape (m_samples, n_features) representing the test feature set. - `n_neighbors`: An integer specifying the number of neighbors to use for classification. - `metric`: A string indicating the distance metric to use. Choose from: `\'euclidean\'`, `\'manhattan\'`, `\'chebyshev\'`, or `\'minkowski\'`. - `algorithm`: A string specifying the algorithm to use for finding the nearest neighbors. Choose from: `\'auto\'`, `\'ball_tree\'`, `\'kd_tree\'`, or `\'brute\'`. **Output Format:** - `y_pred`: A 1D NumPy array of shape (m_samples,) containing the predicted class labels for the test set. **Example:** ```python from sklearn.datasets import make_classification import numpy as np # Generate a synthetic dataset X_train, y_train = make_classification(n_samples=100, n_features=5, n_classes=3, random_state=42) X_test, _ = make_classification(n_samples=10, n_features=5, n_classes=3, random_state=43) # Call the custom KNN classifier function y_pred = custom_knn_classifier(X_train, y_train, X_test, n_neighbors=3, metric=\'euclidean\', algorithm=\'auto\') print(y_pred) ``` **Constraints:** - The function should handle exceptions and edge cases such as: - Invalid input shapes. - Unsupported metric or algorithm. - Cases where `n_neighbors` is greater than the number of training samples. **Performance Requirements:** - The function should efficiently handle datasets with up to 10,000 samples and 100 features. **Evaluation:** Your implementation will be evaluated based on the correctness of the predicted class labels, the efficiency of the code, and the handling of different metrics and algorithms.","solution":"import numpy as np from sklearn.neighbors import KNeighborsClassifier def custom_knn_classifier(X_train, y_train, X_test, n_neighbors, metric, algorithm): Fit a KNN model and predict class labels for the test set. Parameters: - X_train (numpy.ndarray): Training feature set. - y_train (numpy.ndarray): Training labels. - X_test (numpy.ndarray): Test feature set. - n_neighbors (int): Number of neighbors to use. - metric (str): The distance metric to use. Supported metrics: [\'euclidean\', \'manhattan\', \'chebyshev\', \'minkowski\']. - algorithm (str): Algorithm to compute the nearest neighbors. Supported algorithms: [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\']. Returns: - y_pred (numpy.ndarray): Predicted class labels for the test set. # Validate input parameters if n_neighbors <= 0 or n_neighbors > X_train.shape[0]: raise ValueError(\\"n_neighbors must be between 1 and the number of training samples\\") if metric not in [\'euclidean\', \'manhattan\', \'chebyshev\', \'minkowski\']: raise ValueError(\\"metric must be one of [\'euclidean\', \'manhattan\', \'chebyshev\', \'minkowski\']\\") if algorithm not in [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\']: raise ValueError(\\"algorithm must be one of [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\']\\") # Create and fit the KNN model knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, algorithm=algorithm) knn.fit(X_train, y_train) # Predict labels for the test set y_pred = knn.predict(X_test) return y_pred"},{"question":"# Question: You are given a CSV file containing product reviews with the following structure: ``` review_id,product_id,review_text,rating,verified_purchase 1,1001,\\"Great product!\\",5,true 2,1002,\\"Not what I expected\\",2,false 3,1001,\\"Works well, but quite expensive\\",4,true 4,1003,\\"Terrible quality\\",1,false 5,1002,\\"Average performance\\",3,pd.NA ... ``` You need to implement a function that reads this CSV file into a pandas DataFrame and then: 1. Filters out reviews that have an unknown `verified_purchase` status (i.e., `pd.NA`). 2. For the remaining reviews, perform the following analysis using Kleene logic: - Add a new column `positive_review`, which is `True` if the rating is 4 or 5, otherwise `False`. - Add another column `highlight_review`, which is `True` if the review is verified and positive or if the review is positive and unverified purchase status is unknown (`pd.NA`). Finally, return the modified DataFrame. # Input: - `file_path` (str): The file path to the CSV file containing the product reviews. # Output: - A pandas DataFrame with three new columns: 1. `positive_review` (boolean) 2. `highlight_review` (boolean) 3. `filtered_reviews` (boolean): `True` indicates the review has defined boolean values, `False` indicates `NA`. # Example: Given the following reviews, the function would filter and process them as follows: CSV content: ``` review_id,product_id,review_text,rating,verified_purchase 1,1001,\\"Great product!\\",5,true 2,1002,\\"Not what I expected\\",2,false 3,1001,\\"Works well, but quite expensive\\",4,true 4,1003,\\"Terrible quality\\",1,false 5,1002,\\"Average performance\\",3,pd.NA ``` Function call: ```python df = process_review_file(\'reviews.csv\') ``` Expected DataFrame: ``` review_id ... positive_review highlight_review filtered_reviews 0 1 ... True True True 1 2 ... False False True 2 3 ... True True True 3 4 ... False False True ``` # Constraints: - The file path provided will be valid and the file will exist in the specified location. - NA values in `verified_purchase` should be handled as described. # Function Signature: ```python def process_review_file(file_path: str) -> pd.DataFrame: pass ``` # Note: Ensure you use the `pandas` library to leverage its nullable Boolean data type and handle `pd.NA` appropriately.","solution":"import pandas as pd def process_review_file(file_path: str) -> pd.DataFrame: # Read the CSV file into a pandas DataFrame df = pd.read_csv(file_path) # Remove rows where \'verified_purchase\' is NA filtered_df = df.dropna(subset=[\'verified_purchase\']) # Add a new column \'positive_review\' which is True if rating is 4 or 5, otherwise False filtered_df[\'positive_review\'] = filtered_df[\'rating\'].apply(lambda x: x >= 4) # Add a new column \'highlight_review\' filtered_df[\'highlight_review\'] = filtered_df.apply( lambda x: x[\'positive_review\'] and (x[\'verified_purchase\'] or pd.isna(x[\'verified_purchase\'])), axis=1 ) # Add a new column \'filtered_reviews\' to indicate reviews with defined boolean values filtered_df[\'filtered_reviews\'] = ~df[\'verified_purchase\'].isna() return filtered_df[[\'review_id\', \'product_id\', \'review_text\', \'rating\', \'verified_purchase\', \'positive_review\', \'highlight_review\', \'filtered_reviews\']]"},{"question":"# Problem: Data Filtering with Nullable Booleans You are provided with a dataset tracking employee attendance. This dataset includes a column indicating if an employee was present on a given day, where the presence status might be unknown (NA). You need to implement a function that filters out the employees who were present on a given day based on the provided dataset. To accomplish this, you need to make use of Pandas\' nullable boolean data type and its indexing features. Function Signature ```python def filter_present_employees(attendance_df, column_name): Filters the DataFrame to include only those employees who were present. Parameters: attendance_df (pd.DataFrame): A DataFrame containing employee attendance information. column_name (str): The name of the column with boolean attendance data. Returns: pd.DataFrame: A filtered DataFrame with only those employees who were present. ``` Input: - `attendance_df`: A Pandas DataFrame with at least one column containing attendance data. The attendance data is of nullable boolean type and may include `True`, `False`, and `NA`. - `column_name`: A string representing the column name within `attendance_df` which contains the attendance data. Output: - A Pandas DataFrame that includes only the rows where the employee was present (attendance is `True`). Example: ```python import pandas as pd import numpy as np data = { \'employee_id\': [101, 102, 103, 104, 105], \'attendance\': pd.array([True, False, pd.NA, True, pd.NA], dtype=\'boolean\') } attendance_df = pd.DataFrame(data) print(filter_present_employees(attendance_df, \'attendance\')) ``` Expected Output: ``` employee_id attendance 0 101 True 3 104 True ``` Constraints: - Utilize Pandas\' nullable boolean data type and associated functions. - Do not use dropna(). Instead, leverage indexing with boolean masks and appropriate handling of NA values as per the provided documentation.","solution":"import pandas as pd def filter_present_employees(attendance_df, column_name): Filters the DataFrame to include only those employees who were present. Parameters: attendance_df (pd.DataFrame): A DataFrame containing employee attendance information. column_name (str): The name of the column with boolean attendance data. Returns: pd.DataFrame: A filtered DataFrame with only those employees who were present. return attendance_df[attendance_df[column_name] == True]"},{"question":"**JSON Encoder and Decoder Implementation** # Question: You have been provided with a dataset in a JSON format which contains information about various users. Your task is to implement two functions: 1. `encode_data_to_json(data: dict) -> str`: This function should take a dictionary containing user specifications and convert it into a JSON formatted string. 2. `decode_json_to_data(json_str: str) -> dict`: This function should take a JSON formatted string and convert it back into a dictionary maintaining the same structure as the input. Both functions must handle exceptions and edge cases such as invalid data types or malformed JSON strings. # Input and Output: - `encode_data_to_json(data: dict) -> str` - **Input**: A dictionary. Example: `{\\"users\\": [{\\"name\\": \\"Alice\\", \\"age\\": 30}, {\\"name\\": \\"Bob\\", \\"age\\": 25}]}` - **Output**: A JSON string. Example: `\'{\\"users\\": [{\\"name\\": \\"Alice\\", \\"age\\": 30}, {\\"name\\": \\"Bob\\", \\"age\\": 25}]}\'` - `decode_json_to_data(json_str: str) -> dict` - **Input**: A JSON string. Example: `\'{\\"users\\": [{\\"name\\": \\"Alice\\", \\"age\\": 30}, {\\"name\\": \\"Bob\\", \\"age\\": 25}]}\'` - **Output**: A dictionary. Example: `{\\"users\\": [{\\"name\\": \\"Alice\\", \\"age\\": 30}, {\\"name\\": \\"Bob\\", \\"age\\": 25}]}` # Constraints: 1. The input dictionary for `encode_data_to_json` will contain nested dictionaries and lists. 2. The JSON string input for `decode_json_to_data` will be a properly formatted JSON string but ensure robustness by managing potential exceptions. 3. The functions should not use any third-party libraries; only the standard Python library is allowed. 4. Handling of ASCII and Unicode characters within JSON strings must be correctly implemented. 5. Ensure the solution is efficient in terms of time complexity. # Performance Requirements: - Your implementation should efficiently handle JSON strings and dictionaries with up to 1,000,000 nested elements. # Example Implementation: You can use the `json` module in Python to implement the methods. The following example showcases the expected outline: ```python import json def encode_data_to_json(data: dict) -> str: try: return json.dumps(data) except (TypeError, OverflowError) as e: raise ValueError(f\\"Invalid data provided: {e}\\") def decode_json_to_data(json_str: str) -> dict: try: return json.loads(json_str) except json.JSONDecodeError as e: raise ValueError(f\\"Invalid JSON string provided: {e}\\") # Example usage: data = {\\"users\\": [{\\"name\\": \\"Alice\\", \\"age\\": 30}, {\\"name\\": \\"Bob\\", \\"age\\": 25}]} json_str = encode_data_to_json(data) print(json_str) # Output: \'{\\"users\\": [{\\"name\\": \\"Alice\\", \\"age\\": 30}, {\\"name\\": \\"Bob\\", \\"age\\": 25}]}\' decoded_data = decode_json_to_data(json_str) print(decoded_data) # Output: {\'users\': [{\'name\': \'Alice\', \'age\': 30}, {\'name\': \'Bob\', \'age\': 25}]} ``` You are expected to handle edge cases and exceptions, ensuring meaningful error messages are provided when the input is invalid.","solution":"import json def encode_data_to_json(data: dict) -> str: Encodes a dictionary into a JSON formatted string. :param data: Dictionary containing user specifications :return: JSON formatted string :raises ValueError: If the input data is not serializable to JSON try: return json.dumps(data) except (TypeError, OverflowError) as e: raise ValueError(f\\"Invalid data provided: {e}\\") def decode_json_to_data(json_str: str) -> dict: Decodes a JSON formatted string into a dictionary. :param json_str: JSON formatted string :return: A dictionary corresponding to the JSON string :raises ValueError: If the input string is not a valid JSON try: return json.loads(json_str) except json.JSONDecodeError as e: raise ValueError(f\\"Invalid JSON string provided: {e}\\")"},{"question":"# Question: Import Modules from ZIP Archives You are tasked with implementing a function that imports a Python module from a given ZIP archive and executes a specified function from that imported module. The function should handle any potential errors gracefully using appropriate exception handling mechanisms. Function Signature ```python def import_and_execute(zip_path: str, module_name: str, function_name: str, *args, **kwargs): pass ``` Parameters - `zip_path` (str): The path to the ZIP archive. - `module_name` (str): The fully qualified name of the module to import from the ZIP archive. - `function_name` (str): The name of the function to execute in the imported module. - `args`: The positional arguments to pass to the function being executed. - `kwargs`: The keyword arguments to pass to the function being executed. Returns - The result returned by the executed function in the imported module. Raises - `FileNotFoundError`: If the specified ZIP archive doesn\'t exist. - `zipimport.ZipImportError`: If any issues occur during the importing of the module from the ZIP archive. - `AttributeError`: If the specified function does not exist in the imported module. Example ```python # Assuming the ZIP archive contains a module named \'example\' with a function called \'greet\' # /path/to/example.zip # â””â”€â”€ example.py # â””â”€â”€ def greet(name): # return f\\"Hello, {name}!\\" # Example usage: result = import_and_execute(\'/path/to/example.zip\', \'example\', \'greet\', \'Alice\') print(result) # Expected Output: \\"Hello, Alice!\\" ``` Constraints - Do not use any external libraries other than the standard `zipimport`, `sys`, and `os` modules. - Ensure proper handling of exceptions and return meaningful error messages if exceptions are raised. Implement the function `import_and_execute` to solve the problem described.","solution":"import zipimport import sys import os def import_and_execute(zip_path: str, module_name: str, function_name: str, *args, **kwargs): Imports a Python module from a given ZIP archive and executes a specified function from that imported module. Parameters: zip_path (str): The path to the ZIP archive. module_name (str): The fully qualified name of the module to import from the ZIP archive. function_name (str): The name of the function to execute in the imported module. args: The positional arguments to pass to the function being executed. kwargs: The keyword arguments to pass to the function being executed. Returns: The result returned by the executed function in the imported module. Raises: FileNotFoundError: If the specified ZIP archive doesn\'t exist. zipimport.ZipImportError: If any issues occur during the importing of the module from the ZIP archive. AttributeError: If the specified function does not exist in the imported module. try: if not os.path.isfile(zip_path): raise FileNotFoundError(f\\"The ZIP archive \'{zip_path}\' does not exist.\\") # Create a zipimporter instance for the ZIP file importer = zipimport.zipimporter(zip_path) # Import the specified module module = importer.load_module(module_name) # Retrieve the specified function from the module function = getattr(module, function_name) # Execute the function with the provided arguments return function(*args, **kwargs) except zipimport.ZipImportError: raise zipimport.ZipImportError(f\\"Failed to import module \'{module_name}\' from zip archive \'{zip_path}\'.\\") except AttributeError: raise AttributeError(f\\"The function \'{function_name}\' does not exist in module \'{module_name}\'.\\") # This is an example of usage. # Assuming the ZIP archive contains a module named \'example\' with a function called \'greet\': # result = import_and_execute(\'/path/to/example.zip\', \'example\', \'greet\', \'Alice\') # print(result) # Expected Output: \\"Hello, Alice!\\""},{"question":"# **Coding Assessment Question** **Objective:** Demonstrate your understanding of Python\'s `mailbox` module, particularly working with `MaildirMessage` and `mboxMessage` objects. **Task:** You need to implement a function `transfer_mail_messages` that: - Transfers messages from a **source** mailbox (Maildir format) to a **target** mailbox (mbox format). - Converts messagesâ€™ states and flags appropriately between `MaildirMessage` and `mboxMessage`. - Ensures that the message state is preserved during this transfer. **Function Signature:** ```python def transfer_mail_messages(source_dir: str, target_path: str) -> None: pass ``` **Input:** - `source_dir` (str): The directory path of the source Maildir mailbox. - `target_path` (str): The file path of the target mbox mailbox. **Output:** - The function returns `None` but performs the message transfer as specified. **Constraints:** - Source mailbox uses the Maildir format. You need to use `mailbox.Maildir` to interface with this mailbox. - Target mailbox uses the mbox format. You need to use `mailbox.mbox` to interface with this mailbox. - The function should handle any `mailbox.Error` exceptions gracefully. **Additional Requirements:** 1. Convert the state flags from `MaildirMessage` to `mboxMessage`. 2. Ensure the target mailbox file is updated after each addition (use `flush()`). 3. Handle concurrent modification safety â€” use `lock()` and `unlock()` methods around critical sections modifying the mailboxes. 4. Preserve the `From` line information for `mboxMessage`. # Example Usage ```python source_dir = \'~/Maildir\' target_path = \'~/mbox\' transfer_mail_messages(source_dir, target_path) ``` # Note: The provided example paths are for illustration. You should use valid paths on your filesystem. --- **Hints:** 1. Review the conversion tables between `MaildirMessage` and `mboxMessage`. 2. Make use of the `lock()` and `unlock()` methods when accessing mailboxes. 3. Ensure to handle exceptions to avoid partial transfer in case of errors.","solution":"import mailbox import os def transfer_mail_messages(source_dir: str, target_path: str) -> None: try: source_mailbox = mailbox.Maildir(source_dir) target_mailbox = mailbox.mbox(target_path) target_mailbox.lock() for message in source_mailbox: mbox_msg = mailbox.mboxMessage(message) mbox_msg.set_flags(message.get_flags()) target_mailbox.add(mbox_msg) target_mailbox.flush() except mailbox.Error as e: print(\\"Mailbox error:\\", e) finally: target_mailbox.unlock()"},{"question":"# Advanced Seaborn Visualization Task **Objective:** To assess your comprehension of fundamental and advanced concepts of plotting with seaborn, including handling different data structures (long-form, wide-form, and messy) and transforming data appropriately. **Problem Statement:** You are provided with a dataset that captures monthly temperatures recorded in different cities over several years. The dataset is in a wide-form format, where each column represents the temperatures for a specific month, and each row represents a different city. Your task is to: 1. Convert this dataset into a long-form format suitable for seaborn plotting. 2. Generate a line plot to visualize the monthly average temperature across all cities. 3. Generate a boxplot to compare the temperature distributions across different months. Given the following wide-form dataset (`temperature_data`): | City | Jan | Feb | Mar | Apr | May | Jun | Jul | Aug | Sep | Oct | Nov | Dec | |------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----| | CityA | 30 | 32 | 45 | 60 | 70 | 75 | 80 | 79 | 70 | 55 | 45 | 32 | | CityB | 40 | 42 | 50 | 65 | 72 | 78 | 85 | 83 | 70 | 60 | 50 | 42 | | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | **Tasks to Complete:** 1. **Data Transformation**: - Convert the wide-form `temperature_data` dataset into a long-form dataset. The long-form dataset should have columns: `City`, `Month`, and `Temperature`. 2. **Monthly Average Temperature Plot**: - Using the long-form dataset, create a line plot that shows the average temperature for each month across all cities. The x-axis should represent the months, and the y-axis should represent the average temperature. 3. **Temperature Distribution Boxplot**: - Create a boxplot using the long-form dataset to compare the temperature distributions across different months. The x-axis should represent the months, and the y-axis should represent the temperatures. **Constraints:** - Utilize the pandas library for data manipulation. - Utilize seaborn for plotting. - Ensure that month names are treated as ordered categorical data, so the plots\' x-axes display the months in the correct order. **Input Format:** - A pandas DataFrame named `temperature_data` in wide-form. **Output Format:** - Show the transformed long-form DataFrame. - Display a seaborn line plot showing the monthly average temperatures. - Display a seaborn boxplot comparing the temperature distributions across different months. # Example of Function Implementation in Python: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Sample Data (Wide-form) data = { \'City\': [\'CityA\', \'CityB\'], \'Jan\': [30, 40], \'Feb\': [32, 42], \'Mar\': [45, 50], \'Apr\': [60, 65], \'May\': [70, 72], \'Jun\': [75, 78], \'Jul\': [80, 85], \'Aug\': [79, 83], \'Sep\': [70, 70], \'Oct\': [55, 60], \'Nov\': [45, 50], \'Dec\': [32, 42] } temperature_data = pd.DataFrame(data) # Solution # 1. Data Transformation (Wide-form to Long-form) temperature_long = pd.melt(temperature_data, id_vars=[\'City\'], var_name=\'Month\', value_name=\'Temperature\') # Ensure the correct order of months months_order = [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"] temperature_long[\'Month\'] = pd.Categorical(temperature_long[\'Month\'], categories=months_order, ordered=True) temperature_long = temperature_long.sort_values(\'Month\') print(temperature_long) # Display transformed data # 2. Monthly Average Temperature Line Plot plt.figure(figsize=(10, 6)) sns.lineplot(data=temperature_long, x=\'Month\', y=\'Temperature\', estimator=\'mean\') plt.title(\'Monthly Average Temperature Across Cities\') plt.show() # 3. Temperature Distribution Boxplot plt.figure(figsize=(10, 6)) sns.boxplot(data=temperature_long, x=\'Month\', y=\'Temperature\') plt.title(\'Temperature Distribution Across Months\') plt.show() ``` # Additional Notes: - Ensure that the plots are properly labeled and have appropriate titles. - Handle any missing values accordingly if they exist in your dataset.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def transform_temperature_data(temperature_data): Converts wide-form temperature data into long-form format suitable for seaborn plotting. Args: - temperature_data (pd.DataFrame): Wide-form DataFrame with temperatures for each month as columns. Returns: - long_df (pd.DataFrame): Long-form DataFrame with columns \'City\', \'Month\', and \'Temperature\'. # Data Transformation (Wide-form to Long-form) temperature_long = pd.melt(temperature_data, id_vars=[\'City\'], var_name=\'Month\', value_name=\'Temperature\') # Ensure the correct order of months months_order = [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"] temperature_long[\'Month\'] = pd.Categorical(temperature_long[\'Month\'], categories=months_order, ordered=True) temperature_long = temperature_long.sort_values(\'Month\') return temperature_long def plot_monthly_average_temperature(temperature_long): Generates a seaborn line plot of the average temperature for each month across all cities. Args: - temperature_long (pd.DataFrame): Long-form DataFrame with columns \'City\', \'Month\', and \'Temperature\'. # Monthly Average Temperature Line Plot plt.figure(figsize=(10, 6)) sns.lineplot(data=temperature_long, x=\'Month\', y=\'Temperature\', estimator=\'mean\', ci=None) plt.title(\'Monthly Average Temperature Across Cities\') plt.xlabel(\'Month\') plt.ylabel(\'Average Temperature\') plt.show() def plot_temperature_distribution(temperature_long): Generates a seaborn boxplot to compare the temperature distributions across different months. Args: - temperature_long (pd.DataFrame): Long-form DataFrame with columns \'City\', \'Month\', and \'Temperature\'. # Temperature Distribution Boxplot plt.figure(figsize=(10, 6)) sns.boxplot(data=temperature_long, x=\'Month\', y=\'Temperature\') plt.title(\'Temperature Distribution Across Months\') plt.xlabel(\'Month\') plt.ylabel(\'Temperature\') plt.show()"},{"question":"Objective Write a Python function that uses the `seaborn` library to load the \\"diamonds\\" dataset and create a bar plot. The plot should show the median carat weight for each level of the \\"clarity\\" variable, grouped by the \\"cut\\" variable, with dodged bars for each \\"cut\\". Requirements: 1. Load the \\"diamonds\\" dataset using `seaborn.load_dataset()`. 2. Create a `Plot` object using `seaborn.objects` with \\"clarity\\" on the x-axis and \\"carat\\" on the y-axis. 3. Add a bar plot layer with median aggregation for \\"carat\\". 4. Group bars by \\"cut\\" using the `Dodge` transform. 5. Ensure the plot has proper labeling and is presentable. Function Signature ```python def create_dodged_median_barplot(): pass ``` Example Output A plot will be displayed showing the median carat weight for each clarity level with bars grouped and dodged by the \\"cut\\". Constraints: - Use only the methods and functions from the seaborn library demonstrated in the provided documentation. Additional Notes - Ensure seaborn and its dependencies are properly installed in your environment. - You may use additional `matplotlib` functions to enhance the plot if necessary but prioritize `seaborn` methods for the primary task. Expected Performance - The function should execute and generate the plot within a few seconds on a standard machine.","solution":"import seaborn as sns from matplotlib import pyplot as plt def create_dodged_median_barplot(): # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create a bar plot plot = sns.catplot( data=diamonds, kind=\\"bar\\", x=\\"clarity\\", y=\\"carat\\", hue=\\"cut\\", ci=\\"sd\\", palette=\\"dark\\", alpha=.6, height=6, estimator=lambda x: x.median(), # Use median as the aggregation function dodge=True # Group by cut ) # Add labels and title for the plot plot.set_axis_labels(\\"Clarity\\", \\"Median Carat Weight\\") plot.fig.suptitle(\\"Median Carat Weight by Clarity, Grouped by Cut\\", fontsize=16) plt.show()"},{"question":"**Kernel Density Estimation with scikit-learn** # Problem Statement You are given a dataset representing 2D points drawn from a bimodal distribution. Your task is to use Kernel Density Estimation (KDE) from `scikit-learn` to estimate the density of the points. Specifically, you need to implement a function `estimate_density` that: 1. Accepts a 2D numpy array `X` of shape (n_samples, 2) representing the dataset, a string `kernel_type` indicating the type of kernel to be used (default is `\'gaussian\'`), and a float `bandwidth` representing the bandwidth of the kernel (default is 0.5). 2. Fits a KDE model to the data using the specified kernel type and bandwidth. 3. Uses the fitted model to compute the log density of points in the dataset. 4. Plots the KDE result as a contour plot, showing the estimated densities over the range of the data. # Function Signature ```python import numpy as np def estimate_density(X: np.ndarray, kernel_type: str=\'gaussian\', bandwidth: float=0.5): Estimate the density of the given 2D data using Kernel Density Estimation. Parameters: - X: np.ndarray; shape (n_samples, 2). The input data. - kernel_type: str; default=\'gaussian\'. The type of kernel to use for KDE. - bandwidth: float; default=0.5. The bandwidth of the kernel. Returns: - log_density: np.ndarray; shape (n_samples,). The log of the estimated density at each point in X. pass ``` # Example Given the dataset defined as follows: ```python import numpy as np X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [8, 7], [6, 5], [4, 3], [2, 1], [0, 1]]) ``` 1. **Fit the KDE model**: Use `sklearn.neighbors.KernelDensity` to fit the KDE model to `X` with `kernel=\'gaussian\'` and `bandwidth=0.5`. 2. **Compute the log density**: Use the fitted model to compute the log density of the points in `X`. 3. **Create a contour plot**: Generate a mesh grid over the range of the data and plot the contours showing the estimated densities. # Performance Constraints - The function should be able to handle datasets up to size `(1000, 2)` efficiently. - Ensure that the KDE model is fit appropriately and the log density is computed accurately. # Additional Notes - You may use libraries such as `matplotlib` for generating plots. - Normalize the plot over the range of the data for better visualization.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def estimate_density(X, kernel_type=\'gaussian\', bandwidth=0.5): Estimate the density of the given 2D data using Kernel Density Estimation. Parameters: - X: np.ndarray; shape (n_samples, 2). The input data. - kernel_type: str; default=\'gaussian\'. The type of kernel to use for KDE. - bandwidth: float; default=0.5. The bandwidth of the kernel. Returns: - log_density: np.ndarray; shape (n_samples,). The log of the estimated density at each point in X. # Fit the KDE model kde = KernelDensity(kernel=kernel_type, bandwidth=bandwidth) kde.fit(X) # Compute the log density of the points in X log_density = kde.score_samples(X) # Create a mesh grid over the range of the data for plotting x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100)) grid = np.c_[xx.ravel(), yy.ravel()] # Compute the density on the grid z = np.exp(kde.score_samples(grid)).reshape(xx.shape) # Plot the contour plot plt.figure() plt.contourf(xx, yy, z, cmap=\'Blues\') plt.scatter(X[:, 0], X[:, 1], c=\'red\', s=5) plt.title(\'Kernel Density Estimation\') plt.xlabel(\'X-axis\') plt.ylabel(\'Y-axis\') plt.colorbar(label=\'Density\') plt.show() return log_density"},{"question":"# Question: Randomness and Reproducibility in PyTorch In machine learning experiments, controlling randomness is crucial for reproducibility. PyTorch provides the `torch.random` module to handle random number generation and seed setting. Objective: Create a function `generate_random_tensors` that generates and returns a list of random tensors. The function should also be able to set a seed to ensure that the randomness is reproducible. You will then use these random tensors in a simple linear regression problem. Function Signature: ```python def generate_random_tensors(seed: int, num_tensors: int, tensor_shape: tuple) -> list: Generates a list of random tensors of a specified shape. Args: - seed (int): The seed for the random number generator to ensure reproducibility. - num_tensors (int): The number of random tensors to generate. - tensor_shape (tuple): The shape of each random tensor. Returns: - list: A list of generated random tensors. ``` Constraints: - Each random tensor should be created using PyTorch and must be of type `torch.Tensor`. - The random tensors should be generated using a seed to ensure reproducibility. - You may assume `num_tensors` > 0 and `tensor_shape` is a valid shape for a tensor in PyTorch (e.g., (3, 3), (5,), etc.). - Use `torch.randn` for generating random tensors with values from a normal distribution. Example: ```python >>> tensors = generate_random_tensors(seed=42, num_tensors=3, tensor_shape=(2, 2)) >>> for tensor in tensors: ... print(tensor) Output: tensor([[ 0.3367, 0.1288], [ 0.2345, -0.7530]]) tensor([[ 0.9060, -0.7012], [-1.7206, 0.0926]]) tensor([[ 0.7350, 0.3216], [-1.3605, 0.1593]]) ``` Next Step: After generating the random tensors, use them to: 1. Create feature matrices (X) and corresponding target vectors (y) for a simple linear regression task. 2. Implement a simple linear regression model using PyTorch to fit these tensors and predict the targets. Show that the results are reproducible by running the regression twice with the same seed and verifying that the outputs are identical.","solution":"import torch def generate_random_tensors(seed: int, num_tensors: int, tensor_shape: tuple) -> list: Generates a list of random tensors of a specified shape. Args: - seed (int): The seed for the random number generator to ensure reproducibility. - num_tensors (int): The number of random tensors to generate. - tensor_shape (tuple): The shape of each random tensor. Returns: - list: A list of generated random tensors. torch.manual_seed(seed) return [torch.randn(tensor_shape) for _ in range(num_tensors)] # Example usage of the function to verify output (not part of the solution code) if __name__ == \\"__main__\\": tensors = generate_random_tensors(seed=42, num_tensors=3, tensor_shape=(2, 2)) for tensor in tensors: print(tensor)"},{"question":"Plot with Pandas Objective: Demonstrate your ability to use the Pandas plotting capabilities described in the documentation. # Problem Statement: You are provided with a CSV file, `store_sales.csv`, which represents the sales performance of a retail store. The dataset includes the following columns: - `Date`: Date of the sales record - `Store`: Store identifier - `Sales`: Sales amount in dollars - `Customers`: Number of customers on that day - `StateHoliday`: Whether that day was a state holiday (categories: `0`, `a`, `b`, `c`) - `SchoolHoliday`: Whether that day was a school holiday Your task is to implement the following series of functions to visualize the dataset effectively: 1. **Function 1: Line Plot of Sales Over Time** ```python def line_plot_sales_over_time(file_path: str): Reads the CSV file and creates a line plot of total sales over time. Params: - file_path: str : The path to the CSV file. Returns: None ``` - **Input:** - Path to the CSV file. - **Output:** - A line plot showing the daily total sales over time. - **Constraints:** - Ensure the line plot has appropriate axis labels and a title. 2. **Function 2: Bar Plot of Average Monthly Sales** ```python def bar_plot_avg_monthly_sales(file_path: str): Reads the CSV file and creates a bar plot of average sales for each month. Params: - file_path: str : The path to the CSV file. Returns: None ``` - **Input:** - Path to the CSV file. - **Output:** - A bar plot showing the average monthly sales. - **Constraints:** - Ensure the bar plot has appropriate axis labels and a title. - Use the \'month\' extracted from the \'Date\' column for x-axis. 3. **Function 3: Scatter Plot of Sales vs. Customers** ```python def scatter_plot_sales_vs_customers(file_path: str): Reads the CSV file and creates a scatter plot with Sales on the y-axis and Number of Customers on the x-axis. Params: - file_path: str : The path to the CSV file. Returns: None ``` - **Input:** - Path to the CSV file. - **Output:** - A scatter plot showing the relationship between sales and number of customers. - **Constraints:** - Ensure the scatter plot has appropriate axis labels and a title. 4. **Function 4: Boxplot of Sales by State Holiday** ```python def boxplot_sales_by_state_holiday(file_path: str): Reads the CSV file and creates a boxplot of sales grouped by state holidays. Params: - file_path: str : The path to the CSV file. Returns: None ``` - **Input:** - Path to the CSV file. - **Output:** - A boxplot showing the distribution of sales across different state holidays. - **Constraints:** - Ensure the boxplot has appropriate axis labels and a title. 5. **Function 5: Hexbin Plot of Sales and Customers** ```python def hexbin_plot_sales_and_customers(file_path: str): Reads the CSV file and creates a hexbin plot of Sales and Number of Customers. Params: - file_path: str : The path to the CSV file. Returns: None ``` - **Input:** - Path to the CSV file. - **Output:** - A hexbin plot for sales and number of customers. - **Constraints:** - Ensure the hexbin plot has appropriate axis labels and a title. Completion Requirements: - Implement all the specified functions. - Provide appropriate comments and docstrings in your code. - Ensure plots are well-formatted with appropriate titles and axis labels. - Use the given dataset to test your functions. Example Dataset (store_sales.csv): ``` Date,Store,Sales,Customers,StateHoliday,SchoolHoliday 2015-07-31,1,5263,555,0,1 2015-07-30,1,5020,546,0,1 2015-07-29,1,4782,523,0,1 ... ``` You can download the dataset `store_sales.csv` from this [link](https://raw.githubusercontent.com/pandas-dev/pandas/main/pandas/tests/io/data/csv/iris.csv). Good luck and happy coding!","solution":"import pandas as pd import matplotlib.pyplot as plt def line_plot_sales_over_time(file_path: str): Reads the CSV file and creates a line plot of total sales over time. Params: - file_path: str : The path to the CSV file. Returns: None df = pd.read_csv(file_path, parse_dates=[\'Date\']) df = df.groupby(\'Date\')[\'Sales\'].sum().reset_index() plt.figure(figsize=(10, 6)) plt.plot(df[\'Date\'], df[\'Sales\'], marker=\'o\') plt.title(\'Total Sales Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') plt.grid(True) plt.show() def bar_plot_avg_monthly_sales(file_path: str): Reads the CSV file and creates a bar plot of average sales for each month. Params: - file_path: str : The path to the CSV file. Returns: None df = pd.read_csv(file_path, parse_dates=[\'Date\']) df[\'Month\'] = df[\'Date\'].dt.month df = df.groupby(\'Month\')[\'Sales\'].mean().reset_index() plt.figure(figsize=(10, 6)) plt.bar(df[\'Month\'], df[\'Sales\'], color=\'orange\') plt.title(\'Average Monthly Sales\') plt.xlabel(\'Month\') plt.ylabel(\'Average Sales\') plt.xticks(range(1, 13)) plt.grid(axis=\'y\') plt.show() def scatter_plot_sales_vs_customers(file_path: str): Reads the CSV file and creates a scatter plot with Sales on the y-axis and Number of Customers on the x-axis. Params: - file_path: str : The path to the CSV file. Returns: None df = pd.read_csv(file_path) plt.figure(figsize=(10, 6)) plt.scatter(df[\'Customers\'], df[\'Sales\'], alpha=0.5) plt.title(\'Sales vs. Customers\') plt.xlabel(\'Number of Customers\') plt.ylabel(\'Sales\') plt.grid(True) plt.show() def boxplot_sales_by_state_holiday(file_path: str): Reads the CSV file and creates a boxplot of sales grouped by state holidays. Params: - file_path: str : The path to the CSV file. Returns: None df = pd.read_csv(file_path) plt.figure(figsize=(10, 6)) df.boxplot(column=\'Sales\', by=\'StateHoliday\', grid=False) plt.title(\'Sales by State Holiday\') plt.suptitle(\'\') plt.xlabel(\'State Holiday\') plt.ylabel(\'Sales\') plt.show() def hexbin_plot_sales_and_customers(file_path: str): Reads the CSV file and creates a hexbin plot of Sales and Number of Customers. Params: - file_path: str : The path to the CSV file. Returns: None df = pd.read_csv(file_path) plt.figure(figsize=(10, 6)) plt.hexbin(df[\'Customers\'], df[\'Sales\'], gridsize=30, cmap=\'Blues\') plt.colorbar(label=\'Count\') plt.title(\'Hexbin Plot of Sales and Customers\') plt.xlabel(\'Number of Customers\') plt.ylabel(\'Sales\') plt.grid(True) plt.show()"},{"question":"**Objective:** Demonstrate understanding of Python\'s `ast` module by parsing a Python function, transforming it in a specific way, and then generating the modified function code back from the AST. **Problem Statement:** You are given a piece of Python code that defines a function. Your task is to: 1. Parse this code into an Abstract Syntax Tree (AST). 2. Transform the AST by modifying the function so that all integer literals are doubled. 3. Generate and return the modified function as a string. **Function Signature:** ```python def double_integers_in_function_code(code: str) -> str: # your code here ``` **Input:** - A string `code` containing the source code of a Python function. For example: ```python def example_function(x, y): return x + y + 2 ``` **Output:** - A string containing the modified source code of the function where all integer literals are doubled. For example, given the above input, the output should be: ```python def example_function(x, y): return x + y + 4 ``` **Constraints:** - You can assume that the input will always be valid Python code defining a function. - Consider only integer literals within the function body for doubling. - The function name, parameters, and other parts of the code should remain unchanged. **Example:** Input: ```python def sample_function(a, b): result = a * 2 if result > 10: return result - 3 return result ``` Output: ```python def sample_function(a, b): result = a * 4 if result > 20: return result - 6 return result ``` **Instructions:** - Read the input code. - Parse the code into an AST using the `ast.parse` function. - Define a custom `ast.NodeTransformer` to double all integer literals. - Generate the modified source code from the transformed AST using `ast.unparse`. - Return the resulting code. ```python import ast def double_integers_in_function_code(code: str) -> str: class IntegerDoublingTransformer(ast.NodeTransformer): def visit_Constant(self, node): if isinstance(node.value, int): node.value *= 2 return self.generic_visit(node) tree = ast.parse(code) transformer = IntegerDoublingTransformer() transformer.visit(tree) return ast.unparse(tree) ``` **Task Requirements:** 1. Implement the `double_integers_in_function_code` function as described. 2. Ensure your code handles the example inputs and produces the expected outputs. 3. Your implementation should use the `ast` module effectively, demonstrating your understanding of AST manipulation.","solution":"import ast def double_integers_in_function_code(code: str) -> str: class IntegerDoublingTransformer(ast.NodeTransformer): def visit_Constant(self, node): if isinstance(node.value, int): node.value *= 2 return self.generic_visit(node) tree = ast.parse(code) transformer = IntegerDoublingTransformer() transformer.visit(tree) return ast.unparse(tree)"},{"question":"# **CUDA Device and Memory Management with PyTorch** **Objective:** Write a Python program using PyTorch to perform the following operations on a CUDA-enabled GPU: 1. **Initialize** the CUDA device. 2. **Retrieve and display** the number of available CUDA devices. 3. **Set the current device** to the first CUDA device. 4. **Display the properties** of the set CUDA device. 5. **Allocate a tensor** on the CUDA device. 6. **Synchronize** the CUDA device. 7. **Measure memory usage** and display the allocated memory, cached memory, and memory summary. 8. **Empty the CUDA cache**. 9. **Allocate another tensor**, perform a basic operation (addition) and display the result. 10. **Display memory usage again** to observe the changes. **Input and Output Formats:** - **Input:** No direct input from the user is required. - **Output:** Print statements for all the requested operations. ```python import torch # Step-by-step operations: # 1. Initialize the CUDA device torch.cuda.init() # 2. Retrieve and display the number of available CUDA devices device_count = torch.cuda.device_count() print(f\\"Number of available CUDA devices: {device_count}\\") # 3. Set the current device to the first CUDA device torch.cuda.set_device(0) current_device = torch.cuda.current_device() print(f\\"Current CUDA device set to: {current_device}\\") # 4. Display the properties of the set CUDA device device_properties = torch.cuda.get_device_properties(current_device) print(f\\"Properties of current CUDA device: {device_properties}\\") # 5. Allocate a tensor on the CUDA device tensor_on_cuda = torch.randn(1000, 1000, device=\'cuda:0\') print(f\\"Tensor allocated on CUDA device: {tensor_on_cuda.shape}\\") # 6. Synchronize the CUDA device torch.cuda.synchronize() print(\\"CUDA device synchronized.\\") # 7. Measure memory usage allocated_memory = torch.cuda.memory_allocated() cached_memory = torch.cuda.memory_reserved() memory_summary = torch.cuda.memory_summary() print(f\\"Allocated Memory: {allocated_memory}\\") print(f\\"Cached Memory: {cached_memory}\\") print(\\"Memory Summary:\\") print(memory_summary) # 8. Empty the CUDA cache torch.cuda.empty_cache() print(\\"CUDA cache emptied.\\") # 9. Allocate another tensor, perform a basic operation, and display the result tensor_on_cuda2 = torch.ones(1000, 1000, device=\'cuda:0\') result_tensor = tensor_on_cuda + tensor_on_cuda2 print(f\\"Result of addition on CUDA device: {result_tensor.shape}\\") # 10. Display memory usage again allocated_memory_after = torch.cuda.memory_allocated() cached_memory_after = torch.cuda.memory_reserved() memory_summary_after = torch.cuda.memory_summary() print(f\\"Allocated Memory After: {allocated_memory_after}\\") print(f\\"Cached Memory After: {cached_memory_after}\\") print(\\"Memory Summary After:\\") print(memory_summary_after) ``` **Constraints and Considerations:** - Ensure that the system running the code has a CUDA-capable GPU. - Use proper error handling to address the cases where CUDA is not available. - The operations should be efficient and utilize GPU resources effectively. **Performance Requirements:** The task should be executed reasonably fast, given the simplicity of tensor operations. Ensure that the code does not run into memory allocation issues or crashes due to insufficient GPU resources.","solution":"import torch def cuda_operations(): operations_log = [] # Step 1: Initialize the CUDA device if not torch.cuda.is_available(): return [\\"CUDA is not available.\\"] operations_log.append(\\"CUDA initialized.\\") # Step 2: Retrieve and display the number of available CUDA devices device_count = torch.cuda.device_count() operations_log.append(f\\"Number of available CUDA devices: {device_count}\\") # Step 3: Set the current device to the first CUDA device torch.cuda.set_device(0) current_device = torch.cuda.current_device() operations_log.append(f\\"Current CUDA device set to: {current_device}\\") # Step 4: Display the properties of the set CUDA device device_properties = torch.cuda.get_device_properties(current_device) operations_log.append(f\\"Properties of current CUDA device: {device_properties}\\") # Step 5: Allocate a tensor on the CUDA device tensor_on_cuda = torch.randn(1000, 1000, device=\'cuda:0\') operations_log.append(f\\"Tensor allocated on CUDA device: {tensor_on_cuda.shape}\\") # Step 6: Synchronize the CUDA device torch.cuda.synchronize() operations_log.append(\\"CUDA device synchronized.\\") # Step 7: Measure memory usage allocated_memory = torch.cuda.memory_allocated() cached_memory = torch.cuda.memory_reserved() memory_summary = torch.cuda.memory_summary() operations_log.append(f\\"Allocated Memory: {allocated_memory}\\") operations_log.append(f\\"Cached Memory: {cached_memory}\\") operations_log.append(\\"Memory Summary:\\") operations_log.append(memory_summary) # Step 8: Empty the CUDA cache torch.cuda.empty_cache() operations_log.append(\\"CUDA cache emptied.\\") # Step 9: Allocate another tensor, perform a basic operation, and display the result tensor_on_cuda2 = torch.ones(1000, 1000, device=\'cuda:0\') result_tensor = tensor_on_cuda + tensor_on_cuda2 operations_log.append(f\\"Result of addition on CUDA device: {result_tensor.shape}\\") # Step 10: Display memory usage again allocated_memory_after = torch.cuda.memory_allocated() cached_memory_after = torch.cuda.memory_reserved() memory_summary_after = torch.cuda.memory_summary() operations_log.append(f\\"Allocated Memory After: {allocated_memory_after}\\") operations_log.append(f\\"Cached Memory After: {cached_memory_after}\\") operations_log.append(\\"Memory Summary After:\\") operations_log.append(memory_summary_after) return operations_log"},{"question":"# Seaborn Coding Assessment Question **Objective:** To assess the students\' ability to use Seaborn\'s `seaborn.objects` module to create and customize line plots. **Problem Statement:** You are provided with two datasets: `dowjones` and `fmri`. Your task is to accomplish the following: 1. Using the `dowjones` dataset, create a line plot that shows the trend of the Dow Jones Industrial Average over time. Include the following customizations: - The x-axis should represent the date and the y-axis should represent the price. - The line should be blue. - Add markers (`\'o\'`) to indicate each data point. The markers should have white edges. 2. Using the `fmri` dataset, create a line plot showing the `signal` over `timepoint` for each `event`. Include the following customizations: - Group the data by `event` and `region`. - Color the lines by `region` and use different line styles for each `event`. - Add a shaded error band to represent the standard error around the line. **Detailed Requirements:** 1. **Dow Jones Line Plot:** - Input: None (The data is loaded within the provided code). - Output: Display the plot. - Constraints: Use `seaborn.objects.Plot` for the implementation. 2. **FMRI Line Plot:** - Input: None (The data is loaded within the provided code). - Output: Display the plot. - Constraints: Use `seaborn.objects.Plot` for the implementation. **Starter Code:** ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Dow Jones Line Plot def plot_dowjones(): # Create and customize the line plot plot = so.Plot(dowjones, x=\\"Date\\", y=\\"Price\\") plot.add(so.Line(color=\\"blue\\", marker=\\"o\\", edgecolor=\\"w\\")) # Display the plot plot.show() # FMRI Line Plot def plot_fmri(): # Filter specific data if needed p = so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") # Add lines with error bands p.add(so.Line()).add(so.Band()) # Display the plot p.show() # Call the functions to display plots plot_dowjones() plot_fmri() ``` Your task is to complete the `plot_dowjones` and `plot_fmri` functions to meet the specified requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Dow Jones Line Plot def plot_dowjones(): # Create and customize the line plot plot = ( so.Plot(dowjones, x=\\"Date\\", y=\\"Price\\") .add(so.Line(color=\\"blue\\", marker=\\"o\\", edgecolor=\\"white\\")) ) # Display the plot plot.show() # FMRI Line Plot def plot_fmri(): # Create and customize the line plot plot = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") .add(so.Line()) .add(so.Band()) ) # Display the plot plot.show() # Call the functions to display plots plot_dowjones() plot_fmri()"},{"question":"**Complex Tensor Operations with PyTorch** # Objective You are to implement a function that takes a list of real and imaginary parts in two separate lists, creates a complex tensor from these lists, performs certain operations, and returns a dictionary containing the results. # Function Signature ```python import torch def complex_tensor_operations(real_parts: list, imag_parts: list) -> dict: pass ``` # Input - `real_parts (list)`: A list of real parts of the complex numbers. (e.g., `[1, 2, 3]`) - `imag_parts (list)`: A list of imaginary parts of the complex numbers. (e.g., `[4, 5, 6]`) # Output A dictionary containing the following keys and their corresponding values: - `\'complex_tensor\'`: The tensor created from the real and imaginary parts. - `\'magnitude\'`: The magnitude of each complex number in the tensor. - `\'angle\'`: The angle of each complex number in the tensor. - `\'conjugate\'`: The complex conjugate of each complex number in the tensor. # Constraints 1. The real_parts and imag_parts lists will always have the same length (n). 2. The values in real_parts and imag_parts will always be real numbers. # Example ```python >>> real_parts = [1, 2, 3] >>> imag_parts = [4, 5, 6] >>> result = complex_tensor_operations(real_parts, imag_parts) >>> print(result) { \'complex_tensor\': tensor([1.+4.j, 2.+5.j, 3.+6.j]), \'magnitude\': tensor([4.1231, 5.3852, 6.7082]), \'angle\': tensor([1.3258, 1.1903, 1.1071]), \'conjugate\': tensor([1.-4.j, 2.-5.j, 3.-6.j]) } ``` # Notes 1. You can use `torch.tensor` to create tensors from lists. 2. Use appropriate PyTorch functions to compute the magnitude (`torch.abs`), angle (`torch.angle`), and conjugate of the complex tensor. Implement the function following these requirements.","solution":"import torch def complex_tensor_operations(real_parts: list, imag_parts: list) -> dict: # Create complex number tensors real_tensor = torch.tensor(real_parts, dtype=torch.float32) imag_tensor = torch.tensor(imag_parts, dtype=torch.float32) complex_tensor = torch.complex(real_tensor, imag_tensor) # Calculate the magnitude magnitude = torch.abs(complex_tensor) # Calculate the angle angle = torch.angle(complex_tensor) # Calculate the conjugate conjugate = torch.conj(complex_tensor) return { \'complex_tensor\': complex_tensor, \'magnitude\': magnitude, \'angle\': angle, \'conjugate\': conjugate }"},{"question":"Coding Assessment Question # Objective You are tasked with implementing a custom error handler for a distributed PyTorch application. This handler should record and manage errors that occur in distributed child processes. # Problem Statement Implement a custom error handler in PyTorch that: 1. Records errors that occur in child processes. 2. Raises a `ChildFailedError` if any child process fails. # Function Signature ```python from torch.distributed.elastic.multiprocessing.errors import record, ChildFailedError, ErrorHandler, ProcessFailure class CustomErrorHandler(ErrorHandler): def __init__(self): super().__init__() self.errors = [] def handle(self, failure: ProcessFailure): Records the error and raises a ChildFailedError if any child process fails. Args: - failure (ProcessFailure): The failure instance representing the process failure. Raises: - ChildFailedError: Raised when any child process fails. # Use the `record` function to log the failure record(failure) self.errors.append(failure.error) raise ChildFailedError(f\\"Child process failed with error: {failure.error}\\") ``` # Constraints - You must handle and log errors only using the distributed multiprocessing error handling methods provided by PyTorch. - You should assume that the `failure` argument passed to the `handle` method is an instance of `ProcessFailure` with an `error` attribute. # Input The input will be managed by the PyTorch distributed framework, and you won\'t need to worry about reading input in your implementation. # Output - Your implementation should raise a `ChildFailedError` with a descriptive message if any child process fails. # Example - Assume a distributed process encounters an error. Your `handle` method should record this error and then raise a `ChildFailedError`. # Performance Requirements - Your implementation should efficiently record errors and handle multiple error scenarios without significant performance overhead. Your task is to complete the implementation of the `CustomErrorHandler` class to achieve the above requirements.","solution":"from torch.distributed.elastic.multiprocessing.errors import record, ChildFailedError, ErrorHandler, ProcessFailure class CustomErrorHandler(ErrorHandler): def __init__(self): super().__init__() self.errors = [] def handle(self, failure: ProcessFailure): Records the error and raises a ChildFailedError if any child process fails. Args: - failure (ProcessFailure): The failure instance representing the process failure. Raises: - ChildFailedError: Raised when any child process fails. # Use the `record` function to log the failure record(failure) # Append the error to the errors list self.errors.append(failure.error) # Raise a ChildFailedError with the error message raise ChildFailedError(f\\"Child process failed with error: {failure.error}\\") # A placeholder implementation for the necessary import statements since we can\'t actually import these modules. class ProcessFailure: def __init__(self, error): self.error = error def record(failure): pass class ChildFailedError(Exception): pass class ErrorHandler: def __init__(self): pass"},{"question":"Objective Write a Python function that demonstrates the use of the `posix` module to access environment variables, and then illustrates how the `os` module can be used to update these variables effectively. The function should fetch a specific environment variable, update its value, and then confirm the update using both modules. Function Signature ```python def update_environment_variable(var_name: str, new_value: str) -> str: Fetches the value of the given environment variable, updates it to the new value using the `os` module, and confirms the update using both `os` and `posix` modules. Parameters: var_name (str): The name of the environment variable to be updated. new_value (str): The new value to be set for the environment variable. Returns: str: A string confirmation that includes the updated value from both `os` and `posix` modules. ``` Input - `var_name`: A string representing the environment variable name you wish to update. - `new_value`: A string representing the new value you want to assign to the environment variable. Output - A string confirming the update, including the new value from both the `os` and `posix` modules. Constraints - You should handle exceptions that may arise from trying to fetch or update environment variables. - If the environment variable does not exist, create it using the `os` module. - Ensure compatibility with Unix systems where environment variables are represented as bytes in the `posix` module. - Assume the environment variable name given is a valid string and exists in the system (or will be created if it doesn\'t). Example ```python import os import posix def update_environment_variable(var_name: str, new_value: str) -> str: # Fetch the environment variable using posix posix_env_before = posix.environ.get(var_name.encode()) if hasattr(posix, \'environ\') else None # Update the environment variable using os module os.environ[var_name] = new_value # Fetch the updated variable using both modules os_env_after = os.environ.get(var_name) posix_env_after = posix.environ.get(var_name.encode()) if hasattr(posix, \'environ\') else None # Create confirmation message confirmation_message = ( f\\"Environment variable \'{var_name}\' updated successfully.n\\" f\\"Using os module: {os_env_after}n\\" f\\"Using posix module: {posix_env_after.decode() if posix_env_after else \'Not Available\'}\\" ) return confirmation_message # Example usage print(update_environment_variable(\'TEST_VAR\', \'new_value\')) ``` Expected output: ```plaintext Environment variable \'TEST_VAR\' updated successfully. Using os module: new_value Using posix module: new_value ``` Notes 1. If the `posix.environ` attribute is not available, handle it appropriately by indicating that in the confirmation message. 2. Ensure the function demonstrates a clear understanding of the interaction between `posix` and `os` modules concerning environment variables.","solution":"import os import posix def update_environment_variable(var_name: str, new_value: str) -> str: Fetches the value of the given environment variable, updates it to the new value using the `os` module, and confirms the update using both `os` and `posix` modules. Parameters: var_name (str): The name of the environment variable to be updated. new_value (str): The new value to be set for the environment variable. Returns: str: A string confirmation that includes the updated value from both `os` and `posix` modules. # Fetch the environment variable using posix posix_env_before = posix.environ.get(var_name.encode()) if hasattr(posix, \'environ\') else None # Update the environment variable using os module os.environ[var_name] = new_value # Fetch the updated variable using both modules os_env_after = os.environ.get(var_name) posix_env_after = posix.environ.get(var_name.encode()) if hasattr(posix, \'environ\') else None # Create confirmation message confirmation_message = ( f\\"Environment variable \'{var_name}\' updated successfully.n\\" f\\"Using os module: {os_env_after}n\\" f\\"Using posix module: {posix_env_after.decode() if posix_env_after else \'Not Available\'}\\" ) return confirmation_message"},{"question":"# Description You are required to write a function `safe_tensor_operations` that performs a series of tensor manipulations while ensuring proper synchronization between CUDA streams to avoid data races. # Function Signature ```python def safe_tensor_operations(): # Your code here ``` # Requirements 1. Initialize a tensor `a` of shape (10000,) with random values on the default CUDA stream. 2. Create a new CUDA stream. 3. Modify the tensor `a` by multiplying it by a scalar value (e.g., 5) within the new stream. 4. Ensure proper synchronization between the streams before performing the multiplication to avoid data races. 5. Print the tensor `a` after all operations are complete. # Input - No input parameters. # Output - Print the resulting tensor after all operations, ensuring no synchronization errors occur. # Constraints - The solution must handle proper synchronization and avoid any data races. - You must use PyTorch\'s CUDA stream and synchronization capabilities. # Example ```python def safe_tensor_operations(): import torch a = torch.rand(10000, device=\\"cuda\\") new_stream = torch.cuda.Stream() with torch.cuda.stream(new_stream): # Synchronize the new stream with the default stream torch.cuda.current_stream().wait_stream(torch.cuda.default_stream()) torch.mul(a, 5, out=a) # Synchronize the streams to ensure safe final print torch.cuda.synchronize() print(a) # Call the function safe_tensor_operations() ``` # Explanation - `a = torch.rand(10000, device=\\"cuda\\")`: Initializes a tensor on the default stream. - `new_stream = torch.cuda.Stream()`: Creates a new CUDA stream. - `torch.cuda.current_stream().wait_stream(torch.cuda.default_stream())`: Ensures the new stream waits for the default stream to complete its operations. - `torch.mul(a, 5, out=a)`: Multiplies the tensor by 5 within the new stream. - `torch.cuda.synchronize()`: Ensures all streams are synchronized before printing the tensor, preventing any unfinished operations from causing data races. Ensure your implementation meets these requirements and handles synchronization correctly using the CUDA Stream Sanitizer principles discussed.","solution":"def safe_tensor_operations(): import torch # Initialize a tensor `a` of shape (10000,) with random values on the default CUDA stream. a = torch.rand(10000, device=\\"cuda\\") # Create a new CUDA stream. new_stream = torch.cuda.Stream() with torch.cuda.stream(new_stream): # Synchronize the new stream with the default stream torch.cuda.current_stream().wait_stream(torch.cuda.default_stream()) # Modify the tensor `a` by multiplying it by a scalar value (5) within the new stream. torch.mul(a, 5, out=a) # Synchronize the streams to ensure safe final print torch.cuda.synchronize() print(a)"},{"question":"**Command Line File Processor** Implement a script in Python that processes files in a directory, performs regular expression operations on each file, and manages outputs to a new directory. The task will test your understanding of file handling, command line argument parsing, regular expressions, and directory operations. # Objectives 1. **Script Name**: `file_processor.py` 2. **Command Line Arguments**: - `--search-dir`: The directory where files to be processed are located. - `--output-dir`: The directory where the processed files will be saved. - `--pattern`: The regular expression pattern to match in each file. - `--replace`: The string to replace the matched patterns. - `--file-ext`: (Optional) File extension to filter files in the `--search-dir`. Example command: ``` python file_processor.py --search-dir ./input --output-dir ./output --pattern \\"bfoob\\" --replace \\"bar\\" --file-ext \\".txt\\" ``` 3. **Functional Requirements**: - Parse the command line arguments using the `argparse` module. - Verify that `--search-dir` exists and is a directory. - Create the `--output-dir` if it does not exist. - Process all files in the `--search-dir` that match the `--file-ext` (or all files if `--file-ext` is not provided). - For each file, read its contents, use the `re` module to find all occurrences of the `--pattern`, and replace them with the `--replace` string. - Write the processed content to a new file in the `--output-dir` with the same filename as the original. 4. **Constraints**: - Ensure the script handles large files efficiently. - Validate all input arguments; provide user-friendly error messages if invalid arguments are provided. - Use appropriate exception handling for file operations. 5. **Performance**: - Ensure that the script handles a large number of files without significant performance degradation. # Example Suppose `--search-dir` contains a file `example.txt` with the content: ``` foo is a placeholder text, foo should be replaced. ``` Running the script with: ``` python file_processor.py --search-dir ./input --output-dir ./output --pattern \\"bfoob\\" --replace \\"bar\\" --file-ext \\".txt\\" ``` Should produce `./output/example.txt` with the content: ``` bar is a placeholder text, bar should be replaced. ``` # Implementation Tips 1. Use the `os` and `shutil` modules for directory and file operations. 2. Use the `glob` module if needed to list files matching the provided extension. 3. Use the `re` module for pattern matching and replacement. 4. Handle file reading and writing using context managers to ensure proper resource management. 5. Test the script with different edge cases and large directories to ensure robustness.","solution":"import os import argparse import re import shutil def process_files(search_dir, output_dir, pattern, replace, file_ext=None): if not os.path.exists(search_dir) or not os.path.isdir(search_dir): raise ValueError(f\\"{search_dir} does not exist or is not a directory\\") if not os.path.exists(output_dir): os.makedirs(output_dir) for root, dirs, files in os.walk(search_dir): for file in files: if file_ext and not file.endswith(file_ext): continue file_path = os.path.join(root, file) with open(file_path, \'r\') as f: content = f.read() new_content = re.sub(pattern, replace, content) output_file_path = os.path.join(output_dir, file) with open(output_file_path, \'w\') as f: f.write(new_content) if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Process files with regexp replacement\\") parser.add_argument(\\"--search-dir\\", required=True, help=\\"Directory to search files in\\") parser.add_argument(\\"--output-dir\\", required=True, help=\\"Directory to save processed files\\") parser.add_argument(\\"--pattern\\", required=True, help=\\"Pattern to search in files\\") parser.add_argument(\\"--replace\\", required=True, help=\\"Replacement string for the pattern\\") parser.add_argument(\\"--file-ext\\", help=\\"File extension to filter files (optional)\\") args = parser.parse_args() process_files(args.search_dir, args.output_dir, args.pattern, args.replace, args.file_ext)"},{"question":"**Question: Introspecting Function Calls** Given the functions described in the `python310` package, your task is to implement a function `introspect_function_calls` that returns detailed information about the currently executing function. Specifically, you will make use of the following functions: - `PyEval_GetGlobals` - `PyEval_GetLocals` - `PyFrame_GetCode` - `PyFrame_GetLineNumber` - `PyEval_GetFuncName` - `PyEval_GetFuncDesc` Your function should return a dictionary containing the following keys and their corresponding values: - `\\"function_name\\"`: The name of the currently executing function. - `\\"function_description\\"`: A description of the currently executing function. - `\\"globals\\"`: A dictionary of the global variables in the current execution frame. - `\\"locals\\"`: A dictionary of the local variables in the current execution frame. - `\\"current_line\\"`: The line number that the current frame is executing. # Function Signature ```python def introspect_function_calls() -> dict: pass ``` # Example Output Assuming the function `example_function` is running the `introspect_function_calls`, the output might look something like: ```python { \\"function_name\\": \\"example_function\\", \\"function_description\\": \\"()\\", \\"globals\\": { ... dictionary of global variables ... }, \\"locals\\": { ... dictionary of local variables ... }, \\"current_line\\": 10 } ``` # Constraints - Implement the function in Python using the `python310` package methods as specified. - Make sure the function handles cases where frame or execution state might be `NULL`. # Performance Requirements This function will typically be used for debugging and introspection, and it should aim to execute in constant or linear time relative to the size of the globals and locals dictionaries. **Note**: Python\'s native `inspect` module might be helpful for context and inspiration, but please focus on using the provided methods from the `python310` package for your implementation.","solution":"import inspect def introspect_function_calls() -> dict: This function returns a dictionary containing detailed information about the currently executing function. # Get the current frame current_frame = inspect.currentframe() if current_frame is None: return {} # Get the current function\'s frame caller_frame = current_frame.f_back if caller_frame is None: return {} # Extracting necessary information function_name = caller_frame.f_code.co_name function_description = inspect.formatargvalues(*inspect.getargvalues(caller_frame)) globals_dict = caller_frame.f_globals locals_dict = caller_frame.f_locals current_line = caller_frame.f_lineno # Return the gathered information in a dictionary return { \\"function_name\\": function_name, \\"function_description\\": function_description, \\"globals\\": globals_dict, \\"locals\\": locals_dict, \\"current_line\\": current_line } def example_function(): a = 10 b = 20 return introspect_function_calls()"},{"question":"# Multi-Threaded Task Scheduler with Logging You are required to implement a multi-threaded task scheduler in Python that performs background tasks while logging each step in detail. You will demonstrate your understanding of the `threading` and `logging` modules provided in Python. Task Description: 1. **Task Scheduler**: - Create a class `TaskScheduler` that manages multiple tasks running in the background. - The tasks should be represented as instances of an inner class `Task` that extends `threading.Thread`. - Each `Task` should have a unique `task_id`, a `function` to execute, and any arguments required by the `function`. 2. **Logging**: - Set up a logging system that tracks when each task is started and when it is completed. - Log any errors that occur during the execution of a task. 3. **Methods to Implement**: - `add_task(task_id: str, function: Callable, args: tuple)`: Adds a task to the scheduler. - `run_all()`: Starts all tasks and ensures the main program waits until all tasks are completed. - `cancel_task(task_id: str)`: Cancels a task if it has not yet started. - `task_status(task_id: str) -> str`: Returns the status of a task (`Pending`, `Running`, `Completed`, `Cancelled`). Constraints: - Each `function` provided should be a Callable that can be executed with the provided `args`. - Task identifiers (`task_id`) will be unique strings. - You must handle exceptions that occur during task execution and log them appropriately. Example: ```python import threading import logging from time import sleep from typing import Callable class TaskScheduler: ... # Example function to be executed by tasks def example_task(duration): sleep(duration) print(f\\"Task completed in {duration} seconds.\\") # Configure and run the task scheduler if __name__ == \\"__main__\\": scheduler = TaskScheduler() # Adding tasks scheduler.add_task(\\"task_1\\", example_task, (2,)) scheduler.add_task(\\"task_2\\", example_task, (4,)) # Running all tasks scheduler.run_all() # Check task status print(scheduler.task_status(\\"task_1\\")) # Output: Completed print(scheduler.task_status(\\"task_2\\")) # Output: Completed ``` This question requires careful handling of threading and logging along with clear and organized class and method designs. It aims to evaluate the student\'s understanding of concurrency, exception handling, and logging in a real-world software context.","solution":"import threading import logging from time import sleep from typing import Callable, Dict, Tuple class TaskScheduler: def __init__(self): self.tasks = {} self.tasks_status = {} self.lock = threading.Lock() logging.basicConfig(level=logging.INFO, format=\'%(asctime)s [%(levelname)s] %(message)s\') class Task(threading.Thread): def __init__(self, task_id: str, function: Callable, args: Tuple, scheduler): super().__init__() self.task_id = task_id self.function = function self.args = args self.scheduler = scheduler def run(self): self.scheduler.update_task_status(self.task_id, \\"Running\\") logging.info(f\'Task {self.task_id} started.\') try: self.function(*self.args) except Exception as e: logging.error(f\'Task {self.task_id} encountered an error: {e}\') else: self.scheduler.update_task_status(self.task_id, \\"Completed\\") logging.info(f\'Task {self.task_id} completed.\') def add_task(self, task_id: str, function: Callable, args: Tuple): with self.lock: if task_id in self.tasks: raise ValueError(f\\"Task with id {task_id} already exists.\\") task = self.Task(task_id, function, args, self) self.tasks[task_id] = task self.tasks_status[task_id] = \\"Pending\\" def run_all(self): with self.lock: threads = list(self.tasks.values()) for thread in threads: thread.start() for thread in threads: thread.join() def cancel_task(self, task_id: str): with self.lock: if task_id in self.tasks: if self.tasks_status[task_id] == \\"Pending\\": self.tasks_status[task_id] = \\"Cancelled\\" del self.tasks[task_id] logging.info(f\'Task {task_id} has been cancelled.\') def update_task_status(self, task_id: str, status: str): with self.lock: if task_id in self.tasks_status: self.tasks_status[task_id] = status def task_status(self, task_id: str) -> str: with self.lock: return self.tasks_status.get(task_id, \\"Task not found\\") # Example function to be executed by tasks def example_task(duration): sleep(duration) print(f\\"Task completed in {duration} seconds.\\")"},{"question":"You are provided with the \\"diamonds\\" dataset from the seaborn library. The dataset contains information on nearly 54,000 diamonds, including the price, carat, cut, color, clarity, and other attributes. Your task is to write a Python function that generates a complex visualization using seaborn\'s categorical plotting functionalities. The function should do the following: 1. Load the \\"diamonds\\" dataset from seaborn. 2. Create a boxen plot to compare the price of diamonds across different cuts. 3. Use the `hue` semantic to add clarity as an additional categorical dimension. 4. Customize the plot with the following requirements: - Set the palette to \\"muted\\". - Arrange the legend to be outside the plot area, to the right side. - Set the title of the plot to \\"Diamond Prices by Cut and Clarity\\". - Include grid lines only on the y-axis. 5. Save the final plot as \\"diamond_prices.png\\". # Function Signature ```python def visualize_diamond_prices(): pass ``` # Example Output Running the function `visualize_diamond_prices()` should create a plot meeting the above requirements and save it as \\"diamond_prices.png\\". # Additional Information - Ensure you have seaborn and matplotlib libraries installed. - You can refer to the seaborn documentation to explore different customization options for the plot. # Constraints - The function should load the dataset internally, and should not take any input parameters. - The function should not return any values but must save the plot as specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_diamond_prices(): # Load the diamonds dataset from seaborn diamonds = sns.load_dataset(\\"diamonds\\") # Create the boxen plot plt.figure(figsize=(12, 8)) sns.boxenplot(x=\\"cut\\", y=\\"price\\", hue=\\"clarity\\", data=diamonds, palette=\\"muted\\") # Customize the plot plt.title(\\"Diamond Prices by Cut and Clarity\\") plt.legend(loc=\'center left\', bbox_to_anchor=(1, 0.5), title=\\"Clarity\\") plt.grid(axis=\'y\') # Save the plot plt.savefig(\\"diamond_prices.png\\", bbox_inches=\\"tight\\") plt.close()"},{"question":"# PyTorch CUDA Environment Variables Configuration Objective You are required to create a PyTorch-based script that configures various CUDA runtime environment variables to tune the behavior of PyTorch operations. The configuration should feature handling memory management, GPU visibility, and debugging settings. Task 1. Write a Python function `configure_pytorch_environment` that sets specific environment variables to control PyTorch\'s CUDA behavior. 2. Your function should: - Disable caching of memory allocations. - Ensure CUDA calls are synchronous for debugging. - Limit the cuDNN v8 API cache to 5000. - Make only the first GPU device visible to CUDA. 3. Write another function `verify_configuration` to check if the settings have been applied correctly. Input and Output - Your function `configure_pytorch_environment` should not take any parameters and should not return any values. However, it should set the environment variables as specified. - Your function `verify_configuration` should return a dictionary where keys are environment variable names and values are their current settings, confirming they have been correctly configured. Constraints - Assume the system has at least one GPU available. - Ensure your solution does not cause any side effects outside the function scope. ```python import os def configure_pytorch_environment(): Sets specific environment variables for PyTorch\'s CUDA behavior. os.environ[\'PYTORCH_NO_CUDA_MEMORY_CACHING\'] = \'1\' os.environ[\'CUDA_LAUNCH_BLOCKING\'] = \'1\' os.environ[\'TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\'] = \'5000\' os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\' def verify_configuration(): Verifies the configuration of the environment variables. Returns: dict: A dictionary containing the variable names and their values. config = { \'PYTORCH_NO_CUDA_MEMORY_CACHING\': os.getenv(\'PYTORCH_NO_CUDA_MEMORY_CACHING\', None), \'CUDA_LAUNCH_BLOCKING\': os.getenv(\'CUDA_LAUNCH_BLOCKING\', None), \'TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\': os.getenv(\'TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\', None), \'CUDA_VISIBLE_DEVICES\': os.getenv(\'CUDA_VISIBLE_DEVICES\', None) } return config # Example usage: configure_pytorch_environment() print(verify_configuration()) ``` Evaluation To successfully complete this task, your implementation will be tested on: 1. Correctly setting the specified environment variables. 2. Correctly retrieving these configurations and returning them in the expected format. Good luck!","solution":"import os def configure_pytorch_environment(): Sets specific environment variables for PyTorch\'s CUDA behavior. os.environ[\'PYTORCH_NO_CUDA_MEMORY_CACHING\'] = \'1\' os.environ[\'CUDA_LAUNCH_BLOCKING\'] = \'1\' os.environ[\'TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\'] = \'5000\' os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\' def verify_configuration(): Verifies the configuration of the environment variables. Returns: dict: A dictionary containing the variable names and their values. config = { \'PYTORCH_NO_CUDA_MEMORY_CACHING\': os.getenv(\'PYTORCH_NO_CUDA_MEMORY_CACHING\'), \'CUDA_LAUNCH_BLOCKING\': os.getenv(\'CUDA_LAUNCH_BLOCKING\'), \'TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\': os.getenv(\'TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\'), \'CUDA_VISIBLE_DEVICES\': os.getenv(\'CUDA_VISIBLE_DEVICES\') } return config"},{"question":"**Objective**: To evaluate the student\'s ability to use the `configparser` module for reading, writing, and manipulating INI configuration files in Python. # Problem Statement Implement a Python function `process_config_file(file_path: str) -> dict` that reads an INI configuration file, processes its contents, and returns a dictionary with the following operations: 1. Read the configuration file specified by `file_path`. 2. Add a new section named `\\"new_section\\"` with a single key-value pair `\\"key\\": \\"value\\"`. 3. For the section named `\\"settings\\"`, update (or add if not present) the following key-value pairs: - `\\"theme\\": \\"dark\\"` - `\\"version\\": \\"2.0\\"` 4. If the section `\\"user\\"` exists, remove the key-value pair `\\"password\\"`. 5. Save the updated configuration back to a new file named `\\"updated_config.ini\\"` in the same directory as the input file. 6. Return a dictionary containing all the sections and their key-value pairs from the updated configuration file. # Input - `file_path` (str): The path to the input INI configuration file. # Output - A dictionary where keys are section names, and values are dictionaries of key-value pairs within those sections from the updated configuration file. # Constraints - The input file will be a well-formatted INI file. - Ensure that any I/O operations are properly handled, i.e., files are closed after reading/writing. # Example Suppose the input file `config.ini` contains: ``` [settings] theme = light version = 1.0 [user] username = admin password = secret ``` After processing the file, the new `updated_config.ini` should contain: ``` [settings] theme = dark version = 2.0 [user] username = admin [new_section] key = value ``` The function should return: ```python { \\"settings\\": { \\"theme\\": \\"dark\\", \\"version\\": \\"2.0\\" }, \\"user\\": { \\"username\\": \\"admin\\" }, \\"new_section\\": { \\"key\\": \\"value\\" } } ``` # Implementation Notes - Utilize the `configparser` module as the primary tool for reading and writing the INI files. - Pay attention to adding sections, setting key-value pairs, and removing keys where necessary. - Make sure the solution is efficient and handles possible edge cases, such as missing sections or keys.","solution":"import configparser import os def process_config_file(file_path: str) -> dict: config = configparser.ConfigParser() # Read the configuration file config.read(file_path) # Add a new section \'new_section\' with a key-value pair \'key\': \'value\' if \'new_section\' not in config: config.add_section(\'new_section\') config.set(\'new_section\', \'key\', \'value\') # Update (or add if not present) \'theme\' and \'version\' in the section \'settings\' if \'settings\' not in config: config.add_section(\'settings\') config.set(\'settings\', \'theme\', \'dark\') config.set(\'settings\', \'version\', \'2.0\') # Remove \'password\' key from the \'user\' section if it exists if \'user\' in config and \'password\' in config[\'user\']: config.remove_option(\'user\', \'password\') # Save the updated configuration to a new file updated_file_path = os.path.join(os.path.dirname(file_path), \'updated_config.ini\') with open(updated_file_path, \'w\') as updated_config_file: config.write(updated_config_file) # Convert the updated configuration to a dictionary updated_config_dict = {section: dict(config.items(section)) for section in config.sections()} return updated_config_dict"},{"question":"**Title:** Multi-threaded and Multi-processing Task Execution Objective To assess the studentâ€™s ability to implement both threading and multiprocessing in Python to achieve concurrent execution of tasks. Problem Statement You are given the task to calculate the sum of squares for a given list of integers using concurrent execution. Implement two functions: 1. **multi_threaded_sum_of_squares(int_list: List[int], num_threads: int) -> int:** - Uses the `threading` module to calculate the sum of squares. - `int_list` is the list of integers whose squares need to be summed. - `num_threads` is the number of threads to be used. - Returns the sum of squares of the integers in the list. 2. **multi_processed_sum_of_squares(int_list: List[int], num_processes: int) -> int:** - Uses the `multiprocessing` module to calculate the sum of squares. - `int_list` is the list of integers whose squares need to be summed. - `num_processes` is the number of processes to be used. - Returns the sum of squares of the integers in the list. Input - `int_list`: A list of integers. (0 <= len(int_list) <= 10^6, -10^6 <= int_list[i] <= 10^6) - `num_threads` or `num_processes`: An integer specifying the number of threads or processes to be used. (1 <= num_threads, num_processes <= 100) Output - An integer representing the sum of squares of the integers in the list. Constraints - You must use the `threading` module for the first function. - You must use the `multiprocessing` module for the second function. - Ensure that the solution is efficient and handles large lists efficiently. Example ```python from typing import List def multi_threaded_sum_of_squares(int_list: List[int], num_threads: int) -> int: # Implementation using threading module def multi_processed_sum_of_squares(int_list: List[int], num_processes: int) -> int: # Implementation using multiprocessing module # Example usage: int_list = [1, 2, 3, 4] num_threads = 2 num_processes = 2 print(multi_threaded_sum_of_squares(int_list, num_threads)) # Output: 30 print(multi_processed_sum_of_squares(int_list, num_processes)) # Output: 30 ``` Notes 1. Consider dividing the work as evenly as possible between the threads/processes. 2. Utilize thread-safe and process-safe mechanisms where necessary. 3. Ensure that you properly handle the creation and joining of threads and processes.","solution":"from typing import List import threading import multiprocessing def sum_of_squares_part(int_list, result, index): result[index] = sum(x * x for x in int_list) def multi_threaded_sum_of_squares(int_list: List[int], num_threads: int) -> int: if not int_list: return 0 # Splitting the list for threads length = len(int_list) split = length // num_threads threads = [] results = [0] * num_threads for i in range(num_threads): start_index = i * split # Handling the last chunk to include the remainder if i == num_threads - 1: end_index = length else: end_index = (i + 1) * split thread = threading.Thread(target=sum_of_squares_part, args=(int_list[start_index:end_index], results, i)) threads.append(thread) thread.start() for thread in threads: thread.join() return sum(results) def multi_processed_sum_of_squares(int_list: List[int], num_processes: int) -> int: if not int_list: return 0 # Splitting the list for processes length = len(int_list) split = length // num_processes processes = [] with multiprocessing.Manager() as manager: results = manager.list([0] * num_processes) for i in range(num_processes): start_index = i * split # Handling the last chunk to include the remainder if i == num_processes - 1: end_index = length else: end_index = (i + 1) * split process = multiprocessing.Process(target=sum_of_squares_part, args=(int_list[start_index:end_index], results, i)) processes.append(process) process.start() for process in processes: process.join() return sum(results)"},{"question":"# PyTorch Named Tensors: Dimension Alignment and Operations **Objective**: Implement a function that performs a sequence of operations on named tensors and verifies the resultant names according to the documentation provided. Function Signature ```python import torch def named_tensor_operations(t1: torch.Tensor, t2: torch.Tensor) -> torch.Tensor: Performs a series of tensor operations on the given named tensors t1 and t2. Args: t1 (torch.Tensor): First input tensor with named dimensions. t2 (torch.Tensor): Second input tensor with named dimensions. Returns: torch.Tensor: Resultant tensor after the operations with correct names. # Implement the function ``` Input - `t1`: A named tensor `torch.Tensor` of shape (A, B, C) with names (\'N\', \'C\', \'W\'). - `t2`: A named tensor `torch.Tensor` of shape (A, B) with names (\'N\', \'H\'). Output - The function should return a single tensor after performing the following operations. - Element-wise addition with a tensor of shape (A, B, C), propagating names correctly. - Perform a matrix multiplication with a tensor of appropriate shape (B, E) and evaluate the names. - Finally, sum the tensor over the last dimension and check name inference. # Steps and Constraints 1. **Addition Operation**: - Create a tensor `t3` of shape (A, B, C) with names (\'N\', \'C\', \'W\'). - Perform element-wise addition: `result1 = torch.add(t1, t3)`. 2. **Matrix Multiplication**: - Create a tensor `t4` of shape (C, E) with names (\'W\', \'E\'). - Perform matrix multiplication: `result2 = torch.matmul(result1, t4)`. - Ensure resultant tensor names are aligned as expected. 3. **Reduction Operation**: - Sum the tensor `result2` over the last dimension without keeping dimensions: `result_final = result2.sum(\'E\')`. 4. **Name Checks**: - Check correctness: The names of the final tensor should be validated to match expected names based on the operations. You should avoid using explicit name assignment (like `.names =`) and rely solely on the operations for name inference. # Example ```python t1 = torch.randn(2, 3, 4, names=(\'N\', \'C\', \'W\')) t2 = torch.randn(2, 3, names=(\'N\', \'H\')) result = named_tensor_operations(t1, t2) print(result.names) # Expected output: (\'N\', \'C\') ``` Notes - Use provided tensors directly, avoid reshaping or renaming directly via assignment. - Only rely on tensor operations to propagate and infer names correctly. - Follow the rules in the documentation to ensure proper name handling and propagation.","solution":"import torch def named_tensor_operations(t1: torch.Tensor, t2: torch.Tensor) -> torch.Tensor: Performs a series of tensor operations on the given named tensors t1 and t2. Args: t1 (torch.Tensor): First input tensor with named dimensions. t2 (torch.Tensor): Second input tensor with named dimensions. Returns: torch.Tensor: Resultant tensor after the operations with correct names. # Create a tensor of shape (A, B, C) with names (\'N\', \'C\', \'W\') t3 = torch.randn(t1.size(), names=t1.names) # Element-wise addition result1 = torch.add(t1, t3) # Create a tensor of shape (C, E) with names (\'W\', \'E\') C, W = t1.size()[1], t1.size()[2] E = 5 # Assuming an arbitrary size for E t4 = torch.randn(W, E, names=(\'W\', \'E\')) # Matrix multiplication result2 = torch.matmul(result1.align_to(\'N\', \'C\', \'W\'), t4) # Sum over the last dimension without keeping dimensions result_final = result2.sum(\'E\') return result_final"},{"question":"# Question: Custom JSON Encoder and Decoder You are tasked with creating a custom JSON encoder and decoder to handle a special data structure in Python. The data structure `Person` holds information about an individual, including their name, age, and a list of their friends (also `Person` objects). Define the `Person` class with the following attributes: - `name` (str) - `age` (int) - `friends` (list of `Person` objects) Implement a custom JSON encoder and decoder for the `Person` class using the `json` module. **Instructions:** 1. Define the `Person` class with an appropriate `__init__` method and a `__repr__` method for string representation. 2. Implement a custom JSON encoder `PersonEncoder` that extends `json.JSONEncoder`. It should handle the serialization of `Person` objects. 3. Implement a custom JSON decoder function `person_decoder` that can deserialize JSON strings back into `Person` objects. 4. Write a function `serialize_person` that takes a `Person` object and returns a JSON string. 5. Write a function `deserialize_person` that takes a JSON string and returns a `Person` object. 6. Demonstrate the functionality with a test case. **Constraints:** - The custom encoder should handle nested `Person` objects in the `friends` attribute. - Ensure that all attributes are correctly serialized and deserialized. **Example:** ```python # Example Person object alice = Person(\\"Alice\\", 30, []) bob = Person(\\"Bob\\", 25, [alice]) charlie = Person(\\"Charlie\\", 20, [alice, bob]) # Serialize the Person object json_str = serialize_person(charlie) print(json_str) # Deserialize the JSON string back into a Person object person_obj = deserialize_person(json_str) print(person_obj) ``` **Expected Output:** The JSON string should represent the nested structure of `Person` objects. Upon deserialization, the output should correctly reconstruct the `Person` object with its nested friends.","solution":"import json class Person: def __init__(self, name, age, friends=None): self.name = name self.age = age self.friends = friends if friends else [] def __repr__(self): return f\'Person(name={self.name}, age={self.age}, friends={self.friends})\' class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): return { \'name\': obj.name, \'age\': obj.age, \'friends\': obj.friends } return super().default(obj) def person_decoder(dct): if \'name\' in dct and \'age\' in dct: return Person(dct[\'name\'], dct[\'age\'], dct[\'friends\']) return dct def serialize_person(person): return json.dumps(person, cls=PersonEncoder) def deserialize_person(json_str): return json.loads(json_str, object_hook=person_decoder)"},{"question":"Objective: Write a Python program demonstrating the use of the `asyncio` and `ssl` modules to create a secure client-server communication system. The server should be able to handle multiple clients simultaneously. Task: You need to implement a secure server that listens to incoming connections and echoes back any message it receives from clients. Clients should connect to this server, send messages, and print the echoed responses from the server. Specifications: 1. **Server Implementation:** - Use the `asyncio` module to handle multiple clients asynchronously. - Use the `ssl` module to secure the communication channel (TLS/SSL). - The server program should run indefinitely, waiting for client connections. - Upon receiving a message from a client, it should send the same message back to the client. - Server should run on `localhost` and port `8888`. 2. **Client Implementation:** - Use the `asyncio` module to connect to the server. - Use the `ssl` module to secure the connection (TLS/SSL). - The client should connect to the server, send a message, receive the echoed message, and print it to the console. - The client should accept a message input from the user, send it to the server, and print the server\'s response. Requirements: - The server and client must use TLS/SSL for secure communication. - The solution should include proper exception handling for network errors. Input Format: - For the client, the input is a single line message entered by the user. Output Format: - For the client, the output is the echoed message received from the server displayed on the console. Constraints: - Use only the `asyncio` and `ssl` modules for asynchronous I/O and secure communication, respectively. - Ensure the server can handle multiple clients simultaneously. Example: - If the user inputs the message \\"Hello, World!\\" in the client program, the server should echo \\"Hello, World!\\" back to the client. Additional Information: You might need to generate SSL certificates for testing purposes. You can use the following commands to create self-signed certificates: ``` openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes ``` Provide both client and server scripts in your solution.","solution":"# server.py import asyncio import ssl async def handle_client(reader, writer): try: while True: data = await reader.read(100) if not data: break message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") print(f\\"Send: {message}\\") writer.write(data) await writer.drain() print(\\"Close the connection\\") writer.close() await writer.wait_closed() except asyncio.CancelledError: print(\\"Server Task cancelled\\") async def main(): ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH) ssl_context.load_cert_chain(certfile=\\"cert.pem\\", keyfile=\\"key.pem\\") server = await asyncio.start_server( handle_client, \'127.0.0.1\', 8888, ssl=ssl_context) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \\"__main__\\": try: asyncio.run(main()) except KeyboardInterrupt: print(\\"Server stopped by the user\\") # client.py import asyncio import ssl async def tcp_echo_client(message): ssl_context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH) ssl_context.load_verify_locations(\'cert.pem\') reader, writer = await asyncio.open_connection( \'127.0.0.1\', 8888, ssl=ssl_context) try: print(f\'Send: {message}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') except asyncio.CancelledError: print(\\"Client Task cancelled\\") finally: print(\'Close the connection\') writer.close() await writer.wait_closed() async def main(message): await tcp_echo_client(message) if __name__ == \\"__main__\\": user_message = input(\\"Enter message to send: \\") try: asyncio.run(main(user_message)) except KeyboardInterrupt: print(\\"Client stopped by the user\\")"},{"question":"You are tasked with implementing a function that generates a summary report of the metadata for a given installed Python package. This function should utilize various functionalities provided by the `importlib.metadata` module in Python 3.10 to fetch and compile the necessary information. Function Signature ```python def generate_package_summary(package_name: str) -> dict: Generate a summary report of the metadata for a given installed Python package. :param package_name: Name of the installed package to query. :return: A dictionary containing the package\'s metadata summary. ``` Function Details 1. **Input:** - The function should accept a single string parameter `package_name` representing the name of the installed package. 2. **Output:** - The function should return a dictionary with the following keys and corresponding values: - `\'version\'`: The version of the package. - `\'entry_points\'`: A dictionary where keys are entry point groups and values are lists of entry point names within each group. - `\'metadata\'`: A dictionary of the package\'s metadata fields and their values. - `\'files\'`: A list of file paths contained in the package\'s distribution. - `\'requirements\'`: A list of requirements specified by the package. 3. **Constraints:** - Ensure that the function can handle packages without specific metadata gracefully. For instance, if a package does not have entry points, the function should include an empty dictionary for `\'entry_points\'`. Example Usage ```python summary = generate_package_summary(\'wheel\') print(summary) # Expected output (format may vary but should include these keys): # { # \'version\': \'0.32.3\', # \'entry_points\': { # \'console_scripts\': [\'wheel\', ...], # ... # }, # \'metadata\': { # \'Name\': \'wheel\', # \'Version\': \'0.32.3\', # ... # }, # \'files\': [ # \'wheel/__init__.py\', # \'wheel/util.py\', # ... # ], # \'requirements\': [ # \'pytest (>=3.0.0) ; extra == \'test\'\', # \'pytest-cov ; extra == \'test\'\', # ... # ] # } ``` Notes - Ensure that you use the appropriate functions from the `importlib.metadata` module to gather the required information. - Pay attention to handling possible exceptions, such as when the specified package does not exist or does not have certain metadata fields. Good luck and happy coding!","solution":"import importlib.metadata def generate_package_summary(package_name: str) -> dict: Generate a summary report of the metadata for a given installed Python package. :param package_name: Name of the installed package to query. :return: A dictionary containing the package\'s metadata summary. try: distribution = importlib.metadata.distribution(package_name) metadata = distribution.metadata version = metadata.get(\'Version\') entry_points = {} try: entry_point_groups = distribution.entry_points for entry_point in entry_point_groups: group = entry_point.group if group not in entry_points: entry_points[group] = [] entry_points[group].append(entry_point.name) except AttributeError: entry_points = {} files = list(distribution.files) if distribution.files else [] files = [str(file) for file in files] requirements = list(distribution.requires) if distribution.requires else [] summary = { \'version\': version, \'entry_points\': entry_points, \'metadata\': {key: metadata.get(key) for key in metadata}, \'files\': files, \'requirements\': requirements } return summary except importlib.metadata.PackageNotFoundError: return {\\"error\\": f\\"Package \'{package_name}\' not found.\\"}"},{"question":"**Question: Unix User Password Database Query** Using the `pwd` module, write a function `get_user_info(parameter)` that takes a single parameter, which can be either an integer (numeric user ID) or a string (user name). Depending on the type and validity of the input, the function should return the corresponding user information from the Unix password database. Your function should follow these specifications: - If the parameter is a numeric user ID, use `pwd.getpwuid(uid)` to retrieve the user information. - If the parameter is a user name (string), use `pwd.getpwnam(name)` to retrieve the user information. - If the parameter does not match any user ID or user name, the function should return `None`. - The function should be able to handle any exceptions, returning `None` if an error occurs (e.g., if the user ID or name does not exist). The expected output for the function should be a dictionary with the following keys: - `name` - `passwd` - `uid` - `gid` - `gecos` - `dir` - `shell` For example: ```python get_user_info(0) ``` might return: ```python { \'name\': \'root\', \'passwd\': \'x\', \'uid\': 0, \'gid\': 0, \'gecos\': \'root\', \'dir\': \'/root\', \'shell\': \'/bin/bash\' } ``` And: ```python get_user_info(\'root\') ``` might return the same dictionary as above. Constraints: - You must handle the retrieval of information using `pwd.getpwuid(uid)` and `pwd.getpwnam(name)` appropriately. - Ensure that your function is efficient and performs necessary validations on the input. ```python import pwd def get_user_info(parameter): # Your implementation here pass ``` **Notes:** - This function assumes a Unix-based system. - Ensure your code handles edge cases and invalid inputs gracefully.","solution":"import pwd def get_user_info(parameter): try: if isinstance(parameter, int): user_info = pwd.getpwuid(parameter) elif isinstance(parameter, str): user_info = pwd.getpwnam(parameter) else: return None return { \'name\': user_info.pw_name, \'passwd\': user_info.pw_passwd, \'uid\': user_info.pw_uid, \'gid\': user_info.pw_gid, \'gecos\': user_info.pw_gecos, \'dir\': user_info.pw_dir, \'shell\': user_info.pw_shell } except KeyError: return None"},{"question":"You are tasked with creating a Python function that simulates the behavior of the `mailcap.findmatch` function from the deprecated `mailcap` module. Your function should process a single mailcap file and return the command to handle a specified MIME type given certain conditions. # Function Signature ```python def custom_findmatch(mailcap_file: str, MIMEtype: str, key: str = \'view\', filename: str = \'/dev/null\', plist: list = []) -> tuple: pass ``` # Input - `mailcap_file` (str): Path to the mailcap file. - `MIMEtype` (str): The MIME type to search for within the mailcap file. - `key` (str): The type of activity to be performed, default is \'view\'. - `filename` (str): The filename to substitute for \\"%s\\" in the command line, default is \'/dev/null\'. - `plist` (list): A list containing named parameters in the form `[\'name=value\', ...]`. # Output - Returns a 2-tuple: - The first element is the command line to be executed (string). - The second element is the mailcap entry for the given MIME type (dictionary). - If no matching MIME type can be found, return (None, None). # Constraints - Mailcap file format specifics are derived from RFC 1524. - The function should handle potential security issues with shell metacharacters by ignoring entries or components that contain disallowed characters. - The function should handle named parameters in the `plist` and substitute them properly in the command string. # Example Consider the following mailcap file content: ``` text/plain; cat %s video/mpeg; xmpeg %s; test=test -n DISPLAY ``` Example usage: ```python mailcap_content = text/plain; cat %s video/mpeg; xmpeg %s; test=test -n DISPLAY # Writing the content to an example mailcap file. with open(\'example.mailcap\', \'w\') as f: f.write(mailcap_content) # Calling the custom_findmatch function. print(custom_findmatch(\'example.mailcap\', \'video/mpeg\', filename=\'sample_video.mpg\')) ``` Expected Output: ```python (\'xmpeg sample_video.mpg\', {\'view\': \'xmpeg %s\', \'test\': \'test -n DISPLAY\'}) ``` # Note - Ensure your function is robust with proper error handling and edge cases considered. - Ensure parameters in `plist` are correctly processed and substituted in the command string.","solution":"import re def parse_mailcap_file(mailcap_file): mailcap_entries = [] with open(mailcap_file, \'r\') as f: for line in f: line = line.strip() if not line or line.startswith(\'#\'): continue mime_type, rest = line.split(\';\', 1) mime_type = mime_type.strip() fields = {} for part in rest.split(\';\'): key_value = part.split(\'=\', 1) if len(key_value) == 2: key, value = key_value fields[key.strip()] = value.strip() else: fields[\'view\'] = part.strip() mailcap_entries.append((mime_type, fields)) return mailcap_entries def custom_findmatch(mailcap_file: str, MIMEtype: str, key: str = \'view\', filename: str = \'/dev/null\', plist: list = []) -> tuple: def substitute_params(command, plist): for param in plist: name, value = param.split(\'=\', 1) command = command.replace(f\'%{{{name}}}\', value) return command mailcap_entries = parse_mailcap_file(mailcap_file) for mime_type, fields in mailcap_entries: if re.match(mime_type.replace(\'*\', \'.*\'), MIMEtype) and key in fields: command = fields[key] if \'%s\' in command: command = command.replace(\'%s\', filename) command = substitute_params(command, plist) return (command, fields) return (None, None)"},{"question":"Objective: Implement a Python class `Cell` that mimics the behavior of the `PyCellObject` as described in the documentation. The class should provide methods to create a cell, set a value to the cell, and get the value from the cell, including safety checks and reference management. Requirements: 1. **Class Definition**: Define a class named `Cell`. 2. **Initialization**: - The class should be initialized with an optional value. 3. **Methods**: - `get_value(self) -> Any`: Returns the current value of the cell. If the cell is empty (`None`), it should return `None`. - `set_value(self, value: Any) -> None`: Sets the value of the cell to the provided value. - `clear_value(self) -> None`: Clears the current value in the cell (sets it to `None`). 4. **Error Handling**: - Raise a `TypeError` if an invalid operation is attempted, such as setting an invalid type of value. 5. **Reference Management**: - Keep track of the number of times the cell has been set and cleared. - Implement the reference counting mechanism (a method to return the current count). Constraints: - **Performance**: Ensure the methods operate efficiently without unnecessary overhead. - **Safety**: Implement necessary checks to avoid issues like setting values that are not allowed. Example Usage: ```python # Define a cell with an initial value cell = Cell(10) # Get the value of the cell assert cell.get_value() == 10 # Set a new value to the cell cell.set_value(20) assert cell.get_value() == 20 # Clear the value of the cell cell.clear_value() assert cell.get_value() is None # Check reference count management cell.set_value(30) cell.set_value(40) assert cell.get_ref_count() == 2 # Two sets performed cell.clear_value() assert cell.get_ref_count() == 3 # Clear called once ``` Additional Guidelines: - Ensure your class handles all edge cases, such as setting and retrieving `None`. - You may use Python\'s `type` function to check for type compatibility where necessary. - Document your code and provide necessary comments for clarity.","solution":"class Cell: def __init__(self, value=None): self._value = value self._ref_count = 0 if value is None else 1 def get_value(self): return self._value def set_value(self, value): self._value = value self._ref_count += 1 def clear_value(self): self._value = None self._ref_count += 1 def get_ref_count(self): return self._ref_count"},{"question":"**Question: Implement and Demonstrate Terminal Mode Switching** You are tasked with writing a Python program that demonstrates the use of both `setraw` and `setcbreak` functions from the `tty` module to manipulate the terminal\'s I/O modes. # Objectives: 1. Implement two functions, `enable_raw_mode(fd)` and `enable_cbreak_mode(fd)`, each of which sets the terminal to raw and cbreak modes respectively. 2. Implement a main function that reads user input in both modes to demonstrate the different behaviors. # Function Definitions: - `enable_raw_mode(fd: int) -> None`: * Changes the terminal mode of the provided file descriptor `fd` to raw mode. * Uses `tty.setraw(fd)` to achieve this. - `enable_cbreak_mode(fd: int) -> None`: * Changes the terminal mode of the provided file descriptor `fd` to cbreak mode. * Uses `tty.setcbreak(fd)` to achieve this. # Main Function: Write a main function that: 1. Gets the file descriptor for the terminal (usually using `sys.stdin.fileno()`). 2. Switches the terminal to raw mode and reads and prints characters one by one until the user presses \'q\'. 3. Switches the terminal to cbreak mode and reads and prints characters one by one until the user presses \'q\'. 4. Resets the terminal to the original mode before exiting. # Input and Output Format: - While in raw mode, all characters should be printed as they are typed. Special characters (like Ctrl+C) should not interrupt the program. - While in cbreak mode, the terminal should allow typical control character behaviors but still print each typed character immediately. # Constraints: - The program should function on Unix-like systems where the `tty` and `termios` modules are available. - You must ensure that the terminal state is restored to its original mode upon exiting (even if the program is interrupted). # Example: ```python import tty import termios import sys import atexit def enable_raw_mode(fd): tty.setraw(fd) def enable_cbreak_mode(fd): tty.setcbreak(fd) def restore_terminal_mode(fd, original_attributes): termios.tcsetattr(fd, termios.TCSAFLUSH, original_attributes) def main(): fd = sys.stdin.fileno() original_attributes = termios.tcgetattr(fd) atexit.register(restore_terminal_mode, fd, original_attributes) try: print(\\"Switching to raw mode. Press \'q\' to exit raw mode.\\") enable_raw_mode(fd) while True: ch = sys.stdin.read(1) if ch == \'q\': break print(f\'Raw mode: {ch}\') print(\\"Switching to cbreak mode. Press \'q\' to exit cbreak mode.\\") enable_cbreak_mode(fd) while True: ch = sys.stdin.read(1) if ch == \'q\': break print(f\'Cbreak mode: {ch}\') finally: restore_terminal_mode(fd, original_attributes) print(\\"Terminal mode restored\\") if __name__ == \\"__main__\\": main() ``` Ensure your solution handles these aspects correctly.","solution":"import tty import termios import sys import atexit def enable_raw_mode(fd): Changes the terminal mode of the provided file descriptor to raw mode. tty.setraw(fd) def enable_cbreak_mode(fd): Changes the terminal mode of the provided file descriptor to cbreak mode. tty.setcbreak(fd) def restore_terminal_mode(fd, original_attributes): Restores the terminal to its original mode. termios.tcsetattr(fd, termios.TCSAFLUSH, original_attributes) def main(): fd = sys.stdin.fileno() original_attributes = termios.tcgetattr(fd) atexit.register(restore_terminal_mode, fd, original_attributes) try: print(\\"Switching to raw mode. Press \'q\' to exit raw mode.\\") enable_raw_mode(fd) while True: ch = sys.stdin.read(1) if ch == \'q\': break print(f\'Raw mode: {ch}\') print(\\"Switching to cbreak mode. Press \'q\' to exit cbreak mode.\\") enable_cbreak_mode(fd) while True: ch = sys.stdin.read(1) if ch == \'q\': break print(f\'Cbreak mode: {ch}\') finally: restore_terminal_mode(fd, original_attributes) print(\\"Terminal mode restored\\") if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with creating a Python function that connects to a POP3 server, authenticates using a provided username and password, retrieves the list of emails, and prints the subject of each email. You will use the **poplib** module to accomplish this. Function Signature ```python def retrieve_email_subjects(host: str, username: str, password: str, port: int = 110, use_ssl: bool = False) -> List[str]: pass ``` Input - `host`: A string representing the hostname of the POP3 server. - `username`: A string for the username to authenticate with the POP3 server. - `password`: A string for the password to authenticate with the POP3 server. - `port`: An integer specifying the port number; defaults to 110 for non-SSL and 995 for SSL connections. - `use_ssl`: A boolean indicating if the connection should use SSL; defaults to `False`. Output - A list of strings, where each string is the subject of an email in the inbox. Constraints - If `use_ssl` is `True`, connect to the server using the `POP3_SSL` class. - If `use_ssl` is `False`, connect using the `POP3` class. - You should handle the `poplib.error_proto` exception to print an appropriate error message and return an empty list. - You should ensure the connection is safely closed using `quit()` to avoid leaving the mailbox in an inconsistent state. Example ```python subjects = retrieve_email_subjects(host=\'pop.example.com\', username=\'testuser\', password=\'testpass\', port=110, use_ssl=False) print(subjects) ``` Additional Information - The email subjects can be extracted from the email headers. Look for the \\"Subject\\" field in the email header lines. - You can ignore parsing multipart email content and focus only on reading the headers. Performance Requirements - The function should handle up to 1000 email subjects efficiently. - Ensure proper usage of network connections and avoid keeping connections open longer than necessary. Hints: - Use the `top(which, howmuch)` method to retrieve email headers without marking emails as read. - Combine usage of `user(username)` and `pass_(password)` for authentication. - Handle each step of the process with appropriate error checking and debug logging if necessary.","solution":"import poplib from typing import List from email.parser import HeaderParser def retrieve_email_subjects(host: str, username: str, password: str, port: int = 110, use_ssl: bool = False) -> List[str]: try: if use_ssl: mail_server = poplib.POP3_SSL(host, port) else: mail_server = poplib.POP3(host, port) mail_server.user(username) mail_server.pass_(password) email_count, total_size = mail_server.stat() subjects = [] for i in range(1, email_count + 1): response, lines, octets = mail_server.top(i, 0) msg = b\'n\'.join(lines).decode(\'utf-8\') header = HeaderParser().parsestr(msg) subject = header.get(\'subject\', \'No Subject\') subjects.append(subject) mail_server.quit() return subjects except poplib.error_proto as e: print(f\\"POP3 protocol error: {e}\\") return [] except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"Coding Assessment Question # Objective Implement a Decision Tree Classifier using the scikit-learn library and evaluate its performance on a given dataset. # Task You are required to write a function named `train_and_evaluate_decision_tree` that: 1. Takes in two parameters: - A DataFrame `data` where the last column is the target variable to be predicted. - A list of feature names `features` which specifies the columns to be used as the input features. 2. Splits the dataset into training and testing sets using a 70-30 split ratio. 3. Trains a Decision Tree Classifier on the training set. 4. Predicts the target on the testing set. 5. Returns a dictionary containing: - `accuracy_score`: The accuracy score of the model on the testing set. - `confusion_matrix`: The confusion matrix of predictions on the testing set. # Function Signature ```python def train_and_evaluate_decision_tree(data: pd.DataFrame, features: list) -> dict: pass ``` # Constraints - Use `train_test_split` from sklearn to split the data. - Use `DecisionTreeClassifier` from sklearn to train the model. - Use `accuracy_score` and `confusion_matrix` from sklearn to evaluate the model. # Input - `data`: A Pandas DataFrame where the last column is the target variable. - `features`: A list of column names to be used as features for the model. # Output - A dictionary containing: - `accuracy_score`: A float representing the accuracy of the model. - `confusion_matrix`: A 2D list representing the confusion matrix of the predictions. # Example ```python import pandas as pd # Example DataFrame data = pd.DataFrame({ \'feature1\': [1, 2, 3, 4, 5, 6], \'feature2\': [2, 3, 4, 5, 6, 7], \'target\': [0, 1, 0, 1, 0, 1] }) features = [\'feature1\', \'feature2\'] result = train_and_evaluate_decision_tree(data, features) # Expected output format (actual values may vary) # { # \'accuracy_score\': 0.5, # \'confusion_matrix\': [ # [1, 1], # [1, 1] # ] # } ``` # Evaluation Criteria - Correctness: The function should accurately train, predict, and evaluate the decision tree model. - Completeness: The solution should correctly handle the data splitting, model training, prediction, and evaluation steps. - Efficiency: The solution should follow best practices in using scikit-learn for model training and evaluation.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score, confusion_matrix def train_and_evaluate_decision_tree(data: pd.DataFrame, features: list) -> dict: # Define target variable and features X = data[features] y = data.iloc[:, -1] # Split the dataset into training and testing sets with a 70-30 split ratio X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42) # Initialize the Decision Tree Classifier clf = DecisionTreeClassifier() # Train the model clf.fit(X_train, y_train) # Predict the target on the testing set y_pred = clf.predict(X_test) # Calculate accuracy score and confusion matrix accuracy = accuracy_score(y_test, y_pred) confusion = confusion_matrix(y_test, y_pred) # Return results as dictionary result = { \\"accuracy_score\\": accuracy, \\"confusion_matrix\\": confusion.tolist() # Convert to list for better readability } return result"},{"question":"# Question: Accessing and Manipulating Python Configuration Information Using the `sysconfig` module in Python, write a function `extract_python_config_info()` that retrieves and processes specific configuration and installation path details. The function should perform the following tasks: 1. Retrieve the platform name. 2. Retrieve the Python version in the \\"MAJOR.MINOR\\" format. 3. Retrieve the default scheme name for the current platform. 4. Retrieve all configuration variables and their values. 5. Retrieve the paths for the default scheme. The output should be a dictionary with the following structure: ```python { \\"platform\\": \\"platform_name\\", \\"python_version\\": \\"MAJOR.MINOR\\", \\"default_scheme\\": \\"scheme_name\\", \\"config_vars\\": { ... }, # dictionary of all config vars \\"paths\\": { ... } # dictionary of paths for the default scheme } ``` # Constraints: - You must use the functions provided by the `sysconfig` module to retrieve the necessary information. - Handle cases where some values might not be found by ensuring the dictionary remains complete with possible `None` values. Input - The function does not take any input. Output - The function returns a dictionary. # Example: ```python import sysconfig def extract_python_config_info(): info = { \\"platform\\": sysconfig.get_platform(), \\"python_version\\": sysconfig.get_python_version(), \\"default_scheme\\": sysconfig.get_default_scheme(), \\"config_vars\\": sysconfig.get_config_vars(), \\"paths\\": sysconfig.get_paths() } return info # Example call to the function result = extract_python_config_info() print(result) ``` # Evaluation Criteria: - Correct usage of the `sysconfig` module functions. - Accurate retrieval and organization of the required information. - Correct handling of return values, especially in cases where some configuration variables might be absent.","solution":"import sysconfig def extract_python_config_info(): Extracts Python configuration and installation path details. Returns: dict: A dictionary with platform, Python version, default scheme, all config vars, and paths. info = { \\"platform\\": sysconfig.get_platform(), \\"python_version\\": sysconfig.get_python_version(), \\"default_scheme\\": sysconfig.get_default_scheme(), \\"config_vars\\": sysconfig.get_config_vars(), \\"paths\\": sysconfig.get_paths() } return info"},{"question":"# Question You are required to write a Python function that uses the `traceback` module to handle and format exceptions in a specific way. Given a Python function that may throw an exception, you need to capture the traceback information, format it, and print it out with additional custom formatting. Task Implement a function `capture_and_format_traceback` which takes another function `func` and its arguments `*args` and `**kwargs`. The `capture_and_format_traceback` should: 1. Execute the function `func` with the given arguments. 2. If an exception occurs during execution, capture the traceback and format it such that: - Each line of the traceback is prefixed with \\"Error Line: \\". - Limit the traceback to the last 3 entries. - Include the exception type and message at the end of the formatted traceback. 3. Print the custom formatted traceback to `sys.stdout`. Implementation ```python import traceback import sys def capture_and_format_traceback(func, *args, **kwargs): Capture and format traceback with custom format. Parameters: func (callable): The function to execute. *args: Variable length argument list for func. **kwargs: Arbitrary keyword arguments for func. try: func(*args, **kwargs) except Exception as e: tb = sys.exc_info()[2] formatted_tb = traceback.format_tb(tb, limit=3) formatted_traceback = \\"Error Line: n\\".join(formatted_tb) exception_message = traceback.format_exception_only(type(e), e) print(f\\"Traceback (most recent call last):nError Line: {formatted_traceback}\\") print(f\\"{exception_message[-1]}\\") # Example usage: def example_function(): return 1 / 0 # This will raise a ZeroDivisionError, which will be caught and formatted. capture_and_format_traceback(example_function) ``` Constraints 1. The traceback should only show the last 3 entries. 2. The custom formatted output should prefix each line of the traceback with \\"Error Line: \\". 3. The solution should work with any function that may raise an exception due to incorrect inputs or logic errors. Example Suppose we have a function with nested calls that causes an exception: ```python def level_one(): level_two() def level_two(): level_three() def level_three(): raise ValueError(\\"An example error\\") ``` Running `capture_and_format_traceback(level_one)` should produce an output similar to: ``` Traceback (most recent call last): Error Line: File \\"script.py\\", line XX, in level_one level_two() Error Line: File \\"script.py\\", line XX, in level_two level_three() Error Line: File \\"script.py\\", line XX, in level_three raise ValueError(\\"An example error\\") ValueError: An example error ``` Note: Replace XX with appropriate line numbers in your script.","solution":"import traceback import sys def capture_and_format_traceback(func, *args, **kwargs): Capture and format traceback with custom format. Parameters: func (callable): The function to execute. *args: Variable length argument list for func. **kwargs: Arbitrary keyword arguments for func. try: func(*args, **kwargs) except Exception as e: tb = sys.exc_info()[2] formatted_tb = traceback.format_tb(tb, limit=3) formatted_traceback = \\"Error Line: \\".join(formatted_tb) exception_message = traceback.format_exception_only(type(e), e) print(f\\"Traceback (most recent call last):nError Line: {formatted_traceback}\\", end=\\"\\") print(f\\"{exception_message[-1]}\\", end=\\"\\") # Example usage: def example_function(): return 1 / 0 # This will raise a ZeroDivisionError, which will be caught and formatted. capture_and_format_traceback(example_function)"},{"question":"**Objective:** Implement a function that uses the `pkgutil` module to discover and list all modules within a specified package path, and fetch the content of a specific resource file from one of the modules. # Problem Statement You are required to implement a function `discover_and_fetch_modules(path, resource_module, resource_file)`. This function should perform the following tasks: 1. **Discover all modules:** It should use the `pkgutil.iter_modules` function to discover all modules in the provided `path`. 2. **List modules:** It should return a list of names of all discovered modules. 3. **Fetch resource content:** Using the `pkgutil.get_data` function, it should fetch the content of a specified resource file from a specified module. 4. **Return results:** The function should return a tuple where the first element is a list of module names and the second element is the binary content of the resource file from the specified module. # Function Signature ```python def discover_and_fetch_modules(path, resource_module, resource_file): Discover all modules in the given path and fetch the content of a specific resource file from one module. Parameters: - path (str): The path to the directory containing the package. - resource_module (str): The name of the module which contains the resource file. - resource_file (str): The relative path to the resource file within the module. Returns: - tuple: A tuple containing: 1. A list of names of all discovered modules. 2. The binary content of the resource file from the specified module. pass ``` # Inputs - `path` (str): Directory path containing the package/modules. - `resource_module` (str): Name of the module to fetch the resource from (in the format `\'package.module\'` or just `\'module\'`). - `resource_file` (str): Relative path to the resource file within the module. # Outputs - A tuple containing: - A list of strings: Names of all discovered modules. - A binary string: Contents of the resource file from the specified module. # Constraints 1. The provided `path` should be a valid directory on the filesystem. 2. The specified `resource_module` should be among the discovered modules. 3. The specified `resource_file` should exist within the specified module. 4. Use the `pkgutil` module for discovering modules and fetching the resource file. # Example Usage Suppose you have a directory `./mypackage/` containing several modules, and one of the modules `moduleA` contains a resource file `data/resource.txt`. You can use the function as follows: ```python modules, data = discover_and_fetch_modules(\'./mypackage\', \'moduleA\', \'data/resource.txt\') print(modules) # Output: [\'moduleA\', \'moduleB\', \'moduleC\'] print(data) # Output: b\'Content of the resource file\' ``` **Note:** The specified paths and module/resource names should be adjusted according to the actual file structure in a test environment.","solution":"import pkgutil def discover_and_fetch_modules(path, resource_module, resource_file): Discover all modules in the given path and fetch the content of a specific resource file from one module. Parameters: - path (str): The path to the directory containing the package. - resource_module (str): The name of the module which contains the resource file. - resource_file (str): The relative path to the resource file within the module. Returns: - tuple: A tuple containing: 1. A list of names of all discovered modules. 2. The binary content of the resource file from the specified module. # Discover all modules in the given path discovered_modules = [name for _, name, _ in pkgutil.iter_modules([path])] # Fetch the content of the specified resource file resource_content = pkgutil.get_data(resource_module, resource_file) return discovered_modules, resource_content"},{"question":"**Coding Assessment Question:** # Objective Write a Python program that demonstrates the use of both `asyncio` and `socket` modules for creating a simple asynchronous TCP server that echoes back any data it receives from clients. # Requirements 1. **Server Implementation**: - Use the `asyncio` module to manage asynchronous I/O operations. - Use the `socket` module to create a TCP server. - The server should accept connections from multiple clients asynchronously. - For each client, the server should read any data sent by the client and immediately send it back (echo it). 2. **Client Implementation**: - Use the `asyncio` module to create an asynchronous client that connects to the TCP server. - The client should send some data to the server and print the echoed response from the server. # Constraints and Limitations - The server should listen on `localhost` and a specified port (for example, port 8888). - Both the server and client should handle all exceptions gracefully. - The server should be capable of handling multiple clients simultaneously. - The client should demonstrate sending and receiving data by sending a specific message (e.g., \\"Hello, World!\\") and printing the server\'s response. # Expected Input and Output - **Server**: No input required. The server should run indefinitely and handle incoming client connections. - **Client**: Connect to the server, send the message \\"Hello, World!\\", and print the echoed response. **Example Output**: ``` Client connected to the server. Sent: Hello, World! Received: Hello, World! ``` # Performance Requirements - The server should handle high loads, ensuring that multiple client connections are handled concurrently without blocking. # Hints - Use `asyncio.start_server` to create the server and manage client connections asynchronously. - Use `asyncio.open_connection` to create the client and connect to the server. - Use `socket` to create the underlying TCP connections for the server. # Implementation Details Provide the implementation for both the server and client in separate functions or scripts. Testing can be done by running the server script and then running multiple instances of the client script to demonstrate the server\'s capability to handle multiple connections. **Note**: The provided code should be well-documented and include comments explaining the key parts of the code.","solution":"import asyncio import socket async def handle_client(reader, writer): try: data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") print(f\\"Send: {message}\\") writer.write(data) await writer.drain() except Exception as e: print(f\\"Error while handling client: {e}\\") finally: print(\\"Close the connection\\") writer.close() await writer.wait_closed() async def run_server(host, port): server = await asyncio.start_server(handle_client, host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def client(host, port, message): try: reader, writer = await asyncio.open_connection(host, port) print(f\'Send: {message}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') except Exception as e: print(f\\"Error in client: {e}\\") finally: print(\\"Close the connection\\") writer.close() await writer.wait_closed() def main(): host = \'127.0.0.1\' port = 8888 loop = asyncio.get_event_loop() try: loop.run_until_complete(client(host, port, \\"Hello, World!\\")) finally: loop.close() if __name__ == \\"__main__\\": asyncio.run(run_server(\'127.0.0.1\', 8888))"},{"question":"# Task: Implement a Dynamic Module Importer and Manager In this task, you are required to implement a function-based dynamic module importer using the `importlib` package. The function should be able to import a module dynamically, reload it if itâ€™s already imported (using caching), and provide functionality to find specifications and manage bytecode caching. # Function Requirements: 1. **Signature**: ```python def dynamic_module_manager(module_name: str, reload_module: bool = False) -> ModuleType: ``` 2. **Parameters**: - `module_name` (str): The name of the module to be imported. - `reload_module` (bool): If set to True, reload the module if it is already imported. 3. **Returns**: - A module type object representing the imported (or reloaded) module. # Functionality: 1. **Dynamic Import**: - Use `importlib.import_module` to import the specified module dynamically. 2. **Reloading**: - If `reload_module` is `True` and the module is already imported, reload the module using `importlib.reload`. 3. **Specification Management**: - Retrieve and manage the module specification using `importlib.util.find_spec`. 4. **Bytecode Caching**: - Use `importlib.util.cache_from_source` to handle bytecode caching if applicable. 5. **Performance**: - Ensure that the dynamic importing and reloading operations are efficient and do not reload unnecessary modules. # Example Usage: ```python # Loading a module dynamically math_module = dynamic_module_manager(\'math\') # Reloading a previously imported module math_module_reloaded = dynamic_module_manager(\'math\', reload_module=True) ``` # Constraints: - Do not use any external libraries other than `importlib` and built-ins. - Ensure the function handles non-existent modules gracefully by raising appropriate exceptions. - The function should support modules with both source code and bytecode.","solution":"import importlib import importlib.util from types import ModuleType def dynamic_module_manager(module_name: str, reload_module: bool = False) -> ModuleType: Dynamically imports a module and optionally reloads it. Args: - module_name (str): The name of the module to be imported. - reload_module (bool): If set to True, reloads the module if it is already imported. Returns: - A module type object representing the imported (or reloaded) module. try: # Check if the module is already imported if module_name in importlib.sys.modules: module = importlib.sys.modules[module_name] if reload_module: # Reload the module if requested module = importlib.reload(module) else: # Import the module dynamically module = importlib.import_module(module_name) # Find the module specification spec = importlib.util.find_spec(module_name) return module except ModuleNotFoundError as e: raise ImportError(f\\"The module \'{module_name}\' could not be found.\\") from e"},{"question":"**Coding Assessment Question: Understanding and Implementing Function Transformations Using `torch.func`** # Objective Implement a function using PyTorch that computes a specific mathematical operation on batched tensor inputs and ensure it adheres to the constraints and limitations of the `vmap` transformation provided by `torch.func`. # Problem Statement You are given a function `f(x)` that applies the following operations to a tensor `x`: 1. Computes the sine of `x`. 2. Calculates the square of the sine value. 3. Returns the sum of the squared sine values. You need to: 1. Transform this function to make it compatible with `torch.func.vmap`. 2. Create a batched version of this function that can be applied to a batch of inputs. 3. Ensure the transformed function adheres to the limitations specified, such as avoiding in-place operations and handling randomness correctly. # Function Signature ```python import torch from torch.func import vmap, grad def transformed_f(x: torch.Tensor) -> torch.Tensor: Transforms the given function f to be compatible with torch.func.vmap. Parameters: x (torch.Tensor): A tensor of arbitrary shape. Returns: torch.Tensor: A transformed tensor after applying the operations. pass def batched_f(inputs: torch.Tensor) -> torch.Tensor: Applies the transformed function to a batch of inputs using vmap. Parameters: inputs (torch.Tensor): A batch of input tensors of shape (batch_size, *). Returns: torch.Tensor: A tensor of results after applying the transformed function to each input in the batch. pass ``` # Implementation Requirements 1. The implementation of `transformed_f` should use pure functions and avoid any in-place operations incompatible with `vmap`. 2. The `batched_f` function should use `torch.func.vmap` to apply `transformed_f` across a batch of inputs. 3. Ensure that the behavior of `transformed_f` remains consistent with the individual application of the non-batched function `f`. # Constraints 1. `inputs` tensor provided to `batched_f` will have a shape of (batch_size, *) where `batch_size` is the number of items in the batch and `*` represents the shape of individual input tensors. 2. Ensure there are no global variable assignments within `transformed_f`. # Example ```python # Example input inputs = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]) # Applying batched_f results = batched_f(inputs) # Example output (values may vary due to randomness) print(results) # tensor([0.0147, 0.1017]) ``` Note: The exact output values will depend on the implementation of the mathematical operations within the function. # Evaluation Criteria - Correct transformation of the function to be compatible with `vmap`. - Proper use of `torch.func.vmap` to handle batched input. - Adherence to the constraints provided. - Clarity and efficiency of the code.","solution":"import torch from torch.func import vmap def f(x: torch.Tensor) -> torch.Tensor: # Compute sine of x, then the square of the sine, and return the sum of the squared sine values sine_x = torch.sin(x) squared_sine_x = sine_x ** 2 return torch.sum(squared_sine_x) def transformed_f(x: torch.Tensor) -> torch.Tensor: Transforms the given function f to be compatible with torch.func.vmap. Parameters: x (torch.Tensor): A tensor of arbitrary shape. Returns: torch.Tensor: A transformed tensor after applying the operations. return f(x) def batched_f(inputs: torch.Tensor) -> torch.Tensor: Applies the transformed function to a batch of inputs using vmap. Parameters: inputs (torch.Tensor): A batch of input tensors of shape (batch_size, *). Returns: torch.Tensor: A tensor of results after applying the transformed function to each input in the batch. # Use vmap to apply transformed_f to each input in the batch return vmap(transformed_f)(inputs)"},{"question":"# Question: You are tasked with creating a visual representation of data using seaborn\'s HUSL color palettes. The performance of your function will be assessed based on the correct usage of seaborn\'s `husl_palette` function and its parameters. Requirements: Create a function `create_custom_husl_palette(num_colors, lightness, saturation, start_hue, continuous)` that returns a seaborn color palette based on the following inputs: - `num_colors` (int): The number of colors to generate in the palette. Must be an integer between 1 and 20 inclusive. - `lightness` (float): Lightness of the colors, ranging from 0 to 1. - `saturation` (float): Saturation of the colors, ranging from 0 to 1. - `start_hue` (float): Starting point for hue sampling, ranging from 0 to 1. - `continuous` (bool): If True, returns a continuous colormap; if False, returns a discrete list of colors. Input: - `num_colors` (integer): An integer value indicating the number of colors. - `lightness` (float): A float value indicating the lightness of colors from 0 to 1. - `saturation` (float): A float value indicating the saturation of colors from 0 to 1. - `start_hue` (float): A float value indicating the starting hue from 0 to 1. - `continuous` (bool): A boolean value indicating if the colormap should be continuous. Output: - Returns a seaborn color palette based on the provided parameters. The output type varies: - If `continuous` is True, returns a seaborn continuous colormap. - If `continuous` is False, returns a list of RGB colors created by the `husl_palette`. Constraints: - `num_colors` should be between 1 and 20 inclusive. - `lightness`, `saturation`, and `start_hue` should all be between 0 and 1 inclusive. Example: ```python def create_custom_husl_palette(num_colors, lightness, saturation, start_hue, continuous): import seaborn as sns return sns.husl_palette(n_colors=num_colors, l=lightness, s=saturation, h=start_hue, as_cmap=continuous) # Example use case palette = create_custom_husl_palette(8, 0.5, 0.7, 0.3, False) print(palette) ``` # Note: In your solution, ensure all parameters are correctly validated and handled to meet the expected input constraints.","solution":"import seaborn as sns def create_custom_husl_palette(num_colors, lightness, saturation, start_hue, continuous): Generates a HUSL color palette using seaborn\'s husl_palette function. Parameters: - num_colors (int): The number of colors to generate in the palette. Must be between 1 and 20 inclusive. - lightness (float): Lightness of the colors, ranging from 0 to 1. - saturation (float): Saturation of the colors, ranging from 0 to 1. - start_hue (float): Starting point for hue sampling, ranging from 0 to 1. - continuous (bool): If True, returns a continuous colormap; if False, returns a discrete list of colors. Returns: - palette: A seaborn color palette or a continuous colormap based on the input parameters. if not (1 <= num_colors <= 20): raise ValueError(\\"num_colors must be between 1 and 20 inclusive.\\") if not (0 <= lightness <= 1): raise ValueError(\\"lightness must be between 0 and 1 inclusive.\\") if not (0 <= saturation <= 1): raise ValueError(\\"saturation must be between 0 and 1 inclusive.\\") if not (0 <= start_hue <= 1): raise ValueError(\\"start_hue must be between 0 and 1 inclusive.\\") return sns.husl_palette(n_colors=num_colors, l=lightness*100, s=saturation*100, h=start_hue*360, as_cmap=continuous)"},{"question":"**Question: Implement and Compare Principal Component Analysis and Independent Component Analysis** **Problem Statement:** You are given a dataset consisting of multiple features. Your task is to implement two different decomposition techniques: Principal Component Analysis (PCA) and Independent Component Analysis (ICA). You must apply these techniques to the dataset, project it to a lower dimensional space, and compare the results by visualizing the first two components and calculating the explained variance (for PCA) and the kurtosis (for ICA). Additionally, implement a classification task using the decomposed data to compare their effectiveness. **Dataset:** - Use the `Iris` dataset available in `sklearn.datasets`. **Steps to follow:** 1. **Load the dataset:** - Load the Iris dataset and separate it into features (X) and target labels (y). 2. **Implement PCA:** - Apply PCA with 2 components. - Project the data onto the first two principal components. - Calculate the explained variance ratio. - Visualize the first two principal components in a 2D scatter plot. 3. **Implement ICA:** - Apply ICA with 2 components. - Project the data onto the first two independent components. - Calculate the kurtosis of the independent components. - Visualize the first two independent components in a 2D scatter plot. 4. **Comparison:** - Implement a classification task (e.g., using a Support Vector Classifier) on the original, PCA-transformed, and ICA-transformed datasets. - Evaluate and compare the classification performance using accuracy scores. **Constraints:** - You must use `sklearn` for decomposition and classification tasks. - Ensure to scale the data before applying any decomposition. - For visualization, use `matplotlib` or any other preferred visualization library. **Input:** - None, load the Iris dataset from `sklearn.datasets`. **Output:** - Explained variance ratio from PCA. - Kurtosis values from ICA. - 2D scatter plots for PCA and ICA results. - Accuracy scores for classification using original, PCA-transformed, and ICA-transformed datasets. **Code Template:** ```python import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA, FastICA from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from scipy.stats import kurtosis # Load the dataset data = load_iris() X = data.data y = data.target # Standardize the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Implement PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) explained_variance = pca.explained_variance_ratio_ # Implement ICA ica = FastICA(n_components=2, random_state=0) X_ica = ica.fit_transform(X_scaled) components_kurtosis = kurtosis(X_ica) # Visualize PCA plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\') plt.title(\'PCA: First two components\') plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') # Visualize ICA plt.subplot(1, 2, 2) plt.scatter(X_ica[:, 0], X_ica[:, 1], c=y, cmap=\'viridis\') plt.title(\'ICA: First two components\') plt.xlabel(\'Independent Component 1\') plt.ylabel(\'Independent Component 2\') plt.tight_layout() plt.show() # Classification on original data X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) svc = SVC() svc.fit(X_train, y_train) y_pred = svc.predict(X_test) accuracy_original = accuracy_score(y_test, y_pred) # Classification on PCA-transformed data X_train_pca, X_test_pca = train_test_split(X_pca, test_size=0.3, random_state=42) svc.fit(X_train_pca, y_train) y_pred_pca = svc.predict(X_test_pca) accuracy_pca = accuracy_score(y_test, y_pred_pca) # Classification on ICA-transformed data X_train_ica, X_test_ica = train_test_split(X_ica, test_size=0.3, random_state=42) svc.fit(X_train_ica, y_train) y_pred_ica = svc.predict(X_test_ica) accuracy_ica = accuracy_score(y_test, y_pred_ica) # Output Results print(\\"Explained variance ratio (PCA):\\", explained_variance) print(\\"Kurtosis of components (ICA):\\", components_kurtosis) print(\\"Classification Accuracy on Original Data:\\", accuracy_original) print(\\"Classification Accuracy on PCA Data:\\", accuracy_pca) print(\\"Classification Accuracy on ICA Data:\\", accuracy_ica) ``` **Expectations:** - Understand the difference between PCA and ICA. - Demonstrate ability to preprocess data and apply different decomposition techniques. - Analyze and interpret the results from the decompositions. - Implement and evaluate a classification task using the decomposed data.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA, FastICA from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from scipy.stats import kurtosis def perform_pca_ica(): # Load the dataset data = load_iris() X = data.data y = data.target # Standardize the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Implement PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) explained_variance = pca.explained_variance_ratio_ # Implement ICA ica = FastICA(n_components=2, random_state=0) X_ica = ica.fit_transform(X_scaled) components_kurtosis = kurtosis(X_ica) # Visualize PCA plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\') plt.title(\'PCA: First two components\') plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') # Visualize ICA plt.subplot(1, 2, 2) plt.scatter(X_ica[:, 0], X_ica[:, 1], c=y, cmap=\'viridis\') plt.title(\'ICA: First two components\') plt.xlabel(\'Independent Component 1\') plt.ylabel(\'Independent Component 2\') plt.tight_layout() plt.show() # Classification on original data X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) svc = SVC() svc.fit(X_train, y_train) y_pred = svc.predict(X_test) accuracy_original = accuracy_score(y_test, y_pred) # Classification on PCA-transformed data X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.3, random_state=42) svc.fit(X_train_pca, y_train_pca) y_pred_pca = svc.predict(X_test_pca) accuracy_pca = accuracy_score(y_test_pca, y_pred_pca) # Classification on ICA-transformed data X_train_ica, X_test_ica, y_train_ica, y_test_ica = train_test_split(X_ica, y, test_size=0.3, random_state=42) svc.fit(X_train_ica, y_train_ica) y_pred_ica = svc.predict(X_test_ica) accuracy_ica = accuracy_score(y_test_ica, y_pred_ica) # Output Results result = { \\"explained_variance_ratio\\": explained_variance, \\"kurtosis_components\\": components_kurtosis, \\"accuracy_original\\": accuracy_original, \\"accuracy_pca\\": accuracy_pca, \\"accuracy_ica\\": accuracy_ica } return result"},{"question":"# Dynamic Class Creation and Type Manipulation in Python Objective Create a dynamic class implementation using the `types` module, ensuring that the class supports specific attributes and methods. Task Implement a Python function `create_dynamic_class` that: 1. Dynamically creates a class named `DynamicClass`. 2. The class should inherit from a base class (default to `object` if none provided). 3. The class should include a class method `describe_class` that returns the name of the class and its base classes. 4. The class should include an instance method `set_and_get_attr` that: - Sets an attribute on the instance. - Retrieves and returns the value of that attribute. Function Signature ```python def create_dynamic_class(base=object): Creates a dynamic class using the types module. Parameters: base (type): the base class from which DynamicClass inherits (default is `object`). Returns: type: the dynamically created class. ``` Example Usage ```python DynamicClass = create_dynamic_class() # Create an instance of the dynamic class instance = DynamicClass() # Use the instance method instance.set_and_get_attr(\'new_attribute\', 42) # Should return: 42 # Use the class method DynamicClass.describe_class() # Should return: \\"DynamicClass, bases: (object)\\" ``` Constraints 1. The solution must use the `types.new_class` or `types.prepare_class` functions for dynamic class creation. 2. No external libraries other than the Python standard library should be used. 3. The code should handle any Python version-specific nuances (where applicable). Requirements - Ensure proper documentation of the code. - Provide clear and concise docstrings for the functions and methods. - Follow Python naming conventions and best coding practices.","solution":"import types def create_dynamic_class(base=object): Creates a dynamic class using the types module. Parameters: base (type): the base class from which DynamicClass inherits (default is `object`). Returns: type: the dynamically created class. # Define the class dictionary class_dict = {} # Define the instance method \'set_and_get_attr\' def set_and_get_attr(self, attr_name, value): setattr(self, attr_name, value) return getattr(self, attr_name) # Define the class method \'describe_class\' @classmethod def describe_class(cls): class_name = cls.__name__ base_names = \', \'.join(base.__name__ for base in cls.__bases__) return f\\"{class_name}, bases: ({base_names})\\" # Add methods to the class dictionary class_dict[\'set_and_get_attr\'] = set_and_get_attr class_dict[\'describe_class\'] = describe_class # Create the new dynamic class DynamicClass = types.new_class(\'DynamicClass\', (base,), {}, lambda ns: ns.update(class_dict)) return DynamicClass"},{"question":"Matrix Multiplication with Controlled Resources using torch.xpu You are required to implement a function `matrix_multiplication_with_control` that leverages several functionalities of the `torch.xpu` module to perform matrix multiplication in a resource-controlled manner. The key requirements are: 1. **Device Availability:** Check if an XPU device is available. 2. **Device and Stream Handling:** Perform the matrix multiplication on the specified device using `StreamContext` to manage resources efficiently. 3. **Memory Management:** Track and print the following memory usage details before and after the multiplication: `memory_allocated`, `memory_reserved`, and `max_memory_allocated`. 4. **Random Number Generation:** Ensure reproducibility by setting a random seed before creating the matrices. 5. **Error Handling:** Ensure proper handling and notification if the device isn\'t available. Function Signature ```python import torch def matrix_multiplication_with_control(a_shape, b_shape, seed=42): Perform matrix multiplication on an XPU device with controlled resources. Parameters: a_shape (tuple): Shape of the first matrix (rows, columns). b_shape (tuple): Shape of the second matrix (rows, columns). Note: a_shape[1] must be equal to b_shape[0] for valid multiplication. seed (int): Seed for random number generation to ensure reproducibility. Returns: result (torch.Tensor): The resulting matrix after the multiplication. memory_logs (dict): Dictionary containing memory usage details. # Function implementation goes here ``` Constraints 1. Ensure `a_shape[1] == b_shape[0]` for valid matrix multiplication. 2. Handle cases where the device is not available gracefully. # Testing The function can be tested with various matrix shapes and the seed value. Ensure to validate it does the multiplication correctly and outputs the required memory details. Example: ```python a_shape = (200, 300) b_shape = (300, 400) result, memory_logs = matrix_multiplication_with_control(a_shape, b_shape) print(result) # Should print the resultant matrix print(memory_logs) # Should print memory usage details ``` For this task, assume the `torch.xpu` follows similar conventions as CUDA devices in PyTorch, even if you are not fully familiar with the specific XPU device. Focus on demonstrating your understanding of device context management, memory tracking, and random number generation.","solution":"import torch def matrix_multiplication_with_control(a_shape, b_shape, seed=42): Perform matrix multiplication on an XPU device with controlled resources. Parameters: a_shape (tuple): Shape of the first matrix (rows, columns). b_shape (tuple): Shape of the second matrix (rows, columns). Note: a_shape[1] must be equal to b_shape[0] for valid multiplication. seed (int): Seed for random number generation to ensure reproducibility. Returns: result (torch.Tensor): The resulting matrix after the multiplication. memory_logs (dict): Dictionary containing memory usage details. if not torch.xpu.is_available(): raise RuntimeError(\\"XPU device not available.\\") if a_shape[1] != b_shape[0]: raise ValueError(\\"Inner dimensions must match for matrix multiplication.\\") torch.manual_seed(seed) device = torch.device(\'xpu\') # Create random matrices on the XPU device with torch.xpu.stream(torch.xpu.StreamContext()): a = torch.randn(a_shape, device=device) b = torch.randn(b_shape, device=device) # Record initial memory usage initial_memory_allocated = torch.xpu.memory_allocated(device) initial_memory_reserved = torch.xpu.memory_reserved(device) # Perform matrix multiplication result = torch.matmul(a, b) # Synchronize torch.xpu.synchronize(device) # Record final memory usage final_memory_allocated = torch.xpu.memory_allocated(device) final_memory_reserved = torch.xpu.memory_reserved(device) max_memory_allocated = torch.xpu.max_memory_allocated(device) memory_logs = { \'initial_memory_allocated\': initial_memory_allocated, \'initial_memory_reserved\': initial_memory_reserved, \'final_memory_allocated\': final_memory_allocated, \'final_memory_reserved\': final_memory_reserved, \'max_memory_allocated\': max_memory_allocated } return result, memory_logs"},{"question":"# Question: Model Evaluation with Custom Scoring Metrics You have just trained several machine learning models on a classification dataset. To ensure the reliability of your models, you need to evaluate their performance using multiple scoring metrics and compare them. Additionally, for a specific business case, you need to create a custom scoring function that weights false negatives more heavily than false positives. You have the following trained models available: `model1`, `model2`, and `model3`. # Task 1. Define a list of string-based scorers to evaluate the models for standard metrics, including \'accuracy\', \'precision\', and \'recall\'. Use these scorers in a cross-validation setup. 2. Implement a custom scoring function named `custom_weighted_scoring` which: - Weighs false negatives twice as heavily as false positives. - Returns a weighted accuracy score. 3. Use cross-validation to evaluate the models with the custom scoring function and standard metrics. 4. Summarize the results in a dictionary where keys are model names and values are dictionaries containing the scores for each metric. # Input You will be provided with: - A feature matrix `X` (2D numpy array) - A label vector `y` (1D numpy array) - Three pre-trained models: `model1`, `model2`, `model3` # Output The function `evaluate_models` should return a dictionary summarizing the cross-validation results as described above. # Function Signature ```python import numpy as np from sklearn.model_selection import cross_validate from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score def custom_weighted_scoring(y_true, y_pred): Custom scoring function that weighs false negatives more heavily than false positives. Parameters: - y_true (numpy array): True labels - y_pred (numpy array): Predicted labels Returns: - score: Weighted accuracy score # Implement the custom scoring logic here pass def evaluate_models(X: np.ndarray, y: np.ndarray, model1, model2, model3) -> dict: Evaluates the given models using standard and custom scoring metrics. Parameters: - X (numpy.ndarray): Feature matrix - y (numpy.ndarray): Label vector - model1: Pre-trained model 1 - model2: Pre-trained model 2 - model3: Pre-trained model 3 Returns: - results (dict): Dictionary summarizing the cross-validation scores for each model and metric # Define standard scorers # Implement cross-validation evaluations # Collect and return the results pass ``` # Example Usage ```python # Assuming X, y, model1, model2 and model3 are predefined results = evaluate_models(X, y, model1, model2, model3) print(results) ``` # Constraints - Use 5-fold cross-validation for the evaluations. - Ensure the custom scoring function correctly weighs false negatives. - Use `make_scorer` to define the custom scoring function for use in cross-validation.","solution":"import numpy as np from sklearn.model_selection import cross_validate from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score def custom_weighted_scoring(y_true, y_pred): Custom scoring function that weighs false negatives more heavily than false positives. Parameters: - y_true (numpy array): True labels - y_pred (numpy array): Predicted labels Returns: - score: Weighted accuracy score fp = np.sum((y_true == 0) & (y_pred == 1)) fn = np.sum((y_true == 1) & (y_pred == 0)) tp = np.sum((y_true == 1) & (y_pred == 1)) tn = np.sum((y_true == 0) & (y_pred == 0)) weighted_accuracy = (tp + tn) / (tp + tn + (2 * fn) + fp) return weighted_accuracy def evaluate_models(X: np.ndarray, y: np.ndarray, model1, model2, model3) -> dict: Evaluates the given models using standard and custom scoring metrics. Parameters: - X (numpy.ndarray): Feature matrix - y (numpy.ndarray): Label vector - model1: Pre-trained model 1 - model2: Pre-trained model 2 - model3: Pre-trained model 3 Returns: - results (dict): Dictionary summarizing the cross-validation scores for each model and metric scorers = { \'accuracy\': \'accuracy\', \'precision\': \'precision\', \'recall\': \'recall\', \'custom\': make_scorer(custom_weighted_scoring) } models = {\'model1\': model1, \'model2\': model2, \'model3\': model3} results = {} for name, model in models.items(): scores = cross_validate(model, X, y, scoring=scorers, cv=5, return_train_score=False) results[name] = {metric: np.mean(scores[f\'test_{metric}\']) for metric in scorers} return results"},{"question":"# Question: Custom Image Type Detection You are given a Python script that utilizes the deprecated \\"imghdr\\" module to determine the type of image contained in a file or byte stream. Your task is to extend this module to recognize a custom image format: \\"myimg\\". The \\"myimg\\" format is simple and can be identified by the presence of a specific header at the start of the file. **Specifications**: 1. Implement a function `is_myimg(h, file)` that takes two arguments: - `h`: A byte stream (if provided). - `file`: An open file-like object (or None if the byte stream is provided). 2. The `is_myimg` function should return the string `\'myimg\'` if the byte stream starts with the bytes `b\'MYIMG\'` and `None` otherwise. 3. Add your `is_myimg` function to the list of test functions in `imghdr.tests`. 4. Write a main program that: - Accepts a path to an image file as an input. - Uses the `imghdr.what` function to determine and print the type of the image, including support for the \\"myimg\\" format. You are not required to install or use any libraries outside of the standard Python library. Ensure your code is well-documented and handles edge cases effectively. **Input**: - A path to an image file as a string (e.g., \'images/sample.myimg\'). **Output**: - A string indicating the type of the image (e.g., \'jpeg\', \'png\', \'myimg\'). **Example**: ```python # Assuming \'sample.myimg\' starts with the bytes b\'MYIMG\' image_type = detect_image_type(\'sample.myimg\') print(image_type) # Output should be \'myimg\' ``` **Constraints**: - You must not modify the existing functions in \\"imghdr\\" except to add your custom test to `imghdr.tests`. - Your solution should be compatible with Python 3.10.","solution":"import imghdr def is_myimg(h, file): Test if the provided header or file corresponds to the \'myimg\' image format. Args: h (bytes): Byte stream if provided file (file-like object): An open file-like object (or None if h is provided) Returns: str: \'myimg\' if the byte stream starts with b\'MYIMG\', otherwise None byte_sequence = h if h else file.read(10) if byte_sequence.startswith(b\'MYIMG\'): return \'myimg\' return None # Registering the custom image type in imghdr imghdr.tests.append(is_myimg) def detect_image_type(filepath): Determine and return the type of image for the given file path. Args: filepath (str): Path to the image file Returns: str: The type of the image (e.g., \'jpeg\', \'png\', \'myimg\', etc.) return imghdr.what(filepath) if __name__ == \\"__main__\\": import sys if len(sys.argv) != 2: print(\\"Usage: python script.py <image-file-path>\\") sys.exit(1) image_path = sys.argv[1] image_type = detect_image_type(image_path) print(image_type)"},{"question":"Objective: Your task is to create a function that uses the `py_compile` module to compile multiple Python source files provided as a list of file paths and handles potential compilation errors gracefully. Problem Statement: Implement a function `compile_source_files(files: List[str], optimize: int = -1, invalidation_mode: str = \'TIMESTAMP\', quiet: int = 0) -> Dict[str, str]` that takes the following inputs: - `files` - A list of strings, where each string is a path to a Python source file that needs to be compiled. - `optimize` - An integer that specifies the optimization level (default is -1). - `invalidation_mode` - A string indicating the invalidation mode (`\'TIMESTAMP\'`, `\'CHECKED_HASH\'`, `\'UNCHECKED_HASH\'`). Default is `\'TIMESTAMP\'`. - `quiet` - An integer that determines the error handling behavior. Possible values are: - `0` (default): Print errors to `sys.stderr`, - `1`: Print less verbose errors to `sys.stderr`, - `2`: Suppress all error messages. The function should return a dictionary where: - Each key is the source file path. - Each value is the path to the compiled bytecode file or an error message if the compilation failed. Constraints: 1. If `invalidation_mode` is not one of `\'TIMESTAMP\'`, `\'CHECKED_HASH\'`, or `\'UNCHECKED_HASH\'`, raise a `ValueError`. 2. Use the `py_compile.compile` function for compilation, ensuring to handle errors based on the `quiet` parameter. 3. Any raised `FileExistsError` or `py_compile.PyCompileError` should be caught and included in the output dictionary as error messages. Example: ```python import py_compile from typing import List, Dict def compile_source_files(files: List[str], optimize: int = -1, invalidation_mode: str = \'TIMESTAMP\', quiet: int = 0) -> Dict[str, str]: # Your implementation here # Usage Example files_to_compile = [\'script1.py\', \'script2.py\', \'non_existent.py\'] results = compile_source_files(files_to_compile, optimize=2, invalidation_mode=\'CHECKED_HASH\', quiet=1) print(results) # Expected Output (example) # {\'script1.py\': \'/path/to/script1.cpython-310.pyc\', \'script2.py\': \'/path/to/script2.cpython-310.pyc\', \'non_existent.py\': \'Error: [Errno 2] No such file or directory: \'non_existent.py\'\'} ``` Use the provided example output format and error messages as a guideline while implementing your function. Note: - Ensure you adhere to the constraints and handle all potential edge cases. - Test your solution rigorously with different scenarios to ensure its accuracy and robustness.","solution":"import py_compile from typing import List, Dict def compile_source_files(files: List[str], optimize: int = -1, invalidation_mode: str = \'TIMESTAMP\', quiet: int = 0) -> Dict[str, str]: if invalidation_mode not in {\'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\'}: raise ValueError(f\\"Invalid invalidation_mode: {invalidation_mode}\\") result = {} for file in files: try: compiled_file = py_compile.compile(file, cfile=None, dfile=None, doraise=True, optimize=optimize, invalidation_mode=invalidation_mode, quiet=quiet) result[file] = compiled_file except (FileNotFoundError, py_compile.PyCompileError) as e: result[file] = f\\"Error: {str(e)}\\" return result"},{"question":"Objective Design a function that uses the `decimal` module to perform high-precision arithmetic for financial calculations and another function that uses the `statistics` module to analyze the results. Problem Statement You have been hired by a financial firm to build a Python program that will perform high-precision calculations on a list of financial transactions and then analyze these transactions using statistical methods. 1. **High-precision arithmetic**: * Implement a function `perform_financial_calculations(transactions: List[str]) -> decimal.Decimal` that takes a list of financial transactions (strings representing decimal numbers) and returns the sum of these transactions using the `decimal.Decimal` type to ensure high precision. * Constraints: * Each transaction in the list is a string representation of a decimal number, and the list can have up to 10,000 transactions. * The function should handle transactions with up to 20 decimal places. 2. **Statistical Analysis**: * Implement a function `analyze_transactions(transactions: List[str]) -> Dict[str, float]` that takes the same list of financial transactions and returns a dictionary with the following key-value pairs: * `\'mean\'`: The mean of the transactions. * `\'median\'`: The median of the transactions. * `\'variance\'`: The variance of the transactions. * `\'standard_deviation\'`: The standard deviation of the transactions. * Constraints: * Use the `statistics` module for calculations. * The output values should be of type `float`. Input and Output Formats **Input:** * A list of strings, each representing a financial transaction with up to 20 decimal places. **Output:** * For `perform_financial_calculations`: A `decimal.Decimal` representing the sum of the transactions. * For `analyze_transactions`: A dictionary with keys `\'mean\'`, `\'median\'`, `\'variance\'`, and `\'standard_deviation\'`, each mapped to the corresponding statistical measure (float). Example ```python from decimal import Decimal from typing import List, Dict def perform_financial_calculations(transactions: List[str]) -> Decimal: pass def analyze_transactions(transactions: List[str]) -> Dict[str, float]: pass # Example Usage transactions = [\\"100.12345678901234567890\\", \\"200.98765432109876543210\\", \\"300.45678901234567890123\\"] print(perform_financial_calculations(transactions)) # Output: Decimal(\'601.56790012245679001223\') print(analyze_transactions(transactions)) # Output: {\'mean\': 200.52263337415226, \'median\': 200.98765432109877, \'variance\': 10000.451924193163, \'standard_deviation\': 100.0022596178465} ``` Notes 1. Ensure precision using the `decimal.Decimal` type for the financial calculations. 2. Use the `statistics` module to perform statistical analyses. 3. Handle up to 10,000 transactions efficiently.","solution":"from decimal import Decimal, getcontext from typing import List, Dict import statistics def perform_financial_calculations(transactions: List[str]) -> Decimal: # Set precision to at least handle up to 20 decimal places getcontext().prec = 30 total = Decimal(\'0\') for transaction in transactions: total += Decimal(transaction) return total def analyze_transactions(transactions: List[str]) -> Dict[str, float]: decimal_transactions = [Decimal(t) for t in transactions] float_transactions = [float(t) for t in decimal_transactions] mean = statistics.mean(float_transactions) median = statistics.median(float_transactions) variance = statistics.variance(float_transactions) standard_deviation = statistics.stdev(float_transactions) return { \'mean\': mean, \'median\': median, \'variance\': variance, \'standard_deviation\': standard_deviation }"},{"question":"**Objective:** Utilize the `fileinput` module to implement a Python function that processes multiple files, possibly including compressed files, and modifies their content in place based on a specified mapping of search-replace pairs. **Task:** Write a function `process_files(files, search_replace_dict)` that: 1. Takes a list of filenames and a dictionary of search-replace pairs. 2. Reads the content of each file line by line (supports compressed files with extensions `.gz` and `.bz2`). 3. Replaces occurrences of each key in `search_replace_dict` with its corresponding value. 4. Writes the modified content back to the original file. **Input:** - `files`: List of strings, each representing a filename (e.g., `[\\"file1.txt\\", \\"file2.txt.gz\\"]`). - `search_replace_dict`: Dictionary with search-replace pairs (e.g., `{\\"search1\\": \\"replace1\\", \\"search2\\": \\"replace2\\"}`). **Output:** - The function does not return anything. However, the files listed in `files` should be modified in place with the specified replacements applied. **Constraints:** - If a file in `files` does not exist or cannot be read, raise a `FileNotFoundError`. - Handle text files encoded in `utf-8` and compressed files with `.gz` or `.bz2` extensions. - Ensure in-place modification is atomic: either all changes are applied, or none (no partial updates). # Example: ```python # Given files: file1.txt, file2.txt.gz # Content of file1.txt: \\"This is the first test file.\\" # Content of file2.txt.gz: \\"Another test file for the process.\\" files = [\\"file1.txt\\", \\"file2.txt.gz\\"] search_replace_dict = {\\"first\\": \\"1st\\", \\"test\\": \\"exam\\"} process_files(files, search_replace_dict) # Content of file1.txt after processing: \\"This is the 1st exam file.\\" # Content of file2.txt.gz after processing: \\"Another exam file for the process.\\" ``` # Function Signature ```python def process_files(files: list, search_replace_dict: dict): pass ``` Make sure your implementation is efficient and handles edge cases like empty files or files with no search keywords present.","solution":"import fileinput import gzip import bz2 def process_files(files, search_replace_dict): for filename in files: try: if filename.endswith(\'.gz\'): with gzip.open(filename, \'rt\', encoding=\'utf-8\') as f: content = f.readlines() with gzip.open(filename, \'wt\', encoding=\'utf-8\') as f: for line in content: for search, replace in search_replace_dict.items(): line = line.replace(search, replace) f.write(line) elif filename.endswith(\'.bz2\'): with bz2.open(filename, \'rt\', encoding=\'utf-8\') as f: content = f.readlines() with bz2.open(filename, \'wt\', encoding=\'utf-8\') as f: for line in content: for search, replace in search_replace_dict.items(): line = line.replace(search, replace) f.write(line) else: with fileinput.FileInput(filename, inplace=True, backup=\'.bak\', mode=\'r\', encoding=\'utf-8\') as file: for line in file: for search, replace in search_replace_dict.items(): line = line.replace(search, replace) print(line, end=\'\') except FileNotFoundError: raise FileNotFoundError(f\\"The file {filename} does not exist or cannot be read.\\")"},{"question":"# CSV File Processing with Python **Objective**: Implement a Python function that processes a CSV file containing student scores and writes a summarized version of this data to another CSV file. **Problem Statement**: You are given a CSV file named `student_scores.csv` with the following format: ``` Name,Subject,Score Alice,Mathematics,88 Bob,Science,90 Alice,Science,79 Bob,Mathematics,85 ``` Each row corresponds to a student\'s score in a particular subject. Your task is to write a function `summarize_scores(input_file: str, output_file: str) -> None` that reads the input CSV file, calculates the average score for each student, and writes the results to the output CSV file with the format: ``` Name,Average Score Alice,83.5 Bob,87.5 ``` **Function Signature**: ```python def summarize_scores(input_file: str, output_file: str) -> None: pass ``` **Input**: - `input_file`: A string representing the path to the input CSV file (`student_scores.csv`). - `output_file`: A string representing the path to the output CSV file where summarized data should be written. **Output**: - The function should not return anything. It should create a CSV file with the summarized data. **Constraints**: - Each student may have scores for multiple subjects. - The input file is guaranteed to have the correct format. - Handle any file reading and writing exceptions gracefully. **Requirements**: - The input and output files should use a comma as the delimiter. - Ensure that the output CSV file includes a header row. # Example If the input CSV file `student_scores.csv` contains: ``` Name,Subject,Score Alice,Mathematics,88 Bob,Science,90 Alice,Science,79 Bob,Mathematics,85 ``` After calling `summarize_scores(\'student_scores.csv\', \'summary.csv\')`, the `summary.csv` file should contain: ``` Name,Average Score Alice,83.5 Bob,87.5 ``` # Notes - You may use Python\'s `csv` module to handle file reading and writing. - Consider using dictionaries to aggregate and compute the average scores for each student. **Hint**: You may find the `csv.DictReader` and `csv.DictWriter` classes helpful for processing the input and output files, respectively.","solution":"import csv def summarize_scores(input_file: str, output_file: str) -> None: student_scores = {} try: # Read the input CSV file with open(input_file, mode=\'r\') as file: csv_reader = csv.DictReader(file) # Aggregate scores for each student for row in csv_reader: name = row[\'Name\'] score = float(row[\'Score\']) if name not in student_scores: student_scores[name] = [] student_scores[name].append(score) # Calculate average scores student_averages = {name: sum(scores) / len(scores) for name, scores in student_scores.items()} # Write the output CSV file with summarized data with open(output_file, mode=\'w\', newline=\'\') as file: csv_writer = csv.writer(file) csv_writer.writerow([\'Name\', \'Average Score\']) for name, avg_score in student_averages.items(): csv_writer.writerow([name, avg_score]) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective Design and implement a solution using scikit-learn\'s dataset generators to generate, process, and visualize multiple artificial datasets and then apply a classification algorithm to these datasets. Task 1. **Dataset Generation:** - Generate three different datasets: 1. `make_blobs`: Create a multiclass dataset with three clusters. 2. `make_moons`: Create a binary classification dataset with added noise. 3. `make_regression`: Create a regression dataset with 1000 samples and 10 features. 2. **Preprocessing:** - For the `make_blobs` and `make_moons` datasets, standardize the features using `StandardScaler`. - For the `make_regression` dataset, split it into training and test sets (80-20 split). 3. **Model Application and Evaluation:** - For the `make_blobs` and `make_moons` datasets: - Train a K-Nearest Neighbors (KNN) classifier on each dataset. - Print the accuracy of the model on the dataset. - For the `make_regression` dataset: - Train a Linear Regression model on the training set. - Evaluate the model using Mean Squared Error (MSE) on the test set. 4. **Visualization:** - Create scatter plots to visualize `make_blobs` and `make_moons` datasets with their respective class labels. - Plot the first two features of the `make_regression` dataset and color the points based on their regression targets. Expected Input and Output Formats - No input is required as the datasets are to be generated within the script. - Output should include: - Accuracy scores for the models on `make_blobs` and `make_moons` datasets. - Mean Squared Error for the Linear Regression model on the `make_regression` dataset. - Visualization of the datasets and regression results. Constraints and Performance Requirements - Ensure that appropriate preprocessing steps like standardization and data splitting are performed. - Visualization should be clear and appropriately labeled. - Code should be well-documented and follow best practices. Sample Solution ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_moons, make_regression from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LinearRegression from sklearn.metrics import accuracy_score, mean_squared_error # Step 1: Dataset Generation X_blobs, y_blobs = make_blobs(centers=3, cluster_std=0.5, random_state=0) X_moons, y_moons = make_moons(noise=0.1, random_state=0) X_regression, y_regression = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=0) # Step 2: Preprocessing scaler_blobs = StandardScaler() X_blobs_scaled = scaler_blobs.fit_transform(X_blobs) scaler_moons = StandardScaler() X_moons_scaled = scaler_moons.fit_transform(X_moons) X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_regression, y_regression, test_size=0.2, random_state=0) # Step 3: Model Application and Evaluation # Classification with KNN for blobs dataset knn_blobs = KNeighborsClassifier() knn_blobs.fit(X_blobs_scaled, y_blobs) y_blobs_pred = knn_blobs.predict(X_blobs_scaled) accuracy_blobs = accuracy_score(y_blobs, y_blobs_pred) # Classification with KNN for moons dataset knn_moons = KNeighborsClassifier() knn_moons.fit(X_moons_scaled, y_moons) y_moons_pred = knn_moons.predict(X_moons_scaled) accuracy_moons = accuracy_score(y_moons, y_moons_pred) # Regression with Linear Regression for regression dataset lin_reg = LinearRegression() lin_reg.fit(X_reg_train, y_reg_train) y_reg_pred = lin_reg.predict(X_reg_test) mse_regression = mean_squared_error(y_reg_test, y_reg_pred) # Step 4: Visualization # Scatter plot for blobs dataset plt.figure(figsize=(12, 4)) plt.subplot(1, 3, 1) plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs) plt.title(\\"make_blobs dataset\\") # Scatter plot for moons dataset plt.subplot(1, 3, 2) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons) plt.title(\\"make_moons dataset\\") # Scatter plot for regression dataset plt.subplot(1, 3, 3) plt.scatter(X_reg_test[:, 0], y_reg_test, label=\'True\') plt.scatter(X_reg_test[:, 0], y_reg_pred, label=\'Predicted\', alpha=0.5) plt.title(\\"make_regression dataset\\") plt.legend() plt.tight_layout() plt.show() # Print performance metrics print(f\'Accuracy on make_blobs dataset: {accuracy_blobs:.2f}\') print(f\'Accuracy on make_moons dataset: {accuracy_moons:.2f}\') print(f\'Mean Squared Error on make_regression dataset: {mse_regression:.2f}\') ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_moons, make_regression from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.linear_model import LinearRegression from sklearn.metrics import accuracy_score, mean_squared_error def generate_and_process_datasets(): # Step 1: Dataset Generation X_blobs, y_blobs = make_blobs(centers=3, cluster_std=1.0, random_state=0) X_moons, y_moons = make_moons(noise=0.3, random_state=0) X_regression, y_regression = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=0) # Step 2: Preprocessing scaler_blobs = StandardScaler() X_blobs_scaled = scaler_blobs.fit_transform(X_blobs) scaler_moons = StandardScaler() X_moons_scaled = scaler_moons.fit_transform(X_moons) X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_regression, y_regression, test_size=0.2, random_state=0) # Step 3: Model Application and Evaluation results = {} # Classification with KNN for blobs dataset knn_blobs = KNeighborsClassifier() knn_blobs.fit(X_blobs_scaled, y_blobs) y_blobs_pred = knn_blobs.predict(X_blobs_scaled) accuracy_blobs = accuracy_score(y_blobs, y_blobs_pred) results[\'accuracy_blobs\'] = accuracy_blobs # Classification with KNN for moons dataset knn_moons = KNeighborsClassifier() knn_moons.fit(X_moons_scaled, y_moons) y_moons_pred = knn_moons.predict(X_moons_scaled) accuracy_moons = accuracy_score(y_moons, y_moons_pred) results[\'accuracy_moons\'] = accuracy_moons # Regression with Linear Regression for regression dataset lin_reg = LinearRegression() lin_reg.fit(X_reg_train, y_reg_train) y_reg_pred = lin_reg.predict(X_reg_test) mse_regression = mean_squared_error(y_reg_test, y_reg_pred) results[\'mse_regression\'] = mse_regression # Step 4: Visualization # Scatter plot for blobs dataset plt.figure(figsize=(12, 4)) plt.subplot(1, 3, 1) plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs) plt.title(\\"make_blobs dataset\\") # Scatter plot for moons dataset plt.subplot(1, 3, 2) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons) plt.title(\\"make_moons dataset\\") # Scatter plot for regression dataset plt.subplot(1, 3, 3) plt.scatter(X_reg_test[:, 0], y_reg_test, label=\'True\') plt.scatter(X_reg_test[:, 0], y_reg_pred, label=\'Predicted\', alpha=0.5) plt.title(\\"make_regression dataset\\") plt.legend() plt.tight_layout() plt.show() return results"},{"question":"**Question: Implement a function to safely extract and evaluate annotations** We need a function that safely extracts and evaluates the annotations from a given Python object, considering the best practices for accessing `__annotations__` in different versions of Python. # Function Signature ```python def extract_annotations(obj: Any, stringize_annotations: bool = False) -> dict: pass ``` # Input 1. `obj` - A Python object that can be a function, class, or module. 2. `stringize_annotations` - A boolean which when set to `True`, evaluates any stringized annotations into their actual Python types/values. # Output - A dictionary of annotations for the provided object. If there are no annotations, return an empty dictionary. # Constraints - The function should handle various Python objects including functions, classes, and modules. - It should consider the different quirks and best practices for Python versions 3.10 and newer and versions 3.9 and older. - If the `stringize_annotations` parameter is `True`, the function should evaluate stringized annotations safely. # Example ```python from typing import Any # Example class with annotations class Example: x: int y: str = \'hello\' # Example function with annotations def sample_func(a: int, b: \\"str\\") -> float: return float(a) annotations = extract_annotations(Example) print(annotations) # Output: {\'x\': int, \'y\': str} annotations = extract_annotations(sample_func, stringize_annotations=True) print(annotations) # Output: {\'a\': int, \'b\': str, \'return\': float} ``` # Notes - The function should utilize `inspect.get_annotations()` when available (Python 3.10 and newer) for the safest access. - For Python versions 3.9 and older, the function should follow the respective best practices as outlined in the documentation. - The `stringize_annotations` parameter, when set to `True`, should ensure that any string annotations are converted to actual types using `eval()` in a safe and controlled manner.","solution":"from typing import Any, get_type_hints import sys def extract_annotations(obj: Any, stringize_annotations: bool = False) -> dict: if stringize_annotations: # Safe eval using obj\'s global and local namespaces annotations = get_type_hints(obj) else: # Directly access __annotations__ if available try: annotations = obj.__annotations__ except AttributeError: annotations = {} return annotations"},{"question":"# Custom Display Class for Mean Squared Error You are required to implement a custom `mean_squared_error` (MSE) visualization class utilizing scikit-learn\'s plotting API. Class Definition: Create a class `MeanSquaredErrorDisplay` with the following methods: 1. `__init__(self, mse, estimator_name)`: Initializes the display object with Mean Squared Error (MSE) and the estimator\'s name. 2. `from_estimator(cls, estimator, X, y_true)`: Class method that: - Takes a fitted estimator, input data `X`, and true values `y_true`. - Predicts the values using the estimator. - Calculates the Mean Squared Error (MSE). - Returns an instance of `MeanSquaredErrorDisplay` initialized with the calculated MSE and the estimator\'s name. 3. `from_predictions(cls, y_true, y_pred, estimator_name)`: Class method that: - Takes true values `y_true` and predicted values `y_pred`. - Calculates the Mean Squared Error (MSE). - Returns an instance of `MeanSquaredErrorDisplay` initialized with the calculated MSE and the estimator\'s name. 4. `plot(self, ax=None)`: Method that: - Plots the MSE on the provided matplotlib axis. - If no axis is provided, it should create a new figure and axis. Input and Output: - **Input**: `from_estimator` and `from_predictions` methods take numpy arrays or pandas DataFrame for `X` and `y_true`/`y_pred`. - **Output**: A plotted graph displaying the MSE. Constraints: - The MSE calculation should be accurate and conform to the standard formula. - Proper exceptions should be handled for invalid inputs. Additionally, you should ensure that the MSE plot is clearly labeled with the MSE value and the estimator\'s name. Example: ```python from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt # Sample Data X = [[1], [2], [3], [4], [5]] y_true = [1, 2, 3, 4, 5] y_pred = [1, 2, 2.9, 4.1, 5.2] # Fitting model model = LinearRegression().fit(X, y_true) # Using from_estimator mse_display = MeanSquaredErrorDisplay.from_estimator(model, X, y_true) mse_display.plot() # Using from_predictions mse_display = MeanSquaredErrorDisplay.from_predictions(y_true, y_pred, \\"LinearRegression\\") mse_display.plot() plt.show() ``` Your task is to implement the `MeanSquaredErrorDisplay` class as described. Ensure your implementation is robust and efficient.","solution":"import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error class MeanSquaredErrorDisplay: def __init__(self, mse, estimator_name): self.mse = mse self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y_true): y_pred = estimator.predict(X) mse = mean_squared_error(y_true, y_pred) return cls(mse, estimator.__class__.__name__) @classmethod def from_predictions(cls, y_true, y_pred, estimator_name): mse = mean_squared_error(y_true, y_pred) return cls(mse, estimator_name) def plot(self, ax=None): if ax is None: fig, ax = plt.subplots() ax.bar([self.estimator_name], [self.mse], color=\'blue\') ax.set_ylim(0, max(1, self.mse * 1.1)) ax.set_ylabel(\'Mean Squared Error\') ax.set_title(f\'Mean Squared Error: {self.mse:.2f}\') plt.show()"},{"question":"# PyTorch Backend Configuration for Optimized Performance **Objective:** You are tasked with configuring PyTorch to ensure that it optimally uses the available hardware during computations, specifically focusing on GPU and CPU optimizations. This involves setting up various backend configurations for CUDA and cuDNN. **Problem Statement:** Write a Python function using PyTorch that: 1. Checks if CUDA is available and whether the build supports CUDA. 2. Configures the TensorFloat-32 (TF32) feature for CUDA matrix multiplications and cuDNN convolutions on Ampere or newer GPUs. 3. Enables the benchmarking feature of cuDNN to select the fastest convolution algorithms. 4. Configures the cuFFT plan cache\'s maximum size and clears the plan cache. 5. Ensures that deterministic algorithms are enabled for reproducible results. **Function Signature:** ```python def configure_pytorch_backends(tf32_enabled: bool, benchmark_limit: int, cufft_cache_max_size: int) -> dict: Configures PyTorch backends for optimized performance on both GPU and CPU. Parameters: - tf32_enabled (bool): Whether to enable TensorFloat-32 (TF32) on Ampere or newer GPUs. - benchmark_limit (int): The maximum number of cuDNN convolution algorithms to try when benchmarking. - cufft_cache_max_size (int): The maximum size for the cuFFT plan cache. Returns: - dict: A dictionary with the status of the configurations indicating success (`True`) or specific errors (`str`). pass ``` **Input and Output:** - Input: - `tf32_enabled`: A boolean value to enable or disable the TF32 feature. - `benchmark_limit`: An integer specifying the maximum number of cuDNN convolution algorithms to try. - `cufft_cache_max_size`: An integer to set the maximum size of the cuFFT plan cache. - Output: - A dictionary with keys: \\"cuda_available\\", \\"tf32_configured\\", \\"cudnn_benchmark\\", \\"cufft_cache_size_set\\", \\"deterministic_algorithms_set\\", where each value is either `True` for success or a string indicating an error. **Example:** ```python result = configure_pytorch_backends(tf32_enabled=True, benchmark_limit=5, cufft_cache_max_size=1024) print(result) # Output might be: # { # \'cuda_available\': True, # \'tf32_configured\': True, # \'cudnn_benchmark\': True, # \'cufft_cache_size_set\': True, # \'deterministic_algorithms_set\': True # } ``` **Constraints and Assumptions:** 1. The function should handle scenarios where CUDA is not available or not built with the current PyTorch installation. 2. Ensure that errors are handled gracefully, and the output dictionary clearly indicates which configurations were successfully applied and which failed. 3. Assume that the environment has the necessary permissions to change these configurations. **Hint:** Refer to the `torch.backends` modules in the documentation to find the relevant functions and attributes to use for setting these configurations.","solution":"import torch def configure_pytorch_backends(tf32_enabled: bool, benchmark_limit: int, cufft_cache_max_size: int) -> dict: Configures PyTorch backends for optimized performance on both GPU and CPU. Parameters: - tf32_enabled (bool): Whether to enable TensorFloat-32 (TF32) on Ampere or newer GPUs. - benchmark_limit (int): The maximum number of cuDNN convolution algorithms to try when benchmarking. - cufft_cache_max_size (int): The maximum size for the cuFFT plan cache. Returns: - dict: A dictionary with the status of the configurations indicating success (`True`) or specific errors (`str`). status = { \'cuda_available\': False, \'tf32_configured\': \'CUDA not available or Torch not built with CUDA\', \'cudnn_benchmark\': \'CUDA not available or Torch not built with CUDA\', \'cufft_cache_size_set\': \'CUDA not available or Torch not built with CUDA\', \'deterministic_algorithms_set\': \'CUDA not available or Torch not built with CUDA\' } if torch.cuda.is_available() and torch.cuda.is_built(): status[\'cuda_available\'] = True # Set TF32 configuration for CUDA matrix multiplications and cuDNN convolutions try: torch.backends.cuda.matmul.allow_tf32 = tf32_enabled torch.backends.cudnn.allow_tf32 = tf32_enabled status[\'tf32_configured\'] = True except Exception as e: status[\'tf32_configured\'] = str(e) # Enable cuDNN benchmark try: torch.backends.cudnn.benchmark = True torch.backends.cudnn.benchmark_limit = benchmark_limit status[\'cudnn_benchmark\'] = True except Exception as e: status[\'cudnn_benchmark\'] = str(e) # Configure cuFFT plan cache size try: torch.backends.cuda.cufft_plan_cache.max_size = cufft_cache_max_size torch.backends.cuda.cufft_plan_cache.clear() status[\'cufft_cache_size_set\'] = True except Exception as e: status[\'cufft_cache_size_set\'] = str(e) # Ensure deterministic algorithms for reproducibility try: torch.backends.cudnn.deterministic = True torch.use_deterministic_algorithms(True, warn_only=True) status[\'deterministic_algorithms_set\'] = True except Exception as e: status[\'deterministic_algorithms_set\'] = str(e) return status"},{"question":"# Question: Optimizing Pickle Data using pickletools You are tasked with demonstrating the usage of the `pickletools` module by creating a function that combines its key features. Your function will: 1. Optimize a given pickled object by eliminating unused \\"PUT\\" opcodes. 2. Disassemble and return a symbolic representation of the optimized pickle. # Function Signature ```python def optimize_and_disassemble_pickle(pickled_obj: bytes) -> str: pass ``` # Input - `pickled_obj`: A bytes object representing a pickled Python object. # Output - Returns a string containing the symbolic disassembly of the optimized pickle. # Constraints - The length of `pickled_obj` will be no more than 1 MB. - Assume the `pickled_obj` is correctly formatted and created using Python\'s `pickle` module. # Requirements - Use `pickletools.optimize` to optimize the given pickled object. - Use `pickletools.dis` to disassemble the optimized pickle and capture the result in a string. - Do not print the result directly; instead, return it as a string. # Example ```python import pickle data = {\'a\': 1, \'b\': 2} pickled_data = pickle.dumps(data) result = optimize_and_disassemble_pickle(pickled_data) print(result) ``` Expected Output Format: ``` 0: x80 PROTO 4 2: x95 FRAME 33 ... ``` # Note - The actual disassembled content will vary based on the input pickle. - You may find `io.StringIO` useful for capturing the output of `pickletools.dis`.","solution":"import pickletools import io def optimize_and_disassemble_pickle(pickled_obj: bytes) -> str: Optimizes a pickled object by eliminating unused \\"PUT\\" opcodes and returns a string containing the symbolic disassembly of the optimized pickle. # Optimize the pickled object optimized_pickle = pickletools.optimize(pickled_obj) # Create a StringIO object to capture the output of pickletools.dis output = io.StringIO() pickletools.dis(optimized_pickle, out=output) # Get the disassembled pickle content as a string disassembled_output = output.getvalue() return disassembled_output"},{"question":"**Objective**: Implement a function in PyTorch that performs element-wise operations on two input tensors using broadcasting semantics. Additionally, handle specific scenarios where in-place operations cannot be performed due to size mismatches after broadcasting. Requirements 1. Implement the function `broadcast_and_add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:` that: - Takes two input tensors `x` and `y`. - Returns the sum of the two tensors using PyTorch broadcasting semantics. - If the tensors are not directly broadcastable, the function should raise a `ValueError` with an appropriate error message. 2. Implement the function `broadcast_and_add_inplace(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:` that: - Takes two input tensors `x` and `y`. - Attempts to add `y` to `x` in-place using PyTorch broadcasting. - If in-place addition is not possible due to size mismatch, the function should raise a `RuntimeError` with an appropriate error message. 3. Both functions should adhere to the constraints outlined in the broadcast semantics documentation. Input Format - `x` and `y` are PyTorch tensors. Output Format - The function `\\"broadcast_and_add\\"` returns a new tensor resulting from adding `x` and `y` using broadcasting. - The function `\\"broadcast_and_add_inplace\\"` returns the tensor `x` after attempting to add `y` to it in-place. Examples **Example 1:** ```python import torch x = torch.empty(5, 3, 4, 1) y = torch.empty(3, 1, 1) result = broadcast_and_add(x, y) print(result.size()) # Output: torch.Size([5, 3, 4, 1]) try: in_place_result = broadcast_and_add_inplace(x, y) print(in_place_result.size()) # Output: torch.Size([5, 3, 4, 1]) except RuntimeError as e: print(e) ``` **Example 2:** ```python import torch x = torch.empty(1, 3, 1) y = torch.empty(3, 1, 7) try: result = broadcast_and_add(x, y) print(result.size()) # This should raise a ValueError and not print any size except ValueError as e: print(e) # Output: Error message about broadcastability try: in_place_result = broadcast_and_add_inplace(x, y) print(in_place_result.size()) # This should raise a RuntimeError and not print any size except RuntimeError as e: print(e) # Output: Error message about in-place addition ``` Notes - Make sure to import necessary PyTorch libraries. - Adequately document your code.","solution":"import torch def broadcast_and_add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: Adds two tensors using broadcasting semantics. Args: - x (torch.Tensor): The first input tensor. - y (torch.Tensor): The second input tensor. Returns: - torch.Tensor: A tensor containing the sum of x and y. Raises: - ValueError: If the tensors are not broadcastable. try: result = x + y except RuntimeError as e: raise ValueError(\\"Tensors are not broadcastable\\") from e return result def broadcast_and_add_inplace(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: Attempts to add the second tensor to the first tensor in-place using broadcasting. Args: - x (torch.Tensor): The first input tensor which will be modified. - y (torch.Tensor): The second input tensor. Returns: - torch.Tensor: The tensor x after the addition. Raises: - RuntimeError: If in-place addition is not possible due to size mismatch. try: x += y except RuntimeError as e: raise RuntimeError(\\"In-place addition is not possible due to size mismatch\\") from e return x"},{"question":"**Question:** You are required to demonstrate your understanding of the Seaborn `so.Plot` class by creating a series of plots based on a provided dataset. Your task is to create two figures with the following specifications: 1. **Single Plot with Custom Dimensions:** - Create a scatter plot of a dataset. - Set the overall dimensions of the figure to (8, 5). - Save this plot as \\"scatter_plot.png\\". 2. **Faceted Plot with Custom Engine and Extent:** - Use the same dataset to create a faceted plot with two variables used for faceting. - Use a `constrained` layout engine for the faceted plot. - Control the size of the plot relative to the figure with `extent=[0, 0, .9, 1]`. - Save this faceted plot as \\"faceted_plot.png\\". For this task, we will use the built-in Seaborn dataset `penguins`. Use `bill_length_mm` and `flipper_length_mm` as the x and y axes for the scatter plot. For the faceted plot, use the same x and y axes, and facet the data by `species` and `sex`. # Requirements: - Your code should be contained within a function `create_plots()` that: - Downloads the `penguins` dataset. - Creates the required plots and saves them as specified. - Ensure the plots are clear and labels are properly set. # Example Format: ```python import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Creating the scatter plot with custom dimensions scatter_plot = so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"flipper_length_mm\\") scatter_plot.layout(size=(8, 5)) scatter_plot.show() plt.savefig(\\"scatter_plot.png\\") # Creating the faceted plot with custom engine and extent faceted_plot = so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"flipper_length_mm\\").facet(\\"species\\", \\"sex\\") faceted_plot.layout(engine=\\"constrained\\", extent=[0, 0, .9, 1]) faceted_plot.show() plt.savefig(\\"faceted_plot.png\\") # Call the function to execute the plot creation create_plots() ``` You may need to modify this example to ensure that the plots are generated and saved correctly as per the requirements.","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Creating the scatter plot with custom dimensions scatter_plot = so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"flipper_length_mm\\") scatter_plot.layout(size=(8, 5)).add(so.Dots()) scatter_plot.show() plt.savefig(\\"scatter_plot.png\\") plt.clf() # Clear the current figure # Creating the faceted plot with custom engine and extent faceted_plot = so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"flipper_length_mm\\").facet(\\"species\\", \\"sex\\") faceted_plot.layout(engine=\\"constrained\\", extent=[0, 0, .9, 1]).add(so.Dots()) faceted_plot.show() plt.savefig(\\"faceted_plot.png\\") plt.clf() # Clear the current figure # Call the function to execute the plot creation create_plots()"},{"question":"# Python Buffer Protocol Question In this task, you are required to implement a Python class `CustomBuffer` that uses the buffer protocol to provide direct access to its underlying data. Specifications: 1. **Class Definition:** - Define a class `CustomBuffer` which will hold a contiguous memory buffer of bytes. 2. **Constructor:** - The constructor should take an integer `size` as an argument, which represents the size of the buffer to be created. - Allocate a buffer of the specified `size` and initialize it with zeros. 3. **Buffer Protocol Methods:** - Implement the `__buffer__` method for the buffer protocol in the `CustomBuffer` class. The method should fill in the `Py_buffer` structure appropriately based on the flags provided. 4. **Write Data:** - Implement a method `write_data(self, offset: int, data: bytes)` that writes the `data` to the buffer starting at the given `offset`. Raise an `IndexError` if the write operation exceeds the buffer bounds. 5. **Read Data**: - Implement a method `read_data(self, offset: int, length: int) -> bytes` that returns `length` bytes from the buffer starting at the given `offset`. Raise an `IndexError` if the read operation exceeds the buffer bounds. 6. **Constraints:** - `size`, `offset`, and `length` should be non-negative integers. - The size of the buffer will be at most 10^6 bytes. Example Usage: ```python # Creating a CustomBuffer of size 20 buffer = CustomBuffer(20) # Writing data to the buffer buffer.write_data(5, b\\"hello\\") # Reading data from the buffer print(buffer.read_data(5, 5)) # Output: b\'hello\' # Accessing the underlying buffer import ctypes # Get the pointer to the buffer view = memoryview(buffer) print(view.nbytes) # Output: 20 print(view[5:10].tobytes()) # Output: b\'hello\' ``` Evaluation: Your implementation will be evaluated based on the following criteria: - Correctness: The class and methods should behave as specified. - Efficient Memory Use: The implementation should handle storage and access efficiently. - Buffer Protocol Comprehension: Correct use and implementation of the buffer protocol methods. - Code Quality: Code should be clean, well-commented, and follow Python conventions.","solution":"import ctypes class CustomBuffer: def __init__(self, size: int): if size < 0: raise ValueError(\\"Size must be a non-negative integer\\") self.size = size self.buffer = (ctypes.c_byte * size)() def __buffer__(self, flags): view = memoryview(self.buffer) return view def write_data(self, offset: int, data: bytes): if offset < 0 or offset >= self.size: raise IndexError(\\"Offset is out of buffer bounds\\") if offset + len(data) > self.size: raise IndexError(\\"Write operation exceeds buffer bounds\\") for i in range(len(data)): self.buffer[offset + i] = data[i] def read_data(self, offset: int, length: int) -> bytes: if offset < 0 or offset >= self.size: raise IndexError(\\"Offset is out of buffer bounds\\") if offset + length > self.size: raise IndexError(\\"Read operation exceeds buffer bounds\\") return bytes(self.buffer[offset:offset+length])"},{"question":"**Objective:** Assess your understanding of Python\'s `sunau` module. **Problem Description:** You are required to implement a function that converts a given AU (Sun AU) audio file to a WAV audio file. The function should read the AU file, extract the audio data and parameters, and then write that data into a new WAV file with the same parameters. **Function Signature:** ```python def convert_au_to_wav(au_file_path: str, wav_file_path: str) -> None: Converts an AU format audio file to a WAV format audio file with the same parameters. :param au_file_path: A string representing the path to the input AU file. :param wav_file_path: A string representing the path to the output WAV file. :raises sunau.Error: If there is an error reading the AU file. pass ``` **Input:** - `au_file_path`: A string representing the path to the input AU file. - `wav_file_path`: A string representing the path to the output WAV file. **Output:** - The function should create a WAV file at the specified `wav_file_path` with the same audio data and parameters as the input AU file. **Constraints:** - The input AU file is guaranteed to exist and be a valid AU file. - The output directory is writable. **Performance Requirements:** - The function should handle large audio files efficiently. - Minimize memory consumption by processing audio data in chunks if necessary. **Example:** ```python convert_au_to_wav(\\"input.au\\", \\"output.wav\\") ``` The above function call should read the audio data and parameters from \\"input.au\\" and create \\"output.wav\\" with the same audio content and settings. **Hints:** - Use the `sunau.open` function to open the AU file in read mode and extract audio parameters and data. - Use the `wave` module from the Python standard library to write the data to a WAV file. **Your task is to implement the `convert_au_to_wav` function.**","solution":"import sunau import wave def convert_au_to_wav(au_file_path: str, wav_file_path: str) -> None: Converts an AU format audio file to a WAV format audio file with the same parameters. :param au_file_path: A string representing the path to the input AU file. :param wav_file_path: A string representing the path to the output WAV file. :raises sunau.Error: If there is an error reading the AU file. with sunau.open(au_file_path, \'rb\') as au_file: n_channels = au_file.getnchannels() sampwidth = au_file.getsampwidth() framerate = au_file.getframerate() n_frames = au_file.getnframes() audio_data = au_file.readframes(n_frames) with wave.open(wav_file_path, \'wb\') as wav_file: wav_file.setnchannels(n_channels) wav_file.setsampwidth(sampwidth) wav_file.setframerate(framerate) wav_file.writeframes(audio_data)"},{"question":"# Objective: Create a Python function that demonstrates the use of Unix-specific services, specifically focusing on the `resource` module to manage resource limits, and the `syslog` module to log information. The aim is to assess your ability to work with these low-level Unix interfaces. # Problem Statement: You are required to implement a function `log_resource_usage_and_limit()` that performs the following tasks: 1. Set a CPU time limit for the current process. 2. Retrieve and log the current resource usage of the process. 3. In case the resource usage exceeds a certain threshold (e.g., 50% of the limit), log a warning message via the syslog module. # Function Signature: ```python def log_resource_usage_and_limit(cpu_time_limit: int, usage_threshold: float) -> None: Sets a CPU time limit, logs current resource usage, and logs a warning if usage exceeds the threshold. Args: cpu_time_limit (int): The maximum CPU time (in seconds) that the process is allowed to use. usage_threshold (float): The threshold (between 0 and 1) to trigger a warning in syslog if the CPU usage exceeds this proportion of the limit. Returns: None ``` # Input: 1. `cpu_time_limit`: An integer representing the maximum CPU time (in seconds) for the process. 2. `usage_threshold`: A float between 0 and 1 representing the proportion of the CPU time limit at which a warning should be logged. # Constraints: - `0 < cpu_time_limit <= 3600` (maximum CPU time is one hour) - `0 < usage_threshold <= 1` # Expected Behavior: 1. The function should use the `resource` module to set the CPU time limit. 2. It should then log the current CPU time consumed using the `syslog` module. 3. If the current CPU time exceeds `usage_threshold * cpu_time_limit`, log a warning message using the `syslog` module. # Example: ```python log_resource_usage_and_limit(10, 0.5) ``` This means the process is allowed to use up to 10 seconds of CPU time. The function will log the current CPU time usage, and if the usage exceeds 5 seconds (50% of the limit), it will log a warning. # Notes: - You need to familiarize yourself with the `resource` and `syslog` modules. - Make sure to handle potential exceptions and errors gracefully. - The syslog messages should contain enough information to identify the resource usage and the thresholds. # Hints: - Use `resource.setrlimit` to set limits and `resource.getrusage` to get the resource usage. - Use `syslog.syslog` to log messages to the syslog.","solution":"import resource import syslog def log_resource_usage_and_limit(cpu_time_limit: int, usage_threshold: float) -> None: Sets a CPU time limit, logs current resource usage, and logs a warning if usage exceeds the threshold. Args: cpu_time_limit (int): The maximum CPU time (in seconds) that the process is allowed to use. usage_threshold (float): The threshold (between 0 and 1) to trigger a warning in syslog if the CPU usage exceeds this proportion of the limit. Returns: None if not (0 < cpu_time_limit <= 3600): raise ValueError(\\"cpu_time_limit must be between 1 and 3600 seconds.\\") if not (0 < usage_threshold <= 1): raise ValueError(\\"usage_threshold must be between 0 and 1.\\") # Set the CPU time limit for the current process resource.setrlimit(resource.RLIMIT_CPU, (cpu_time_limit, cpu_time_limit)) # Retrieve and log the current resource usage usage = resource.getrusage(resource.RUSAGE_SELF) cpu_time_used = usage.ru_utime + usage.ru_stime syslog.syslog(syslog.LOG_INFO, f\\"Current CPU time usage: {cpu_time_used:.2f} seconds\\") # Check if usage exceeds the threshold if cpu_time_used > usage_threshold * cpu_time_limit: syslog.syslog(syslog.LOG_WARNING, f\\"WARNING: CPU time usage ({cpu_time_used:.2f} seconds) exceeds {usage_threshold*100:.1f}% of the limit ({cpu_time_limit} seconds)\\")"},{"question":"Objective: The goal of this assessment is to evaluate your understanding of the `hls_palette` function and its application in customizing data visualizations with seaborn. Problem Statement: Design a function named `plot_custom_hls_palette` that takes four parameters: - `num_colors` (int): The number of colors to include in the palette. - `lightness` (float): The lightness of the colors (between 0 and 1). - `saturation` (float): The saturation of the colors (between 0 and 1). - `as_cmap` (bool): A boolean indicating whether to return the palette as a continuous colormap. Your function should: 1. Create an HLS palette using the provided parameters. 2. Generate a bar plot (with 10 bars) using seaborn, with the bars colored according to the specified palette. 3. Display the plot using `matplotlib`. # Function Signature: ```python def plot_custom_hls_palette(num_colors: int, lightness: float, saturation: float, as_cmap: bool) -> None: pass ``` # Constraints: - `num_colors` should be a positive integer. - `lightness` should be a float between 0 and 1. - `saturation` should be a float between 0 and 1. - You may assume valid inputs for the parameters (no need to handle invalid inputs for this assessment). # Example Usage: ```python plot_custom_hls_palette(10, 0.5, 0.7, False) ``` This would create an HLS palette with 10 colors, a lightness of 0.5, and a saturation of 0.7, and then use this palette to generate a bar plot with 10 bars. Important Notes: - Ensure seaborn and matplotlib are correctly imported in your solution. - Use `sns.set_theme()` to set the theme for your plots. - The resulting visualization should be displayed within the function, so there is no need to return any values. **Evaluate** your solution against various parameter combinations to ensure correctness and robustness of the implementation. **Bonus**: Add titles and labels to the plot to enhance readability.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def plot_custom_hls_palette(num_colors: int, lightness: float, saturation: float, as_cmap: bool) -> None: Creates an HLS palette and generates a bar plot colored according to the specified palette. Parameters: - num_colors (int): The number of colors to include in the palette. - lightness (float): The lightness of the colors (between 0 and 1). - saturation (float): The saturation of the colors (between 0 and 1). - as_cmap (bool): Whether to return the palette as a continuous colormap. Returns: - None: Displays the plot using matplotlib. sns.set_theme() # Create an HLS palette palette = sns.color_palette(\\"hls\\", n_colors=num_colors) if not as_cmap else sns.hls_palette(n_colors=num_colors, l=lightness, s=saturation, as_cmap=True) # Generate data data = np.random.rand(10) # Create the bar plot plt.figure(figsize=(10, 5)) sns.barplot(x=np.arange(1, 11), y=data, palette=palette if not as_cmap else None) # Customize the plot plt.title(\'Bar Plot with Custom HLS Palette\') plt.xlabel(\'Categories\') plt.ylabel(\'Values\') plt.show()"},{"question":"Manipulating MaskedTensors in PyTorch Background In this task, you will demonstrate your understanding of PyTorch\'s `MaskedTensor` by performing various tensor operations while considering masked semantics. `MaskedTensor` allows selective ignoring of elements during computations using a data tensor and a corresponding mask tensor. Problem Statement Write a function `masked_tensor_operations` that performs the following operations: 1. **Create** two `MaskedTensor` objects `mt1` and `mt2`: - `mt1` with data tensor: ``` [[1, 2, 0], [4, 0, 6], [7, 8, 9]] ``` and mask tensor: ``` [[ True, True, False], [ True, False, True], [ True, True, True]] ``` - `mt2` with data tensor: ``` [[1, 0, 3], [0, 5, 6], [7, 0, 9]] ``` and mask tensor: ``` [[ True, False, True], [ False, True, True], [ True, False, True]] ``` 2. **Perform the following operations** on these masked tensors: - **Addition** of `mt1` and `mt2`. - **Element-wise multiplication** of `mt1` and `mt2`. - **Sum reduction** of `mt1`. 3. Return the results as a tuple of three items: - Result of the addition (a MaskedTensor). - Result of the multiplication (a MaskedTensor). - Sum of the elements of `mt1` that are not masked out (a scalar). Function Signature ```python def masked_tensor_operations(): pass ``` Constraints and Assumptions - You should use PyTorch\'s `masked` module to handle the task. - Assume that the masks for binary operations are compatible. - Make sure to handle the mask correctly where necessary. Expected Output The function should return expected results considering the masking rules: - For addition, only elements that are unmasked in both tensors should be added. - For element-wise multiplication, only elements that are unmasked in both tensors should be multiplied. - For reduction, only unmasked elements should be considered. Ensure to validate the results based on the provided tensors and masks. Example Usage ```python result_addition, result_multiplication, result_sum = masked_tensor_operations() print(result_addition) # Expected MaskedTensor output for addition print(result_multiplication) # Expected MaskedTensor output for multiplication print(result_sum) # Expected scalar output for sum reduction ```","solution":"import torch def masked_tensor_operations(): # Create data tensors data1 = torch.tensor([[1, 2, 0], [4, 0, 6], [7, 8, 9]], dtype=torch.float) data2 = torch.tensor([[1, 0, 3], [0, 5, 6], [7, 0, 9]], dtype=torch.float) # Create mask tensors mask1 = torch.tensor([[True, True, False], [True, False, True], [True, True, True]], dtype=torch.bool) mask2 = torch.tensor([[True, False, True], [False, True, True], [True, False, True]], dtype=torch.bool) # Apply the mask to create MaskedTensor(s) data1.masked_fill_(~mask1, 0) data2.masked_fill_(~mask2, 0) # Adding masked elements masked_add = data1 + data2 mask_combine_add = mask1 & mask2 masked_add.masked_fill_(~mask_combine_add, 0) # Element-wise multiplication masked elements masked_mult = data1 * data2 mask_combine_mult = mask1 & mask2 masked_mult.masked_fill_(~mask_combine_mult, 0) # Sum reduction of mt1 sum_reduction = data1[mask1].sum().item() return masked_add, masked_mult, sum_reduction"},{"question":"**Objective:** Implement a feature selection pipeline using scikit-learn to improve the performance of a classifier on a high-dimensional dataset. **Problem Statement:** You are provided with the `Breast Cancer Wisconsin (Diagnostic)` dataset from `sklearn.datasets`. This dataset consists of features computed from digitized images of breast cancer fine needle aspirates (FNA). Your task is to preprocess the dataset by selecting the most relevant features, train a classifier, and evaluate its performance. **Requirements:** 1. **Load the Dataset:** - Use `load_breast_cancer` from `sklearn.datasets` to load the dataset. - Split the dataset into training and testing sets with a test size of 20%. 2. **Variance Thresholding:** - Apply `VarianceThreshold` to remove all features with variance less than a threshold of `0.1`. 3. **Univariate Feature Selection:** - Use `SelectKBest` with the `chi2` scoring function to select the top 10 features based on univariate statistical tests. 4. **Model-Based Feature Selection:** - Train an `ExtraTreesClassifier` and use `SelectFromModel` to select features based on their importance scores. 5. **Recursive Feature Elimination (RFE):** - Use `LogisticRegression` with `RFE` to recursively remove features and select the top 5 most important features. 6. **Create a Pipeline:** - Combine these steps into a `Pipeline` which includes a final classification step using `RandomForestClassifier`. 7. **Evaluate the Model:** - Train the pipeline on the training set. - Evaluate the performance of the classifier on the test set using accuracy. **Constraints:** - You must use the specified feature selection methods in the given order. - You should use default parameters for all estimators unless specified otherwise. **Expected Input and Output:** - **Input:** - The function does not take any input arguments. - **Output:** - The function should output the accuracy of the classifier on the test set. **Code Template:** ```python from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, SelectFromModel, RFE from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score def feature_selection_pipeline(): # Load the dataset data = load_breast_cancer() X, y = data.data, data.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Variance Thresholding variance_thresholder = VarianceThreshold(threshold=0.1) # Univariate Feature Selection univariate_selector = SelectKBest(score_func=chi2, k=10) # Model-Based Feature Selection tree_based_selector = SelectFromModel(ExtraTreesClassifier(n_estimators=50)) # Recursive Feature Elimination rfe_selector = RFE(LogisticRegression(), n_features_to_select=5) # Classifier classifier = RandomForestClassifier(random_state=42) # Create and evaluate pipeline pipeline = Pipeline([ (\'variance_threshold\', variance_thresholder), (\'univariate_selection\', univariate_selector), (\'model_based_selection\', tree_based_selector), (\'rfe_selection\', rfe_selector), (\'classification\', classifier) ]) pipeline.fit(X_train, y_train) y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy # Example usage print(\\"Model accuracy:\\", feature_selection_pipeline()) ``` **Notes:** - Ensure that all necessary libraries are imported. - Follow the steps and constraints to ensure a valid solution. - Test your implementation to ensure correctness.","solution":"from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, SelectFromModel, RFE from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score def feature_selection_pipeline(): # Load the dataset data = load_breast_cancer() X, y = data.data, data.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Variance Thresholding variance_thresholder = VarianceThreshold(threshold=0.1) # Univariate Feature Selection univariate_selector = SelectKBest(score_func=chi2, k=10) # Model-Based Feature Selection tree_based_selector = SelectFromModel(ExtraTreesClassifier(n_estimators=50)) # Recursive Feature Elimination rfe_selector = RFE(LogisticRegression(solver=\'liblinear\', max_iter=10000), n_features_to_select=5) # Classifier classifier = RandomForestClassifier(random_state=42) # Create and evaluate pipeline pipeline = Pipeline([ (\'variance_threshold\', variance_thresholder), (\'univariate_selection\', univariate_selector), (\'model_based_selection\', tree_based_selector), (\'rfe_selection\', rfe_selector), (\'classification\', classifier) ]) pipeline.fit(X_train, y_train) y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Complex Number Operations Your task is to create a class in Python that emulates some of the internal workings of complex number operations as described in Python\'s C API documentation. **Objective**: Implement a `MyComplex` class that supports initialization from both Python complex numbers and custom structures, and provides basic arithmetic operations. Class Interface and Methods ```python class MyComplex: def __init__(self, real: float = 0.0, imag: float = 0.0): Initialize a MyComplex object :param real: The real part of the complex number. :param imag: The imaginary part of the complex number. self.real = real self.imag = imag @classmethod def from_python_complex(cls, c: complex): Initialize a MyComplex object from a Python complex object. :param c: A Python complex number. :return: A MyComplex object return cls(c.real, c.imag) def to_python_complex(self) -> complex: Convert the MyComplex object to a Python complex number. :return: A Python complex number. return complex(self.real, self.imag) def __add__(self, other: \'MyComplex\') -> \'MyComplex\': Add two MyComplex objects. :param other: Another MyComplex object. :return: The sum as a new MyComplex object. return MyComplex(self.real + other.real, self.imag + other.imag) def __sub__(self, other: \'MyComplex\') -> \'MyComplex\': Subtract another MyComplex object from this one. :param other: Another MyComplex object. :return: The difference as a new MyComplex object. return MyComplex(self.real - other.real, self.imag - other.imag) def __mul__(self, other: \'MyComplex\') -> \'MyComplex\': Multiply two MyComplex objects. :param other: Another MyComplex object. :return: The product as a new MyComplex object. real = (self.real * other.real) - (self.imag * other.imag) imag = (self.real * other.imag) + (self.imag * other.real) return MyComplex(real, imag) def __truediv__(self, other: \'MyComplex\') -> \'MyComplex\': Divide this MyComplex object by another. :param other: Another MyComplex object. :return: The quotient as a new MyComplex object. denom = other.real ** 2 + other.imag ** 2 real = (self.real * other.real + self.imag * other.imag) / denom imag = (self.imag * other.real - self.real * other.imag) / denom return MyComplex(real, imag) ``` # Task Requirements 1. Implement the `MyComplex` class with the methods as described above. 2. Ensure that your class can correctly initialize objects from both real and imaginary parts and from Python\'s built-in complex type. 3. Implement arithmetic operations (+, -, *, /) for the `MyComplex` class. 4. Write appropriate tests to verify your implementation. Input and Output Format - Initialization and conversion should support using real numbers, imaginary numbers, and complex numbers. - Arithmetic operations should return new `MyComplex` objects. ```python # Example Usage: c1 = MyComplex(3.0, 2.0) c2 = MyComplex.from_python_complex(complex(1.0, 1.0)) result_add = c1 + c2 # MyComplex(4.0, 3.0) result_sub = c1 - c2 # MyComplex(2.0, 1.0) result_mul = c1 * c2 # MyComplex(1.0, 5.0) result_div = c1 / c2 # MyComplex(2.5, 0.5) print(result_add.to_python_complex()) # (4+3j) print(result_sub.to_python_complex()) # (2+1j) print(result_mul.to_python_complex()) # (1+5j) print(result_div.to_python_complex()) # (2.5+0.5j) ``` Constraints - Division by zero should be handled appropriately (return `None` or raise an exception). - It is assumed that inputs provided are valid complex number representations.","solution":"class MyComplex: def __init__(self, real: float = 0.0, imag: float = 0.0): Initialize a MyComplex object :param real: The real part of the complex number. :param imag: The imaginary part of the complex number. self.real = real self.imag = imag @classmethod def from_python_complex(cls, c: complex): Initialize a MyComplex object from a Python complex object. :param c: A Python complex number. :return: A MyComplex object return cls(c.real, c.imag) def to_python_complex(self) -> complex: Convert the MyComplex object to a Python complex number. :return: A Python complex number. return complex(self.real, self.imag) def __add__(self, other: \'MyComplex\') -> \'MyComplex\': Add two MyComplex objects. :param other: Another MyComplex object. :return: The sum as a new MyComplex object. return MyComplex(self.real + other.real, self.imag + other.imag) def __sub__(self, other: \'MyComplex\') -> \'MyComplex\': Subtract another MyComplex object from this one. :param other: Another MyComplex object. :return: The difference as a new MyComplex object. return MyComplex(self.real - other.real, self.imag - other.imag) def __mul__(self, other: \'MyComplex\') -> \'MyComplex\': Multiply two MyComplex objects. :param other: Another MyComplex object. :return: The product as a new MyComplex object. real = (self.real * other.real) - (self.imag * other.imag) imag = (self.real * other.imag) + (self.imag * other.real) return MyComplex(real, imag) def __truediv__(self, other: \'MyComplex\') -> \'MyComplex\': Divide this MyComplex object by another. :param other: Another MyComplex object. :return: The quotient as a new MyComplex object. Returns None if division by zero occurs. denom = other.real ** 2 + other.imag ** 2 if denom == 0: return None real = (self.real * other.real + self.imag * other.imag) / denom imag = (self.imag * other.real - self.real * other.imag) / denom return MyComplex(real, imag)"},{"question":"**Coding Assessment Question** # Objective Implement a function that processes a dataset, performs specific operations, and generates a detailed memory usage report. This task will assess your ability to manage memory efficiently, handle boolean conditions properly, and ensure thread safety while using pandas. # Problem Statement You are provided with a dataset as a pandas DataFrame. Your task is to: 1. Add a new column \\"processed\\" to the DataFrame, which indicates whether each row meets certain criteria based on existing numeric and boolean columns. 2. Ensure that your implementation avoids any mutation issues when applying a UDF on the DataFrame. 3. Generate and return a detailed memory usage report of the DataFrame. # Requirements - Implement the function `process_and_report_memory_usage(df: pd.DataFrame) -> pd.DataFrame`. - The function should process the DataFrame as follows: 1. Add a \\"processed\\" column: The value should be `True` if a specified numeric column exceeds a threshold and a boolean column is `True`. 2. When applying UDFs, ensure that the original DataFrame is not mutated. - For the memory usage report: 1. Use the `df.memory_usage(deep=True)` method to get the memory usage of each column. 2. Present the memory usage in a readable format as part of the returned DataFrame, including the total memory usage. # Input - `df (pd.DataFrame)`: A pandas DataFrame containing at least one numeric column and one boolean column. # Output - `pd.DataFrame`: The processed DataFrame with an added \\"processed\\" column and a human-readable memory usage report including the total memory usage. # Example ```python import pandas as pd import numpy as np # Example input DataFrame data = { \'numeric_col\': np.random.randint(0, 100, size=1000), \'bool_col\': np.random.choice([True, False], size=1000) } df = pd.DataFrame(data) # Function implementation def process_and_report_memory_usage(df): # Add \\"processed\\" column threshold = 50 df[\'processed\'] = (df[\'numeric_col\'] > threshold) & (df[\'bool_col\']) # Safeguard against mutation in UDF operations # Here, just as an example, no mutation-causing UDFs will be applied # Generate memory usage report memory_usage = df.memory_usage(deep=True).to_frame(\'Memory Usage (bytes)\') memory_usage.loc[\'Total\'] = memory_usage.sum() return df, memory_usage # Call the function processed_df, memory_report = process_and_report_memory_usage(df) print(processed_df.head()) print(memory_report) ``` # Constraints - The DataFrame will have at least one numeric column and one boolean column. - Ensure thread safety if operations are performed in a multithreaded environment. # Notes - Avoid directly modifying DataFrame columns inside UDFs to prevent unintended side effects. - Use appropriate methods (such as copy or creating new DataFrame) to handle mutable operations.","solution":"import pandas as pd def process_and_report_memory_usage(df: pd.DataFrame) -> pd.DataFrame: Processes the DataFrame by adding a \\"processed\\" column and generates a memory usage report. Parameters: df (pd.DataFrame): The input DataFrame containing at least one numeric column and one boolean column. Returns: pd.DataFrame: The processed DataFrame with an added \\"processed\\" column and the memory usage report. threshold = 50 # Add \\"processed\\" column indicating if numeric column is greater than threshold and boolean column is True df_copy = df.copy() df_copy[\'processed\'] = (df_copy[\'numeric_col\'] > threshold) & (df_copy[\'bool_col\']) # Generate memory usage report memory_usage = df_copy.memory_usage(deep=True).to_frame(\'Memory Usage (bytes)\') memory_usage.loc[\'Total\'] = memory_usage.sum() return df_copy, memory_usage"},{"question":"# Python Coding Assessment Question Intercept and Log Function Calls **Objective:** Implement a program that uses the `sys.settrace()` function to intercept and log all function calls made during the execution of a script. This includes both built-in functions and user-defined functions. The program should output the name of the function being called, the number of times it has been called, and the arguments passed to it. **Instructions:** 1. Create a Python script that uses the `sys.settrace()` function to trace function calls. 2. Define a trace function that logs necessary information. 3. The trace function should collect the function name, number of times it has been called, and arguments passed to it. 4. Implement a mechanism to display the collected information upon the scriptâ€™s completion. 5. Ensure the program handles both built-in functions and user-defined functions. **Requirements:** - The output should be formatted as follows: ``` Function \'func_name\' called N times with arguments: - call 1: (arg1, arg2, ...) - call 2: (arg1, arg2, ...) ``` - Your implementation must not cause any side effects or alter the original behavior of the traced functions. **Example:** ```python import sys # Example function to be traced def example_function(a, b): return a + b def trace_calls(frame, event, arg): if event != \'call\': return func_name = frame.f_code.co_name call_args = frame.f_locals # Assuming a dictionary to store function call logs if func_name not in trace_calls.logs: trace_calls.logs[func_name] = [] trace_calls.logs[func_name].append(call_args) def trace_lines(frame, event, arg): return trace_lines return trace_lines trace_calls.logs = {} def main(): sys.settrace(trace_calls) # Example calls result = example_function(1, 2) result = example_function(3, 4) sys.settrace(None) # Disable tracing # Display collected logs for func, calls in trace_calls.logs.items(): print(f\\"Function \'{func}\' called {len(calls)} times with arguments:\\") for i, args in enumerate(calls, start=1): print(f\\" - call {i}: {args}\\") if __name__ == \\"__main__\\": main() ``` **Constraints:** - Do not use any external libraries. - Ensure compatibility with Python version 3.10. - The tracing mechanism should be lightweight and should not significantly impact the performance of the script being traced. **Notes:** - Consider edge cases such as functions that do not take any arguments. - Ensure the script correctly handles multiple calls to the same function. - The script should be modular and well-commented. --- Your task is to complete the provided skeleton code to fully implement the described functionality. This will test your ability to work with low-level system functions and your understanding of function tracing and logging in Python. Happy coding!","solution":"import sys # Example function to be traced def example_function(a, b): return a + b def trace_calls(frame, event, arg): if event != \'call\': return func_name = frame.f_code.co_name call_args = frame.f_locals if func_name not in trace_calls.logs: trace_calls.logs[func_name] = { \'count\': 0, \'calls\': [] } trace_calls.logs[func_name][\'count\'] += 1 trace_calls.logs[func_name][\'calls\'].append(call_args) def trace_lines(frame, event, arg): return trace_lines return trace_lines trace_calls.logs = {} def main(): sys.settrace(trace_calls) # Example calls result1 = example_function(1, 2) result2 = example_function(3, 4) result3 = example_function(5, 6) sys.settrace(None) # Disable tracing # Display collected logs for func, info in trace_calls.logs.items(): print(f\\"Function \'{func}\' called {info[\'count\']} times with arguments:\\") for i, args in enumerate(info[\'calls\'], start=1): print(f\\" - call {i}: {args}\\") if __name__ == \\"__main__\\": main()"},{"question":"**URL Parsing and Reformatting Challenge** In this challenge, you will demonstrate your understanding of the `urllib.parse` module by writing several functions to manipulate URLs. You are required to implement the following functions: 1. `extract_query_params(url: str) -> dict`: - Takes a URL string as input. - Parses the URL to extract its query parameters and returns them as a dictionary. - For example: ```python url = \'http://example.com/path/to/page?name=ferret&color=purple\' assert extract_query_params(url) == {\'name\': [\'ferret\'], \'color\': [\'purple\']} ``` 2. `remove_fragment(url: str) -> str`: - Takes a URL string as input. - Removes the fragment part of the URL (if any) and returns the modified URL. - For example: ```python url = \'http://example.com/path/to/page?name=ferret&color=purple#section2\' assert remove_fragment(url) == \'http://example.com/path/to/page?name=ferret&color=purple\' ``` 3. `recombine_url(scheme: str, netloc: str, path: str, params: str, query: str, fragment: str) -> str`: - Takes individual components of a URL as input. - Recombines them into a complete URL string. - For example: ```python assert recombine_url(\'http\', \'example.com\', \'/path/to/page\', \'\', \'name=ferret&color=purple\', \'section2\') == \'http://example.com/path/to/page?name=ferret&color=purple#section2\' ``` 4. `safe_quote_path(path: str) -> str`: - Takes a path string as input. - Returns the path string with special characters quoted using the `urllib.parse.quote` function. - For example: ```python path = \'/El NiÃ±o/\' assert safe_quote_path(path) == \'/El%20Ni%C3%B1o/\' ``` # Constraints: - The input URL will always be a valid URL. - You must use functions from the `urllib.parse` module to implement the solutions. # Performance Requirements: - Solutions should be efficient and handle typical URL lengths within a reasonable time frame. Implement the functions and test them with the provided examples to ensure they work correctly.","solution":"from urllib.parse import urlparse, parse_qs, urlunparse, quote def extract_query_params(url: str) -> dict: Extracts query parameters from the given URL and returns them as a dictionary. parsed_url = urlparse(url) return parse_qs(parsed_url.query) def remove_fragment(url: str) -> str: Removes the fragment from the given URL and returns the modified URL. parsed_url = urlparse(url) # Create a new URL without the fragment new_url = parsed_url._replace(fragment=\'\') return urlunparse(new_url) def recombine_url(scheme: str, netloc: str, path: str, params: str, query: str, fragment: str) -> str: Recombines individual components of a URL into a complete URL string. return urlunparse((scheme, netloc, path, params, query, fragment)) def safe_quote_path(path: str) -> str: Quotes special characters in the given path and returns the path. return quote(path)"},{"question":"Question: Creating and Using Mocks in Unit Testing with `unittest.mock` You are tasked with creating unit tests for a simple database access layer in a Python application. The class `DatabaseClient` is responsible for connecting to a database, inserting records, and retrieving data. Here is the class: ```python class DatabaseClient: def __init__(self, connection_string): self.connection_string = connection_string self.connection = None def connect(self): # Simulates the behavior of connecting to a database self.connection = open_database_connection(self.connection_string) def insert_record(self, record): if not self.connection: raise Exception(\\"Not connected to a database\\") # Simulate inserting a record return self.connection.insert(record) def get_records(self, query): if not self.connection: raise Exception(\\"Not connected to a database\\") # Simulate retrieving records based on a query return self.connection.find(query) def open_database_connection(connection_string): # External function to open a database connection pass ``` # Your Task: 1. Write a test class `TestDatabaseClient` using the `unittest` module, focused on testing the `DatabaseClient` class. 2. Use the `unittest.mock` module to mock dependencies and interactions. Specifically: - Mock the `open_database_connection` function to return a mock connection object. - Test the `connect` method to ensure it correctly sets the `connection` attribute. - Test the `insert_record` method to ensure it raises an exception if the client is not connected and properly calls the `insert` method on the connection. - Test the `get_records` method to ensure it raises an exception if the client is not connected and properly calls the `find` method on the connection. 3. Ensure your tests include different assertions provided by `unittest.mock` to validate the interactions with the mocks. # Example: ```python import unittest from unittest.mock import patch, Mock class TestDatabaseClient(unittest.TestCase): @patch(\'module_under_test.open_database_connection\') def test_connect(self, mock_open_db): # Example test for the connect method pass def test_insert_record(self): # Example test for the insert_record method pass def test_get_records(self): # Example test for the get_records method pass if __name__ == \'__main__\': unittest.main() ``` Note: - Replace `module_under_test` with the actual module name containing the `DatabaseClient` class and `open_database_connection` function. - Implement the test methods with appropriate use of `unittest.mock` functionalities.","solution":"import unittest from unittest.mock import Mock, patch class DatabaseClient: def __init__(self, connection_string): self.connection_string = connection_string self.connection = None def connect(self): # Simulates the behavior of connecting to a database self.connection = open_database_connection(self.connection_string) def insert_record(self, record): if not self.connection: raise Exception(\\"Not connected to a database\\") # Simulate inserting a record return self.connection.insert(record) def get_records(self, query): if not self.connection: raise Exception(\\"Not connected to a database\\") # Simulate retrieving records based on a query return self.connection.find(query) def open_database_connection(connection_string): # External function to open a database connection pass"},{"question":"**Objective:** Assess the student\'s proficiency in using Seaborn\'s `seaborn.objects` interface to create complex visualizations with multiple layers. **Problem Statement:** You are given two datasets named `penguins` and `diamonds` loaded from Seaborn\'s built-in datasets. Using Seaborn\'s `seaborn.objects` interface, you are required to create two plots demonstrating specific visualizations for each dataset. 1. **Penguins Dataset Visualization:** Create a plot using the `penguins` dataset with the following requirements: - Plot `species` on the x-axis and `body_mass_g` on the y-axis. - Add dots for each data point with jitter applied to prevent overlap. - Include a layer to display the interquartile range (25th to 75th percentile) shifted slightly to the right on the x-axis. 2. **Diamonds Dataset Visualization:** Create a plot using the `diamonds` dataset with the following requirements: - Plot `carat` on the x-axis and `clarity` on the y-axis. - Add dots for each data point with jitter applied to prevent overlap. - Include a layer to display the interquartile range (25th to 75th percentile) shifted slightly downwards on the y-axis. **Function Signature:** You need to implement the function `create_plots()` containing no parameters. The function should return a tuple containing two Seaborn Plot objects, one for each visualization described above. ```python import seaborn.objects as so from seaborn import load_dataset def create_plots(): penguins = load_dataset(\\"penguins\\") diamonds = load_dataset(\\"diamonds\\") # Penguins Dataset Visualization penguins_plot = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.2)) ) # Diamonds Dataset Visualization diamonds_plot = ( so.Plot(diamonds, \\"carat\\", \\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=.25)) ) return penguins_plot, diamonds_plot ``` **Constraints:** - Use only the `seaborn.objects` interface for creating the plots. - Ensure the function returns the plot objects correctly to facilitate further analysis or rendering. **Note:** To test your function, you may need to use a Jupyter Notebook or any Python environment where Seaborn\'s visual output can be rendered.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_plots(): penguins = load_dataset(\\"penguins\\") diamonds = load_dataset(\\"diamonds\\") # Penguins Dataset Visualization penguins_plot = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.2)) ) # Diamonds Dataset Visualization diamonds_plot = ( so.Plot(diamonds, \\"carat\\", \\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=.25)) ) return penguins_plot, diamonds_plot"},{"question":"# Mocking and Patching in Python Unit Tests **Objective**: To assess your understanding and ability to use the `unittest.mock` library to mock and patch objects in Python unit tests, including handling synchronous and asynchronous functions, configuring mock behaviors, and making assertions about the usage of mocks. Problem Statement You are developing a test suite for a financial application. The application has a `BankAccount` class, which interacts with a `Database` class to fetch and update account balances. Your task is to: 1. Mock the `Database` class methods to test the `BankAccount` methods without interacting with the actual database. 2. Configure the mocks to return specific values and raise exceptions as needed. 3. Use the `patch` decorator to replace parts of the system under test during your unit tests. 4. Make assertions about how the mocks were called. Classes ```python class Database: def get_balance(self, account_id: str) -> float: Fetch the account balance from the database. pass def update_balance(self, account_id: str, amount: float) -> None: Update the account balance in the database. pass class BankAccount: def __init__(self, account_id: str, db: Database): self.account_id = account_id self.db = db def deposit(self, amount: float) -> float: Deposits the given amount into the bank account. current_balance = self.db.get_balance(self.account_id) new_balance = current_balance + amount self.db.update_balance(self.account_id, new_balance) return new_balance def withdraw(self, amount: float) -> float: Withdraws the given amount from the bank account if sufficient funds are available. current_balance = self.db.get_balance(self.account_id) if amount > current_balance: raise ValueError(\\"Insufficient funds\\") new_balance = current_balance - amount self.db.update_balance(self.account_id, new_balance) return new_balance ``` Tasks 1. **Mocking and Patching**: - Write unit tests for the `deposit` method of the `BankAccount` class. - Mock the `get_balance` and `update_balance` methods of the `Database` class. - Configure the `get_balance` method to return a specific balance. - Configure the `update_balance` method to not perform any actual update but verify it was called with the correct arguments. 2. **Assertions**: - Write unit tests for the `withdraw` method of the `BankAccount` class. - Mock the `get_balance` and `update_balance` methods of the `Database` class. - Configure the `get_balance` method to return values and raise exceptions as needed. - Make assertions to ensure the `update_balance` method is called with the correct arguments only if sufficient funds are available. - Ensure the `withdraw` method raises a `ValueError` if there are insufficient funds. 3. **Asynchronous Testing**: (Advanced) - Imagine there is an asynchronous method `async_fetch_balance` in the `Database` class that fetches the account balance asynchronously. - Demonstrate how to mock this method using `AsyncMock` and test an asynchronous function in the `BankAccount` class that uses this method. Guidelines - Use the `unittest` framework for writing the tests. - Use the `unittest.mock` library features including `Mock`, `MagicMock`, `patch`, `create_autospec`, and assertions. - Ensure your tests are comprehensive and handle various scenarios including normal operation and exception handling. Hint: Refer to the `unittest.mock` documentation for examples of how to mock and patch methods, configure return values and side effects, and make assertions about method calls. **Submission**: Provide your unit test code with explanations for each part.","solution":"from unittest.mock import Mock class Database: def get_balance(self, account_id: str) -> float: Fetch the account balance from the database. pass def update_balance(self, account_id: str, amount: float) -> None: Update the account balance in the database. pass class BankAccount: def __init__(self, account_id: str, db: Database): self.account_id = account_id self.db = db def deposit(self, amount: float) -> float: Deposits the given amount into the bank account. current_balance = self.db.get_balance(self.account_id) new_balance = current_balance + amount self.db.update_balance(self.account_id, new_balance) return new_balance def withdraw(self, amount: float) -> float: Withdraws the given amount from the bank account if sufficient funds are available. current_balance = self.db.get_balance(self.account_id) if amount > current_balance: raise ValueError(\\"Insufficient funds\\") new_balance = current_balance - amount self.db.update_balance(self.account_id, new_balance) return new_balance"},{"question":"# Question: Handling Nullable Integer Data Types with Pandas You are provided with a DataFrame containing information about a series of items with their respective quantities and identifiers. Your task is to ensure proper usage of nullable integer data types in this DataFrame, perform several operations, and handle missing data appropriately. Implement the following functions based on the given descriptions: 1. **Convert and Initialize DataFrame**: - `initialize_dataframe(items: List[Dict[str, Any]]) -> pd.DataFrame` - **Input**: A list of dictionaries where each dictionary represents an item with keys `id`, `quantity`, and `description`. `id` and `quantity` can have `None` values. - **Output**: A DataFrame with columns `ID`, `Quantity`, and `Description`. The `ID` and `Quantity` columns should use nullable integer data types. 2. **Fill Missing Quantities**: - `fill_missing_quantities(df: pd.DataFrame, fill_value: int) -> pd.DataFrame` - **Input**: A DataFrame with `Quantity` as nullable integer type and an integer `fill_value`. - **Output**: The DataFrame with all missing values in `Quantity` replaced by `fill_value`. 3. **Compute Total Quantity**: - `total_quantity(df: pd.DataFrame) -> int` - **Input**: A DataFrame with `Quantity` as nullable integer type. - **Output**: The sum of all quantities in the DataFrame. If there are no quantities, return 0. 4. **Find Items by Quantity**: - `find_items_by_quantity(df: pd.DataFrame, quantity: int) -> pd.DataFrame` - **Input**: A DataFrame with `Quantity` as nullable integer type and an integer `quantity`. - **Output**: A DataFrame with all rows where the `Quantity` matches the given `quantity`. Implement the functions ensuring proper handling of nullable integer data types and missing values. Here is a template to get you started: ```python import pandas as pd from typing import List, Dict, Any def initialize_dataframe(items: List[Dict[str, Any]]) -> pd.DataFrame: # Your code here pass def fill_missing_quantities(df: pd.DataFrame, fill_value: int) -> pd.DataFrame: # Your code here pass def total_quantity(df: pd.DataFrame) -> int: # Your code here pass def find_items_by_quantity(df: pd.DataFrame, quantity: int) -> pd.DataFrame: # Your code here pass ``` # Example Usage: ```python items = [ {\\"id\\": 1, \\"quantity\\": 10, \\"description\\": \\"Item A\\"}, {\\"id\\": 2, \\"quantity\\": None, \\"description\\": \\"Item B\\"}, {\\"id\\": None, \\"quantity\\": 5, \\"description\\": \\"Item C\\"}, {\\"id\\": 4, \\"quantity\\": None, \\"description\\": \\"Item D\\"} ] df = initialize_dataframe(items) print(df) df = fill_missing_quantities(df, 0) print(df) total = total_quantity(df) print(total) items_with_quantity_0 = find_items_by_quantity(df, 0) print(items_with_quantity_0) ``` **Note:** Pay special attention to handling `None` values and ensure the data types are as specified.","solution":"import pandas as pd from typing import List, Dict, Any def initialize_dataframe(items: List[Dict[str, Any]]) -> pd.DataFrame: Initializes a DataFrame with nullable integer data types for \'ID\' and \'Quantity\' columns. df = pd.DataFrame(items) df[\'id\'] = df[\'id\'].astype(\'Int64\') df[\'quantity\'] = df[\'quantity\'].astype(\'Int64\') df = df.rename(columns={\'id\': \'ID\', \'quantity\': \'Quantity\', \'description\': \'Description\'}) return df def fill_missing_quantities(df: pd.DataFrame, fill_value: int) -> pd.DataFrame: Fills missing values in the \'Quantity\' column with \'fill_value\'. df[\'Quantity\'] = df[\'Quantity\'].fillna(fill_value) return df def total_quantity(df: pd.DataFrame) -> int: Computes the total quantity, handling NULL values properly. return df[\'Quantity\'].sum() def find_items_by_quantity(df: pd.DataFrame, quantity: int) -> pd.DataFrame: Finds and returns items with a specified quantity. return df[df[\'Quantity\'] == quantity]"},{"question":"# Advanced Python Programming Problem Using the `curses` Library **Objective:** Demonstrate your ability to create a complex text-based user interface using the `curses` library in Python. # Problem Description: You are tasked with creating a simple text editor using the `curses` library. The editor should have the following features: 1. A main window that displays the text buffer. 2. A status bar at the bottom that shows information like cursor position and the current mode (insert/normal). 3. The ability to switch between \'insert mode\' and \'normal mode\'. 4. The ability to navigate the text using arrow keys. 5. Text editing capabilities: - In \'insert mode\', typed characters should be inserted at the cursor position. - In \'normal mode\', pressing \'i\' should switch to \'insert mode\'. 6. Basic commands: - `Ctrl+S` to save the text buffer to a file. - `Ctrl+Q` to quit the editor. 7. Visual indication of the current mode by changing the status bar color. # Input and Output Input: - Interactions are directly managed within the terminal window using keyboard input. Output: - Displayed in the terminal window using the `curses` library\'s methods. # Constraints: 1. Use only the `curses` library for handling the interface. 2. The editor should handle basic edge cases such as navigating beyond the text bounds gracefully. 3. Assume the editor deals with simple ASCII text (no multi-byte characters). # Performance Requirements: - The editor should respond to key presses without noticeable lag. # Your Task: Implement a Python function, `run_editor`, which initializes the curses environment and runs the text editor. The function signature should be: ```python def run_editor(): ``` # Example Usage: When the `run_editor` function is run, users should be able to interact with the editor as described in the problem statement. ``` # You do not need to provide an example input/output as this is an interactive application. ``` # Hints: 1. Look into the `curses.wrapper` function for initializing the curses environment. 2. Use `curses.newwin` to create different windows for the main text buffer and the status bar. 3. Handle key events such as `KEY_UP`, `KEY_DOWN`, `KEY_LEFT`, `KEY_RIGHT`, and character inserts. 4. Use `curses.textpad.Textbox` for managing text input in a specific window, if needed. 5. Refer to the provided curses documentation for detailed function references and usage patterns. Good luck, and happy coding!","solution":"import curses def save_to_file(buffer, filename=\\"output.txt\\"): with open(filename, \\"w\\") as f: for line in buffer: f.write(line + \\"n\\") def run_editor(): def main(stdscr): # Init curses curses.curs_set(1) # Make the cursor visible curses.noecho() curses.cbreak() stdscr.keypad(True) # Colors curses.start_color() curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_WHITE) curses.init_pair(2, curses.COLOR_WHITE, curses.COLOR_GREEN) # Set up initial variables max_y, max_x = stdscr.getmaxyx() text_window = curses.newwin(max_y - 1, max_x, 0, 0) status_window = curses.newwin(1, max_x, max_y - 1, 0) buffer = [\\"\\"] cursor_y, cursor_x = 0, 0 insert_mode = False while True: # Draw the status bar status_window.clear() status_text = f\\"Position: {cursor_y + 1},{cursor_x + 1} | Mode: {\'INSERT\' if insert_mode else \'NORMAL\'}\\" status_window.attron(curses.color_pair(2 if insert_mode else 1)) status_window.addstr(0, 0, status_text[:max_x - 1]) status_window.clrtoeol() status_window.attroff(curses.color_pair(2 if insert_mode else 1)) status_window.refresh() # Draw the text buffer text_window.clear() for i, line in enumerate(buffer): text_window.addstr(i, 0, line) text_window.move(cursor_y, cursor_x) text_window.refresh() # Handle key press key = stdscr.getch() if insert_mode: if key == 27: # ESC key to switch to NORMAL mode insert_mode = False elif key in (curses.KEY_BACKSPACE, 127): # Handle backspace if cursor_x > 0: buffer[cursor_y] = buffer[cursor_y][:cursor_x-1] + buffer[cursor_y][cursor_x:] cursor_x -= 1 elif cursor_y > 0: cursor_x = len(buffer[cursor_y-1]) buffer[cursor_y-1] += buffer[cursor_y] buffer.pop(cursor_y) cursor_y -= 1 elif key == curses.KEY_ENTER or key == 10: # Handle Enter key buffer.insert(cursor_y + 1, buffer[cursor_y][cursor_x:]) buffer[cursor_y] = buffer[cursor_y][:cursor_x] cursor_y += 1 cursor_x = 0 elif 32 <= key <= 126: # Handle printable characters buffer[cursor_y] = buffer[cursor_y][:cursor_x] + chr(key) + buffer[cursor_y][cursor_x:] cursor_x += 1 else: if key == ord(\'i\'): # Switch to INSERT mode insert_mode = True elif key == curses.KEY_UP: if cursor_y > 0: cursor_y -= 1 cursor_x = min(cursor_x, len(buffer[cursor_y])) elif key == curses.KEY_DOWN: if cursor_y < len(buffer) - 1: cursor_y += 1 cursor_x = min(cursor_x, len(buffer[cursor_y])) elif key == curses.KEY_LEFT: if cursor_x > 0: cursor_x -= 1 elif cursor_y > 0: cursor_y -= 1 cursor_x = len(buffer[cursor_y]) elif key == curses.KEY_RIGHT: if cursor_x < len(buffer[cursor_y]): cursor_x += 1 elif cursor_y < len(buffer) - 1: cursor_y += 1 cursor_x = 0 elif key == 24: # Ctrl+X to quit break elif key == 19: # Ctrl+S to save save_to_file(buffer) curses.endwin() curses.wrapper(main)"},{"question":"# CSV File Operations and Custom Dialect Creation You are provided with a CSV file named `data.csv` containing the following data: ``` Name,Age,Department Alice,30,Engineering Bob,25,HR Charlie,35,Finance ``` Your task is to perform three operations: 1. **Create a custom dialect named `semicolon_dialect`**, where: - The delimiter is a semicolon (`;`). - Quoting is minimal. - Double quotes are used to quote fields containing special characters. 2. **Write a Python function `convert_to_semicolon_dialect(input_file, output_file)`** that: - Reads the CSV data from `input_file` using the default CSV format. - Writes the data to `output_file` using the `semicolon_dialect`. 3. **Write a Python function `read_semicolon_dialect(file)`** that: - Reads and prints the CSV data from `file` using the `semicolon_dialect`. - Prints each row in the format: `\\"Name: {Name}, Age: {Age}, Department: {Department}\\"`. # Requirements: - **Input and Output Format**: - Both `input_file` and `output_file` are strings representing the file paths. - The `read_semicolon_dialect` function takes a single string argument representing the file path. - **Constraints and Limitations**: - Assume the input file always exists and is properly formatted. - Ensure proper closing of file handles. - Handle any potential CSV errors gracefully. - **Performance Requirements**: - Aim for efficient file reading/writing operations. # Example Usage: ```python convert_to_semicolon_dialect(\'data.csv\', \'output.csv\') read_semicolon_dialect(\'output.csv\') ``` Expected output from `read_semicolon_dialect(\'output.csv\')`: ``` Name: Alice, Age: 30, Department: Engineering Name: Bob, Age: 25, Department: HR Name: Charlie, Age: 35, Department: Finance ``` # Implementation: Below is a skeleton to help you get started: ```python import csv def create_semicolon_dialect(): csv.register_dialect( \'semicolon_dialect\', delimiter=\';\', quoting=csv.QUOTE_MINIMAL, quotechar=\'\\"\', doublequote=True ) def convert_to_semicolon_dialect(input_file, output_file): create_semicolon_dialect() with open(input_file, newline=\'\') as infile: reader = csv.reader(infile) with open(output_file, \'w\', newline=\'\') as outfile: writer = csv.writer(outfile, dialect=\'semicolon_dialect\') for row in reader: writer.writerow(row) def read_semicolon_dialect(file): create_semicolon_dialect() with open(file, newline=\'\') as infile: reader = csv.DictReader(infile, dialect=\'semicolon_dialect\') for row in reader: print(f\\"Name: {row[\'Name\']}, Age: {row[\'Age\']}, Department: {row[\'Department\']}\\") # Example usage if __name__ == \\"__main__\\": convert_to_semicolon_dialect(\'data.csv\', \'output.csv\') read_semicolon_dialect(\'output.csv\') ```","solution":"import csv def create_semicolon_dialect(): Creates a CSV dialect for semicolon-separated values with minimal quoting. csv.register_dialect( \'semicolon_dialect\', delimiter=\';\', quoting=csv.QUOTE_MINIMAL, quotechar=\'\\"\', doublequote=True ) def convert_to_semicolon_dialect(input_file, output_file): Converts a CSV file from default format to a semicolon-separated format using the custom dialect. Parameters: input_file (str): Path to the input CSV file in default format. output_file (str): Path to the output CSV file in semicolon-separated format. create_semicolon_dialect() with open(input_file, newline=\'\') as infile: reader = csv.reader(infile) with open(output_file, \'w\', newline=\'\') as outfile: writer = csv.writer(outfile, dialect=\'semicolon_dialect\') for row in reader: writer.writerow(row) def read_semicolon_dialect(file): Reads a semicolon-separated CSV file and prints each row in a formatted string. Parameters: file (str): Path to the semicolon-separated CSV file. create_semicolon_dialect() with open(file, newline=\'\') as infile: reader = csv.DictReader(infile, dialect=\'semicolon_dialect\') for row in reader: print(f\\"Name: {row[\'Name\']}, Age: {row[\'Age\']}, Department: {row[\'Department\']}\\") # Example usage if __name__ == \\"__main__\\": convert_to_semicolon_dialect(\'data.csv\', \'output.csv\') read_semicolon_dialect(\'output.csv\')"},{"question":"Objective In this assignment, you will demonstrate your understanding of Seaborn\'s theming and customization capabilities. Problem Statement Write a function `customize_seaborn_plots` that takes the following arguments: 1. `data` (dict): A dictionary where keys are category labels and values are numeric values. 2. `theme_style` (str): The Seaborn theme style to use (e.g., \\"darkgrid\\", \\"whitegrid\\"). 3. `palette` (str): The color palette to use (e.g., \\"deep\\", \\"pastel\\"). 4. `custom_rc` (dict): A dictionary of additional matplotlib rc parameters to customize (e.g., `{\\"axes.spines.right\\": False}`). The function should create a bar plot using the provided data, theme style, and palette. Additionally, it should apply the custom rc parameters specified. **Function Signature:** ```python def customize_seaborn_plots(data: dict, theme_style: str, palette: str, custom_rc: dict) -> None: pass ``` **Constraints:** - The `data` dictionary will contain between 1 and 10 items. - All numeric values will be between 0 and 100. - `theme_style` will be one of the valid theme styles supported by Seaborn. - `palette` will be one of the valid palettes supported by Seaborn. - `custom_rc` will contain valid matplotlib rc parameters. **Example Usage:** ```python data = {\\"A\\": 10, \\"B\\": 15, \\"C\\": 7} theme_style = \\"whitegrid\\" palette = \\"pastel\\" custom_rc = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} customize_seaborn_plots(data, theme_style, palette, custom_rc) ``` The function should generate a bar plot with the following customizations: - Apply the \'whitegrid\' theme with the \'pastel\' color palette. - Custom rc parameters to hide the right and top spines on the plots. Assessment Criteria 1. Correctness: Does the function correctly apply the specified themes, palettes, and custom rc parameters? 2. Code Quality: Is the code clean, well-organized, and commented appropriately? 3. Functionality: Does the function achieve the objectives stated, and is the plot generated correctly reflecting the given inputs?","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_seaborn_plots(data: dict, theme_style: str, palette: str, custom_rc: dict) -> None: Customizes a Seaborn plot based on the given parameters. Parameters: - data (dict): A dictionary where keys are category labels and values are numeric values. - theme_style (str): The Seaborn theme style to use (e.g., \\"darkgrid\\", \\"whitegrid\\"). - palette (str): The color palette to use (e.g., \\"deep\\", \\"pastel\\"). - custom_rc (dict): A dictionary of additional matplotlib rc parameters to customize. sns.set_theme(style=theme_style, palette=palette, rc=custom_rc) # Creating the bar plot categories = list(data.keys()) values = list(data.values()) sns.barplot(x=categories, y=values) # Display the plot plt.show()"},{"question":"**PyTorch Coding Assessment Question** # Objective: Implement a custom PyTorch dataset and a custom collate function to efficiently load and preprocess data using the `torch.utils.data.DataLoader`. # Problem Statement: You are given a list of file paths to image files and a corresponding list of labels. Your task is to create a custom map-style dataset that loads and preprocesses these images, and a custom collate function that preprocesses batches of these images efficiently. # Requirements: 1. Implement a custom dataset named `CustomImageDataset` that: - Inherits from `torch.utils.data.Dataset`. - Takes `image_paths` and `labels` as input, where `image_paths` is a list of file paths to images, and `labels` is a list of corresponding labels. - Implements the `__len__` method to return the number of items in the dataset. - Implements the `__getitem__` method to load an image and its corresponding label given an index. 2. Implement a custom collate function named `custom_collate_fn` that: - Takes a list of samples (tuples of images and labels) as input. - Resizes all images in the batch to a fixed size (e.g., 128x128). - Converts the images and labels to PyTorch tensors. - Returns a dictionary with keys \\"images\\" and \\"labels\\" containing the corresponding tensors. 3. Use the implemented `CustomImageDataset` and `custom_collate_fn` with a PyTorch `DataLoader` to load the data in batches. # Input: - `image_paths`: List of file paths to images. - `labels`: List of numerical labels corresponding to each image. - `batch_size`: Integer representing the batch size for the DataLoader. # Output: - A dictionary containing two keys: - \\"images\\": A tensor of shape `(batch_size, 3, 128, 128)` containing the image data. - \\"labels\\": A tensor of shape `(batch_size)` containing the labels. # Constraints: - Ensure that the DataLoader uses multi-process data loading with 4 worker processes. - Enable memory pinning to speed up host to GPU transfers by setting `pin_memory=True`. # Performance Requirements: - The dataset and collate function should efficiently handle loading and preprocessing of images to avoid bottlenecks during training. # Example Usage: ```python from torch.utils.data import DataLoader # Example file paths and labels image_paths = [\'path/to/image1.jpg\', \'path/to/image2.jpg\', ...] labels = [0, 1, ...] # Custom dataset and collate function class CustomImageDataset(torch.utils.data.Dataset): def __init__(self, image_paths, labels): self.image_paths = image_paths self.labels = labels def __len__(self): return len(self.image_paths) def __getitem__(self, idx): image = load_image(self.image_paths[idx]) # Implement load_image function label = self.labels[idx] return image, label def custom_collate_fn(batch): images, labels = zip(*batch) images = [resize_image(image, (128, 128)) for image in images] # Implement resize_image function images = torch.stack([torch.tensor(image) for image in images]) labels = torch.tensor(labels) return {\\"images\\": images, \\"labels\\": labels} # DataLoader dataset = CustomImageDataset(image_paths, labels) dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True, collate_fn=custom_collate_fn) # Iterating through the DataLoader for batch in dataloader: images = batch[\\"images\\"] labels = batch[\\"labels\\"] # Use images and labels for model training/evaluation ``` # Note: - You need to implement the `load_image` and `resize_image` functions to load and resize the images from the file paths. This is indicated in the usage example.","solution":"import torch from torch.utils.data import Dataset from PIL import Image import numpy as np class CustomImageDataset(Dataset): def __init__(self, image_paths, labels): self.image_paths = image_paths self.labels = labels def __len__(self): return len(self.image_paths) def __getitem__(self, idx): image = Image.open(self.image_paths[idx]) label = self.labels[idx] return np.array(image), label def custom_collate_fn(batch): images, labels = zip(*batch) images = [Image.fromarray(image).resize((128, 128)) for image in images] images = torch.stack([torch.tensor(np.array(image)).permute(2, 0, 1) for image in images]) labels = torch.tensor(labels) return {\\"images\\": images, \\"labels\\": labels}"},{"question":"# Advanced Python Iterators Implementation Problem Statement You are required to implement two custom iterator classes in Python, similar to the behavior described for `PySeqIter_Type` and `PyCallIter_Type`. This will test your comprehension of iterators, sequences, and callable objects in Python. 1. **SequenceIterator**: - This iterator takes a sequence (e.g., list, tuple) as input. - The iteration should end when an `IndexError` is raised due to out-of-bounds indexing. 2. **CallableIterator**: - This iterator takes a callable object and a sentinel value as input. - The iteration should end when the callable returns a value equal to the sentinel. Input and Output Formats 1. **SequenceIterator**: - **Input**: A sequence (e.g., list, tuple). - **Output**: An iterator that yields elements from the sequence until an `IndexError` is encountered. Example: ```python seq_iter = SequenceIterator([1, 2, 3]) print(list(seq_iter)) # Output: [1, 2, 3] ``` 2. **CallableIterator**: - **Input**: A callable that takes no parameters and a sentinel value. - **Output**: An iterator that yields values returned by the callable until the sentinel value is returned. Example: ```python def counter(): i = 0 while True: yield i i += 1 call_iter = CallableIterator(counter().__next__, 3) print(list(call_iter)) # Output: [0, 1, 2] ``` Constraints - For **SequenceIterator**, the input sequence should support the `__getitem__` method. - For **CallableIterator**, the callable should be able to be called with no arguments. Performance Requirements - The implementation should efficiently iterate over the provided sequence and callable responses without unnecessary computations. Function Definitions You are required to implement the following classes: ```python class SequenceIterator: def __init__(self, sequence): # Initialize with a sequence def __iter__(self): # Return the iterator object def __next__(self): # Implement the iteration logic class CallableIterator: def __init__(self, callable_obj, sentinel): # Initialize with a callable and a sentinel value def __iter__(self): # Return the iterator object def __next__(self): # Implement the iteration logic ```","solution":"class SequenceIterator: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.sequence): raise StopIteration result = self.sequence[self.index] self.index += 1 return result class CallableIterator: def __init__(self, callable_obj, sentinel): self.callable_obj = callable_obj self.sentinel = sentinel def __iter__(self): return self def __next__(self): result = self.callable_obj() if result == self.sentinel: raise StopIteration return result"},{"question":"# Asynchronous Text File Processing with `concurrent.futures` You are required to implement a Python function using the `concurrent.futures` package for asynchronously processing a large number of text files. The function should count the occurrences of a specific word in given text files concurrently. Requirements: 1. Read the content of each file concurrently. 2. Count the occurrences of a specified word in each file. 3. Return a dictionary where the keys are the file names and the values are the respective word counts. Implementation: - Use `ThreadPoolExecutor` to achieve concurrent file processing. - Create appropriate exception handling to ensure robust execution, logging if any file fails to process. # Function Signature: ```python def count_word_in_files(word: str, files: List[str]) -> Dict[str, int]: pass ``` # Input: - `word` (str): The word to count occurrences of. - `files` (List[str]): List of file paths to process. # Output: - Dict[str, int]: A dictionary with file names as keys and the count of the specified word as values. # Constraints: - Assume the number of files is large but predefined. - Assume file paths are valid and these files contain text data. # Example: ```python files = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] word = \\"example\\" output = count_word_in_files(word, files) # Example Output: # { # \\"file1.txt\\": 5, # \\"file2.txt\\": 3, # \\"file3.txt\\": 7 # } ``` # Additional Notes: - Make use of `executor.submit()` for submitting individual file processing tasks. - Efficiently handle the case where file content is large. - Ensure your function handles file read errors gracefully and logs appropriate error messages.","solution":"import concurrent.futures from typing import List, Dict def count_word_in_files(word: str, files: List[str]) -> Dict[str, int]: Count the occurrences of a specified word in given text files concurrently. Parameters: word (str): The word to count occurrences of. files (List[str]): List of file paths to process. Returns: Dict[str, int]: A dictionary where the keys are the file names and the values are the respective word counts. word_count = {} def count_word_in_file(file): try: with open(file, \'r\') as f: content = f.read() return file, content.lower().split().count(word.lower()) except Exception as e: print(f\\"Error processing file {file}: {e}\\") return file, 0 with concurrent.futures.ThreadPoolExecutor() as executor: future_to_file = {executor.submit(count_word_in_file, file): file for file in files} for future in concurrent.futures.as_completed(future_to_file): file, count = future.result() word_count[file] = count return word_count"},{"question":"# Custom Email Encoder Implementation You are required to implement a simplified custom email encoder module that mimics some of the functionalities provided by the legacy `email.encoders` module. Specifically, you will implement two functions: `encode_to_quopri` and `encode_to_base64` to encode the payloads of email messages and set the appropriate `Content-Transfer-Encoding` headers. Function: `encode_to_quopri(msg)` This function takes an email message object as input, encodes its payload in quoted-printable format, and sets the `Content-Transfer-Encoding` header to \\"quoted-printable\\". **Input:** - `msg`: An email message object with a payload that can be encoded. **Output:** - The function will modify the `msg` in place by encoding its payload and setting the `Content-Transfer-Encoding` header. **Constraints:** - The `msg` should be a single-part message. - If the message is multiparts, you should raise a `TypeError`. Function: `encode_to_base64(msg)` This function takes an email message object as input, encodes its payload in base64 format, and sets the `Content-Transfer-Encoding` header to \\"base64\\". **Input:** - `msg`: An email message object with a payload that can be encoded. **Output:** - The function will modify the `msg` in place by encoding its payload and setting the `Content-Transfer-Encoding` header. **Constraints:** - The `msg` should be a single-part message. - If the message is multiparts, you should raise a `TypeError`. Example Usage: ```python from email.message import EmailMessage import quopri import base64 def encode_to_quopri(msg): if msg.is_multipart(): raise TypeError(\\"Cannot encode a multipart message\\") payload = msg.get_payload() encoded_payload = quopri.encodestring(payload.encode(\'utf-8\')).decode(\'utf-8\') msg.set_payload(encoded_payload) msg[\'Content-Transfer-Encoding\'] = \'quoted-printable\' def encode_to_base64(msg): if msg.is_multipart(): raise TypeError(\\"Cannot encode a multipart message\\") payload = msg.get_payload() encoded_payload = base64.b64encode(payload.encode(\'utf-8\')).decode(\'utf-8\') msg.set_payload(encoded_payload) msg[\'Content-Transfer-Encoding\'] = \'base64\' # Example Creation of EmailMessage msg = EmailMessage() msg.set_payload(\\"This is an example payload\\") # Encoding the message using encode_to_quopri encode_to_quopri(msg) print(msg[\'Content-Transfer-Encoding\']) # Output: quoted-printable print(msg.get_payload()) # Encoded payload in quoted-printable form # Encoding the message using encode_to_base64 encode_to_base64(msg) print(msg[\'Content-Transfer-Encoding\']) # Output: base64 print(msg.get_payload()) # Encoded payload in base64 form ``` Your task is to implement the `encode_to_quopri` and `encode_to_base64` functions based on the given specifications. **Note:** 1. You should not use the functions from the deprecated `email.encoders` module directly. 2. The `quopri` and `base64` modules should be used for encoding.","solution":"from email.message import EmailMessage import quopri import base64 def encode_to_quopri(msg): if msg.is_multipart(): raise TypeError(\\"Cannot encode a multipart message\\") payload = msg.get_payload() encoded_payload = quopri.encodestring(payload.encode(\'utf-8\')).decode(\'utf-8\') msg.set_payload(encoded_payload) msg[\'Content-Transfer-Encoding\'] = \'quoted-printable\' def encode_to_base64(msg): if msg.is_multipart(): raise TypeError(\\"Cannot encode a multipart message\\") payload = msg.get_payload() encoded_payload = base64.b64encode(payload.encode(\'utf-8\')).decode(\'utf-8\') msg.set_payload(encoded_payload) msg[\'Content-Transfer-Encoding\'] = \'base64\'"},{"question":"**Objective:** You are tasked with implementing a Python function that evaluates a model using both validation curves and learning curves. This will help in understanding how different hyperparameter values and training set sizes affect the performance of the model. **Context:** You need to use `scikit-learn` to analyze the performance of an SVM classifier with a linear kernel on a dataset. You will generate both validation and learning curves and provide plots to visualize the results. **Task:** Write a Python function `analyze_model_performance` that performs the following actions: 1. **Loads the Iris dataset**. 2. **Shuffles the dataset** (for randomization). 3. **Generates a validation curve** for the hyperparameter `C` with values ranging from `1e-7` to `1e3`. 4. **Generates a learning curve** with training sizes `[50, 80, 110]`. 5. **Plots the validation curve** and **learning curve** using `ValidationCurveDisplay` and `LearningCurveDisplay`. # Function Signature ```python def analyze_model_performance(): pass ``` # Expected Output - Plots a validation curve showing training and validation scores for various values of C. - Plots a learning curve showing training and validation scores for different training sample sizes. # Constraints - Use SVM with a linear kernel. - Use a 5-fold cross-validation approach. - Plots should include legends and proper labels for clarity. # Example Usage ```python analyze_model_performance() ``` The function does not return any values but should display the plots when executed. *Note*: Ensure you handle any necessary imports within your function. **Hints**: - Use the `load_iris` function from `sklearn.datasets` to load the data. - Use `np.random.shuffle` for shuffling the dataset. - Use `validation_curve` and `learning_curve` from `sklearn.model_selection`. - Use `ValidationCurveDisplay` and `LearningCurveDisplay` for plot visualizations.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve from sklearn.model_selection import ShuffleSplit from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.utils import shuffle def analyze_model_performance(): # Load and shuffle the Iris dataset iris = load_iris() X, y = shuffle(iris.data, iris.target, random_state=0) # Create an SVM model pipeline with data standardization model = make_pipeline(StandardScaler(), SVC(kernel=\'linear\')) # Define the hyperparameter range for C param_range = np.logspace(-7, 3, 10) # Generate the validation curve train_scores, test_scores = validation_curve( model, X, y, param_name=\\"svc__C\\", param_range=param_range, scoring=\\"accuracy\\", n_jobs=1, cv=5) # Compute mean and standard deviation for training and test scores train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) # Plot the validation curve plt.figure(figsize=(10, 6)) plt.title(\\"Validation Curve with SVM (linear kernel)\\") plt.xlabel(\\"Parameter C\\") plt.ylabel(\\"Accuracy Score\\") plt.ylim(0.0, 1.1) plt.semilogx(param_range, train_scores_mean, label=\\"Training score\\", color=\\"darkorange\\", lw=2) plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"darkorange\\", lw=2) plt.semilogx(param_range, test_scores_mean, label=\\"Cross-validation score\\", color=\\"navy\\", lw=2) plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\\"navy\\", lw=2) plt.legend(loc=\\"best\\") plt.show() # Generate the learning curve train_sizes, train_scores, test_scores = learning_curve( model, X, y, train_sizes=[50, 80, 110], cv=5, scoring=\\"accuracy\\", n_jobs=1) # Compute mean and standard deviation for training and test scores train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) # Plot the learning curve plt.figure(figsize=(10, 6)) plt.title(\\"Learning Curve with SVM (linear kernel)\\") plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Accuracy Score\\") plt.ylim(0.0, 1.1) plt.plot(train_sizes, train_scores_mean, label=\\"Training score\\", color=\\"darkorange\\", lw=2) plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"darkorange\\", lw=2) plt.plot(train_sizes, test_scores_mean, label=\\"Cross-validation score\\", color=\\"navy\\", lw=2) plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\\"navy\\", lw=2) plt.legend(loc=\\"best\\") plt.show()"},{"question":"Objective: You are required to demonstrate your understanding of unsupervised dimensionality reduction techniques in scikit-learn by applying Principal Component Analysis (PCA) and Feature Agglomeration on a given dataset. You will then use a simple classifier to predict target labels using the reduced data. Dataset: You will work with the `digits` dataset from scikit-learn. Task: 1. **Data Loading and Preprocessing**: - Load the `digits` dataset using `sklearn.datasets.load_digits`. - Standardize the features using `StandardScaler`. 2. **Dimensionality Reduction**: - Apply two separate dimensionality reduction techniques: PCA and Feature Agglomeration. - Reduce the number of features to 20 for both methods. 3. **Model Training and Evaluation**: - Use a `LogisticRegression` model to train and evaluate the performance on the reduced datasets. - Calculate the accuracy of the logistic regression model for each reduction technique. 4. **Pipeline Creation**: - Create and apply a pipeline that integrates the preprocessing, dimensionality reduction, and logistic regression steps into a single pipeline. Expected Functions: - `load_and_preprocess_data() -> Tuple[np.ndarray, np.ndarray]` - Load and standardize the `digits` dataset. - Returns a tuple of the standardized features and target labels. - `reduce_dimensionality_PCA(X: np.ndarray) -> np.ndarray` - Apply PCA to reduce the number of features to 20. - Returns the reduced feature set. - `reduce_dimensionality_Agglomeration(X: np.ndarray) -> np.ndarray` - Apply Feature Agglomeration to reduce the number of features to 20. - Returns the reduced feature set. - `train_and_evaluate_model(X: np.ndarray, y: np.ndarray) -> float` - Train a logistic regression model and evaluate the accuracy. - Returns the accuracy score. - `create_pipeline() -> sklearn.pipeline.Pipeline` - Create a pipeline that includes StandardScaler, PCA, and LogisticRegression. - Returns the constructed pipeline. Constraints: - Use `n_components=20` for PCA. - Use `n_clusters=20` for Feature Agglomeration. - Use default settings for `LogisticRegression`. Input/Output: - Input: None (Functions take input parameters directly) - Output: Function return values should adhere to specified types and formats. Performance Requirements: - Ensure the model training and evaluation process is efficient and completes within a reasonable time frame. Example: ```python from sklearn.datasets import load_digits from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import FeatureAgglomeration from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def load_and_preprocess_data() -> Tuple[np.ndarray, np.ndarray]: digits = load_digits() X = digits.data y = digits.target X = StandardScaler().fit_transform(X) return X, y def reduce_dimensionality_PCA(X: np.ndarray) -> np.ndarray: pca = PCA(n_components=20) return pca.fit_transform(X) def reduce_dimensionality_Agglomeration(X: np.ndarray) -> np.ndarray: agglo = FeatureAgglomeration(n_clusters=20) return agglo.fit_transform(X) def train_and_evaluate_model(X: np.ndarray, y: np.ndarray) -> float: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) model = LogisticRegression(max_iter=10000) model.fit(X_train, y_train) y_pred = model.predict(X_test) return accuracy_score(y_test, y_pred) def create_pipeline() -> Pipeline: pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'reducer\', PCA(n_components=20)), # You can switch this with FeatureAgglomeration (\'classifier\', LogisticRegression(max_iter=10000)) ]) return pipeline ``` Test the functions using the digits dataset and report the accuracy of the logistic regression model with both PCA and Feature Agglomeration.","solution":"from sklearn.datasets import load_digits from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import FeatureAgglomeration from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def load_and_preprocess_data(): Load and standardize the digits dataset. Returns: Tuple[np.ndarray, np.ndarray]: standardized features and target labels. digits = load_digits() X = digits.data y = digits.target X = StandardScaler().fit_transform(X) return X, y def reduce_dimensionality_PCA(X): Apply PCA to reduce the number of features to 20. Args: X (np.ndarray): standardized features. Returns: np.ndarray: reduced feature set. pca = PCA(n_components=20) return pca.fit_transform(X) def reduce_dimensionality_Agglomeration(X): Apply Feature Agglomeration to reduce the number of features to 20. Args: X (np.ndarray): standardized features. Returns: np.ndarray: reduced feature set. agglo = FeatureAgglomeration(n_clusters=20) return agglo.fit_transform(X) def train_and_evaluate_model(X, y): Train a logistic regression model and evaluate the accuracy. Args: X (np.ndarray): features. y (np.ndarray): target labels. Returns: float: accuracy score. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) model = LogisticRegression(max_iter=10000) model.fit(X_train, y_train) y_pred = model.predict(X_test) return accuracy_score(y_test, y_pred) def create_pipeline(): Create a pipeline that includes StandardScaler, PCA, and LogisticRegression. Returns: Pipeline: constructed pipeline. pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'reducer\', PCA(n_components=20)), # Switch to FeatureAgglomeration if needed (\'classifier\', LogisticRegression(max_iter=10000)) ]) return pipeline"},{"question":"**Context:** You have been provided with a conceptual understanding of how Python\'s C API handles iteration. Instead of working directly with the C API in a Python environment, you will implement similar functionality in pure Python. This exercise will test your understanding of iterators, asynchronous iterators, and the iteration protocol in Python. **Task:** 1. Create a class `MyIterator` that mimics the behavior of a typical Python iterator. 2. Implement another class `MyAsyncIterator` that mimics the behavior of an asynchronous iterator. 3. Write a function `next_item(iterator)` that retrieves the next item from an iterator, raising a `StopIteration` exception when the iterator is exhausted. 4. Write a function `send_item(async_iterator, value)` that sends a value into an asynchronous iterator, handling the stop and yield patterns. **Specifications:** 1. **`MyIterator` Class:** - Should implement the iterator protocol (`__iter__` and `__next__` methods). - The constructor should take an iterable and initialize an internal iterator. - The `__next__` method should return the next item from the internal iterator or raise `StopIteration` when exhausted. ```python class MyIterator: def __init__(self, iterable): # Implement constructor def __iter__(self): # Return iterator self def __next__(self): # Return next item or raise StopIteration ``` 2. **`MyAsyncIterator` Class:** - Should implement the asynchronous iterator protocol (`__aiter__` and `__anext__` methods). - The constructor should take an iterable and initialize an internal iterator. - The `__anext__` method should be an async method returning the next item or raising `StopAsyncIteration` when exhausted. ```python class MyAsyncIterator: def __init__(self, iterable): # Implement constructor def __aiter__(self): # Return asynchronous iterator self async def __anext__(self): # Return next item or raise StopAsyncIteration ``` 3. **`next_item(iterator)` Function:** - Takes a `MyIterator` instance as an argument. - Returns the next item or raises `StopIteration` when the iterator is exhausted. ```python def next_item(iterator): # Implement function to get next item or raise StopIteration ``` 4. **`send_item(async_iterator, value)` Function:** - Takes a `MyAsyncIterator` instance and a value as arguments. - Sends the value into the asynchronous iterator, handles the yield and return values, and handles potential exceptions. ```python async def send_item(async_iterator, value): # Implement function to send value into async iterator ``` **Constraints:** - Do not use any external libraries. - Use Python\'s standard features and built-in classes. **Example Usage:** ```python # Example for MyIterator it = MyIterator([1, 2, 3]) print(next_item(it)) # Output: 1 print(next_item(it)) # Output: 2 print(next_item(it)) # Output: 3 print(next_item(it)) # Raises StopIteration # Example for MyAsyncIterator import asyncio async def main(): ait = MyAsyncIterator([1, 2, 3]) print(await send_item(ait, None)) # Output: 1 print(await send_item(ait, None)) # Output: 2 print(await send_item(ait, None)) # Output: 3 print(await send_item(ait, None)) # Raises StopAsyncIteration asyncio.run(main()) ``` **Performance Requirements:** - Ensure that the classes and functions handle sequences efficiently. - Aim for an optimal runtime within O(n) for n items in the iterable.","solution":"class MyIterator: def __init__(self, iterable): self._iter = iter(iterable) def __iter__(self): return self def __next__(self): return next(self._iter) class MyAsyncIterator: def __init__(self, iterable): self._iter = iter(iterable) def __aiter__(self): return self async def __anext__(self): try: return next(self._iter) except StopIteration: raise StopAsyncIteration def next_item(iterator): return next(iterator) async def send_item(async_iterator, value): return await async_iterator.__anext__()"},{"question":"# Task You are required to write a Python module that performs a specific function and a comprehensive test suite for this module using the `unittest` framework and utilities provided in the `test.support` package. Follow the details below: # Part 1: Function Implementation Implement a function `count_words_in_file(filepath: str) -> dict` that reads a file and returns a dictionary where the keys are words (case-insensitive) and the values are the counts of each word in the file. Requirements: 1. The function should ignore punctuation and count words in a case-insensitive manner. 2. Handle file reading and ensure the file is properly closed after reading. 3. The function should raise a `FileNotFoundError` if the given file path does not exist. # Part 2: Unit Test Implementation Write a comprehensive test suite for `count_words_in_file(filepath: str) -> dict` using `unittest` and relevant utilities from `test.support`. Requirements: 1. **Test cases** should cover: - Basic functionality with typical input. - Edge cases like handling empty files. - Error handling (e.g., non-existent file). - Ensuring the function handles mixed case sensitivity properly. - Proper punctuation handling. 2. **Use utilities** from `test.support` to: - Create temporary files for testing and ensure they are properly cleaned up. - Implement any useful context managers/decorators for setting up and tearing down environment conditions needed for tests. 3. **Optional enhancements**: - Test performance for large files if time permits. Example `unittest` test class structure: ```python import unittest from test import support import os import tempfile from your_module import count_words_in_file class TestCountWordsInFile(unittest.TestCase): def setUp(self): # Setup code, if necessary self.temp_dir = support.temp_dir() os.makedirs(self.temp_dir, exist_ok=True) def tearDown(self): # Teardown code to clean up after tests support.rmtree(self.temp_dir) def test_basic_functionality(self): # Create a temporary file with some text filepath = os.path.join(self.temp_dir, \'testfile.txt\') with open(filepath, \'w\') as f: f.write(\\"Hello world! Hello everyone.\\") expected_result = {\'hello\': 2, \'world\': 1, \'everyone\': 1} self.assertEqual(count_words_in_file(filepath), expected_result) def test_empty_file(self): # Another test case for an empty file filepath = os.path.join(self.temp_dir, \'emptyfile.txt\') with open(filepath, \'w\') as f: pass expected_result = {} self.assertEqual(count_words_in_file(filepath), expected_result) def test_file_not_found(self): # Test case for file not found with self.assertRaises(FileNotFoundError): count_words_in_file(\'/non/existent/file.txt\') # Add more test methods covering other scenarios if __name__ == \'__main__\': unittest.main() ``` # Submission Ensure your submission includes: 1. The `count_words_in_file` function implemented in a module. 2. The comprehensive `unittest` test suite for the function.","solution":"import re from collections import defaultdict def count_words_in_file(filepath: str) -> dict: Reads a file and returns a dictionary where the keys are words (case-insensitive) and the values are the counts of each word in the file. :param filepath: Path to the file to be read. :return: Dictionary with word counts. word_counts = defaultdict(int) try: with open(filepath, \'r\') as file: for line in file: # Remove punctuation and split into words words = re.findall(r\'bw+b\', line.lower()) for word in words: word_counts[word] += 1 except FileNotFoundError: raise FileNotFoundError(f\\"The file at {filepath} was not found.\\") return dict(word_counts)"},{"question":"**Objective**: Demonstrate knowledge of PyTorch backend configurations to optimize tensor operations. # Problem Statement Implement a function `optimize_tensor_computation` that performs the following steps: 1. Checks the availability of the CUDA backend. 2. If CUDA is available, enable TensorFloat-32 tensor cores if they are not already enabled. 3. Perform a matrix multiplication using PyTorch tensors and record the time taken for the computation. 4. Disable TensorFloat-32 tensor cores and perform the matrix multiplication again, recording the time taken. 5. Return a dictionary with the timing results of both configurations. # Input - No direct input arguments. # Output - A dictionary with two key-value pairs: - `\'tf32_enabled\'`: Time taken for matrix multiplication with TensorFloat-32 enabled. - `\'tf32_disabled\'`: Time taken for matrix multiplication with TensorFloat-32 disabled. # Constraints - The matrix size for the multiplication is 1000x1000. - Use GPU if CUDA is available; otherwise, use CPU. - Assume PyTorch is correctly installed and CUDA is configured on the system if available. # Performance Requirements - Efficiently check and toggle backend settings. - Use PyTorch\'s built-in time measurement functions for accurate timing. # Function Signature ```python def optimize_tensor_computation() -> dict: pass ``` # Example Usage ```python result = optimize_tensor_computation() print(result) # Expected output (example) # {\'tf32_enabled\': 0.005, \'tf32_disabled\': 0.008} ``` # Implementation Notes - Use `torch.cuda.is_available()` to check for CUDA availability. - Modify `torch.backends.cuda.matmul.allow_tf32` to enable/disable TensorFloat-32. - Use `torch.randn` to generate random tensors for matrix multiplication. - Use `torch.cuda.synchronize()` to ensure accurate timing on GPU computations. # Hints - Utilize `time.time()` or PyTorch profilers for timing the operations. - Ensure switching of backend settings is done properly before each multiplication operation.","solution":"import torch import time def optimize_tensor_computation() -> dict: results = { \'tf32_enabled\': None, \'tf32_disabled\': None } # Generate random tensors for matrix multiplication tensor_size = (1000, 1000) A = torch.randn(tensor_size, device=\'cuda\' if torch.cuda.is_available() else \'cpu\') B = torch.randn(tensor_size, device=\'cuda\' if torch.cuda.is_available() else \'cpu\') # Ensure we are on the right device if torch.cuda.is_available(): torch.cuda.synchronize() # Enable TensorFloat-32 tensor cores torch.backends.cuda.matmul.allow_tf32 = True start = time.time() C = torch.mm(A, B) torch.cuda.synchronize() results[\'tf32_enabled\'] = time.time() - start # Disable TensorFloat-32 tensor cores torch.backends.cuda.matmul.allow_tf32 = False start = time.time() C = torch.mm(A, B) torch.cuda.synchronize() results[\'tf32_disabled\'] = time.time() - start return results"},{"question":"You are required to write a Python function `normalize_and_compare` that compares two Unicode strings for case-insensitive equality, accounting for Unicode normalization. **Function Signature:** ```python def normalize_and_compare(s1: str, s2: str) -> bool: pass ``` **Input:** - `s1` (str): The first input Unicode string. - `s2` (str): The second input Unicode string. **Output:** - Returns `True` if the strings are considered equal when compared in a case-insensitive manner after normalizing them. - Returns `False` otherwise. **Steps to Consider:** 1. Normalize both strings to the same Unicode form using the `unicodedata.normalize()` function. Use Form D (NFD) for decomposition. 2. Convert both strings to a case-insensitive form using the `casefold()` method. 3. Compare the normalized, casefolded strings for equality. **Examples:** ```python print(normalize_and_compare(\'Ãª\', \'eu0302\')) # True, as \'Ãª\' (U+00EA) is the same as \'e\' followed by \' Ì„\' (U+0065 U+0302). print(normalize_and_compare(\'StraÃŸe\', \'strasse\')) # True, \'ÃŸ\' casefolds to \'ss\'. print(normalize_and_compare(\'cafÃ©\', \'cafeu0301\')) # True, \'Ã©\' (U+00E9) is equivalent to \'e\' followed by \' Ì\' (U+0065 U+0301) print(normalize_and_compare(\'Python\', \'PÃTHON\')) # False, \'Ã\' is not lowercase \'y\'. ``` You must handle all possible Unicode scenarios and ensure that the function works efficiently for large strings. **Constraints:** - The input strings can be up to 10^5 characters long. Use the `unicodedata` module and methods for handling string conversions whenever necessary. **Note:** You may assume that the provided input strings always represent valid Unicode text.","solution":"import unicodedata def normalize_and_compare(s1: str, s2: str) -> bool: # Normalize both strings to the same Unicode form s1_normalized = unicodedata.normalize(\'NFD\', s1).casefold() s2_normalized = unicodedata.normalize(\'NFD\', s2).casefold() # Compare the normalized, case-insensitive strings for equality return s1_normalized == s2_normalized"},{"question":"You are required to implement a function that processes a list of various Python objects to categorize them into different types and returns a dictionary summarizing the counts of each type. In particular, your function will: 1. **Type-check** each object in the given list. 2. **Categorize** by the following types: `int`, `bool`, `float`, `complex`, `bytes`, `bytearray`, `str`, `tuple`, `list`, `dict`, `set`. 3. Any object not falling into the above types should be categorized as `other`. # Function Signature ```python def categorize_objects(objects: list) -> dict: Categorizes a list of various Python objects into their respective counts by type. Args: objects (list): A list containing various Python objects. Returns: dict: A dictionary where keys are types (as described) and values are counts of objects of those types. ``` # Input - A list `objects` containing up to `10^5` items where each item can be any type of Python object. # Output - A dictionary summarizing the counts of each type. The keys should be the type names (strings) and the values should be the counts of those types. # Constraints - The input list will contain no more than `10^5` elements. - Each key in the output dictionary must be one of the specified types or `other`. # Example ```python objects = [1, 2.5, \'hello\', 3, 5, (1,2), [1,2,3], {1: 2}, {1, 2}, True, False, b\'byte\', bytearray(b\'test\'), 1+2j] output = categorize_objects(objects) # Example output can be: # { # \'int\': 3, # \'bool\': 2, # \'float\': 1, # \'complex\': 1, # \'bytes\': 1, # \'bytearray\': 1, # \'str\': 1, # \'tuple\': 1, # \'list\': 1, # \'dict\': 1, # \'set\': 1, # \'other\': 0 # } ``` # Additional Information Ensure your function is efficient and can handle the maximum input size within a reasonable time frame. # Notes: - Use appropriate type checking mechanisms to ensure the objects are categorized correctly. - Ensure that your function handles the \\"other\\" category correctly for types not listed.","solution":"def categorize_objects(objects): Categorizes a list of various Python objects into their respective counts by type. Args: objects (list): A list containing various Python objects. Returns: dict: A dictionary where keys are types (as described) and values are counts of objects of those types. type_counts = { \'int\': 0, \'bool\': 0, \'float\': 0, \'complex\': 0, \'bytes\': 0, \'bytearray\': 0, \'str\': 0, \'tuple\': 0, \'list\': 0, \'dict\': 0, \'set\': 0, \'other\': 0 } for obj in objects: obj_type = type(obj).__name__ if obj_type in type_counts: type_counts[obj_type] += 1 else: type_counts[\'other\'] += 1 return type_counts"},{"question":"Your task is to transform a set of labels using `LabelBinarizer`, `MultiLabelBinarizer`, and `LabelEncoder`, and demonstrate understanding of their application and differences. # Input and Output Formats 1. **LabelBinarizer** - **Input**: A list of multiclass integer labels. - **Output**: A one-hot encoded binary indicator matrix. 2. **MultiLabelBinarizer** - **Input**: A nested list of integer labels (each sublist representing multiple labels for a single instance). - **Output**: A binary indicator matrix. 3. **LabelEncoder** - **Input**: A list of non-numeric labels (e.g., strings). - **Output**: A list of encoded integer labels. # Task Write three functions `label_binarizer_transform`, `multi_label_binarizer_transform`, and `label_encoder_transform` that perform the following transformations: 1. **label_binarizer_transform(list_of_labels: List[int]) -> np.ndarray:** - Takes a list of multiclass labels (e.g., [1, 2, 6, 4, 2]). - Returns a binary indicator matrix. 2. **multi_label_binarizer_transform(list_of_label_lists: List[List[int]]) -> np.ndarray:** - Takes a nested list of multi-label data (e.g., [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]). - Returns a binary indicator matrix. 3. **label_encoder_transform(list_of_non_numeric_labels: List[str]) -> (List[int], List[str]):** - Takes a list of non-numeric labels (e.g., [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"]). - Returns a tuple consisting of a list of encoded integer labels and a list of the original classes. # Constraints - You may assume the input lists are non-empty. - You may use any imports from `sklearn.preprocessing`. # Example ```python import numpy as np from sklearn import preprocessing from typing import List def label_binarizer_transform(list_of_labels: List[int]) -> np.ndarray: lb = preprocessing.LabelBinarizer() lb.fit(list_of_labels) return lb.transform(list_of_labels) def multi_label_binarizer_transform(list_of_label_lists: List[List[int]]) -> np.ndarray: mlb = preprocessing.MultiLabelBinarizer() return mlb.fit_transform(list_of_label_lists) def label_encoder_transform(list_of_non_numeric_labels: List[str]) -> (List[int], List[str]): le = preprocessing.LabelEncoder() le.fit(list_of_non_numeric_labels) encoded_labels = le.transform(list_of_non_numeric_labels).tolist() original_classes = le.classes_.tolist() return encoded_labels, original_classes # Test cases print(label_binarizer_transform([1, 2, 6, 4, 2])) print(multi_label_binarizer_transform([[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]])) print(label_encoder_transform([\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"])) ``` # Output ```python [[1 0 0 0] [0 1 0 0] [0 0 1 0] [0 0 0 1]] [[0 0 1 1 1] [0 0 1 0 0] [1 1 0 1 0] [1 1 1 1 1] [1 1 1 0 0]] ([1, 1, 2, 0], [\'amsterdam\', \'paris\', \'tokyo\']) ```","solution":"import numpy as np from sklearn import preprocessing from typing import List, Tuple def label_binarizer_transform(list_of_labels: List[int]) -> np.ndarray: Transform a list of multiclass integer labels to a one-hot encoded binary indicator matrix. lb = preprocessing.LabelBinarizer() lb.fit(list_of_labels) return lb.transform(list_of_labels) def multi_label_binarizer_transform(list_of_label_lists: List[List[int]]) -> np.ndarray: Transform a nested list of multi-label data to a binary indicator matrix. mlb = preprocessing.MultiLabelBinarizer() return mlb.fit_transform(list_of_label_lists) def label_encoder_transform(list_of_non_numeric_labels: List[str]) -> Tuple[List[int], List[str]]: Transform a list of non-numeric labels to a list of encoded integer labels and return the original classes. le = preprocessing.LabelEncoder() le.fit(list_of_non_numeric_labels) encoded_labels = le.transform(list_of_non_numeric_labels).tolist() original_classes = le.classes_.tolist() return (encoded_labels, original_classes)"},{"question":"**Objective:** Design a function that fetches content from a list of URLs, handles common HTTP errors, and reports the status of each URL fetch operation. **Task:** Implement a function `fetch_urls(url_list: list) -> dict` that takes in a list of URLs and returns a dictionary containing the URL as the key and the result as the value. The result for each URL should be: - \\"Content Retrieved\\" if the URL was successfully fetched. - \\"Not Found\\" if a 404 error occurred. - \\"Unauthorized\\" if a 401 error occurred. - \\"Forbidden\\" if a 403 error occurred. - \\"Other Error\\" for any other HTTP error. - \\"URL Error\\" if a `URLError` occurred. **Function Signature:** ```python def fetch_urls(url_list: list) -> dict: pass ``` **Input:** - `url_list`: A list of strings where each string is a URL. **Output:** - Returns a dictionary where the keys are URLs from the input list, and the values are the result strings as specified above. **Constraints:** - The function should handle a reasonably sized list of URLs (up to 50 URLs). **Requirements:** 1. Use `urllib.request` to handle fetching the URLs. 2. Handle HTTP and URL errors as specified. 3. Retrieve only the HTTP status codes mentioned and handle them accordingly. 4. Ensure the function is robust and handles network-related errors gracefully. **Example:** ```python url_list = [ \\"http://example.com\\", # Assume this URL is fetched successfully. \\"http://example.com/404\\", # Assume this URL returns a 404 error. \\"http://example.com/401\\", # Assume this URL returns a 401 error. \\"http://example.com/403\\", # Assume this URL returns a 403 error. \\"http://invalid.url\\", # Assume this URL raises a URLError. ] result = fetch_urls(url_list) # Possible output: # { # \\"http://example.com\\": \\"Content Retrieved\\", # \\"http://example.com/404\\": \\"Not Found\\", # \\"http://example.com/401\\": \\"Unauthorized\\", # \\"http://example.com/403\\": \\"Forbidden\\", # \\"http://invalid.url\\": \\"URL Error\\", # } ```","solution":"import urllib.request import urllib.error def fetch_urls(url_list: list) -> dict: results = {} for url in url_list: try: response = urllib.request.urlopen(url) if response.status == 200: results[url] = \\"Content Retrieved\\" else: results[url] = \\"Other Error\\" except urllib.error.HTTPError as e: if e.code == 404: results[url] = \\"Not Found\\" elif e.code == 401: results[url] = \\"Unauthorized\\" elif e.code == 403: results[url] = \\"Forbidden\\" else: results[url] = \\"Other Error\\" except urllib.error.URLError: results[url] = \\"URL Error\\" return results"},{"question":"# Advanced URL Handling with `urllib` **Objective:** Write a function `fetch_url_info(url: str) -> dict` that performs multiple tasks using Python\'s `urllib` package. The function should: 1. **Fetch the content** of the input URL. 2. **Handle errors** appropriately. 3. **Parse and return URL components**. 4. **Check robots.txt** for whether crawling the page is allowed. **Input:** - `url`: A string representing the URL to fetch, e.g., \\"http://example.com\\". **Output:** - A dictionary with the following structure: ```python { \\"content\\": \\"Content of the URL as a string\\", \\"parsed_url\\": { \\"scheme\\": \\"http\\", \\"netloc\\": \\"example.com\\", \\"path\\": \\"/\\", \\"params\\": \\"\\", \\"query\\": \\"\\", \\"fragment\\": \\"\\" }, \\"can_crawl\\": True # Boolean indicating if crawling is allowed per robots.txt } ``` **Constraints:** - If an error occurs during fetching, the `content` should be set to `None` and an appropriate error message should be returned in the dictionary as `error`. - The `robots.txt` check should derive the URL for the `robots.txt` file from the input URL. **Performance Requirements:** - The function should handle URLs efficiently, and network operations should be robust. **Example:** ```python >>> fetch_url_info(\\"http://example.com\\") { \\"content\\": \\"<!doctype html>...\\", \\"parsed_url\\": { \\"scheme\\": \\"http\\", \\"netloc\\": \\"example.com\\", \\"path\\": \\"/\\", \\"params\\": \\"\\", \\"query\\": \\"\\", \\"fragment\\": \\"\\" }, \\"can_crawl\\": True } ``` **Notes:** - Utilize `urllib.request` to open and read the URL. - Use `urllib.error` to catch and handle exceptions during URL fetching. - Parse the URL using `urllib.parse`. - Check the `robots.txt` rules using `urllib.robotparser`. Implement the function `fetch_url_info(url: str) -> dict` in Python.","solution":"import urllib.request import urllib.error import urllib.parse import urllib.robotparser def fetch_url_info(url: str) -> dict: result = { \\"content\\": None, \\"parsed_url\\": None, \\"can_crawl\\": False, \\"error\\": None, } try: # Fetch URL content with urllib.request.urlopen(url) as response: content = response.read().decode() result[\\"content\\"] = content # Parse URL parsed_url = urllib.parse.urlparse(url) result[\\"parsed_url\\"] = { \\"scheme\\": parsed_url.scheme, \\"netloc\\": parsed_url.netloc, \\"path\\": parsed_url.path, \\"params\\": parsed_url.params, \\"query\\": parsed_url.query, \\"fragment\\": parsed_url.fragment } # Check robots.txt robots_url = urllib.parse.urljoin(url, \\"/robots.txt\\") rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_url) rp.read() result[\\"can_crawl\\"] = rp.can_fetch(\\"*\\", url) except urllib.error.URLError as e: result[\\"error\\"] = str(e) return result"},{"question":"# Question: POP3 Mailbox Interaction Script You are required to write a Python script that interacts with a POP3 server using the `poplib` module. Your script should perform the following actions: 1. Connect to the POP3 server. 2. Log in using the provided username and password. 3. Retrieve the list of messages in the mailbox. 4. For each message, retrieve the message headers and the first 10 lines of the message. 5. Print the headers and the first 10 lines of each message to the console. 6. Log out and close the connection. # Input - Host address of the POP3 server (string) - Port number (integer) - Username (string) - Password (string) # Output - Printed headers and the first 10 lines of each message in the mailbox. # Constraints - You should handle potential errors that may arise during the connection, authentication, or retrieval processes. - The script should gracefully exit if it encounters any error, with an appropriate message. # Example ```python HOST = \\"pop.example.com\\" PORT = 110 USERNAME = \\"example_user\\" PASSWORD = \\"example_password\\" # Expected output: # +OK POP3 server ready # +OK 2 message [320] octets # Headers and first 10 lines of message 1 # -------------------------------------- # From: sender@example.com # Subject: Test Email 1 # ... # # Headers and first 10 lines of message 2 # -------------------------------------- # From: another@example.com # Subject: Test Email 2 # ... # # +OK Pop server signing off ``` Your implementation should include error handling to display user-friendly messages in case of issues like incorrect login credentials, connection timeouts, etc. # Extra Credit - Implement a feature to save the retrieved messages to files, with each message stored in a separate file named according to its unique ID. # Notes - Do not use any third-party libraries; only use the standard Python library. - Make sure to test your script with an actual POP3 server to verify its correctness.","solution":"import poplib from email.parser import Parser def fetch_emails(host, port, username, password): try: # Connect to the server server = poplib.POP3(host, port) print(server.getwelcome().decode(\'utf-8\')) # Log in to the server server.user(username) server.pass_(password) # Get the list of messages message_numbers = server.list()[1] message_count = len(message_numbers) print(f\\"+OK {message_count} messages available\\") # Process each message for i in range(1, message_count + 1): print(f\\"nHeaders and first 10 lines of message {i}\\") print(\\"-\\" * 50) # Retrieve the message headers and first 10 lines of the message response, lines, octets = server.top(i, 10) # Parse the headers msg_content = b\\"n\\".join(lines).decode(\'utf-8\') message = Parser().parsestr(msg_content) # Print headers for header in [\'From\', \'To\', \'Subject\', \'Date\']: print(f\\"{header}: {message[header]}\\") # Print first 10 lines of the message print(\\"n\\".join(lines)) # Log out and close connection server.quit() print(\\"+OK Pop server signing off\\") except poplib.error_proto as e: print(f\\"POP3 Protocol Error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Objective**: To assess the students\' understanding and ability to utilize the seaborn library to visualize and interpret data distributions using violin plots, along with customizing these plots using various parameters. Question You are provided with the Titanic dataset that needs to be visualized using violin plots. Implement a function `visualize_titanic_data()` that performs the following tasks: 1. **Preprocess the Data**: - Load the Titanic dataset using the seaborn library. - Handle any missing values in the \'age\' column by filling them with the median age. 2. **Generate Plots**: - Create a violin plot to visualize the age distribution of passengers. - Generate a second violin plot to compare the age distribution across different classes (`class` column). - Create a third violin plot that shows the same comparison as the second plot, but stratified by survival status (`alive` column). - Customize the third plot by: - Splitting the violins. - Adding a small gap between the dodged violins. - Representing every observation inside the distribution using sticks. 3. **Display the Plots**: - Ensure that all the generated plots are displayed in a single output. Function Signature ```python def visualize_titanic_data(): pass ``` # Example: ```python visualize_titanic_data() ``` # Expected Output: Three violin plots should be displayed: 1. A single violin plot depicting the age distribution. 2. A violin plot comparing age distributions across passenger classes. 3. A customized violin plot comparing age distributions across classes and survival status, with added split, gap, and inner sticks representation. Constraints: - Use only seaborn and matplotlib libraries for data visualization. - Ensure the plots are clear and well-labeled for better readability. # Additional Information: - The Titanic dataset can be loaded using `sns.load_dataset(\\"titanic\\")`. Notes: - Make sure to handle any missing data appropriately before plotting. - Experiment with different seaborn parameters to enhance the visual appeal of the plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def visualize_titanic_data(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Handle missing values in the \'age\' column by filling them with the median age titanic[\'age\'].fillna(titanic[\'age\'].median(), inplace=True) # Create a violin plot to visualize the age distribution of passengers plt.figure(figsize=(10, 6)) sns.violinplot(x=titanic[\'age\']) plt.title(\\"Age Distribution of Titanic Passengers\\") plt.xlabel(\\"Age\\") plt.show() # Generate a second violin plot to compare the age distribution across different classes (`class` column) plt.figure(figsize=(10, 6)) sns.violinplot(x=\'class\', y=\'age\', data=titanic) plt.title(\\"Age Distribution Across Passenger Classes\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Age\\") plt.show() # Create a third violin plot showing the same comparison as the second plot, but stratified by survival status (`alive` column) plt.figure(figsize=(10, 6)) sns.violinplot(x=\'class\', y=\'age\', hue=\'alive\', data=titanic, split=True, dodge=True, inner=\'stick\') plt.title(\\"Age Distribution Across Passenger Classes Stratified by Survival Status\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Age\\") plt.show()"},{"question":"# Python Coding Assessment: Custom Class Implementation Objective Implement a custom class in Python that combines the behaviors of a sequence, a custom descriptor, and asynchronous context management. Problem Statement You are required to create a class `MagicSequence` that behaves like a sequence (a list) and supports the following functionalities: 1. **Initialization and basic sequence operations**: Your class should be initialized with an arbitrary number of numerical elements. It should support sequence operations such as indexing, slicing, and iteration. 2. **Custom Attributes via Descriptor**: Implement a nested descriptor class `CustomAttribute` that manages a special attribute `status` within `MagicSequence`. The `status` attribute should store a value that can be read, modified, and deleted. 3. **Asynchronous Context Management**: `MagicSequence` should work with `async with` statements by implementing asynchronous context manager methods. Function Specifications 1. **Initialization**: - Signature: `def __init__(self, *args)`. - Initializes the sequence with the given arguments. 2. **Sequence Operations**: - Implement `__len__`, `__getitem__`, `__setitem__`, `__delitem__`, `__iter__`, and other necessary methods for sequence behavior. 3. **Custom Descriptor**: - Implement a nested class `CustomAttribute` with `__get__`, `__set__`, and `__delete__` methods to manage the `status` attribute. 4. **Asynchronous Context Manager**: - Implement `__aenter__` and `__aexit__` methods to perform custom actions upon entering and exiting the context. Example Usage ```python # Initialize and perform sequence operations seq = MagicSequence(1, 2, 3, 4) print(len(seq)) # Output: 4 print(seq[0]) # Output: 1 seq[1] = 10 print(list(seq)) # Output: [1, 10, 3, 4] # Use custom descriptor for attribute management seq.status = \\"Active\\" print(seq.status) # Output: Active del seq.status # Use as an asynchronous context manager import asyncio async def main(): async with seq as s: print(\\"Inside async context\\") asyncio.run(main()) ``` Constraints - The elements of the sequence will always be integers. - You must use built-in Python modules and functions only. - The sequence should support typical list behavior including but not limited to append, remove, and clear methods. - The `status` attribute should behave as a normal attribute with controlled access using the descriptor methods. - Implement appropriate exception handling for the defined operations. Your task is to implement this class and ensure it passes the provided example usage and other edge cases you consider during testing.","solution":"class MagicSequence: def __init__(self, *args): self._sequence = list(args) self.status = None # Initialize it here but managed by the descriptor def __len__(self): return len(self._sequence) def __getitem__(self, index): return self._sequence[index] def __setitem__(self, index, value): self._sequence[index] = value def __delitem__(self, index): del self._sequence[index] def __iter__(self): return iter(self._sequence) def append(self, value): self._sequence.append(value) def remove(self, value): self._sequence.remove(value) def clear(self): self._sequence.clear() class CustomAttribute: def __init__(self): self._value = None def __get__(self, instance, owner): return self._value def __set__(self, instance, value): self._value = value def __delete__(self, instance): self._value = None status = CustomAttribute() async def __aenter__(self): print(\\"Entering async context\\") return self async def __aexit__(self, exc_type, exc_value, traceback): print(\\"Exiting async context\\")"},{"question":"**Question: Implement a Custom Netrc Parser** Implement a custom parser for a `.netrc` file in Python. Your custom parser should replicate the core functionalities of the `netrc` module as described below. # Requirements: 1. **Class Name**: `CustomNetrc` 2. **Initialization**: - The class should accept an optional parameter `file` that specifies the path to the `.netrc` file to parse. If no file is provided, use the `.netrc` file from the user\'s home directory. - If the file does not exist, raise a `FileNotFoundError`. - If the file has syntax errors, raise a custom exception `CustomNetrcParseError` with attributes `msg`, `filename`, and `lineno`. 3. **Methods**: - `authenticators(host)`: Returns a 3-tuple `(login, account, password)` of authenticators for the specified `host`. If no such host exists, return the tuple for the `default` entry. If neither is available, return `None`. - `__repr__()`: Returns a string representation of the `.netrc` file content, discarding comments and possibly reordering entries. 4. **Attributes**: - `hosts`: A dictionary mapping host names to `(login, account, password)` tuples. - `macros`: A dictionary mapping macro names to lists of strings. # Constraints: - Passwords are limited to ASCII characters (whitespace and non-printable characters are not allowed). - Assume the `.netrc` file format is strict and follows common conventions (e.g., lines starting with `machine` denote a host entry). # Performance: - Ensure efficient parsing and data retrieval operations. # Example: Given a `.netrc` file with the following content: ``` machine host1 login user1 password pass1 machine host2 login user2 password pass2 default login user_default password pass_default ``` Your implementation should handle operations like: ```python netrc_instance = CustomNetrc(\'path_to_netrc_file\') print(netrc_instance.authenticators(\'host1\')) # Output: (\'user1\', None, \'pass1\') print(netrc_instance.authenticators(\'host3\')) # Output: (\'user_default\', None, \'pass_default\') print(netrc_instance) # Output: Representation of the .netrc file content ``` # Note: - Ensure you handle potential security checks similar to the original `netrc` class where necessary. - Provide appropriate error handling and validation.","solution":"import os class CustomNetrcParseError(Exception): def __init__(self, msg, filename, lineno): self.msg = msg self.filename = filename self.lineno = lineno super().__init__(f\\"{msg} (file: {filename}, line: {lineno})\\") class CustomNetrc: def __init__(self, file=None): self.hosts = {} self.macros = {} if file is None: file = os.path.join(os.path.expanduser(\\"~\\"), \\".netrc\\") if not os.path.exists(file): raise FileNotFoundError(f\\".netrc file not found: {file}\\") self._parse(file) def _parse(self, file): try: with open(file, \'r\') as f: lines = f.readlines() except Exception as e: raise IOError(f\\"Cannot read .netrc file: {file}\\") from e current_host = None lineno = 0 for line in lines: lineno += 1 line = line.strip() if not line or line.startswith(\\"#\\"): continue parts = line.split() if parts[0] == \\"machine\\" and len(parts) > 1: current_host = parts[1] self.hosts[current_host] = [None, None, None] elif parts[0] == \\"default\\": current_host = \\"default\\" self.hosts[current_host] = [None, None, None] elif current_host is not None: if parts[0] == \\"login\\" and len(parts) > 1: self.hosts[current_host][0] = parts[1] elif parts[0] == \\"account\\" and len(parts) > 1: self.hosts[current_host][1] = parts[1] elif parts[0] == \\"password\\" and len(parts) > 1: self.hosts[current_host][2] = parts[1] else: raise CustomNetrcParseError(\\"Syntax error\\", file, lineno) else: raise CustomNetrcParseError(\\"Syntax error\\", file, lineno) def authenticators(self, host): auth = self.hosts.get(host) if auth is None: auth = self.hosts.get(\\"default\\") return tuple(auth) if auth else None def __repr__(self): result = [] for host, (login, account, password) in self.hosts.items(): if host == \\"default\\": result.append(\\"default\\") else: result.append(f\\"machine {host}\\") if login: result.append(f\\" login {login}\\") if account: result.append(f\\" account {account}\\") if password: result.append(f\\" password {password}\\") return \\"n\\".join(result)"},{"question":"Objective: Create a custom neural network in PyTorch that includes a convolutional layer followed by a normalization layer. Replace BatchNorm with GroupNorm and ensure your network can handle varying batch sizes. Requirements: 1. Your custom neural network `CustomNet` should inherit from `torch.nn.Module`. 2. The network should contain: - A convolutional layer (`nn.Conv2d`). - A normalization layer where BatchNorm is replaced with GroupNorm as per the documentation. - An activation layer (`ReLU`). 3. Implement both the regular and replacing operations for the normalization layer directly in your module. Constraints: - The number of input channels, output channels, kernel size for the convolutional layer, and the number of groups for GroupNorm should be given as parameters to the network. - The input tensor shape will be `[batch_size, in_channels, image_height, image_width]` where `batch_size`, `image_height`, and `image_width` can vary. - Ensure the GroupNorm setup adheres to `C % G == 0`. Input: - `in_channels`: Integer, the number of input channels. - `out_channels`: Integer, the number of output channels. - `kernel_size`: Integer, the kernel size for the convolutional layer. - `num_groups`: Integer, the number of groups for GroupNorm. - `x`: A tensor of shape `[batch_size, in_channels, image_height, image_width]`. Output: - The forward method should return the output of the network after applying the convolutional, normalization, and activation layers. Example Usage: ```python import torch import torch.nn as nn class CustomNet(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, num_groups): super(CustomNet, self).__init__() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size) self.norm = nn.GroupNorm(num_groups, out_channels) self.activation = nn.ReLU() def forward(self, x): x = self.conv(x) x = self.norm(x) x = self.activation(x) return x # Test case if __name__ == \\"__main__\\": batch_size = 8 in_channels = 3 out_channels = 16 kernel_size = 3 image_height, image_width = 32, 32 num_groups = 4 # make sure out_channels % num_groups == 0 model = CustomNet(in_channels, out_channels, kernel_size, num_groups) x = torch.randn(batch_size, in_channels, image_height, image_width) output = model(x) print(output.shape) # Should return [batch_size, out_channels, new_height, new_width] ``` Note: - Explain any assumptions or considerations you made in your implementation. - Discuss how GroupNorm helps resolve issues associated with BatchNorm in `vmap`.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomNet(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, num_groups): super(CustomNet, self).__init__() self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2) self.norm = nn.GroupNorm(num_groups, out_channels) self.activation = nn.ReLU() def forward(self, x): x = self.conv(x) x = self.norm(x) x = self.activation(x) return x"},{"question":"**Coding Assessment Question** # Objective Design and implement a Python program that reads multiple files, compresses them using various algorithms, archives the compressed files into a single ZIP file, and provides options to decompress and extract the archived files. # Problem Statement You are required to write a Python program with the following functionalities: 1. **Compression**: - Read a list of file paths from a given directory. - Compress each file using `gzip`, `bz2`, and `lzma` algorithms. - Save the compressed files with appropriate extensions (`.gz`, `.bz2`, `.xz`). 2. **Archiving**: - Create a ZIP archive that contains all the compressed files. - The name of the ZIP archive should be provided as input. 3. **Decompression and Extraction**: - Provide a function to decompress all files within a given ZIP archive. - The decompressed files should be extracted to a specified directory. # Input and Output Format `compress_and_archive(dir_path: str, archive_name: str) -> None` - **Input**: - `dir_path`: A string representing the directory path containing the files to be compressed. - `archive_name`: A string representing the name of the output ZIP archive (without the `.zip` extension). - **Output**: - None `decompress_and_extract(zip_path: str, extract_dir: str) -> None` - **Input**: - `zip_path`: A string representing the path to the ZIP archive. - `extract_dir`: A string representing the directory path where the files should be extracted. - **Output**: - None # Constraints - Assume the directory contains only files and no subdirectories. - There may be a large number of files, so ensure the solution is efficiently designed. - Handle exceptions related to file operations and compression/decompression errors. # Example Usage ```python # Compress and archive files in \'input_files\' directory to \'compressed_files.zip\' compress_and_archive(\'input_files\', \'compressed_files\') # Decompress and extract files from \'compressed_files.zip\' to \'extracted_files\' directory decompress_and_extract(\'compressed_files.zip\', \'extracted_files\') ``` # Additional Notes - Make sure to handle edge cases like empty directories, inaccessible files, and unsupported file formats. - Document the code and provide clear comments explaining each part of the implementation.","solution":"import os import gzip import bz2 import lzma import zipfile import shutil def compress_and_archive(dir_path, archive_name): Compresses files in a given directory using gzip, bz2, and lzma, and archives them into a single ZIP file. :param dir_path: Directory path containing the files to be compressed. :param archive_name: Name of the output ZIP archive (without the .zip extension). compressed_files = [] for filename in os.listdir(dir_path): file_path = os.path.join(dir_path, filename) if os.path.isfile(file_path): # Compress using gzip with open(file_path, \'rb\') as f_in, gzip.open(file_path + \'.gz\', \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) compressed_files.append(file_path + \'.gz\') # Compress using bz2 with open(file_path, \'rb\') as f_in, bz2.open(file_path + \'.bz2\', \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) compressed_files.append(file_path + \'.bz2\') # Compress using lzma with open(file_path, \'rb\') as f_in, lzma.open(file_path + \'.xz\', \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) compressed_files.append(file_path + \'.xz\') # Create the zip file with zipfile.ZipFile(archive_name + \'.zip\', \'w\') as zipf: for file in compressed_files: zipf.write(file, os.path.basename(file)) os.remove(file) # Clean up compressed files def decompress_and_extract(zip_path, extract_dir): Decompresses and extracts files from the given ZIP archive into a specified directory. :param zip_path: Path to the ZIP archive. :param extract_dir: Directory path where the files should be extracted. with zipfile.ZipFile(zip_path, \'r\') as zipf: zipf.extractall(extract_dir) for filename in os.listdir(extract_dir): file_path = os.path.join(extract_dir, filename) if filename.endswith(\'.gz\'): with gzip.open(file_path, \'rb\') as f_in, open(file_path[:-3], \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) os.remove(file_path) elif filename.endswith(\'.bz2\'): with bz2.open(file_path, \'rb\') as f_in, open(file_path[:-4], \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) os.remove(file_path) elif filename.endswith(\'.xz\'): with lzma.open(file_path, \'rb\') as f_in, open(file_path[:-3], \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) os.remove(file_path)"},{"question":"You are required to implement a function `calculate_expression(expressions: List[str]) -> List[Optional[float]]` that evaluates a list of numeric expressions using functions from the `python310` numeric API. Each expression will be provided as a string and include basic operations such as addition (\'+\'), subtraction (\'-\'), multiplication (\'*\'), true division (\'/\'), and power (\'**\'). The result should be a list of floating-point numbers. If any expression is invalid or results in an error, the corresponding element in the result should be `None`. # Input - `expressions`: A list of strings `expressions` where each string is a numeric expression consisting of integers and the operators `+`, `-`, `*`, `/`, and `**`. # Output - A list of floating-point results corresponding to each expression, or `None` if an expression is invalid. # Constraints - Each string in `expressions` will be a valid numeric expression that can be evaluated. - The length of `expressions` list will not exceed 1000. - Each numeric expression string will not exceed a length of 100 characters. - Division by zero should be handled properly returning `None`. # Example ```python expressions = [\\"3 + 5\\", \\"10 - 3 * 2\\", \\"8 / 0\\", \\"2 ** 10\\"] print(calculate_expression(expressions)) # Output: [8.0, 4.0, None, 1024.0] ``` # Note Utilize the functions from `python310` numeric API for performing the numeric operations. You can assume that the expressions are simple and do not contain parentheses or nested operations beyond basic precedence. # Implementation Details 1. Parse each expression to identify numbers and operators. 2. Use the numeric API functions provided in the `python310` documentation to perform calculations. 3. Ensure proper exception handling and referencing counts as indicated in the API documentation. # Hints - You may find it useful to implement a helper function that takes two numbers and an operator, then does the calculation using the appropriate `python310` API function. - Be careful to manage reference counts and error handling using NULL checks.","solution":"from typing import List, Optional def calculate_expression(expressions: List[str]) -> List[Optional[float]]: results = [] for expression in expressions: try: result = eval(expression) results.append(result) except (ZeroDivisionError, Exception): results.append(None) return results"},{"question":"Title: University Course Management System You need to implement a simple course management system for a university. The system tracks courses, students, and the enrollment of students in courses. Given the following requirements, you should make use of Python\'s typing capabilities, focusing on `TypedDict`, `TypeVar`, `Callable`, and generics to ensure your types are correctly defined and validated. # Requirements: 1. **Student and Course Data** - Define a `TypedDict` called `Student` with the following fields: - `id`: A unique integer ID for the student. - `name`: The student\'s name as a string. - `email`: The student\'s email as a string. - Define a `TypedDict` called `Course` with the following fields: - `code`: A string representing the course code (e.g., \\"CS101\\"). - `title`: The title of the course as a string. - `instructor`: The instructor\'s name as a string. 2. **Enrollment Usage** - Define a `TypeVar` called `T` constrained to types `Student` and `Course`. 3. **Higher-order Function** - Implement a function `apply_to_enrolled` that takes a list of `T` (either `Student` or `Course`) and a callable operation which operates on elements of the list. Use `ParamSpec` and `Concatenate` from the typing module to ensure type safety. 4. **Creating New Subtypes** - Define a new type `StudentID` using `NewType` to create distinct type from `int`. 5. **Mandatory Structural Type** - Define a protocol `Enrollable` which mandates an `enroll` method. Ensure that both `Student` and `Course` can be enrolled. # Specifications: 1. **Function signatures and type requirements:** ```python from typing import TypedDict, TypeVar, Callable, NewType, List from typing import Protocol, runtime_checkable, ParamSpec, Concatenate # 1. Define Student and Course TypedDicts class Student(TypedDict): id: int name: str email: str class Course(TypedDict): code: str title: str instructor: str # 2. Define a TypeVar `T` for Student and Course T = TypeVar(\'T\', Student, Course) # 3. Define a new type `StudentID` that is distinct from `int` StudentID = NewType(\'StudentID\', int) # 4. Define a Protocol for Enrollable @runtime_checkable class Enrollable(Protocol): def enroll(student: Student) -> None: ... # 5. Implement the higher-order function which applies operations to enrolled members P = ParamSpec(\'P\') def apply_to_enrolled(elements: List[T], operation: Callable[Concatenate[T, P], None], *args: P.args, **kwargs: P.kwargs) -> None: for element in elements: operation(element, *args, **kwargs) ``` Follow these steps and define the necessary data structures and functions, ensuring to maintain type safety as indicated. # Example Usage ```python students = [ Student(id=1, name=\'Alice\', email=\'alice@example.com\'), Student(id=2, name=\'Bob\', email=\'bob@example.com\'), Student(id=3, name=\'Charlie\', email=\'charlie@example.com\') ] courses = [ Course(code=\'CS101\', title=\'Intro to Computer Science\', instructor=\'Prof. Smith\'), Course(code=\'MA101\', title=\'Calculus I\', instructor=\'Prof. Johnson\') ] def print_student(student: Student) -> None: print(f\\"Student ID: {student[\'id\']}, Name: {student[\'name\']}, Email: {student[\'email\']}\\") def print_course(course: Course) -> None: print(f\\"Course Code: {course[\'code\']}, Title: {course[\'title\']}, Instructor: {course[\'instructor\']}\\") apply_to_enrolled(students, print_student) apply_to_enrolled(courses, print_course) ``` Constraints: - Ensure you are using the correct typing constructs. - Use `Callable` accurately for the higher-order function. - Use `TypedDict`, `TypeVar`, and `NewType` as specified. This question assesses your understanding of advanced typing in Python and ensures you can create robust, type-safe code using Python\'s typing module.","solution":"from typing import TypedDict, TypeVar, Callable, NewType, List, Protocol, runtime_checkable, ParamSpec, Concatenate # Define Student and Course TypedDicts class Student(TypedDict): id: int name: str email: str class Course(TypedDict): code: str title: str instructor: str # Define a TypeVar `T` for Student and Course T = TypeVar(\'T\', bound=TypedDict(\'Enrollable\', {\'id\': int})) # Define a new type `StudentID` that is distinct from `int` StudentID = NewType(\'StudentID\', int) # Define a Protocol for Enrollable @runtime_checkable class Enrollable(Protocol): def enroll(student: Student) -> None: ... # Implement the higher-order function which applies operations to enrolled members P = ParamSpec(\'P\') def apply_to_enrolled(elements: List[T], operation: Callable[Concatenate[T, P], None], *args: P.args, **kwargs: P.kwargs) -> None: for element in elements: operation(element, *args, **kwargs)"},{"question":"You are tasked with implementing a function that logs messages with various priority levels and facilities. Your function should be able to: 1. Open a syslog with specific options and facilities. 2. Log messages with specified priorities. 3. Close the syslog. # Detailed Requirements Write a function `log_messages(messages, ident=None, logoption=0, facility=syslog.LOG_USER)` that logs a list of messages to the syslog. - **Inputs:** - `messages`: A list of tuples, where each tuple contains a priority level (integer) and the message string (str) to be logged. - `ident`: An optional string to be prepended to every message (default: None). - `logoption`: An optional log option (default: 0). - `facility`: An optional facility to send the messages to (default: `syslog.LOG_USER`). - **Outputs:** - The function should not return any value but should log the messages as specified. - **Constraints:** - The `priority` in messages should be one of the predefined priority levels (from \\"LOG_EMERG\\" to \\"LOG_DEBUG\\"). - The function should handle possible errors arising from incorrect priorities or other syslog operations properly. # Example ```python import syslog def log_messages(messages, ident=None, logoption=0, facility=syslog.LOG_USER): try: syslog.openlog(ident=ident, logoption=logoption, facility=facility) for priority, message in messages: syslog.syslog(priority, message) except Exception as e: print(f\\"Logging error: {e}\\") finally: syslog.closelog() # Usage example messages = [ (syslog.LOG_INFO, \\"Info message.\\"), (syslog.LOG_ERR, \\"Error message.\\"), (syslog.LOG_DEBUG, \\"Debug message.\\") ] log_messages(messages, ident=\\"TestApp\\", logoption=syslog.LOG_PID | syslog.LOG_CONS, facility=syslog.LOG_LOCAL0) ``` In this problem, students are expected to understand and appropriately use the `syslog` module functions (`syslog()`, `openlog()`, and `closelog()`). The question tests their ability to handle logging operations, including setting appropriate priorities, handling exceptions, and properly closing the log.","solution":"import syslog def log_messages(messages, ident=None, logoption=0, facility=syslog.LOG_USER): Logs a list of messages to the syslog with specified priority levels and facilities. :param messages: List of tuples where each tuple contains a priority level (int) and message (str). :param ident: Optional string to be prepended to every message. :param logoption: Optional log option (default: 0). :param facility: Optional facility to send the messages to (default: syslog.LOG_USER). try: syslog.openlog(ident=ident, logoption=logoption, facility=facility) for priority, message in messages: syslog.syslog(priority, message) except Exception as e: print(f\\"Logging error: {e}\\") finally: syslog.closelog()"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const s=this.searchQuery.trim().toLowerCase();return s?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(s)||e.solution&&e.solution.toLowerCase().includes(s)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(s=>setTimeout(s,1e3)),this.visibleCount+=4,this.isLoading=!1}}},R={class:"search-container"},D={class:"card-container"},q={key:0,class:"empty-state"},F=["disabled"],N={key:0},M={key:1};function O(s,e,l,m,o,i){const h=_("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"ðŸ¤”prompts chatðŸ§ ")])],-1)),t("div",R,[e[3]||(e[3]=t("span",{class:"search-icon"},"ðŸ”",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>o.searchQuery=r),placeholder:"Search..."},null,512),[[y,o.searchQuery]]),o.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>o.searchQuery="")}," âœ• ")):d("",!0)]),t("div",D,[(a(!0),n(b,null,v(i.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),i.displayedPoems.length===0?(a(),n("div",q,' No results found for "'+c(o.searchQuery)+'". ',1)):d("",!0)]),i.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:o.isLoading,onClick:e[2]||(e[2]=(...r)=>i.loadMore&&i.loadMore(...r))},[o.isLoading?(a(),n("span",M,"Loading...")):(a(),n("span",N,"See more"))],8,F)):d("",!0)])}const U=p(z,[["render",O],["__scopeId","data-v-c6cabf39"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"quotes/60.md","filePath":"quotes/60.md"}'),L={name:"quotes/60.md"},H=Object.assign(L,{setup(s){return(e,l)=>(a(),n("div",null,[x(U)]))}});export{Y as __pageData,H as default};
