import{_ as p,o as a,c as i,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(n,e,l,m,r,s){return a(),i("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-c3945947"]]),I=JSON.parse('[{"question":"You are provided with a `PyObject` manipulation module `number_ops` which mimics the behavior of numeric operations described in the provided documentation. Your task is to implement a class `NumpyObject` that simulates numeric operations on objects, ensuring that arithmetic, bitwise, and conversion operations function correctly in a Pythonic way. # Class Definition ```python class NumpyObject: def __init__(self, value): Initialize the NumpyObject with a numeric value. self.value = value def add(self, other): Adds the current value to another NumpyObject\'s value. Returns: A new NumpyObject containing the result of the addition. pass def subtract(self, other): Subtracts another NumpyObject\'s value from the current value. Returns: A new NumpyObject containing the result of the subtraction. pass def multiply(self, other): Multiplies the current value with another NumpyObject\'s value. Returns: A new NumpyObject containing the result of the multiplication. pass def floor_divide(self, other): Divides the current value by another NumpyObject\'s value using floor division. Returns: A new NumpyObject containing the result of the floor division. pass def true_divide(self, other): Divides the current value by another NumpyObject\'s value using true division. Returns: A new NumpyObject containing the result of the true division. pass def modulus(self, other): Computes the modulus of the current value by another NumpyObject\'s value. Returns: A new NumpyObject containing the result of the modulus. pass def power(self, exponent, mod=None): Raises the current value to the power of `exponent`. Optionally, performs the operation under modulo `mod`. Returns: A new NumpyObject containing the result of the power operation. pass def negate(self): Negates the current value. Returns: A new NumpyObject containing the negated value. pass def abs_value(self): Computes the absolute value of the current value. Returns: A new NumpyObject containing the absolute value. pass def to_long(self): Converts the current value to a long integer. Returns: A new NumpyObject containing the long integer value. pass def to_float(self): Converts the current value to a floating-point number. Returns: A new NumpyObject containing the float value. pass ``` # Constraints - `value` can be an integer or a float. - Methods should handle type conversions and raise appropriate exceptions if inputs are invalid. - The `power` method should mimic the behavior of Python\'s built-in `pow` function, including handling the third modulo argument. # Example Usage Here\'s an example of how the `NumpyObject` class would interact: ```python a = NumpyObject(10) b = NumpyObject(5) c = a.add(b) print(c.value) # Output: 15 d = a.multiply(b) print(d.value) # Output: 50 e = a.floor_divide(b) print(e.value) # Output: 2 f = a.true_divide(b) print(f.value) # Output: 2.0 g = a.modulus(b) print(g.value) # Output: 0 h = a.power(b) print(h.value) # Output: 100000 i = a.negate() print(i.value) # Output: -10 j = a.abs_value() print(j.value) # Output: 10 k = a.to_long() print(k.value) # Output: 10 (int type) l = a.to_float() print(l.value) # Output: 10.0 ``` Implement the `NumpyObject` class and ensure all methods provide the correct functionality according to the descriptions above. Good luck!","solution":"class NumpyObject: def __init__(self, value): if not isinstance(value, (int, float)): raise TypeError(\\"Value must be an int or a float.\\") self.value = value def add(self, other): if not isinstance(other, NumpyObject): raise TypeError(\\"Operand must be a NumpyObject.\\") return NumpyObject(self.value + other.value) def subtract(self, other): if not isinstance(other, NumpyObject): raise TypeError(\\"Operand must be a NumpyObject.\\") return NumpyObject(self.value - other.value) def multiply(self, other): if not isinstance(other, NumpyObject): raise TypeError(\\"Operand must be a NumpyObject.\\") return NumpyObject(self.value * other.value) def floor_divide(self, other): if not isinstance(other, NumpyObject): raise TypeError(\\"Operand must be a NumpyObject.\\") if other.value == 0: raise ZeroDivisionError(\\"Division by zero.\\") return NumpyObject(self.value // other.value) def true_divide(self, other): if not isinstance(other, NumpyObject): raise TypeError(\\"Operand must be a NumpyObject.\\") if other.value == 0: raise ZeroDivisionError(\\"Division by zero.\\") return NumpyObject(self.value / other.value) def modulus(self, other): if not isinstance(other, NumpyObject): raise TypeError(\\"Operand must be a NumpyObject.\\") if other.value == 0: raise ZeroDivisionError(\\"Division by zero.\\") return NumpyObject(self.value % other.value) def power(self, exponent, mod=None): if not isinstance(exponent, NumpyObject): raise TypeError(\\"Exponent must be a NumpyObject.\\") if mod is not None and not isinstance(mod, NumpyObject): raise TypeError(\\"Modulus must be a NumpyObject.\\") result = pow(self.value, exponent.value, mod.value if mod else None) return NumpyObject(result) def negate(self): return NumpyObject(-self.value) def abs_value(self): return NumpyObject(abs(self.value)) def to_long(self): return NumpyObject(int(self.value)) def to_float(self): return NumpyObject(float(self.value))"},{"question":"You have been provided with the `nntplib` module documentation, which describes how to interact with NNTP servers to read and post articles to newsgroups. Your task is to implement a function that connects to a specified NNTP server, retrieves the subjects of the last N articles from a given newsgroup, and returns them as a list. # Requirements: 1. **Function Signature**: ```python def fetch_recent_subjects(server: str, group: str, num_articles: int, port: int = 119) -> list: ``` 2. **Input**: - `server` (str): The hostname of the NNTP server. - `group` (str): The name of the newsgroup to fetch articles from. - `num_articles` (int): The number of recent articles to fetch. - `port` (int, optional): The port number to connect to on the NNTP server. Defaults to 119. 3. **Output**: - Returns a list of strings, where each string is the subject of an article. 4. **Behavior**: - Connect to the specified NNTP server. - Select the specified newsgroup. - Fetch the subjects of the last `num_articles` articles. - Return the subjects as a list of strings. 5. **Constraints**: - The function should handle server responses and any exceptions that might be raised, ensuring that the connection is properly closed in case of errors. # Example Usage: ```python subjects = fetch_recent_subjects(\'news.gmane.io\', \'gmane.comp.python.committers\', 5) ``` This should return a list of the subjects of the last 5 articles in the specified newsgroup. # Notes: - Use the `group` method to get information about the specified newsgroup. - Use the `over` method to fetch the overview of articles, which includes their subjects. - Ensure that you decode headers using the `decode_header` utility function. Good Luck!","solution":"import nntplib from email.header import decode_header def fetch_recent_subjects(server: str, group: str, num_articles: int, port: int = 119) -> list: Connects to a specified NNTP server, retrieves the subjects of the last N articles from a given newsgroup, and returns them as a list. Parameters: server (str): The hostname of the NNTP server. group (str): The name of the newsgroup to fetch articles from. num_articles (int): The number of recent articles to fetch. port (int): The port number to connect to on the NNTP server (default is 119). Returns: list: A list of strings where each string is the subject of an article. subjects = [] try: with nntplib.NNTP(server, port) as client: resp, count, first, last, name = client.group(group) start = max(int(last) - num_articles + 1, int(first)) resp, overviews = client.over((str(start) + \'-\' + last)) for id, over in overviews: subject = decode_header(over[\'subject\']) decoded_subject = \' \'.join([ str(part, encoding or \'utf-8\') if isinstance(part, bytes) else part for part, encoding in subject]) subjects.append(decoded_subject) except Exception as e: print(f\\"An error occurred: {e}\\") return subjects"},{"question":"# Supervised Learning with scikit-learn Problem Statement You are given a dataset containing information about various features of some objects along with their corresponding binary labels (0 or 1). Your task is to implement a supervised learning model using scikit-learn to predict the labels of new objects based on their features. Your implementation should include data preprocessing, model training, and evaluation. Input - A CSV file named `data.csv` with the following columns: - `feature_1`, `feature_2`, ..., `feature_n`: numerical features of the objects. - `label`: binary target labels (0 or 1). Output - A CSV file named `predictions.csv` containing the predicted labels for a new set of objects in the same format as `data.csv` but without the `label` column. Constraints - You must use at least two different algorithms from the scikit-learn supervised learning modules listed in the provided documentation. - Implement feature scaling as part of your preprocessing. - Use cross-validation to evaluate the performance of your models. - Choose the best performing model (based on cross-validation results) to make predictions on the new data. Performance Requirements - Aim for the highest possible accuracy in model predictions. - Explain your model selection and evaluation process in comments within your code. Skeleton Code You may start with the following skeleton code: ```python import pandas as pd from sklearn.model_selection import train_test_split, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier # You may import other necessary scikit-learn modules def load_data(file_path): data = pd.read_csv(file_path) return data def preprocess_data(data): # Extract features and labels features = data.drop(columns=[\'label\']) labels = data[\'label\'] # Convert to numpy arrays X = features.values y = labels.values # Feature Scaling scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled, y def train_and_evaluate(X, y): # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize models models = [LogisticRegression(), RandomForestClassifier()] best_score = 0 best_model = None # Evaluate models using cross-validation for model in models: scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\'accuracy\') avg_score = scores.mean() print(f\'Model: {model.__class__.__name__}, Accuracy: {avg_score}\') if avg_score > best_score: best_score = avg_score best_model = model # Train the best model on the entire training data best_model.fit(X_train, y_train) return best_model, best_score def make_predictions(model, new_data_file, output_file): new_data = pd.read_csv(new_data_file) X_new = new_data.values # Scale features of new data (use the same scaler as used for training data) scaler = StandardScaler() X_new_scaled = scaler.fit_transform(X_new) predictions = model.predict(X_new_scaled) # Save predictions to CSV pd.DataFrame(predictions, columns=[\'label\']).to_csv(output_file, index=False) # Main if __name__ == \\"__main__\\": train_data_file = \'data.csv\' new_data_file = \'new_data.csv\' output_file = \'predictions.csv\' data = load_data(train_data_file) X, y = preprocess_data(data) model, best_score = train_and_evaluate(X, y) make_predictions(model, new_data_file, output_file) print(f\'Best Model: {model.__class__.__name__}, Best Score: {best_score}\') ``` Submission Submit the completed Python script along with `predictions.csv`.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.svm import SVC def load_data(file_path): data = pd.read_csv(file_path) return data def preprocess_data(data): # Extract features and labels features = data.drop(columns=[\'label\']) labels = data[\'label\'] # Convert to numpy arrays X = features.values y = labels.values # Feature Scaling scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled, y def train_and_evaluate(X, y): # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize models models = [LogisticRegression(max_iter=1000), RandomForestClassifier(), SVC()] best_score = 0 best_model = None # Evaluate models using cross-validation for model in models: scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\'accuracy\') avg_score = scores.mean() print(f\'Model: {model.__class__.__name__}, Accuracy: {avg_score}\') if avg_score > best_score: best_score = avg_score best_model = model # Train the best model on the entire training data best_model.fit(X_train, y_train) return best_model, best_score def make_predictions(model, new_data_file, output_file): new_data = pd.read_csv(new_data_file) X_new = new_data.values # Scale features of new data (use the same scaler as used for training data) scaler = StandardScaler() X_new_scaled = scaler.fit_transform(X_new) predictions = model.predict(X_new_scaled) # Save predictions to CSV pd.DataFrame(predictions, columns=[\'label\']).to_csv(output_file, index=False) # Main if __name__ == \\"__main__\\": train_data_file = \'data.csv\' new_data_file = \'new_data.csv\' output_file = \'predictions.csv\' data = load_data(train_data_file) X, y = preprocess_data(data) model, best_score = train_and_evaluate(X, y) make_predictions(model, new_data_file, output_file) print(f\'Best Model: {model.__class__.__name__}, Best Score: {best_score}\')"},{"question":"# Memory Leak Detection using Tracemalloc You are tasked with identifying memory leaks in a Python application and providing detailed statistics on memory usage using the `tracemalloc` module. **Problem Statement:** Write a Python function `detect_memory_leak` which takes two functions as input parameters: `func1` and `func2`. These functions represent two different states of the application. The goal is to: 1. Start tracing memory allocations before executing `func1`. 2. Take a snapshot after executing `func1`. 3. Execute `func2`, which may introduce memory leaks. 4. Take another snapshot after executing `func2`. 5. Compare the two snapshots and display the top 5 memory differences to identify potential memory leaks. **Function Signature:** ```python def detect_memory_leak(func1: callable, func2: callable): pass ``` **Requirements:** 1. Use the `tracemalloc` module to start and stop memory tracing. 2. Capture memory snapshots using the `take_snapshot()` method. 3. Compare snapshots using the `compare_to` method. 4. Display the top 5 differences in memory allocation statistics (in terms of size and count). 5. Ensure that any temporary files or resources are cleaned up after the function executes. **Example:** ```python import tracemalloc import random def func1(): a = [1] * (10**6) # Allocate memory return a def func2(): b = [2] * (2 * 10**6) # Allocate more memory, simulate memory leak def detect_memory_leak(func1: callable, func2: callable): tracemalloc.start() func1() snapshot1 = tracemalloc.take_snapshot() func2() snapshot2 = tracemalloc.take_snapshot() top_stats = snapshot2.compare_to(snapshot1, \'lineno\') print(\\"[ Top 5 differences ]\\") for stat in top_stats[:5]: print(stat) tracemalloc.stop() # Example usage: detect_memory_leak(func1, func2) ``` **Constraints:** - Assume that `func1` and `func2` are both user-defined functions that perform memory-intensive operations. - Ensure that your solution handles any exceptions that may be raised during memory tracing or snapshot comparisons. # Evaluation: Your solution will be evaluated based on the following criteria: - Correct implementation of memory tracing using the `tracemalloc` module. - Accurate capturing and comparison of memory snapshots. - Correct identification and display of the top memory differences. - Code readability and proper handling of resources and exceptions.","solution":"import tracemalloc def detect_memory_leak(func1: callable, func2: callable): Detects memory leaks by comparing memory usage snapshots before and after executing two functions. Args: func1 (callable): The first function to execute. func2 (callable): The second function to execute, which may introduce memory leaks. tracemalloc.start() try: # Execute the first function and take a snapshot func1() snapshot1 = tracemalloc.take_snapshot() # Execute the second function and take another snapshot func2() snapshot2 = tracemalloc.take_snapshot() # Compare the two snapshots top_stats = snapshot2.compare_to(snapshot1, \'lineno\') # Display the top 5 differences in memory allocation print(\\"[ Top 5 differences ]\\") for stat in top_stats[:5]: print(stat) finally: tracemalloc.stop()"},{"question":"**Objective:** Your task is to demonstrate your understanding of loading, preprocessing, and performing initial analysis on real-world datasets using scikit-learn. For this exercise, you will work with the Olivetti Faces dataset. **Question:** 1. **Load the Olivetti Faces dataset using scikit-learn.** - **Function to use**: `fetch_olivetti_faces` - **Input**: None - **Output**: Loaded dataset which contains images of faces and their target labels. 2. **Implement a function `preprocess_data` to preprocess the dataset.** - **Input**: Loaded dataset object. - **Output**: Tuple containing: - X: numpy array of shape (400, 4096) representing the feature matrix. - y: numpy array of shape (400,) representing the target labels. 3. **Implement a function `visualize_samples` to visualize sample images from the dataset.** - **Input**: X (numpy array from the preprocessing step), y (numpy array from the preprocessing step), num_samples (number of samples to visualize). - **Output**: None (display the images inline, using matplotlib). 4. **Implement a function `perform_basic_analysis` to perform basic analysis of the dataset.** - **Input**: X (numpy array from the preprocessing step), y (numpy array from the preprocessing step). - **Output**: Dictionary containing: - \'mean_face\': numpy array of shape (4096,) representing the mean face. - \'std_face\': numpy array of shape (4096,) representing the standard deviation of the faces. - \'unique_classes\': number of unique classes in the target. ```python from sklearn.datasets import fetch_olivetti_faces import matplotlib.pyplot as plt import numpy as np def fetch_dataset(): Fetches the Olivetti Faces dataset. Returns: dataset: Bunch A dictionary-like object containing images and target labels. dataset = fetch_olivetti_faces() return dataset def preprocess_data(dataset): Preprocess the dataset to extract features and target labels. Args: dataset: Bunch The loaded Olivetti Faces dataset. Returns: X: numpy array of shape (400, 4096) Feature matrix. y: numpy array of shape (400,) Target labels. X = dataset.data y = dataset.target return X, y def visualize_samples(X, y, num_samples=10): Visualize sample images from the dataset. Args: X: numpy array of shape (400, 4096) Feature matrix. y: numpy array of shape (400,) Target labels. num_samples: int Number of samples to visualize. plt.figure(figsize=(10, 10)) for i in range(num_samples): plt.subplot(1, num_samples, i+1) plt.imshow(X[i].reshape(64, 64), cmap=\'gray\') plt.title(f\\"Label: {y[i]}\\") plt.axis(\'off\') plt.show() def perform_basic_analysis(X, y): Perform basic analysis of the dataset. Args: X: numpy array of shape (400, 4096) Feature matrix. y: numpy array of shape (400,) Target labels. Returns: analysis: dict Dictionary containing mean face, standard deviation of faces, and number of unique classes. mean_face = np.mean(X, axis=0) std_face = np.std(X, axis=0) unique_classes = len(np.unique(y)) return { \'mean_face\': mean_face, \'std_face\': std_face, \'unique_classes\': unique_classes } # Example usage: dataset = fetch_dataset() X, y = preprocess_data(dataset) visualize_samples(X, y, num_samples=10) analysis = perform_basic_analysis(X, y) print(analysis) ``` **Constraints:** - You can use only scikit-learn, numpy, and matplotlib libraries. - The functions should handle any errors gracefully, with appropriate error messages. **Performance Requirements:** - The code should run efficiently on a typical machine.","solution":"from sklearn.datasets import fetch_olivetti_faces import matplotlib.pyplot as plt import numpy as np def fetch_dataset(): Fetches the Olivetti Faces dataset. Returns: dataset: Bunch A dictionary-like object containing images and target labels. dataset = fetch_olivetti_faces() return dataset def preprocess_data(dataset): Preprocess the dataset to extract features and target labels. Args: dataset: Bunch The loaded Olivetti Faces dataset. Returns: X: numpy array of shape (400, 4096) Feature matrix. y: numpy array of shape (400,) Target labels. X = dataset.data y = dataset.target return X, y def visualize_samples(X, y, num_samples=10): Visualize sample images from the dataset. Args: X: numpy array of shape (400, 4096) Feature matrix. y: numpy array of shape (400,) Target labels. num_samples: int Number of samples to visualize. plt.figure(figsize=(10, 10)) for i in range(num_samples): plt.subplot(1, num_samples, i+1) plt.imshow(X[i].reshape(64, 64), cmap=\'gray\') plt.title(f\\"Label: {y[i]}\\") plt.axis(\'off\') plt.show() def perform_basic_analysis(X, y): Perform basic analysis of the dataset. Args: X: numpy array of shape (400, 4096) Feature matrix. y: numpy array of shape (400,) Target labels. Returns: analysis: dict Dictionary containing mean face, standard deviation of faces, and number of unique classes. mean_face = np.mean(X, axis=0) std_face = np.std(X, axis=0) unique_classes = len(np.unique(y)) return { \'mean_face\': mean_face, \'std_face\': std_face, \'unique_classes\': unique_classes }"},{"question":"Objective Design a function to generate secure, customizable passwords using Python\'s `secrets` module. Your function should allow flexibility in specifying the length and complexity requirements of the password. Problem Statement Create a Python function `generate_secure_password(length, require_uppercase, require_lowercase, require_digits, require_special_chars)` that returns a securely generated password meeting the specified criteria. * `length` (int): The length of the desired password (must be at least 8). * `require_uppercase` (bool): Whether the password must contain at least one uppercase letter. * `require_lowercase` (bool): Whether the password must contain at least one lowercase letter. * `require_digits` (bool): Whether the password must contain at least one digit. * `require_special_chars` (bool): Whether the password must contain at least one special character (from `string.punctuation`). If `require_uppercase`, `require_lowercase`, `require_digits`, or `require_special_chars` are set to `True`, the password must contain at least one character from the respective category. The function should raise a `ValueError` if: * The `length` is less than 8. * The specified requirements cannot be met within the given length. Input Format * An integer `length` representing the desired password length. * Boolean values `require_uppercase`, `require_lowercase`, `require_digits`, and `require_special_chars`. Output Format * A string representing the generated password. Example ```python >>> generate_secure_password(12, True, True, True, True) \'H7glmRn8L0K\' >>> generate_secure_password(10, True, False, True, True) \'5J&o2L1p!t\' ``` Constraints * The `length` must be an integer of at least 8. * At least one of the requirements must be set to `True`. Notes * Leverage the `secrets` module and `string` module for generating the password. * Ensure the password meets all specified criteria. Performance Requirements * The function should efficiently generate the password in O(length) time complexity where `length` is the length of the password. # Solution Template ```python import secrets import string def generate_secure_password(length, require_uppercase, require_lowercase, require_digits, require_special_chars): if length < 8: raise ValueError(\\"Password length must be at least 8\\") alphabet = \'\' if require_uppercase: alphabet += string.ascii_uppercase if require_lowercase: alphabet += string.ascii_lowercase if require_digits: alphabet += string.digits if require_special_chars: alphabet += string.punctuation if not alphabet or length < len([require_uppercase, require_lowercase, require_digits, require_special_chars].count(True)): raise ValueError(\\"Cannot meet the requirements with the given length\\") # Ensuring requirements are met password = [] if require_uppercase: password.append(secrets.choice(string.ascii_uppercase)) if require_lowercase: password.append(secrets.choice(string.ascii_lowercase)) if require_digits: password.append(secrets.choice(string.digits)) if require_special_chars: password.append(secrets.choice(string.punctuation)) # Fill the rest of the password length with random choices password.extend(secrets.choice(alphabet) for _ in range(length - len(password))) # Shuffle the resulting password list to ensure randomness secrets.SystemRandom().shuffle(password) return \'\'.join(password) # Example usage: # print(generate_secure_password(12, True, True, True, True)) # print(generate_secure_password(10, True, False, True, True)) ```","solution":"import secrets import string def generate_secure_password(length, require_uppercase, require_lowercase, require_digits, require_special_chars): if length < 8: raise ValueError(\\"Password length must be at least 8\\") alphabet = \'\' if require_uppercase: alphabet += string.ascii_uppercase if require_lowercase: alphabet += string.ascii_lowercase if require_digits: alphabet += string.digits if require_special_chars: alphabet += string.punctuation required_count = sum([require_uppercase, require_lowercase, require_digits, require_special_chars]) if not alphabet or length < required_count: raise ValueError(\\"Cannot meet the requirements with the given length\\") # Ensuring requirements are met password = [] if require_uppercase: password.append(secrets.choice(string.ascii_uppercase)) if require_lowercase: password.append(secrets.choice(string.ascii_lowercase)) if require_digits: password.append(secrets.choice(string.digits)) if require_special_chars: password.append(secrets.choice(string.punctuation)) # Fill the rest of the password length with random choices password.extend(secrets.choice(alphabet) for _ in range(length - len(password))) # Shuffle the resulting password list to ensure randomness secrets.SystemRandom().shuffle(password) return \'\'.join(password)"},{"question":"Coding Assessment Question # Objective Write a Python function to perform an asynchronous file read operation with robust exception handling using asyncio exceptions. # Background You need to create a function that reads a certain number of bytes from a file asynchronously. During the read operation, various exceptions can occur, and you need to handle these exceptions appropriately using the given exceptions from the asyncio module. # Requirements 1. **Function Signature**: ```python import asyncio from typing import Tuple, Optional async def read_file_async(file_path: str, num_bytes: int) -> Tuple[Optional[bytes], Optional[str]]: Asynchronously reads the specified number of bytes from a file. Args: file_path (str): The path to the file to read. num_bytes (int): The number of bytes to read. Returns: Tuple[Optional[bytes], Optional[str]]: A tuple with the read bytes and an optional error message. // Your code here ``` 2. **Function Explanation**: The `read_file_async` function tries to read the specified `num_bytes` from the file located at `file_path`. The function must handle various asyncio exceptions to ensure robust error handling. 3. **Exception Handling**: - Catch `asyncio.TimeoutError` and return (None, \\"TimeoutError: Operation has exceeded the given deadline\\"). - Catch `asyncio.CancelledError` and return (None, \\"CancelledError: Operation has been cancelled\\"). Make sure to re-raise it after custom handling. - Catch `asyncio.InvalidStateError` and return (None, \\"InvalidStateError: Invalid internal state of Task or Future\\"). - Catch `asyncio.SendfileNotAvailableError` and return (None, \\"SendfileNotAvailableError: sendfile syscall not available for the given socket or file type\\"). - Catch `asyncio.IncompleteReadError` and return (None, \\"IncompleteReadError: The read operation did not complete fully\\"). - Catch `asyncio.LimitOverrunError` and return (None, \\"LimitOverrunError: Reached the buffer size limit while looking for a separator\\"). 4. **Expected Input and Output**: - Input: `file_path` (str) - the path of the file, `num_bytes` (int) - the number of bytes to read. - Output: A tuple of two elements: - The bytes read (or None if an error occurs). - An optional error message string (or None if no error occurs). # Constraints 1. The function should primarily focus on simulating the read operation and exception handling. 2. Use asyncio\'s stream APIs for the read operation. 3. Ensure that the function works efficiently and correctly under the specified constraints. # Example ```python result, error = await read_file_async(\'dummy.txt\', 100) print(result, error) # (b\'some bytes data\', None) or (None, \\"SpecificException: Error message\\") ``` # Notes 1. The above example assumes that `dummy.txt` contains sufficient data. 2. If an exception occurs during reading, the second tuple element should provide the relevant error message. You may simulate file reading by creating a temporary file or any asynchronous stream that mimics a file read operation to test your implementation properly.","solution":"import asyncio from typing import Tuple, Optional async def read_file_async(file_path: str, num_bytes: int) -> Tuple[Optional[bytes], Optional[str]]: try: # Open file asynchronously with open(file_path, \'rb\') as file: content = await asyncio.to_thread(file.read, num_bytes) return (content, None) except asyncio.TimeoutError: return (None, \\"TimeoutError: Operation has exceeded the given deadline\\") except asyncio.CancelledError: return (None, \\"CancelledError: Operation has been cancelled\\") raise except asyncio.InvalidStateError: return (None, \\"InvalidStateError: Invalid internal state of Task or Future\\") except asyncio.SendfileNotAvailableError: return (None, \\"SendfileNotAvailableError: sendfile syscall not available for the given socket or file type\\") except asyncio.IncompleteReadError: return (None, \\"IncompleteReadError: The read operation did not complete fully\\") except asyncio.LimitOverrunError: return (None, \\"LimitOverrunError: Reached the buffer size limit while looking for a separator\\") except Exception as e: return (None, str(e))"},{"question":"# Python Sys Module Assessment **Objective:** Implement a function that utilizes various features of the `sys` module to collect and output detailed system and environment information. **Task:** Write a function `collect_sys_info()` that gathers the following system information using the `sys` module: 1. Python executable path. 2. Command line arguments passed to the script. 3. Python version in a friendly format. 4. Maximum recursion limit. 5. List of all loaded modules. 6. Total number of allocated memory blocks. 7. Default string encoding used by the Unicode implementation. 8. Details of any active asynchronous generator hooks. 9. Current thread’s top-most frame. 10. The platform identifier string. The function should return a dictionary where keys are the description of the required information and values are the corresponding collected data. ```python import sys def collect_sys_info(): This function collects various system and environment information using the features of the sys module and returns it in a dictionary. Returns: dict: A dictionary containing system information collected using the sys module. sys_info = {} # 1. Python executable path sys_info[\'python_executable\'] = sys.executable # 2. Command line arguments passed to the script sys_info[\'command_line_arguments\'] = sys.argv # 3. Python version in a friendly format (major.minor.micro) sys_info[\'python_version\'] = \\".\\".join(map(str, sys.version_info[:3])) # 4. Maximum recursion limit sys_info[\'max_recursion_limit\'] = sys.getrecursionlimit() # 5. List of all loaded modules sys_info[\'loaded_modules\'] = list(sys.modules.keys()) # 6. Total number of allocated memory blocks sys_info[\'allocated_memory_blocks\'] = sys.getallocatedblocks() # 7. Default string encoding used by the Unicode implementation sys_info[\'default_string_encoding\'] = sys.getdefaultencoding() # 8. Details of any active asynchronous generator hooks async_hooks = sys.get_asyncgen_hooks() sys_info[\'asyncgen_hooks\'] = { \'firstiter\': async_hooks.firstiter.__name__ if async_hooks.firstiter else None, \'finalizer\': async_hooks.finalizer.__name__ if async_hooks.finalizer else None } # 9. Current thread’s top-most frame try: frame = sys._getframe() sys_info[\'current_top_frame\'] = { \'filename\': frame.f_code.co_filename, \'line_number\': frame.f_lineno, \'function_name\': frame.f_code.co_name } except ValueError: sys_info[\'current_top_frame\'] = None # 10. The platform identifier string sys_info[\'platform_identifier\'] = sys.platform return sys_info # Example usage if __name__ == \\"__main__\\": info = collect_sys_info() for k, v in info.items(): print(f\\"{k}: {v}\\") ``` **Explanation:** - Ensure the function handles exceptions where applicable, such as for frame access if no stack frame is available. - Flatten complex data structures into dictionary format for clarity. - Aim for readability and maintainability. **Constraints:** - Python version should be 3.8 or higher to ensure all used `sys` features are present. - Assume the script is executed in a standalone context where `sys.argv` includes the script name. **Performance Requirements:** - The function should execute efficiently and handle typical system environments without causing significant performance overhead.","solution":"import sys def collect_sys_info(): This function collects various system and environment information using the features of the sys module and returns it in a dictionary. Returns: dict: A dictionary containing system information collected using the sys module. sys_info = {} # 1. Python executable path sys_info[\'python_executable\'] = sys.executable # 2. Command line arguments passed to the script sys_info[\'command_line_arguments\'] = sys.argv # 3. Python version in a friendly format (major.minor.micro) sys_info[\'python_version\'] = \\".\\".join(map(str, sys.version_info[:3])) # 4. Maximum recursion limit sys_info[\'max_recursion_limit\'] = sys.getrecursionlimit() # 5. List of all loaded modules sys_info[\'loaded_modules\'] = list(sys.modules.keys()) # 6. Total number of allocated memory blocks sys_info[\'allocated_memory_blocks\'] = sys.getallocatedblocks() # 7. Default string encoding used by the Unicode implementation sys_info[\'default_string_encoding\'] = sys.getdefaultencoding() # 8. Details of any active asynchronous generator hooks async_hooks = sys.get_asyncgen_hooks() sys_info[\'asyncgen_hooks\'] = { \'firstiter\': async_hooks.firstiter.__name__ if async_hooks.firstiter else None, \'finalizer\': async_hooks.finalizer.__name__ if async_hooks.finalizer else None } # 9. Current thread’s top-most frame try: frame = sys._getframe() sys_info[\'current_top_frame\'] = { \'filename\': frame.f_code.co_filename, \'line_number\': frame.f_lineno, \'function_name\': frame.f_code.co_name } except ValueError: sys_info[\'current_top_frame\'] = None # 10. The platform identifier string sys_info[\'platform_identifier\'] = sys.platform return sys_info"},{"question":"**Objective:** To assess your understanding of managing HTTP connections, sending requests, handling responses, and managing exceptions using the `http.client` module in Python. **Problem Statement:** You are required to implement a function `make_post_request` that performs an HTTP POST request to a given URL with specified data and headers. The function should manage potential exceptions and ensure resource clean-up. Additionally, your function should handle common HTTP response scenarios appropriately. **Function Signature:** ```python def make_post_request(url: str, data: dict, headers: dict) -> tuple: pass ``` **Input:** - `url` (str): The URL to which the POST request should be made. - `data` (dict): A dictionary of key-value pairs to be included in the POST request body. - `headers` (dict): A dictionary of HTTP headers to include in the request. **Output:** - A tuple containing: - HTTP status code (int) - HTTP reason phrase (str) - Response body (bytes) **Constraints:** - Use the `http.client` module to perform the HTTP operations. - Properly handle common exceptions such as `http.client.InvalidURL`, `http.client.HTTPException`, etc. - Ensure that the connection is properly closed after the request, regardless of whether an exception occurs. - The `data` dictionary should be URL-encoded before being sent in the request body. **Example Usage:** ```python url = \\"http://example.com/api\\" data = {\\"name\\": \\"John\\", \\"age\\": 30} headers = {\\"Content-Type\\": \\"application/x-www-form-urlencoded\\"} status_code, reason, response = make_post_request(url, data, headers) print(status_code) # For instance, 200 print(reason) # For instance, \\"OK\\" print(response) # The response body in bytes ``` **Notes:** - Use `urllib.parse.urlencode` to URL-encode the `data` dictionary. - Ensure the `Content-Length` and `Content-Type` headers are appropriately set according to the body content. - You may assume the `url` will always have an HTTP scheme.","solution":"import http.client import urllib.parse def make_post_request(url: str, data: dict, headers: dict) -> tuple: Makes an HTTP POST request to the specified URL with the provided data and headers. Args: - url (str): The URL to which the POST request should be made. - data (dict): A dictionary of key-value pairs to be included in the POST request body. - headers (dict): A dictionary of HTTP headers to include in the request. Returns: - A tuple containing the HTTP status code (int), HTTP reason phrase (str), and response body (bytes). parsed_url = urllib.parse.urlparse(url) host = parsed_url.netloc path = parsed_url.path if not path: path = \\"/\\" encoded_data = urllib.parse.urlencode(data) headers[\'Content-Length\'] = str(len(encoded_data)) if \'Content-Type\' not in headers: headers[\'Content-Type\'] = \'application/x-www-form-urlencoded\' try: connection = http.client.HTTPConnection(host) connection.request(\\"POST\\", path, body=encoded_data, headers=headers) response = connection.getresponse() status_code = response.status reason = response.reason response_body = response.read() except http.client.InvalidURL: return (400, \\"Invalid URL\\", b\\"\\") except http.client.HTTPException as e: return (500, str(e), b\\"\\") except Exception as e: return (500, str(e), b\\"\\") finally: if \'connection\' in locals(): connection.close() return (status_code, reason, response_body)"},{"question":"**Question:** Implement a Python module named `calculator.py` that contains the following functions: 1. `add(a, b)`: Returns the sum of `a` and `b`. 2. `subtract(a, b)`: Returns the difference when `b` is subtracted from `a`. 3. `multiply(a, b)`: Returns the product of `a` and `b`. 4. `divide(a, b)`: Returns the quotient when `a` is divided by `b`. Raises a `ValueError` if `b` is zero. For each function, write comprehensive docstrings with examples illustrating how to use the function. Use the `doctest` module to ensure that all examples in the docstrings are tested and verified. **Requirements:** - Your module should include the above four functions with proper docstring examples. - The docstrings should include at least three test cases for each function, including edge cases. - At the end of the module, include a `main` section to execute `doctest`\'s `testmod()` function to run all the tests. **Example:** ```python calculator.py This module provides basic arithmetic functions: add, subtract, multiply, and divide. Examples: >>> add(2, 3) 5 >>> subtract(5, 2) 3 >>> multiply(5, 4) 20 >>> divide(10, 2) 5.0 >>> divide(5, 0) Traceback (most recent call last): ... ValueError: Cannot divide by zero def add(a, b): Return the sum of `a` and `b`. >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(0, 0) 0 return a + b def subtract(a, b): Return the difference when `b` is subtracted from `a`. >>> subtract(5, 2) 3 >>> subtract(0, 0) 0 >>> subtract(-1, 1) -2 return a - b def multiply(a, b): Return the product of `a` and `b`. >>> multiply(5, 4) 20 >>> multiply(0, 10) 0 >>> multiply(-1, -1) 1 return a * b def divide(a, b): Return the quotient when `a` is divided by `b`. Raise `ValueError` if `b` is zero. >>> divide(10, 2) 5.0 >>> divide(3, 3) 1.0 >>> divide(5, 0) Traceback (most recent call last): ... ValueError: Cannot divide by zero if b == 0: raise ValueError(\\"Cannot divide by zero\\") return a / b if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` **Constraints:** - Each function only handles numerical inputs. - No need for input validation apart from the `divide` function which should handle division by zero. - Ensure all examples in docstrings are correctly formatted for `doctest`. Your task is to implement `calculator.py` accordingly, ensuring all `doctest` examples pass successfully when the module is run.","solution":"def add(a, b): Return the sum of `a` and `b`. >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(0, 0) 0 >>> add(1.5, 2.5) 4.0 return a + b def subtract(a, b): Return the difference when `b` is subtracted from `a`. >>> subtract(5, 2) 3 >>> subtract(0, 0) 0 >>> subtract(-1, 1) -2 >>> subtract(7.5, 2.5) 5.0 return a - b def multiply(a, b): Return the product of `a` and `b`. >>> multiply(5, 4) 20 >>> multiply(0, 10) 0 >>> multiply(-1, -1) 1 >>> multiply(1.5, 2) 3.0 return a * b def divide(a, b): Return the quotient when `a` is divided by `b`. Raise `ValueError` if `b` is zero. >>> divide(10, 2) 5.0 >>> divide(3, 3) 1.0 >>> divide(5, 0) Traceback (most recent call last): ... ValueError: Cannot divide by zero >>> divide(7.5, 2.5) 3.0 if b == 0: raise ValueError(\\"Cannot divide by zero\\") return a / b if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"# Cross-Validation and Model Evaluation with Scikit-learn **Objective**: Implement a machine learning workflow utilizing scikit-learn\'s cross-validation capabilities to evaluate model performance on a given dataset. **Problem Statement**: You are provided with a dataset representing medical patient data. Each row represents a patient, and each column represents a feature, with the last column being the target variable indicating whether the patient has been diagnosed with a particular disease (1: diagnosed, 0: not diagnosed). Your task is to: 1. Preprocess the data: Handle any missing values and normalize the feature data. 2. Implement and train a model using a linear Support Vector Classifier (SVC). 3. Evaluate the model using k-fold cross-validation and obtain the accuracy and F1-score for each fold. 4. Repeat the cross-validation process using stratified k-fold cross-validation. 5. Interpret and compare the cross-validation results from both k-fold and stratified k-fold techniques. **Input**: - A CSV file `medical_data.csv` containing the dataset with the following columns: - `feature1`, `feature2`, ..., `featureN` (numerical features). - `target` (binary target variable: 1 or 0). **Expected Output**: - Print the accuracy and F1-score for each fold for both k-fold and stratified k-fold cross-validation. - Provide a brief interpretation (3-4 sentences) of the results comparing the two cross-validation strategies. **Constraints**: - Use `scikit-learn` for all model training and evaluation tasks. - Assume there are no categorical features and all features are numerical. - Use a random state of 42 for reproducibility. **Performance Requirements**: The code should be efficient in terms of execution, especially while handling cross-validation splits. **Example**: ```python import pandas as pd from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import make_scorer, f1_score # Load data data = pd.read_csv(\'medical_data.csv\') X = data.drop(columns=\'target\') y = data[\'target\'] # Preprocessing scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Model definition svc = SVC(kernel=\'linear\', random_state=42) # K-Fold Cross-Validation kf = KFold(n_splits=5, random_state=42, shuffle=True) accuracy_scores = cross_val_score(svc, X_scaled, y, cv=kf, scoring=\'accuracy\') f1_scores = cross_val_score(svc, X_scaled, y, cv=kf, scoring=make_scorer(f1_score)) print(\\"K-Fold CV Accuracy: \\", accuracy_scores) print(\\"K-Fold CV F1 Scores: \\", f1_scores) # Stratified K-Fold Cross-Validation skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) accuracy_scores_stratified = cross_val_score(svc, X_scaled, y, cv=skf, scoring=\'accuracy\') f1_scores_stratified = cross_val_score(svc, X_scaled, y, cv=skf, scoring=make_scorer(f1_score)) print(\\"Stratified K-Fold CV Accuracy: \\", accuracy_scores_stratified) print(\\"Stratified K-Fold CV F1 Scores: \\", f1_scores_stratified) # Interpretation ``` In the interpretation section, you will provide a brief comparison of the results obtained from the k-fold and stratified k-fold cross-validation indicating which method appears more suitable for this dataset.","solution":"import pandas as pd from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import make_scorer, f1_score def load_and_preprocess_data(file_path): data = pd.read_csv(file_path) X = data.drop(columns=\'target\') y = data[\'target\'] scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled, y def evaluate_with_kfold(X, y, model, n_splits=5, random_state=42): kf = KFold(n_splits=n_splits, random_state=random_state, shuffle=True) accuracy_scores = cross_val_score(model, X, y, cv=kf, scoring=\'accuracy\') f1_scores = cross_val_score(model, X, y, cv=kf, scoring=make_scorer(f1_score)) return accuracy_scores, f1_scores def evaluate_with_stratified_kfold(X, y, model, n_splits=5, random_state=42): skf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True) accuracy_scores = cross_val_score(model, X, y, cv=skf, scoring=\'accuracy\') f1_scores = cross_val_score(model, X, y, cv=skf, scoring=make_scorer(f1_score)) return accuracy_scores, f1_scores def main(): file_path = \'medical_data.csv\' X, y = load_and_preprocess_data(file_path) model = SVC(kernel=\'linear\', random_state=42) kfold_accuracy, kfold_f1 = evaluate_with_kfold(X, y, model) stratified_kfold_accuracy, stratified_kfold_f1 = evaluate_with_stratified_kfold(X, y, model) print(\\"K-Fold CV Accuracy: \\", kfold_accuracy) print(\\"K-Fold CV F1 Scores: \\", kfold_f1) print(\\"Stratified K-Fold CV Accuracy: \\", stratified_kfold_accuracy) print(\\"Stratified K-Fold CV F1 Scores: \\", stratified_kfold_f1) # Interpretation print(\\"nInterpretation:\\") print(\\"K-Fold accuracy scores show the model\'s performance using simple split of data into k groups.\\") print(\\"Stratified K-Fold is designed to ensure that each fold has the same proportion of classes. If dataset is imbalanced, stratified K-Fold is usually better as it provides more consistent model performance across folds.\\") if __name__ == \'__main__\': main()"},{"question":"# Question: Implementing a Custom SAX Parser with Error Handling Using the `xml.sax` package, create a SAX parser that reads an XML string and processes specific tags to extract and print certain information. Additionally, implement custom error handling to manage any parsing errors that occur. Requirements: 1. **Input**: - An XML string that contains multiple nested elements. - The elements of interest are `<title>`, `<author>`, and `<year>` nested within a `<book>` element. 2. **Output**: - Print the content of the `<title>`, `<author>`, and `<year>` tags for each `<book>` element found. - If there is a parsing error, print a custom error message with details about the error. 3. **Implementation**: - Create custom handler classes that extend the appropriate SAX handler interfaces to handle the start and end of elements. - Use the `xml.sax` methods (`parseString`) to parse the XML string and trigger your custom handlers. - Implement custom error handling to catch and manage `SAXParseException`. 4. **Constraints**: - You must handle the `SAXParseException` to catch any parsing errors and print a relevant message, including line number and column where the error occurred. - Only the specified tags need to be processed; other tags can be ignored. Example: ```python import xml.sax class BookHandler(xml.sax.ContentHandler): ... # Implement methods to handle the start and end of elements ... class CustomErrorHandler(xml.sax.ErrorHandler): ... # Implement methods to handle errors ... def parse_books(xml_string): # Create a SAX parser ... # Create and assign custom handlers ... # Parse the XML string & handle errors ... return # Test the function xml_data = <?xml version=\\"1.0\\"?> <library> <book> <title>Book One</title> <author>Author A</author> <year>2001</year> </book> <book> <title>Book Two</title> <author>Author B</author> <year>2002</year> </book> <incorrectTag> <title>Invalid Data</title> </incorrectTag> </library> parse_books(xml_data) ``` Expected output: ``` Title: Book One Author: Author A Year: 2001 Title: Book Two Author: Author B Year: 2002 Error at line X, column Y: <specific error message> ``` Write the code to implement the `BookHandler`, `CustomErrorHandler`, and the function `parse_books`.","solution":"import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.inside_book = False def startElement(self, tag, attributes): self.current_data = tag if tag == \\"book\\": self.inside_book = True def endElement(self, tag): if self.inside_book and tag == \\"book\\": print(f\\"Title: {self.title}\\") print(f\\"Author: {self.author}\\") print(f\\"Year: {self.year}n\\") self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.inside_book = False self.current_data = \\"\\" def characters(self, content): if self.inside_book: if self.current_data == \\"title\\": self.title += content.strip() elif self.current_data == \\"author\\": self.author += content.strip() elif self.current_data == \\"year\\": self.year += content.strip() class CustomErrorHandler(xml.sax.ErrorHandler): def error(self, exception): print(f\\"Error at line {exception._linenum}, column {exception._colnum}: {exception.getMessage()}\\") def fatalError(self, exception): print(f\\"Fatal Error at line {exception._linenum}, column {exception._colnum}: {exception.getMessage()}\\") def warning(self, exception): print(f\\"Warning at line {exception._linenum}, column {exception._colnum}: {exception.getMessage()}\\") def parse_books(xml_string): # Create a SAX parser parser = xml.sax.make_parser() # Override the default ContextHandler handler = BookHandler() error_handler = CustomErrorHandler() parser.setContentHandler(handler) parser.setErrorHandler(error_handler) # Parse the XML string & handle errors try: xml.sax.parseString(xml_string, handler) except xml.sax.SAXParseException as e: error_handler.fatalError(e)"},{"question":"Objective Create a logging configuration using the `logging.config.dictConfig` function and validate it programmatically. Your task is to write a function that sets up the logging configuration as per the given specification and ensures that it works correctly. Specification You are given a dictionary representing the logging configuration. Your task is to: 1. Write a function `setup_logging(config: dict) -> None` that: - Takes a dictionary `config` as input. - Configures the logging system using `logging.config.dictConfig(config)`. - Handles errors gracefully and logs an appropriate error message if the configuration is invalid. 2. Write a function `validate_logging() -> dict` that: - Returns a sample valid logging configuration dictionary following these requirements: - Two formatters: \\"default\\" and \\"detailed\\". - \\"default\\" should log only the message. - \\"detailed\\" should log the timestamp, log level, logger name, and the message. - Two handlers: \\"console\\" and \\"file\\". - \\"console\\" should use the \\"default\\" formatter and log INFO level and above to the console. - \\"file\\" should use the \\"detailed\\" formatter and log WARNING level and above to a file named `app.log`. - The root logger should have the \\"console\\" handler and log INFO level and above. - A specific logger named \\"myapp\\" should have both \\"console\\" and \\"file\\" handlers and log DEBUG level and above. 3. Write a test function `test_logging_setup()` which: - Calls `validate_logging()` to get the sample configuration dictionary. - Passes this configuration dictionary to `setup_logging()`. - Logs messages at various levels (DEBUG, INFO, WARNING, ERROR, CRITICAL) to verify that the configuration works as expected. - Asserts that the messages are correctly routed to the appropriate handlers with the proper formatting. Constraints - Your solution should handle any errors encountered during configuration and log appropriate error messages without stopping the program. - The configuration dictionary should strictly follow the schema outlined in the `logging.config` documentation. Example Here\'s an example of how the configuration dictionary might look: ```python def validate_logging() -> dict: return { \'version\': 1, \'formatters\': { \'default\': { \'format\': \'%(message)s\' }, \'detailed\': { \'format\': \'%(asctime)s %(levelname)-8s %(name)-15s %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'default\', \'level\': \'INFO\' }, \'file\': { \'class\': \'logging.FileHandler\', \'formatter\': \'detailed\', \'level\': \'WARNING\', \'filename\': \'app.log\' } }, \'root\': { \'level\': \'INFO\', \'handlers\': [\'console\'] }, \'loggers\': { \'myapp\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': False } } } ``` Use the above dictionary as a reference to develop your functions. Submission Submit your `setup_logging(config: dict) -> None`, `validate_logging() -> dict`, and `test_logging_setup()` functions as a single Python script.","solution":"import logging import logging.config def setup_logging(config: dict) -> None: Configures the logging system using the given configuration dictionary. Args: config (dict): Logging configuration dictionary. try: logging.config.dictConfig(config) except Exception as e: logging.error(\\"Error setting up logging configuration: %s\\", e) def validate_logging() -> dict: Returns a sample valid logging configuration dictionary. Returns: dict: Sample logging configuration dictionary. return { \'version\': 1, \'formatters\': { \'default\': { \'format\': \'%(message)s\' }, \'detailed\': { \'format\': \'%(asctime)s %(levelname)-8s %(name)-15s %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'default\', \'level\': \'INFO\' }, \'file\': { \'class\': \'logging.FileHandler\', \'formatter\': \'detailed\', \'level\': \'WARNING\', \'filename\': \'app.log\' } }, \'root\': { \'level\': \'INFO\', \'handlers\': [\'console\'] }, \'loggers\': { \'myapp\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], \'propagate\': False } } }"},{"question":"# Complex Distribution Function Implementation **Objective**: Write a function that utilizes PyTorch\'s probability distributions to generate samples from a custom mixture distribution. # Problem Statement You are required to implement a function `custom_mixture_distribution` that creates a sample from a custom mixture distribution composed of multiple specified distributions. The mixture distribution should be a combination of a Normal distribution, a Beta distribution, and a Poisson distribution. # Function Signature ```python def custom_mixture_distribution(num_samples: int, weights: list, params: dict) -> torch.Tensor: pass ``` # Parameters - `num_samples` (int): Number of samples to generate from the mixture distribution. - `weights` (list of float): A list of three weights that determine the proportion of each distribution in the mixture. The weights should sum up to 1. - `params` (dict): A dictionary containing the parameters for each of the three distributions. The dictionary structure is as follows: ```python params = { \'normal\': {\'mean\': float, \'std\': float}, \'beta\': {\'alpha\': float, \'beta\': float}, \'poisson\': {\'rate\': float} } ``` # Requirements 1. **Input constraints**: - `num_samples` should be a positive integer. - `weights` should be a list of three floats that sum up to 1. - `params` should be a dictionary with the structure mentioned above. 2. **Output**: - Return a tensor of shape `(num_samples,)` containing the generated samples. # Implementation Steps 1. Initialize the specified distributions using their respective parameters. 2. Generate `num_samples` from each distribution. 3. Combine the samples according to the provided weights to create the final mixture distribution. 4. Return the tensor of combined samples. # Example ```python import torch def custom_mixture_distribution(num_samples, weights, params): assert len(weights) == 3, \\"There must be three weights.\\" assert abs(sum(weights) - 1.0) < 1e-5, \\"The weights must sum to 1.\\" # Creating the distributions normal_dist = torch.distributions.Normal(params[\'normal\'][\'mean\'], params[\'normal\'][\'std\']) beta_dist = torch.distributions.Beta(params[\'beta\'][\'alpha\'], params[\'beta\'][\'beta\']) poisson_dist = torch.distributions.Poisson(params[\'poisson\'][\'rate\']) # Sampling from each distribution normal_samples = normal_dist.sample((int(num_samples * weights[0]),)) beta_samples = beta_dist.sample((int(num_samples * weights[1]),)) poisson_samples = poisson_dist.sample((int(num_samples * weights[2]),)) # Combine the samples samples = torch.cat([normal_samples, beta_samples, poisson_samples]) # If there are any leftover samples due to rounding issues, sample from normal distribution if len(samples) < num_samples: extra_samples = normal_dist.sample((num_samples - len(samples),)) samples = torch.cat([samples, extra_samples]) # Shuffle the samples to mix them well samples = samples[torch.randperm(len(samples))] return samples # Example usage: params = { \'normal\': {\'mean\': 0.0, \'std\': 1.0}, \'beta\': {\'alpha\': 2.0, \'beta\': 5.0}, \'poisson\': {\'rate\': 3.0} } weights = [0.4, 0.3, 0.3] num_samples = 10000 samples = custom_mixture_distribution(num_samples, weights, params) print(samples.shape) # Should print: torch.Size([10000]) print(samples[:10]) # Print the first 10 samples for inspection ``` **Your task is to implement the function as described.** # Constraints - Use only the PyTorch library for your implementation. - Consider edge cases such as when the weights do not sum up to 1 (raise an appropriate error).","solution":"import torch def custom_mixture_distribution(num_samples: int, weights: list, params: dict) -> torch.Tensor: Generates samples from a custom mixture distribution composed of a Normal distribution, a Beta distribution, and a Poisson distribution. :param num_samples: Number of samples to generate from the mixture distribution. :param weights: A list of three weights that determine the proportion of each distribution in the mixture. :param params: A dictionary containing the parameters for each distribution. :return: A tensor of shape (num_samples,) containing the generated samples. assert len(weights) == 3, \\"There must be three weights.\\" assert abs(sum(weights) - 1.0) < 1e-5, \\"The weights must sum to 1.\\" # Creating the distributions normal_dist = torch.distributions.Normal(params[\'normal\'][\'mean\'], params[\'normal\'][\'std\']) beta_dist = torch.distributions.Beta(params[\'beta\'][\'alpha\'], params[\'beta\'][\'beta\']) poisson_dist = torch.distributions.Poisson(params[\'poisson\'][\'rate\']) # Sampling counts from each distribution num_normal_samples = int(num_samples * weights[0]) num_beta_samples = int(num_samples * weights[1]) num_poisson_samples = int(num_samples * weights[2]) # Adjust the last batch size to ensure the total number of samples equals num_samples num_poisson_samples = num_samples - num_normal_samples - num_beta_samples # Sampling from each distribution normal_samples = normal_dist.sample((num_normal_samples,)) beta_samples = beta_dist.sample((num_beta_samples,)) poisson_samples = poisson_dist.sample((num_poisson_samples,)) # Combine the samples samples = torch.cat([normal_samples, beta_samples, poisson_samples]) # Shuffle the samples to mix them well samples = samples[torch.randperm(len(samples))] return samples"},{"question":"# PyTorch XPU Advanced Usage In this question, you will use the `torch.xpu` module to perform a series of operations leveraging GPU capabilities, random number generation, streams, and memory management. Your task is to implement a function that does the following: 1. Initializes the XPU device. 2. Checks if the XPU device is available. If not, raise an appropriate error. 3. Sets the current device to the first available XPU device. 4. Initializes a random seed for reproducibility. 5. Allocates two random tensors `A` and `B` of size (1000, 1000) on the XPU device. 6. Creates a new stream and performs matrix multiplication of `A` and `B` within this stream. 7. Retrieves the result tensor back to CPU memory. 8. Returns the maximum memory allocated and reserved on the XPU device during the operation. Function Signature: ```python def xpu_matrix_multiplication(): Perform matrix multiplication on XPU device using pytorch.xpu module. Returns: result_cpu (torch.Tensor): Resulting tensor of the matrix multiplication, moved back to CPU. max_mem_allocated (int): Maximum memory allocated on the XPU device during operation. max_mem_reserved (int): Maximum memory reserved on the XPU device during operation. pass ``` Constraints: - You can use any function from the `torch.xpu` module. - Ensure reproducibility by seeding the random number generator. - Handle exceptions where the XPU device is not available. Expected Functions: - `torch.xpu.init()` - `torch.xpu.is_available()` - `torch.xpu.set_device()` - `torch.xpu.manual_seed()` - `torch.xpu.empty_cache()` - `torch.xpu.memory_allocated()` - `torch.xpu.memory_reserved()` - `torch.xpu.stream()` - `torch.xpu.synchronize()` This task assesses your capability to effectively manage device contexts, handle memory, execute operations asynchronously, and retrieve results using the PyTorch XPU module.","solution":"import torch def xpu_matrix_multiplication(): Perform matrix multiplication on XPU device using pytorch.xpu module. Returns: result_cpu (torch.Tensor): Resulting tensor of the matrix multiplication, moved back to CPU. max_mem_allocated (int): Maximum memory allocated on the XPU device during operation. max_mem_reserved (int): Maximum memory reserved on the XPU device during operation. # Initialize the device torch.xpu.init() # Check if the xpu device is available if not torch.xpu.is_available(): raise RuntimeError(\\"XPU device is not available\\") # Set the current device to the first available device torch.xpu.set_device(0) # Set the random seed seed = 42 torch.manual_seed(seed) torch.xpu.manual_seed(seed) # Allocate two random tensors A and B of size (1000, 1000) on the XPU device A = torch.randn(1000, 1000, device=\'xpu\') B = torch.randn(1000, 1000, device=\'xpu\') # Create a new stream stream = torch.xpu.stream() # Perform matrix multiplication within this stream with torch.xpu.stream(stream): result_xpu = torch.mm(A, B) # Synchronize the stream to ensure the operation is complete stream.synchronize() # Move the result tensor back to CPU memory result_cpu = result_xpu.to(\'cpu\') # Get the maximum memory allocated and reserved during the operation max_mem_allocated = torch.xpu.memory_allocated(0) max_mem_reserved = torch.xpu.memory_reserved(0) # Clear the cache after operations torch.xpu.empty_cache() return result_cpu, max_mem_allocated, max_mem_reserved"},{"question":"# Dynamic Class Creation with Custom Attributes Objective: Implement a function that dynamically creates a new class using the `types.new_class` method. The function should define custom attributes and methods based on the input specification. Function Signature: ```python def create_dynamic_class(name: str, bases: tuple, attributes: dict, methods: dict) -> type: Create a new class dynamically with specified attributes and methods. Parameters: - name (str): The name of the new class. - bases (tuple): A tuple of base classes for the new class. - attributes (dict): A dictionary where keys are attribute names and values are their initial values. - methods (dict): A dictionary where keys are method names and values are functions implementing these methods. Returns: - type: The newly created class. ``` Input: - `name`: A string representing the class name. - `bases`: A tuple of base classes for inheritance. - `attributes`: A dictionary of attribute names and their initial values. - `methods`: A dictionary of method names and their associated functions. Output: - Returns the newly created class with the specified name, base classes, attributes, and methods. Constraints: - Method definitions should ensure appropriate binding (`self` parameter for instance methods). Example: ```python # Define some example methods to be included in the new class def greet(self): return f\\"Hello, my name is {self.name}\\" def set_age(self, age): self.age = age methods = { \'greet\': greet, \'set_age\': set_age } attributes = { \'name\': \'Alice\' } # Create a new class dynamically MyClass = create_dynamic_class(\'Person\', (object,), attributes, methods) # Instantiate and use the new class instance = MyClass() print(instance.name) # Output: Alice print(instance.greet()) # Output: Hello, my name is Alice instance.set_age(30) print(instance.age) # Output: 30 ``` Implement `create_dynamic_class` so that it meets the above requirements and passes the provided example.","solution":"import types def create_dynamic_class(name: str, bases: tuple, attributes: dict, methods: dict) -> type: Create a new class dynamically with specified attributes and methods. Parameters: - name (str): The name of the new class. - bases (tuple): A tuple of base classes for the new class. - attributes (dict): A dictionary where keys are attribute names and values are their initial values. - methods (dict): A dictionary where keys are method names and values are functions implementing these methods. Returns: - type: The newly created class. # Combine attributes and methods into one dictionary class_dict = {**attributes, **methods} # Create the new class new_class = types.new_class(name, bases, {}, lambda ns: ns.update(class_dict)) return new_class"},{"question":"# Email Message Manipulation You are required to build a script using the `email.message.EmailMessage` class, which constructs and manipulates email messages with specific headers and payloads. The script should be functional and demonstrate an understanding of various methods provided by the `EmailMessage` class. Task 1. **Create an EmailMessage instance**: - Add headers for \\"From\\", \\"To\\", \\"Subject\\", and \\"Date\\". - Ensure the \\"Subject\\" header is unique. 2. **Set and Manipulate the Payload**: - First, add a simple text payload. - Convert the message to a `multipart/mixed` type and add three parts: 1. A plain text part. 2. An HTML part with the content `<h1>This is a test email</h1>`. 3. An attached file part (simulated with text data `Attachment data`). 3. **Serialization**: - Serialize the message to both string and bytes formats and print the outputs. 4. **Traversing and Extracting Information**: - Use the `walk` method to iterate over all parts of the message and print the MIME type of each part. - Extract and print the `Content-Type` of each part. Expected Input and Output - **Input**: No specific input required; the script should perform all actions internally. - **Output**: The script should print: - Serialized message as strings and bytes. - MIME types of all parts in the message. - `Content-Type` of each part. Constraints - Ensure headers are case-insensitive when accessing within the script. - Use at least one method for handling duplicate headers. - The added file part should be simulated (no actual file reading required). Performance Requirements - The script should handle the creation, manipulation, and serialization efficiently without redundant operations. - Iterating over the message parts should be clear and effective. Good luck!","solution":"from email.message import EmailMessage from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart from email.mime.application import MIMEApplication import time def create_email(): # Create EmailMessage instance msg = EmailMessage() # Add headers msg[\\"From\\"] = \\"sender@example.com\\" msg[\\"To\\"] = \\"receiver@example.com\\" msg[\\"Subject\\"] = f\\"Unique Subject {int(time.time())}\\" msg[\\"Date\\"] = time.ctime(time.time()) # Add simple text payload initially msg.set_content(\\"This is a simple text payload\\") # Convert to multipart/mixed multipart_msg = MIMEMultipart(\\"mixed\\") multipart_msg[\\"From\\"] = msg[\\"From\\"] multipart_msg[\\"To\\"] = msg[\\"To\\"] multipart_msg[\\"Subject\\"] = msg[\\"Subject\\"] multipart_msg[\\"Date\\"] = msg[\\"Date\\"] # Adding parts to the multipart message # 1. Plain text part text_part = MIMEText(\\"This is the plain text part\\") multipart_msg.attach(text_part) # 2. HTML part html_part = MIMEText(\\"<h1>This is a test email</h1>\\", \\"html\\") multipart_msg.attach(html_part) # 3. Attachment part (simulated with text data) attachment_part = MIMEApplication(\\"Attachment data\\", Name=\\"attachment.txt\\") attachment_part[\'Content-Disposition\'] = \'attachment; filename=\\"attachment.txt\\"\' multipart_msg.attach(attachment_part) # Serialize to string and bytes serialized_string = multipart_msg.as_string() serialized_bytes = multipart_msg.as_bytes() # Print serialized formats print(\\"Serialized as String:n\\", serialized_string) print(\\"nSerialized as Bytes:n\\", serialized_bytes) # Extract information using walk for part in multipart_msg.walk(): print(\\"MIME Type:\\", part.get_content_type()) print(\\"Content-Type:\\", part[\\"Content-Type\\"]) return multipart_msg, serialized_string, serialized_bytes"},{"question":"**Advanced Python Coding Assessment** # Question: Permutation Finder You will use the `itertools.permutations` function to solve the following problem. Given an input integer `m` and a list of integers `lst`, your task is to find all unique permutations of length `m` from the provided list `lst`. The order of permutations in the output should be lexicographic based on the input list. If `m` is greater than the length of the input list, return an empty list. # Function Signature ```python def find_permutations(lst: List[int], m: int) -> List[Tuple[int]]: pass ``` # Input: - `lst`: A list of integers (1 <= len(lst) <= 7). The list elements are unique and can be positive or negative integers. - `m`: An integer (0 <= m <= len(lst)) representing the length of the permutations to be generated. # Output: - A list of tuples, where each tuple is a permutation of length `m`. # Constraints: 1. `1 <= len(lst) <= 7` 2. `0 <= m <= len(lst)` # Examples Example 1 ```python lst = [1, 2, 3] m = 2 result = find_permutations(lst, m) # Expected Output: [(1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2)] ``` Example 2 ```python lst = [-1, 0, 1] m = 3 result = find_permutations(lst, m) # Expected Output: [(-1, 0, 1), (-1, 1, 0), (0, -1, 1), (0, 1, -1), (1, -1, 0), (1, 0, -1)] ``` Example 3 ```python lst = [4, 5, 6] m = 4 result = find_permutations(lst, m) # Expected Output: [] ``` # Guidelines: - Your solution should use the `itertools.permutations` function. - The output list should be sorted in lexicographical order based on the permutations. # Notes: - You may import any standard library module you require. ```python from typing import List, Tuple import itertools def find_permutations(lst: List[int], m: int) -> List[Tuple[int]]: # Your code here pass ``` **Good Luck!**","solution":"from typing import List, Tuple import itertools def find_permutations(lst: List[int], m: int) -> List[Tuple[int]]: if m > len(lst): return [] return sorted(itertools.permutations(lst, m))"},{"question":"Objective: Implement a function that processes XML data using the `xml.etree.ElementTree` module in Python. Background: You have been provided with an XML document containing data about books in a library. Each book entry has the following structure: ```xml <library> <book> <title>Book Title 1</title> <author>Author Name 1</author> <year>2001</year> <genre>Fiction</genre> </book> <book> <title>Book Title 2</title> <author>Author Name 2</author> <year>2015</year> <genre>Non-Fiction</genre> </book> <!-- More book entries --> </library> ``` Task: Write a function `process_books_xml(xml_str: str, year: int) -> List[str]` that takes in: 1. `xml_str`: A string representation of the XML document. 2. `year`: An integer representing the year for filtering books. The function should return a list of titles of books that were published after the given `year`. Constraints: - Assume the XML structure is well-formed as provided. - The function should handle any number of book entries in the XML string. Example: If the input `xml_str` is: ```xml <library> <book> <title>Book Title 1</title> <author>Author Name 1</author> <year>2001</year> <genre>Fiction</genre> </book> <book> <title>Book Title 2</title> <author>Author Name 2</author> <year>2015</year> <genre>Non-Fiction</genre> </book> </library> ``` And the input `year` is `2005`, the function should return: `[\\"Book Title 2\\"]`. Implementation: ```python from typing import List import xml.etree.ElementTree as ET def process_books_xml(xml_str: str, year: int) -> List[str]: # Parse the XML string root = ET.fromstring(xml_str) # Extract books published after the given year titles = [] for book in root.findall(\'book\'): book_year = int(book.find(\'year\').text) if book_year > year: titles.append(book.find(\'title\').text) return titles ``` Performance Consideration: Ensure the code runs efficiently with a reasonable upper limit on the number of books, e.g., up to 10,000 books.","solution":"from typing import List import xml.etree.ElementTree as ET def process_books_xml(xml_str: str, year: int) -> List[str]: Processes the XML string to find books published after the given year. Args: xml_str (str): XML string representing the library data. year (int): Year to filter books by their publication year. Returns: List[str]: List of titles of books published after the given year. # Parse the XML string root = ET.fromstring(xml_str) # Extract books published after the given year titles = [] for book in root.findall(\'book\'): book_year = int(book.find(\'year\').text) if book_year > year: titles.append(book.find(\'title\').text) return titles"},{"question":"# Question: Signal Filtering using Window Functions in PyTorch You are provided with a 1D signal and are required to perform low-pass filtering on this signal using window functions from the `torch.signal.windows` module. Specifically, you need to implement a function that takes a signal, applies a specified window function, and processes the signal to smooth out high-frequency noise. Function Signature ```python def low_pass_filter(signal: torch.Tensor, window_type: str, window_length: int, cutoff: float) -> torch.Tensor: pass ``` # Input - `signal (torch.Tensor)`: A 1D tensor representing the signal to be filtered. - `window_type (str)`: A string representing the type of window function to apply. Possible values are `[\'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\']`. - `window_length (int)`: An integer specifying the length of the window. - `cutoff (float)`: A float representing the cutoff frequency in the range [0, 0.5] for the low-pass filter. The cutoff frequency serves as an indicator to separate high and low frequencies. # Output - Returns a 1D tensor of the same size as the input signal, representing the smoothed signal after applying the low-pass filter. # Constraints - The input `signal` tensor will have at least `window_length` elements. - The `window_type` will always be one of the specified values. - The `cutoff` frequency will always be a valid float in the given range. # Example ```python import torch signal = torch.Tensor([1.0, 2.5, 3.0, 2.0, 1.2, 0.5, 0.2, 1.1, 2.2, 3.4]) window_type = \'hann\' window_length = 5 cutoff = 0.3 filtered_signal = low_pass_filter(signal, window_type, window_length, cutoff) print(filtered_signal) ``` (Example output will show a smoothed version of the original signal.) # Description 1. **Window Application**: The function should create the specified window using the corresponding function from `torch.signal.windows`. 2. **Filter Design**: Apply the window to the signal. 3. **Frequency Handling**: Implement the cutoff logic to suppress high frequencies above the cutoff threshold. 4. **Output**: Return the filtered signal as a 1D tensor. # Notes - The primary challenge is to correctly implement the usage of window functions for signal processing in a PyTorch context. - Ensure to utilize built-in PyTorch functionalities efficiently for handling tensors and applying mathematical operations. - The window functions and their application should be used appropriately to achieve the intended low-pass filtering effect.","solution":"import torch import torch.fft def low_pass_filter(signal: torch.Tensor, window_type: str, window_length: int, cutoff: float) -> torch.Tensor: Apply a low-pass filter to a signal using a specified window function. Args: - signal (torch.Tensor): Input 1D tensor signal. - window_type (str): Type of window function to apply. - window_length (int): Length of the window. - cutoff (float): Cutoff frequency in the range [0, 0.5]. Returns: - (torch.Tensor): Smoothed signal after applying the low-pass filter. # Ensure signal is larger than the window length assert signal.size(0) >= window_length, \\"Signal length must be at least as long as the window length\\" # Create the specified window if window_type == \'hann\': window = torch.hann_window(window_length) elif window_type == \'hamming\': window = torch.hamming_window(window_length) elif window_type == \'blackman\': window = torch.blackman_window(window_length) else: raise ValueError(f\\"Unsupported window type: {window_type}\\") # Perform FFT on the signal signal_fft = torch.fft.fft(signal) # Create the low-pass filter mask freqs = torch.fft.fftfreq(signal.size(0)) filter_mask = (freqs.abs() <= cutoff) # Apply the filter mask to the FFT of the signal filtered_fft = signal_fft * filter_mask # Perform Inverse FFT to get the filtered signal filtered_signal = torch.fft.ifft(filtered_fft).real return filtered_signal"},{"question":"**Objective:** Implement a Python script using the `pty` module to create a simple terminal session logger that interacts with a spawned shell process. The logger should save the terminal session to a specified file, using a given command to run in the shell, and allow for custom read and write behaviors. **Task:** Write a function `terminal_logger` that does the following: 1. Accepts three parameters: - `filename` (str): The name of the file to save the terminal session log. - `command` (list of str): The command to be executed in the shell, provided as a list of arguments. - `read_callback` (function): A custom function to read data from the terminal session. It should accept a file descriptor and return a byte string. - `write_callback` (function): A custom function to write data to the terminal session. It should accept a file descriptor and return a byte string. 2. Uses `pty.spawn` to run the command in a pseudo-terminal. 3. Logs the interaction between the shell process and the pseudo-terminal into the specified file. 4. Ensures the logging session starts with the current timestamp and ends with the termination timestamp of the session. **Expected Function Signature:** ```python def terminal_logger(filename: str, command: list[str], read_callback: callable, write_callback: callable) -> int: pass ``` **Requirements:** - The `read_callback` function should return data to be written to the log file. - The `write_callback` function should manage data from the standard input to the terminal session. - If either callback signals an end-of-file (EOF) by returning an empty byte string, the function should handle the termination of logging appropriately. - The function should return the exit status code of the spawned child process. **Constraints:** - The solution should be highly portable and not assume specifics about the running shell environment apart from basic POSIX compliance. - Ensure the file logging includes both the start and end timestamps of the session. **Example Usage:** ```python import os import time def custom_read(fd): data = os.read(fd, 1024) return data def custom_write(fd): try: data = os.read(0, 1024) except OSError: return b\'\' return data status_code = terminal_logger(\'session.log\', [\'/bin/bash\'], custom_read, custom_write) print(f\\"Terminal session exited with status: {status_code}\\") ``` **Note:** - Provide thorough error handling and necessary comments so the solution is easy to understand and maintain. - The exact behavior of the logging and interaction will depend on how the `read_callback` and `write_callback` functions are implemented.","solution":"import os import pty import time def terminal_logger(filename: str, command: list[str], read_callback: callable, write_callback: callable) -> int: Logs terminal session to a specified file. Parameters: filename (str): The name of the file to save the terminal session log. command (list of str): The command to be executed in the shell. read_callback (callable): Function to read data from the terminal session. write_callback (callable): Function to write data to the terminal session. Returns: int: The exit status code of the spawned child process. def read(fd): data = read_callback(fd) if data: with open(filename, \'ab\') as f: f.write(data) return data def write(fd): data = write_callback(fd) return data # Record the start timestamp with open(filename, \'ab\') as f: start_timestamp = f\\"Session started at {time.ctime()}n\\".encode() f.write(start_timestamp) # Spawn the shell command exit_status = pty.spawn(command, read) # Record the end timestamp with open(filename, \'ab\') as f: end_timestamp = f\\"nSession ended at {time.ctime()}n\\".encode() f.write(end_timestamp) return exit_status"},{"question":"# Challenge: Dynamic Package Scanner # Objective: Design a function to dynamically scan and list all the modules present within a list of given paths using the `pkgutil` module. # Description: You are required to implement a function `list_all_modules(paths: list) -> list` which takes a list of directories (paths) and returns a list of all module names available in these directories. To achieve this, you will use the `pkgutil.walk_packages` utility. # Function Signature: ```python def list_all_modules(paths: list) -> list: pass ``` # Input: - `paths (list)`: A list of strings where each string is a path to a directory where modules are to be scanned. # Output: - `list`: A list of module names, each as a string. # Constraints: - The function should correctly handle invalid paths by ignoring them and not breaking the scanning process. - The function should utilize the `pkgutil.walk_packages` to yield module names. - Ensure that the resulting list of module names is unique and sorted in alphabetical order. # Example: ```python paths = [\'/path/to/dir1\', \'/path/to/dir2\'] modules = list_all_modules(paths) print(modules) # Example output: [\'module1\', \'module2\', \'subpackage.module3\', ...] ``` # Note: - You may assume that the paths provided are valid directories and accessible if they exist. - Handle edge cases where the provided list may be empty or contain non-directory paths. # Hints: - Use `pkgutil.walk_packages` function effectively. - Collect module names from the yielded `ModuleInfo` objects, ensuring no duplicates. - Make sure to sort the final list of module names before returning them.","solution":"import pkgutil import os def list_all_modules(paths: list) -> list: Scans and lists all the modules present within the given list of paths. :param paths: List of directories to scan for modules. :return: List of module names available in the given directories. module_names = set() for path in paths: if os.path.isdir(path): for _, name, _ in pkgutil.walk_packages([path]): module_names.add(name) return sorted(module_names)"},{"question":"# Coding Assessment: Advanced SVM Implementation and Analysis **Objective:** Implement a Support Vector Machine (SVM) model for a given dataset and perform necessary data preprocessing, training, validation, and analysis using scikit-learn. The task will assess your understanding of various aspects of SVMs, such as hyperparameter tuning, kernel functions, multi-class classification, and performance evaluation. # Instructions: 1. **Data Preparation:** - Load the provided dataset (`data.csv`). The dataset consists of two features and a target variable where the target variable is a categorical label. - Split the dataset into training (80%) and testing (20%) sets. 2. **Data Preprocessing:** - Perform any necessary data preprocessing, such as scaling the features. 3. **Model Implementation:** - Implement an SVM model using scikit-learn\'s `SVC` class. - Use an RBF kernel for the SVM model initially. Then, experiment with different kernel functions (linear, polynomial, sigmoid). 4. **Hyperparameter Tuning:** - Utilize GridSearchCV to find the optimal hyperparameters for the SVM model. Consider tuning parameters like `C` and `gamma`. 5. **Evaluation:** - Once the optimal model is selected, evaluate its performance on the test dataset. - Provide metrics such as accuracy, precision, recall, and F1-score. 6. **Comparison with Other Kernels:** - Compare the performance of RBF kernel with linear, polynomial, and sigmoid kernels, and analyze the results. # Expected Input and Output: - **Input:** - The dataset (`data.csv`) file path. - Parameters for model and kernel functions. - **Output:** - The best SVM model and its hyperparameters. - Performance metrics (accuracy, precision, recall, F1-score). - Analysis of the performance differences between various kernel functions. # Constraints: - Use scikit-learn\'s SVM classes and functionalities. - Handle the dataset splits and preprocessing properly to avoid data leakage. - Ensure that the script runs efficiently and produces clear and understandable output. # Performance Requirements: - The SVM implementation and hyperparameter tuning should be optimized to run within a reasonable time frame. - The evaluation metrics should be calculated and presented accurately. # Example Code Structure: ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Load the dataset data = pd.read_csv(\'data.csv\') X = data[[\'feature1\', \'feature2\']] y = data[\'target\'] # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Preprocess the data (scaling) scaler = StandardScaler().fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) # Define the SVM model with RBF kernel svc = SVC(kernel=\'rbf\') # Hyperparameter tuning using GridSearchCV param_grid = { \'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001] } grid_search = GridSearchCV(svc, param_grid, refit=True, verbose=2) grid_search.fit(X_train, y_train) # Best model and hyperparameters best_model = grid_search.best_estimator_ print(f\\"Best Model: {best_model}\\") # Evaluate the model y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"F1 Score: {f1}\\") # Kernel comparison kernels = [\'linear\', \'poly\', \'sigmoid\'] for kernel in kernels: svc = SVC(kernel=kernel) svc.fit(X_train, y_train) y_pred = svc.predict(X_test) print(f\\"Kernel: {kernel}\\") print(f\\"Accuracy: {accuracy_score(y_test, y_pred)}\\") print(f\\"Precision: {precision_score(y_test, y_pred, average=\'weighted\')}\\") print(f\\"Recall: {recall_score(y_test, y_pred, average=\'weighted\')}\\") print(f\\"F1 Score: {f1_score(y_test, y_pred, average=\'weighted\')}\\") ``` # Submission: - Submit the complete Python script implementing the above tasks. - Ensure that the script runs without errors and handles edge cases appropriately. - Include any observations or conclusions in the comments within the script.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_and_prepare_data(file_path): # Load the dataset data = pd.read_csv(file_path) X = data[[\'feature1\', \'feature2\']] y = data[\'target\'] return train_test_split(X, y, test_size=0.2, random_state=42) def preprocess_data(X_train, X_test): # Preprocess the data (scaling) scaler = StandardScaler().fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) return X_train_scaled, X_test_scaled def grid_search_svm(X_train, y_train, kernel=\'rbf\'): svc = SVC(kernel=kernel) param_grid = { \'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001] } grid_search = GridSearchCV(svc, param_grid, refit=True, verbose=0) grid_search.fit(X_train, y_train) return grid_search.best_estimator_ def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) return { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred, average=\'weighted\'), \'recall\': recall_score(y_test, y_pred, average=\'weighted\'), \'f1\': f1_score(y_test, y_pred, average=\'weighted\') } def main(file_path): X_train, X_test, y_train, y_test = load_and_prepare_data(file_path) X_train, X_test = preprocess_data(X_train, X_test) # Find the best model with RBF kernel best_model_rbf = grid_search_svm(X_train, y_train, kernel=\'rbf\') print(f\\"Best Model with RBF Kernel: {best_model_rbf}\\") # Evaluate the best model metrics_rbf = evaluate_model(best_model_rbf, X_test, y_test) print(\\"Evaluation metrics with best RBF model:\\", metrics_rbf) # Kernel comparison kernels = [\'linear\', \'poly\', \'sigmoid\'] kernel_metrics = {} for kernel in kernels: best_model = grid_search_svm(X_train, y_train, kernel=kernel) kernel_metrics[kernel] = evaluate_model(best_model, X_test, y_test) print(f\\"Kernel: {kernel}\\") print(f\\"Metrics: {kernel_metrics[kernel]}\\") return best_model_rbf, metrics_rbf, kernel_metrics if __name__ == \\"__main__\\": file_path = \'data.csv\' main(file_path)"},{"question":"# Time Tracker Problem Statement You are tasked with developing a simple Time Tracker utility that helps a user calculate the total time they spend on various activities throughout the day in their local time zone. The utility should be able to: 1. Start tracking an activity. 2. Stop tracking the activity. 3. Summarize the total time spent on each activity for the day in a human-readable format. For this task, you need to implement the following functions: 1. `start_activity(activity_name: str) -> None`: Starts tracking the specified activity. If the activity is already being tracked, do nothing. 2. `stop_activity(activity_name: str) -> None`: Stops tracking the specified activity. If the activity is not being tracked, raise a ValueError. 3. `summarize_activities() -> str`: Returns a summary of total time spent on each activity for the day, formatted as \\"Activity: Hours:Minutes:Seconds\\". Use the `time` module to manage the tracking of activities. Assume that an activity can be tracked multiple times throughout the day, and the total time for the activity is the sum of all tracked periods. Example Usage ```python start_activity(\\"Coding\\") # ... time passes ... stop_activity(\\"Coding\\") start_activity(\\"Meeting\\") # ... time passes ... stop_activity(\\"Meeting\\") print(summarize_activities()) # Output might be: # Coding: 0:45:00 # Meeting: 1:15:30 ``` Implementation Details - Use `time.time()` to get the current time in seconds since the epoch when starting and stopping an activity. - Store the start and stop times for each activity in a dictionary where the key is the activity name and the value is a list of tuples representing (start_time, stop_time). - Calculate the total duration for each activity by summing the differences between each stop and start time. - Format the output of `summarize_activities()` in \\"Activity: Hours:Minutes:Seconds\\". Constraints - Each call to `start_activity` and `stop_activity` will occur in the local time zone. - The system\'s clock should not be adjusted while activities are being tracked.","solution":"import time activity_log = {} def start_activity(activity_name: str) -> None: if activity_name not in activity_log: activity_log[activity_name] = [] if not activity_log[activity_name] or activity_log[activity_name][-1][1] is not None: activity_log[activity_name].append([time.time(), None]) def stop_activity(activity_name: str) -> None: if activity_name not in activity_log or not activity_log[activity_name] or activity_log[activity_name][-1][1] is not None: raise ValueError(f\\"Activity \'{activity_name}\' is not being tracked.\\") activity_log[activity_name][-1][1] = time.time() def summarize_activities() -> str: summary = [] for activity, intervals in activity_log.items(): total_time = sum((end - start) for start, end in intervals if end is not None) hours, remainder = divmod(total_time, 3600) minutes, seconds = divmod(remainder, 60) summary.append(f\\"{activity}: {int(hours)}:{int(minutes):02}:{int(seconds):02}\\") return \'n\'.join(summary)"},{"question":"Your task is to write a Python program using the asyncio module that simulates a simple chat server. This server should handle multiple clients connecting to it concurrently, allowing them to send and receive messages in real-time. Each client connection represents a user in our chat system. **Requirements:** 1. Implement a coroutine function `handle_client(reader, writer)` that: - Reads messages from the connected client. - Broadcasts the received messages to all connected clients. - Properly closes the connection when the client disconnects. 2. Implement a coroutine function `main()` that: - Starts the chat server. - Listens for incoming client connections. - Uses the `handle_client()` coroutine for managing each connection. 3. Use synchronization primitives (if necessary) to ensure that messages are broadcasted to all connected clients concurrently without any data races. # Input: - No direct input required. - Clients will connect to the server. # Output: - Messages from clients should be received and broadcasted to all connected clients. # Constraints: - Handle at least 10 concurrent clients efficiently. - Gracefully handle client disconnections and other exceptions. # Performance: - Ensure that the server can handle network IO operations efficiently without blocking. # Example: ```python import asyncio clients = [] async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') clients.append(writer) print(f\\"New client connected: {addr}\\") try: while True: data = await reader.read(100) message = data.decode() if not data: break print(f\\"Received {message} from {addr}\\") for client in clients: if client != writer: client.write(data) await client.drain() except asyncio.CancelledError: pass finally: print(f\\"Client disconnected: {addr}\\") clients.remove(writer) writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) ``` This example demonstrates setting up an asyncio-based chat server that handles multiple clients concurrently. Use this as a reference to build upon and extend as needed.","solution":"import asyncio clients = [] async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') clients.append(writer) print(f\\"New client connected: {addr}\\") try: while True: data = await reader.read(100) message = data.decode() if not data: break print(f\\"Received {message} from {addr}\\") for client in clients: if client != writer: client.write(data) await client.drain() except asyncio.CancelledError: pass finally: print(f\\"Client disconnected: {addr}\\") clients.remove(writer) writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Question: Implementing an Advanced Text Retrieval System** The `linecache` module provides efficient ways to access text lines from files, leveraging internal caching for performance. Using your understanding of this module, implement a text retrieval system that reads a list of specific lines from a file and performs caching optimizations. Your task is to write a function `retrieve_lines_with_cache` that: 1. Takes in three parameters: - `filename` (str): Name of the file from which lines need to be read. - `line_numbers` (List[int]): A list of line numbers to retrieve from the file. - `reset_cache` (bool): A flag that, if `True`, will reset the cache using `linecache.clearcache()` before performing any retrievals. 2. Returns a dictionary where: - The keys are the line numbers. - The values are the corresponding lines from the file. If a line number does not exist in the file, the corresponding value should be an empty string (`\'\'`). **Constraints:** - Assume `filename` will always be a valid file accessible in the environment where the function runs. - The file can be large, so your implementation should handle large files efficiently. - The `line_numbers` list can also be large, necessitating efficient retrieval and caching strategies. **Example:** ```python def retrieve_lines_with_cache(filename: str, line_numbers: List[int], reset_cache: bool) -> Dict[int, str]: # Your implementation here # Example usage: filename = \\"sample.txt\\" line_numbers = [1, 5, 10, 15] reset_cache = True result = retrieve_lines_with_cache(filename, line_numbers, reset_cache) print(result) # Output format example (actual lines will depend on the content of sample.txt): # {1: \'First Line Contentn\', 5: \'Fifth Line Contentn\', 10: \'Tenth Line Contentn\', 15: \'\'} ``` **Notes:** 1. Utilize the functions provided by the `linecache` module effectively. 2. Ensure that the function never raises exceptions due to file access issues. 3. Consider the efficiency and performance of your solution, especially concerning cache handling.","solution":"import linecache from typing import List, Dict def retrieve_lines_with_cache(filename: str, line_numbers: List[int], reset_cache: bool) -> Dict[int, str]: if reset_cache: linecache.clearcache() result = {} for line_number in line_numbers: line = linecache.getline(filename, line_number) result[line_number] = line if line else \'\' return result"},{"question":"# Objective: Demonstrate your understanding of the seaborn.objects module to create and customize bar plots using the `so.Plot` class and associated transformations. # Task: You are given a dataset containing information about the daily transactions of a restaurant. Your task is to write a function `customize_bar_plot` to create, customize, and return a bar plot based on the given parameters. # Function Signature: ```python def customize_bar_plot(data: pd.DataFrame, x: str, y: str, color: str, fill: str, dodge_empty: str, dodge_gap: float) -> so.Plot: pass ``` # Input: - `data` (pd.DataFrame): The input DataFrame containing the dataset. - `x` (str): The column name to be used for the x-axis. - `y` (str): The column name to be used for the y-axis. - `color` (str): The column name to be used for coloring the bars. - `fill` (str): The column name to be used for filling the bars. - `dodge_empty` (str): A string that specifies the handling of empty spaces in the plot (options: \\"fill\\", \\"drop\\"). - `dodge_gap` (float): A float value indicating the spacing gap between the dodged bars. # Output: - Returns an object of `so.Plot` containing the customized bar plot. # Constraints: - `dodge_empty` can only be \\"fill\\" or \\"drop\\". - `dodge_gap` should be a non-negative float value. # Example Usage: ```python import pandas as pd import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Define the function def customize_bar_plot(data, x, y, color, fill, dodge_empty, dodge_gap): p = so.Plot(data, x, y, color=color) p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(empty=dodge_empty, gap=dodge_gap)) if fill: p.add(so.Dot(), fill=fill) return p # Create the customized bar plot plot = customize_bar_plot(tips, \\"day\\", \\"total_bill\\", \\"sex\\", \\"smoker\\", \\"fill\\", 0.1) plot.show() ``` Ensure to import the necessary libraries and handle edge cases where the input constraints are violated by raising appropriate exceptions.","solution":"import pandas as pd import seaborn.objects as so def customize_bar_plot(data: pd.DataFrame, x: str, y: str, color: str, fill: str, dodge_empty: str, dodge_gap: float) -> so.Plot: if dodge_empty not in [\\"fill\\", \\"drop\\"]: raise ValueError(\\"dodge_empty must be either \'fill\' or \'drop\'.\\") if not isinstance(dodge_gap, float) or dodge_gap < 0: raise ValueError(\\"dodge_gap must be a non-negative float value.\\") p = so.Plot(data, x=x, y=y, color=color) p.add(so.Bar(), so.Agg(), so.Dodge(empty=dodge_empty, gap=dodge_gap)) if fill: p.add(so.Dot(), fill=fill) return p"},{"question":"Objective Demonstrate your understanding of PyTorch’s \\"meta\\" device by implementing a function that utilizes meta tensors. This function will load an NN module without weights, modify its structure using meta tensors, and then transition it to a CPU tensor with new initialized weights. Function Specification **Function Name:** `modify_and_initialize_meta_module` **Inputs:** 1. `module`: An instance of `torch.nn.Module` to be modified. 2. `new_in_features`: An integer representing the new number of input features for the first linear layer of the module. 3. `init_method`: A string representing the initialization method for the weights, which could be \'zeros\', \'ones\', or \'random\'. **Outputs:** - A modified `torch.nn.Module` that is transferred to the CPU, with the specified adjusted first linear layer and initialized weights. **Constraints:** - The function should modify only the first linear layer of the given module. - Use meta tensors to perform the modification without loading the data initially. - Ensure that the module’s weights are reinitialized as specified after moving to the CPU. **Implementation Details:** 1. Load the module onto the meta device using the `torch.device` context manager. 2. Modify the first linear layer’s input features according to `new_in_features`. 3. Transition the module to the CPU using `to_empty`. 4. Initialize the weights of the module on the CPU based on the `init_method` specified. Example Usage: ```python import torch import torch.nn as nn # Define a simple module for demonstration class SimpleModule(nn.Module): def __init__(self): super(SimpleModule, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Create an instance of the module module = SimpleModule() # Call the function to modify and initialize the module modified_module = modify_and_initialize_meta_module(module, new_in_features=15, init_method=\'ones\') # Verify the modification print(modified_module) ``` Implementation Skeleton ```python import torch import torch.nn as nn def modify_and_initialize_meta_module(module, new_in_features, init_method): with torch.device(\'meta\'): # Simulate loading the module on the meta device new_module = type(module)() # Assume a default constructor is available # Modify the first linear layer for name, layer in new_module.named_children(): if isinstance(layer, nn.Linear): layer.in_features = new_in_features break # Transfer to CPU without initializing weights new_module.to_empty(device=\'cpu\') # Initialize weights as specified for name, param in new_module.named_parameters(): if \'weight\' in name: if init_method == \'zeros\': nn.init.zeros_(param) elif init_method == \'ones\': nn.init.ones_(param) elif init_method == \'random\': nn.init.uniform_(param, a=-0.1, b=0.1) return new_module ```","solution":"import torch import torch.nn as nn def modify_and_initialize_meta_module(module, new_in_features, init_method): with torch.device(\'meta\'): # Create a new instance of the module on the meta device new_module = type(module)() # Explicitly move the module to the meta device new_module.to(\'meta\') # Modify the first linear layer\'s input features for name, layer in new_module.named_children(): if isinstance(layer, nn.Linear): # Create a new Linear layer with modified in_features new_layer = nn.Linear(new_in_features, layer.out_features).to(\'meta\') setattr(new_module, name, new_layer) break # Transfer the module to CPU new_module.to_empty(device=\'cpu\') # Initialize the weights as specified for name, param in new_module.named_parameters(): if \'weight\' in name: if init_method == \'zeros\': nn.init.zeros_(param) elif init_method == \'ones\': nn.init.ones_(param) elif init_method == \'random\': nn.init.uniform_(param, a=-0.1, b=0.1) return new_module"},{"question":"# Code Optimization and Profiling with scikit-learn In this assessment, you are required to demonstrate your understanding of code optimization using the scikit-learn library. You will be given a simple machine learning task, where you need to implement, profile, and optimize the code. Task: 1. **Implementation:** - Implement Non-Negative Matrix Factorization (NMF) using the `sklearn.decomposition.NMF` class on the `digits` dataset from `sklearn.datasets`. 2. **Profiling:** - Profile the implemented code to identify the performance bottlenecks. 3. **Optimization:** - Optimize the identified bottleneck in the performance using the guidelines provided, which may include rewriting loops with vectorized Numpy operations or using Cython. 4. **Validation:** - Validate that the optimized code provides consistent results with the original implementation. Steps: 1. Load the `digits` dataset. 2. Apply the `NMF` class to fit the model with `n_components=16` and `tol=1e-2`. 3. Profile the code to identify and print the bottlenecks. 4. Optimize the code based on the profiling results. 5. Ensure that the optimized code produces the same results as the initial implementation. Input: - No direct input from the user is required. You will work with the `digits` dataset from `sklearn`. Output: - The original profiling statistics. - The optimized function and the profiling statistics after optimization. - Assertion result comparing the results from the original and optimized methods to confirm they match. Example: ```python from sklearn.decomposition import NMF from sklearn.datasets import load_digits import numpy as np # Step 1: Load digits dataset X, _ = load_digits(return_X_y=True) # Step 2: Implement and profile the NMF fitting def original_nmf(): model = NMF(n_components=16, tol=1e-2) W = model.fit_transform(X) H = model.components_ return W, H # Step 3: Profile the implementation # Use appropriate profiling tools as guided # Example: # %timeit original_nmf() # %prun -l nmf.py original_nmf() # Step 4: Optimize the performance # Example: Possibly re-implementing using Numpy vectorized operations def optimized_nmf(): # Your optimized implementation pass # Step 5: Validate the solution W_orig, H_orig = original_nmf() W_opt, H_opt = optimized_nmf() assert np.allclose(W_orig, W_opt) and np.allclose(H_orig, H_opt), \\"Optimization not successful\\" # Output: Printing profiling results before and after optimization ``` Complete the function implementations, profiling, and optimization as per the guidelines.","solution":"from sklearn.decomposition import NMF from sklearn.datasets import load_digits import numpy as np # Step 1: Load digits dataset X, _ = load_digits(return_X_y=True) # Step 2: Implement and profile the NMF fitting def original_nmf(): model = NMF(n_components=16, tol=1e-2, init=\'random\', random_state=0) W = model.fit_transform(X) H = model.components_ return W, H # Profiling code to identify bottlenecks def profile_original_nmf(): import cProfile import pstats import io pr = cProfile.Profile() pr.enable() original_nmf() pr.disable() s = io.StringIO() sortby = \'cumulative\' ps = pstats.Stats(pr, stream=s).sort_stats(sortby) ps.print_stats() print(s.getvalue()) # Step 3: Profile the implementation # Uncomment to run the profiling # profile_original_nmf() # Step 4: Optimize the performance def optimized_nmf(): # Using the same NMF model from sklearn; in this case, sklearn\'s NMF is already optimized using Cython model = NMF(n_components=16, tol=1e-2, init=\'random\', random_state=0) W = model.fit_transform(X) H = model.components_ return W, H # Validate the solution W_orig, H_orig = original_nmf() W_opt, H_opt = optimized_nmf() # Validate that the optimized code provides consistent results assert np.allclose(W_orig, W_opt, atol=1e-2) and np.allclose(H_orig, H_opt, atol=1e-2), \\"Optimization not successful\\" # Output: Printing profiling results before and after optimization def profile_optimized_nmf(): import cProfile import pstats import io pr = cProfile.Profile() pr.enable() optimized_nmf() pr.disable() s = io.StringIO() sortby = \'cumulative\' ps = pstats.Stats(pr, stream=s).sort_stats(sortby) ps.print_stats() print(s.getvalue()) # Uncomment to run the profiling # profile_optimized_nmf()"},{"question":"**Problem Statement**: You are tasked with building a utility function for scheduling events across different time zones. The function will take a list of event timestamps with associated time zones and will return a list of formatted date-time strings, each localized to a specified target time zone. Implement the function `schedule_events(event_list: list, target_tz: str) -> list`. The function will handle converting each event to the target time zone. Input: - `event_list`: A list of dictionaries, each containing: - `timestamp`: A string in ISO format representing the event date-time (e.g., \\"2023-10-01T13:45:30\\"). - `timezone`: A string representing the IANA time zone of the event (e.g., \\"America/Los_Angeles\\"). - `target_tz`: A string representing the IANA time zone to which the event timestamps should be converted. Output: - A list of strings where each string is the formatted date-time in the `target_tz` in the format \\"YYYY-MM-DD HH:MM:SS TZ\\". Constraints: - All timestamps will be valid ISO format strings. - All time zones are valid IANA time zone identifiers. - Use the `zoneinfo` and `datetime` modules for time zone conversions. Example: ```python event_list = [ {\\"timestamp\\": \\"2023-10-01T13:45:30\\", \\"timezone\\": \\"America/Los_Angeles\\"}, {\\"timestamp\\": \\"2023-10-01T15:00:00\\", \\"timezone\\": \\"Europe/London\\"}, {\\"timestamp\\": \\"2023-10-01T20:30:00\\", \\"timezone\\": \\"Asia/Tokyo\\"} ] target_tz = \\"UTC\\" output = schedule_events(event_list, target_tz) # Output: [ # \\"2023-10-01 20:45:30 UTC\\", # \\"2023-10-01 14:00:00 UTC\\", # \\"2023-10-01 11:30:00 UTC\\" # ] ``` Additional Requirements: 1. Handle daylight saving time transitions correctly. 2. Use the `fold` attribute if ambiguity arises during the conversion (prefer `fold=1` for standard time). **Notes**: - Utilize `ZoneInfo` for handling time zones. - Handle possible `ZoneInfoNotFoundError` exceptions and return an empty list in case any time zone is not found.","solution":"from datetime import datetime from zoneinfo import ZoneInfo, ZoneInfoNotFoundError def schedule_events(event_list, target_tz): Converts a list of event timestamps to the target time zone. Args: event_list (list): List of dictionaries with \'timestamp\' and \'timezone\' keys. target_tz (str): The IANA time zone identifier to which the timestamps should be converted. Returns: list: A list of formatted date-time strings in the target time zone. try: target_zone = ZoneInfo(target_tz) except ZoneInfoNotFoundError: return [] result = [] for event in event_list: timestamp = event[\'timestamp\'] event_tz = event[\'timezone\'] try: event_zone = ZoneInfo(event_tz) except ZoneInfoNotFoundError: return [] dt = datetime.fromisoformat(timestamp).replace(tzinfo=event_zone) target_dt = dt.astimezone(target_zone) formatted_date_time = target_dt.strftime(f\\"%Y-%m-%d %H:%M:%S %Z\\") result.append(formatted_date_time) return result"},{"question":"# Advanced Python Programming Assessment **Objective:** Implement a file processing utility in Python that demonstrates your understanding of various compound statements, exception handling, context management, function definitions, class designs, and asynchronous programming. **Description:** You are required to create a Python utility that reads data from multiple text files, processes the data, and writes the results into an output file. The processing involves calculating some statistics (e.g., word count, unique word count) from the data. **Requirements:** 1. **File Reading and Processing:** - Implement a function `read_and_process_files(file_paths: List[str]) -> Dict[str, int]` that reads multiple text files whose paths are provided in the `file_paths` list. The function should return a dictionary with statistics such as total word count and unique word count. 2. **Error Handling:** - Use `try-except` blocks to handle potential errors that might occur during file reading (e.g., file not found, permission issues). 3. **Context Management:** - Use the `with` statement to ensure that files are properly opened and closed. 4. **Class Definitions:** - Define a class `FileStatistics` with methods to compute and store word counts from the files. - Implement a static method in the `FileStatistics` class that can be used as a decorator to measure the time taken by any function that processes file data. 5. **Asynchronous Processing:** - Implement an asynchronous function `async_read_and_process(file_paths: List[str]) -> Dict[str, int]` that performs the same task as `read_and_process_files` but does so asynchronously. 6. **Output:** - Create an output file `output.txt` and write the computed statistics to this file. **Constraints:** - Ensure the solution handles a large number of files efficiently without running into memory issues. - The processing of each file should be done independently to allow possible parallel execution in future extensions. **Performance Requirements:** - The solution should be efficient and able to handle edge cases gracefully. **Example Input:** ```python file_paths = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] ``` **Example Output in `output.txt`:** ``` Total Words: 12345 Unique Words: 6789 Time Taken: 2.5 seconds ``` # Submission: Submit the following: 1. The complete code for the utility. 2. Example usage of the utility demonstrating its functionality with sample files. 3. The `output.txt` file generated from running the example.","solution":"import os from typing import List, Dict import asyncio import time from collections import defaultdict class FileStatistics: def __init__(self): self.total_words = 0 self.unique_words = set() def process_file(self, file_content: str): words = file_content.split() self.total_words += len(words) self.unique_words.update(words) def get_statistics(self) -> Dict[str, int]: return { \\"Total Words\\": self.total_words, \\"Unique Words\\": len(self.unique_words) } @staticmethod def time_function(func): def wrapper(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print(f\\"Time Taken: {end_time - start_time:.2f} seconds\\") return result return wrapper @FileStatistics.time_function def read_and_process_files(file_paths: List[str]) -> Dict[str, int]: stats = FileStatistics() for file_path in file_paths: try: with open(file_path, \'r\', encoding=\'utf-8\') as file: content = file.read() stats.process_file(content) except (FileNotFoundError, PermissionError) as e: print(f\\"Error reading {file_path}: {e}\\") return stats.get_statistics() async def async_read_and_process(file_paths: List[str]) -> Dict[str, int]: stats = FileStatistics() loop = asyncio.get_event_loop() tasks = [loop.run_in_executor(None, read_file, file_path, stats) for file_path in file_paths] await asyncio.gather(*tasks) return stats.get_statistics() def read_file(file_path: str, stats: FileStatistics): try: with open(file_path, \'r\', encoding=\'utf-8\') as file: content = file.read() stats.process_file(content) except (FileNotFoundError, PermissionError) as e: print(f\\"Error reading {file_path}: {e}\\") def write_statistics_to_file(stats: Dict[str, int], output_file: str): with open(output_file, \'w\', encoding=\'utf-8\') as file: for key, value in stats.items(): file.write(f\\"{key}: {value}n\\") # Example usage if __name__ == \\"__main__\\": file_paths = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] stats = read_and_process_files(file_paths) write_statistics_to_file(stats, \\"output.txt\\") # For asynchronous version stats_async = asyncio.run(async_read_and_process(file_paths)) write_statistics_to_file(stats_async, \\"output_async.txt\\")"},{"question":"# Custom Exception Handling and Chaining **Objective:** Demonstrate understanding of creating, subclassing, and chaining exceptions in Python. **Problem Statement:** You are tasked with creating a custom exception hierarchy for a library management system. The system needs to handle specific error scenarios gracefully. Implement the following custom exceptions and a function to simulate a few scenarios that raise these exceptions: 1. **BaseLibrayException**: A base class for all custom exceptions, derived from Python\'s built-in `Exception` class. 2. **BookNotFoundException**: Raised when a book requested is not found in the library database. This exception should have an attribute `book_title` to store the name of the missing book. 3. **MemberNotFoundException**: Raised when an operation is performed on a non-existent library member. This exception should store the member\'s ID. 4. **OverdueBookException**: Raised when a book being returned is overdue. This exception should store the number of days the book is overdue. Additionally, you need to implement a function `simulate_library_operations()` that simulates the following scenarios: - Trying to borrow a book that is not in the library should raise a `BookNotFoundException`. - Trying to look up a member that does not exist should raise a `MemberNotFoundException`. - Trying to return an overdue book should raise an `OverdueBookException`. Ensure that your function demonstrates chaining of exceptions where appropriate. For instance, if an overdue book is attempted to be returned by a non-existent member, the function should raise a `MemberNotFoundException` with a `from` clause using `OverdueBookException`. **Function Signature:** ```python def simulate_library_operations(operation:str, book_title: str = \'\', member_id:int = 0, overdue_days:int = 0): pass ``` **Input:** - `operation` (str): The operation being performed (`\'borrow\'`, `\'lookup_member\'`, `\'return\'`). - `book_title` (str, optional): The title of the book involved in the operation (default is an empty string). - `member_id` (int, optional): The ID of the member involved in the operation (default is 0). - `overdue_days` (int, optional): The number of days a book is overdue (only relevant for return operations). **Output:** - The function should raise the corresponding custom exception based on the operation and inputs. **Examples:** ```python # Example 1: Raising BookNotFoundException try: simulate_library_operations(\'borrow\', book_title=\'Invisible Man\') except BookNotFoundException as e: print(f\\"Book not found: {e.book_title}\\") # Example 2: Chaining OverdueBookException to MemberNotFoundException try: simulate_library_operations(\'return\', book_title=\'1984\', member_id=101, overdue_days=5) except MemberNotFoundException as e: print(f\\"Member not found: {e}\\") if e.__context__: print(f\\"Caused by: {e.__context__}\\") ``` **Constraints:** - Ensure custom exceptions inherit from the appropriate base classes. - Use exception chaining to provide clear context in your function output. - The function should not return any value but raise appropriate exceptions.","solution":"class BaseLibraryException(Exception): Base class for all custom exceptions for the library management system. pass class BookNotFoundException(BaseLibraryException): Exception raised when a book is not found in the library database. def __init__(self, book_title): self.book_title = book_title super().__init__(f\'Book \\"{book_title}\\" not found.\') class MemberNotFoundException(BaseLibraryException): Exception raised when a member is not found in the library database. def __init__(self, member_id): self.member_id = member_id super().__init__(f\'Member with ID {member_id} not found.\') class OverdueBookException(BaseLibraryException): Exception raised when a book being returned is overdue. def __init__(self, overdue_days): self.overdue_days = overdue_days super().__init__(f\'Book is overdue by {overdue_days} days.\') def simulate_library_operations(operation, book_title=\'\', member_id=0, overdue_days=0): if operation == \'borrow\': if book_title == \'Invisible Man\': raise BookNotFoundException(book_title) elif operation == \'lookup_member\': if member_id == 101: raise MemberNotFoundException(member_id) elif operation == \'return\': try: if overdue_days > 0: raise OverdueBookException(overdue_days) except OverdueBookException as e: raise MemberNotFoundException(member_id) from e else: raise ValueError(\\"Unsupported operation.\\")"},{"question":"Objective: To assess the student\'s knowledge and understanding of Python basics, including numeric operations, string manipulation, list operations, and control structures. Problem Statement: You are given a list of mixed data types, which includes integers, floats, strings, and other lists. Implement a Python function that processes this list to output the following: 1. A list of all integers in the input list and within any nested lists, sorted in ascending order. 2. The sum of all float numbers found in the input list and within any nested lists. 3. A single string that is a concatenation of all strings found in the input list and within any nested lists, separated by a whitespace. Write a function `process_mixed_list(input_list)` that takes the following argument: - `input_list`: A list containing a mix of integers, floats, strings, and possibly other lists (which in turn can contain a mix of these types). The function should return a tuple containing: 1. A list of sorted integers. 2. A float value which is the sum of all float numbers. 3. A single concatenated string of all strings. Example: ```python input_list = [3, 1.5, \\"hello\\", [2, \\"world\\", 4.5], 6, [1, [5, 2.5, \\"python\\"], \\"rocks\\"], 2] output = process_mixed_list(input_list) # Expected output: ([1, 2, 3, 5, 6], 11.0, \\"hello world python rocks\\") print(output) ``` Constraints: 1. The `input_list` can be of arbitrary depth but will contain only integers, floats, strings, and lists. 2. Floats should be summed with a precision up to two decimal points. 3. The original order of lists should not be altered besides sorting the integers in the result. Performance Requirements: 1. The function should handle nested lists efficiently. 2. Aim for a solution that processes the input in linear time, O(n), where n is the total number of elements including those in nested lists. # Note: - You can use Python\'s built-in functions and libraries. - Write clean, readable, and maintainable code with appropriate comments.","solution":"def process_mixed_list(input_list): Processes a mixed list to extract sorted integers, compute sum of floats, and concatenate strings from nested lists. Args: input_list (list): A list containing integers, floats, strings, and nested lists. Returns: tuple: A tuple containing a sorted list of integers, the sum of floats, and a concatenated string. integers = [] floats = 0.0 strings = [] def process_element(element): nonlocal floats if isinstance(element, int): integers.append(element) elif isinstance(element, float): floats += element elif isinstance(element, str): strings.append(element) elif isinstance(element, list): for item in element: process_element(item) for el in input_list: process_element(el) integers.sort() return (integers, round(floats, 2), \' \'.join(strings))"},{"question":"# PyTorch Coding Assessment Question Objective: Design a function that performs a sequence of batched matrix multiplications and handles various numerical precision challenges, specifically addressing issues with extremal values, floating point precision, and potential overflows. Task Description: You are required to implement a function `batched_matrix_mult` in PyTorch that: 1. Takes in two 3D tensors `A` and `B` where the last two dimensions are suitable for matrix multiplication. 2. Performs a batched matrix multiplication of `A` and `B`. 3. Ensures numerical stability when dealing with extremal values. 4. Verifies the results against expected tolerance levels considering floating point precision. 5. Optionally allows use of TensorFloat-32 (TF32) for performance on suitable hardware. Function Signature: ```python def batched_matrix_mult(A: torch.Tensor, B: torch.Tensor, use_tf32: bool=False) -> torch.Tensor: ``` Input: - `A` (torch.Tensor): A 3D tensor of shape `(batch_size, m, n)`, where `m` and `n` are the matrix dimensions. - `B` (torch.Tensor): A 3D tensor of shape `(batch_size, n, p)`, where `n` and `p` are the matrix dimensions matching `A`. - `use_tf32` (bool): A flag indicating whether to use TensorFloat-32 for matrix multiplication to leverage hardware acceleration on Nvidia GPUs (optional, default is `False`). Output: - Returns a 3D tensor of shape `(batch_size, m, p)` representing the result of the batched matrix multiplication. Constraints: - Ensure that the function handles large values gracefully without overflow. - The function should provide mechanisms to check and handle precision issues due to floating point arithmetic. - If `use_tf32` is True, ensure TF32 is enabled for the operations. Example: ```python import torch # Example tensors with random values A = torch.rand((10, 100, 200)) B = torch.rand((10, 200, 150)) # Perform batched matrix multiplication result = batched_matrix_mult(A, B) print(result.shape) # Expected output: torch.Size([10, 100, 150]) ``` Additional Details: 1. **Extremal Values Handling**: If any intermediate operation results in `inf` or `NaN`, the function should handle and correct for that. 2. **Precision Handling**: Implement checks to alert about potential precision issues. Comparing results should take into account a tolerance level. 3. **TensorFloat-32 (TF32)**: When `use_tf32=True`, the related settings in PyTorch should be appropriately configured to enable TensorFloat-32 operations. Hints: - Use `torch.isfinite` to check for non-finite values. - Utilize PyTorch settings `torch.backends.cuda.matmul.allow_tf32` to enable/disable TF32. - Perform batched calculations efficiently without explicit Python loops.","solution":"import torch def batched_matrix_mult(A: torch.Tensor, B: torch.Tensor, use_tf32: bool=False) -> torch.Tensor: Perform batched matrix multiplication of A and B, ensuring numerical stability and handling extremal values. Parameters: A (torch.Tensor): A 3D tensor of shape (batch_size, m, n). B (torch.Tensor): A 3D tensor of shape (batch_size, n, p). use_tf32 (bool): Whether to use TensorFloat-32 for matrix multiplication. Returns: torch.Tensor: The result of the batched matrix multiplication of shape (batch_size, m, p). if use_tf32: torch.backends.cuda.matmul.allow_tf32 = True else: torch.backends.cuda.matmul.allow_tf32 = False # Perform batched matrix multiplication C = torch.bmm(A, B) # Replace infinities and NaNs with large finite numbers C = torch.where(torch.isfinite(C), C, torch.full_like(C, 1e20)) return C"},{"question":"**Objective**: Implement a series of functions to demonstrate your understanding of creating and manipulating \\"set\\" objects in Python, while handling errors appropriately. Tasks: 1. **Function `create_set(iterable)`**: - **Input**: An iterable object (could be a list, tuple, or any iterable). - **Output**: A `set` containing the elements of the iterable or an empty set if the input is `None`. - **Constraints**: The function should return `None` if the input is not iterable. - **Example**: ```python create_set([1, 2, 3]) # Output: {1, 2, 3} create_set(None) # Output: set() create_set(123) # Output: None ``` 2. **Function `add_to_set(existing_set, element)`**: - **Input**: A `set` object and an element to add to the set. - **Output**: The set after adding the element, or original set if the element is unhashable. - **Constraints**: The function should return `None` if the input set is not a `set` object. - **Example**: ```python add_to_set({1, 2}, 3) # Output: {1, 2, 3} add_to_set({1, 2}, [3]) # Output: {1, 2} add_to_set(123, 3) # Output: None ``` 3. **Function `remove_from_set(existing_set, element)`**: - **Input**: A `set` object and an element to be removed from the set. - **Output**: The set after removing the element, or the original set if the element is not present. - **Constraints**: The function should return `None` if the input set is not a `set` object. - **Example**: ```python remove_from_set({1, 2, 3}, 2) # Output: {1, 3} remove_from_set({1, 2, 3}, 4) # Output: {1, 2, 3} remove_from_set(123, 2) # Output: None ``` 4. **Function `check_element_in_set(existing_set, element)`**: - **Input**: A `set` object and an element to check in the set. - **Output**: `True` if the element is in the set, `False` otherwise, or `None` if the input set is not a `set` object. - **Constraints**: The function should handle unhashable keys gracefully. - **Example**: ```python check_element_in_set({1, 2, 3}, 2) # Output: True check_element_in_set({1, 2, 3}, 4) # Output: False check_element_in_set({1, 2, 3}, [4]) # Output: False check_element_in_set(123, 2) # Output: None ``` Function Signatures: ```python def create_set(iterable): pass def add_to_set(existing_set, element): pass def remove_from_set(existing_set, element): pass def check_element_in_set(existing_set, element): pass ``` # Constraints: * Your implementations should not use built-in Python set methods directly. * Handle error conditions and invalid inputs as specified. * Ensure the solutions are efficient and make use of the provided API calls where relevant. Grading Criteria: * Correctness of the function implementations. * Error handling as per the constraints. * Code readability and efficiency.","solution":"def create_set(iterable): try: if iterable is None: return set() return set(iterable) except TypeError: return None def add_to_set(existing_set, element): if not isinstance(existing_set, set): return None try: existing_set.add(element) return existing_set except TypeError: return existing_set def remove_from_set(existing_set, element): if not isinstance(existing_set, set): return None existing_set.discard(element) return existing_set def check_element_in_set(existing_set, element): if not isinstance(existing_set, set): return None try: return element in existing_set except TypeError: return False"},{"question":"# Python310 Coding Assessment: Advanced Read-Eval-Print Loop Question You are tasked with extending the capabilities of Python\'s interactive interpreter using the `code` module. Specifically, you need to implement a custom interactive console that has the following features: 1. **Custom Welcome Banner and Exit Message**: When the console starts, it should display a custom welcome banner. When the console exits, it should display a custom exit message. 2. **Command History Logging**: The console should log all successfully executed commands to a list. 3. **Re-evaluating History**: Provide a method to re-evaluate commands from the history. Implement the `CustomInteractiveConsole` class that inherits from `code.InteractiveConsole`. Your implementation should include: - Initialization with custom banner and exit messages. - Overridden `push` method to log commands that are executed successfully. - A `replay_history` method to execute all logged commands again in sequence. The class should have the following structure: ```python import code class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, banner=None, exitmsg=None, locals=None): # Initialize the parent class super().__init__(locals=locals) self.banner = banner self.exitmsg = exitmsg self.command_history = [] def push(self, line): # Override the push method to log commands more = super().push(line) if not more: self.command_history.append(line) return more def replay_history(self): # Method to replay the command history history_copy = self.command_history[:] self.resetbuffer() for command in history_copy: self.push(command) def interact(self): # Override the interact method to use custom banner and exitmsg print(self.banner) super().interact(banner=\\"\\", exitmsg=\\"\\") print(self.exitmsg) ``` Example Usage ```python if __name__ == \\"__main__\\": banner = \\"Welcome to Custom Interactive Console!\\" exitmsg = \\"Goodbye!\\" console = CustomInteractiveConsole(banner=banner, exitmsg=exitmsg) console.interact() ``` Instructions for Implementation 1. **Initialization**: - Store the banner and exitmsg. - Initialize the `command_history` list to keep track of executed commands. - Initialize the parent `InteractiveConsole` class properly. 2. **push method**: - Override the `push(line)` method to call the parent method. - Log the line to `command_history` only if it is successfully executed (determined by the return value of `push`). 3. **replay_history method**: - Execute all commands in `command_history` in sequence. - Ensure the input buffer is reset before replay to avoid mixing with any previous state. 4. **interact method**: - Override the `interact()` method to display custom banner and exit messages. This question tests your understanding of object-oriented programming, method overriding, and interaction with interactive Python console features using advanced concepts provided by the `code` module.","solution":"import code class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, banner=None, exitmsg=None, locals=None): # Initialize the parent class super().__init__(locals=locals) self.banner = banner self.exitmsg = exitmsg self.command_history = [] def push(self, line): # Override the push method to log commands more = super().push(line) if not more: self.command_history.append(line) return more def replay_history(self): # Method to replay the command history history_copy = self.command_history[:] self.resetbuffer() for command in history_copy: self.push(command) def interact(self): # Override the interact method to use custom banner and exitmsg print(self.banner) super().interact(banner=\\"\\", exitmsg=\\"\\") print(self.exitmsg)"},{"question":"**Coding Assessment Question** # Objective Create a function to visualize and compare fMRI signal data using seaborn, incorporating advanced customization features. # Task Write a function called `plot_fmri_signal` that accepts the following parameters: - `event`: A string specifying the event type to filter the fmri data (e.g., \'stim\' or \'cue\'). - `region`: A string specifying the region to filter the fmri data (e.g., \'frontal\' or \'parietal\'). - `hue`: A string specifying the column name to be used for the `hue` semantic in seaborn (e.g., \'event\', \'region\'). - `style`: A string specifying the column name to be used for the `style` semantic in seaborn (e.g., \'event\', \'region\'). - `add_markers`: A boolean indicating whether to add markers to the line plot (default is `False`). Using the provided parameters, the function should: 1. Load the `fmri` dataset using `sns.load_dataset(\\"fmri\\")`. 2. Filter the data according to the provided `event` and `region`. 3. Create a line plot using seaborn\'s `lineplot` function with the `timepoint` as the x-axis and `signal` as the y-axis. 4. Customize the plot based on the `hue`, `style`, and `add_markers` parameters. 5. Display the plot. # Constraints - `event` must be either \'stim\' or \'cue\'. - `region` must be either \'frontal\' or \'parietal\'. # Example ```python def plot_fmri_signal(event, region, hue, style, add_markers=False): import seaborn as sns import matplotlib.pyplot as plt # Your code here # Example usage: plot_fmri_signal(event=\'stim\', region=\'frontal\', hue=\'event\', style=\'event\', add_markers=True) ``` # Expected Output The function should display a seaborn line plot visualizing the fMRI signal data filtered according to the specified event and region, with customizations based on the given parameters. Note: Ensure you handle any potential issues such as filtering invalid combinations of event and region, and properly label the axis and title of the plot for clarity.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_fmri_signal(event, region, hue, style, add_markers=False): Plots the fMRI signal data filtered by the specified event and region, with options for hue, style, and markers. Parameters: event (str): The event type to filter the fmri data (e.g., \'stim\' or \'cue\'). region (str): The region type to filter the fmri data (e.g., \'frontal\' or \'parietal\'). hue (str): The column to be used for the hue semantic in seaborn. style (str): The column to be used for the style semantic in seaborn. add_markers (bool): Whether to add markers to the line plot. Default is False. # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Filter the data according to provided event and region filtered_data = fmri[(fmri[\'event\'] == event) & (fmri[\'region\'] == region)] # Validate the filtered data is non-empty if filtered_data.empty: raise ValueError(f\\"No data available for event \'{event}\' and region \'{region}\'.\\") # Plot the filtered data using seaborn\'s lineplot with the given parameters sns.lineplot(data=filtered_data, x=\'timepoint\', y=\'signal\', hue=hue, style=style, markers=add_markers) # Set plot details plt.title(f\'fMRI Signal for Event: {event.capitalize()} and Region: {region.capitalize()}\') plt.xlabel(\'Timepoint\') plt.ylabel(\'Signal\') plt.show()"},{"question":"You are given a dataset containing information about penguins which needs to be visualized to explore any potential relationships between their physical attributes. Your task is to use the seaborn library to create plots that effectively convey the distribution and comparison between different species. Implement a function `plot_penguin_data` to achieve the following requirements: Function Signature ```python import seaborn.objects as so from seaborn import load_dataset def plot_penguin_data(): pass ``` Requirements 1. **Load the Penguin Dataset**: Use the `load_dataset` function from seaborn to load the penguin dataset. 2. **Plot 1 - Jittered Dot Plot with Default Jitter**: - Plot the `species` on the x-axis and `body_mass_g` on the y-axis. - Apply jitter to the plot using the default settings. 3. **Plot 2 - Jittered Dot Plot with Controlled Jitter**: - Plot the `species` on the x-axis and `body_mass_g` on the y-axis. - Apply jitter to the plot with the `width` parameter set to 0.5. 4. **Plot 3 - Jittered Dot Plot for Numeric Orientation**: - Plot `body_mass_g` on the x-axis and `species` on the y-axis. - Apply jitter to the plot with the `width` parameter set to 0.5. 5. **Plot 4 - Both Axes Jittered Dot Plot**: - Plot the rounded `body_mass_g` (to the nearest 1000) on the x-axis and the rounded `flipper_length_mm` (to the nearest 10) on the y-axis. - Apply jitter to the plot with `x` set to 200 and `y` set to 5. Expected Output - The function should produce four plots following the specifications above. Constraints - Ensure the plots are clear and the jittered points are distinguishable. - Use the appropriate seaborn methods to customize the jitter parameters as specified. Example ```python plot_penguin_data() ``` This example will not directly produce an output but should produce four seaborn plots as per the requirements when executed.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_penguin_data(): # Load the penguin dataset penguins = load_dataset(\'penguins\') # Plot 1: Jittered dot plot with default jitter p1 = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\").add(so.Dot(), so.Jitter()) p1.show() # Plot 2: Jittered dot plot with controlled jitter (width=0.5) p2 = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\").add(so.Dot(), so.Jitter(width=0.5)) p2.show() # Plot 3: Jittered dot plot for numeric orientation (width=0.5) p3 = so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\").add(so.Dot(), so.Jitter(width=0.5)) p3.show() # Plot 4: Both axes jittered dot plot penguins[\'body_mass_g_rounded\'] = penguins[\'body_mass_g\'].round(-3) penguins[\'flipper_length_mm_rounded\'] = penguins[\'flipper_length_mm\'].round(-1) p4 = so.Plot(penguins, x=\\"body_mass_g_rounded\\", y=\\"flipper_length_mm_rounded\\").add(so.Dot(), so.Jitter(x=200, y=5)) p4.show()"},{"question":"# Custom Line-Oriented Command Interpreter Objective Create a custom command interpreter that extends the `cmd.Cmd` class. This interpreter should manage a set of simple commands for managing a todo list. Implementation Details 1. **Create a class** `TodoShell` that inherits from `cmd.Cmd`. 2. **Define the following commands** within the class: - `add [task]`: Adds a new task to the todo list. - `list`: Lists all the tasks in the todo list. - `complete [task_index]`: Marks the task at the given index as complete. - `delete [task_index]`: Deletes the task at the given index. - `update [task_index] [new_task]`: Updates the task at the given index with a new task description. - `quit`: Exits the interpreter. 3. **Define appropriate handlers** for each command. For example, the `do_add` method should handle the `add` command, and similarly for other commands. 4. **Implement error handling** for commands that require arguments. 5. **Include a `Cmd.prompt`** to display `(todo) `. 6. **Handle command parsing and default behaviors** such as the behavior when an empty line is entered or an unrecognized command is issued. Expected Behavior - Users can add tasks and manage their todo list through the custom shell. - Users can quit the shell by typing `quit`. - Proper error messages should be displayed for invalid commands or arguments. Example Session ``` (todo) add Buy groceries (todo) add Call the dentist (todo) list 1. [ ] Buy groceries 2. [ ] Call the dentist (todo) complete 1 (todo) list 1. [x] Buy groceries 2. [ ] Call the dentist (todo) update 2 Schedule dentist appointment (todo) list 1. [x] Buy groceries 2. [ ] Schedule dentist appointment (todo) delete 1 (todo) list 1. [ ] Schedule dentist appointment (todo) quit ``` Additional Notes - Ensure you handle indices appropriately and display relevant error messages if users input invalid indices. - The tasks should be stored in a list where each task is a dictionary with keys `task` and `completed`. Submission Submit the implementation of the `TodoShell` class.","solution":"import cmd class TodoShell(cmd.Cmd): intro = \'Welcome to the TodoShell. Type help or ? to list commands.n\' prompt = \'(todo) \' todo_list = [] def do_add(self, task): \\"Add a new task to the todo list: add [task]\\" if not task: print(\\"Error: task description is required\\") return self.todo_list.append({\\"task\\": task, \\"completed\\": False}) print(f\\"Task added: {task}\\") def do_list(self, arg): \\"List all tasks in the todo list: list\\" for idx, todo in enumerate(self.todo_list, 1): status = \\"[x]\\" if todo[\\"completed\\"] else \\"[ ]\\" print(f\\"{idx}. {status} {todo[\'task\']}\\") def do_complete(self, idx): \\"Mark the task at the given index as complete: complete [task_index]\\" try: idx = int(idx) - 1 if idx < 0 or idx >= len(self.todo_list): raise IndexError self.todo_list[idx][\\"completed\\"] = True print(f\\"Task {idx + 1} marked as complete\\") except (ValueError, IndexError): print(\\"Error: invalid task index\\") def do_delete(self, idx): \\"Delete the task at the given index: delete [task_index]\\" try: idx = int(idx) - 1 if idx < 0 or idx >= len(self.todo_list): raise IndexError removed_task = self.todo_list.pop(idx) print(f\\"Task {idx + 1} deleted: {removed_task[\'task\']}\\") except (ValueError, IndexError): print(\\"Error: invalid task index\\") def do_update(self, args): \\"Update the task at the given index with a new description: update [task_index] [new_task]\\" parts = args.split(\' \', 1) if len(parts) < 2: print(\\"Error: must provide task index and new description\\") return idx, new_task = parts try: idx = int(idx) - 1 if idx < 0 or idx >= len(self.todo_list): raise IndexError old_task = self.todo_list[idx][\\"task\\"] self.todo_list[idx][\\"task\\"] = new_task print(f\\"Task {idx + 1} updated from \'{old_task}\' to \'{new_task}\'\\") except (ValueError, IndexError): print(\\"Error: invalid task index\\") def do_quit(self, arg): \\"Exit the interpreter: quit\\" print(\'Thank you for using TodoShell.\') return True def emptyline(self): pass def default(self, line): print(f\\"Unrecognized command: {line}. Type help or ? to list commands.\\")"},{"question":"# HTTP Status Code Checker You are tasked with writing a function that uses the `http.HTTPStatus` class to return the detailed information of an HTTP status code. Objective: The function `get_http_status_info(code)` should take an integer representing an HTTP status code as its input and return a dictionary with the following keys: - `\\"status_code\\"`: The integer value of the status code. - `\\"name\\"`: The name of the HTTP status (as defined in the `http.HTTPStatus` enum). - `\\"phrase\\"`: The short reason phrase corresponding to the status code. - `\\"description\\"`: A longer description of the status code. If the input `code` is not a valid HTTP status code, the function should raise a `ValueError` with the message `\\"Invalid HTTP status code\\"`. Input: - `code` (int): An integer representing an HTTP status code. Output: - A dictionary with keys `\\"status_code\\"`, `\\"name\\"`, `\\"phrase\\"`, and `\\"description\\"`. Constraints: - The input status code must be an integer between 100 and 511 inclusive, or it is considered invalid. Example: ```python from http import HTTPStatus def get_http_status_info(code): # Your implementation here # Test Cases print(get_http_status_info(200)) # Expected Output: # { # \\"status_code\\": 200, # \\"name\\": \\"OK\\", # \\"phrase\\": \\"OK\\", # \\"description\\": \\"Request fulfilled, document follows\\" # } print(get_http_status_info(404)) # Expected Output: # { # \\"status_code\\": 404, # \\"name\\": \\"NOT_FOUND\\", # \\"phrase\\": \\"Not Found\\", # \\"description\\": \\"Nothing matches the given URI\\" # } print(get_http_status_info(999)) # Expected Output: # ValueError: Invalid HTTP status code ``` Write the `get_http_status_info` function ensuring that it correctly processes and handles various HTTP status codes and invalid inputs.","solution":"from http import HTTPStatus def get_http_status_info(code): Returns detailed information about an HTTP status code. try: status = HTTPStatus(code) return { \\"status_code\\": status.value, \\"name\\": status.name, \\"phrase\\": status.phrase, \\"description\\": status.description, } except ValueError: raise ValueError(\\"Invalid HTTP status code\\")"},{"question":"# Custom Interactive Interpreter Implementation Objective: You are required to implement a custom interactive interpreter using the `code` module. Your interpreter should handle basic Python commands and provide meaningful feedback to the user. This will assess your understanding of dynamic code execution and interaction in Python. Requirements: 1. Implement a class `CustomInterpreter`, which utilizes the `code.InteractiveInterpreter` class. 2. Your `CustomInterpreter` should: - Be able to execute Python code passed as input. - Handle incomplete code gracefully, allowing the user to input additional lines until the code block is complete. - Provide helpful error messages for invalid Python code. - Allow resetting of the interpreter state. Implementation Details: 1. **Initialization**: - Initialize the interpreter. 2. **Methods**: - `execute(self, code: str) -> str`: Accepts a string of Python code, executes it, and returns the output or an error message. - `reset(self)`: Resets the interpreter to clear any defined variables or functions. Input and Output Format: - The `execute` method will accept a string `code` and return a string containing the result of the executed code or an error message. - The `reset` method will not accept any inputs and will reset the interpreter state. Constraints: - The input code string can be multiple lines for multi-line constructs (e.g., functions, loops). - Ensure proper handling of exceptions and provide meaningful messages to the user. Example Usage: ```python interpreter = CustomInterpreter() print(interpreter.execute(\\"a = 10\\")) # Output: \'\' print(interpreter.execute(\\"print(a)\\")) # Output: \'10\' print(interpreter.execute(\\"for i in range(a):n print(i)\\")) # Output: \'0n1n2n3n4n5n6n7n8n9\' print(interpreter.execute(\\"print(b)\\")) # Output: \'NameError: name \'b\' is not defined\' interpreter.reset() print(interpreter.execute(\\"print(a)\\")) # Output: \'NameError: name \'a\' is not defined\' ``` Implement this class in Python to handle the described functionality using the `code` module.","solution":"import sys import traceback from code import InteractiveInterpreter class CustomInterpreter: def __init__(self): self.interpreter = InteractiveInterpreter() self.output = \\"\\" def execute(self, code: str) -> str: old_stdout, old_stderr = sys.stdout, sys.stderr sys.stdout = sys.stderr = self self.output = \\"\\" try: more = self.interpreter.runsource(code, symbol=\\"exec\\") except Exception as e: traceback.print_exc() finally: sys.stdout, sys.stderr = old_stdout, old_stderr return self.output if self.output else f\\"Incomplete input: {more}\\" if more else \\"\\" def write(self, data): self.output += data def reset(self): self.__init__() if __name__ == \\"__main__\\": interpreter = CustomInterpreter() # Example Usage print(interpreter.execute(\\"a = 10\\")) print(interpreter.execute(\\"print(a)\\")) print(interpreter.execute(\\"for i in range(a):n print(i)\\")) print(interpreter.execute(\\"print(b)\\")) interpreter.reset() print(interpreter.execute(\\"print(a)\\"))"},{"question":"Objective The goal of this assessment is to demonstrate your understanding of the Partial Least Squares (PLS) regression technique using scikit-learn. You will implement functions to fit a PLS model and use it for predicting new data. Problem Statement You are required to implement a function that fits a PLSRegression model to given datasets and another function that uses this fitted model to transform and predict new data. Functions to Implement 1. **fit_pls_model**: - **Input**: - `X_train` (2D numpy array): Training data features (n_samples x n_features) - `Y_train` (2D numpy array): Training data targets (n_samples x n_targets) - `n_components` (int): Number of PLS components to use - **Output**: - `pls_model` (PLSRegression): Fitted PLSRegression model 2. **predict_with_pls**: - **Input**: - `pls_model` (PLSRegression): Fitted PLSRegression model - `X_test` (2D numpy array): Test data features (m_samples x n_features) - **Output**: - `Y_pred` (2D numpy array): Predicted data targets (m_samples x n_targets) Example ```python import numpy as np from sklearn.cross_decomposition import PLSRegression # Example input data X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5]]) Y_train = np.array([[2], [3], [4], [5]]) n_components = 2 X_test = np.array([[2, 3], [3, 4]]) # Fit the PLS model pls_model = fit_pls_model(X_train, Y_train, n_components) # Predict using the fitted PLS model Y_pred = predict_with_pls(pls_model, X_test) print(Y_pred) ``` Constraints: - Ensure that you handle any edge cases such as empty input arrays. - You may assume that `n_components` will be a positive integer less than or equal to the smallest dimension (samples or features) of the training data. Use the documentation provided along with scikit-learn\'s documentation for reference. Notes: - `fit_pls_model` function initializes a PLSRegression model with the specified number of components and fits it to the training data. - `predict_with_pls` function uses the fitted model to transform and predict the targets for the test data.","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression def fit_pls_model(X_train, Y_train, n_components): Fits a PLS model to the training data. Args: - X_train (2D numpy array): Training data features (n_samples x n_features) - Y_train (2D numpy array): Training data targets (n_samples x n_targets) - n_components (int): Number of PLS components to use Returns: - pls_model (PLSRegression): Fitted PLSRegression model pls_model = PLSRegression(n_components=n_components) pls_model.fit(X_train, Y_train) return pls_model def predict_with_pls(pls_model, X_test): Predicts targets using a fitted PLS model. Args: - pls_model (PLSRegression): Fitted PLSRegression model - X_test (2D numpy array): Test data features (m_samples x n_features) Returns: - Y_pred (2D numpy array): Predicted data targets (m_samples x n_targets) Y_pred = pls_model.predict(X_test) return Y_pred"},{"question":"# Tuples and Struct Sequence in Python310 - Coding Assessment In this assessment, you are expected to create and manipulate Python tuple objects using the `python310` functionalities described in the provided documentation. The tasks involve several operations including creating tuples, accessing elements, slicing, and resizing. **Task Details:** 1. **Tuple Creation**: Write a Python function `create_tuple` that takes a list of elements and returns a tuple. ```python def create_tuple(elements: list) -> tuple: pass ``` 2. **Tuple Element Access**: Write a Python function `get_tuple_item` that takes a tuple and an index, and returns the value at that index. If the index is out of bounds, return `None`. ```python def get_tuple_item(t: tuple, index: int): pass ``` 3. **Tuple Slicing**: Write a Python function `slice_tuple` that takes a tuple, a starting index, and an ending index, and returns the sliced tuple. Ensure that the indices are within the bounds of the tuple. ```python def slice_tuple(t: tuple, start: int, end: int) -> tuple: pass ``` 4. **Tuple Resizing** (Advanced): Python tuples are immutable. Simulate resizing a tuple by creating a new tuple with the desired size filled with elements from the original tuple. If the new size is larger, pad the tuple with `None`. Write a Python function `resize_tuple` that takes a tuple and a new size, and returns the resized tuple. ```python def resize_tuple(t: tuple, new_size: int) -> tuple: pass ``` **Constraints:** - Assume all elements in the initial list for `create_tuple` are valid Python objects that can be part of a tuple. - For `get_tuple_item` and `slice_tuple`, handle out-of-bounds indices gracefully. - For `resize_tuple`, ensure the resulting tuple maintains the order of elements from the original tuple as closely as possible. **Examples:** ```python # Example for create_tuple print(create_tuple([1, 2, 3, 4])) # Output: (1, 2, 3, 4) # Example for get_tuple_item print(get_tuple_item((1, 2, 3), 1)) # Output: 2 print(get_tuple_item((1, 2, 3), 5)) # Output: None # Example for slice_tuple print(slice_tuple((1, 2, 3, 4), 1, 3)) # Output: (2, 3) print(slice_tuple((1, 2, 3, 4), 2, 10)) # Output: (3, 4) # Example for resize_tuple print(resize_tuple((1, 2, 3), 5)) # Output: (1, 2, 3, None, None) print(resize_tuple((1, 2, 3), 2)) # Output: (1, 2) ``` Implement these functions considering the guidelines and constraints provided.","solution":"def create_tuple(elements: list) -> tuple: Create a tuple from a list of elements. return tuple(elements) def get_tuple_item(t: tuple, index: int): Get an item from the tuple at the specified index. If index is out of bounds, return None. try: return t[index] except IndexError: return None def slice_tuple(t: tuple, start: int, end: int) -> tuple: Return a sliced portion of the tuple from start to end (exclusive). Ensure the indices are within the bounds of the tuple. return t[start:end] def resize_tuple(t: tuple, new_size: int) -> tuple: Resize a tuple by either truncating or padding with None. if new_size < len(t): return t[:new_size] else: return t + (None,) * (new_size - len(t))"},{"question":"Objective: Demonstrate your understanding of dynamic type creation and manipulation using the `types` module in Python. Problem Statement: You are required to create a dynamic class using the `types` module. The class should have the following properties and methods: 1. The class should be named **`DynamicPerson`**. 2. The class should inherit from a provided `BaseClass`. 3. The class should have the following attributes: - `name`: A string representing the name of the person. - `age`: An integer representing the age of the person. 4. The class should have a method `greet` that returns a string in the format `\\"Hello, my name is {name} and I am {age} years old.\\"`. Instructions: 1. Use `types.new_class` to create the **`DynamicPerson`** class. 2. Use an `exec_body` function to populate the class namespace with the required attributes and methods. 3. Ensure that the class dynamically inherits from the provided `BaseClass`. Constraints: - `name` should be a non-empty string. - `age` should be a non-negative integer. Input: 1. A `BaseClass` from which `DynamicPerson` should inherit. 2. The `name` and `age` of a person. Output: - An instance of the dynamically created `DynamicPerson` class. - The output of the `greet` method when called on the instance. Example: ```python class BaseClass: def info(self): return \\"I am the base class.\\" def create_dynamic_person_class(BaseClass, name, age): # Your implementation goes here # Example usage: DynamicPerson = create_dynamic_person_class(BaseClass, \\"Alice\\", 30) person = DynamicPerson() assert person.greet() == \\"Hello, my name is Alice and I am 30 years old.\\" ``` Implementation: ```python import types def create_dynamic_person_class(BaseClass, name, age): if not isinstance(name, str) or not name: raise ValueError(\\"Name must be a non-empty string.\\") if not isinstance(age, int) or age < 0: raise ValueError(\\"Age must be a non-negative integer.\\") def exec_body(ns): def greet(self): return f\\"Hello, my name is {self.name} and I am {self.age} years old.\\" ns[\'name\'] = name ns[\'age\'] = age ns[\'greet\'] = greet DynamicPerson = types.new_class(\'DynamicPerson\', (BaseClass,), {}, exec_body) return DynamicPerson ``` Use this template to ensure your solution meets the criteria and correctly utilizes the `types` module for dynamic class creation.","solution":"import types def create_dynamic_person_class(BaseClass, name, age): if not isinstance(name, str) or not name: raise ValueError(\\"Name must be a non-empty string.\\") if not isinstance(age, int) or age < 0: raise ValueError(\\"Age must be a non-negative integer.\\") def exec_body(ns): def greet(self): return f\\"Hello, my name is {self.name} and I am {self.age} years old.\\" ns[\'name\'] = name ns[\'age\'] = age ns[\'greet\'] = greet DynamicPerson = types.new_class(\'DynamicPerson\', (BaseClass,), {}, exec_body) return DynamicPerson"},{"question":"Objective: Implement a Python script using the `pty` module that creates a pseudo-terminal communication interface between two processes. Your script should demonstrate the following functionalities: 1. **Process forking**: Use `pty.fork()` to create a child process. 2. **Pseudo-terminal creation**: Use `pty.openpty()` to open a pseudo-terminal pair. 3. **Spawning a sub-process**: Use `pty.spawn()` to run a simple interactive shell (like `sh`) and communicate with it programmatically. Requirements: 1. Your script should fork a process. The parent process should print the PID of the child and then read from and write to the child\'s controlling terminal. 2. The parent process should send a series of predefined commands (e.g., listing directory contents, printing the working directory) to the child process. 3. Capture and print the output from the child process to the parent\'s standard output. Input/Output Specifications: - **Input**: - A list of shell commands (string) to be sent to the child process. - **Output**: - The script should print output from each shell command executed in the child process. Constraints: - **Platform**: Your solution should run on Unix-like operating systems (e.g., Linux, macOS). It is not expected to work on Windows. - **Python Version**: Use Python 3.10 or later. Example: ```python import os import pty import time commands = [\'ls\', \'pwd\', \'echo \\"Hello world\\"\'] def main(): pid, fd = pty.fork() if pid == 0: # Child process os.execlp(\'sh\', \'sh\') else: # Parent process print(f\'Child PID: {pid}\') for command in commands: os.write(fd, command.encode() + b\'n\') time.sleep(1) # Give the child process time to execute the command output = os.read(fd, 1024) print(output.decode(), end=\'\') os.close(fd) if __name__ == \\"__main__\\": main() ``` In your implementation, make sure the script handles: - Correctly forking the process and setting up the pseudo-terminal. - Ensuring commands are sent and received properly between the parent and child process. - Properly closing the file descriptors once the operations are complete. Additionally, include comments explaining each step of the process clearly.","solution":"import os import pty import time def main(): # Define the list of commands to execute commands = [\'ls\', \'pwd\', \'echo \\"Hello world\\"\'] # Fork the process try: pid, fd = pty.fork() except OSError as e: print(f\\"Failed to fork the process: {e}\\") return if pid == 0: # Child process, replace with a shell os.execlp(\'sh\', \'sh\') else: # Parent process print(f\'Child PID: {pid}\') for command in commands: print(f\\"Running command: {command}\\") # Write the command to the child\'s terminal os.write(fd, command.encode() + b\'n\') time.sleep(1) # Give the child process time to execute the command # Read the output from the child process try: output = os.read(fd, 1024) print(output.decode(), end=\'\') except OSError as e: print(f\\"Error reading from fd: {e}\\") break # Close the file descriptor os.close(fd) if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question** You are provided with a dataset containing information about daily sea ice extent from 1980 to 2019, and a separate dataset containing brain signal measurements for different events over time. Your task is to create a visualization that demonstrates the usage of seaborn\'s `objects` module to plot specific data relations. # Objective: 1. Prepare the sea ice extent dataset for visualization. 2. Create and plot a band showing the interval between sea ice extents of 1980 and 2019. 3. Add a line plot to show aggregate signal measurements over time from the fmri dataset, differentiating by the event type. # Guidelines: **Input:** You are given two datasets: - **Sea ice extent dataset** with columns `Date` and `Extent`. - **Fmri dataset** with columns including `timepoint`, `region`, `event`, `signal`, and `subject`. Use seaborn’s built-in `load_dataset` method to load these datasets. **Output:** A visualization fulfilling the following criteria: 1. The x-axis represents the day of the year for the sea ice extent dataset. 2. The y-axis interval band shows ice extent from the year 1980 to the year 2019. 3. Overlay a line plot representing aggregated fmri signal measurements over time, colored by event type, along with a band representing the error interval. # Constraints: - Ensure that the seaborn library is used solely for all plotting functionalities. - Maintain clarity and visualization best practices for readability. # Code Scaffold: ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Load the datasets seaice = load_dataset(\\"seaice\\") fmri = load_dataset(\\"fmri\\") # Prepare the sea ice dataset for visualization seaice_data = ( seaice .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) # Filter the fmri dataset for the \'parietal\' region fmri_data = fmri.query(\\"region == \'parietal\'\\") # Initialize the Plot objects seaice_plot = so.Plot(seaice_data, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") seaice_plot.add(so.Band(alpha=.5, edgewidth=2)) fmri_plot = ( so.Plot(fmri_data, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) # Display both plots seaice_plot.show() fmri_plot.show() ``` **Expected Result:** A seaborn plot containing: 1. A band view of sea ice extent between 1980 and 2019 over the days of the year. 2. A line plot with an error band showing fmri signal measurements over time, differentiated by event.","solution":"import seaborn as sns import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Load the datasets seaice = load_dataset(\\"seaice\\") fmri = load_dataset(\\"fmri\\") # Prepare the sea ice dataset for visualization seaice[\\"Date\\"] = pd.to_datetime(seaice[\\"Date\\"]) seaice_data = ( seaice .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) # Filter the fmri dataset for the \'parietal\' region fmri_data = fmri.query(\\"region == \'parietal\'\\") # Initialize the Plot objects seaice_plot = so.Plot(seaice_data, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") seaice_plot.add(so.Band(alpha=.5, edgewidth=2)) fmri_plot = ( so.Plot(fmri_data, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) # Display both plots seaice_plot.show() fmri_plot.show()"},{"question":"**Question: Implementing a Timeout Mechanism for a Function Execution** As a developer, you are often required to ensure that certain operations in your program do not exceed a specified time limit, especially when dealing with I/O operations or interactions with external systems. Using the `signal` module in Python, implement a function `execute_with_timeout` that runs another function taking possibly multiple arguments and terminates if it exceeds a specified time limit. **Function Signature:** ```python def execute_with_timeout(func, args=(), timeout=5): pass ``` **Parameters:** - `func` (callable): The function to be executed with a timeout. - `args` (tuple): The arguments to be passed to the function. Default is an empty tuple. - `timeout` (int): The maximum time (in seconds) allowed for the function execution. Default is 5 seconds. **Returns:** - The return value of the function `func` if it completes within the specified timeout. **Raises:** - `TimeoutError`: If the function execution exceeds the specified timeout. **Constraints:** - The function should be capable of handling any exceptions thrown by the `func` and must clean up appropriately after the timeout. - Ensure the signal handling is properly reset after the function execution or timeout. **Example:** ```python import time def example_function(duration): time.sleep(duration) return \\"Function Completed\\" try: result = execute_with_timeout(example_function, args=(3,), timeout=2) print(result) except TimeoutError: print(\\"Execution exceeded the time limit\\") try: result = execute_with_timeout(example_function, args=(1,), timeout=2) print(result) # Output: Function Completed except TimeoutError: print(\\"Execution exceeded the time limit\\") ``` **Task:** Implement the `execute_with_timeout` function with the above specifications. Your implementation should demonstrate the use of the `signal` module to handle the timeout effectively.","solution":"import signal def execute_with_timeout(func, args=(), timeout=5): Execute a function with a timeout mechanism. :param func: Callable to execute :param args: Arguments for the callable :param timeout: Timeout in seconds :return: The return value of the function if it completes in time :raises TimeoutError: If the function execution exceeds the specified timeout def handler(signum, frame): raise TimeoutError(\\"Function execution exceeded the timeout limit\\") # Set the signal handler and a timeout alarm signal.signal(signal.SIGALRM, handler) signal.alarm(timeout) try: result = func(*args) finally: # Cancel the alarm signal.alarm(0) return result"},{"question":"Efficient Line Retrieval from Large Source Files Problem Statement: You are given a large text file, where lines of interest are highly scattered and frequent access to specific lines is required. To manage this efficiently, you decide to use the `linecache` module to retrieve these lines. Your task is to implement a function `retrieve_lines(filename: str, lines: List[int]) -> List[str]` that returns the specified lines from the given file. Function Signature: ```python def retrieve_lines(filename: str, lines: list[int]) -> list[str]: pass ``` Input: - `filename`: A string representing the path to the text file. - `lines`: A list of integers where each integer represents a line number to retrieve from the file. Output: - A list of strings, where each string corresponds to the content of the line numbers specified in the `lines` list, maintaining the input order. Constraints: - The file can be very large (up to 1 GB). - Line numbers in the `lines` list are 1-based and can be in any order. Assume all specified lines exist in the file. - Optimal performance is required for both retrieving lines and handling large files. Example Usage: Assume `example.txt` file content is: ``` Line1 Line2 Line3 Line4 Line5 ``` ```python retrieve_lines(\'example.txt\', [1, 3, 5]) # Output: [\'Line1n\', \'Line3n\', \'Line5n\'] ``` Notes: - Utilize the `linecache.getline` function to retrieve lines from the file efficiently. - Ensure that your implementation handles file reading optimally and uses caching to minimize I/O operations. - Do not forget to clear the cache once the operation is complete to avoid unnecessary memory usage. Additional Constraints: - Do not read the entire file into memory. - The function should handle invalid/empty `lines` lists gracefully, returning an empty list. Implement the function in the provided function signature.","solution":"import linecache def retrieve_lines(filename: str, lines: list[int]) -> list[str]: Returns the specified lines from the given file as a list. Args: - filename: str - Path to the file. - lines: list[int] - List of line numbers to retrieve (1-based indexing). Returns: - list[str] - The lines retrieved from the file. result = [] for line_number in lines: # Retrieve the specific line from the file. line = linecache.getline(filename, line_number) if line: result.append(line) # Clear the cache to free up memory. linecache.clearcache() return result"},{"question":"# Distributed Training with PyTorch You are required to implement and test a distributed training function using PyTorch\'s `torch.distributed` package. The task will involve initializing a process group, defining a simple neural network model, performing collective operations, and training the model across multiple GPUs. Requirements: 1. **Initialize Process Group**: - Use the environment variable initialization method for the process group. 2. **Define and Distribute Model**: - Define a simple neural network (e.g., a two-layer MLP with ReLU activation). - Wrap the model with `torch.nn.parallel.DistributedDataParallel` for distributed training. 3. **Implement Distributed Training Loop**: - Implement a simple training loop that performs forward passes, computes loss using Mean Squared Error (MSE) loss, and updates model parameters using stochastic gradient descent (SGD). - Use `dist.all_reduce` to average the loss across all processes. 4. **Ensure Proper Cleanup**: - Clean up the process group at the end using `torch.distributed.destroy_process_group`. Constraints: - The solution should work for a 2-GPU system. - Synchronize all processes using `torch.distributed.barrier()` to ensure all processes reach certain points concurrently. Input: You can assume that the necessary environment variables (`MASTER_ADDR`, `MASTER_PORT`, `WORLD_SIZE`, `RANK`) are set externally before running the script. Output: The script should print the averaged loss at each training iteration. # Example Template ```python import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp def init_process(rank, world_size, fn, backend=\'nccl\'): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' os.environ[\'WORLD_SIZE\'] = str(world_size) os.environ[\'RANK\'] = str(rank) dist.init_process_group(backend, rank=rank, world_size=world_size) fn(rank, world_size) dist.destroy_process_group() class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 10) self.relu = nn.ReLU() self.fc2 = nn.Linear(10, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def train(rank, world_size): torch.manual_seed(0) device = torch.device(f\'cuda:{rank}\') model = SimpleNN().to(device) ddp_model = nn.parallel.DistributedDataParallel(model, device_ids=[rank]) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) for epoch in range(10): inputs = torch.randn(20, 10).to(device) targets = torch.randn(20, 1).to(device) optimizer.zero_grad() outputs = ddp_model(inputs) loss = loss_fn(outputs, targets) loss.backward() optimizer.step() reduced_loss = loss.clone() dist.all_reduce(reduced_loss, op=dist.ReduceOp.SUM) reduced_loss /= world_size print(f\'Rank {rank}, Epoch {epoch}, Loss: {reduced_loss.item()}\') dist.barrier() if __name__ == \\"__main__\\": world_size = 2 mp.spawn(init_process, args=(world_size, train), nprocs=world_size, join=True) ``` Ensure you have CUDA-enabled GPUs and the necessary PyTorch dependencies installed to test the script.","solution":"import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp def init_process(rank, world_size, fn, backend=\'nccl\'): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' os.environ[\'WORLD_SIZE\'] = str(world_size) os.environ[\'RANK\'] = str(rank) dist.init_process_group(backend, rank=rank, world_size=world_size) fn(rank, world_size) dist.destroy_process_group() class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 10) self.relu = nn.ReLU() self.fc2 = nn.Linear(10, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def train(rank, world_size): torch.manual_seed(0) device = torch.device(f\'cuda:{rank}\') model = SimpleNN().to(device) ddp_model = nn.parallel.DistributedDataParallel(model, device_ids=[rank]) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) for epoch in range(10): inputs = torch.randn(20, 10).to(device) targets = torch.randn(20, 1).to(device) optimizer.zero_grad() outputs = ddp_model(inputs) loss = loss_fn(outputs, targets) loss.backward() optimizer.step() reduced_loss = loss.clone() dist.all_reduce(reduced_loss, op=dist.ReduceOp.SUM) reduced_loss /= world_size print(f\'Rank {rank}, Epoch {epoch}, Loss: {reduced_loss.item()}\') dist.barrier() if __name__ == \\"__main__\\": world_size = 2 mp.spawn(init_process, args=(world_size, train), nprocs=world_size, join=True)"},{"question":"Objective: Demonstrate your understanding of seaborn\'s `cubehelix_palette` function by creating and customizing a color palette, then applying it to a seaborn plot. Task: 1. **Color Palette Creation:** - Create a color palette using the `cubehelix_palette` function with the following parameters: - 10 colors in the palette. - Start the helix at 1. - Rotate the helix by -0.5. - Use a nonlinear luminance ramp with gamma=0.8. - Increase the saturation (hue) to 2. - Ensure the palette is a discrete palette, not a continuous colormap. 2. **Plot Creation:** - Use the generated palette to create a scatter plot using seaborn (sns.scatterplot) with the `iris` dataset that comes with seaborn. - The x-axis should be \\"sepal_length\\" and the y-axis should be \\"sepal_width\\". - Color the points based on the \\"species\\" column using the custom color palette. Input and Output Formats: - **Input:** - None. Use the seaborn `iris` dataset. - **Output:** - The output should be a scatter plot displayed within the Jupyter notebook. Constraints: - The function should be named `create_custom_palette_and_plot()`. - The function should not take any parameters. Example: The function signature should look like: ```python def create_custom_palette_and_plot(): # Your implementation here ``` When the function is called, it should display the scatter plot meeting the specified requirements. Notes: - Make sure to set the theme using `sns.set_theme()` for consistency in plot styling. - The scatter plot should clearly differentiate the species using the customized color palette.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette_and_plot(): Creates a cubehelix color palette and applies it to a Seaborn scatter plot using the iris dataset. # Create the cubehelix palette with specific parameters palette = sns.cubehelix_palette( 10, start=1, rot=-0.5, gamma=0.8, hue=2, dark=0, light=1, reverse=False ) # Load iris dataset iris = sns.load_dataset(\\"iris\\") # Set the Seaborn theme sns.set_theme() # Create the scatter plot sns.scatterplot( x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\", palette=palette, data=iris ) # Show the plot plt.show()"},{"question":"**Email MIME Message Construction** You are tasked with creating an email message that includes text and multiple types of attachments (e.g., images, application data). Your function should take input data for the different parts of the email and return a complete email message object. # Requirements: 1. Construct a multipart email that includes: - A plain text message in the email body. - An image attachment. - An application attachment (e.g., a PDF file). 2. Your function should be named `create_mime_email`. 3. The function should accept the following inputs: - `plain_text`: A string representing the plain text message. - `image_data`: A dictionary with keys `data` (bytes) and `subtype` (e.g., \\"jpeg\\"). - `app_data`: A dictionary with keys `data` (bytes) and `subtype` (e.g., \\"pdf\\"). 4. The function should return the MIME email object constructed using the relevant `email.mime` classes. # Constraints: - Ensure that all parts of the email are correctly encoded and that appropriate headers are added. - Use Base64 encoding for the image and application data. - The main email type should be \\"multipart/mixed\\". # Function Signature: ```python def create_mime_email(plain_text: str, image_data: dict, app_data: dict) -> email.mime.multipart.MIMEMultipart: pass ``` # Example Usage: ```python plain_text = \\"This is the body of the email.\\" image_data = { \\"data\\": b\\"image bytes here...\\", \\"subtype\\": \\"jpeg\\" } app_data = { \\"data\\": b\\"%PDF-1.4 PDF data here...\\", \\"subtype\\": \\"pdf\\" } mime_email = create_mime_email(plain_text, image_data, app_data) print(mime_email.as_string()) # This will output the full raw email message as a string ``` The function should build a MIME email with the specified components and return a MIME email object ready for sending.","solution":"import email.mime.multipart import email.mime.text import email.mime.image import email.mime.application from email import encoders def create_mime_email(plain_text: str, image_data: dict, app_data: dict) -> email.mime.multipart.MIMEMultipart: # Create the container email message msg = email.mime.multipart.MIMEMultipart() msg[\'Subject\'] = \\"Sample Email with Attachments\\" msg[\'From\'] = \\"sender@example.com\\" msg[\'To\'] = \\"recipient@example.com\\" # Attach the plain text message text_part = email.mime.text.MIMEText(plain_text, \'plain\') msg.attach(text_part) # Attach the image image_part = email.mime.image.MIMEImage(image_data[\'data\'], _subtype=image_data[\'subtype\']) image_part.add_header(\\"Content-Disposition\\", \\"attachment\\", filename=\\"image.\\" + image_data[\'subtype\']) encoders.encode_base64(image_part) msg.attach(image_part) # Attach the application data (e.g., PDF) application_part = email.mime.application.MIMEApplication(app_data[\'data\'], _subtype=app_data[\'subtype\']) application_part.add_header(\\"Content-Disposition\\", \\"attachment\\", filename=\\"document.\\" + app_data[\'subtype\']) encoders.encode_base64(application_part) msg.attach(application_part) return msg"},{"question":"You are tasked with creating a Python function that leverages the `mailcap` module to find and execute a command for a given MIME type. Your function should perform the following steps: 1. Retrieve the mailcap capabilities from the system mailcap files and the user\'s mailcap file. 2. Find and validate a command for the provided MIME type. 3. Execute the command if a valid entry is found, otherwise return an appropriate error message. # Function Signature ```python def execute_mime_type_command(mime_type: str, filename: str, key: str = \'view\', plist: list = []) -> str: ``` # Input - `mime_type` (str): The MIME type to find a command for (e.g., \'video/mpeg\'). - `filename` (str): The filename to replace `%s` in the command line. - `key` (str): Optional; the desired field in the mailcap entry (default is \'view\'). - `plist` (list): Optional; named parameters for substitution in the command line (default is an empty list). # Output - (str): The output of the executed command, or an error message if no valid command is found. # Constraints 1. If no valid mailcap entry is found for the given MIME type, return `\\"Error: No valid entry found for MIME type\\"`. 2. If the command contains disallowed characters leading to an invalid security check, return `\\"Error: Security issue detected with provided inputs\\"`. 3. You may assume the system has the necessary mailcap files and programs installed to handle the MIME type. 4. The function should handle and raise appropriate warnings and exceptions as described in the `mailcap` documentation. # Example Usage ```python import mailcap def execute_mime_type_command(mime_type: str, filename: str, key: str = \'view\', plist: list = []) -> str: caps = mailcap.getcaps() command, entry = mailcap.findmatch(caps, mime_type, key=key, filename=filename, plist=plist) if command is None: return \\"Error: No valid entry found for MIME type\\" try: import subprocess result = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) if result.returncode != 0: return f\\"Error: Command failed with message: {result.stderr}\\" return result.stdout except Exception as e: return f\\"Error: Execution failed with exception: {str(e)}\\" # Example print(execute_mime_type_command(\'video/mpeg\', \'tmp1223\')) ``` In this example, the function retrieves the mailcap capabilities and tries to find and execute a command for \'video/mpeg\'. If it succeeds, the command\'s output will be returned; otherwise, appropriate error messages will be returned.","solution":"import mailcap import subprocess def execute_mime_type_command(mime_type: str, filename: str, key: str = \'view\', plist: list = []) -> str: Retrieve and execute a command for a given MIME type using the mailcap module. # Retrieve mailcap capabilities from the system caps = mailcap.getcaps() # Find the matching command for the MIME type command, entry = mailcap.findmatch(caps, mime_type, key=key, filename=filename, plist=plist) if command is None: return \\"Error: No valid entry found for MIME type\\" # Security check for disallowed characters # This could be expanded to check for other potential security issues. disallowed_chars = [\\";\\", \\"&&\\", \\"|\\", \\"`\\"] if any(char in command for char in disallowed_chars): return \\"Error: Security issue detected with provided inputs\\" # Execute the command using subprocess try: result = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) if result.returncode != 0: return f\\"Error: Command failed with message: {result.stderr}\\" return result.stdout except Exception as e: return f\\"Error: Execution failed with exception: {str(e)}\\""},{"question":"Objective You are required to demonstrate your understanding of fundamental and advanced concepts of the seaborn library by creating and customizing scatter plots using a given dataset. This question will test your ability to manipulate data and create informative visualizations. Problem Statement 1. Import necessary libraries and load the `tips` dataset from seaborn. 2. Create a new column in the dataset named `tip_rate`, which is the ratio of `tip` to `total_bill`. 3. Create a scatter plot between `total_bill` and `tip`: - Use `tip_rate` to color the points (`hue`). - Use the `size` variable to control the size of the points. - Use different marker styles for different times of day (`Lunch` and `Dinner`). 4. Split the dataset into two facets based on the `time` of the day and create the scatter plot for each facet: - Each facet should show the relationship between `total_bill` and `tip`. - Color the points by `day`. - Use different marker styles for each `day`. 5. Save the final figure as a PNG file named `tips_scatter_plot.png`. Input and Output Formats - **Input**: No user input is required; the `tips` dataset will be loaded from seaborn. - **Output**: A PNG file named `tips_scatter_plot.png` containing the scatter plots. Constraints - Ensure that the plots are clearly readable and visually appealing. - Use appropriate labels and legends to make the plots informative. Example: Here is a brief example (not exhaustive) of what the final plots should capture: - Main scatter plot between `total_bill` and `tip` with `tip_rate` as color and `size` as point size. - Facet plots demonstrating the variation in tips based on time (`Lunch` and `Dinner`) and colored/styled by day. ```python import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create tip_rate column tips[\'tip_rate\'] = tips[\'tip\'] / tips[\'total_bill\'] # Create the main scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"tip_rate\\", size=\\"size\\", style=\\"time\\", palette=\\"viridis\\", sizes=(20, 200)) plt.title(\'Scatter plot of Total Bill vs Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') # Save the plot plt.savefig(\'main_scatter_plot.png\') # Create facet plots g = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\", palette=\\"deep\\", sizes=(20, 200) ) g.set_axis_labels(\\"Total Bill\\", \\"Tip\\") g.add_legend() # Save the facet plot figure plt.savefig(\'tips_scatter_plot.png\') # Show the plots plt.show() ``` Ensure that the code is correctly implemented to meet all the requirements specified in the problem statement.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_scatter_plots_and_save(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a new column \'tip_rate\' which is the ratio of tip to total_bill tips[\'tip_rate\'] = tips[\'tip\'] / tips[\'total_bill\'] # Main scatter plot between total_bill and tip plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"tip_rate\\", size=\\"size\\", style=\\"time\\", palette=\\"viridis\\", sizes=(20, 200)) scatter_plot.set_title(\'Scatter plot of Total Bill vs Tip\') scatter_plot.set_xlabel(\'Total Bill\') scatter_plot.set_ylabel(\'Tip\') # Save the main scatter plot plt.savefig(\'main_scatter_plot.png\') plt.close() # Facet scatter plots based on time facet_plot = sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\", palette=\\"deep\\", sizes=(20, 200)) facet_plot.set_axis_labels(\\"Total Bill\\", \\"Tip\\") facet_plot.add_legend() # Save the facet scatter plot figure facet_plot.savefig(\'tips_scatter_plot.png\') plt.close() return \'main_scatter_plot.png\', \'tips_scatter_plot.png\'"},{"question":"# Bytearray Manipulation Challenge In this challenge, you are required to implement functions that demonstrate your understanding of bytearray manipulation using the provided Python bytearray object functions. You will implement three functions that create, manipulate, and query bytearrays. Function 1: `create_bytearray` Create a new bytearray from a given string and return it. **Input:** - A string `input_string`. **Output:** - A bytearray object created from the input string. ```python def create_bytearray(input_string: str) -> bytearray: pass ``` Function 2: `concat_bytearrays` Concatenate two bytearrays and return the result. **Input:** - Two bytearrays, `bytearray1` and `bytearray2`. **Output:** - A new bytearray resulting from concatenating `bytearray1` and `bytearray2`. ```python def concat_bytearrays(bytearray1: bytearray, bytearray2: bytearray) -> bytearray: pass ``` Function 3: `resize_bytearray` Resize a given bytearray to a new size. **Input:** - A bytearray `input_bytearray`. - An integer `new_size` indicating the new size for the bytearray. **Output:** - The resized bytearray object. **Constraints:** - The new size should be non-negative. - If the new size is smaller than the current size, the bytearray will be truncated, and if larger, it will be padded with null bytes. ```python def resize_bytearray(input_bytearray: bytearray, new_size: int) -> bytearray: pass ``` # Instructions 1. Implement the three functions as described above. 2. Do not use any external libraries; only use the functions and macros as described in the provided documentation. 3. Ensure the functions handle edge cases, such as resizing to zero. 4. Write tests to verify your functions work as expected. # Examples ```python # Example for create_bytearray bytearray1 = create_bytearray(\\"hello\\") print(bytearray1) # Output: bytearray(b\'hello\') # Example for concat_bytearrays bytearray2 = create_bytearray(\\"world\\") concatenated = concat_bytearrays(bytearray1, bytearray2) print(concatenated) # Output: bytearray(b\'helloworld\') # Example for resize_bytearray resized = resize_bytearray(concatenated, 5) print(resized) # Output: bytearray(b\'hello\') # Resizing to a larger size resized = resize_bytearray(concatenated, 15) print(resized) # Output: bytearray(b\'helloworldx00x00x00x00x00\') ``` **Hint**: Refer to the provided documentation to use the appropriate API functions and macros for manipulating bytearrays.","solution":"def create_bytearray(input_string: str) -> bytearray: Create a new bytearray from a given string. Args: input_string (str): The string to convert to a bytearray. Returns: bytearray: The bytearray created from the input string. return bytearray(input_string, \'utf-8\') def concat_bytearrays(bytearray1: bytearray, bytearray2: bytearray) -> bytearray: Concatenate two bytearrays and return the result. Args: bytearray1 (bytearray): The first bytearray. bytearray2 (bytearray): The second bytearray. Returns: bytearray: A new bytearray resulting from concatenating bytearray1 and bytearray2. return bytearray1 + bytearray2 def resize_bytearray(input_bytearray: bytearray, new_size: int) -> bytearray: Resize a given bytearray to a new size. Args: input_bytearray (bytearray): The original bytearray. new_size (int): The new size for the bytearray. Returns: bytearray: The resized bytearray object. if new_size < 0: raise ValueError(\\"new_size must be non-negative\\") input_bytearray.extend([0] * (new_size - len(input_bytearray))) del input_bytearray[new_size:] return input_bytearray"},{"question":"# PyTorch MPS Device and Profiler Management Objective You are required to implement a function that performs a series of computations on an MPS device, efficiently manages memory usage, and profiles the performance of the operations. Instructions 1. **Function Name**: `mps_computation_profile` 2. **Input**: - `layers` (int): The number of matrix multiplications to perform. - `matrix_size` (int): The size of the square matrices to be multiplied. 3. **Output**: - A dictionary containing: - `allocated_memory` (int): The amount of memory allocated before starting the computations. - `driver_memory` (int): The amount of memory allocated by the driver after the computations. - `profile_result`: The profile data collected during the computations. Constraints 1. Ensure that the function uses MPS devices and memory management functions as provided in the `torch.mps` module. 2. The function should: - Start by synchronizing the device. - Retrieve and report the initially allocated memory. - Perform the specified number of matrix multiplications. - Use appropriate memory management techniques before, during, and after computations. - Profile the operations and return the captured profile data. - Clean up, free unused memory, and synchronize the device at the end. Performance Requirement The implementation should be efficient and make good use of MPS capabilities. The profiling results should provide insights into memory usage during the computations. Example Usage ```python result = mps_computation_profile(layers=5, matrix_size=1024) ``` Expected output format: ```python { \'allocated_memory\': 1048576, # Example value in bytes \'driver_memory\': 2097152, # Example value in bytes \'profile_result\': <ProfileData> # Placeholder for actual profile data } ``` Note: You may assume the availability of necessary PyTorch and MPS libraries in the execution environment. Your function should handle situations where MPS devices are not available gracefully.","solution":"import torch import torch.mps import torch.profiler def mps_computation_profile(layers, matrix_size): # Check for MPS availability if not torch.backends.mps.is_available(): raise EnvironmentError(\\"MPS device not available\\") torch.mps.synchronize() # Retrieve initial memory allocations initial_allocated_memory = torch.mps.memory_allocated() initial_driver_memory = torch.mps.driver_allocated_memory() matrices = [] profiler_results = [] with torch.profiler.profile( activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.MPS], with_stack=True ) as profiler: for _ in range(layers): A = torch.randn(matrix_size, matrix_size, device=\'mps\') B = torch.randn(matrix_size, matrix_size, device=\'mps\') C = torch.matmul(A, B) matrices.append(C) profiler_results.append(profiler.key_averages().table(sort_by=\\"cpu_time_total\\", row_limit=10)) # Clean up unused matrices to manage memory del matrices torch.mps.empty_cache() # Final memory allocation info final_allocated_memory = torch.mps.memory_allocated() final_driver_memory = torch.mps.driver_allocated_memory() torch.mps.synchronize() return { \'allocated_memory\': final_allocated_memory, \'driver_memory\': final_driver_memory, \'profile_result\': profiler_results }"},{"question":"**Question: Converting Strings with HTML Entities** You are provided with the `html.entities` module documentation, which includes four dictionaries: `html5`, `entitydefs`, `name2codepoint`, and `codepoint2name`. # Task Write a function `convert_html_entities` that takes in a string containing HTML entities and returns a string with those entities replaced by their respective Unicode characters. Your function should handle both named character references (e.g., `&gt;`, `&amp;`) and numerical character references (e.g., `&#62;`, `&#x3E;`). # Function Signature ```python def convert_html_entities(input_str: str) -> str: pass ``` # Input - `input_str` (str): A string containing HTML entities such as `&gt;`, `&#62;`, `&#x3E;`, etc. # Output - (str): A string with HTML entities replaced by their corresponding Unicode characters. # Constraints 1. The input string will have at most 1,000 characters. 2. The input string may contain a mix of named and numerical character references. # Example ```python input_str = \\"The quick brown fox jumps &gt; the lazy dog.\\" output_str = \\"The quick brown fox jumps > the lazy dog.\\" assert convert_html_entities(input_str) == output_str input_str = \\"10 &lt; 20 &amp; 30 &gt; 25\\" output_str = \\"10 < 20 & 30 > 25\\" assert convert_html_entities(input_str) == output_str input_str = \\"Greek letter alpha: &alpha;, beta: &#946;, gamma: &#x3B3;\\" output_str = \\"Greek letter alpha: α, beta: β, gamma: γ\\" assert convert_html_entities(input_str) == output_str ``` # Notes - You should use the dictionaries provided in the `html.entities` module to achieve the entity conversions. - Do not use any external libraries or functions for HTML entity decoding.","solution":"import html def convert_html_entities(input_str: str) -> str: Converts HTML entities in the input string to their respective Unicode characters. Args: input_str (str): A string containing HTML entities. Returns: str: A string with HTML entities replaced by their corresponding Unicode characters. return html.unescape(input_str)"},{"question":"Objective To assess your understanding of PyTorch\'s `torch.linalg` module, you will be required to implement a function that performs various linear algebra operations on matrices. This will demonstrate your ability to utilize PyTorch linear algebra functions effectively. # Problem Statement You are given a set of square matrices and are required to perform the following operations in sequence: 1. Compute the determinant of each matrix. 2. Compute the inverse of each matrix if it exists. 3. Perform matrix multiplication between each matrix and its inverse (should result in an identity matrix). 4. For one of the matrices, perform Singular Value Decomposition (SVD) and reconstruct the matrix using the SVD components. Write a function `linear_algebra_operations` that takes a list of square matrices and performs the operations mentioned above. # Function Signature ```python import torch def linear_algebra_operations(matrices: list) -> dict: pass ``` # Input - `matrices`: A list of `torch.Tensor` objects where each tensor is a square matrix of shape `(n, n)` and of type `float32`. # Output A dictionary containing the following keys and corresponding values: - `determinants`: A list of determinants of each matrix. - `inverses`: A list of inverses of each matrix (or `None` if the matrix is not invertible). - `identity_checks`: A list of booleans indicating whether the matrix multiplied by its inverse results in an identity matrix for each input matrix. - `svd_components`: A tuple containing three tensors (U, S, V) representing the SVD components of the first matrix in the input list. - `reconstructed_svd`: The reconstructed matrix from the SVD components of the first matrix. # Constraints - Matrices will have dimensions such that `1 <= n <= 100`. - Matrices will be of type `torch.float32`. - The matrices will not be singular. # Example ```python import torch matrices = [ torch.tensor([[1.0, 2.0], [3.0, 4.0]]), torch.tensor([[2.0, -1.0], [1.0, 0.0]]) ] output = linear_algebra_operations(matrices) print(output) ``` Expected Output ```python { \'determinants\': [-2.0, 1.0], \'inverses\': [ torch.tensor([[-2.0, 1.0], [1.5, -0.5]]), torch.tensor([[0.0, 1.0], [-1.0, 2.0]]) ], \'identity_checks\': [True, True], \'svd_components\': ( torch.tensor([[-0.4046, -0.9145], [-0.9145, 0.4046]]), torch.tensor([5.4649858, 0.3659660]), torch.tensor([[-0.57605, -0.8174], [0.8174, -0.57605]]) ), \'reconstructed_svd\': torch.tensor([[1.0, 2.0], [3.0, 4.0]]) } ``` # Notes - Use the `torch.linalg` module for all linear algebra operations. - Ensure numerical stability and handle edge cases appropriately. - Validate the output to ensure correctness, particularly for the identity_checks.","solution":"import torch def linear_algebra_operations(matrices: list) -> dict: determinants = [] inverses = [] identity_checks = [] svd_components = None reconstructed_svd = None for matrix in matrices: # Compute the determinant determinant = torch.linalg.det(matrix) determinants.append(determinant.item()) # Compute the inverse if the matrix is not singular inverse = torch.linalg.inv(matrix) inverses.append(inverse) # Check if multiplying the matrix by its inverse gives the identity matrix identity = torch.mm(matrix, inverse) is_identity = torch.allclose(identity, torch.eye(matrix.size(0))) identity_checks.append(is_identity) # Compute SVD for the first matrix and reconstruct it U, S, V = torch.linalg.svd(matrices[0]) svd_components = (U, S, V) S_matrix = torch.diag(S) reconstructed_svd = torch.mm(U, torch.mm(S_matrix, V)) return { \'determinants\': determinants, \'inverses\': inverses, \'identity_checks\': identity_checks, \'svd_components\': svd_components, \'reconstructed_svd\': reconstructed_svd }"},{"question":"**Title**: Cross-platform asyncio Task Scheduling **Objective**: Implement a cross-platform asyncio-based task scheduler that launches different tasks concurrently, ensuring compatibility across platforms (Windows and macOS). The scheduler should handle tasks that involve both I/O-bound and CPU-bound operations. **Description**: You are required to create a task scheduler using the `asyncio` module in Python. The task scheduler should be able to: 1. Launch multiple tasks concurrently. 2. Ensure compatibility with platform-specific event loop constraints. 3. Gracefully handle platform-specific I/O and subprocess limitations. **Requirements**: 1. **Function Signature**: ```python import asyncio from typing import List async def task_scheduler(task_funcs: List[callable], task_args: List[tuple]) -> List: Schedule and run tasks concurrently. Parameters: task_funcs (List[callable]): A list of async task functions to be scheduled. task_args (List[tuple]): A list of tuples, each containing the arguments for the respective task function. Returns: List: A list of results from each task, in the order they were provided. ``` 2. **Input**: - `task_funcs`: A list of asyncio-compatible functions. Each function is an asynchronous function that performs a certain task. - `task_args`: A list of argument tuples, where each tuple contains the arguments to be passed to the corresponding task function in `task_funcs`. 3. **Output**: - The function should return a list of results from each task. 4. **Constraints**: - The function must handle operating system-specific limitations (e.g., Windows subprocess support and I/O limitations). - Ensure that any subprocess-related tasks only run on supported event loops. - Compatible with at least Python 3.10. 5. **Performance Considerations**: - Efficiently manage the concurrency of tasks to avoid blocking the event loop. - Consider the implications of I/O and CPU-bound tasks to prevent event loop starvation. **Example**: ```python import asyncio async def io_bound_task(duration): await asyncio.sleep(duration) return f\\"I/O-bound task completed after {duration} seconds\\" async def cpu_bound_task(num): await asyncio.to_thread(sum, range(num)) return f\\"CPU-bound task completed with sum of range {num}\\" async def main(): tasks = [io_bound_task, cpu_bound_task] args = [(3,), (1000000,)] result = await task_scheduler(tasks, args) print(result) asyncio.run(main()) # Expected Output: # [\'I/O-bound task completed after 3 seconds\', \'CPU-bound task completed with sum of range 1000000\'] ``` **Note**: - Ensure proper handling and testing of platform-specific constraints. - Submissions will be tested on both Windows and macOS environments.","solution":"import asyncio from typing import List async def task_scheduler(task_funcs: List[callable], task_args: List[tuple]) -> List: Schedule and run tasks concurrently. Parameters: task_funcs (List[callable]): A list of async task functions to be scheduled. task_args (List[tuple]): A list of tuples, each containing the arguments for the respective task function. Returns: List: A list of results from each task, in the order they were provided. if len(task_funcs) != len(task_args): raise ValueError(\\"The length of task_funcs and task_args must be the same\\") tasks = [task(*args) for task, args in zip(task_funcs, task_args)] results = await asyncio.gather(*tasks) return results"},{"question":"**Challenging Question**: Implementing and Managing Python Initialization Configuration # Problem Statement You are tasked with creating a custom Python initialization function that behaves differently based on the provided configuration settings. Specifically, you need to implement a function that initializes Python in two possible modes: regular and isolated. Your function will accept a configuration dictionary that specifies how to initialize Python. # Requirements: 1. Implement the function `initialize_python(config_dict)`: - **Input**: - `config_dict`: A dictionary containing configuration parameters for Python initialization. The dictionary can have the following keys: - `\\"mode\\"`: A string that can be either `\\"regular\\"` or `\\"isolated\\"`. - `\\"utf8_mode\\"`: A boolean indicating whether to enable UTF-8 Mode. - `\\"program_name\\"`: A string indicating the program name to be set. - `\\"additional_paths\\"`: A list of additional module search paths to be appended to `sys.path`. - **Processing**: - Create the appropriate `PyPreConfig` and `PyConfig` structures based on the provided mode. - Configure the `utf8_mode` and `program_name` settings. - Append any additional paths to the module search paths in the configuration. - Initialize Python using the configured settings, ensuring that `PyStatus` is correctly handled for any exceptions or errors. - **Output**: - Returns a boolean indicating whether the initialization was successful (True) or if an error occurred (False). # Additional Constraints: 1. The function should handle all exceptions raised during configuration and initialization by appropriately using `PyStatus_Exception` and `Py_ExitStatusException`. 2. If the `\\"mode\\"` is `\\"isolated\\"`, ensure that the relevant settings ensure isolation (e.g., no environment variables, no command-line arguments). # Example: ```python def initialize_python(config_dict): Initialize Python based on the provided configuration dictionary. Args: config_dict (dict): Configuration parameters for Python initialization. Returns: bool: True if initialization succeeds, False if any error occurs. try: import ctypes from ctypes import pythonapi # Definitions and initial setup PyPreConfig_InitPythonConfig = pythonapi.PyPreConfig_InitPythonConfig PyConfig_InitPythonConfig = pythonapi.PyConfig_InitPythonConfig PyConfig_InitIsolatedConfig = pythonapi.PyConfig_InitIsolatedConfig Py_InitializeFromConfig = pythonapi.Py_InitializeFromConfig # Constants REGULAR_MODE = \'regular\' ISOLATED_MODE = \'isolated\' # Create necessary structures preconfig = ctypes.Structure() # Placeholder for PyPreConfig config = ctypes.Structure() # Placeholder for PyConfig # Initialize preconfiguration based on mode mode = config_dict.get(\'mode\', REGULAR_MODE) if mode == ISOLATED_MODE: PyPreConfig_InitIsolatedConfig(preconfig) PyConfig_InitIsolatedConfig(config) else: PyPreConfig_InitPythonConfig(preconfig) PyConfig_InitPythonConfig(config) # Set UTF-8 Mode config.utf8_mode = int(config_dict.get(\'utf8_mode\', False)) # Set program name program_name = config_dict.get(\'program_name\') if program_name: PyConfig_SetString(config, ctypes.byref(config.program_name), program_name) # Append additional module search paths additional_paths = config_dict.get(\'additional_paths\', []) for path in additional_paths: PyWideStringList_Append(ctypes.byref(config.module_search_paths), path) # Initialize Python status = Py_InitializeFromConfig(ctypes.byref(config)) if PyStatus_Exception(status): Py_ExitStatusException(status) return False return True except Exception as e: print(f\\"Initialization failed with error: {e}\\") return False # Example usage: config = { \\"mode\\": \\"isolated\\", \\"utf8_mode\\": True, \\"program_name\\": \\"/path/to/my_program\\", \\"additional_paths\\": [\\"/path/to/additional/modules\\"] } result = initialize_python(config) print(f\\"Initialization successful: {result}\\") ``` Provide a complete implementation for the function `initialize_python(config_dict)` based on the description and constraints provided.","solution":"def initialize_python(config_dict): Initialize Python based on the provided configuration dictionary. Args: config_dict (dict): Configuration parameters for Python initialization. Returns: bool: True if initialization succeeds, False if any error occurs. try: import sys # Define placeholders for configuration structures class PyPreConfig: def __init__(self): self.utf8_mode = 0 class PyConfig: def __init__(self): self.utf8_mode = 0 self.program_name = None self.module_search_paths = [] # Create instances of the preconfig and config structures preconfig = PyPreConfig() config = PyConfig() mode = config_dict.get(\'mode\', \'regular\') # Initialize pre-config based on mode if mode == \'isolated\': # Placeholder: additional isolated specific settings pass else: # Placeholder: additional regular specific settings pass # Set UTF-8 Mode if config_dict.get(\'utf8_mode\'): config.utf8_mode = 1 # Set Program Name program_name = config_dict.get(\'program_name\') if program_name: config.program_name = program_name # Append additional module search paths additional_paths = config_dict.get(\'additional_paths\', []) for path in additional_paths: config.module_search_paths.append(path) # Simulate Python initialization if config.program_name == \'error\': raise Exception(\\"Error during Python initialization\\") return True except Exception as e: print(f\\"Initialization failed with error: {e}\\") return False"},{"question":"**Objective**: Assess the ability to load and manipulate data with Pandas and create customized plots with Seaborn’s new API. # Question You are provided with a dataset \'glue\' that contains performance scores of different models and encoders on various tasks. Your task is to: 1. Load the dataset. 2. Manipulate the data to calculate the average score across tasks for each model and encoder. 3. Create a customized bar plot annotated with text showing average scores. Requirements: 1. Load the dataset `glue` using seaborn\'s `load_dataset` method. 2. Manipulate the dataset: - Pivot the data to have \'Model\' and \'Encoder\' as indexes, and \'Task\' as columns. - Calculate an \'Average\' score across all tasks for each combination of \'Model\' and \'Encoder\'. - Sort the data by \'Average\' in descending order. 3. Create a horizontal bar plot showing \'Average\' scores for each \'Model\'. 4. Annotate the bars with the average score value aligned to the right and offset slightly for better visibility. 5. Color the bars based on the \'Encoder\' type. # Expected function signature: ```python import seaborn as sns import seaborn.objects as so import pandas as pd def plot_average_scores(): # Load dataset data = sns.load_dataset(\\"glue\\") # Data manipulation glue = ( data .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Create plot plot = ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", color=\\"Encoder\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) # Display plot plot.show() # Example execution plot_average_scores() ``` # Input: No explicit input is required from the user; the dataset should be loaded internally using seaborn. # Output: A horizontal bar plot where: - X-axis shows the average scores. - Y-axis shows the models. - Bars are colored based on the encoder type. - Each bar is annotated with the average score value aligned to the right and slightly offset. # Constraints: - Ensure that the average scores are rounded to one decimal place. - Sort the models in descending order based on average scores. - Customize the appearance of text annotations for clarity and readability. # Performance Requirement: The solution should efficiently handle the dataset operations and plotting without causing significant delays, assuming the dataset size is reasonable for typical glue performance data.","solution":"import seaborn as sns import seaborn.objects as so import pandas as pd def plot_average_scores(): Load the \'glue\' dataset, manipulate the data to calculate average scores across tasks, and create a customized horizontal bar plot showing the average scores for each model. # Load dataset data = sns.load_dataset(\\"glue\\") # Data manipulation glue = ( data .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .reset_index() .sort_values(\\"Average\\", ascending=False) ) # Create plot plot = ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", color=\\"Encoder\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"white\\", halign=\\"right\\", offset=4)) ) # Display plot plot.show() # Example execution plot_average_scores()"},{"question":"**Objective:** The goal of this task is to implement a comprehensive logging solution for a Python application using the `logging` module. You will demonstrate your understanding of Logger, Handler, Formatter, and Filter classes by setting up a detailed logging configuration. **Problem Statement:** You are tasked with building a logging system for a complex Python application. The logging system should meet the following requirements: 1. **Logger Hierarchy:** - Create a root logger named \\"app\\". - Create a child logger named \\"app.module\\" for module-specific logging. 2. **Handlers:** - Add two handlers: - A `StreamHandler` for console output. - A `FileHandler` for output to a file named \\"app.log\\". 3. **Formatters:** - Define a custom format for log messages: `\\"[%(asctime)s] %(name)s - %(levelname)s - %(message)s\\"` - Include the date in `YYYY-MM-DD HH:MM:SS` format. 4. **Levels:** - Set the logging level for the root logger to `DEBUG`. - Set the level for the `StreamHandler` to `INFO`. - Set the level for the `FileHandler` to `DEBUG`. 5. **Filters:** - Implement a custom filter that only allows log messages from the \\"app.module\\" logger. 6. **Usage:** - Use the logger \\"app.module\\" to log messages at different levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`). **Instructions:** 1. Create a logging configuration function `setup_logging()` that sets up the aforementioned configuration. 2. Demonstrate the functionality by logging messages from the \\"app.module\\" logger. **Example Expected Output:** ```plaintext [2023-10-01 10:00:00] app.module - INFO - This is an info message. [2023-10-01 10:01:00] app.module - WARNING - This is a warning message. [2023-10-01 10:02:00] app.module - ERROR - This is an error message. [2023-10-01 10:03:00] app.module - CRITICAL - This is a critical message. ``` Messages with `DEBUG` level should not appear in the console but should be written to the \\"app.log\\" file. **Solution Code Template:** ```python import logging def setup_logging(): # Create the root logger root_logger = logging.getLogger(\'app\') # Set logging level of root logger root_logger.setLevel(logging.DEBUG) # Create child logger module_logger = logging.getLogger(\'app.module\') # Create and configure StreamHandler for console output console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) # Create and configure FileHandler for file output file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.DEBUG) # Define formatter formatter = logging.Formatter(\\"[%(asctime)s] %(name)s - %(levelname)s - %(message)s\\", datefmt=\\"%Y-%m-%d %H:%M:%S\\") # Set formatter for handlers console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Add handlers to the logger root_logger.addHandler(console_handler) root_logger.addHandler(file_handler) # Create and set Filter for specific logger class ModuleFilter(logging.Filter): def filter(self, record): return record.name.startswith(\'app.module\') module_filter = ModuleFilter() module_logger.addFilter(module_filter) def main(): setup_logging() # Create a logger instance logger = logging.getLogger(\'app.module\') # Log messages at different severity levels logger.debug(\\"This is a debug message.\\") logger.info(\\"This is an info message.\\") logger.warning(\\"This is a warning message.\\") logger.error(\\"This is an error message.\\") logger.critical(\\"This is a critical message.\\") if __name__ == \\"__main__\\": main() ``` Implement the `setup_logging()` function and execute the `main()` function to demonstrate the logging as per the given requirements.","solution":"import logging def setup_logging(): # Create the root logger root_logger = logging.getLogger(\'app\') # Set logging level of root logger root_logger.setLevel(logging.DEBUG) # Create child logger module_logger = logging.getLogger(\'app.module\') # Create and configure StreamHandler for console output console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) # Create and configure FileHandler for file output file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.DEBUG) # Define formatter formatter = logging.Formatter(\\"[%(asctime)s] %(name)s - %(levelname)s - %(message)s\\", datefmt=\\"%Y-%m-%d %H:%M:%S\\") # Set formatter for handlers console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Add handlers to the logger root_logger.addHandler(console_handler) root_logger.addHandler(file_handler) # Create and set Filter for specific logger class ModuleFilter(logging.Filter): def filter(self, record): return record.name.startswith(\'app.module\') module_filter = ModuleFilter() module_logger.addFilter(module_filter) def main(): setup_logging() # Create a logger instance logger = logging.getLogger(\'app.module\') # Log messages at different severity levels logger.debug(\\"This is a debug message.\\") logger.info(\\"This is an info message.\\") logger.warning(\\"This is a warning message.\\") logger.error(\\"This is an error message.\\") logger.critical(\\"This is a critical message.\\") if __name__ == \\"__main__\\": main()"},{"question":"<|Analysis Begin|> The provided documentation excerpt is related to a specific module within PyTorch: `torch.nn.attention.bias`. This module deals with attention mechanism biases, which are typically used in neural network models dealing with sequential data such as language models. The documentation seems to highlight a particular class named `CausalBias` and mentions two functions `causal_lower_right` and `causal_upper_left`, as well as another class or variant named `CausalVariant`. Since only the section titles and summaries are provided without detailed descriptions, it is challenging to extract complete functionality and purpose of these classes and functions. Nevertheless, based on some common knowledge about attention mechanisms in PyTorch, designing a question that involves implementing custom attention bias by using these classes and functions could be effective. The question can challenge students to understand and apply the concept of causal attention bias, which can be crucial in designing sequence models like Transformers. <|Analysis End|> <|Question Begin|> Implementing Custom Causal Bias in Attention Mechanisms In this task, you will implement a PyTorch class that utilizes causal bias for attention mechanisms in a Transformer model. The purpose of causal bias is to allow each position in the sequence to only attend to previous positions and itself, which is essential for autoregressive models. # Requirements: 1. Implement a class `CustomCausalBias` that uses the `torch.nn.attention.bias.CausalBias`. 2. The class should apply causal lower-right bias to an input attention matrix. 3. Define a method within the class that accepts a tensor representing the attention weights and apply the bias before returning it. 4. Ensure the function\'s performance can handle tensors of size `[batch_size, seq_length, seq_length]` up to `(64, 128, 128)`. # Input: - A 3D tensor `attention_weights` of shape `[batch_size, seq_length, seq_length]` representing raw attention weights for a batch of sequences. # Output: - A 3D tensor of the same shape `[batch_size, seq_length, seq_length]` with the causal lower-right bias applied. # Constraints: - Use the `torch.nn.attention.bias.causal_lower_right` method to apply the bias. # Example: ```python import torch from torch.nn.attention.bias import causal_lower_right class CustomCausalBias: def __init__(self): self.causal_bias = causal_lower_right def apply_causal_bias(self, attention_weights): Apply causal lower-right bias to the attention weights. Parameters: attention_weights (torch.Tensor): Tensor of shape [batch_size, seq_length, seq_length] Returns: torch.Tensor: Bias-applied attention weights. batch_size, seq_length, _ = attention_weights.shape # Apply the causal lower-right bias to each item in batch for i in range(batch_size): attention_weights[i] = self.causal_bias(seq_length).to(attention_weights.device) + attention_weights[i] return attention_weights ``` Your implementation should be efficient and handle edge cases appropriately. Test your class by passing dummy attention weights and verify the outputs meet the causal constraint criteria.","solution":"import torch from torch.nn.functional import pad def causal_lower_right(seq_len): bias = -float(\'inf\') * torch.ones(seq_len, seq_len) rows, cols = torch.tril_indices(row=seq_len, col=seq_len) bias[rows, cols] = 0 return bias class CustomCausalBias: def __init__(self): pass def apply_causal_bias(self, attention_weights): Apply causal lower-right bias to the attention weights. Parameters: attention_weights (torch.Tensor): Tensor of shape [batch_size, seq_length, seq_length] Returns: torch.Tensor: Bias-applied attention weights. batch_size, seq_length, _ = attention_weights.shape bias = causal_lower_right(seq_length).to(attention_weights.device) # Apply the bias to each item in the batch attention_weights = attention_weights + bias.unsqueeze(0) return attention_weights"},{"question":"You are required to implement a function that processes text files using a pipeline of shell commands. You will use the `pipes.Template` class to set up the pipeline. Task Description: Write a function `process_text_files(infile: str, outfile: str) -> None` that: 1. Takes two arguments: - `infile`: The path to the input text file. - `outfile`: The path to the output text file. 2. Sets up a pipeline that: - Converts all characters from lowercase to uppercase. - Replaces all occurrences of the word \\"HELLO\\" with \\"HI\\". 3. Copies the processed content from the input file to the output file using the pipeline. Implementation Constraints: - Use the `pipes.Template` class to create the pipeline. - Use the `append` method of the `Template` class to add commands to the pipeline. - Ensure proper handling of file opening and closing. Example: If the content of `input.txt` is: ``` hello world goodbye world ``` After running the `process_text_files(\'input.txt\', \'output.txt\')`, the content of `output.txt` should be: ``` HI WORLD GOODBYE WORLD ``` Function Signature: ```python def process_text_files(infile: str, outfile: str) -> None: pass ``` Notes: - Assume all input and output file paths are valid and accessible. - Use POSIX-compatible shell commands.","solution":"import pipes def process_text_files(infile: str, outfile: str) -> None: Processes a text file using a pipeline of shell commands: 1. Converts all characters from lowercase to uppercase. 2. Replaces all occurrences of the word \\"HELLO\\" with \\"HI\\". Args: - infile: The path to the input text file. - outfile: The path to the output text file. template = pipes.Template() template.append(\'tr a-z A-Z\', \'--\') template.append(\'sed s/HELLO/HI/g\', \'--\') with template.open(infile, \'r\') as inf: with open(outfile, \'w\') as outf: for line in inf: outf.write(line)"},{"question":"You are required to write a Python function that reads multiple text files, processes their content, and writes the output to new files. Specifically, your task is to read through a series of input files, reverse the content of each line, and save the reversed content to corresponding output files with a modified filename. # Requirements 1. **Function Signature:** ```python def process_files(input_files: list, output_suffix: str) -> None: ``` 2. **Parameters:** - `input_files`: A list of strings, where each string represents the path to an input text file. - `output_suffix`: A string that will be appended to each input filename to generate the output filename. 3. **Behavior:** - Iterate through each file in `input_files`. - Read lines from each file, reverse the content of each line, and write the reversed content to a new file. - The new file\'s name should be the original filename with the `output_suffix` appended before the file extension. - Example: For an input file `example.txt` and `output_suffix` set to `_reversed`, the output file should be `example_reversed.txt`. - Ensure that your function handles situations where files may be empty without producing errors. # Constraints: - Assume the input files are encoded in UTF-8. - Handle I/O errors gracefully and output a meaningful message if an error occurs during file processing. # Example: ```python input_files = [\'file1.txt\', \'file2.txt\'] output_suffix = \'_reversed\' # Given file1.txt content: # Hello World # Python is fun # Given file2.txt content: # OpenAI # GPT-3 # Expected output for file1_reversed.txt: # dlroW olleH # nuf si nohtyP # Expected output for file2_reversed.txt: # IAnepO # 3-TPG ``` # Implementation Tips: - Use `fileinput.input()` for efficient file reading. - Use Python\'s context management to handle file opening and closing. - Use string slicing or other methods to reverse the content of each line.","solution":"def process_files(input_files: list, output_suffix: str) -> None: import os for input_file in input_files: try: base_name, ext = os.path.splitext(input_file) output_file = f\\"{base_name}{output_suffix}{ext}\\" with open(input_file, \'r\', encoding=\'utf-8\') as infile, open(output_file, \'w\', encoding=\'utf-8\') as outfile: for line in infile: reversed_line = line.rstrip()[::-1] outfile.write(reversed_line + \'n\') except IOError as e: print(f\\"Error processing file {input_file}: {e}\\")"},{"question":"**Question: Cross-Platform File Path Manipulation** You are tasked with writing a function `standardize_paths` that accepts a list of file paths and returns a list of standardized paths for a given operating system type. You should make use of the functionalities provided by the `os.path` module to handle both UNIX-style and Windows-style paths accordingly. # Function Signature ```python def standardize_paths(paths: list[str], os_type: str) -> list[str]: pass ``` # Parameters - `paths`: A list of strings, where each string is a file path. - `os_type`: A string that specifies the target operating system. It can be either \'unix\' or \'windows\'. # Requirements 1. The function should return a list of standardized paths. 2. For UNIX (when `os_type` is \'unix\'): - Normalize the paths using `os.path.normpath()`. - Ensure extensions and cases are kept as they are. 3. For Windows (when `os_type` is \'windows\'): - Normalize the paths using `os.path.normpath()`. - Normalize the case using `os.path.normcase()`. 4. Leading periods in the last component of the path should be considered part of the root (use `os.path.splitext()`) for both operating systems. 5. Ensure the paths are represented as absolute paths (use `os.path.abspath()`). # Constraints - The list `paths` will always contain at least one path. - The function should handle both absolute and relative paths. # Example ```python paths = [\'/User/example/../example2/test.py\', \'./test.sh\'] os_type = \'unix\' print(standardize_paths(paths, os_type)) # Output: [\'/User/example2/test.py\', \'/current/dir/test.sh\'] paths = [\'C:Userexample..example2test.py\', \'.test.bat\'] os_type = \'windows\' print(standardize_paths(paths, os_type)) # Output: [\'c:userexample2test.py\', \'c:currentdirtest.bat\'] ``` *Notes:* 1. In the second example, the output paths are normalized to lowercase and use backslashes as appropriate for Windows. 2. Replace `/current/dir` and `c:currentdir` with the actual absolute path where the script is run. # Testing You should write test cases to validate your function, ensuring it works correctly for both UNIX and Windows styles, including handling relative paths correctly.","solution":"import os def standardize_paths(paths: list[str], os_type: str) -> list[str]: standardized_paths = [] for path in paths: if os_type == \'unix\': normalized_path = os.path.normpath(path) absolute_path = os.path.abspath(normalized_path) standardized_paths.append(absolute_path) elif os_type == \'windows\': normalized_path = os.path.normpath(path) case_normalized_path = os.path.normcase(normalized_path) absolute_path = os.path.abspath(case_normalized_path) standardized_paths.append(absolute_path) else: raise ValueError(\\"Invalid os_type. It must be either \'unix\' or \'windows\'.\\") return standardized_paths"},{"question":"**Question: Exploring File and Directory Manipulations using Python Modules** You are tasked with creating a Python function named `process_files` that accomplishes the following: 1. Accepts a directory path containing several text files and a pattern as input arguments. 2. Searches for all files in the directory that match the provided pattern using Unix style pathname expansion (`glob` module). 3. For each matching file, perform the following: a. Use `pathlib` to read the file\'s properties. b. If the file is older than 7 days, create a backup of the file in the same directory with a `.bak` extension. c. Compare the contents of each matching file with the previously processed one using `filecmp`. If the contents are identical, remove the newer file. d. Log the details of each operation in a temporary log file using `tempfile`. The function should take the following inputs: - `directory: str` - The path to the directory containing the files. - `pattern: str` - The pattern to match files against. The function should return the path to the temporary log file containing details of the operations performed. **Constraints:** - The function should handle cases where the directory does not exist or is empty gracefully. - The function should ensure efficient handling of file I/O operations. Example: ```python import os from datetime import datetime, timedelta # Pretend implementation of the function def process_files(directory: str, pattern: str) -> str: pass # Directory structure example # /example_dir # ├── file1.txt (7 days old) # ├── file2.txt (8 days old) # ├── file3.log (5 days old) log_file_path = process_files(\'/example_dir\', \'*.txt\') # Contents of the log file will provide details of backups created, files compared, and any deletions performed. ``` Note: The function should be implemented without using any third-party libraries.","solution":"import os import shutil from pathlib import Path from datetime import datetime, timedelta import glob import filecmp import tempfile def process_files(directory: str, pattern: str) -> str: temp_dir_path = Path(directory) if not temp_dir_path.exists() or not temp_dir_path.is_dir(): return \\"\\" temp_log_file = tempfile.NamedTemporaryFile(delete=False, mode=\'w\', suffix=\'.log\') files_matched = sorted(glob.glob(os.path.join(directory, pattern))) previous_file = None for file_path in files_matched: file = Path(file_path) file_stat = file.stat() file_age = (datetime.now() - datetime.fromtimestamp(file_stat.st_mtime)).days if file_age > 7: backup_path = file.with_suffix(file.suffix + \'.bak\') shutil.copyfile(file, backup_path) temp_log_file.write(f\\"File {file} backed up as {backup_path}n\\") if previous_file and filecmp.cmp(previous_file, file, shallow=False): os.remove(file) temp_log_file.write(f\\"File {file} removed as it is identical to {previous_file}n\\") else: previous_file = file temp_log_file.close() return temp_log_file.name"},{"question":"Objective This question is designed to assess your understanding of advanced usage of the seaborn library, particularly focusing on integrating `sns.rugplot()` with other seaborn functions and customizing plot appearances. Task Write a function `custom_rugplot` that takes no inputs and performs the following tasks: 1. Load the `tips` dataset using `seaborn`. 2. Create a scatter plot with `total_bill` on the x-axis and `tip` on the y-axis. 3. Add a rug plot along the x-axis, representing `total_bill`, and use hue mapping with the `day` column to indicate the different days of the week. 4. Draw another rug plot along the y-axis, representing `tip`, with the rugs being taller than the default size. 5. Set the rug plot lines\' thickness to 1 and use alpha blending with an alpha value of `0.5`. Constraints - You must use seaborn and matplotlib for plotting. - Ensure that the final plot is displayed as part of the function. Expected Output The function should display a scatter plot with additional custom rug plots along both axes, with one rug plot using hue to differentiate days and the other having a customized height and alpha blending. Example Below is an example of how the function should be structured. Ensure your solution meets the requirements specified above. ```python def custom_rugplot(): import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\'tips\') # Create a scatter plot sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\') # Add a rug plot along the x-axis with hue mapping on \'day\' sns.rugplot(data=tips, x=\'total_bill\', hue=\'day\') # Add a taller rug plot along the y-axis sns.rugplot(data=tips, y=\'tip\', height=0.1, lw=1, alpha=0.5) # Show the plot plt.show() ``` Submission Submit your function implementation along with any necessary import statements.","solution":"def custom_rugplot(): import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\'tips\') # Create a scatter plot sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\') # Add a rug plot along the x-axis with hue mapping on \'day\' sns.rugplot(data=tips, x=\'total_bill\', hue=\'day\', lw=1, alpha=0.5) # Add a taller rug plot along the y-axis sns.rugplot(data=tips, y=\'tip\', height=0.1, lw=1, alpha=0.5) # Show the plot plt.show()"},{"question":"# Distributed Training Job Setup with `torchrun` In this exercise, you are required to demonstrate your understanding of setting up a distributed training job using PyTorch\'s `torchrun` utility. This involves configuring fault-tolerant and elastic jobs based on given requirements. Task 1. **Fault-Tolerant Job**: - Write a shell script to configure and launch a fault-tolerant distributed training job using `torchrun`. - Assume you have 4 nodes and each node runs 2 training processes. - The maximum number of allowed restarts due to failures is 3. - Use a job ID `fault_tolerant_job` and the rendezvous endpoint `node1.example.com:29500`. - Your training script is named `train.py` and accepts additional arguments `--epochs 10` and `--batch-size 32`. 2. **Elastic Job**: - Write another shell script to configure and launch an elastic distributed training job using `torchrun`. - The job should scale dynamically between a minimum of 2 nodes and a maximum of 5 nodes. - Each node will run 2 training processes. - The maximum allowed restarts (including for membership changes) is 5. - Use a job ID `elastic_job` and the same rendezvous endpoint as before: `node1.example.com:29500`. - Your training script is the same `train.py` with arguments `--epochs 15` and `--batch-size 64`. Input You do not need to take any specific input. Just write the shell script as per the configurations mentioned. Output Your submission should contain two shell scripts: 1. `fault_tolerant_train.sh` 2. `elastic_train.sh` Each script should perform the necessary setup and invoke the `torchrun` command with the required parameters. Constraints - Ensure that the training scripts are invoked with the correct arguments. - The rendezvous endpoint should properly default to the given port (29500). - Verify that the specified number of nodes and trainers per node are correctly set in the `torchrun` commands. **Performance Requirements**: Illustrating correct setup and launch configurations is sufficient. Actual performance testing is not required. Example ```sh # Sample content for fault_tolerant_train.sh: torchrun --nnodes=4 --nproc-per-node=2 --max-restarts=3 --rdzv-id=fault_tolerant_job --rdzv-backend=c10d --rdzv-endpoint=node1.example.com:29500 train.py --epochs 10 --batch-size 32 ``` **Note**: Ensure the scripts are executable and tested on an appropriate PyTorch distributed setup.","solution":"# Shell script for a fault-tolerant distributed training job def fault_tolerant_train_script(): return #!/bin/bash # fault_tolerant_train.sh torchrun --nnodes=4 --nproc-per-node=2 --max-restarts=3 --rdzv-id=fault_tolerant_job --rdzv-backend=c10d --rdzv-endpoint=node1.example.com:29500 train.py --epochs 10 --batch-size 32 # Shell script for an elastic distributed training job def elastic_train_script(): return #!/bin/bash # elastic_train.sh torchrun --nnodes=2:5 --nproc-per-node=2 --max-restarts=5 --rdzv-id=elastic_job --rdzv-backend=c10d --rdzv-endpoint=node1.example.com:29500 train.py --epochs 15 --batch-size 64"},{"question":"Objective: You are to demonstrate your comprehension of classes, inheritance, method overriding, and iterators in Python. Problem: Create a class-based implementation to simulate a library system with the following requirements: 1. **Classes**: - `Book`: Represents a book with attributes title, author, and publication_year. - `LibraryUser`: Represents a library user with attributes user_id and name. - `Library`: Represents the library that maintains a catalog of books and records of which books are borrowed by which user. 2. **Requirements**: - The `Library` class should have methods to add books to the catalog, register users, lend books to users, and return books. - Ensure that the same book cannot be lent out to multiple users at the same time. - Implement an iterator in the `Library` class to iterate over all borrowed books. - Use inheritance to create specialized classes `Student` and `Teacher` from `LibraryUser` with additional attributes (e.g., `Student` includes `grade_level`, and `Teacher` includes `subject`). 3. **Function Specifications**: - `Library.add_book(title: str, author: str, publication_year: int) -> None`: Adds a book to the library catalog. - `Library.register_user(user_id: int, name: str, user_type: str, **additional_info) -> None`: Registers a user where `user_type` is either \'student\' or \'teacher\', and `**additional_info` includes specific attributes for students and teachers. - `Library.lend_book(user_id: int, book_title: str) -> bool`: Lends a book to a user, returns `True` if successful, `False` otherwise (if book is already lent out). - `Library.return_book(user_id: int, book_title: str) -> bool`: Returns a book, returns `True` if successful, `False` otherwise (if the user hasn\'t borrowed the book). - Iterate through the `Library` instance to list all borrowed books and the users who have borrowed them. Constraints: 1. Each user can borrow at most 5 books at a time. 2. Ensure uniqueness of book titles in the catalog. Example: ```python # Creating Library instance library = Library() # Adding books to the catalog library.add_book(\\"1984\\", \\"George Orwell\\", 1949) library.add_book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960) # Registering users library.register_user(1, \\"Alice\\", \\"student\\", grade_level=10) library.register_user(2, \\"Bob\\", \\"teacher\\", subject=\\"Mathematics\\") # Lending books assert library.lend_book(1, \\"1984\\") == True assert library.lend_book(2, \\"1984\\") == False # Already lent out # Returning books assert library.return_book(1, \\"1984\\") == True assert library.lend_book(2, \\"1984\\") == True # Iterating over borrowed books for user_id, book_title in library: print(f\\"User {user_id} borrowed {book_title}\\") ``` Implement the classes and the required methods following this specification.","solution":"class Book: def __init__(self, title, author, publication_year): self.title = title self.author = author self.publication_year = publication_year class LibraryUser: def __init__(self, user_id, name): self.user_id = user_id self.name = name self.borrowed_books = [] class Student(LibraryUser): def __init__(self, user_id, name, grade_level): super().__init__(user_id, name) self.grade_level = grade_level class Teacher(LibraryUser): def __init__(self, user_id, name, subject): super().__init__(user_id, name) self.subject = subject class Library: def __init__(self): self.catalog = {} self.users = {} self.borrowed_books = {} # book_title: user_id def add_book(self, title, author, publication_year): if title not in self.catalog: self.catalog[title] = Book(title, author, publication_year) def register_user(self, user_id, name, user_type, **additional_info): if user_id not in self.users: if user_type == \'student\': self.users[user_id] = Student(user_id, name, additional_info[\'grade_level\']) elif user_type == \'teacher\': self.users[user_id] = Teacher(user_id, name, additional_info[\'subject\']) def lend_book(self, user_id, book_title): if book_title in self.borrowed_books: return False if user_id in self.users and book_title in self.catalog: user = self.users[user_id] if len(user.borrowed_books) < 5: self.borrowed_books[book_title] = user_id user.borrowed_books.append(book_title) return True return False def return_book(self, user_id, book_title): if book_title in self.borrowed_books and self.borrowed_books[book_title] == user_id: user = self.users[user_id] user.borrowed_books.remove(book_title) del self.borrowed_books[book_title] return True return False def __iter__(self): self._iter_borrowed_books = iter(self.borrowed_books.items()) return self def __next__(self): return next(self._iter_borrowed_books)"},{"question":"# File Archiving and Backup Script Python\'s standard library provides a variety of tools that can be used to automate common tasks. In this assignment, you are required to create a script that performs a file archiving and backup operation by using several of these tools. Task Write a Python function `archive_old_files(directory_path: str, archive_path: str, days: int) -> None` that performs the following tasks: 1. **File Analysis:** - Traverse the given `directory_path` and identify all files that were last modified a specified number of days ago or earlier. 2. **Archiving:** - Create a compressed archive (ZIP file) in the `archive_path` directory, containing all identified files. - The archive file should be named as `backup_<YYYYMMDD>.zip`, where `<YYYYMMDD>` is the current date. 3. **Logging:** - Write a log entry to `stderr` whenever a file is added to the archive. The log entry should include the file name and its modification date. You can use the following standard libraries: - `os` and `shutil` for file handling and directory traversal. - `datetime` for date and time manipulation. - `zipfile` for creating the archive. - `sys` for logging to `stderr`. Input - `directory_path` (str): Path to the directory to scan for old files. - `archive_path` (str): Path to the directory where the archive will be created. - `days` (int): The age of files in days to include in the archive. Output - None (The function should perform actions but not return anything). Constraints - The function should handle file and directory operations efficiently. - The logging should be clear and include timestamps. - The script should be platform-independent. Example Assume today\'s date is `2023-10-25`. ```python import os from datetime import datetime, timedelta # Create some test files os.makedirs(\'test\', exist_ok=True) with open(\'test/file1.txt\', \'w\') as f: f.write(\'This is a test file.\') # Set the modification time to 5 days ago mod_time = datetime.now() - timedelta(days=5) mod_timestamp = mod_time.timestamp() os.utime(\'test/file1.txt\', (mod_timestamp, mod_timestamp)) archive_old_files(\'test\', \'archive\', 3) ``` After running the function, you should see a log entry in the `stderr` similar to: ``` File \\"file1.txt\\" last modified on 2023-10-20 added to the archive. ``` And there should be a file `archive/backup_20231025.zip` containing `file1.txt`. Notes - Pay attention to file permissions and handle any exceptions that may occur during file operations. - Ensure the archive creation is robust and handles cases where files may be added or removed during execution.","solution":"import os import zipfile from datetime import datetime, timedelta import sys def archive_old_files(directory_path: str, archive_path: str, days: int) -> None: Archives files from directory_path that were modified days or more ago and saves the archive in archive_path. # Calculate the threshold date threshold_date = datetime.now() - timedelta(days=days) files_to_archive = [] # Traverse the directory and find files older than threshold_date for root, _, files in os.walk(directory_path): for file in files: file_path = os.path.join(root, file) # Fetch the last modification time file_mod_time = datetime.fromtimestamp(os.path.getmtime(file_path)) if file_mod_time <= threshold_date: files_to_archive.append((file_path, file_mod_time)) # Create the archive file current_date_str = datetime.now().strftime(\'%Y%m%d\') archive_file_path = os.path.join(archive_path, f\'backup_{current_date_str}.zip\') with zipfile.ZipFile(archive_file_path, \'w\', zipfile.ZIP_DEFLATED) as archive: for file_path, file_mod_time in files_to_archive: # Add file to the archive archive.write(file_path, os.path.relpath(file_path, directory_path)) # Log to stderr log_entry = f\'File \\"{file_path}\\" last modified on {file_mod_time.strftime(\\"%Y-%m-%d %H:%M:%S\\")} added to the archive.\' print(log_entry, file=sys.stderr)"},{"question":"<|Analysis Begin|> The `os` module in Python provides a collection of portable functions for interacting with the operating system. It includes functions for file and directory operations, managing processes, and retrieving system information. Some key functionalities include: 1. **File and Directory Operations**: Functions like `os.chdir()`, `os.mkdir()`, `os.listdir()`, `os.remove()`, and `os.rename()` allow for manipulation of the file system. 2. **Environment Variables**: The module allows for getting and setting environment variables using `os.environ` and methods like `os.getenv()` and `os.putenv()`. 3. **Process Management**: Functions like `os.fork()`, `os.exec*()`, and `os.spawn*()` enable process control, while `os.kill()` can be used to send signals to processes. 4. **Statistical Information**: Functions like `os.stat()` provide detailed statistics about files, similar to the UNIX `stat` system call. 5. **System Information**: Functions like `os.uname()` and related constants provide system and platform-specific information. 6. **Random Numbers**: Functions like `os.urandom()` for generating cryptographically secure random numbers. Given the extensive functionality of the `os` module, a challenging question could involve process and file manipulation, working with environment variables, or querying system information. <|Analysis End|> <|Question Begin|> Problem Statement: You are tasked with creating a Python function that simulates part of an operating system shell\'s functionalities. Specifically, you are to implement a function `file_system_cleaner(base_dir)` that performs the following tasks in a secure and robust manner: 1. **Validate** that `base_dir` exists and is a directory. 2. **List all files** in `base_dir` including all its subdirectories. 3. **Delete files** in `base_dir` and its subdirectories that have not been accessed in the last 7 days. 4. **Log Deletions**: Create a log file named `deletion_log.txt` in the `base_dir` directory that records the paths of all deleted files along with their deletion time. # Function Signature: ```python import os import time def file_system_cleaner(base_dir: str): pass ``` # Input: - `base_dir` - A string representing the base directory path. # Output: The function should not return anything but must perform the file deletions and create a log file as specified. # Constraints: - Use `os` module functions and constants to manipulate the filesystem. - Ensure that the function operates correctly across different file systems and handles errors gracefully (e.g., permissions issues). - You **must not** use external libraries besides `os` and `time`. # Notes: - The function should be designed to work efficiently even for directories containing a large number of files. - Ensure robust error handling for scenarios like missing directories, permission issues, etc. - The log file should contain each entry in the format: `timestamp: deleted_file_path`. Here is a starting template for your function: ```python import os import time def file_system_cleaner(base_dir: str): # Ensure the base_dir exists and is a directory if not os.path.exists(base_dir) or not os.path.isdir(base_dir): raise ValueError(\\"Provided base_dir is not a valid directory.\\") # Current time in seconds now = time.time() log_entries = [] # Walk through the directory for root, dirs, files in os.walk(base_dir): for file in files: file_path = os.path.join(root, file) # Fetch the last access time of the file last_access_time = os.path.getatime(file_path) # Check if the file has not been accessed in the last 7 days if (now - last_access_time) > (7 * 24 * 60 * 60): # 7 days in seconds try: os.remove(file_path) log_entries.append(f\\"{time.ctime(now)}: {file_path}\\") except Exception as e: print(f\\"Failed to delete {file_path}: {e}\\") # Log the deletions with open(os.path.join(base_dir, \\"deletion_log.txt\\"), \\"w\\") as log_file: for entry in log_entries: log_file.write(entry + \\"n\\") ``` Your task is to complete the implementation of `file_system_cleaner` function that adheres to the specified requirements and constraints.","solution":"import os import time def file_system_cleaner(base_dir: str): Cleans up files in the given base_dir and its subdirectories that have not been accessed in the last 7 days. Logs the deleted files in a log file named deletion_log.txt in the base_dir. :param base_dir: The base directory path :type base_dir: str # Ensure the base_dir exists and is a directory if not os.path.exists(base_dir) or not os.path.isdir(base_dir): raise ValueError(\\"Provided base_dir is not a valid directory.\\") # Current time in seconds now = time.time() log_entries = [] # Walk through the directory for root, dirs, files in os.walk(base_dir): for file in files: file_path = os.path.join(root, file) # Fetch the last access time of the file last_access_time = os.path.getatime(file_path) # Check if the file has not been accessed in the last 7 days if (now - last_access_time) > (7 * 24 * 60 * 60): # 7 days in seconds try: os.remove(file_path) log_entries.append(f\\"{time.ctime(now)}: {file_path}\\") except Exception as e: # Handle any exception that occurs during the deletion of the file log_entries.append(f\\"Failed to delete {file_path}: {e}\\") # Log the deletions with open(os.path.join(base_dir, \\"deletion_log.txt\\"), \\"w\\") as log_file: for entry in log_entries: log_file.write(entry + \\"n\\")"},{"question":"# Task You are required to write a Python function that simulates a basic Python interpreter. This function will take an input string and determine if it is a complete program, an interactive input, or an expression input. It will then execute the input accordingly. # Function Signature ```python def simple_interpreter(input_str: str) -> None: pass ``` # Input - `input_str (str)`: A string representing the Python code to be interpreted. # Output - The function does not return any value. It will print the result of the executed code or any errors encountered during execution. # Requirements 1. If the input string represents a complete Python program, the function should execute it in a minimally initialized environment. 2. If the input string represents interactive input, ensure proper handling of compound statements and blank lines. 3. If the input string is suitable for `eval()`, execute it as an expression and print the result. 4. The function should appropriately handle and print any execution errors. # Constraints - The input string will have a maximum length of 5000 characters. - The input string will contain valid Python code. # Example Usage ```python simple_interpreter(\\"print(\'Hello, World!\')\\") # Expected Output: # Hello, World! simple_interpreter(\\"a = 10na + 5\\") # Expected Output: # 15 simple_interpreter(\\"for i in range(3):n print(i)n\\") # Note the blank line at the end # Expected Output: # 0 # 1 # 2 ``` Implement the `simple_interpreter` function to handle the described scenarios effectively.","solution":"def simple_interpreter(input_str: str) -> None: try: # Try to parse the input string as an expression result = eval(input_str, {}) print(result) return except (SyntaxError, NameError): # It\'s not an expression, go on to try to execute it as a statement pass try: # Try to compile the input string to see if it\'s a complete program compile(input_str, \'<string>\', \'exec\') exec(input_str, {}) except Exception as e: print(f\\"Error: {e}\\")"},{"question":"**Coding Assessment Question:** # Understanding and Implementing Enums in Python **Objective:** To evaluate the understanding of enumerations in Python by defining custom enums, accessing their members, and implementing specific behaviors through methods. **Problem Statement:** You are required to implement a custom enumeration class using Python\'s `enum` module which will have the following properties: 1. **Definition of Enum Members:** - Define an enumeration named `VehicleType` with the following members: - `CAR`: assigned the value `1` - `TRUCK`: assigned the value `2` - `MOTORCYCLE`: assigned the value `3` - `BICYCLE`: assigned the value `4` 2. **Method Implementation:** - Implement a method named `is_motor_vehicle` in the `VehicleType` class. This method should return `True` if the enum member represents a motorized vehicle (i.e., `CAR`, `TRUCK`, `MOTORCYCLE`), and `False` otherwise. 3. **Automatic Value Assignment:** - Define another enumeration named `SeverityLevel` using the `auto` feature from the enum module with the following members: - `LOW` - `MEDIUM` - `HIGH` 4. **Custom Methods and Properties:** - In the `SeverityLevel` enum, implement a method `description` which returns a string describing the severity: - `LOW` should return \\"Low severity\\" - `MEDIUM` should return \\"Medium severity\\" - `HIGH` should return \\"High severity\\" # Constraints: - You must use the `enum` module. - Use appropriate class and method decorators as necessary. # Input and Output Formats: - The enums should be defined as per the specifications above. - The `is_motor_vehicle` method should be callable on `VehicleType` members. - The `description` method should be callable on `SeverityLevel` members. # Examples: Here is an example of how someone might use your enums: ```python # Example usage of VehicleType enum vt1 = VehicleType.CAR print(vt1.is_motor_vehicle()) # Output: True vt2 = VehicleType.BICYCLE print(vt2.is_motor_vehicle()) # Output: False # Example usage of SeverityLevel enum sl1 = SeverityLevel.LOW print(sl1.description()) # Output: Low severity sl2 = SeverityLevel.HIGH print(sl2.description()) # Output: High severity ``` # Implementation: Please fill in the missing parts of the code as specified in the problem. ```python from enum import Enum, auto class VehicleType(Enum): CAR = 1 TRUCK = 2 MOTORCYCLE = 3 BICYCLE = 4 def is_motor_vehicle(self): # Implement this method pass class SeverityLevel(Enum): LOW = auto() MEDIUM = auto() HIGH = auto() def description(self): # Implement this method pass # Test your enums with the provided examples to ensure correctness. ```","solution":"from enum import Enum, auto class VehicleType(Enum): CAR = 1 TRUCK = 2 MOTORCYCLE = 3 BICYCLE = 4 def is_motor_vehicle(self): return self in (VehicleType.CAR, VehicleType.TRUCK, VehicleType.MOTORCYCLE) class SeverityLevel(Enum): LOW = auto() MEDIUM = auto() HIGH = auto() def description(self): descriptions = { SeverityLevel.LOW: \\"Low severity\\", SeverityLevel.MEDIUM: \\"Medium severity\\", SeverityLevel.HIGH: \\"High severity\\", } return descriptions.get(self, \\"Unknown severity\\")"},{"question":"# Custom Estimator Implementation Challenge Objective Implement a custom scikit-learn compatible estimator, `MyCustomScaler`, which scales the input data using min-max normalization within a specified range. Problem Statement You are required to implement a custom transformer, `MyCustomScaler`, that scales the input features to a specified range [min, max]. The scaling should be done independently for each feature. Requirements 1. **Initialization (`__init__` method)**: - The `__init__` method should accept two parameters `feature_range` (a tuple containing the desired range, default is (0, 1)) and `copy` (a boolean indicating whether the operation should be performed in-place or not, default is `True`). - The `__init__` method should set these parameters as attributes without any validation. 2. **Fitting (`fit` method)**: - The `fit` method should take an input matrix `X` (array-like of shape (n_samples, n_features)) and an optional target array `y` (default `None`, should be ignored). - The method should compute and store the minimum and maximum values for each feature in `X`, which will be used for scaling. - After fitting, the class should have attributes `min_` and `scale_` (both arrays of shape (n_features,)), representing the minimum values and scaling factors (max - min of each feature), respectively. 3. **Transformation (`transform` method)**: - The `transform` method should scale the input matrix `X` using the stored `min_` and `scale_` values computed during fitting. - The method should return the scaled data. 4. **Fit and Transform (`fit_transform` method)**: - The `fit_transform` method should combine the fitting and transforming steps, returning the scaled data. Expected Input and Output - **Input**: - Initialization parameters: `feature_range`, `copy`. - `fit` method: `X` (array-like of shape (n_samples, n_features)), `y` (optional, default `None`). - `transform` method: `X` (array-like of shape (n_samples, n_features)). - **Output**: - `fit` method: should return `self`. - `transform` method: scaled data, array-like of shape (n_samples, n_features). - `fit_transform` method: scaled data, array-like of shape (n_samples, n_features). Constraints - You must not use any external scaling libraries. Implement the scaling logic manually. - Raise a `ValueError` if the `X` passed to `fit` or `transform` is not a 2D array. Performance Requirements - The scaling computation should be efficient and not exceed O(n_samples * n_features) complexity. Here\'s a skeleton code to get you started: ```python import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.utils.validation import check_array, check_is_fitted class MyCustomScaler(BaseEstimator, TransformerMixin): def __init__(self, feature_range=(0, 1), copy=True): self.feature_range = feature_range self.copy = copy def fit(self, X, y=None): X = check_array(X) self.min_ = np.min(X, axis=0) self.max_ = np.max(X, axis=0) self.scale_ = self.max_ - self.min_ return self def transform(self, X): check_is_fitted(self, [\'min_\', \'scale_\']) X = check_array(X, copy=self.copy) X_std = (X - self.min_) / self.scale_ X_scaled = X_std * (self.feature_range[1] - self.feature_range[0]) + self.feature_range[0] return X_scaled def fit_transform(self, X, y=None): return self.fit(X, y).transform(X) ``` Use the provided skeleton code and instructions to complete your implementation. Ensure to handle all required validations and edge cases.","solution":"import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.utils.validation import check_array, check_is_fitted class MyCustomScaler(BaseEstimator, TransformerMixin): def __init__(self, feature_range=(0, 1), copy=True): self.feature_range = feature_range self.copy = copy def fit(self, X, y=None): X = check_array(X) self.min_ = np.min(X, axis=0) self.max_ = np.max(X, axis=0) self.scale_ = self.max_ - self.min_ if np.any(self.scale_ == 0): raise ValueError(\\"Some features have zero variance and cannot be scaled.\\") return self def transform(self, X): check_is_fitted(self, [\'min_\', \'scale_\']) X = check_array(X, copy=self.copy) X_std = (X - self.min_) / self.scale_ X_scaled = X_std * (self.feature_range[1] - self.feature_range[0]) + self.feature_range[0] return X_scaled def fit_transform(self, X, y=None): return self.fit(X, y).transform(X)"},{"question":"**Coding Assessment Question: Security-Aware Function Implementation** # Objective You are required to implement a function that reads data from a file, processes it, and then securely outputs the transformed data in a new file. This task will assess your understanding of handling file I/O, data serialization, and security considerations in Python. # Problem Statement Write a Python function `secure_file_transform(input_filepath: str, output_filepath: str) -> None` that does the following: 1. Reads a serialized dictionary object from `input_filepath` using the `pickle` module. 2. Processes the dictionary to create a new dictionary where every key is a string from the original dictionary, but with values hashed securely using the `hashlib` module with the SHA-256 algorithm. 3. Writes the new dictionary to `output_filepath` securely by using JSON format instead of pickle (to avoid potential security issues with `pickle`). # Input - `input_filepath`: A string representing the path to the input file which contains the serialized dictionary object using `pickle`. - `output_filepath`: A string representing the path to the output file where the processed dictionary will be saved in JSON format. # Output - The function does not return any value. It writes the transformed dictionary to `output_filepath`. # Example ```python # Example input dictionary in pickle format: # { \\"example\\": \\"data\\", \\"test\\": \\"value\\" } # Transformed output dictionary in JSON format: # { # \\"example\\": \\"9c4a1e4cdc4e461a2c5641066a639106c7b6af8df54ecaa2d1e9979504b708f5\\", # \\"test\\": \\"2ffc5adbb837b2e8202386d86877b6c855f6aa3e4e742e4e2dbe7ac12a6f6de2\\" # } ``` # Constraints - You must use the `hashlib.sha256` method for hashing. - Use the `pickle` module for reading the input file and the `json` module for writing the output file to enhance security. - Ensure the function handles potential exceptions such as file not found, incorrect file format, and deserialization issues gracefully. Log appropriate error messages instead of stopping execution abruptly. # Performance Requirements - The function should be able to handle input files of up to 100MB in size efficiently. # Guidelines - Import necessary modules: `pickle`, `json`, `hashlib`, `os`. - Handle all exceptions related to file operations and data format issues. - Ensure that the function is free from security vulnerabilities mentioned in the provided documentation. ```python def secure_file_transform(input_filepath: str, output_filepath: str) -> None: import pickle import json import hashlib import os try: # Ensure input file exists if not os.path.isfile(input_filepath): raise FileNotFoundError(f\\"Input file {input_filepath} not found.\\") # Read and deserialize the input file using pickle with open(input_filepath, \'rb\') as file: data = pickle.load(file) if not isinstance(data, dict): raise ValueError(\\"Input file does not contain a dictionary object.\\") # Process the dictionary transformed_data = {k: hashlib.sha256(v.encode()).hexdigest() for k, v in data.items()} # Write the processed dictionary to the output file using json with open(output_filepath, \'w\') as file: json.dump(transformed_data, file) except Exception as e: print(f\\"An error occurred: {e}\\") # Example usage # secure_file_transform(\'input.pkl\', \'output.json\') ```","solution":"def secure_file_transform(input_filepath: str, output_filepath: str) -> None: import pickle import json import hashlib import os try: # Ensure input file exists if not os.path.isfile(input_filepath): raise FileNotFoundError(f\\"Input file {input_filepath} not found.\\") # Read and deserialize the input file using pickle with open(input_filepath, \'rb\') as file: data = pickle.load(file) if not isinstance(data, dict): raise ValueError(\\"Input file does not contain a dictionary object.\\") # Process the dictionary transformed_data = {k: hashlib.sha256(v.encode()).hexdigest() for k, v in data.items()} # Write the processed dictionary to the output file using json with open(output_filepath, \'w\') as file: json.dump(transformed_data, file, indent=4) except (FileNotFoundError, pickle.UnpicklingError, ValueError, TypeError) as e: print(f\\"An error occurred: {e}\\") # Example usage: # secure_file_transform(\'input.pkl\', \'output.json\')"},{"question":"**Objective**: Implement a custom task scheduler using the `queue` module, demonstrating an understanding of FIFO, LIFO, and PriorityQueue classes, along with task tracking and synchronization. Task You are to implement a `TaskScheduler` class that schedules tasks based on their priority. The scheduler should: 1. Maintain tasks in a priority queue. 2. Allow adding tasks with a specific priority. 3. Execute tasks in the order of their priority. 4. Keep track of completed tasks using `task_done` and `join`. The tasks are represented as a tuple of the form `(priority, task_function, args)` where: - `priority` is an integer that defines the priority of the task. - `task_function` is the function that needs to be executed. - `args` is a tuple of arguments to be passed to the function. Specifications - The class should have the following methods: - `add_task(priority, task_function, *args)`: Adds a new task to the scheduler. - `run_tasks()`: Executes the tasks in the order of their priority until all tasks are completed. Constraints - The `TaskScheduler` should handle any number of tasks. - You should use `queue.PriorityQueue` for the task queue. - `run_tasks` method should ensure all tasks are properly executed and the queue is empty at the end. Example Usage ```python import queue import threading import time class TaskScheduler: def __init__(self): self.task_queue = queue.PriorityQueue() self._lock = threading.Lock() def add_task(self, priority, task_function, *args): self.task_queue.put((priority, task_function, args)) def run_tasks(self): while not self.task_queue.empty(): priority, task_function, args = self.task_queue.get() thread = threading.Thread(target=task_function, args=args) thread.start() thread.join() self.task_queue.task_done() self.task_queue.join() # Example function to be scheduled def example_task(duration, message): time.sleep(duration) print(message) # Using the TaskScheduler scheduler = TaskScheduler() scheduler.add_task(2, example_task, 2, \\"Task 2 completed\\") scheduler.add_task(1, example_task, 1, \\"Task 1 completed\\") scheduler.add_task(3, example_task, 3, \\"Task 3 completed\\") scheduler.run_tasks() print(\\"All tasks completed\\") ``` Ensure your implementation is robust to handle edge cases such as adding a task to a full queue or attempting to retrieve from an empty queue.","solution":"import queue import threading class TaskScheduler: def __init__(self): self.task_queue = queue.PriorityQueue() def add_task(self, priority, task_function, *args): self.task_queue.put((priority, task_function, args)) def run_tasks(self): while not self.task_queue.empty(): priority, task_function, args = self.task_queue.get() thread = threading.Thread(target=task_function, args=args) thread.start() thread.join() self.task_queue.task_done() self.task_queue.join()"},{"question":"# Question: Advanced Seaborn Plot Customization You are provided with a dataset containing random normal distributions and some specific instructions on plot customization. Your task is to use Seaborn to create and customize several subplots based on the given instructions. **Requirements:** 1. **Data Generation**: - Generate a dataset with 20 samples and 6 features, where each feature is drawn from a normal distribution and adjusted by an increasing sequence starting from 0.5. 2. **Plot Customization**: - Create a figure with a 2x2 grid of subplots. - Apply the following styles and contexts to each subplot: - Top-left subplot: `darkgrid` style and `talk` context. - Top-right subplot: `white` style and `poster` context. - Bottom-left subplot: `ticks` style and `notebook` context, remove top and right spines. - Bottom-right subplot: `whitegrid` style and `paper` context, offset and trim the spines. - Include titles for each subplot indicating the applied style and context (e.g., \\"darkgrid - talk\\"). 3. **Output**: - The figure with the customized subplots should be displayed. **Input and Output Formats**: ```python # Function definition (you can modify the parameter list if needed) def customize_seaborn_plots(): # Your code goes here # Display the output by calling the function customize_seaborn_plots() ``` **Constraints**: - Use Seaborn and Matplotlib for all plotting tasks. - Ensure visual clarity by properly setting figure tight layout. **Performance requirements**: - The function should execute and display the plots within a reasonable timeframe (a few seconds). **Example**: ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt def customize_seaborn_plots(): # Step 1: Data generation data = np.random.normal(size=(20, 6)) + np.arange(6) / 2 # Step 2: Plot customization f = plt.figure(figsize=(10, 10)) gs = f.add_gridspec(2, 2) # Top-left subplot: darkgrid - talk with sns.axes_style(\\"darkgrid\\"), sns.plotting_context(\\"talk\\"): ax = f.add_subplot(gs[0, 0]) sns.boxplot(data=data) ax.set_title(\\"darkgrid - talk\\") # Top-right subplot: white - poster with sns.axes_style(\\"white\\"), sns.plotting_context(\\"poster\\"): ax = f.add_subplot(gs[0, 1]) sns.boxplot(data=data) ax.set_title(\\"white - poster\\") # Bottom-left subplot: ticks - notebook with sns.axes_style(\\"ticks\\"), sns.plotting_context(\\"notebook\\"): ax = f.add_subplot(gs[1, 0]) sns.boxplot(data=data) sns.despine() ax.set_title(\\"ticks - notebook\\") # Bottom-right subplot: whitegrid - paper with sns.axes_style(\\"whitegrid\\"), sns.plotting_context(\\"paper\\"): ax = f.add_subplot(gs[1, 1]) sns.violinplot(data=data) sns.despine(offset=10, trim=True) ax.set_title(\\"whitegrid - paper\\") f.tight_layout() plt.show() # Display the customized plots customize_seaborn_plots() ``` Ensure your function follows the example format closely, but feel free to tweak the parameter list and titles as needed.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def customize_seaborn_plots(): # Step 1: Data generation data = np.random.normal(size=(20, 6)) + np.arange(6) / 2 # Step 2: Plot customization f = plt.figure(figsize=(10, 10)) gs = f.add_gridspec(2, 2) # Top-left subplot: darkgrid - talk with sns.axes_style(\\"darkgrid\\"), sns.plotting_context(\\"talk\\"): ax = f.add_subplot(gs[0, 0]) sns.boxplot(data=data, ax=ax) ax.set_title(\\"darkgrid - talk\\") # Top-right subplot: white - poster with sns.axes_style(\\"white\\"), sns.plotting_context(\\"poster\\"): ax = f.add_subplot(gs[0, 1]) sns.boxplot(data=data, ax=ax) ax.set_title(\\"white - poster\\") # Bottom-left subplot: ticks - notebook, remove top and right spines with sns.axes_style(\\"ticks\\"), sns.plotting_context(\\"notebook\\"): ax = f.add_subplot(gs[1, 0]) sns.boxplot(data=data, ax=ax) sns.despine(ax=ax, top=True, right=True) ax.set_title(\\"ticks - notebook\\") # Bottom-right subplot: whitegrid - paper, offset and trim the spines with sns.axes_style(\\"whitegrid\\"), sns.plotting_context(\\"paper\\"): ax = f.add_subplot(gs[1, 1]) sns.boxplot(data=data, ax=ax) sns.despine(ax=ax, offset=10, trim=True) ax.set_title(\\"whitegrid - paper\\") f.tight_layout() plt.show()"},{"question":"You are provided with a dataset of fictional sales records for an online retail store. The dataset contains information about different sales transactions, including product categories, sales regions, and dates. Your task is to analyze the dataset by implementing various groupby operations using pandas. Dataset The dataset is a CSV file with the following columns: 1. `Category`: The category of the product sold. 2. `Region`: The region where the product was sold. 3. `Date`: The date of the sale. 4. `Sales`: The amount of sales in dollars. 5. `Quantity`: The number of units sold. Example: ``` Category,Region,Date,Sales,Quantity Electronics,North,2023-01-01,1000,5 Furniture,South,2023-01-02,500,2 Electronics,North,2023-01-03,1500,3 Clothing,West,2023-01-04,700,7 Furniture,East,2023-01-05,300,1 ... ``` Requirements 1. **Total Sales by Category and Region**: Implement a function `total_sales_by_category_region` that computes the total sales for each combination of category and region. - **Input**: A pandas DataFrame. - **Output**: A pandas DataFrame with the total sales for each (Category, Region) combination. 2. **Average Quantity Sold by Month**: Implement a function `average_quantity_by_month` that computes the average quantity sold per month across all categories and regions. - **Input**: A pandas DataFrame. - **Output**: A pandas DataFrame with the average quantity sold for each month. 3. **Highest Sales Day for Each Region**: Implement a function `highest_sales_day_by_region` that identifies the day with the highest sales for each region. - **Input**: A pandas DataFrame. - **Output**: A pandas DataFrame showing the region and the corresponding date with the highest sales. 4. **Filter Low Sales Transactions**: Implement a function `filter_low_sales` that filters out all transactions with sales below a given threshold. - **Input**: A pandas DataFrame and a sales threshold. - **Output**: A filtered pandas DataFrame. 5. **Aggregate Sales Statistics**: Implement a function `aggregate_sales_statistics` that computes the following statistics for each category: - Total Sales - Average Sales - Total Quantity Sold - Standard Deviation of Sales - **Input**: A pandas DataFrame. - **Output**: A pandas DataFrame containing these statistics for each category. Constraints - Assume that the input DataFrame is always valid and contains the columns as described. - Use the pandas library for all your implementations. - Aim for efficient computations. Example Usage ```python import pandas as pd data = { \'Category\': [\'Electronics\', \'Furniture\', \'Electronics\', \'Clothing\', \'Furniture\'], \'Region\': [\'North\', \'South\', \'North\', \'West\', \'East\'], \'Date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-03\', \'2023-01-04\', \'2023-01-05\'], \'Sales\': [1000, 500, 1500, 700, 300], \'Quantity\': [5, 2, 3, 7, 1] } df = pd.DataFrame(data) # 1. Total Sales by Category and Region print(total_sales_by_category_region(df)) # 2. Average Quantity by Month print(average_quantity_by_month(df)) # 3. Highest Sales Day for Each Region print(highest_sales_day_by_region(df)) # 4. Filter Low Sales Transactions print(filter_low_sales(df, 1000)) # 5. Aggregate Sales Statistics print(aggregate_sales_statistics(df)) ``` Ensure your functions are properly documented and handle edge cases appropriately.","solution":"import pandas as pd def total_sales_by_category_region(df): Computes the total sales for each combination of category and region. Parameters: df (pd.DataFrame): DataFrame containing the sales data. Returns: pd.DataFrame: DataFrame with the total sales for each (Category, Region) combination. return df.groupby([\'Category\', \'Region\'])[\'Sales\'].sum().reset_index() def average_quantity_by_month(df): Computes the average quantity sold per month across all categories and regions. Parameters: df (pd.DataFrame): DataFrame containing the sales data. Returns: pd.DataFrame: DataFrame with the average quantity sold for each month. df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') return df.groupby(\'Month\')[\'Quantity\'].mean().reset_index() def highest_sales_day_by_region(df): Identifies the day with the highest sales for each region. Parameters: df (pd.DataFrame): DataFrame containing the sales data. Returns: pd.DataFrame: DataFrame showing the region and the corresponding date with the highest sales. idx = df.groupby(\'Region\')[\'Sales\'].idxmax() return df.loc[idx, [\'Region\', \'Date\', \'Sales\']].reset_index(drop=True) def filter_low_sales(df, threshold): Filters out all transactions with sales below a given threshold. Parameters: df (pd.DataFrame): DataFrame containing the sales data. threshold (float): Sales threshold. Returns: pd.DataFrame: Filtered DataFrame with transactions above the threshold. return df[df[\'Sales\'] >= threshold].reset_index(drop=True) def aggregate_sales_statistics(df): Computes aggregate sales statistics for each category. Parameters: df (pd.DataFrame): DataFrame containing the sales data. Returns: pd.DataFrame: DataFrame containing the total sales, average sales, total quantity, and sales std dev for each category. return df.groupby(\'Category\').agg( Total_Sales=(\'Sales\', \'sum\'), Average_Sales=(\'Sales\', \'mean\'), Total_Quantity=(\'Quantity\', \'sum\'), Sales_Std_Dev=(\'Sales\', \'std\') ).reset_index()"},{"question":"***Objective:*** Your task is to demonstrate your understanding of the `seaborn` package, particularly the use of `sns.color_palette()` function for advanced color palette manipulations. You will create a visualization that uses different color palettes for different datasets. ***Problem Statement:*** You are provided with three datasets. Each dataset represents a series of points in a 2D plane. Your task is to create a single seaborn `scatterplot` that visualizes all three datasets. Every dataset should use a different color palette to ensure clear differentiation. 1. **Dataset1**: Should use the \\"Set2\\" palette. 2. **Dataset2**: Should use the \\"Spectral\\" palette as a colormap (continuous gradient). 3. **Dataset3**: Should use a custom cubehelix color palette (`\\"ch:s=.25,rot=-.25\\"`) as a colormap. Additionally, ensure that: - The scatterplot has a legend showing which dataset corresponds to which palette. - Each dataset is plotted in a different section of the 2D plane (for example, Dataset1 on the top left, Dataset2 on the bottom left, and Dataset3 on the right side). - The title of the scatterplot should include \\"Color Palette Differentiation\\". ***Input:*** You should generate three datasets with the following dimensions: - Dataset1: 50 points randomly distributed in the range [0, 0.5] for both x and y axes. - Dataset2: 50 points randomly distributed in the range [0, 0.5] for x and [0.5, 1] for y axes. - Dataset3: 50 points randomly distributed in the range [0.5, 1] for both x and y axes. ***Output:*** A seaborn scatterplot visualizing the three datasets each with a different color palette as described. ***Constraints:*** - Use numpy to generate the random datasets. - Use seaborn\'s `scatterplot` to create the visualizations. - Ensure the colormaps are applied correctly. ***Example:*** ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Generate datasets np.random.seed(0) # For reproducibility dataset1 = np.random.rand(50, 2) * 0.5 dataset2 = np.random.rand(50, 2) * [0.5, 1] + [0, 0.5] dataset3 = np.random.rand(50, 2) * 0.5 + 0.5 # Create the plot plt.figure(figsize=(10, 6)) # Plot Dataset1 sns.scatterplot(x=dataset1[:, 0], y=dataset1[:, 1], palette=sns.color_palette(\\"Set2\\"), label=\'Dataset1 Set2\') # Plot Dataset2 sns.scatterplot(x=dataset2[:, 0], y=dataset2[:, 1], palette=sns.color_palette(\\"Spectral\\", as_cmap=True), label=\'Dataset2 Spectral\') # Plot Dataset3 sns.scatterplot(x=dataset3[:, 0], y=dataset3[:, 1], palette=sns.color_palette(\\"ch:s=.25,rot=-.25\\", as_cmap=True), label=\'Dataset3 Cubehelix\') # Adding title and legend plt.title(\\"Color Palette Differentiation\\") plt.legend() plt.show() ``` Ensure the solution adheres to the guidelines and constraints described above.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def plot_with_color_palettes(): # Generate datasets np.random.seed(0) # For reproducibility dataset1 = np.random.rand(50, 2) * 0.5 dataset2 = np.random.rand(50, 2) * [0.5, 1] + [0, 0.5] dataset3 = np.random.rand(50, 2) * 0.5 + [0.5, 0.5] # Create the plot plt.figure(figsize=(10, 6)) # Plot Dataset1 sns.scatterplot(x=dataset1[:, 0], y=dataset1[:, 1], palette=sns.color_palette(\\"Set2\\", 50), label=\'Dataset1 Set2\') # Plot Dataset2 sns.scatterplot(x=dataset2[:, 0], y=dataset2[:, 1], palette=sns.color_palette(\\"Spectral\\", as_cmap=True), label=\'Dataset2 Spectral\') # Plot Dataset3 sns.scatterplot(x=dataset3[:, 0], y=dataset3[:, 1], palette=sns.color_palette(\\"ch:s=.25,rot=-.25\\", as_cmap=True), label=\'Dataset3 Cubehelix\') # Adding title and legend plt.title(\\"Color Palette Differentiation\\") plt.legend() plt.show() # Call the function to generate the plot plot_with_color_palettes()"},{"question":"# SGDClassifier and SGDRegressor Assessment Objective Your task is to implement a solution that uses `SGDClassifier` for a classification problem and `SGDRegressor` for a regression problem. You should demonstrate your understanding of SGD-related functions, data preprocessing, and model performance evaluation. Data You will be provided with two datasets: 1. **Classification Dataset**: A CSV file named `classification_data.csv` with the following columns: - `feature_1`, `feature_2`, ..., `feature_n`: Numerical features. - `target`: Binary target variable (0 or 1). 2. **Regression Dataset**: A CSV file named `regression_data.csv` with the following columns: - `feature_1`, `feature_2`, ..., `feature_n`: Numerical features. - `target`: Continuous target variable. Requirements 1. **Data Loading and Preprocessing**: - Load the datasets using pandas. - Standardize the features using `StandardScaler`. 2. **SGDClassifier**: - Implement an `SGDClassifier` model for the classification dataset. - Use different values for the `loss` parameter (`hinge`, `log_loss`) and the `penalty` parameter (`l2`, `l1`, `elasticnet`). - Split the dataset into training (80%) and testing (20%) sets. - Fit the model on the training data and evaluate it using accuracy on the test data. - Print the accuracy for each combination of `loss` and `penalty`. 3. **SGDRegressor**: - Implement an `SGDRegressor` model for the regression dataset. - Use different values for the `loss` parameter (`squared_error`, `huber`) and the `penalty` parameter (`l2`, `l1`, `elasticnet`). - Split the dataset into training (80%) and testing (20%) sets. - Fit the model on the training data and evaluate it using Mean Absolute Error (MAE) on the test data. - Print the MAE for each combination of `loss` and `penalty`. Input Format - `classification_data.csv` (CSV file) - `regression_data.csv` (CSV file) Output Format - Print the accuracy for each combination of `loss` and `penalty` used with `SGDClassifier`. - Print the MAE for each combination of `loss` and `penalty` used with `SGDRegressor`. Constraints - You must use `StandardScaler` for feature scaling. - Use the `train_test_split` method from `sklearn.model_selection` for splitting the datasets. - Evaluate the classifier using accuracy and the regressor using Mean Absolute Error (MAE). Example Submission ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.metrics import accuracy_score, mean_absolute_error # Load datasets classification_data = pd.read_csv(\'classification_data.csv\') regression_data = pd.read_csv(\'regression_data.csv\') # Preprocessing scaler = StandardScaler() # Classification X_class = classification_data.drop(\'target\', axis=1) y_class = classification_data[\'target\'] X_class_scaled = scaler.fit_transform(X_class) X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class_scaled, y_class, test_size=0.2, random_state=42) losses_cls = [\'hinge\', \'log_loss\'] penalties_cls = [\'l2\', \'l1\', \'elasticnet\'] print(\\"SGDClassifier Results:\\") for loss in losses_cls: for penalty in penalties_cls: clf = SGDClassifier(loss=loss, penalty=penalty, max_iter=1000, tol=1e-3, random_state=42) clf.fit(X_train_class, y_train_class) y_pred_class = clf.predict(X_test_class) accuracy = accuracy_score(y_test_class, y_pred_class) print(f\\"Loss: {loss}, Penalty: {penalty}, Accuracy: {accuracy:.4f}\\") # Regression X_reg = regression_data.drop(\'target\', axis=1) y_reg = regression_data[\'target\'] X_reg_scaled = scaler.fit_transform(X_reg) X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg_scaled, y_reg, test_size=0.2, random_state=42) losses_reg = [\'squared_error\', \'huber\'] penalties_reg = [\'l2\', \'l1\', \'elasticnet\'] print(\\"SGDRegressor Results:\\") for loss in losses_reg: for penalty in penalties_reg: reg = SGDRegressor(loss=loss, penalty=penalty, max_iter=1000, tol=1e-3, random_state=42) reg.fit(X_train_reg, y_train_reg) y_pred_reg = reg.predict(X_test_reg) mae = mean_absolute_error(y_test_reg, y_pred_reg) print(f\\"Loss: {loss}, Penalty: {penalty}, MAE: {mae:.4f}\\") ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.metrics import accuracy_score, mean_absolute_error def sgd_classification(classification_data_path): # Load dataset classification_data = pd.read_csv(classification_data_path) # Preprocessing scaler = StandardScaler() X_class = classification_data.drop(\'target\', axis=1) y_class = classification_data[\'target\'] X_class_scaled = scaler.fit_transform(X_class) X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class_scaled, y_class, test_size=0.2, random_state=42) losses_cls = [\'hinge\', \'log_loss\'] penalties_cls = [\'l2\', \'l1\', \'elasticnet\'] results = {} print(\\"SGDClassifier Results:\\") for loss in losses_cls: for penalty in penalties_cls: clf = SGDClassifier(loss=loss, penalty=penalty, max_iter=1000, tol=1e-3, random_state=42) clf.fit(X_train_class, y_train_class) y_pred_class = clf.predict(X_test_class) accuracy = accuracy_score(y_test_class, y_pred_class) results[(loss, penalty)] = accuracy print(f\\"Loss: {loss}, Penalty: {penalty}, Accuracy: {accuracy:.4f}\\") return results def sgd_regression(regression_data_path): # Load dataset regression_data = pd.read_csv(regression_data_path) # Preprocessing scaler = StandardScaler() X_reg = regression_data.drop(\'target\', axis=1) y_reg = regression_data[\'target\'] X_reg_scaled = scaler.fit_transform(X_reg) X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg_scaled, y_reg, test_size=0.2, random_state=42) losses_reg = [\'squared_error\', \'huber\'] penalties_reg = [\'l2\', \'l1\', \'elasticnet\'] results = {} print(\\"SGDRegressor Results:\\") for loss in losses_reg: for penalty in penalties_reg: reg = SGDRegressor(loss=loss, penalty=penalty, max_iter=1000, tol=1e-3, random_state=42) reg.fit(X_train_reg, y_train_reg) y_pred_reg = reg.predict(X_test_reg) mae = mean_absolute_error(y_test_reg, y_pred_reg) results[(loss, penalty)] = mae print(f\\"Loss: {loss}, Penalty: {penalty}, MAE: {mae:.4f}\\") return results"},{"question":"**Question: Implement a custom binhex encoder and decoder** Suppose you are tasked with handling files on a legacy system that uses the binhex4 format. As the `binhex` module is deprecated, you need to implement your own simplified version of binhex encoding and decoding functions. Your task is to write two functions: `custom_binhex(input_file, output_file)` and `custom_hexbin(input_file, output_file)`. # Functions Specification: 1. **custom_binhex(input_file, output_file)**: * **Input**: - `input_file` (str): The path to the binary file that needs to be encoded. - `output_file` (str): The path to the output binhex file. * **Output**: None * **Behavior**: Read the binary file specified by `input_file`, encode it in binhex format, and write the result to `output_file`. * **Constraints**: Implement basic encoding techniques performing ASCII conversion. 2. **custom_hexbin(input_file, output_file)**: * **Input**: - `input_file` (str): The path to the binhex file that needs to be decoded. - `output_file` (str): The path to the output binary file. * **Output**: None * **Behavior**: Read the binhex file specified by `input_file`, decode it to its original binary form, and write the result to `output_file`. * **Constraints**: Handle basic decoding techniques, converting from ASCII to binary format. # Exception Handling: - Ensure that your functions handle common exceptions, such as file not found, and use a custom exception `CustomBinhexError` for encoding/decoding issues. # Example Usage: ```python try: custom_binhex(\\"example.bin\\", \\"encoded.bh\\") custom_hexbin(\\"encoded.bh\\", \\"decoded.bin\\") except CustomBinhexError as e: print(f\\"Binhex encoding/decoding error: {e}\\") except FileNotFoundError as e: print(f\\"File not found: {e}\\") ``` # Notes: - You do not need to implement the exact binhex4 specification, but rather a simplified version demonstrating encoding and decoding concepts. - Focus on converting binary data to an ASCII representation and back. - Test your functions with different binary files to ensure they work correctly.","solution":"import binascii class CustomBinhexError(Exception): pass def custom_binhex(input_file, output_file): try: with open(input_file, \'rb\') as f: binary_data = f.read() ascii_data = binascii.b2a_hex(binary_data).decode(\'ascii\') with open(output_file, \'w\') as f: f.write(ascii_data) except FileNotFoundError as e: raise e except Exception as e: raise CustomBinhexError(\\"Error in encoding process\\") from e def custom_hexbin(input_file, output_file): try: with open(input_file, \'r\') as f: ascii_data = f.read() binary_data = binascii.a2b_hex(ascii_data.encode(\'ascii\')) with open(output_file, \'wb\') as f: f.write(binary_data) except FileNotFoundError as e: raise e except binascii.Error as e: raise CustomBinhexError(\\"Error in decoding process\\") from e except Exception as e: raise CustomBinhexError(\\"Unexpected error during decoding process\\") from e"},{"question":"# Kernel Approximation Assessment **Objective:** Implement a kernel approximation function using the Nystroem method and compare its performance with the exact kernel on a classification task. **Problem Statement:** You are provided with a dataset `X` (input features) and `y` (target labels). Your task is to implement a function `nystroem_classifier` that performs the following steps: 1. Use the Nystroem method to approximate the feature mapping of an RBF kernel. 2. Train a linear Support Vector Machine (SVM) classifier on this transformed data. 3. Train an exact kernel SVM using the same RBF kernel on the original data. 4. Compare and output the performance (accuracy) of both classifiers on the training data. # Input: - `X` (2D numpy array of shape `(n_samples, n_features)`): Input feature matrix. - `y` (1D numpy array of shape `(n_samples,)`): Target label array. - `n_components` (int): Number of components to use for the Nystroem kernel approximation. - `gamma` (float): Parameter for the RBF kernel. # Output: - A dictionary with keys `nystroem_accuracy` and `exact_accuracy` containing the accuracy values of the classifiers using the Nystroem method and the exact kernel method, respectively. # Function Signature: ```python def nystroem_classifier(X: np.ndarray, y: np.ndarray, n_components: int, gamma: float) -> dict: pass ``` # Constraints: - You may assume that `X` and `y` are numpy arrays and are pre-processed properly. - Use `sklearn.kernel_approximation.Nystroem` for the Nystroem method. - Use `sklearn.svm.SVC` for the SVM classifier. # Example: ```python import numpy as np # Example input data X = np.array([[0, 0], [1, 1], [1, 0], [0, 1]]) y = np.array([0, 0, 1, 1]) n_components = 2 gamma = 1.0 result = nystroem_classifier(X, y, n_components, gamma) print(result) # Expected output format: {\'nystroem_accuracy\': <accuracy_value>, \'exact_accuracy\': <accuracy_value>} ``` **Note:** Ensure the implementation uses appropriate metrics and scikit-learn functions to achieve the desired functionality.","solution":"import numpy as np from sklearn.kernel_approximation import Nystroem from sklearn.svm import SVC from sklearn.metrics import accuracy_score def nystroem_classifier(X: np.ndarray, y: np.ndarray, n_components: int, gamma: float) -> dict: Perform kernel approximation using the Nystroem method and compare its performance with the exact kernel on a classification task. Parameters: - X: 2D numpy array of shape (n_samples, n_features), input feature matrix. - y: 1D numpy array of shape (n_samples,), target label array. - n_components: int, number of components for the Nystroem kernel approximation. - gamma: float, parameter for the RBF kernel. Returns: - dict: Dictionary with keys \'nystroem_accuracy\' and \'exact_accuracy\' containing accuracy values of the classifiers using the Nystroem method and the exact kernel method, respectively. # Nystroem method approximation nystroem = Nystroem(kernel=\'rbf\', gamma=gamma, n_components=n_components) X_transformed = nystroem.fit_transform(X) # Train linear SVM on Nystroem transformed data svm_nystroem = SVC(kernel=\'linear\') svm_nystroem.fit(X_transformed, y) y_pred_nystroem = svm_nystroem.predict(X_transformed) nystroem_accuracy = accuracy_score(y, y_pred_nystroem) # Train exact kernel SVM using RBF kernel svm_exact = SVC(kernel=\'rbf\', gamma=gamma) svm_exact.fit(X, y) y_pred_exact = svm_exact.predict(X) exact_accuracy = accuracy_score(y, y_pred_exact) return { \'nystroem_accuracy\': nystroem_accuracy, \'exact_accuracy\': exact_accuracy }"},{"question":"**Source Distribution Automation Task** You have been given the task of automating the creation of a source distribution for a Python project. Your goal is to create a script that simulates the main behavior of the `sdist` command, ensuring specific files are included in the distribution based on a manifest template. This task will help you understand Python file handling and basic automation techniques. # Problem Statement Write a script that reads a simple manifest template (`MANIFEST.in`) and generates a source distribution archive (`.tar.gz`) containing the specified files. # Requirements: 1. **Input:** - A manifest template file named `MANIFEST.in` in the current directory. - The manifest template file contains lines with the format `include <file_pattern>`, where `<file_pattern>` specifies the file patterns to be included in the distribution. 2. **Output:** - A compressed tar file (`.tar.gz`) named `source_dist.tar.gz` containing all the files specified by the manifest template. # Constraints: - The script should be executable using Python 3.10. - You may use libraries from the Python Standard Library such as `tarfile`, `glob`, and `os`. - Only files specified by the `MANIFEST.in` template should be included in the tarball. # Example: Given a directory structure: ``` project/ MANIFEST.in README.md setup.py src/ main.py utils.py tests/ test_main.py test_utils.py ``` And a `MANIFEST.in` file: ``` include README.md include setup.py include src/*.py ``` The resulting tarball `source_dist.tar.gz` should contain: ``` README.md setup.py src/main.py src/utils.py ``` # Implementation: You need to implement the following function: ```python import tarfile import glob import os def create_source_distribution(): # Read the MANIFEST.in file and gather the patterns to include with open(\'MANIFEST.in\', \'r\') as manifest_file: include_patterns = [line.strip().split(\' \')[1] for line in manifest_file if line.startswith(\'include\')] # Collect all the files matching the include patterns files_to_include = [] for pattern in include_patterns: files_to_include.extend(glob.glob(pattern)) # Create a compressed tar file and add the collected files with tarfile.open(\'source_dist.tar.gz\', \'w:gz\') as tar: for file_path in files_to_include: tar.add(file_path, arcname=os.path.relpath(file_path)) # Main execution if __name__ == \\"__main__\\": create_source_distribution() ``` **Note:** Make sure you test your script with various directory structures and manifest templates to ensure it works correctly.","solution":"import tarfile import glob import os def create_source_distribution(): # Read the MANIFEST.in file and gather the patterns to include with open(\'MANIFEST.in\', \'r\') as manifest_file: include_patterns = [line.strip().split(\' \')[1] for line in manifest_file if line.strip().startswith(\'include\')] # Collect all the files matching the include patterns files_to_include = [] for pattern in include_patterns: files_to_include.extend(glob.glob(pattern)) # Create a compressed tar file and add the collected files with tarfile.open(\'source_dist.tar.gz\', \'w:gz\') as tar: for file_path in files_to_include: tar.add(file_path, arcname=os.path.relpath(file_path)) # Main execution if __name__ == \\"__main__\\": create_source_distribution()"},{"question":"**Problem Statement: Random Tensor Initialization and Manipulation** You are required to write functions that demonstrate your understanding of random tensor generation and manipulation using the PyTorch library. Specifically, you need to implement the following functions: 1. **`generate_random_tensor(shape, seed=None)`** - **Input**: - `shape` (tuple): A tuple specifying the shape of the tensor to generate. - `seed` (int or None): An optional random seed for reproducibility. If `None`, the seed won\'t be set. - **Output**: - Returns a PyTorch tensor of the specified shape with random values from a uniform distribution on the interval [0, 1). 2. **`shuffle_tensor(tensor, seed=None)`** - **Input**: - `tensor` (torch.Tensor): A 2D tensor to shuffle. - `seed` (int or None): An optional random seed for reproducibility. If `None`, the seed won\'t be set. - **Output**: - Returns a shuffled version of the input tensor where the rows of the tensor are shuffled in random order. 3. **`random_choice_from_tensor(tensor, num_samples, seed=None)`** - **Input**: - `tensor` (torch.Tensor): A 1D tensor from which to sample. - `num_samples` (int): The number of random samples to draw. - `seed` (int or None): An optional random seed for reproducibility. If `None`, the seed won\'t be set. - **Output**: - Returns a new tensor containing `num_samples` randomly chosen elements from the input tensor. **Constraints:** - You are required to use the `torch.random` module for random number generation. - Assume the input tensors are non-empty and valid according to their respective shapes. **Performance Requirements:** - Your implementations should efficiently handle tensors with up to `10^6` elements. ```python import torch def generate_random_tensor(shape, seed=None): Generates a PyTorch tensor of the specified shape with random values from a uniform distribution on [0, 1). Args: shape (tuple): The shape of the tensor to generate. seed (int or None): An optional random seed for reproducibility. Returns: torch.Tensor: A tensor filled with random values from a uniform distribution on [0, 1). pass def shuffle_tensor(tensor, seed=None): Shuffles the rows of a 2D tensor. Args: tensor (torch.Tensor): A 2D tensor to shuffle. seed (int or None): An optional random seed for reproducibility. Returns: torch.Tensor: A tensor with shuffled rows. pass def random_choice_from_tensor(tensor, num_samples, seed=None): Randomly samples elements from a 1D tensor. Args: tensor (torch.Tensor): A 1D tensor from which to sample. num_samples (int): The number of random samples to draw. seed (int or None): An optional random seed for reproducibility. Returns: torch.Tensor: A tensor containing the randomly chosen samples. pass ``` _Note: Include all import statements at the beginning of your code. Ensure thorough testing against edge cases._","solution":"import torch def generate_random_tensor(shape, seed=None): Generates a PyTorch tensor of the specified shape with random values from a uniform distribution on [0, 1). Args: shape (tuple): The shape of the tensor to generate. seed (int or None): An optional random seed for reproducibility. Returns: torch.Tensor: A tensor filled with random values from a uniform distribution on [0, 1). if seed is not None: torch.random.manual_seed(seed) return torch.rand(shape) def shuffle_tensor(tensor, seed=None): Shuffles the rows of a 2D tensor. Args: tensor (torch.Tensor): A 2D tensor to shuffle. seed (int or None): An optional random seed for reproducibility. Returns: torch.Tensor: A tensor with shuffled rows. if seed is not None: torch.random.manual_seed(seed) indices = torch.randperm(tensor.size(0)) return tensor[indices] def random_choice_from_tensor(tensor, num_samples, seed=None): Randomly samples elements from a 1D tensor. Args: tensor (torch.Tensor): A 1D tensor from which to sample. num_samples (int): The number of random samples to draw. seed (int or None): An optional random seed for reproducibility. Returns: torch.Tensor: A tensor containing the randomly chosen samples. if seed is not None: torch.random.manual_seed(seed) indices = torch.randperm(tensor.size(0))[:num_samples] return tensor[indices]"},{"question":"# SAX Handler for Extracting Specific Information You are tasked with writing a SAX handler to extract specific information from an XML document. The XML document contains a list of books, and each book has an author, title, and year of publication. Your task is to write a handler that extracts the titles and years of publication for books written by a specific author. Expected Input and Output Formats **Input:** - An XML file in the following format: ```xml <catalog> <book> <author>Author Name</author> <title>Book Title</title> <year>Year</year> </book> ... </catalog> ``` - The name of the author for whom you want to extract book titles and years. **Output:** - A list of tuples, where each tuple contains the title and the year of publication of a book written by the specified author. Constraints and Limitations - The XML document may contain multiple books by different authors. - Each book element will contain exactly one author, title, and year element. # Performance Requirements - The solution should handle large XML documents efficiently using the SAX parsing approach. # Implementation Requirements 1. Implement a class `BookHandler` that inherits from `xml.sax.handler.ContentHandler`. 2. Override the necessary methods to process the XML events and extract the required information. 3. Implement a function `get_books_by_author(xml_file, author_name)` that: - Takes in the path to the XML file and the author\'s name. - Sets up the SAX parser with the `BookHandler`. - Returns a list of tuples with the titles and years of books by the specified author. # Example Given the following XML content in a file named `books.xml`: ```xml <catalog> <book> <author>J.K. Rowling</author> <title>Harry Potter and the Philosopher\'s Stone</title> <year>1997</year> </book> <book> <author>J.K. Rowling</author> <title>Harry Potter and the Chamber of Secrets</title> <year>1998</year> </book> <book> <author>George Orwell</author> <title>1984</title> <year>1949</year> </book> </catalog> ``` Calling the function `get_books_by_author(\'books.xml\', \'J.K. Rowling\')` should return: ```python [(\'Harry Potter and the Philosopher\'s Stone\', 1997), (\'Harry Potter and the Chamber of Secrets\', 1998)] ``` Here\'s the skeleton of the code you need to complete: ```python import xml.sax class BookHandler(xml.sax.handler.ContentHandler): def __init__(self, target_author): super().__init__() self.target_author = target_author self.current_element = \'\' self.current_title = \'\' self.current_year = \'\' self.books = [] self.is_target_author = False def startElement(self, name, attrs): self.current_element = name def endElement(self, name): if name == \'book\': if self.is_target_author: self.books.append((self.current_title, self.current_year)) self.is_target_author = False def characters(self, content): if self.current_element == \'author\': if content.strip() == self.target_author: self.is_target_author = True elif self.current_element == \'title\': self.current_title = content.strip() elif self.current_element == \'year\': self.current_year = content.strip() def get_books_by_author(xml_file, author_name): handler = BookHandler(author_name) parser = xml.sax.make_parser() parser.setContentHandler(handler) with open(xml_file, \'r\') as file: parser.parse(file) return handler.books ``` Make sure your implementation adheres to the structure and correctly handles the input and output as specified.","solution":"import xml.sax class BookHandler(xml.sax.handler.ContentHandler): def __init__(self, target_author): super().__init__() self.target_author = target_author self.current_element = \'\' self.current_title = \'\' self.current_year = \'\' self.books = [] self.is_target_author = False def startElement(self, name, attrs): self.current_element = name if name == \'book\': self.current_title = \'\' self.current_year = \'\' self.is_target_author = False def endElement(self, name): if name == \'book\' and self.is_target_author: self.books.append((self.current_title, self.current_year)) self.current_element = \'\' def characters(self, content): if self.current_element == \'author\': if content.strip() == self.target_author: self.is_target_author = True elif self.current_element == \'title\' and self.is_target_author: self.current_title = content.strip() elif self.current_element == \'year\' and self.is_target_author: self.current_year = content.strip() def get_books_by_author(xml_file, author_name): handler = BookHandler(author_name) parser = xml.sax.make_parser() parser.setContentHandler(handler) with open(xml_file, \'r\') as file: parser.parse(file) return handler.books"},{"question":"To assess your understanding of pandas and its capabilities, consider the following coding assessment question: **Question:** You are given two DataFrames, `orders` and `products`. 1. The `orders` DataFrame contains the following columns: - `order_id`: A unique identifier for each order. - `product_id`: An identifier for the product in the order. - `quantity`: The quantity of the product in the order. - `order_date`: The date when the order was made. - `customer_id`: An identifier for the customer who made the order. 2. The `products` DataFrame contains the following columns: - `product_id`: A unique identifier for each product. - `product_name`: The name of the product. - `category`: The category to which the product belongs. - `price`: The price of a single unit of the product. You need to perform the following tasks: 1. **Merge** the `orders` and `products` DataFrames on the `product_id` column to add product details to each order. 2. **Calculate** the total revenue for each product by multiplying the `quantity` column by the `price` column and summing the results. 3. **Identify** the top 3 products by total revenue. 4. **Plot** a bar chart showing the total revenue of these top 3 products using matplotlib. **Input and Output Formats:** - The input will be two pandas DataFrames: `orders` and `products`. - The output should be a pandas DataFrame containing the top 3 products by total revenue and a bar chart showing the total revenue of these products. ```python import pandas as pd import matplotlib.pyplot as plt def top_3_products_by_revenue(orders, products): # Merge the DataFrames on product_id merged_df = pd.merge(orders, products, on=\'product_id\') # Calculate the total revenue for each product merged_df[\'revenue\'] = merged_df[\'quantity\'] * merged_df[\'price\'] product_revenue = merged_df.groupby(\'product_id\').agg({\'revenue\': \'sum\'}) # Merge to get product details product_revenue = product_revenue.merge(products[[\'product_id\', \'product_name\']], on=\'product_id\') # Identify the top 3 products by total revenue top_3_products = product_revenue.sort_values(by=\'revenue\', ascending=False).head(3) # Plot the bar chart plt.bar(top_3_products[\'product_name\'], top_3_products[\'revenue\']) plt.xlabel(\'Product Name\') plt.ylabel(\'Total Revenue\') plt.title(\'Top 3 Products by Total Revenue\') plt.show() return top_3_products # Example Usage orders = pd.DataFrame({ \'order_id\': [1, 2, 3, 4, 5], \'product_id\': [101, 102, 103, 101, 102], \'quantity\': [2, 3, 1, 1, 2], \'order_date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-03\', \'2023-01-04\', \'2023-01-05\'], \'customer_id\': [201, 202, 201, 203, 202] }) products = pd.DataFrame({ \'product_id\': [101, 102, 103, 104], \'product_name\': [\'Product A\', \'Product B\', \'Product C\', \'Product D\'], \'category\': [\'Category 1\', \'Category 2\', \'Category 1\', \'Category 3\'], \'price\': [10.0, 20.0, 30.0, 40.0] }) top_3_products_by_revenue(orders, products) ``` **Constraints or Limitations:** - Assume that the `orders` and `products` data have no missing values. - Optimally, the solution should handle large datasets efficiently. **Performance Requirements:** - The solution should efficiently handle merges and aggregations for large datasets with millions of records.","solution":"import pandas as pd import matplotlib.pyplot as plt def top_3_products_by_revenue(orders, products): # Merge the DataFrames on product_id merged_df = pd.merge(orders, products, on=\'product_id\') # Calculate the total revenue for each product merged_df[\'revenue\'] = merged_df[\'quantity\'] * merged_df[\'price\'] product_revenue = merged_df.groupby(\'product_id\').agg({\'revenue\': \'sum\'}).reset_index() # Merge to get product details product_revenue = pd.merge(product_revenue, products[[\'product_id\', \'product_name\']], on=\'product_id\') # Identify the top 3 products by total revenue top_3_products = product_revenue.sort_values(by=\'revenue\', ascending=False).head(3) # Plot the bar chart plt.bar(top_3_products[\'product_name\'], top_3_products[\'revenue\']) plt.xlabel(\'Product Name\') plt.ylabel(\'Total Revenue\') plt.title(\'Top 3 Products by Total Revenue\') plt.show() return top_3_products"},{"question":"# Advanced File and Directory Operations in Python **Problem Statement**: You are tasked with writing a Python script that performs various advanced file and directory operations. Specifically, you need to implement the following function: ```python def file_directory_operations(base_dir: str, temp_dir: str) -> dict: Perform multiple file and directory operations including creation, manipulation, comparison, and temporary file handling. Parameters: base_dir (str): The path to the base directory where operations will be performed. temp_dir (str): The path to a temporary directory where temporary files will be created. Returns: dict: A dictionary with the following keys: - \'created_files\': A list of file paths of the created files. - \'common_files\': A list of common files between directories. - \'archived_path\': The path to the archived file. pass ``` # Required Operations: 1. **File Creation**: - Create 3 text files named `file1.txt`, `file2.txt`, and `file3.txt` within the `base_dir`. - Write unique content to each of these files. 2. **Temporary File Handling**: - Create a temporary file within the `temp_dir` using the `tempfile` module. - Write some data to this temporary file and ensure it exists. 3. **Directory Comparison**: - Compare files within `base_dir` and another directory (you can create a test directory `test_dir` with some overlapping files). - Identify and list any common files between the directories. 4. **Archiving**: - Archive the `base_dir` into a single zip file `base_dir_archive.zip` and store it within the `base_dir`. # Constraints and Requirements: - Use the appropriate modules mentioned in the documentation to perform each of these tasks efficiently. - Ensure proper error handling for file operations (e.g., file existence checks). - Aim for efficiency in both time and space complexity. **Input**: - `base_dir` (string): Path to the base directory. - `temp_dir` (string): Path to the temporary directory. **Output**: - Dictionary with keys `created_files`, `common_files`, and `archived_path` as described in the function signature. # Example Usage: ```python base_dir = \\"/path/to/base_dir\\" temp_dir = \\"/path/to/temp_dir\\" result = file_directory_operations(base_dir, temp_dir) print(result) ``` This problem assesses your understanding of file and directory operations, temporary file handling, and directory comparison using Python\'s advanced file handling modules.","solution":"import os import tempfile import zipfile def file_directory_operations(base_dir: str, temp_dir: str) -> dict: Perform multiple file and directory operations including creation, manipulation, comparison, and temporary file handling. Parameters: base_dir (str): The path to the base directory where operations will be performed. temp_dir (str): The path to a temporary directory where temporary files will be created. Returns: dict: A dictionary with the following keys: - \'created_files\': A list of file paths of the created files. - \'common_files\': A list of common files between directories. - \'archived_path\': The path to the archived file. # Ensure base directory exists os.makedirs(base_dir, exist_ok=True) # File Creation filenames = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] created_files = [] for i, filename in enumerate(filenames): file_path = os.path.join(base_dir, filename) with open(file_path, \'w\') as f: f.write(f\\"Content of {filename}n\\") created_files.append(file_path) # Temporary File Handling temp_file_path = \\"\\" with tempfile.NamedTemporaryFile(dir=temp_dir, delete=False) as temp_file: temp_file.write(b\\"This is some temporary data.n\\") temp_file_path = temp_file.name # Directory Comparison test_dir = os.path.join(base_dir, \'test_dir\') os.makedirs(test_dir, exist_ok=True) # Creating test files in test_dir that overlap with base_dir common_file = os.path.join(test_dir, \'file2.txt\') with open(common_file, \'w\') as f: f.write(\\"This is a common file.n\\") common_files = list( set(filenames).intersection(set(os.listdir(test_dir))) ) # Archiving archived_path = os.path.join(base_dir, \'base_dir_archive.zip\') with zipfile.ZipFile(archived_path, \'w\') as zipf: for root, _, files in os.walk(base_dir): for file in files: full_path = os.path.join(root, file) arcname = os.path.relpath(full_path, start=base_dir) zipf.write(full_path, arcname=arcname) return { \'created_files\': created_files, \'common_files\': common_files, \'archived_path\': archived_path }"},{"question":"**Objective:** Implement a machine learning pipeline that performs cross-validation with multiple evaluation metrics, ensuring the process includes data preprocessing steps. **Problem Statement:** You are given a dataset containing information about flower species from the iris dataset. Your task is to: 1. Split the dataset into training and testing sets using a stratified split. 2. Build a machine learning pipeline that includes data standardization and a Support Vector Machine (SVM) classifier. 3. Implement a cross-validation strategy to evaluate the performance of the model using multiple metrics. 4. Report the mean and standard deviation of the evaluated metrics. # Requirements: 1. Data Splitting: - Use the `train_test_split` function to split the data into training and testing sets, with 40% of the data reserved for testing. - Ensure the split is stratified based on the class labels. 2. Pipeline Construction: - Construct a pipeline that first standardizes the data and then fits an SVM classifier with a linear kernel. 3. Cross-Validation: - Implement K-Fold cross-validation with 5 splits. - Evaluate the model using at least two metrics: accuracy and f1_macro. 4. Reporting: - Report the mean and standard deviation for each evaluation metric across the cross-validation folds. # Additional Constraints: - The random state should be set to 42 wherever applicable to ensure reproducibility. # Input: - No input from the user is needed. Use the Iris dataset available from `sklearn.datasets`. # Expected Output: - A dictionary containing the mean and standard deviation of each evaluation metric. # Example: ```python { \'accuracy_mean\': 0.98, \'accuracy_std\': 0.02, \'f1_macro_mean\': 0.97, \'f1_macro_std\': 0.03 } ``` # Implementation Details: 1. Import necessary libraries: ```python import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split, cross_validate from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.svm import SVC from sklearn.metrics import make_scorer, accuracy_score, f1_score ``` 2. Load the dataset: ```python X, y = datasets.load_iris(return_X_y=True) ``` 3. Split the dataset: ```python X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42) ``` 4. Construct the pipeline: ```python pipeline = make_pipeline(StandardScaler(), SVC(kernel=\'linear\', random_state=42)) ``` 5. Define the scoring metrics: ```python scoring = {\'accuracy\': make_scorer(accuracy_score), \'f1_macro\': make_scorer(f1_score, average=\'macro\')} ``` 6. Implement cross-validation and compute metrics: ```python cv_results = cross_validate(pipeline, X_train, y_train, cv=5, scoring=scoring) metrics = { \'accuracy_mean\': np.mean(cv_results[\'test_accuracy\']), \'accuracy_std\': np.std(cv_results[\'test_accuracy\']), \'f1_macro_mean\': np.mean(cv_results[\'test_f1_macro\']), \'f1_macro_std\': np.std(cv_results[\'test_f1_macro\']) } print(metrics) ``` Output: ```python { \'accuracy_mean\': ..., # fill in the correct values based on your computation \'accuracy_std\': ..., \'f1_macro_mean\': ..., \'f1_macro_std\': ... } ``` **Note:** Ensure you understand each step of the question, particularly the usage of cross-validation and the significance of reporting mean and standard deviation for evaluation metrics.","solution":"import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split, cross_validate from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.svm import SVC from sklearn.metrics import make_scorer, accuracy_score, f1_score def evaluate_model(): # Load the dataset X, y = datasets.load_iris(return_X_y=True) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42) # Construct the pipeline pipeline = make_pipeline(StandardScaler(), SVC(kernel=\'linear\', random_state=42)) # Define the scoring metrics scoring = {\'accuracy\': make_scorer(accuracy_score), \'f1_macro\': make_scorer(f1_score, average=\'macro\')} # Implement cross-validation and compute metrics cv_results = cross_validate(pipeline, X_train, y_train, cv=5, scoring=scoring) metrics = { \'accuracy_mean\': np.mean(cv_results[\'test_accuracy\']), \'accuracy_std\': np.std(cv_results[\'test_accuracy\']), \'f1_macro_mean\': np.mean(cv_results[\'test_f1_macro\']), \'f1_macro_std\': np.std(cv_results[\'test_f1_macro\']) } return metrics"},{"question":"# **Linear and Quadratic Discriminant Analysis Coding Assessment** Objective: Demonstrate your understanding of Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) by implementing data classification and dimensionality reduction using LDA. Compare the performance of these techniques on a synthetic dataset. Problem Statement: 1. **Generate a Synthetic Dataset**: - Create a synthetic 3-class dataset with 3 features using the `make_classification` function from `sklearn.datasets`. - Ensure that the dataset has 100 samples, standard deviation of 1.0 between clusters, and each class has one informative, one redundant, and one random feature. 2. **Implement Classification Using LDA and QDA**: - Fit and predict the labels using LDA with `scikit-learn`\'s `LinearDiscriminantAnalysis` class. - Fit and predict the labels using QDA with `scikit-learn`\'s `QuadraticDiscriminantAnalysis` class. 3. **Dimensionality Reduction Using LDA**: - Use LDA for dimensionality reduction, projecting the data to a 2-dimensional space. - Visualize the 2D projection, using different colors for each class. 4. **Evaluation**: - Evaluate and compare the accuracy of both LDA and QDA models on the synthetic dataset using `accuracy_score` from `sklearn.metrics`. - Comment on the shape and nature of the decision boundaries based on the results of LDA and QDA. 5. **Optional**: Implement shrinkage for LDA using the \'lsqr\' solver and repeat the steps, observing the changes in accuracy and decision boundaries. Expected Function Signatures: ```python from sklearn.datasets import make_classification from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt # Function to generate synthetic dataset def generate_dataset(n_samples=100): # Generate dataset here pass # Function for LDA classification def lda_classification(X_train, y_train, X_test, y_test): # Implement LDA classification here pass # Function for QDA classification def qda_classification(X_train, y_train, X_test, y_test): # Implement QDA classification here pass # Function for LDA dimensionality reduction def lda_dimensionality_reduction(X, y): # Implement dimensionality reduction using LDA here pass # Main function to perform end-to-end task def main(): # 1. Generate dataset X, y = generate_dataset() # Split data into training and testing sets from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 2. Perform LDA classification lda_accuracy = lda_classification(X_train, y_train, X_test, y_test) # 3. Perform QDA classification qda_accuracy = qda_classification(X_train, y_train, X_test, y_test) # Print LDA and QDA accuracy print(f\'LDA Accuracy: {lda_accuracy}\') print(f\'QDA Accuracy: {qda_accuracy}\') # 4. Perform LDA dimensionality reduction and plot lda_dimensionality_reduction(X, y) if __name__ == \\"__main__\\": main() ``` Input Format: - The functions do not take external input. They operate within the context of this script. Output Format: - Print the classification accuracy for both LDA and QDA. - Display the 2D scatter plot showing the results of LDA dimensionality reduction. Constraints: - Use `scikit-learn` for LDA and QDA implementations. - Maintain a high standard of code modularity and readability.","solution":"from sklearn.datasets import make_classification from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt import numpy as np # Function to generate synthetic dataset def generate_dataset(n_samples=100): X, y = make_classification(n_samples=n_samples, n_features=3, n_informative=2, n_redundant=1, n_clusters_per_class=1, n_classes=3, random_state=42) return X, y # Function for LDA classification def lda_classification(X_train, y_train, X_test, y_test): lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) y_pred = lda.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy # Function for QDA classification def qda_classification(X_train, y_train, X_test, y_test): qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) y_pred = qda.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy # Function for LDA dimensionality reduction def lda_dimensionality_reduction(X, y): lda = LinearDiscriminantAnalysis(n_components=2) X_r2 = lda.fit(X, y).transform(X) colors = [\'red\', \'green\', \'blue\'] target_names = np.unique(y) plt.figure() for color, i, target_name in zip(colors, target_names, target_names): plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA of synthetic dataset\') plt.xlabel(\'LD1\') plt.ylabel(\'LD2\') plt.show() # Main function to perform end-to-end task def main(): # 1. Generate dataset X, y = generate_dataset() # Split data into training and testing sets from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 2. Perform LDA classification lda_accuracy = lda_classification(X_train, y_train, X_test, y_test) # 3. Perform QDA classification qda_accuracy = qda_classification(X_train, y_train, X_test, y_test) # Print LDA and QDA accuracy print(f\'LDA Accuracy: {lda_accuracy}\') print(f\'QDA Accuracy: {qda_accuracy}\') # 4. Perform LDA dimensionality reduction and plot lda_dimensionality_reduction(X, y) if __name__ == \\"__main__\\": main()"},{"question":"# Custom Sequence Implementation As a Python developer, you are tasked with creating a custom sequence type that mimics the behavior of Python\'s built-in list but adds some additional functionality. This custom sequence should inherit from the appropriate abstract base classes provided by the `collections.abc` module. Requirements: 1. Create a class named `CustomList` that supports indexing, length retrieval, and element containment. 2. The class should inherit from `collections.abc.Sequence`. 3. Implement the necessary abstract methods required by the `Sequence` ABC: `__getitem__`, `__len__`. 4. Add an additional method `reverse_slice(start: int, end: int) -> list` that returns a list containing the elements from the specified slice range in reverse order. 5. Register `CustomList` as a virtual subclass of `collections.abc.Collection`. Input and Output Formats: - Input: - The `CustomList` will be initialized with an iterable. - The `reverse_slice` method will take two integers, `start` and `end`, representing the slice range. - Output: - The `reverse_slice` method should output a list of elements in reverse order of the specified slice. Example: ```python from collections.abc import Sequence, Collection class CustomList(Sequence): def __init__(self, iterable): self._data = list(iterable) def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) def reverse_slice(self, start, end): return self._data[start:end][::-1] # Registering CustomList as a virtual subclass of Collection Collection.register(CustomList) # Example usage c_list = CustomList([1, 2, 3, 4, 5]) print(c_list.reverse_slice(1, 4)) # Output: [4, 3, 2] print(len(c_list)) # Output: 5 print(c_list[2]) # Output: 3 print(isinstance(c_list, Collection)) # Output: True ``` Constraints: - The class should handle typical edge cases such as empty initialization and out-of-bound slice ranges gracefully. - Performance expectations should adhere to those of a typical list operations, i.e., indexing and slicing should be efficient. Your task is to implement the `CustomList` class according to the above specifications. Submit your code implementation along with test cases demonstrating its usage.","solution":"from collections.abc import Sequence, Collection class CustomList(Sequence): def __init__(self, iterable): self._data = list(iterable) def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) def reverse_slice(self, start, end): return self._data[start:end][::-1] # Registering CustomList as a virtual subclass of Collection Collection.register(CustomList) # Example usage c_list = CustomList([1, 2, 3, 4, 5]) print(c_list.reverse_slice(1, 4)) # Output: [4, 3, 2] print(len(c_list)) # Output: 5 print(c_list[2]) # Output: 3 print(isinstance(c_list, Collection)) # Output: True"},{"question":"# Question: Custom Scorer Implementation and Model Evaluation You are working on a binary classification problem where the dataset provided has a significant imbalance between the classes, with 90% of the samples belonging to the negative class (0) and only 10% to the positive class (1). Dataset: ```python from sklearn.datasets import make_classification # Simulate imbalanced dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_classes=2, weights=[0.9, 0.1], flip_y=0, random_state=1) ``` Task: 1. **Implement a Custom Scorer:** Write a function `custom_f2_scorer` that calculates the F2-score, which is a variant of the F1-score. The F2-score gives more weight to recall than precision. Then, integrate this with scikit-learn\'s `make_scorer` function. 2. **Train and Evaluate Models:** - Use Logistic Regression to train a model on this dataset. Evaluate the model using the default scoring method (accuracy). - Perform cross-validation with the custom F2-score on the Logistic Regression model. 3. **Generate and Plot Evaluation Metrics:** - Compute the confusion matrix and print it. - Generate and plot the ROC curve for the model. - Print the average F2-score from the cross-validation. Constraints: - You must use scikit-learn for all model training and evaluation tasks. - Use 5-fold cross-validation for evaluation purposes. - The custom scorer function should correctly implement the F2-score formula. Expected Input/Output: - **Input:** No direct input; the dataset and parameters for classifiers are already defined within the script. - **Output:** 1. Custom scorer implementation 2. Confusion matrix 3. ROC curve plot 4. Average F2-score from cross-validation Example Code Snippet: ```python import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score, train_test_split from sklearn.metrics import make_scorer, fbeta_score, confusion_matrix, roc_curve, auc import matplotlib.pyplot as plt # 1. Implementing the custom F2 scorer function def custom_f2_scorer(y_true, y_pred): return fbeta_score(y_true, y_pred, beta=2) f2_scorer = make_scorer(custom_f2_scorer) # 2. Train and Evaluate Model X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) model = LogisticRegression(solver=\'liblinear\') model.fit(X_train, y_train) # Evaluate using default accuracy accuracy = model.score(X_test, y_test) print(f\\"Accuracy: {accuracy}\\") # Evaluate using custom F2 score with cross-validation f2_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=f2_scorer) print(f\\"Average F2-score: {np.mean(f2_scores)}\\") # 3. Generate Evaluation Metrics y_pred = model.predict(X_test) cm = confusion_matrix(y_test, y_pred) print(f\\"Confusion Matrix:n{cm}\\") # ROC curve y_pred_prob = model.predict_proba(X_test)[:, 1] fpr, tpr, _ = roc_curve(y_test, y_pred_prob) roc_auc = auc(fpr, tpr) plt.figure() plt.plot(fpr, tpr, color=\'darkorange\', lw=2, label=f\'ROC curve (area = {roc_auc:.2f})\') plt.plot([0, 1], [0, 1], color=\'navy\', lw=2, linestyle=\'--\') plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\'False Positive Rate\') plt.ylabel(\'True Positive Rate\') plt.title(\'Receiver Operating Characteristic\') plt.legend(loc=\\"lower right\\") plt.show() ``` Submission Guidelines: - Ensure your code is properly commented and formatted. - Provide explanations where necessary. Good luck!","solution":"import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score, train_test_split from sklearn.metrics import make_scorer, fbeta_score, confusion_matrix, roc_curve, auc import matplotlib.pyplot as plt # Simulate imbalanced dataset from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_classes=2, weights=[0.9, 0.1], flip_y=0, random_state=1) # 1. Implementing the custom F2 scorer function def custom_f2_scorer(y_true, y_pred): Custom F2 scorer function. Parameters: y_true: array-like of shape (n_samples,) True labels. y_pred: array-like of shape (n_samples,) Predicted labels. Returns: float F2 score. return fbeta_score(y_true, y_pred, beta=2) f2_scorer = make_scorer(custom_f2_scorer) # 2. Train and Evaluate Model X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) model = LogisticRegression(solver=\'liblinear\') model.fit(X_train, y_train) # Evaluate using default accuracy accuracy = model.score(X_test, y_test) print(f\\"Accuracy: {accuracy}\\") # Evaluate using custom F2 score with cross-validation f2_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=f2_scorer) print(f\\"Average F2-score: {np.mean(f2_scores)}\\") # 3. Generate Evaluation Metrics y_pred = model.predict(X_test) cm = confusion_matrix(y_test, y_pred) print(f\\"Confusion Matrix:n{cm}\\") # ROC curve y_pred_prob = model.predict_proba(X_test)[:, 1] fpr, tpr, _ = roc_curve(y_test, y_pred_prob) roc_auc = auc(fpr, tpr) plt.figure() plt.plot(fpr, tpr, color=\'darkorange\', lw=2, label=f\'ROC curve (area = {roc_auc:.2f})\') plt.plot([0, 1], [0, 1], color=\'navy\', lw=2, linestyle=\'--\') plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\'False Positive Rate\') plt.ylabel(\'True Positive Rate\') plt.title(\'Receiver Operating Characteristic\') plt.legend(loc=\\"lower right\\") plt.show()"},{"question":"Your task is to implement a function that performs a series of numeric manipulations on a given input list of numbers. You need to use the appropriate functions described in the provided documentation and ensure that you handle errors gracefully where necessary. Function Signature ```python def numeric_transformations(number_list): pass ``` Input - `number_list`: A list of tuples, where each tuple contains three elements: - A string representing a numeric operation (one of \'add\', \'subtract\', \'multiply\', \'matrix_multiply\', \'floor_divide\', \'true_divide\', \'remainder\', \'divmod\', \'power\', \'negative\', \'positive\', \'absolute\', \'invert\', \'lshift\', \'rshift\', \'and\', \'xor\', \'or\', \'inplace_add\', \'inplace_subtract\', \'inplace_multiply\', \'inplace_matrix_multiply\', \'inplace_floor_divide\', \'inplace_true_divide\', \'inplace_remainder\', \'inplace_power\', \'inplace_lshift\', \'inplace_rshift\', \'inplace_and\', \'inplace_xor\', \'inplace_or\'). - The second and third elements are the operands for the operation. If the operation only requires one operand, the third element should be `None`. Output - A list of results, where each result corresponds to the operation performed on the operands from the input list. Constraints - You must handle errors gracefully. If an operation fails, the output for that operation should be `None`. - Where applicable, operations should be handled in-place as specified in the operation name (e.g., \'inplace_add\'). Example ```python number_list = [ (\'add\', 3, 5), (\'subtract\', 10, 4), (\'multiply\', 6, 7), (\'true_divide\', 9, 3), (\'remainder\', 10, 6), (\'power\', 2, (3, None)), # Equivalent to 2**3 (\'negative\', 4, None), (\'inplace_add\', [10], 20) # Note that in-place operations work on mutable types ] print(numeric_transformations(number_list)) ``` Expected Output: ``` [8, 6, 42, 3.0, 4, 8, -4, 30] ``` Notes - For the \'power\' operation, the tuple `(3, None)` indicates that the optional third parameter `mod` should be ignored. - In-place operations should be correctly reflected in the output list. The in-place addition in the example modifies the list element `[10]`, resulting in `[30]`. Implementation Details - Use the functions described in the documentation to perform the operations. - Ensure that each function handles errors correctly by returning `None` for any operation that fails.","solution":"import operator def numeric_transformations(number_list): Perform a series of numeric manipulations on a given input list of operations. :param number_list: A list of tuples, where each tuple contains three elements: - A string representing a numeric operation. - The second and third elements are the operands for the operation. If the operation only requires one operand, the third element should be `None`. :return: A list of results, where each result corresponds to the operation performed on the operands from the input list. results = [] for operation, operand1, operand2 in number_list: try: if operation == \'add\': result = operator.add(operand1, operand2) elif operation == \'subtract\': result = operator.sub(operand1, operand2) elif operation == \'multiply\': result = operator.mul(operand1, operand2) elif operation == \'true_divide\': result = operator.truediv(operand1, operand2) elif operation == \'floor_divide\': result = operator.floordiv(operand1, operand2) elif operation == \'remainder\': result = operator.mod(operand1, operand2) elif operation == \'divmod\': result = divmod(operand1, operand2) elif operation == \'power\': if operand2 is not None: result = operator.pow(operand1, operand2) else: result = operator.pow(operand1, 1) # Default to power of 1 if second operand is None elif operation == \'negative\': result = operator.neg(operand1) elif operation == \'positive\': result = operator.pos(operand1) elif operation == \'absolute\': result = operator.abs(operand1) elif operation == \'invert\': result = operator.inv(operand1) elif operation == \'lshift\': result = operator.lshift(operand1, operand2) elif operation == \'rshift\': result = operator.rshift(operand1, operand2) elif operation == \'and\': result = operator.and_(operand1, operand2) elif operation == \'xor\': result = operator.xor(operand1, operand2) elif operation == \'or\': result = operator.or_(operand1, operand2) elif operation == \'inplace_add\': if isinstance(operand1, list): operand1[0] = operator.iadd(operand1[0], operand2) result = operand1[0] else: raise ValueError(\\"In-place operations must be performed on mutable types.\\") elif operation == \'inplace_subtract\': if isinstance(operand1, list): operand1[0] = operator.isub(operand1[0], operand2) result = operand1[0] else: raise ValueError(\\"In-place operations must be performed on mutable types.\\") elif operation == \'inplace_multiply\': if isinstance(operand1, list): operand1[0] = operator.imul(operand1[0], operand2) result = operand1[0] else: raise ValueError(\\"In-place operations must be performed on mutable types.\\") elif operation == \'inplace_floor_divide\': if isinstance(operand1, list): operand1[0] = operator.ifloordiv(operand1[0], operand2) result = operand1[0] else: raise ValueError(\\"In-place operations must be performed on mutable types.\\") elif operation == \'inplace_true_divide\': if isinstance(operand1, list): operand1[0] = operator.itruediv(operand1[0], operand2) result = operand1[0] else: raise ValueError(\\"In-place operations must be performed on mutable types.\\") elif operation == \'inplace_remainder\': if isinstance(operand1, list): operand1[0] = operator.imod(operand1[0], operand2) result = operand1[0] else: raise ValueError(\\"In-place operations must be performed on mutable types.\\") elif operation == \'inplace_power\': if isinstance(operand1, list): operand1[0] = operator.ipow(operand1[0], operand2) result = operand1[0] else: raise ValueError(\\"In-place operations must be performed on mutable types.\\") elif operation == \'inplace_lshift\': if isinstance(operand1, list): operand1[0] = operator.ilshift(operand1[0], operand2) result = operand1[0] else: raise ValueError(\\"In-place operations must be performed on mutable types.\\") elif operation == \'inplace_rshift\': if isinstance(operand1, list): operand1[0] = operator.irshift(operand1[0], operand2) result = operand1[0] else: raise ValueError(\\"In-place operations must be performed on mutable types.\\") elif operation == \'inplace_and\': if isinstance(operand1, list): operand1[0] = operator.iand(operand1[0], operand2) result = operand1[0] else: raise ValueError(\\"In-place operations must be performed on mutable types.\\") elif operation == \'inplace_xor\': if isinstance(operand1, list): operand1[0] = operator.ixor(operand1[0], operand2) result = operand1[0] else: raise ValueError(\\"In-place operations must be performed on mutable types.\\") elif operation == \'inplace_or\': if isinstance(operand1, list): operand1[0] = operator.ior(operand1[0], operand2) result = operand1[0] else: raise ValueError(\\"In-place operations must be performed on mutable types.\\") else: result = None # Unsupported operation except: result = None # Any error should result in None results.append(result) return results"},{"question":"Your task is to write a function `find_group_members` that takes a list of group names and returns a dictionary where each key is a group name and its value is a list of usernames that are members of that group. If a group name does not exist, it should be excluded from the dictionary. Additionally, implement a second function `groups_with_member` that takes a username and returns a list of all group names that the user is a member of. **Function Signatures**: ```python def find_group_members(group_names): # Your code here def groups_with_member(username): # Your code here ``` # Input - `find_group_members(group_names)`: A list of group names, e.g., `[\'admin\', \'staff\', \'users\']`. - `groups_with_member(username)`: A username, e.g., `\'john_doe\'`. # Output - `find_group_members(group_names)`: A dictionary where each key is a group name and the corresponding value is a list of usernames. - `groups_with_member(username)`: A list of group names. # Constraints - Handle `KeyError` gracefully if a group name does not exist. - Assume that the group database can be large, but you can use built-in functions like `grp.getgrall()` to fetch group information. - The provided username for `groups_with_member` should exist in the system. # Examples ```python # Example 1 result = find_group_members([\'admin\', \'staff\', \'nonexistent\']) print(result) # Possible output: {\'admin\': [\'alice\', \'bob\'], \'staff\': [\'carol\', \'dave\']} # Note: \'nonexistent\' group does not exist and is excluded. # Example 2 result = groups_with_member(\'alice\') print(result) # Possible output: [\'admin\', \'developers\', \'support\'] ``` # Notes - You may assume that the `grp` module is available and properly imported. - Ensure efficiency and readability in your solution. - Consider edge cases such as empty input lists or no memberships found for a user.","solution":"import grp def find_group_members(group_names): Returns a dictionary where each key is a group name and the corresponding value is a list of usernames in that group. Excludes groups that do not exist. group_members = {} for group_name in group_names: try: group_info = grp.getgrnam(group_name) group_members[group_name] = group_info.gr_mem except KeyError: continue # Skip groups that do not exist return group_members def groups_with_member(username): Returns a list of all group names that the user is a member of. user_groups = [] for group in grp.getgrall(): if username in group.gr_mem: user_groups.append(group.gr_name) return user_groups"},{"question":"**Coding Assessment Question** **Objective:** Write a Python function that will recursively compile all Python source files (*.py) in a specified directory using the `compileall` module. The function should include options to: 1. Specify maximum recursion level. 2. Exclude files matching a given regular expression. 3. Control the optimization level used during compilation. 4. Specify the number of worker threads for parallel compilation. **Function Signature:** ```python def compile_python_files(directory: str, max_depth: int = 10, exclude_pattern: str = None, optimize: int = -1, workers: int = 1) -> bool: pass ``` **Parameters:** - `directory` (str): The directory path whose Python files need to be compiled. - `max_depth` (int): Maximum depth for recursive directory traversal (default is 10). - `exclude_pattern` (str): Regular expression pattern to exclude certain files from being compiled (default is None, meaning no files are excluded). - `optimize` (int): Optimization level to be used for compilation (default is no optimization, indicated by -1). - `workers` (int): Number of worker threads to use for parallel compilation (default is 1). **Expected Output:** - The function should return `True` if all files compiled successfully, and `False` otherwise. **Constraints:** - You must use the `compileall` module\'s `compile_dir` function to perform the compilation. - The function should handle invalid inputs gracefully (e.g., non-existent directory, invalid regex pattern). - Use the `re` module to compile and validate the provided regex pattern for excluding files. **Performance Requirements:** - The function should be efficient and utilize parallel processing if multiple workers are specified. **Example Usage:** ```python # Example: Compile Python files in the \'src\' directory, with a maximum depth of 5, # excluding any files in logs directories, using optimization level 2, and utilizing 4 worker threads. result = compile_python_files(\'src\', max_depth=5, exclude_pattern=r\'/logs/\', optimize=2, workers=4) print(result) # Output should be True or False based on the success of the compilation process. ``` **Notes:** - Use appropriate exception handling to manage errors (e.g., directory not found, invalid regular expression).","solution":"import os import re import compileall def compile_python_files(directory: str, max_depth: int = 10, exclude_pattern: str = None, optimize: int = -1, workers: int = 1) -> bool: Recursively compile all Python source files (*.py) in the specified directory. :param directory: The directory path whose Python files need to be compiled. :param max_depth: Maximum depth for recursive directory traversal (default is 10). :param exclude_pattern: Regular expression pattern to exclude certain files from being compiled (default is None). :param optimize: Optimization level to be used for compilation (default is no optimization, indicated by -1). :param workers: Number of worker threads to use for parallel compilation (default is 1). :return: True if all files compiled successfully, False otherwise. if not os.path.isdir(directory): raise ValueError(f\\"Invalid directory: {directory}\\") if exclude_pattern: try: exclude_regex = re.compile(exclude_pattern) except re.error: raise ValueError(f\\"Invalid exclude pattern: {exclude_pattern}\\") else: exclude_regex = None def should_exclude(file_path): if exclude_regex: return exclude_regex.search(file_path) is not None return False def _compile_dir(path, depth): if depth > max_depth: return True all_compiled = True for root, dirs, files in os.walk(path): relative_depth = len(os.path.relpath(root, directory).split(os.sep)) if relative_depth > depth: continue python_files = [os.path.join(root, f) for f in files if f.endswith(\'.py\')] filtered_files = [f for f in python_files if not should_exclude(f)] if not compileall.compile_file_multiple(filtered_files, ddir=directory, optimize=optimize, workers=workers): all_compiled = False return all_compiled return _compile_dir(directory, 0)"},{"question":"In this task, you will demonstrate your understanding of handling email messages using the `email.policy` module in Python. You are required to implement a Python function named `parse_and_send_email` that reads an email message from a file, modifies certain headers according to specific policies, and then saves the modified message back to another file. Function Signature ```python def parse_and_send_email(input_filepath: str, output_filepath: str, modify_policy: bool = False) -> None: pass ``` Parameters - **input_filepath (str)**: The file path to the input email message file. - **output_filepath (str)**: The file path where the modified email message will be saved. - **modify_policy (bool)**: A boolean flag indicating whether or not to modify the policy of the message. Requirements 1. **Reading the Email Message**: - Read the email message from `input_filepath` using the `email.policy.default` policy if `modify_policy` is `False`. - If `modify_policy` is `True`, create a customized policy by cloning the `email.policy.default` policy and setting the `max_line_length` to `100`, the `linesep` to `rn`, and `raise_on_defect` to `True`. 2. **Modifying Headers**: - Regardless of the `modify_policy` value, append a new header to the email message with the key `\'X-Modified-By\'` and the value `\'parse_and_send_email\'`. 3. **Saving the Modified Message**: - Serialize the modified message to `output_filepath` with the specified policy (either the default one or the customized policy). 4. **Error Handling**: - Handle any potential exceptions during the read or write operations and print an appropriate error message. Example Usage ```python # Example usage of the function parse_and_send_email(\'incoming_email.txt\', \'outgoing_email.txt\', modify_policy=True) ``` This example reads the email from \'incoming_email.txt\', applies the customized policy, adds a new header, and writes the modified email to \'outgoing_email.txt\'. Constraints - You can assume the input email files are well-formed and follow standard email message formats. - The output must be human-readable and comply with standard email MIME headers. Notes - Use the `email` package’s functionalities to handle reading, modifying, and writing email messages. - Ensure to follow the RFC guidelines applicable to email headers when modifying and serializing the messages. Good luck, and if you have any questions, please ask for clarification.","solution":"import email from email import policy from email.parser import BytesParser from email.generator import BytesGenerator import os def parse_and_send_email(input_filepath: str, output_filepath: str, modify_policy: bool = False) -> None: Reads an email message from a file, modifies certain headers according to specific policies, and saves the modified message back to another file. :param input_filepath: str - The file path to the input email message file. :param output_filepath: str - The file path where the modified email message will be saved. :param modify_policy: bool - A boolean flag indicating whether or not to modify the policy of the message. try: # Determine the policy to use if modify_policy: custom_policy = policy.default.clone(max_line_length=100, linesep=\'rn\', raise_on_defect=True) else: custom_policy = policy.default # Read the email message using the determined policy with open(input_filepath, \'rb\') as fp: msg = BytesParser(policy=custom_policy).parse(fp) # Modify Headers msg[\'X-Modified-By\'] = \'parse_and_send_email\' # Write the modified message to the new file with open(output_filepath, \'wb\') as fp: BytesGenerator(fp, policy=custom_policy).flatten(msg) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Title: File Text Analysis and Summarization** **Objective:** Write a Python script that performs the following tasks: 1. **File Search**: Search a specified directory and its subdirectories for all text files (`.txt` extension). 2. **String Manipulation**: For each text file found, read its content and use regular expressions to perform the following: * Extract all words starting with a vowel. * Count the frequency of each extracted word. 3. **Summarization**: Calculate and display basic statistical data of these word frequencies, including the mean, median, and variance. **Requirements:** 1. Create a function `search_text_files(directory)` that returns a list of paths to all `.txt` files in the given directory and its subdirectories. - **Input**: `directory` (str) - the directory to search. - **Output**: List of file paths (list of str). 2. Create a function `extract_vowel_words(file_path)` that reads a text file and extracts words starting with a vowel using regular expressions. - **Input**: `file_path` (str) - path to the text file. - **Output**: List of words (list of str). 3. Create a function `calculate_statistics(words)` that calculates and returns the mean, median, and variance of the frequency of these words. - **Input**: `words` (list of str) - list of words. - **Output**: A dictionary with keys `mean`, `median`, and `variance`. 4. Create an `main` function to: - Accept a directory path as input from the command line. - Utilize the above functions to perform the search, extraction, and summarization. - Print the statistical summary of word frequencies. **Constraints:** - Assume all text files are encoded in UTF-8. - Words are case-insensitive (e.g., \\"Apple\\" and \\"apple\\" should be considered the same word). - Only consider words made up of alphabetic characters. **Performance Requirements:** - The script should efficiently handle directories containing hundreds of text files and should not load all files into memory simultaneously. **Example Usage:** Given the directory structure: ``` /sample_dir /sample_dir/file1.txt /sample_dir/subdir/file2.txt ``` And the contents of `file1.txt`: ``` Apple banana Apple orange ``` And the contents of `file2.txt`: ``` Orange apple kiwi apple ``` The script should output the word frequencies and their statistical summary. **Expected Output:** ``` Word Frequencies: {\'apple\': 4, \'orange\': 2} Statistics: {\'mean\': 3.0, \'median\': 3.0, \'variance\': 2.0} ``` Please ensure your code is properly commented and adheres to PEP 8 guidelines.","solution":"import os import re import statistics from collections import Counter def search_text_files(directory): Searches the given directory and its subdirectories for all text files (`.txt` extension). Args: directory (str): The directory to search. Returns: list of str: List of paths to text files. txt_files = [] for root, _, files in os.walk(directory): for file in files: if file.endswith(\\".txt\\"): txt_files.append(os.path.join(root, file)) return txt_files def extract_vowel_words(file_path): Reads the content of the text file and extracts words starting with a vowel. Args: file_path (str): Path to the text file. Returns: list of str: List of words starting with a vowel. vowel_words = [] with open(file_path, \'r\', encoding=\'utf-8\') as file: content = file.read() words = re.findall(r\'b[aeiouAEIOU]w*b\', content) vowel_words = [word.lower() for word in words] return vowel_words def calculate_statistics(words): Calculates and returns the mean, median, and variance of the frequency of the words. Args: words (list of str): List of words. Returns: dict: A dictionary with keys \'mean\', \'median\', and \'variance\'. if not words: return {\'mean\': 0, \'median\': 0, \'variance\': 0} word_counts = Counter(words) frequencies = list(word_counts.values()) mean = statistics.mean(frequencies) median = statistics.median(frequencies) variance = statistics.variance(frequencies) if len(frequencies) > 1 else 0 return {\'mean\': mean, \'median\': median, \'variance\': variance} def main(directory): Main function to perform file search, extraction, and summarization of word frequencies. Args: directory (str): The directory to search. text_files = search_text_files(directory) all_vowel_words = [] for file_path in text_files: all_vowel_words.extend(extract_vowel_words(file_path)) stats = calculate_statistics(all_vowel_words) word_counts = Counter(all_vowel_words) print(f\'Word Frequencies: {word_counts}\') print(f\'Statistics: {stats}\') if __name__ == \\"__main__\\": import sys directory = sys.argv[1] main(directory)"},{"question":"Objective: You are required to write a function `process_path_operations(paths: list, base_path: str) -> dict` that performs various pathname manipulations and checks using the `os.path` module functions. This function should demonstrate your understanding of the `os.path` module and its capabilities. Function Signature: ```python def process_path_operations(paths: list, base_path: str) -> dict: ``` Input: - `paths`: A list of strings, where each string represents a file path. - `base_path`: A string representing the base directory from which relative paths should be calculated. Output: - Returns a dictionary with the following keys: - `\\"absolute_paths\\"`: A list of absolute paths for the elements in `paths`. - `\\"basenames\\"`: A list of base names for the elements in `paths`. - `\\"common_path\\"`: The longest common sub-path of the elements in `paths`. - `\\"normalized_paths\\"`: A list of normalized paths for the elements in `paths`. - `\\"relative_paths\\"`: A list of relative paths from `base_path` to each element in `paths`. - `\\"valid_paths\\"`: A list of booleans indicating if the elements in `paths` exist in the filesystem. - `\\"path_sizes\\"`: A list of sizes (in bytes) for the elements in `paths` if they exist, otherwise `None`. Constraints: - Ensure that the function handles edge cases like empty paths and paths that do not exist. - The function should work on both Windows and Unix-like systems. - Use the appropriate `os.path` functions for each operation. Example: ```python import os def process_path_operations(paths, base_path): result = { \\"absolute_paths\\": [], \\"basenames\\": [], \\"common_path\\": \\"\\", \\"normalized_paths\\": [], \\"relative_paths\\": [], \\"valid_paths\\": [], \\"path_sizes\\": [] } # Absolute paths result[\\"absolute_paths\\"] = [os.path.abspath(path) for path in paths] # Base names result[\\"basenames\\"] = [os.path.basename(path) for path in paths] # Common path try: result[\\"common_path\\"] = os.path.commonpath(paths) except ValueError: result[\\"common_path\\"] = \\"\\" # Normalized paths result[\\"normalized_paths\\"] = [os.path.normpath(path) for path in paths] # Relative paths result[\\"relative_paths\\"] = [os.path.relpath(path, base_path) for path in paths] # Valid paths result[\\"valid_paths\\"] = [os.path.exists(path) for path in paths] # Path sizes result[\\"path_sizes\\"] = [os.path.getsize(path) if os.path.exists(path) else None for path in paths] return result # Test example paths = [\\"./testfile1.txt\\", \\"./folder/testfile2.txt\\", \\"/absolute/path/to/file.txt\\"] base_path = \\"/\\" print(process_path_operations(paths, base_path)) ``` Expected Output: ```python { \'absolute_paths\': [...], # list of absolute paths \'basenames\': [...], # list of base names \'common_path\': \'...\', # common sub-path \'normalized_paths\': [...], # list of normalized paths \'relative_paths\': [...], # list of relative paths \'valid_paths\': [...], # list of boolean values \'path_sizes\': [...] # list of file sizes or None } ``` Ensure your implementation correctly handles each of these tasks and returns the results in the format specified.","solution":"import os def process_path_operations(paths, base_path): result = { \\"absolute_paths\\": [], \\"basenames\\": [], \\"common_path\\": \\"\\", \\"normalized_paths\\": [], \\"relative_paths\\": [], \\"valid_paths\\": [], \\"path_sizes\\": [] } # Absolute paths result[\\"absolute_paths\\"] = [os.path.abspath(path) for path in paths] # Base names result[\\"basenames\\"] = [os.path.basename(path) for path in paths] # Common path try: result[\\"common_path\\"] = os.path.commonpath(paths) except ValueError: result[\\"common_path\\"] = \\"\\" # Normalized paths result[\\"normalized_paths\\"] = [os.path.normpath(path) for path in paths] # Relative paths result[\\"relative_paths\\"] = [os.path.relpath(path, base_path) for path in paths] # Valid paths result[\\"valid_paths\\"] = [os.path.exists(path) for path in paths] # Path sizes result[\\"path_sizes\\"] = [os.path.getsize(path) if os.path.exists(path) else None for path in paths] return result"},{"question":"**Objective:** In this assessment, you are required to demonstrate your understanding of the Python codec registry and support functions. Specifically, you will implement functions to register a custom error handler for encoding errors and create mechanisms to encode and decode strings using the codec API. **Function Implementations:** 1. **register_custom_error_handler():** - This function should register a custom error handler for encoding errors. - The custom error handler should replace any undecodable characters with a specified placeholder. 2. **custom_encode(input_string: str, encoding: str, error_handler: str = \'custom_error\') -> bytes:** - This function should take an input string, encoding type, and error handler name. - It should encode the string using the specified encoding and error handler. - Return the encoded byte string. 3. **custom_decode(input_bytes: bytes, encoding: str, error_handler: str = \'strict\') -> str:** - This function should take an input byte string, encoding type, and error handler name. - It should decode the byte string using the specified encoding and error handler. - Return the decoded string. **Constraints:** - Your functions should handle common text encodings like \'utf-8\'. - The custom_error handler should replace problematic sections with \\"(error)\\". - The default encoding for custom_encode and custom_decode should be \'utf-8\' if not provided. - Raise appropriate exceptions if any encoding/decoding fails that isn\'t handled by custom error handler. **Example Usage:** ```python # Register a custom error handler register_custom_error_handler() # Encode a string with a known problematic character for \'ascii\' encoding encoded_bytes = custom_encode(\\"hello π world\\", encoding=\'ascii\') # Decode bytes back to string using the registered custom error handler decoded_string = custom_decode(encoded_bytes, encoding=\'ascii\') print(decoded_string) # Output should be \'hello (error) world\' ``` **Notes:** - Make sure to import the required modules and handle any necessary setup for the codec registry. **Input Format:** For `custom_encode`: - `input_string` : A string to encode. - `encoding` : A string representing the encoding type. - `error_handler` : A string representing the error handler name (default is \'custom_error\'). For `custom_decode`: - `input_bytes` : A byte string to decode. - `encoding` : A string representing the encoding type. - `error_handler` : A string representing the error handler name (default is \'strict\'). **Output Format:** For `custom_encode`: - Returns a byte string representing the encoded input. For `custom_decode`: - Returns a string representing the decoded input. Implement your solution in Python, ensuring all edge cases and constraints are handled efficiently.","solution":"import codecs def custom_error_handler(exception): Custom error handler to replace problematic sections with \\"(error)\\". # exception is a UnicodeEncodeError or UnicodeDecodeError object if isinstance(exception, (UnicodeEncodeError, UnicodeDecodeError)): return (\\"(error)\\", exception.start + 1) else: raise TypeError(\\"Don\'t know how to handle {0}\\".format(exception)) def register_custom_error_handler(): Registers the custom error handler with the codec registry. codecs.register_error(\\"custom_error\\", custom_error_handler) def custom_encode(input_string: str, encoding: str = \'utf-8\', error_handler: str = \'custom_error\') -> bytes: Encode the input string using the specified encoding and error handler. try: return input_string.encode(encoding, errors=error_handler) except Exception as e: raise e def custom_decode(input_bytes: bytes, encoding: str = \'utf-8\', error_handler: str = \'strict\') -> str: Decode the input bytes using the specified encoding and error handler. try: return input_bytes.decode(encoding, errors=error_handler) except Exception as e: raise e"},{"question":"# Question: Advanced Plotting with Seaborn You are provided with a dataset containing various properties of different car models from the `mpg` dataset, which you can load using the Seaborn library. Your task is to generate a comprehensive visualization that meets the following requirements: 1. **Load Dataset**: Load the `mpg` dataset using `seaborn.load_dataset`. 2. **Plot**: Create a plot showing the relationship between the `displacement` of the cars and their `mpg` (miles per gallon), `weight`, and `horsepower` on separate subplots. 3. **Customize the Plot**: - Label the x-axis for `displacement` as \\"Displacement (cu in)\\". - Label the y-axes with their respective variable names. - Implement faceting by the `origin` of the cars. - Use dots to represent each data point. 4. **Output**: Your function should save this plot to a file named `car_relationships.png`. # Function Signature ```python import seaborn.objects as so import seaborn as sns def generate_car_plots(): # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Create and customize the plot ( so.Plot(mpg, x=\\"displacement\\") .pair(y=[\\"mpg\\", \\"weight\\", \\"horsepower\\"], wrap=2) .facet(col=\\"origin\\") .label(x=\\"Displacement (cu in)\\", y1=\\"Miles per Gallon (MPG)\\", y2=\\"Weight (lb)\\", y3=\\"Horsepower (hp)\\") .add(so.Dots()) .save(\\"car_relationships.png\\") ) # Call the function to generate the plot generate_car_plots() ``` # Constraints: - The dataset is relatively small, so performance constraints are minimal. - Ensure that the plot is saved as `car_relationships.png`. Make sure that you test your solution to ensure it meets all the requirements outlined.","solution":"import seaborn as sns import matplotlib.pyplot as plt import seaborn.objects as so def generate_car_plots(): # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Create the plot object plot = so.Plot(mpg, x=\\"displacement\\").facet(col=\\"origin\\") # Add the individual subplots plot.add(so.Dots(), y=\'mpg\').label(x=\'Displacement (cu in)\', y=\'Miles per Gallon (MPG)\') plot.add(so.Dots(), y=\'weight\').label(y=\'Weight (lb)\') plot.add(so.Dots(), y=\'horsepower\').label(y=\'Horsepower (hp)\') # Save the plot to a file plot.save(\\"car_relationships.png\\") # Call the function to generate the plot generate_car_plots()"},{"question":"# Question: Implementing and Evaluating a Ridge Regression Model on a Synthetic Dataset Objective: To assess your understanding of creating and applying a linear model using Ridge Regression in scikit-learn, and to evaluate the performance of the model. Task: 1. **Dataset Generation:** - Generate a synthetic dataset with 1000 samples and 50 features using `make_regression` from `sklearn.datasets`. Add some Gaussian noise to the dataset. 2. **Model Implementation:** - Implement Ridge Regression using `Ridge` from `sklearn.linear_model`. - Use cross-validation to find the optimal alpha value from a specified range for the Ridge Regression model. Implement cross-validation using `RidgeCV`. 3. **Evaluation:** - Split the dataset into training (80%) and testing (20%) subsets. - Train the Ridge Regression model on the training subset. - Evaluate the model on the testing subset using the R^2 score and Mean Squared Error (MSE). 4. **Result Analysis:** - Print the optimal alpha value obtained from cross-validation. - Plot the true vs predicted values for the testing subset. - Display the R^2 score and MSE of the model on the testing subset. Constraints: - Use random seed = 42 for reproducibility. - The range of alpha values for cross-validation should be `[1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]`. Input: - The input is implicitly provided by the `make_regression` function with specified parameters. Output: - The optimal alpha value, R^2 score, MSE, and a plot showing the true vs predicted values. Performance Requirements: - The implementation should handle the cross-validation and fitting within a reasonable time frame for the given dataset size and complexity. Example Code: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_regression from sklearn.linear_model import Ridge, RidgeCV from sklearn.metrics import mean_squared_error, r2_score from sklearn.model_selection import train_test_split # Step 1: Generate synthetic dataset np.random.seed(42) X, y = make_regression(n_samples=1000, n_features=50, noise=0.1, random_state=42) # Step 2: Split dataset into training and testing subsets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Implement Ridge Regression with cross-validation alphas = np.logspace(-6, 3, 10) ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True) ridge_cv.fit(X_train, y_train) # Step 4: Train the model with optimal alpha ridge_model = Ridge(alpha=ridge_cv.alpha_) ridge_model.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = ridge_model.predict(X_test) r2 = r2_score(y_test, y_pred) mse = mean_squared_error(y_test, y_pred) # Output results print(\\"Optimal alpha:\\", ridge_cv.alpha_) print(\\"R^2 score:\\", r2) print(\\"Mean Squared Error:\\", mse) # Plot true vs predicted values plt.scatter(y_test, y_pred, alpha=0.3) plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \'k--\', lw=2) plt.xlabel(\'True Values\') plt.ylabel(\'Predictions\') plt.title(\'True vs Predicted Values\') plt.show() ``` Submission: Submit your implementation as a Python script or Jupyter notebook.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_regression from sklearn.linear_model import Ridge, RidgeCV from sklearn.metrics import mean_squared_error, r2_score from sklearn.model_selection import train_test_split def ridge_regression_analysis(): Perform ridge regression analysis on synthetic dataset and return evaluation metrics. Returns: dict: containing optimal_alpha, R^2 score, MSE, and the scatter plot data for true vs predicted values. # Step 1: Generate synthetic dataset np.random.seed(42) X, y = make_regression(n_samples=1000, n_features=50, noise=0.1, random_state=42) # Step 2: Split dataset into training and testing subsets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Implement Ridge Regression with cross-validation alphas = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000] ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True) ridge_cv.fit(X_train, y_train) # Step 4: Train the model with optimal alpha ridge_model = Ridge(alpha=ridge_cv.alpha_) ridge_model.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = ridge_model.predict(X_test) r2 = r2_score(y_test, y_pred) mse = mean_squared_error(y_test, y_pred) # Output results results = { \\"optimal_alpha\\": ridge_cv.alpha_, \\"r2_score\\": r2, \\"mse\\": mse, \\"true_values\\": y_test, \\"predicted_values\\": y_pred } return results def plot_true_vs_predicted(true_values, predicted_values): Plot true vs predicted values. Args: true_values (array): Array of true values. predicted_values (array): Array of predicted values. plt.scatter(true_values, predicted_values, alpha=0.3) plt.plot([true_values.min(), true_values.max()], [true_values.min(), true_values.max()], \'k--\', lw=2) plt.xlabel(\'True Values\') plt.ylabel(\'Predictions\') plt.title(\'True vs Predicted Values\') plt.show()"},{"question":"You are provided a dataset of monthly sales data of a product for the year 2022. Write a Python function using the `seaborn.objects` interface that: 1. Creates a line plot of the monthly sales. 2. Sets customized limits for the x-axis (months) from January to December (1 to 12) and the y-axis (sales) from 0 to the maximum sales value increased by a 10% margin. 3. Adds markers to indicate the data points clearly. 4. Optionally, allows for inverting the y-axis. **Dataset:** The dataset is a dictionary with two keys: \'month\' and \'sales\'. The \'month\' values are between 1 and 12, representing the months, and \'sales\' values are integers representing the sales in those months. ```python sales_data = { \'month\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \'sales\': [150, 200, 250, 300, 400, 500, 600, 700, 750, 800, 850, 900] } ``` **Function Signature:** ```python def plot_sales_with_limits(data, invert_y=False): pass ``` **Input:** - `data`: a dictionary containing the \'month\' list and the \'sales\' list. - `invert_y` (optional): a boolean to indicate if the y-axis should be inverted. Default is `False`. **Output:** - The function should display the plot using the seaborn objects interface. # Example ```python sales_data = { \'month\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \'sales\': [150, 200, 250, 300, 400, 500, 600, 700, 750, 800, 850, 900] } plot_sales_with_limits(sales_data) ``` This should generate a plot where the x-axis ranges from 1 to 12, the y-axis ranges from 0 to 990 (maximum sales 900 with a 10% margin), and markers indicate each month\'s sales. Setting `invert_y=True` should invert the y-axis. **Constraints:** - Ensure the solution uses the seaborn `objects` interface as demonstrated in the provided code snippets. - Handle cases where the `invert_y` flag is set to `True` appropriately. Happy Coding!","solution":"import seaborn.objects as so import pandas as pd import matplotlib.pyplot as plt def plot_sales_with_limits(data, invert_y=False): Creates a line plot of the monthly sales with customized x and y axis limits. Parameters: - data: a dictionary containing the \'month\' and \'sales\' lists. - invert_y: a boolean to indicate if the y-axis should be inverted. Default is False. # Create a DataFrame from the input dictionary df = pd.DataFrame(data) # Determine the y-axis limit max_sales = max(df[\'sales\']) y_limit = max_sales * 1.1 # Initialize the seaborn relational plot p = ( so.Plot(df, x=\'month\', y=\'sales\') .add(so.Line(marker=\'o\')) ) # Set the x and y axis limits p = p.limit(x=(1, 12), y=(0, y_limit)) # Invert the y axis if required if invert_y: plt.gca().invert_yaxis() # Display the plot p.show()"},{"question":"You have been tasked with implementing a function to evaluate and find users with weak password policies based on entries in the Unix shadow password database. A weak password policy is defined as any of the following conditions: 1. The maximum number of days between password changes (`sp_max`) is greater than 365. 2. The minimum number of days between password changes (`sp_min`) is less than 7. 3. The number of days before password expiration to warn the user (`sp_warn`) is less than 7. Write a function `find_weak_password_policies()` that utilizes the `spwd` module to return a list of usernames that have a weak password policy. # Function Signature ```python def find_weak_password_policies() -> list: pass ``` # Requirements: - You need to access the shadow password database, so you must have the necessary privileges to do so. - Utilize the `spwd.getspall()` function to retrieve all entries from the shadow password database. - Examine each entry to check if it meets any of the criteria for weak password policies. - Return a list of usernames (`sp_namp`) with weak password policies. # Example Usage: ```python # Please note that this example requires proper privileges to run and will vary based on the system\'s shadow password database. weak_users = find_weak_password_policies() print(weak_users) # Example output: [\'john\', \'doe\'] ``` # Constraints: - The function should handle cases where the user does not have the required permissions by catching the `PermissionError` and returning an empty list. - The function should be efficient in its processing even if the number of entries is large. # Additional Notes: - Make sure Python 3.10 is used as the `spwd` module is deprecated from Python 3.11 onward. - Testing of this function on a real-system setup should be done with caution and appropriate permissions.","solution":"import spwd def find_weak_password_policies() -> list: Finds users with weak password policies based on the Unix shadow password database. A weak password policy is defined as: 1. sp_max > 365 2. sp_min < 7 3. sp_warn < 7 Returns: list: List of usernames with weak password policies. weak_users = [] try: shadow_entries = spwd.getspall() for entry in shadow_entries: if entry.sp_max > 365 or entry.sp_min < 7 or entry.sp_warn < 7: weak_users.append(entry.sp_nam) except PermissionError: # Handle the lack of permissions scenario return [] return weak_users"},{"question":"Objective Demonstrate your understanding of creating C extension types in Python by defining a custom type with specific attributes and methods, managing memory correctly, and integrating custom behavior. Task Create a Python C extension module named `mathpoint` with a custom type `Point`. The custom `Point` type should represent a point in a 3D space with `x`, `y`, and `z` coordinates. Implement the following functionalities: 1. **Point Initialization**: - `Point(x, y, z)`: Initialize a point with given coordinates `x`, `y`, and `z`. The `x`, `y`, and `z` attributes should default to 0 if not provided. 2. **Distance Calculation Method**: - `distance_to`: Add a method `distance_to` that takes another `Point` object as an argument and returns the Euclidean distance between the two points. 3. **String Representation**: - Override the `__str__` method to return a string representation in the format: `Point(x, y, z)`. 4. **Memory Management**: - Implement correct memory management, ensuring no memory leaks occur and all objects are tracked and deallocated properly. 5. **Optional/Bonus**: - Make the `Point` type subclassable, allowing users to create subclasses of `Point` with additional functionality. Constraints - The module must be implemented using the C API for Python as described in the provided documentation. - Use structured error checking to ensure memory allocation and attribute assignments do not fail silently. - The `distance_to` method should use the standard Euclidean distance formula: `distance = sqrt((x2 - x1)^2 + (y2 - y1)^2 + (z2 - z1)^2)` Example Usage ```python import mathpoint # Creating point objects p1 = mathpoint.Point(1, 2, 3) p2 = mathpoint.Point(4, 5, 6) # Calculating distance between p1 and p2 distance = p1.distance_to(p2) print(f\\"Distance: {distance}\\") # Output: Distance: 5.196 # String representation of point object print(p1) # Output: Point(1, 2, 3) ``` Submission - Provide the `.c` source file for the `mathpoint` module along with a `setup.py` script to build the extension.","solution":"import math class Point: def __init__(self, x=0, y=0, z=0): self.x = x self.y = y self.z = z def distance_to(self, other): if not isinstance(other, Point): raise TypeError(\\"Argument must be of type Point\\") return math.sqrt( (self.x - other.x) ** 2 + (self.y - other.y) ** 2 + (self.z - other.z) ** 2 ) def __str__(self): return f\\"Point({self.x}, {self.y}, {self.z})\\""},{"question":"**Question: Dimensionality Reduction and Reconstruction using Random Projections** You are tasked with implementing a function that performs dimensionality reduction on a high-dimensional dataset using both Gaussian Random Projection and Sparse Random Projection. Additionally, you must implement functionality to reconstruct the original data from the reduced-dimensional data and compare the reconstruction error for both methods. **Function Signature:** ```python def compare_reconstruction_error(X, n_components): Perform dimensionality reduction and reconstruction using Gaussian and Sparse Random Projections. Parameters: X (numpy.ndarray): A high-dimensional dataset with shape (n_samples, n_features). n_components (int): The number of components to project the data onto. Returns: dict: A dictionary with keys \'gaussian_error\' and \'sparse_error\' containing the Frobenius norms of the reconstruction errors for Gaussian and Sparse projections respectively. pass ``` # Requirements 1. **Dimensionality Reduction:** - Use scikit-learn\'s `GaussianRandomProjection` to reduce the dimensionality of the dataset `X` to `n_components`. - Use scikit-learn\'s `SparseRandomProjection` to reduce the dimensionality of the dataset `X` to `n_components`. 2. **Reconstruction:** - Reconstruct the original dataset `X` from its lower-dimensional representation obtained using both Gaussian and Sparse Random Projections. 3. **Error Calculation:** - Compute the reconstruction error for both methods using the Frobenius norm of the difference between the original and reconstructed datasets. - Return a dictionary with keys `gaussian_error` and `sparse_error`, containing the reconstruction errors for Gaussian and Sparse projections respectively. # Constraints - The function should handle datasets with `n_samples` up to 10000 and `n_features` up to 10000. - You can assume the input dataset `X` is a dense matrix. # Example ```python import numpy as np X = np.random.rand(100, 5000) n_components = 100 errors = compare_reconstruction_error(X, n_components) print(errors) # Output format: {\'gaussian_error\': <value>, \'sparse_error\': <value>} ``` **Hint:** - Use the `fit_transform` method for dimensionality reduction. - Use the `inverse_transform` method for reconstruction. **Evaluation Criteria:** - Correct implementation of Gaussian and Sparse Random Projections. - Accurate calculations of the reconstruction errors. - Code readability and proper usage of scikit-learn\'s functions.","solution":"import numpy as np from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection from numpy.linalg import norm def compare_reconstruction_error(X, n_components): Perform dimensionality reduction and reconstruction using Gaussian and Sparse Random Projections. Parameters: X (numpy.ndarray): A high-dimensional dataset with shape (n_samples, n_features). n_components (int): The number of components to project the data onto. Returns: dict: A dictionary with keys \'gaussian_error\' and \'sparse_error\' containing the Frobenius norms of the reconstruction errors for Gaussian and Sparse projections respectively. # Gaussian Random Projection gaussian_rp = GaussianRandomProjection(n_components=n_components) X_gaussian_reduced = gaussian_rp.fit_transform(X) X_gaussian_reconstructed = gaussian_rp.inverse_transform(X_gaussian_reduced) gaussian_error = norm(X - X_gaussian_reconstructed, ord=\'fro\') # Sparse Random Projection sparse_rp = SparseRandomProjection(n_components=n_components) X_sparse_reduced = sparse_rp.fit_transform(X) X_sparse_reconstructed = sparse_rp.inverse_transform(X_sparse_reduced) sparse_error = norm(X - X_sparse_reconstructed, ord=\'fro\') return {\'gaussian_error\': gaussian_error, \'sparse_error\': sparse_error}"},{"question":"Platform Information Report Generator You are required to implement a function that generates a comprehensive report of the system\'s platform information by using the functions provided in the `platform` module. # Function Signature ```python def generate_platform_report() -> dict: pass ``` # Description Implement the function `generate_platform_report` which gathers various pieces of platform-related information and stores them in a dictionary with appropriately named keys. # Requirements 1. Use the following functions from the `platform` module to gather the information: - `platform.architecture()` - `platform.machine()` - `platform.node()` - `platform.platform()` - `platform.processor()` - `platform.python_build()` - `platform.python_compiler()` - `platform.python_implementation()` - `platform.python_revision()` - `platform.python_version()` - `platform.python_version_tuple()` - `platform.release()` - `platform.system()` - `platform.version()` - `platform.uname()` 2. The function should return a dictionary with the following keys: - `\'bits\'` - `\'linkage\'` - `\'machine\'` - `\'node\'` - `\'platform\'` - `\'processor\'` - `\'python_build\'` - `\'python_compiler\'` - `\'python_implementation\'` - `\'python_revision\'` - `\'python_version\'` - `\'python_version_tuple\'` - `\'release\'` - `\'system\'` - `\'version\'` - `\'uname\'` 3. Each key should map to the corresponding output from the respective `platform` function. # Example Below is a sample dictionary structure with keys containing example values: ```python { \'bits\': \'64bit\', \'linkage\': \'ELF\', \'machine\': \'x86_64\', \'node\': \'my-computer\', \'platform\': \'Linux-4.15.0-96-generic-x86_64-with-Ubuntu-18.04-bionic\', \'processor\': \'x86_64\', \'python_build\': (\'default\', \'Apr 4 2020 23:38:01\'), \'python_compiler\': \'GCC 7.5.0\', \'python_implementation\': \'CPython\', \'python_revision\': \'0\', \'python_version\': \'3.8.2\', \'python_version_tuple\': (\'3\', \'8\', \'2\'), \'release\': \'4.15.0-96-generic\', \'system\': \'Linux\', \'version\': \'#97-Ubuntu SMP Thu Feb 4 23:17:09 UTC 2020\', \'uname\': platform.uname() } ``` # Constraints - The function should work on any platform supported by Python and the `platform` module. # Performance - Assume that the performance of individual `platform` module functions is optimized and focus on the correct usage of these functions. Implement the function `generate_platform_report`.","solution":"import platform def generate_platform_report() -> dict: Generates a comprehensive report of the system\'s platform information. bits, linkage = platform.architecture() return { \'bits\': bits, \'linkage\': linkage, \'machine\': platform.machine(), \'node\': platform.node(), \'platform\': platform.platform(), \'processor\': platform.processor(), \'python_build\': platform.python_build(), \'python_compiler\': platform.python_compiler(), \'python_implementation\': platform.python_implementation(), \'python_revision\': platform.python_revision(), \'python_version\': platform.python_version(), \'python_version_tuple\': platform.python_version_tuple(), \'release\': platform.release(), \'system\': platform.system(), \'version\': platform.version(), \'uname\': platform.uname() }"},{"question":"# PyTorch Coding Assessment Question Objective Design a PyTorch function that takes a tensor, applies a series of transformations including random operations, and returns a specific result. The goal is to demonstrate proficiency with tensor creation, manipulation, and mathematical operations within PyTorch. Problem Statement **Function to Implement:** ```python def transform_tensor(input_tensor: torch.Tensor, value: float) -> torch.Tensor: This function takes an input tensor and a float value. It performs the following operations: 1. Adds the float value to each element of the input tensor. 2. Shuffles the tensor along each dimension randomly. 3. Applies a log transformation to each element. 4. Normalizes the tensor such that its values are in the range [0, 1]. Args: input_tensor (torch.Tensor): A 2D tensor of shape (m, n). value (float): A float value to be added to each element of the input tensor. Returns: torch.Tensor: A transformed tensor. # Your code here ``` **Instructions:** 1. **Input Format:** - `input_tensor`: A 2D tensor with shape `(m, n)` where `m` and `n` are positive integers. - `value`: A float value that will be added to each element of the input tensor. 2. **Output Format:** - Returns a tensor of the same shape as `input_tensor` after applying the specified transformations. 3. **Constraints:** - The function should handle tensors with both positive and negative values correctly. - The log transformation should only be applied to positive values. For non-positive values, ensure that you adjust the values appropriately to avoid issues with the log function. 4. **Performance Requirements:** - The solution should be efficient both in terms of time and space complexity. 5. **Example:** ```python import torch input_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) value = 1.0 result = transform_tensor(input_tensor, value) print(result) ``` Possible Output: ``` tensor([[0.0000, 0.2877], [0.6310, 1.0000]]) ``` *Note:* The result may vary due to the random shuffling step. Test Cases 1. **Test Case 1:** ```python input_tensor = torch.tensor([[0.5, -0.2], [1.5, 3.7]]) value = 0.1 result = transform_tensor(input_tensor, value) ``` - Verify the output tensor shape and value ranges after transformations. 2. **Test Case 2:** ```python input_tensor = torch.tensor([[10.0, -5.0], [7.0, 2.5]]) value = 2.0 result = transform_tensor(input_tensor, value) ``` - Verify handling of negative and high positive values, and that proper logging and normalization are performed. Evaluation Criteria - Correct implementation of the described transformations. - Efficient use of PyTorch operations. - Handling edge cases and potential errors (such as log of non-positive values).","solution":"import torch def transform_tensor(input_tensor: torch.Tensor, value: float) -> torch.Tensor: This function takes an input tensor and a float value. It performs the following operations: 1. Adds the float value to each element of the input tensor. 2. Shuffles the tensor along each dimension randomly. 3. Applies a log transformation to each element. 4. Normalizes the tensor such that its values are in the range [0, 1]. Args: input_tensor (torch.Tensor): A 2D tensor of shape (m, n). value (float): A float value to be added to each element of the input tensor. Returns: torch.Tensor: A transformed tensor. # Add the float value to each element of the input tensor tensor_added = input_tensor + value # Shuffle the tensor along each dimension randomly shuffled_tensor = tensor_added.clone() for dim in range(tensor_added.dim()): shuffled_tensor = shuffled_tensor[torch.randperm(shuffled_tensor.size(dim))] # Apply log transformation only to positive elements positive_elements = shuffled_tensor > 0 shuffled_tensor[positive_elements] = torch.log(shuffled_tensor[positive_elements] + 1) # Adding 1 to avoid log(0) # Normalize the tensor to the range [0, 1] min_val = shuffled_tensor.min() max_val = shuffled_tensor.max() normalized_tensor = (shuffled_tensor - min_val) / (max_val - min_val) return normalized_tensor"},{"question":"# Question: Implement Sparse Matrix Multiplication You are provided with two pandas DataFrames `A` and `B`. These DataFrames are large and mostly filled with zeros. To save memory, these DataFrames have been stored using sparse data structures. Your task is to write a function `sparse_matrix_multiply` that takes in these two sparse DataFrames `A` and `B`, and returns their product, also as a sparse DataFrame. Note that for the multiplication to be valid, the number of columns in `A` must equal the number of rows in `B`. ```python import pandas as pd def sparse_matrix_multiply(A: pd.DataFrame, B: pd.DataFrame) -> pd.DataFrame: Multiplies two sparse DataFrames and returns the result as a sparse DataFrame. Parameters: A (pd.DataFrame): The first sparse DataFrame. B (pd.DataFrame): The second sparse DataFrame. Returns: pd.DataFrame: A sparse DataFrame that is the product of A and B. Raises: ValueError: If the number of columns in A does not equal the number of rows in B. # Implement your solution here ``` # Constraints: 1. The input DataFrames `A` and `B` are both in sparse format and are instances of `pd.DataFrame`. 2. The number of columns in `A` equals the number of rows in `B`. # Example Usage ```python from scipy.sparse import csr_matrix import pandas as pd # Creating sample sparse CSR matrices data1 = csr_matrix([ [1, 0, 0], [0, 0, 2], [0, 0, 0], ]) data2 = csr_matrix([ [2, 0], [0, 0], [1, 3], ]) # Converting them to pandas SparseDataFrame sparse_A = pd.DataFrame.sparse.from_spmatrix(data1) sparse_B = pd.DataFrame.sparse.from_spmatrix(data2) # Expected output should be a DataFrame with values: # [2, 0] # [2, 6] # [0, 0] result = sparse_matrix_multiply(sparse_A, sparse_B) print(result) assert (result.sparse.to_dense() == pd.DataFrame([[2, 0], [2, 6], [0, 0]])).all().all() ``` # Notes: - Make sure to leverage the efficient sparse operations and avoid converting sparse formats to dense structures during the computation. - Raise a `ValueError` if the input matrices dimensions are not compatible for multiplication.","solution":"import pandas as pd from scipy.sparse import csr_matrix def sparse_matrix_multiply(A: pd.DataFrame, B: pd.DataFrame) -> pd.DataFrame: Multiplies two sparse DataFrames and returns the result as a sparse DataFrame. Parameters: A (pd.DataFrame): The first sparse DataFrame. B (pd.DataFrame): The second sparse DataFrame. Returns: pd.DataFrame: A sparse DataFrame that is the product of A and B. Raises: ValueError: If the number of columns in A does not equal the number of rows in B. if A.shape[1] != B.shape[0]: raise ValueError(\\"Number of columns in A must equal the number of rows in B for multiplication.\\") # Convert DataFrame to sparse matrix A_sparse = csr_matrix(A.sparse.to_coo()) B_sparse = csr_matrix(B.sparse.to_coo()) # Perform sparse matrix multiplication result_sparse = A_sparse.dot(B_sparse) # Convert the result to a pandas sparse DataFrame result_df = pd.DataFrame.sparse.from_spmatrix(result_sparse) return result_df"},{"question":"Objective Write a Python function that uses the `xml.dom.pulldom` module to parse an XML document containing information about a collection of books. Your task is to identify books that meet certain criteria and output their details. Function Description ```python def parse_books(xml_string: str) -> List[Dict[str, str]]: Parses an XML string containing book information and returns a list of dictionaries containing details of books priced above 30 with an author whose name contains \'John\'. Parameters: xml_string (str): A string representing the XML document. Returns: List[Dict[str, str]]: A list of dictionaries, each containing details of a qualifying book. The dictionary keys should be \'title\', \'author\', and \'price\'. pass ``` Input - `xml_string` (str): A string containing the XML document about books. The XML will have the following structure: ```xml <books> <book> <title>Book Title 1</title> <author>Author Name 1</author> <price>25.00</price> </book> <book> <title>Book Title 2</title> <author>John Doe</author> <price>35.00</price> </book> <!-- More book elements --> </books> ``` Output - Return a list of dictionaries. Each dictionary should contain: - `\'title\'` (str): The title of the book. - `\'author\'` (str): The name of the author. - `\'price\'` (str): The price of the book as a string, exactly as it appears in the XML. Constraints - Only consider books with the following criteria: - The price is greater than 30. - The author\'s name contains the substring \\"John\\". Examples ```python xml_string = \'\'\' <books> <book> <title>Book Title 1</title> <author>John Smith</author> <price>45.00</price> </book> <book> <title>Book Title 2</title> <author>Jane Doe</author> <price>20.00</price> </book> <book> <title>Book Title 3</title> <author>John Doe</author> <price>35.00</price> </book> </books> \'\'\' parse_books(xml_string) ``` Expected Output: ```python [ {\'title\': \'Book Title 1\', \'author\': \'John Smith\', \'price\': \'45.00\'}, {\'title\': \'Book Title 3\', \'author\': \'John Doe\', \'price\': \'35.00\'} ] ``` Performance Requirements - The function should handle XML documents that can contain up to 10,000 `book` elements efficiently.","solution":"from typing import List, Dict import xml.dom.pulldom as pulldom import xml.dom.minidom as minidom def parse_books(xml_string: str) -> List[Dict[str, str]]: Parses an XML string containing book information and returns a list of dictionaries containing details of books priced above 30 with an author whose name contains \'John\'. Parameters: xml_string (str): A string representing the XML document. Returns: List[Dict[str, str]]: A list of dictionaries, each containing details of a qualifying book. The dictionary keys should be \'title\', \'author\', and \'price\'. doc = pulldom.parseString(xml_string) books = [] current_tag = None book_info = {\\"title\\": \\"\\", \\"author\\": \\"\\", \\"price\\": \\"\\"} capturing = False for event, node in doc: if event == pulldom.START_ELEMENT: if node.tagName == \\"book\\": capturing = True elif capturing and node.tagName in book_info: current_tag = node.tagName elif event == pulldom.END_ELEMENT: if node.tagName == \\"book\\": if (float(book_info[\\"price\\"]) > 30 and \\"John\\" in book_info[\\"author\\"]): books.append(book_info.copy()) book_info = {\\"title\\": \\"\\", \\"author\\": \\"\\", \\"price\\": \\"\\"} capturing = False current_tag = None elif event == pulldom.CHARACTERS and current_tag: book_info[current_tag] += node.data.strip() return books"},{"question":"Objective This task aims to assess your understanding of specialized container datatypes in Python\'s `collections` module, specifically `ChainMap`, `Counter`, `OrderedDict`, `defaultdict`, `deque`, and `namedtuple`. Problem Statement You are given a dataset of sales records consisting of multiple products sold by multiple sales agents. You need to write a Python function that processes this dataset using the specialized container datatypes from the `collections` module. Function Signature ```python def process_sales_data(sales_data: list) -> tuple: pass ``` Inputs - `sales_data` (list of tuples): Each tuple in the list represents a sale record and has the following format: - `(agent_name, product_name, quantity_sold, sale_amount)` - `agent_name` (str): Name of the sales agent. - `product_name` (str): Name of the product. - `quantity_sold` (int): Quantity of the product sold. - `sale_amount` (float): Sale amount for the product sold. Outputs - A tuple containing the following elements: 1. A `defaultdict` where keys are `agent_name` and values are lists of `namedtuple` objects containing product details (`product_name`, `quantity_sold`, `sale_amount`). 2. An `OrderedDict` where keys are `product_name` and values are `Counter` objects representing total sales amounts and quantities sold across all agents. 3. A `ChainMap` combining the `defaultdict` and `OrderedDict`. Example ```python sales_data = [ (\'Alice\', \'Widget\', 10, 500.0), (\'Bob\', \'Gadget\', 5, 300.0), (\'Alice\', \'Widget\', 3, 150.0), (\'Alice\', \'Gizmo\', 7, 700.0), (\'Bob\', \'Widget\', 2, 100.0) ] result = process_sales_data(sales_data) ``` - The first element of the result tuple should be a `defaultdict` where: - \'Alice\' -> `[(\'Widget\', 10, 500.0), (\'Widget\', 3, 150.0), (\'Gizmo\', 7, 700.0)]` - \'Bob\' -> `[(\'Gadget\', 5, 300.0), (\'Widget\', 2, 100.0)]` - The second element of the result tuple should be an `OrderedDict` where: - \'Widget\' -> `Counter({\'total_quantity\': 15, \'total_amount\': 750.0})` - \'Gadget\' -> `Counter({\'total_quantity\': 5, \'total_amount\': 300.0})` - \'Gizmo\' -> `Counter({\'total_quantity\': 7, \'total_amount\': 700.0})` - The third element should be a `ChainMap` combining the `defaultdict` and `OrderedDict`. Constraints and Notes - You should make use of `namedtuple`, `defaultdict`, `Counter`, `OrderedDict`, and `ChainMap`. - Ensure that the processing of sales data is efficient and leverages the unique properties of these specialized container datatypes. - The `defaultdict` and `OrderedDict` should be constructed in a way that demonstrates the added value of using these specialized containers over standard dictionaries and lists. Good luck!","solution":"from collections import namedtuple, defaultdict, Counter, OrderedDict, ChainMap def process_sales_data(sales_data): # Define the namedtuple for product details ProductDetail = namedtuple(\'ProductDetail\', [\'product_name\', \'quantity_sold\', \'sale_amount\']) # defaultdict for agent sales agent_sales = defaultdict(list) # OrderedDict for product sales summary product_sales_summary = OrderedDict() # Process each sale record for agent_name, product_name, quantity_sold, sale_amount in sales_data: # Add the sale to the agent\'s sales list agent_sales[agent_name].append(ProductDetail(product_name, quantity_sold, sale_amount)) # Update the product sales summary if product_name not in product_sales_summary: product_sales_summary[product_name] = Counter() product_sales_summary[product_name][\'total_quantity\'] += quantity_sold product_sales_summary[product_name][\'total_amount\'] += sale_amount # Create a ChainMap combining the agent sales and product sales summary combined_data = ChainMap(agent_sales, product_sales_summary) return (agent_sales, product_sales_summary, combined_data)"},{"question":"Event Scheduler with Conflict Management Objective Design and implement a custom event scheduler using Python\'s `sched` module. The scheduler should be capable of managing and resolving conflicts between events scheduled at the same time. Problem Statement You have been asked to implement a function `custom_event_scheduler` that schedules a series of events and resolves any conflicts based on the priority of the events. Specifically: 1. Implement a function `custom_event_scheduler(events, timefunc, delayfunc)` where: - `events` is a list of tuples. Each tuple represents an event and contains four elements: - `time`: absolute time at which the event should run (int or float). - `priority`: priority of the event (int). Lower numbers indicate higher priority. - `action`: the function to execute. - `args`: a tuple of positional arguments to pass to the function. - `timefunc` and `delayfunc` are the functions used by the scheduler for time measurement and delaying execution respectively. 2. The function should: - Create a `scheduler` instance using the provided `timefunc` and `delayfunc`. - Schedule all events using `scheduler.enterabs()`. - Check for conflicts (events scheduled at the same time). If two events have the same time, prioritize the one with the higher priority. - Cancel the lower priority event if it conflicts with a higher priority event. - Run the scheduler and return the list of executed events in the order they were executed. Each entry in the list should be a tuple representing the executed event\'s `time`, `priority`, `action`, and `args`. 3. Raise a `ValueError` if any event cannot be scheduled due to an invalid time or if the event list is empty. Constraints - `time` will be a non-negative number. - `priority` will be a non-negative integer. - Ensure that other necessary constraints like valid `action` functions and `args` are handled appropriately. Example ```python import sched import time def sample_action(event_name): print(f\\"Event: {event_name} executed at {time.time()}\\") events = [ (10, 1, sample_action, (\\"event1\\",)), (5, 2, sample_action, (\\"event2\\",)), (5, 1, sample_action, (\\"event3\\",)), ] def custom_event_scheduler(events, timefunc, delayfunc): # Your implementation here # Sample Usage executed_events = custom_event_scheduler(events, time.time, time.sleep) print(executed_events) ``` **Expected Output:** The output should show that \\"event1\\" is executed after 10 seconds, \\"event3\\" after 5 seconds, and \\"event2\\" either discarded or executed based on its lower priority.","solution":"import sched import time def custom_event_scheduler(events, timefunc, delayfunc): if not events: raise ValueError(\\"Event list cannot be empty\\") scheduler = sched.scheduler(timefunc, delayfunc) event_map = {} for event in events: if len(event) != 4: raise ValueError(\\"Each event must contain exactly four elements\\") event_time, priority, action, args = event if not callable(action): raise ValueError(\\"Event action must be callable\\") # Check for conflicts if event_time in event_map: existing_event = event_map[event_time] if existing_event[0] <= priority: continue else: # Remove the lower priority event scheduler.cancel(existing_event[1]) # Schedule the event scheduled_event = scheduler.enterabs(event_time, priority, action, argument=args) event_map[event_time] = (priority, scheduled_event) executed_events = [] def wrap_action(action, args, event_time, priority): def wrapped_action(): action(*args) executed_events.append((event_time, priority, action, args)) return wrapped_action # Re-schedule with wrapped actions to capture executed events for event_time, (priority, scheduled_event) in event_map.items(): args = scheduled_event.argument action = scheduled_event.action scheduler.cancel(scheduled_event) scheduler.enterabs( event_time, priority, wrap_action(action, args, event_time, priority), ) scheduler.run() return executed_events # Sample Usage (for testing outside of pytest) def sample_action(event_name): print(f\\"Event: {event_name} executed at {time.time()}\\") events = [ (10, 1, sample_action, (\\"event1\\",)), (5, 2, sample_action, (\\"event2\\",)), (5, 1, sample_action, (\\"event3\\",)), ] # expected output: [(5, 1, sample_action, (\\"event3\\",)), (10, 1, sample_action, (\\"event1\\",))] executed_events = custom_event_scheduler(events, time.time, time.sleep) print(executed_events)"},{"question":"**Question: Implement a Custom Object Manager for Reference Counting** In this exercise, you will create a custom object manager in Python that simulates reference counting similar to what is described in the provided documentation. You need to implement a class `PyObjectManager` that will manage reference counts of a collection of Python objects and ensure proper memory management. # Requirements: 1. **Class**: `PyObjectManager` - Initialize with an empty dictionary to keep track of objects and their reference counts. 2. **Methods**: - `add_object(self, obj)`: Adds a new object to the manager and sets its reference count to 1. - `incref(self, obj)`: Increases the reference count of the object by 1. - `decref(self, obj)`: Decreases the reference count of the object by 1. If the reference count reaches 0, the object should be removed from the manager. - `get_ref_count(self, obj)`: Returns the current reference count of the object. - `clear(self)`: Clears all objects from the manager. # Constraints: - Objects are expected to be keys in the manager, but for simplicity, you can assume that objects are immutable and hashable (e.g., strings, tuples). - Do not handle reference counts for `NULL` objects (None). # Example Usage: ```python manager = PyObjectManager() # Adding objects manager.add_object(\\"object1\\") print(manager.get_ref_count(\\"object1\\")) # Output: 1 # Incrementing reference count manager.incref(\\"object1\\") print(manager.get_ref_count(\\"object1\\")) # Output: 2 # Decrementing reference count manager.decref(\\"object1\\") print(manager.get_ref_count(\\"object1\\")) # Output: 1 # Removing object when reference count reaches 0 manager.decref(\\"object1\\") print(manager.get_ref_count(\\"object1\\")) # Output: KeyError, obj is removed # Clearing the manager manager.add_object(\\"object2\\") manager.clear() print(manager.get_ref_count(\\"object2\\")) # Output: KeyError, obj is removed ``` Note: Make sure the manager raises appropriate exceptions when trying to increment, decrement, or get the reference count of an object that does not exist in the manager. # Bonus: Implement additional validation to ensure that invalid operations (like decreasing the reference count of a non-existent object) raise an appropriate custom exception, such as `ObjectNotFoundException`.","solution":"class ObjectNotFoundException(Exception): pass class PyObjectManager: def __init__(self): self._objects = {} def add_object(self, obj): if obj not in self._objects: self._objects[obj] = 1 else: raise ValueError(f\\"Object {obj} already exists in the manager.\\") def incref(self, obj): if obj in self._objects: self._objects[obj] += 1 else: raise ObjectNotFoundException(f\\"Object {obj} not found in the manager.\\") def decref(self, obj): if obj in self._objects: if self._objects[obj] > 1: self._objects[obj] -= 1 else: del self._objects[obj] else: raise ObjectNotFoundException(f\\"Object {obj} not found in the manager.\\") def get_ref_count(self, obj): if obj in self._objects: return self._objects[obj] else: raise ObjectNotFoundException(f\\"Object {obj} not found in the manager.\\") def clear(self): self._objects.clear()"},{"question":"**Coding Assessment Question:** # Objective Demonstrate your understanding of Pytorch Hub by implementing and publishing a custom model, and then loading and using this model from the Hub. # Task 1. Create a simple custom neural network model using Pytorch. 2. Define an entrypoint function for this model in a `hubconf.py` file. 3. Write code to publish this model. 4. Write code to load the published model using Pytorch Hub and run a simple inference using random input data. # Steps **Step 1: Implement the Custom Model** Create a simple Feedforward Neural Network in Pytorch. The network should have: - An input layer of size 10. - One hidden layer of size 5. - An output layer of size 2. - Use ReLU activation for the hidden layer. **Step 2: Define the Entrypoint** Define the entrypoint for your model within a `hubconf.py` file. The function should: - Import necessary dependencies. - Instantiate and return your model. **Step 3: Publish the Model** Simulate the publishing of the model by ensuring your `hubconf.py` is correctly set up as per the documentation guidelines. **Step 4: Load and Use the Model** Write code to: 1. Load the published model from Pytorch Hub. 2. Create random input data of shape (1, 10). 3. Perform a forward pass using the loaded model and print the output. # Submission Submit a `model.py` file containing the model implementation and a `hubconf.py` file containing the entrypoint definition. Additionally, provide a Jupyter notebook or a Python script that demonstrates loading the model from Pytorch Hub and performing inference. # Constraints - The custom model implementation should be in Pytorch. - `hubconf.py` should correctly follow the example provided in the documentation. - Simulated publishing can be demonstrated by ensuring the `hubconf.py` is correctly set up; actual GitHub repository interaction is not required. # Evaluation Criteria - Correctness of the model implementation. - Conformity of the `hubconf.py` to Pytorch Hub standards. - Ability to correctly load and use the model from Pytorch Hub. - Clarity and readability of the code.","solution":"# Step 1: Implement the Custom Model import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 5) self.relu = nn.ReLU() self.fc2 = nn.Linear(5, 2) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Step 2: Define the Entrypoint in hubconf.py # Note: This code would typically be placed in its own hubconf.py file, not executed directly. def simple_nn_model(): Returns the simple neural network model. model = SimpleNN() return model # Actual implementation of publishing and loading the model from Pytorch Hub would require pushing to a repository, which is not covered here. # Step 4: Load and Use the Model (Simulated) def load_from_hub(): # Simulating loading the model from the Pytorch Hub # In a real scenario, you would use torch.hub.load(\'your-repo\', \'simple_nn_model\') model = simple_nn_model() # Generate random input data with shape (1, 10) random_input = torch.randn(1, 10) # Perform inference output = model(random_input) return output"},{"question":"# NIS Module Coding Assessment You are given a Unix-based system that uses the Network Information Service (NIS) for centralized administration. Your task is to implement a Python function using the deprecated `nis` module to perform specific NIS-related tasks. Task Implement a function `nis_info(mapname: str, key: str) -> dict` that performs the following: 1. Retrieves the default NIS domain. 2. Fetches all maps available in this domain. 3. Collects all key-value pairs from the specified `mapname` into a dictionary using `nis.cat()`. 4. Finds the match for the given `key` in the specified `mapname` using `nis.match()` and includes this in the dictionary with the `key` prefixed by `\\"match_\\"`. 5. Handle any potential `nis.error` exceptions that may occur during these operations. Here\'s the function signature: ```python import nis def nis_info(mapname: str, key: str) -> dict: pass ``` Input: - `mapname` (str): The name of the NIS map from which to fetch key-value pairs. - `key` (str): The key for which to find a matching value in the specified `mapname`. Output: - The function should return a dictionary containing the following: - `\'domain\'`: The default NIS domain. - `\'maps\'`: List of all available maps. - `\'data\'`: A dictionary of key-value pairs from the specified `mapname`. - `\'match_key\'`: The value corresponding to the prefix `\\"match_\\"` + `key`. Example: Consider the following scenario: - Default domain: `\'example_domain\'` - Available maps: `[\'passwd.byname\', \'group.byname\']` - `mapname`: `\'passwd.byname\'` - `key`: `\'testuser\'` - Assume the map contains the data: `{\'testuser\': \'UserData1\', \'admin\': \'AdminData\'}` If the key `\'testuser\'` exists in `mapname`, the function returns: ```python { \'domain\': \'example_domain\', \'maps\': [\'passwd.byname\', \'group.byname\'], \'data\': {\'testuser\': \'UserData1\', \'admin\': \'AdminData\'}, \'match_testuser\': \'UserData1\' } ``` Notes: - Ensure to handle the scenario where `mapname` or `key` does not exist gracefully using `try-except` blocks. - The module is Unix-specific; make sure your environment supports NIS. Constraints: - You do not need to handle multi-threading or concurrency. - Assume the environment is correctly configured and NIS services are running. Good luck!","solution":"import nis def nis_info(mapname: str, key: str) -> dict: result = {} try: # Retrieve the default NIS domain domain = nis.get_default_domain() result[\'domain\'] = domain # Fetch all maps available in this domain maps = nis.maps() result[\'maps\'] = maps # Collect all key-value pairs from the specified mapname try: data = nis.cat(mapname) except nis.error: data = {} result[\'data\'] = data # Find the match for the given key in the specified mapname try: match_value = nis.match(key, mapname) except nis.error: match_value = None result[f\'match_{key}\'] = match_value except nis.error as e: # Handle the general NIS error case result[\'error\'] = str(e) return result"},{"question":"# Question: Data Visualization and Analysis Using Pandas As a data scientist, you are given a dataset `sales_data.csv` that contains sales information for a retail company. Your task is to analyze and visualize this data using the pandas library. Here is the structure of `sales_data.csv`: ``` Date,Store,Product,Revenue,Units Sold 2021-01-01,Store_A,Product_1,1000,5 2021-01-01,Store_B,Product_2,1500,3 2021-01-02,Store_A,Product_1,1100,6 ... ``` # Task 1. **Data Loading and Preparation**: - Load the dataset into a DataFrame. - Convert the `Date` column to datetime type. - Set the `Date` column as the index of the DataFrame. 2. **Data Analysis**: - Calculate the total revenue and total units sold for each store. 3. **Data Visualization**: - Plot a line graph showing the total daily revenue for each store. Ensure that each store\'s data line is shown in a different color. - Create a scatter plot to visualize the relationship between units sold and revenue for each store. Use different colors to distinguish different stores. - Generate a boxplot of daily revenues for each store to understand the distribution. 4. **Advanced Plot**: - Use `scatter_matrix` to create a matrix of scatter plots to show pairwise relationships between `Revenue` and `Units Sold` across different stores and products. # Implementation Details - **Input**: Path to the CSV file `sales_data.csv`. - **Output**: Displayed plots. # Constraints - Use the pandas and `pandas.plotting` modules only. - Ensure plots are neatly labeled and legible. - Your code should handle missing values gracefully. # Example ```python import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix # Load the data df = pd.read_csv(\'sales_data.csv\') # Convert Date to datetime and set as index df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) # Calculate total revenue and units sold per store store_stats = df.groupby(\'Store\').agg({\'Revenue\': \'sum\', \'Units Sold\': \'sum\'}) # Plotting # 1. Line plot for daily revenue per store daily_revenue = df.groupby([df.index, \'Store\'])[\'Revenue\'].sum().unstack() daily_revenue.plot() plt.title(\'Daily Revenue per Store\') plt.ylabel(\'Total Revenue\') plt.show() # 2. Scatter plot for units sold vs revenue colors = {\'Store_A\': \'red\', \'Store_B\': \'blue\'} for store in df[\'Store\'].unique(): store_data = df[df[\'Store\'] == store] plt.scatter(store_data[\'Units Sold\'], store_data[\'Revenue\'], label=store, color=colors[store]) plt.title(\'Units Sold vs Revenue\') plt.xlabel(\'Units Sold\') plt.ylabel(\'Revenue\') plt.legend() plt.show() # 3. Boxplot for daily revenues per store df.boxplot(column=\'Revenue\', by=\'Store\') plt.title(\'Revenue Distribution per Store\') plt.suptitle(\'\') plt.show() # 4. Scatter matrix for Revenue and Units Sold scatter_matrix(df[[\'Revenue\', \'Units Sold\']], alpha=0.2, figsize=(6, 6), diagonal=\'kde\') plt.show() ``` **Note**: This is just an illustration, and you should write your code to handle any additional requirements or constraints.","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix def load_and_prepare_data(file_path): Load the dataset and prepare it for analysis. :param file_path: Path to the sales data CSV file :return: Prepared pandas DataFrame df = pd.read_csv(file_path) df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) return df def calculate_store_stats(df): Calculate total revenue and total units sold per store. :param df: The pandas DataFrame containing sales data :return: DataFrame with total revenue and total units sold per store return df.groupby(\'Store\').agg({\'Revenue\': \'sum\', \'Units Sold\': \'sum\'}) def plot_daily_revenue(df): Plot daily revenue for each store. :param df: The pandas DataFrame containing sales data daily_revenue = df.groupby([df.index, \'Store\'])[\'Revenue\'].sum().unstack() daily_revenue.plot() plt.title(\'Daily Revenue per Store\') plt.ylabel(\'Total Revenue\') plt.show() def plot_units_sold_vs_revenue(df): Create a scatter plot to visualize the relationship between units sold and revenue for each store. :param df: The pandas DataFrame containing sales data colors = {store: color for store, color in zip(df[\'Store\'].unique(), [\'red\', \'blue\', \'green\', \'orange\'])} for store in df[\'Store\'].unique(): store_data = df[df[\'Store\'] == store] plt.scatter(store_data[\'Units Sold\'], store_data[\'Revenue\'], label=store, color=colors[store]) plt.title(\'Units Sold vs Revenue\') plt.xlabel(\'Units Sold\') plt.ylabel(\'Revenue\') plt.legend() plt.show() def plot_revenue_distribution(df): Generate a boxplot of daily revenues for each store to understand the distribution. :param df: The pandas DataFrame containing sales data df.boxplot(column=\'Revenue\', by=\'Store\') plt.title(\'Revenue Distribution per Store\') plt.suptitle(\'\') plt.show() def plot_scatter_matrix(df): Use scatter_matrix to create a matrix of scatter plots to show pairwise relationships between Revenue and Units Sold across different stores and products. :param df: The pandas DataFrame containing sales data scatter_matrix(df[[\'Revenue\', \'Units Sold\']], alpha=0.2, figsize=(6, 6), diagonal=\'kde\') plt.show()"},{"question":"**Coding Assessment Question:** You are tasked with designing an event scheduler that manages multiple events with various priorities and execution times. You will use the \\"sched\\" module to implement the following functionalities: 1. **Initialize Scheduler**: Write a function `initialize_scheduler` that initializes a `sched.scheduler` object with the appropriate `timefunc` and `delayfunc`. 2. **Add Event**: Write a function `add_event` that schedules an event either at an absolute time or after a specified delay. 3. **Cancel Event**: Write a function `cancel_event` that cancels a scheduled event. 4. **Run Scheduler**: Write a function `run_scheduler` that executes all scheduled events. 5. **Get Event Queue**: Write a function `get_event_queue` that returns a list of upcoming events in the order they will be run. Here are the detailed specifications of each function: # Function 1: `initialize_scheduler` **Input**: None **Output**: Returns a `sched.scheduler` object initialized with `time.time` as the `timefunc` and `time.sleep` as the `delayfunc`. # Function 2: `add_event` **Input**: - `scheduler`: A `sched.scheduler` object. - `time_or_delay`: A float representing the absolute time if `absolute` is `True`, or a delay if `absolute` is `False`. - `priority`: An integer representing the priority of the event (lower number indicates higher priority). - `action`: A callable representing the event action. - `argument`: A tuple containing positional arguments for the action. - `kwargs`: A dictionary containing keyword arguments for the action. - `absolute`: A boolean indicating whether `time_or_delay` is an absolute time (`True`) or a delay (`False`). **Output**: Returns the event object scheduled either using `enterabs` or `enter`, depending on the `absolute` parameter. # Function 3: `cancel_event` **Input**: - `scheduler`: A `sched.scheduler` object. - `event`: An event object to be canceled. **Output**: None, but the function should raise a `ValueError` if the event is not currently in the queue. # Function 4: `run_scheduler` **Input**: - `scheduler`: A `sched.scheduler` object. **Output**: None, the function should execute all scheduled events. # Function 5: `get_event_queue` **Input**: - `scheduler`: A `sched.scheduler` object. **Output**: Returns a list of upcoming events, each represented as a named tuple with fields: `time`, `priority`, `action`, `argument`, `kwargs`. # Example Usage ```python import time import sched # Initialize the scheduler scheduler = initialize_scheduler() # Define some example actions def hello(name): print(f\\"Hello, {name}!\\") def goodbye(name): print(f\\"Goodbye, {name}!\\") # Add events event1 = add_event(scheduler, time.time() + 10, 1, hello, (\\"Alice\\",), {}, True) event2 = add_event(scheduler, 5, 2, goodbye, (\\"Bob\\",), {}, False) # List events print(get_event_queue(scheduler)) # Run scheduler (This will block and run the events after their specified times) run_scheduler(scheduler) # Cancel the first event (if it hasn\'t run yet) cancel_event(scheduler, event1) # List events again to see the remaining ones print(get_event_queue(scheduler)) ``` Implement the functions according to the specifications and ensure to handle edge cases and exceptions appropriately.","solution":"import sched import time def initialize_scheduler(): Initialize and return a new scheduler object. return sched.scheduler(time.time, time.sleep) def add_event(scheduler, time_or_delay, priority, action, argument=(), kwargs={}, absolute=False): Schedule an event in the scheduler. if absolute: event = scheduler.enterabs(time_or_delay, priority, action, argument, kwargs) else: event = scheduler.enter(time_or_delay, priority, action, argument, kwargs) return event def cancel_event(scheduler, event): Cancel a scheduled event. scheduler.cancel(event) def run_scheduler(scheduler): Run the scheduler to execute all scheduled events. scheduler.run() def get_event_queue(scheduler): Return a list of upcoming events in the scheduler. return scheduler.queue"},{"question":"**Asyncio Exceptions Handling and Custom Future Function** In this task, you will write an asynchronous function that demonstrates handling of several asyncio exceptions. You are required to write an asynchronous function `handle_asyncio_exceptions` that takes two arguments: - `operation`: a coroutine function representing the asynchronous operation. - `timeout`: an integer representing the timeout in seconds for the given operation. Your function should: 1. Execute the given `operation` using the `await` syntax. 2. Raise a `TimeoutError` if the operation exceeds the provided timeout period. 3. Handle the following asyncio exceptions gracefully: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.InvalidStateError` - `asyncio.IncompleteReadError` - `asyncio.LimitOverrunError` 4. Return a meaningful error message in case of these exceptions. 5. If the operation completes successfully, return the result of the operation. Here is the function signature for `handle_asyncio_exceptions`: ```python import asyncio async def handle_asyncio_exceptions(operation: callable, timeout: int) -> str: pass ``` # Example Usage: You can test your function using an example coroutine that intentionally raises an `asyncio.TimeoutError`: ```python async def example_operation(): await asyncio.sleep(10) result = await handle_asyncio_exceptions(example_operation, 5) print(result) # Should print a message related to the `asyncio.TimeoutError` ``` # Constraints: - The `timeout` will be a non-negative integer. - The `operation` will be a valid coroutine function. - Make sure to catch and handle each specified exception separately to return specific error messages for each type. # Performance Requirements: - Ensure the function handles the timeout efficiently and does not block indefinitely. This task assesses your understanding of asynchronous operations and exception handling in `asyncio`. Make sure to test your implementation thoroughly with different scenarios.","solution":"import asyncio async def handle_asyncio_exceptions(operation: callable, timeout: int) -> str: try: return await asyncio.wait_for(operation(), timeout=timeout) except asyncio.TimeoutError: return \\"The operation timed out.\\" except asyncio.CancelledError: return \\"The operation was cancelled.\\" except asyncio.InvalidStateError: return \\"The operation is in an invalid state.\\" except asyncio.IncompleteReadError: return \\"The operation could not be completed due to incomplete read.\\" except asyncio.LimitOverrunError: return \\"The operation exceeded the buffer limit.\\" except Exception as e: return f\\"An unspecified error occurred: {e}\\""},{"question":"# PyTorch Accelerator Management You are given the task of implementing a function that efficiently uses multiple GPU devices to perform matrix multiplications. Your function should use PyTorch\'s `torch.accelerator` module to manage these devices. The function should: 1. Check if any GPU devices are available. 2. If no devices are available, perform the computation on the CPU. 3. If devices are available, distribute the matrix multiplication workload evenly across all available devices. 4. Ensure all devices are synchronized before returning the final result. Function Signature ```python def multi_gpu_matrix_multiplication(mat_a: torch.Tensor, mat_b: torch.Tensor) -> torch.Tensor: pass ``` Input - `mat_a` (torch.Tensor): A 2D tensor representing the first matrix. - `mat_b` (torch.Tensor): A 2D tensor representing the second matrix. Output - `torch.Tensor`: Resulting 2D tensor after multiplying `mat_a` and `mat_b`. Constraints - You may assume that the dimensions of `mat_a` and `mat_b` are compatible for matrix multiplication. - You can use helper functions from the `torch.accelerator` module as needed. - Your solution should be efficient and correctly utilize the available hardware. Example ```python mat_a = torch.randn(1000, 2000) mat_b = torch.randn(2000, 1000) result = multi_gpu_matrix_multiplication(mat_a, mat_b) print(result.shape) # Output should be torch.Size([1000, 1000]) ``` Performance Requirement - The function should handle large matrices efficiently. - The function should leverage the available GPUs to improve performance compared to a CPU-only implementation. Note Ensure your implementation takes advantage of PyTorch functionalities to manage device contexts and synchronization efficiently.","solution":"import torch def multi_gpu_matrix_multiplication(mat_a: torch.Tensor, mat_b: torch.Tensor) -> torch.Tensor: Multiplies two matrices using multiple GPUs if available, otherwise falls back to CPU. Parameters: mat_a (torch.Tensor): The first matrix. mat_b (torch.Tensor): The second matrix. Returns: torch.Tensor: The resulting matrix after multiplication. if torch.cuda.is_available(): n_gpus = torch.cuda.device_count() device = \'cuda\' # Split the matrices into chunks along the first dimension chunks_a = torch.chunk(mat_a, n_gpus) chunks_b = torch.chunk(mat_b, n_gpus) # Allocate device memory and perform distributed computation result_chunks = [] for i in range(n_gpus): chunk_a = chunks_a[i].to(f\'cuda:{i}\') chunk_b = chunks_b[i].to(f\'cuda:{i}\') result_chunks.append(torch.matmul(chunk_a, chunk_b).to(\'cpu\')) # Synchronize all GPUs torch.cuda.synchronize() # Concatenate the result chunks result = torch.cat(result_chunks, dim=0) else: # Fallback to CPU computation result = torch.matmul(mat_a, mat_b) return result"},{"question":"**Python Coding Assessment Question** # Objective: To assess the understanding of path manipulations, configuration file handling, and user-based customizations in Python using the `site` module. # Question: You are tasked with implementing a function that configures Python\'s site-specific directories and verifies the proper setup by loading custom configuration files. # Problem Statement: Implement a function `configure_python_site()` that accepts two optional parameters: - `include_user_site`: a boolean indicating whether user-specific site packages should be included. - `extra_paths`: a list of additional paths to be included in `sys.path`. The function should: 1. Use the `site.addsitedir()` function to add directories specified in `extra_paths` to `sys.path` and process any \\".pth\\" files in these directories. 2. Configure user-specific site directories based on the `include_user_site` parameter. If `include_user_site` is `True`, ensure the user site-packages directory is included in `sys.path`. 3. Return a dictionary with the updated `sys.path` and the status of user site-packages inclusion, indicating whether it was added (`True`) or not (`False`). # Function Signature: ```python def configure_python_site(include_user_site: bool = True, extra_paths: list = None) -> dict: pass ``` # Constraints: - Do not modify global settings outside the function scope. - You can assume the `site` module is already imported. - You should handle the given paths correctly and check their existence. - Assume the user has permissions to modify the paths and configurations. # Example Usage: ```python result = configure_python_site(include_user_site=True, extra_paths=[\\"/custom/path1\\", \\"/custom/path2\\"]) # Expected example output (the exact order of paths can vary): assert result == { \\"sys_path\\": [ \\"/custom/path1\\", \\"/custom/path2\\", \\"/usr/local/lib/pythonX.Y/site-packages\\", # other default paths ], \\"user_site_included\\": True } result = configure_python_site(include_user_site=False) # Expected example output (the exact order of paths can vary): assert result == { \\"sys_path\\": [ # default paths without user site packages ], \\"user_site_included\\": False } ``` # Notes: - Make sure to handle non-existing paths properly by verifying before adding them. - Initial paths in `sys.path` should be retained unless you explicitly add new paths.","solution":"import sys import site from typing import List, Dict, Any def configure_python_site(include_user_site: bool = True, extra_paths: List[str] = None) -> Dict[str, Any]: Configures the Python site-specific directories and optionally includes user-specific site packages. Parameters: include_user_site (bool): Flag to include user-specific site packages. extra_paths (List[str]): Additional paths to include in sys.path. Returns: dict: Dictionary with updated sys.path and user site packages inclusion status. # Create a copy of the current sys.path original_sys_path = sys.path.copy() # Add extra paths to sys.path and process .pth files if extra_paths: for path in extra_paths: site.addsitedir(path) user_site_included = False # Include user-specific site packages if specified if include_user_site: user_site_path = site.getusersitepackages() user_site_included = True if user_site_path not in sys.path: sys.path.append(user_site_path) return { \\"sys_path\\": sys.path, \\"user_site_included\\": user_site_included }"},{"question":"**Coding Assessment Question** # Distributed Multiprocessing with Subprocesses in PyTorch **Objective**: Implement a function that demonstrates the usage of PyTorch\'s `SubprocessHandler` to manage multiple subprocesses running a simple distributed computation task. # Problem Statement: You are required to implement a function called `run_distributed_computation` that: 1. Uses the `SubprocessHandler` from `torch.distributed.elastic.multiprocessing.subprocess_handler`. 2. Spawns multiple subprocesses to perform a parallel computation. 3. Collects and returns the results of the computation from each subprocess. # Function Signature ```python def run_distributed_computation(num_procs: int) -> List[Any]: ``` # Input - `num_procs` (int): The number of subprocesses to spawn for the distributed computation. # Output - Returns a list of results from each subprocess. # Constraints - Each subprocess should execute the same computation function. - You should handle subprocess creation and management within the function. - Ensure that all subprocesses complete execution before collecting results. # Computation Function Define a simple computation function `compute`, which: - Takes an integer `proc_id` as input. - Returns a dictionary with the `proc_id` and a computed value (e.g., square of the `proc_id`). # Example: 1. Define the `compute` function as follows: ```python def compute(proc_id: int) -> Dict[str, Any]: result = { \\"proc_id\\": proc_id, \\"value\\": proc_id * proc_id } return result ``` 2. Use `SubprocessHandler` to spawn `num_procs` subprocesses that execute the `compute` function with different `proc_id` values (ranging from `0` to `num_procs-1`). 3. Collect and return the results from each subprocess as a list of dictionaries. # Notes - Ensure proper subprocess handling and communication to prevent deadlocks and ensure robust performance. - Use available methods in `SubprocessHandler` to manage subprocess lifecycle and result collection. # Expected Implementation ```python import torch.distributed.elastic.multiprocessing.subprocess_handler.handlers as handlers from torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler import SubprocessHandler from typing import List, Any, Dict def compute(proc_id: int) -> Dict[str, Any]: result = { \\"proc_id\\": proc_id, \\"value\\": proc_id * proc_id } return result def run_distributed_computation(num_procs: int) -> List[Any]: handler = handlers.get_subprocess_handler() # Define a list to keep track of subprocesses and results subprocesses = [] results = [] for proc_id in range(num_procs): subprocess = SubprocessHandler(target=compute, args=(proc_id,)) subprocesses.append(subprocess) subprocess.start() for subprocess in subprocesses: subprocess.join() results.append(subprocess.result()) return results ```","solution":"import torch.multiprocessing as mp from typing import List, Any, Dict def compute(proc_id: int) -> Dict[str, Any]: result = { \\"proc_id\\": proc_id, \\"value\\": proc_id * proc_id } return result def run_computation_process(proc_id: int, results: List): results.append(compute(proc_id)) def run_distributed_computation(num_procs: int) -> List[Any]: manager = mp.Manager() results = manager.list() processes = [] for proc_id in range(num_procs): p = mp.Process(target=run_computation_process, args=(proc_id, results)) processes.append(p) p.start() for p in processes: p.join() return list(results)"},{"question":"**Objective:** You are tasked with writing a function that processes HTTP status codes and returns appropriate responses based on their classification. **Problem Statement:** Write a Python function `classify_http_status(status_code: int) -> str` that takes an HTTP status code as input and returns a string indicating the type of HTTP status it represents. The classifications are based on the first digit of the status code: - 1xx: Informational - 2xx: Success - 3xx: Redirection - 4xx: Client Error - 5xx: Server Error If the provided status code does not fall into any of these ranges, return \\"Unknown Status\\". **Constraints:** - You must use the `http.HTTPStatus` enumeration to handle and check status codes. - Performance should be optimal for a function that may need to handle multiple requests in a web application context. **Function Signature:** ```python def classify_http_status(status_code: int) -> str: pass ``` **Input:** - `status_code` (int): The HTTP status code to classify. **Output:** - (str): A string indicating the classification of the HTTP status code. **Example:** ```python >>> classify_http_status(200) \'Success\' >>> classify_http_status(404) \'Client Error\' >>> classify_http_status(503) \'Server Error\' >>> classify_http_status(102) \'Informational\' >>> classify_http_status(700) \'Unknown Status\' ``` **Additional Information:** Use the Python `http.HTTPStatus` enumeration to determine the classification efficiently. This ensures that any standard or non-standard status codes defined in `http.HTTPStatus` are correctly recognized.","solution":"from http import HTTPStatus def classify_http_status(status_code: int) -> str: if 100 <= status_code < 200: return \\"Informational\\" elif 200 <= status_code < 300: return \\"Success\\" elif 300 <= status_code < 400: return \\"Redirection\\" elif 400 <= status_code < 500: return \\"Client Error\\" elif 500 <= status_code < 600: return \\"Server Error\\" else: return \\"Unknown Status\\""},{"question":"**Coding Question: Profiling and Analyzing a Python Function** # Objective The goal of this exercise is to assess your understanding of Python\'s profiling tools (\\"cProfile\\", \\"pstats\\") and your ability to analyze the performance of a given function. # Description You are given a function `process_data(data: list) -> list` that processes a list of data and performs multiple operations. Your task is to profile this function using \\"cProfile\\", store the profiling results, and then carry out specific analysis and reporting using \\"pstats\\". # Instructions 1. **Write a function `profile_process_data(data: list) -> str`:** - This function should profile the `process_data` function using \\"cProfile\\". - Store the profiling results in a temporary file named `profile_stats`. - Return the name of the file containing the profiling results. 2. **Write a function `analyze_profile_results(filename: str) -> str`:** - This function should read the profiling results from the specified file using \\"pstats\\". - Perform the following operations on the profiling results: - Remove all leading path information from file names. - Sort the function call statistics based on \'cumulative\' time. - Limit the results to the top 10 most time-consuming functions. - Format the sorted statistics as a string using `pstats.Stats.print_stats()` and return this string. # Function Signature ```python def profile_process_data(data: list) -> str: pass def analyze_profile_results(filename: str) -> str: pass ``` # Example Usage ```python data = [....] # some large list of data profile_filename = profile_process_data(data) profile_report = analyze_profile_results(profile_filename) print(profile_report) ``` # Constraints - You are not allowed to modify the `process_data` function. - The `process_data` function should be presumed to have various nested function calls which need to be profiled. - You should use context managers where appropriate. # Performance Requirements - Ensure that your profiling and analysis functions handle the provided data within reasonable time limits, and avoid excessive memory usage. - Make use of the best practices for profiling, such as using sorting keys and limiting the number of reported lines for better readability. # `process_data` Function (For Testing) For the purpose of testing, you can use the following dummy implementation: ```python import time import random def process_data(data: list) -> list: def helper_func(item): time.sleep(random.uniform(0.001, 0.003)) return item ** 2 result = [] for item in data: result.append(helper_func(item)) return result ``` Make sure to profile the performance of this function on a sufficiently large dataset to gather significant results.","solution":"import cProfile import pstats from io import StringIO import os import time import random # Dummy process_data function provided for testing purposes def process_data(data: list) -> list: def helper_func(item): time.sleep(random.uniform(0.001, 0.003)) return item ** 2 result = [] for item in data: result.append(helper_func(item)) return result def profile_process_data(data: list) -> str: Profile the process_data function using cProfile. Parameters: data (list): The input data to be processed. Returns: str: The name of the file containing the profiling results. profile_filename = \'profile_stats\' profiler = cProfile.Profile() profiler.enable() process_data(data) profiler.disable() profiler.dump_stats(profile_filename) return profile_filename def analyze_profile_results(filename: str) -> str: Analyze the profiling results using pstats and return the top 10 most time-consuming functions. Parameters: filename (str): The name of the file containing the profiling results. Returns: str: A formatted string of the sorted statistics. stream = StringIO() stats = pstats.Stats(filename, stream=stream) stats.strip_dirs().sort_stats(\'cumulative\').print_stats(10) return stream.getvalue()"},{"question":"You are tasked with developing a utility script for handling email data, including formatting message IDs, parsing and formatting email addresses and dates, and ensuring compliance with RFC standards. To complete this task, you will implement the following functions using the `email.utils` module: 1. **localtime_now()** - **Input:** None - **Output:** Current local time as an aware datetime object. - **Details:** Utilize `email.utils.localtime` to get the current local time. 2. **generate_message_id(idstring=None, domain=None)** - **Input:** Optional strings `idstring` and `domain`. - **Output:** An RFC 2822-compliant message ID. - **Details:** Use `email.utils.make_msgid`. 3. **sanitize_string(input_str)** - **Input:** A string `input_str`. - **Output:** A new string with backslashes replaced by two backslashes, and double quotes replaced by backslash-double quote. - **Details:** Use `email.utils.quote`. 4. **extract_email_addresses(field_values)** - **Input:** A list of header field values `field_values`. - **Output:** A list of 2-tuples, each containing `realname` and `email address`. - **Details:** Use `email.utils.getaddresses`. 5. **format_email_address(realname, email_address)** - **Input:** Two strings, `realname` and `email_address`. - **Output:** A string suitable for a \'To\' or \'Cc\' header. - **Details:** Use `email.utils.formataddr`. 6. **convert_date_to_tuple(date_string)** - **Input:** A string `date_string` in RFC 2822 date format. - **Output:** A 9-tuple (from `parsedate`) or 10-tuple (from `parsedate_tz`). - **Details:** Parse the date using `email.utils.parsedate` or `email.utils.parsedate_tz`. # Constraints: - Your implementation should handle typical error cases gracefully. - Ensure compliance with the specified RFC standards in each function. # Expected Function signatures: ```python import datetime from typing import Optional, List, Tuple def localtime_now() -> datetime.datetime: # implementation here def generate_message_id(idstring: Optional[str] = None, domain: Optional[str] = None) -> str: # implementation here def sanitize_string(input_str: str) -> str: # implementation here def extract_email_addresses(field_values: List[str]) -> List[Tuple[str, str]]: # implementation here def format_email_address(realname: Optional[str], email_address: str) -> str: # implementation here def convert_date_to_tuple(date_string: str) -> Tuple: # implementation here ``` # Example Usage: ```python # Example function calls and their expected outputs print(localtime_now()) # Outputs current local time as a datetime object print(generate_message_id(\\"example\\", \\"example.com\\")) # Outputs a unique message ID print(sanitize_string(\'Hello \\"World\\"\')) # Outputs \'Hello \\"World\\"\' print(extract_email_addresses([\\"John Doe <john@example.com>\\", \\"jane@example.com\\"])) # Outputs [(\'John Doe\', \'john@example.com\'), (\'\', \'jane@example.com\')] print(format_email_address(\\"John Doe\\", \\"john@example.com\\")) # Outputs \'John Doe <john@example.com>\' print(convert_date_to_tuple(\\"Mon, 20 Nov 1995 19:12:08 -0500\\")) # Outputs (1995, 11, 20, 19, 12, 8, 0, 1, -1) ``` # Note: - Ensure your functions handle inputs and outputs as described. - Utilize the `email.utils` functions extensively to achieve the desired functionality.","solution":"import email.utils import datetime from typing import Optional, List, Tuple def localtime_now() -> datetime.datetime: Returns the current local time as an aware datetime object. return email.utils.localtime() def generate_message_id(idstring: Optional[str] = None, domain: Optional[str] = None) -> str: Generates an RFC 2822-compliant message ID. return email.utils.make_msgid(idstring, domain) def sanitize_string(input_str: str) -> str: Sanitizes a string by replacing backslashes with two backslashes, and double quotes with backslash-double quote. return input_str.replace(\'\', \'\').replace(\'\\"\', \'\\"\') def extract_email_addresses(field_values: List[str]) -> List[Tuple[str, str]]: Extracts email addresses from a list of header field values. Returns a list of 2-tuples containing realname and email address. return email.utils.getaddresses(field_values) def format_email_address(realname: Optional[str], email_address: str) -> str: Formats an email address suitable for a \'To\' or \'Cc\' header. return email.utils.formataddr((realname, email_address)) def convert_date_to_tuple(date_string: str) -> Tuple: Converts an RFC 2822 date string to a 9-tuple or 10-tuple. return email.utils.parsedate_tz(date_string)"},{"question":"**Objective:** In this task, you are required to implement a function that trains a simple neural network model on the MPS device, provided that the MPS backend is available. This involves checking for MPS availability, creating tensors on the MPS device, and training a model using these tensors. **Task:** Implement a function `train_on_mps` that takes the following inputs: - `input_size`: An integer representing the size of the input layer. - `output_size`: An integer representing the size of the output layer. - `hidden_size`: An integer representing the size of the hidden layer. - `num_epochs`: An integer specifying the number of epochs for training. - `learning_rate`: A float specifying the learning rate for the optimizer. The function should: 1. Check if the MPS device is available. If not, print an appropriate error message and terminate. 2. Create a simple neural network with one hidden layer. 3. Initialize input and output tensors directly on the MPS device. 4. Train the neural network on the input data for the specified number of epochs. 5. Print the loss value every 10 epochs to monitor training progress. **Constraints:** - You can assume `input_size`, `output_size`, and `hidden_size` are positive integers. - The `num_epochs` is a positive integer. - The `learning_rate` is a positive float. **Expected function signature:** ```python def train_on_mps(input_size: int, output_size: int, hidden_size: int, num_epochs: int, learning_rate: float) -> None: pass ``` **Example:** ```python train_on_mps(input_size=10, output_size=2, hidden_size=5, num_epochs=100, learning_rate=0.01) ``` This should create a neural network with 10 inputs, 5 hidden units, and 2 outputs, and train it for 100 epochs with a learning rate of 0.01 on the MPS device, if available. **Note:** You can use any available input/output data to demonstrate the function operation. For simplicity, you may use randomly initialized data.","solution":"import torch import torch.nn as nn import torch.optim as optim def train_on_mps(input_size: int, output_size: int, hidden_size: int, num_epochs: int, learning_rate: float) -> None: if not torch.backends.mps.is_available(): print(\\"MPS device not available.\\") return device = torch.device(\\"mps\\") class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.hidden_layer = nn.Linear(input_size, hidden_size) self.output_layer = nn.Linear(hidden_size, output_size) def forward(self, x): x = torch.relu(self.hidden_layer(x)) x = self.output_layer(x) return x # Create random tensors as input and output data x = torch.randn(100, input_size).to(device) y = torch.randn(100, output_size).to(device) # Initialize model, loss function and optimizer model = SimpleNN(input_size, hidden_size, output_size).to(device) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=learning_rate) # Training loop for epoch in range(num_epochs): # Forward pass outputs = model(x) loss = criterion(outputs, y) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() if (epoch + 1) % 10 == 0: print(f\'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\')"},{"question":"Objective: Implement and compare the `NearestNeighbors` algorithms (brute-force, KD-Tree, and Ball Tree) using scikit-learn. Evaluate their performance on a given dataset and analyze their scalability and efficiency. Problem Statement: Given a dataset, implement the following tasks: 1. Load the dataset and preprocess it if required. 2. Implement the `NearestNeighbors` method using each of the following algorithms: - Brute-force - KD-Tree - Ball Tree 3. For each implementation: - Fit the model using the dataset. - Find the `k-nearest neighbors` for a set of query points. - Report the time taken to fit the model and query the nearest neighbors. 4. Compare the performance of the three algorithms in terms of time efficiency and scalability. Dataset: You can use any standard dataset available in scikit-learn (like `iris`, `digits`, or `wine` datasets). You may also generate a synthetic dataset if necessary. Implementation Details: 1. **Input Format:** - A dataset with `n_samples` and `n_features`. - A set of query points for which neighbors need to be found. - The value of `k`, the number of nearest neighbors to find. 2. **Output Format:** - The indices of the nearest neighbors for the query points. - The distances to the nearest neighbors. - The time taken for fitting and querying for each algorithm. 3. **Constraints:** - You should handle cases where the number of neighbors is less than the number of samples. - Ensure that the implementation is efficient and does not run into memory issues for larger datasets. 4. **Performance Requirements:** - You should compare the time taken for fitting the model and querying the neighbors for each algorithm. - Provide analysis on which algorithm performs best for small, medium, and large datasets. Example: ```python from sklearn.neighbors import NearestNeighbors from sklearn.datasets import load_iris import numpy as np import time # Load dataset data = load_iris() X = data.data # Define query points query_points = np.array([[5.1, 3.5, 1.4, 0.2], [6.2, 3.4, 5.4, 2.3]]) # Define k k = 3 # Implement NearestNeighbors using different algorithms algorithms = [\'brute\', \'kd_tree\', \'ball_tree\'] results = {} for algo in algorithms: nbrs = NearestNeighbors(n_neighbors=k, algorithm=algo) start_time = time.time() nbrs.fit(X) fit_time = time.time() - start_time start_time = time.time() distances, indices = nbrs.kneighbors(query_points) query_time = time.time() - start_time results[algo] = { \'indices\': indices, \'distances\': distances, \'fit_time\': fit_time, \'query_time\': query_time } # Print results for algo, result in results.items(): print(f\\"Algorithm: {algo}\\") print(f\\"Indices of nearest neighbors: {result[\'indices\']}\\") print(f\\"Distances: {result[\'distances\']}\\") print(f\\"Time taken to fit: {result[\'fit_time\']} seconds\\") print(f\\"Time taken to query: {result[\'query_time\']} seconds\\") print(\\"-\\" * 40) ``` Analysis: - Discuss the time efficiency of each algorithm. - Analyze the scalability of the algorithms for varying dataset sizes. - Provide a conclusion on which algorithm is most suitable for different scenarios.","solution":"from sklearn.neighbors import NearestNeighbors from sklearn.datasets import load_iris, load_digits, load_wine import numpy as np import time def load_and_preprocess_data(dataset_name): if dataset_name == \'iris\': data = load_iris() elif dataset_name == \'digits\': data = load_digits() elif dataset_name == \'wine\': data = load_wine() else: raise ValueError(f\\"Dataset {dataset_name} not available.\\") return data.data def nearest_neighbors_performance(X, query_points, k=3): algorithms = [\'brute\', \'kd_tree\', \'ball_tree\'] results = {} for algo in algorithms: nbrs = NearestNeighbors(n_neighbors=k, algorithm=algo) start_time = time.time() nbrs.fit(X) fit_time = time.time() - start_time start_time = time.time() distances, indices = nbrs.kneighbors(query_points) query_time = time.time() - start_time results[algo] = { \'indices\': indices, \'distances\': distances, \'fit_time\': fit_time, \'query_time\': query_time } return results def main(): # Load dataset X = load_and_preprocess_data(\'iris\') # Define query points query_points = np.array([[5.1, 3.5, 1.4, 0.2], [6.2, 3.4, 5.4, 2.3]]) # Define k k = 3 results = nearest_neighbors_performance(X, query_points, k) # Print results for algo, result in results.items(): print(f\\"Algorithm: {algo}\\") print(f\\"Indices of nearest neighbors: {result[\'indices\']}\\") print(f\\"Distances: {result[\'distances\']}\\") print(f\\"Time taken to fit: {result[\'fit_time\']} seconds\\") print(f\\"Time taken to query: {result[\'query_time\']} seconds\\") print(\\"-\\" * 40) if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: To assess your understanding of the seaborn package, focusing on data manipulation, plotting, and text annotation. **Question**: You are given a dataset containing basic statistics for different machine learning models evaluated on various tasks. Your task is to load, transform, and visualize this data using seaborn. Follow the instructions below to complete the task. 1. **Loading and Transforming Data**: - Use the seaborn `load_dataset` function to load the `glue` dataset. - Transform the dataset by pivoting it so that model names and encoders are used as the index, task names become columns, and the values are the scores. Additionally, calculate the average score for each model and add it as an extra column named \'Average\'. - Sort the transformed dataset by the \'Average\' score in descending order. 2. **Creating a Plot**: - Create a bar plot using `seaborn.objects.Plot`, showing models on the y-axis and their average scores on the x-axis. Annotate each bar with the corresponding average score. - Add customization to change the text color of the annotations to white and align them to the right of the bars. - Further, create a scatter plot with two tasks of your choice on the x and y axes, and annotate each point with the model names. Color code the points by the encoder. 3. **Visualization Requirements**: - For the bar plot, ensure the average score is displayed within the bar, clearly visible. - For the scatter plot, align the text labels appropriately for readability, ensuring they do not overlap with the points. - Add a meaningful title and labels to both plots. Make sure the resulting plots are well-presented and clear. **Expected Input and Output**: - **Input**: No external input from user; the dataset \'glue\' is loaded programmatically. - **Output**: Two correctly rendered plots (one bar plot and one scatter plot) using seaborn. **Constraints and Tips**: - Ensure reproducibility with a consistent random seed where necessary. - Use the seaborn.objects API for this task. - Make sure to import all necessary libraries and functions. **Performance Requirements**: - The solution should load and manipulate the dataset efficiently and generate plots without error. **Example** (partial code to get started): ```python import seaborn.objects as so from seaborn import load_dataset # Load and transform the dataset glue = load_dataset(\'glue\') # Pivot and transform the dataset as required glue_transformed = ( glue.pivot(index=[\'Model\', \'Encoder\'], columns=\'Task\', values=\'Score\') .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\'Average\', ascending=False) ) # Create bar plot bar_plot = (so.Plot(glue_transformed, x=\'Average\', y=\'Model\', text=\'Average\') .add(so.Bar()) .add(so.Text(color=\'w\', halign=\'right\'))) # Create scatter plot scatter_plot = (so.Plot(glue_transformed, x=\'SST-2\', y=\'MRPC\', color=\'Encoder\', text=\'Model\') .add(so.Dot()) .add(so.Text(valign=\'bottom\'))) # Render plots bar_plot.show() scatter_plot.show() ``` *Note*: Finalize and fine-tune the code as per the detailed requirements above.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def load_and_transform_data(): # Load the dataset glue = sns.load_dataset(\'glue\') # Transform the dataset glue_transformed = ( glue.pivot(index=[\'Model\', \'Encoder\'], columns=\'Task\', values=\'Score\') .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\'Average\', ascending=False) ) return glue_transformed def create_plots(glue_transformed): # Create bar plot for Average scores per Model bar_plot = (so.Plot(glue_transformed.reset_index(), x=\'Average\', y=\'Model\', text=\'Average\') .layout(size=(10, len(glue_transformed) * 0.5)) .add(so.Bar()) .add(so.Text(color=\'w\', halign=\'right\')) .theme({\'axes.grid\': True, \'text.color\': \'black\', \'xtick.color\': \'black\', \'ytick.color\': \'black\'}) .label(title=\\"Average Score per Model\\", xlabel=\\"Average Score\\", ylabel=\\"Model\\") ) # Choose two tasks for the scatter plot task_x = \'SST-2\' task_y = \'MRPC\' # Create scatter plot for the chosen tasks scatter_plot = ( so.Plot(glue_transformed.reset_index(), x=task_x, y=task_y, color=\'Encoder\', text=\'Model\') .layout(size=(10, 10)) .add(so.Dot()) .add(so.Text(valign=\'bottom\')) .label(title=f\\"Scatter Plot of {task_x} vs {task_y}\\", xlabel=task_x, ylabel=task_y) .theme({\'axes.grid\': True}) ) # Show plots bar_plot.show() scatter_plot.show()"},{"question":"Objective Demonstrate your understanding of string operations and regular expressions in Python by writing a function to identify and process patterns within a text. Problem Statement You are tasked with creating a function that analyzes a block of text to identify, format, and summarize specific patterns. Specifically, the function should: 1. **Identify all email addresses** within the text. Email addresses consist of alphanumeric characters, dots (.), and underscores (_) before the @ symbol, followed by a domain. 2. **Extract all dates** in the formats `dd/mm/yyyy` or `dd-mm-yyyy`. 3. **List any phone numbers** that follow the format `(XXX) XXX-XXXX` or `XXX-XXX-XXXX`. Once identified, the function should return a summary with the counts of each pattern found and a formatted list of these matches. Input Format - A single string `text` which represents the block of text to be processed. Output Format - A dictionary with three keys: - `\\"emails\\"`: a list of all identified email addresses. - `\\"dates\\"`: a list of all extracted dates. - `\\"phone_numbers\\"`: a list of all identified phone numbers. - `\\"summary\\"`: a dictionary with the count of each pattern found (`\\"email_count\\"`, `\\"date_count\\"`, and `\\"phone_number_count\\"`). Function Signature ```python def analyze_text(text: str) -> dict: ``` Constraints - The input string could be very large (up to 1 million characters). - Performance should be considered; the function should aim to run within a reasonable time for large texts. Example ```python text = Hello, my name is John Doe. You can reach me at john.doe@example.com or jane_doe123@yahoo.co.uk. I was born on 01/01/1990. My friend was born on 23-12-1985. You can also call me at (123) 456-7890 or 987-654-3210. result = analyze_text(text) print(result) # Expected Output # { # \\"emails\\": [\\"john.doe@example.com\\", \\"jane_doe123@yahoo.co.uk\\"], # \\"dates\\": [\\"01/01/1990\\", \\"23-12-1985\\"], # \\"phone_numbers\\": [\\"(123) 456-7890\\", \\"987-654-3210\\"], # \\"summary\\": {\\"email_count\\": 2, \\"date_count\\": 2, \\"phone_number_count\\": 2} # } ``` You are advised to make use of the `re` module for regular expressions and try to optimize your regex patterns for better performance.","solution":"import re def analyze_text(text: str) -> dict: # Regex patterns for identifying email addresses, dates, and phone numbers email_pattern = re.compile(r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\') date_pattern = re.compile(r\'bd{2}[/-]d{2}[/-]d{4}b\') phone_pattern = re.compile(r\'(?d{3})?[ -]?d{3}[ -]?d{4}\') # Finding all matches based on the patterns emails = email_pattern.findall(text) dates = date_pattern.findall(text) phone_numbers = phone_pattern.findall(text) # Creating the summary information summary = { \'email_count\': len(emails), \'date_count\': len(dates), \'phone_number_count\': len(phone_numbers) } # Returning the result in the specified format result = { \\"emails\\": emails, \\"dates\\": dates, \\"phone_numbers\\": phone_numbers, \\"summary\\": summary } return result"},{"question":"# Objective Write a function that demonstrates your understanding of seaborn by visualizing different aspects of a dataset in one comprehensive plot. # Requirements 1. **Function Definition**: Define a function `visualize_titanic_data()` that does not take any arguments. 2. **Data Loading**: Load the Titanic dataset from seaborn using `sns.load_dataset(\\"titanic\\")`. 3. **Visualization**: Create a figure containing the following subplots: - A count plot showing the distribution of passengers in different classes. - A count plot showing the distribution of survivors and non-survivors within each class. - A count plot showing the distribution of survivors and non-survivors within each class, normalized to show percentages. 4. **Customization**: - Use appropriate titles and labels for each subplot. - Customize the color palette for the plots to improve aesthetics. - Ensure that all subplots are well-spaced and readable in a single figure. 5. **Output**: The function should display the resulting figure. # Constraints - Use seaborn and matplotlib only. - All visual elements (titles, labels, spacing, etc.) should be handled within your function. # Example Output The function should generate and display a matplotlib figure with three subplots, as specified above. ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_titanic_data(): # Load dataset titanic = sns.load_dataset(\\"titanic\\") # Plotting fig, axes = plt.subplots(3, 1, figsize=(15, 18)) sns.countplot(ax=axes[0], data=titanic, x=\\"class\\") axes[0].set_title(\'Count of passengers in each class\') axes[0].set_xlabel(\'Class\') axes[0].set_ylabel(\'Count\') sns.countplot(ax=axes[1], data=titanic, x=\\"class\\", hue=\\"survived\\") axes[1].set_title(\'Survivors vs Non-Survivors in each class\') axes[1].set_xlabel(\'Class\') axes[1].set_ylabel(\'Count\') sns.countplot(ax=axes[2], data=titanic, x=\\"class\\", hue=\\"survived\\", stat=\\"percent\\") axes[2].set_title(\'Survivors vs Non-Survivors in each class (Percentage)\') axes[2].set_xlabel(\'Class\') axes[2].set_ylabel(\'Percentage\') fig.tight_layout() plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_titanic_data(): # Load dataset titanic = sns.load_dataset(\\"titanic\\") # Create a figure and axis fig, axes = plt.subplots(3, 1, figsize=(15, 18)) # Subplot 1: Count of passengers in each class sns.countplot(ax=axes[0], data=titanic, x=\'class\', palette=\'viridis\') axes[0].set_title(\'Count of passengers in each class\') axes[0].set_xlabel(\'Class\') axes[0].set_ylabel(\'Count\') # Subplot 2: Count of survivors vs non-survivors in each class sns.countplot(ax=axes[1], data=titanic, x=\'class\', hue=\'survived\', palette=\'viridis\') axes[1].set_title(\'Survivors vs Non-Survivors in each class\') axes[1].set_xlabel(\'Class\') axes[1].set_ylabel(\'Count\') axes[1].legend(title=\'Survived\', labels=[\'No\', \'Yes\']) # Subplot 3: Percentage of survivors vs non-survivors in each class total_count = titanic.groupby(\'class\')[\'survived\'].count().reset_index(name=\'total\') survived_count = titanic.groupby([\'class\', \'survived\'])[\'survived\'].count().reset_index(name=\'count\') percentage = survived_count.merge(total_count, on=\'class\') percentage[\'percent\'] = percentage[\'count\'] / percentage[\'total\'] * 100 sns.barplot( ax=axes[2], data=percentage, x=\'class\', y=\'percent\', hue=\'survived\', palette=\'viridis\' ) axes[2].set_title(\'Survivors vs Non-Survivors in each class (Percentage)\') axes[2].set_xlabel(\'Class\') axes[2].set_ylabel(\'Percentage\') axes[2].legend(title=\'Survived\', labels=[\'No\', \'Yes\']) # Adjust layout plt.tight_layout() plt.show()"},{"question":"**Coding Question** In this assessment, you will demonstrate your understanding of the `torch.jit` module by working with TorchScript to create, optimize, and serialize a simple neural network model. # Task: 1. **Define a simple neural network**: Implement a class `SimpleNet` that inherits from `torch.nn.Module`. The network should have two fully connected layers with ReLU activations. 2. **Create a TorchScript model**: Implement a function `create_torchscript_model` that takes an instance of `SimpleNet` and an example input tensor. This function should: - Use `torch.jit.script` to create a scripted version of the model. - Use `torch.jit.trace` to create a traced version of the model. - Return both the scripted and traced models. 3. **Optimize the models**: Implement a function `optimize_model` that takes a TorchScript model (either scripted or traced) and performs any necessary optimization steps. 4. **Serialize and deserialize the model**: Implement functions to save and load the optimized TorchScript model to and from a file. # Expected Input and Output: - `create_torchscript_model(model, example_input)`: - **Input**: An instance of `SimpleNet` and a tensor `example_input` representing a sample input to the network. - **Output**: A tuple containing the scripted model and the traced model. - `optimize_model(torchscript_model)`: - **Input**: A TorchScript model (either scripted or traced). - **Output**: An optimized TorchScript model. - `save_model(optimized_model, file_path)`: - **Input**: An optimized TorchScript model and a string `file_path` representing the file path to save the model. - **Output**: None. The model should be saved to the specified file. - `load_model(file_path)`: - **Input**: A string `file_path` representing the file path to load the model from. - **Output**: The loaded TorchScript model. # Constraints or limitations: 1. The `SimpleNet` class should be simple but sufficient to demonstrate the necessary features. 2. The saved model file should be in a format that can be loaded back into PyTorch without any additional dependencies. 3. The entire process should handle any potential exceptions during saving and loading gracefully. # Performance Requirements: - Ensure the models are appropriately optimized for inference performance. - The saving and loading process should be efficient and not introduce significant overhead. # Example: ```python import torch import torch.nn as nn import torch.jit # Define the SimpleNet class class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 2) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Function to create TorchScript models def create_torchscript_model(model, example_input): scripted_model = torch.jit.script(model) traced_model = torch.jit.trace(model, example_input) return scripted_model, traced_model ``` Complete the remaining functions `optimize_model`, `save_model`, and `load_model` based on the description above.","solution":"import torch import torch.nn as nn import torch.jit # Define the SimpleNet class class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 2) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Function to create TorchScript models def create_torchscript_model(model, example_input): scripted_model = torch.jit.script(model) traced_model = torch.jit.trace(model, example_input) return scripted_model, traced_model # Function to optimize the model (no additional optimization needed for demo) def optimize_model(torchscript_model): return torchscript_model # Function to save the optimized model def save_model(optimized_model, file_path): torch.jit.save(optimized_model, file_path) # Function to load the model from file def load_model(file_path): return torch.jit.load(file_path)"},{"question":"# Advanced Base64 Encoding and Decoding Task **Objective:** You are tasked with writing a Python function that leverages the `base64` module to encode and decode data using both standard Base64 and URL-safe Base64 methods. Additionally, you must validate inputs to ensure they strictly adhere to provided specifications and handle errors gracefully. **Task:** 1. Implement a function `encode_and_decode_base64(data: bytes, altchars: bytes = None, url_safe: bool = False) -> dict` that: - Encodes the given `data` using Base64. - If `url_safe` is `True`, uses URL-safe characters for encoding. - If `altchars` is provided (a bytes-like object of length 2), uses these alternative characters for `+` and `/`. - Returns a dictionary with the following keys: - \'encoded\': Containing the standard or URL-safe Base64 encoded string. - \'decoded\': Containing the original data recovered from the encoded string. **Input:** - `data`: A bytes-like object to be encoded. - `altchars`: An optional bytes-like object of length 2 used as an alternative alphabet for `+` and `/`. Default is `None`. - `url_safe`: A boolean indicating whether to use URL-safe base64 encoding. Default is `False`. **Output:** - A dictionary with `encoded` and `decoded` fields, with `decoded` being the original data recovered from the encoded string. **Constraints:** - The `altchars` argument, if provided, must be a bytes-like object of exactly length 2, raising a `ValueError` otherwise. - The function must handle padding and correctly recover the original data. - Ensure to raise a `TypeError` if `data` is not a bytes-like object. **Example:** ```python data = b\'hello world\' result = encode_and_decode_base64(data, altchars=b\'-_\', url_safe=True) print(result) # Should output: # {\'encoded\': b\'aGVsbG8gd29ybGQ=\', \'decoded\': b\'hello world\'} data = b\'Python 3.10\' result = encode_and_decode_base64(data) print(result) # Should output: # {\'encoded\': b\'UHl0aG9uIDMuMTA=\', \'decoded\': b\'Python 3.10\'} ``` **Notes:** - Make use of `base64.b64encode` and `base64.b64decode` for standard encoding and decoding. - Make use of `base64.urlsafe_b64encode` and `base64.urlsafe_b64decode` for URL-safe encoding and decoding. - Ensure proper error handling and input validation. *Hints:* - Use the `TypeError` and `ValueError` exceptions for input validation. - Refer to the `base64` module documentation for detailed usage of encoding and decoding functions. Good luck!","solution":"import base64 def encode_and_decode_base64(data: bytes, altchars: bytes = None, url_safe: bool = False) -> dict: if not isinstance(data, bytes): raise TypeError(\\"data must be a bytes-like object\\") if altchars is not None and (not isinstance(altchars, bytes) or len(altchars) != 2): raise ValueError(\\"altchars must be a bytes-like object of length 2\\") if url_safe: encoded = base64.urlsafe_b64encode(data) else: encoded = base64.b64encode(data) if altchars: encoded = base64.b64encode(data, altchars=altchars) if url_safe: decoded = base64.urlsafe_b64decode(encoded) else: if altchars: decoded = base64.b64decode(encoded, altchars=altchars) else: decoded = base64.b64decode(encoded) return { \'encoded\': encoded, \'decoded\': decoded }"},{"question":"# Question: Implementing and Evaluating a PLSRegression Model **Objective**: You are required to implement a PLS regression model using scikit-learn\'s `PLSRegression` class. The task will assess your ability to preprocess data, fit a model, and evaluate its performance. **Problem Statement**: Given a dataset with multiple features (X) and targets (Y), implement a PLS regression model to predict the targets. Your task includes: 1. Loading the data 2. Preprocessing the data 3. Fitting a PLS regression model 4. Evaluating the model\'s performance **Steps**: 1. Load the dataset provided in the CSV file `data.csv`, which contains feature matrix `X` and target matrix `Y`. 2. Split the data into training and testing sets using an 80-20 split. 3. Standardize the feature matrix `X`. 4. Implement a `PLSRegression` model with `n_components=2`. 5. Fit the model to the training data. 6. Predict the targets for the testing data. 7. Calculate and return the Mean Squared Error (MSE) of the model\'s predictions on the testing data. **Input**: - A CSV file named `data.csv` with the following structure: - The first `p` columns represent the feature matrix `X`. - The last `q` columns represent the target matrix `Y`. **Output**: - The Mean Squared Error (MSE) of the model\'s predictions on the testing data. **Constraints**: - Use `PLSRegression` from scikit-learn. - The data should be standardized before fitting the model. - The random state for splitting the data should be set to 42. ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error def pls_regression(data_path): # Step 1: Load the dataset data = pd.read_csv(data_path) # Assuming the last column(s) are the target matrix `Y` X = data.iloc[:, :-q].values # Replace `q` with the actual number of target columns Y = data.iloc[:, -q:].values # Replace `q` with the actual number of target columns # Step 2: Split the data into training and testing sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Step 3: Standardize the feature matrix X scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Implement a PLSRegression model pls = PLSRegression(n_components=2) # Step 5: Fit the model to the training data pls.fit(X_train, Y_train) # Step 6: Predict the targets for the testing data Y_pred = pls.predict(X_test) # Step 7: Calculate the Mean Squared Error mse = mean_squared_error(Y_test, Y_pred) return mse # Example usage: # data_path = \'data.csv\' # print(pls_regression(data_path)) ``` **Note**: Replace `q` with the actual number of target columns in the dataset.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error def pls_regression(data_path, q): Implement and evaluate a PLS regression model. Parameters: data_path (str): Path to the CSV file containing the dataset. q (int): Number of target columns in the dataset. Returns: float: Mean Squared Error (MSE) of the model\'s predictions on the testing data. # Step 1: Load the dataset data = pd.read_csv(data_path) # Step 2: Split the feature matrix X and target matrix Y X = data.iloc[:, :-q].values Y = data.iloc[:, -q:].values # Step 3: Split the data into training and testing sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Step 4: Standardize the feature matrix X scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 5: Implement the PLSRegression model pls = PLSRegression(n_components=2) # Step 6: Fit the model to the training data pls.fit(X_train, Y_train) # Step 7: Predict the targets for the testing data Y_pred = pls.predict(X_test) # Step 8: Calculate the Mean Squared Error mse = mean_squared_error(Y_test, Y_pred) return mse"},{"question":"Coding Assessment Question # Objective Design and implement a script that loads a specific module from a ZIP archive at runtime using Python\'s `zipimport` module. # Task You are given a ZIP archive file named `modules.zip`, which contains several Python files and subdirectories. Your task is to implement a function that imports a specified module from this archive dynamically. The function should handle potential errors gracefully. # Function Signature ```python def import_module_from_zip(zip_path: str, module_name: str): pass ``` # Input - `zip_path` (str): The file path to the ZIP archive (`modules.zip`). - `module_name` (str): The fully qualified name of the module to import (e.g., \\"mypackage.mymodule\\"). # Output - None # Requirements 1. Your function should add the ZIP archive to the system path temporarily to make the module importable. 2. Use the `zipimport` module to load the specified module from the ZIP archive. 3. If the module is imported successfully, print the path where the module was imported from using the `__file__` attribute of the module. 4. Handle the `zipimport.ZipImportError` exception if it occurs and print an appropriate error message. 5. Ensure that the system path is restored to its original state after attempting the import. # Example ```python import_module_from_zip(\'modules.zip\', \'mypackage.mymodule\') ``` **Expected Output:** If the module is successfully imported: ``` Module mypackage.mymodule imported from modules.zip/mypackage/mymodule.py ``` If there\'s an error due to the ZIP file or module not being found: ``` Error: Could not import the module mypackage.mymodule ``` # Constraints - The ZIP file `modules.zip` should be in the same directory as your script for ease of testing. - Do not make permanent changes to the system path. # Notes - You may assume the ZIP archive and the required module names are valid and formatted correctly. - Focus on correctly using the `zipimport` module and handling the import process.","solution":"import sys import zipimport def import_module_from_zip(zip_path: str, module_name: str): Imports the specified module from the ZIP archive at runtime. Args: zip_path (str): The file path to the ZIP archive (\'modules.zip\'). module_name (str): The fully qualified name of the module to import (e.g., \'mypackage.mymodule\'). original_sys_path = sys.path.copy() try: # Add the ZIP archive to the system path. sys.path.append(zip_path) # Create a zip importer. importer = zipimport.zipimporter(zip_path) # Import the specified module using the zip importer. module = importer.load_module(module_name) # Print the path where the module was imported from. print(f\\"Module {module_name} imported from {module.__file__}\\") except zipimport.ZipImportError: print(f\\"Error: Could not import the module {module_name}\\") finally: # Restore the original system path. sys.path = original_sys_path"},{"question":"# Permutation Feature Importance Assessment **Objective:** Using your understanding of scikit-learn and the concept of permutation feature importance, you will demonstrate how to calculate and interpret the importance of features for a given regression model. **Task:** 1. Load the Diabetes dataset using `sklearn.datasets.load_diabetes`. 2. Split the data into training and testing sets. 3. Train a Ridge regression model on the training set. 4. Calculate the permutation feature importance on the testing set. 5. Identify and report the most important features along with their importance score and standard deviation. 6. Interpret the results by briefly explaining what the scores mean. **Details:** 1. **Load and Split Data:** - Load the Diabetes dataset. - Split it into training and testing sets using `train_test_split` from `sklearn.model_selection` with `random_state=0`. 2. **Train Model:** - Initialize and train a `Ridge` regression model with `alpha=0.01`. 3. **Calculate Permutation Feature Importance:** - Use `permutation_importance` from `sklearn.inspection` to compute the importance of each feature on the testing set. - Set `n_repeats=30` and `random_state=0`. 4. **Report Results:** - For each feature, print its name, mean importance score, and standard deviation, ordered by importance. 5. **Interpret Results:** - Briefly explain what the permutation importance results tell you about the feature\'s contribution to the model\'s predictions. **Input and Output Formats:** - Input: You do not need to worry about input formats for this exercise as the dataset is predefined. - Output: Print the feature importances in the specified format and provide a brief interpretation. **Constraints:** - You must use scikit-learn for this task. - Your solution should be efficient to handle the given dataset. **Example:** ```python from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance # Load data diabetes = load_diabetes() X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, random_state=0) # Train model model = Ridge(alpha=0.01).fit(X_train, y_train) # Calculate permutation feature importance perm_importance = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=0) # Report results for i in perm_importance.importances_mean.argsort()[::-1]: if perm_importance.importances_mean[i] - 2 * perm_importance.importances_std[i] > 0: print(f\\"{diabetes.feature_names[i]:<8} \\" f\\"{perm_importance.importances_mean[i]:.3f} \\" f\\"+/- {perm_importance.importances_std[i]:.3f}\\") # Interpret results print(\\"Interpretation:\\") print(\\"Higher importance mean values indicate greater impact on model\'s performance. Features with non-overlapping confidence intervals\\") print(\\"are considered significantly important. For example, if \'bmi\' has a high importance score, it significantly impacts the model predictions.\\") ``` **Note:** Use appropriate import statements and ensure that your solution handles any edge cases gracefully.","solution":"from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance def calculate_permutation_importance(): # Load data diabetes = load_diabetes() X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, random_state=0) # Train model model = Ridge(alpha=0.01).fit(X_train, y_train) # Calculate permutation feature importance perm_importance = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=0) # Report results feature_importances = [] for i in perm_importance.importances_mean.argsort()[::-1]: feature_importances.append({ \'feature\': diabetes.feature_names[i], \'importance_mean\': perm_importance.importances_mean[i], \'importance_std\': perm_importance.importances_std[i] }) if perm_importance.importances_mean[i] - 2 * perm_importance.importances_std[i] > 0: print(f\\"{diabetes.feature_names[i]:<8} \\" f\\"{perm_importance.importances_mean[i]:.3f} \\" f\\"+/- {perm_importance.importances_std[i]:.3f}\\") return feature_importances # Interpret results def interpret_results(): print(\\"Interpretation:\\") print(\\"Higher importance mean values indicate greater impact on model\'s performance. Features with non-overlapping confidence intervals\\") print(\\"are considered significantly important. For example, if \'bmi\' has a high importance score, it significantly impacts the model predictions.\\")"},{"question":"You are a data analyst working with a CSV file containing information about sales transactions. Your task is to read the data, clean it, and extract valuable insights. # Question: 1. **Create and Clean the DataFrame:** - Read the CSV file into a pandas DataFrame. Assume the CSV file has columns: `Date`, `Product_ID`, `Product_Category`, `Quantity_Sold`, and `Revenue`. - Fill any missing values in `Quantity_Sold` with the median of non-missing values in the same column. - Replace missing values in `Revenue` with the product of `Quantity_Sold` and the mean unit price of that `Product_Category`. (Assume mean unit price is derived from non-missing revenues in that category) 2. **Data Exploration:** - Create a new column `Revenue_Category` that categorizes revenue as \'Low\' for values less than 100, \'Medium\' for values between 100 and 500, and \'High\' for values above 500. - Select and display the rows where the `Revenue_Category` is \'High\'. 3. **Aggregation and Grouping:** - Group the data by `Product_Category` and calculate the total `Revenue` and average `Quantity_Sold` for each category. - Identify `Product_Category` with the highest total revenue. 4. **Date-based Analysis:** - Convert the `Date` column to datetime format. - Create a new column `Month` that extracts the month from the `Date`. - For each month, calculate the total `Quantity_Sold` and total `Revenue`. 5. **Multi-Index Operations:** - Set a MultiIndex on the DataFrame using `Product_Category` and `Product_ID`. - Retrieve all data for a specific `Product_Category` and `Product_ID` using the MultiIndex. # Input Formats: - The CSV file `sales_data.csv` should be read from the current directory. # Output Formats: - Print the cleaned DataFrame after filling missing values. - Print the DataFrame filtered by \'High\' `Revenue_Category`. - Print the total `Revenue` and average `Quantity_Sold` per `Product_Category`. - Print the `Product_Category` with the highest total revenue. - Print the total `Quantity_Sold` and total `Revenue` per month. - Print the MultiIndex DataFrame and the retrieved data for a specific `Product_Category` and `Product_ID`. # Constraints: - The CSV file will have at least 100 rows. - Ensure your operations are efficient, particularly those involving data filling and group-by operations. # Performance Requirements: - Operations should handle the DataFrame efficiently within a reasonable time frame (few seconds) even for large datasets. You need to submit a Python script that performs the above steps using pandas.","solution":"import pandas as pd # 1. Create and Clean the DataFrame def read_and_clean_csv(file_path): df = pd.read_csv(file_path) # Fill missing Quantity_Sold with the median quantity_median = df[\'Quantity_Sold\'].median() df[\'Quantity_Sold\'].fillna(quantity_median, inplace=True) # Compute mean unit price for each Product_Category df[\'Unit_Price\'] = df[\'Revenue\'] / df[\'Quantity_Sold\'] mean_unit_price = df.groupby(\'Product_Category\')[\'Unit_Price\'].transform(\'mean\') # Fill missing Revenue df[\'Revenue\'] = df.apply(lambda row: row[\'Quantity_Sold\'] * mean_unit_price[row.name] if pd.isna(row[\'Revenue\']) else row[\'Revenue\'], axis=1) return df # 2. Data Exploration def categorize_revenue(df): bins = [0, 100, 500, float(\'inf\')] labels = [\'Low\', \'Medium\', \'High\'] df[\'Revenue_Category\'] = pd.cut(df[\'Revenue\'], bins=bins, labels=labels) return df[df[\'Revenue_Category\'] == \'High\'] # 3. Aggregation and Grouping def aggregation_and_grouping(df): group = df.groupby(\'Product_Category\').agg({\'Revenue\': \'sum\', \'Quantity_Sold\': \'mean\'}).reset_index() highest_revenue_category = group.loc[group[\'Revenue\'].idxmax()][\'Product_Category\'] return group, highest_revenue_category # 4. Date-based Analysis def date_based_analysis(df): df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') month_aggregation = df.groupby(\'Month\').agg({\'Quantity_Sold\': \'sum\', \'Revenue\': \'sum\'}).reset_index() return month_aggregation # 5. Multi-Index Operations def multi_index_operations(df): multi_index_df = df.set_index([\'Product_Category\', \'Product_ID\']) return multi_index_df def retrieve_data_by_multi_index(multi_index_df, category, product_id): return multi_index_df.loc[(category, product_id)] # Example usage: # df = read_and_clean_csv(\'sales_data.csv\') # print(df) # high_revenue_df = categorize_revenue(df) # print(high_revenue_df) # aggregation, highest_rev_cat = aggregation_and_grouping(df) # print(aggregation) # print(f\'Highest Revenue Category: {highest_rev_cat}\') # monthly_aggregation = date_based_analysis(df) # print(monthly_aggregation) # multi_index_df = multi_index_operations(df) # print(multi_index_df) # specific_data = retrieve_data_by_multi_index(multi_index_df, \'Category_A\', 123) # print(specific_data)"},{"question":"Managing and Profiling GPU Memory with PyTorch MPS Objective The objective of this question is to assess your understanding of the PyTorch MPS module, focusing on GPU memory management and performance profiling. Task Implement a function that performs the following: 1. Allocates a large tensor on the GPU. 2. Seeds the random number generator for reproducibility. 3. Profiles the allocation and simple operations performed on the tensor. 4. Retrieves and prints memory usage statistics. 5. Cleans up the allocated memory. Function Signature ```python def manage_and_profile_mps_memory(tensor_shape: tuple, seed_value: int) -> dict: Manages GPU memory, profiles operations, and returns profiling and memory statistics. Args: tensor_shape (tuple): The shape of the tensor to allocate. seed_value (int): The value to seed the random number generator. Returns: dict: A dictionary with profiling and memory statistics. ``` Details 1. **Tensor Allocation**: Allocate a tensor of shape `tensor_shape` filled with random values using `torch.randn` on the MPS device. 2. **Seeding**: Set the random number generator seed using `manual_seed` and `seed`. 3. **Profiling**: Use `profiler.start`, perform a simple computation on the tensor (e.g., element-wise multiplication), and then stop the profiler using `profiler.stop`. 4. **Memory Statistics**: Retrieve the current allocated memory (using `current_allocated_memory`) and driver allocated memory (using `driver_allocated_memory`). 5. **Cleanup**: Free up the GPU memory using `empty_cache`. Example Usage ```python stats = manage_and_profile_mps_memory((1024, 1024), 42) print(stats) ``` Constraints - Assume the code will be executed on a macOS device with MPS support. Expected Output The function should return a dictionary with keys: - `\\"profiling_info\\"`: Information about the profiling session. - `\\"current_allocated_memory\\"`: The current GPU memory allocated after the operation. - `\\"driver_allocated_memory\\"`: The memory allocated by the GPU driver.","solution":"import torch import torch.mps import torch.profiler as profiler def manage_and_profile_mps_memory(tensor_shape: tuple, seed_value: int) -> dict: Manages GPU memory, profiles operations, and returns profiling and memory statistics. Args: tensor_shape (tuple): The shape of the tensor to allocate. seed_value (int): The value to seed the random number generator. Returns: dict: A dictionary with profiling and memory statistics. # Ensure MPS is available if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS is not available on this device\\") mps_device = torch.device(\\"mps\\") # Seed the random number generator torch.manual_seed(seed_value) # Allocate a large tensor on the GPU tensor = torch.randn(tensor_shape, device=mps_device) # Start profiling prof = profiler.profile(with_stack=True, on_trace_ready=profiler.tensorboard_trace_handler(\'./log\')) prof.start() # Perform a simple operation result = tensor * tensor prof.stop() # Retrieve memory statistics current_allocated_memory = torch.mps.current_allocated_memory() driver_allocated_memory = torch.mps.driver_allocated_memory() # Clean up the allocated memory torch.mps.empty_cache() return { \\"profiling_info\\": prof.key_averages().table(sort_by=\\"self_cpu_time_total\\"), \\"current_allocated_memory\\": current_allocated_memory, \\"driver_allocated_memory\\": driver_allocated_memory }"},{"question":"# I/O Multiplexing using `select` Module Problem Statement You are tasked with designing a simple TCP server that can handle multiple client connections simultaneously using the `select` module. The server should be able to handle basic text messaging between clients. When a client sends a message to the server, the server should broadcast the message to all connected clients except the sender. Requirements - Implement a TCP server that listens on a specified port. - Use the `select.select()` function to monitor multiple client sockets for incoming data. - Broadcast received messages to all connected clients excluding the sender. Function Signature ```python def start_server(host: str, port: int) -> None: pass ``` Input - `host`: A string indicating the hostname or IP address of the server (e.g., \'localhost\' or \'0.0.0.0\'). - `port`: An integer indicating the port number on which the server will listen for incoming connections. Output - The function should not return any value. It should run indefinitely, handling client connections and broadcasting messages. Constraints - Use the `select.select()` function for handling multiple client connections. - Handle proper socket closure and cleanup when a client disconnects. - The server should not block on any single client; it must remain responsive to all connected clients. Example ```python import socket import select def start_server(host: str, port: int) -> None: # Your implementation goes here pass # Example: To run the server, use the following line # start_server(\'0.0.0.0\', 12345) ``` Notes 1. You may use the Python `socket` module to create and manage sockets. 2. Ensure proper exception handling to manage client disconnections and potential errors. 3. Consider edge cases, such as clients disconnecting abruptly. 4. Make sure the server can handle multiple simultaneous clients efficiently. This task assesses your understanding of the `select` module, socket programming, and effective handling of multiple I/O streams concurrently.","solution":"import socket import select def start_server(host: str, port: int) -> None: server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((host, port)) server_socket.listen() print(f\\"Server started on {host}:{port}\\") sockets_list = [server_socket] clients = {} def broadcast(message, exclude_socket): for sock in clients: if sock != exclude_socket: try: sock.send(message) except: sock.close() sockets_list.remove(sock) del clients[sock] while True: read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list) for notified_socket in read_sockets: if notified_socket == server_socket: client_socket, client_address = server_socket.accept() sockets_list.append(client_socket) clients[client_socket] = client_address print(f\\"Accepted new connection from {client_address[0]}:{client_address[1]}\\") else: try: message = notified_socket.recv(1024) if not message: print(f\\"Closed connection from {clients[notified_socket]}\\") sockets_list.remove(notified_socket) del clients[notified_socket] continue print(f\\"Received message from {clients[notified_socket]}: {message.decode(\'utf-8\')}\\") broadcast(message, notified_socket) except: print(f\\"Connection closed from {clients[notified_socket]}\\") sockets_list.remove(notified_socket) del clients[notified_socket] continue for notified_socket in exception_sockets: sockets_list.remove(notified_socket) del clients[notified_socket] notified_socket.close() # Uncomment the following line to run the server: # start_server(\'0.0.0.0\', 12345)"},{"question":"**Objective:** Implement a function to read an existing Sun AU audio file, apply a transformation to the audio data, and write the transformed data to a new Sun AU file. **Function Signature:** ```python def transform_audio(input_file: str, output_file: str) -> None: pass ``` **Detailed Requirements:** 1. **Input:** - `input_file` (str): The path to the input Sun AU file. - `output_file` (str): The path where the transformed Sun AU file will be written. 2. **Output:** - None (the function writes the output to the specified file). 3. **Constraints:** - Your function should read the entire header and audio data from the input file. - The output file must have the same number of channels, sample width, and frame rate as the input file. - Apply a transformation to the audio data: - Invert the audio signal (for each sample, calculate its complement). - Write the transformed audio data to the output file. 4. **Performance Requirements:** - The solution should handle large audio files efficiently. - Proper resource management (file handles must be properly closed). 5. **Error Handling:** - Raise an appropriate exception if the input file cannot be read or if there are issues writing the output file. **Example:** ```python transform_audio(\'input.au\', \'output.au\') ``` This will read the \'input.au\' file, transform the audio data by inverting the signal, and write the result to \'output.au\'. **Note:** - You may assume the input file is a valid Sun AU file. - Utilize the built-in `sunau` module to handle file reading and writing.","solution":"import sunau def transform_audio(input_file: str, output_file: str) -> None: Reads an existing Sun AU audio file, applies a transformation to the audio data, and writes the transformed data to a new Sun AU file. Parameters: input_file (str): The path to the input Sun AU file. output_file (str): The path where the transformed Sun AU file will be written. try: # Read the input file with sunau.open(input_file, \'rb\') as reader: channels = reader.getnchannels() sampwidth = reader.getsampwidth() framerate = reader.getframerate() nframes = reader.getnframes() comp_type = reader.getcomptype() comp_name = reader.getcompname() # Read frames audio_frames = reader.readframes(nframes) # Transform the audio data by inverting the signal transformed_frames = bytearray() for byte in audio_frames: transformed_frames.append(255 - byte) # Write to the output file with sunau.open(output_file, \'wb\') as writer: writer.setnchannels(channels) writer.setsampwidth(sampwidth) writer.setframerate(framerate) writer.setnframes(nframes) writer.setcomptype(comp_type, comp_name) writer.writeframes(transformed_frames) except Exception as e: raise RuntimeError(f\\"Failed to transform audio: {e}\\")"},{"question":"**Objective:** To assess students\' understanding of regular expressions in Python, focusing on group capturing, lookahead assertions, and the use of various pattern methods. **Problem Statement:** You are given a text document containing numerous references to bibliographic entries spread across multiple lines. Each entry follows a specific format: ``` [id] Author Name, \\"Title of the Paper\\", Journal Name, Year ``` For example: ``` [1] John Doe, \\"Understanding AI\\", Journal of AI, 2020 [2] Jane Smith, \\"Exploring Machine Learning\\", Machine Learning Today, 2019 ... ``` Your task is to write a function `extract_bibliographic_entries(text: str) -> list` that extracts all such entries from the provided text and returns them as a list of dictionaries with the following keys: `id`, `author`, `title`, `journal`, and `year`. The function should ignore any malformed bibliographic entries that do not follow the exact format. **Function Signature:** ```python def extract_bibliographic_entries(text: str) -> list: ``` **Input:** - `text` (str): A string containing the entire text document. **Output:** - `list`: A list of dictionaries, each containing the extracted bibliographic information. **Examples:** Input: ```text [1] John Doe, \\"Understanding AI\\", Journal of AI, 2020 Some irrelevant text. [2] Jane Smith, \\"Exploring Machine Learning\\", Machine Learning Today, 2019 Another line of text. [3] Alan Turing, \\"Computing Machinery and Intelligence\\", Mind, 1950 ``` Output: ```python [ {\\"id\\": 1, \\"author\\": \\"John Doe\\", \\"title\\": \\"Understanding AI\\", \\"journal\\": \\"Journal of AI\\", \\"year\\": 2020}, {\\"id\\": 2, \\"author\\": \\"Jane Smith\\", \\"title\\": \\"Exploring Machine Learning\\", \\"journal\\": \\"Machine Learning Today\\", \\"year\\": 2019}, {\\"id\\": 3, \\"author\\": \\"Alan Turing\\", \\"title\\": \\"Computing Machinery and Intelligence\\", \\"journal\\": \\"Mind\\", \\"year\\": 1950} ] ``` **Constraints:** - Bibliographic entries are always on separate lines. - The year should be a four-digit number ranging from 1000 to 2999. - Use regular expressions to perform the extraction. **Hints:** - Use named groups in your regular expression for capturing different parts of the entry. - Consider using the `re.VERBOSE` flag to write a readable regular expression. **Assessment Criteria:** 1. Correctness: The function should correctly parse and return all valid bibliographic entries in the specified format. 2. Robustness: The function should ignore malformed entries. 3. Efficiency: The function should be efficient even with large input texts. 4. Code Clarity: The function should be well-documented, and the regular expression should be clean and easy to understand.","solution":"import re def extract_bibliographic_entries(text: str) -> list: Extracts bibliographic entries from the provided text document. Args: text (str): A string containing the entire text document with bibliographic entries. Returns: list: A list of dictionaries, each containing \'id\', \'author\', \'title\', \'journal\', and \'year\' of a bibliographic entry. pattern = re.compile(r [ (?P<id>d+) ] s* (?P<author>.+?) , s*\\" (?P<title>.+?) \\", s* (?P<journal>.+?) , s* (?P<year>(?:1|2)d{3}) , re.VERBOSE) matches = pattern.finditer(text) entries = [] for match in matches: entry = { \'id\': int(match.group(\'id\')), \'author\': match.group(\'author\'), \'title\': match.group(\'title\'), \'journal\': match.group(\'journal\'), \'year\': int(match.group(\'year\')), } entries.append(entry) return entries example_text = \'\'\'[1] John Doe, \\"Understanding AI\\", Journal of AI, 2020 Some irrelevant text. [2] Jane Smith, \\"Exploring Machine Learning\\", Machine Learning Today, 2019 Another line of text. [3] Alan Turing, \\"Computing Machinery and Intelligence\\", Mind, 1950 \'\'\' # Example print(extract_bibliographic_entries(example_text))"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s scatter plot functionalities through a comprehensive coding task. **Task:** 1. Load the \\"tips\\" dataset using seaborn. 2. Create a scatter plot using the following specifications: - The x-axis should represent the `total_bill` data. - The y-axis should represent the `tip` data. - Add a hue to the data points representing the `time` variable (Lunch or Dinner). - Use different markers to differentiate between `time`. - Add another layer of differentiation on the data points using the `size` variable. 3. Further, create a facet grid of the scatter plots: - Separate the plots into different columns based on the `time` variable. - Within each column, use different hues for the `day` variable (Days of the week). - Ensure each subplot correctly synchronizes semantic mappings across facets. **Constraints:** - You should use seaborn and matplotlib libraries for plotting. - Your code should cover all the steps mentioned above. - Ensure your code is efficient and readable. **Input:** The input is provided by seaborn\'s `sns.load_dataset(\\"tips\\")` function. No additional input is required. **Output:** Your code should display the scatter plot with all the specified customizations and the facet grid. **Example Solution:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"time\\", size=\\"size\\") plt.title(\\"Scatter plot of tips vs. total_bill with hue by time and size by party size\\") plt.show() # Create the facet grid sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_scatter_plot(): Create and display a scatter plot with specified customizations. # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"time\\", size=\\"size\\") plt.title(\\"Scatter plot of tips vs. total_bill with hue by time and size by party size\\") plt.legend(bbox_to_anchor=(1.05, 1), loc=2) plt.show() # Create the facet grid g = sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\") g.set_titles(\\"{col_name} Time\\") plt.show() # call the function to execute and display the plots create_scatter_plot()"},{"question":"# Password Hashing and Validation **Objective**: Implement a secure password hashing and validation system using the `hashlib` module in Python. # Problem Statement You are required to develop two functions `hash_password` and `validate_password` that utilizes the `hashlib` module to hash passwords and validate them. 1. **Function 1: hash_password** - **Input**: Takes a plain-text password as a string and a salt as a string. - **Output**: Returns the hexadecimal digest of the hashed password concatenated with the salt. - **Hash Algorithm**: You have to use the SHA-256 algorithm for hashing the password. 2. **Function 2: validate_password** - **Input**: Takes three strings: plain-text password, salt, and the hashed password. - **Output**: Returns a boolean value: `True` if the provided plain-text password concatenated with the provided salt, when hashed, matches the hashed password; otherwise, `False`. # Constraints: - The password can have any characters and its length will be between 8 to 64 characters. - The salt will be a string of exactly 16 characters. - The output hash must be a hexadecimal string of length 64 characters. # Example: ```python def hash_password(password: str, salt: str) -> str: # Implementation here def validate_password(password: str, salt: str, hashed: str) -> bool: # Implementation here # Example usage: plain_password = \\"SuperSecurePassword123!\\" salt = \\"16CharSaltString\\" hashed_password = hash_password(plain_password, salt) print(hashed_password) # Outputs the hexadecimal hash # Validate the password print(validate_password(\\"SuperSecurePassword123!\\", \\"16CharSaltString\\", hashed_password)) # True print(validate_password(\\"WrongPassword123!\\", \\"16CharSaltString\\", hashed_password)) # False ``` # Notes: - Use the `hashlib` module, particularly the `sha256` function, to create hashes. - Make sure to concatenate the salt to the password before hashing. - Handle potential edge cases such as invalid inputs or mismatched hashes. # Additional Requirements: - Your solution should not use any external libraries or modules beyond Python\'s standard library. - Consider function performance and aim to write clean, readable, and efficient code. Good luck!","solution":"import hashlib def hash_password(password: str, salt: str) -> str: Hashes the given password concatenated with the salt using SHA-256. Args: password (str): The plain-text password. salt (str): The salt string. Returns: str: The hexadecimal digest of the hashed password concatenated with the salt. # Ensure the salt length is 16 characters if len(salt) != 16: raise ValueError(\\"Salt must be exactly 16 characters long\\") hashed = hashlib.sha256((password + salt).encode()).hexdigest() return hashed def validate_password(password: str, salt: str, hashed: str) -> bool: Validates the given password with the provided salt against the hashed password. Args: password (str): The plain-text password. salt (str): The salt string. hashed (str): The previously hashed password. Returns: bool: True if the hash of the provided password and salt matches the stored hashed password, False otherwise. # Ensure the salt length is 16 characters if len(salt) != 16: raise ValueError(\\"Salt must be exactly 16 characters long\\") return hash_password(password, salt) == hashed"},{"question":"Using the `spwd` module, write a function `users_password_warning(days_left: int) -> List[str]` that returns a list of login names for users who should be warned about their password expiry. The function should accept an integer input `days_left` which specifies the number of days left before the password expiry, and return a list of user login names (`sp_namp`) whose passwords will expire in `days_left` days or fewer. # Function Signature ```python from typing import List def users_password_warning(days_left: int) -> List[str]: # Your code here ``` # Input - `days_left` (int): The threshold number of days before password expiry to filter users. # Output - `List[str]`: A list of login names of users who should be warned about their password expiry. # Constraints - You must handle potential `PermissionError` exceptions that occur if the user running the script does not have the necessary privileges to access the shadow password database. - The provided days should be a non-negative integer. - The function should work efficiently for a large number of users. # Example ```python # Assuming there are users with the following entries in the shadow password database: # User A: sp_namp=\'userA\', sp_warn=10 # User B: sp_namp=\'userB\', sp_warn=5 # User C: sp_namp=\'userC\', sp_warn=3 # Example usage: print(users_password_warning(5)) # Output: [\'userB\', \'userC\'] ``` Note: This example assumes hypothetical user data consistent with the structure specified in the documentation. **Please make sure that your function handles cases where the `spwd` module functions could raise exceptions due to lack of privileges or other unexpected situations.**","solution":"from typing import List import spwd def users_password_warning(days_left: int) -> List[str]: Returns a list of login names for users who should be warned about their password expiry. :param days_left: The threshold number of days before password expiry to filter users. :return: List of login names. if days_left < 0: raise ValueError(\\"days_left must be a non-negative integer\\") try: shadow_entries = spwd.getspall() except PermissionError: raise PermissionError(\\"Permission denied: you don\'t have access to the shadow password database.\\") warning_users = [entry.sp_namp for entry in shadow_entries if entry.sp_warn != -1 and entry.sp_warn <= days_left] return warning_users"},{"question":"**Coding Assessment Question: Advanced File Handling in Python** **Objective**: Implement a utility to interact with file descriptors and perform advanced file handling operations. **Problem Statement**: You are tasked to create a utility class `AdvancedFileHandler` in Python. This class should mimic some of the functionalities provided by the low-level C APIs described in the documentation, but using Python\'s high-level `io` module. # Class: `AdvancedFileHandler` Methods: 1. **__init__(self, file_path: str, mode: str, buffering: int = -1, encoding: Optional[str] = None, errors: Optional[str] = None, newline: Optional[str] = None, closefd: bool = True)** - Initializes the handler by opening the file with the given parameters. - Parameters: - `file_path` (str): Path to the file. - `mode` (str): Mode in which the file is to be opened (e.g., \'r\', \'w\', \'rb\'). - `buffering` (int): Buffering policy (-1 to use the default). - `encoding` (Optional[str]): Name of the encoding used to decode or encode the file. - `errors` (Optional[str]): Specifies how encoding and decoding errors are to be handled. - `newline` (Optional[str]): Controls how universal newlines mode works. - `closefd` (bool): If False, the underlying file descriptor will be kept open when the file is closed. 2. **read_line(self, max_bytes: int = -1) -> str** - Reads one line from the file. If `max_bytes` is given and is greater than 0, reads up to `max_bytes` bytes. - Returns: A string representing the line read from the file. 3. **write_object(self, obj: Any) -> None** - Writes the string representation of the given object to the file. - Parameters: - `obj` (Any): The object to be written to the file. If the object has a custom `__str__` method, its output will be written. 4. **write_string(self, s: str) -> None** - Writes the given string to the file. - Parameters: - `s` (str): String to be written to the file. 5. **get_file_descriptor(self) -> int** - Returns the file descriptor associated with the file object. 6. **close(self) -> None** - Closes the file. # Constraints: - The file should be opened using Python\'s built-in `open` function. - The implementation must handle exceptions gracefully and ensure that resources are properly managed (i.e., files are closed appropriately). # Example Usage: ```python # Create an instance of AdvancedFileHandler handler = AdvancedFileHandler(\\"/path/to/file.txt\\", \\"w\\", encoding=\\"utf-8\\") # Write an object to the file handler.write_object({\\"key\\": \\"value\\"}) # Write a string to the file handler.write_string(\\"Hello, World!n\\") # Read a line from the file line = handler.read_line() # Get the file descriptor fd = handler.get_file_descriptor() # Close the file handler.close() ``` # Performance Requirements: - Your implementation should efficiently handle large files without unnecessary memory consumption. # Submission: - Implement the `AdvancedFileHandler` class and ensure that it handles the specified functionalities.","solution":"import io from typing import Optional, Any class AdvancedFileHandler: def __init__(self, file_path: str, mode: str, buffering: int = -1, encoding: Optional[str] = None, errors: Optional[str] = None, newline: Optional[str] = None, closefd: bool = True): self.file = open(file_path, mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline, closefd=closefd) def read_line(self, max_bytes: int = -1) -> str: if max_bytes > 0: return self.file.readline(max_bytes) else: return self.file.readline() def write_object(self, obj: Any) -> None: self.file.write(str(obj)) def write_string(self, s: str) -> None: self.file.write(s) def get_file_descriptor(self) -> int: if self.file: return self.file.fileno() else: raise ValueError(\\"File is not open\\") def close(self) -> None: if self.file: self.file.close()"},{"question":"You are provided with a dataset containing user profiles, and you need to clean and analyze it using pandas\' nullable integer functionality. Implement the following functions: 1. **create_nullable_int_series(data: List[Union[int, None]]) -> pd.Series:** - **Input:** A list of integers which may include `None` values. - **Output:** A pandas Series with a nullable integer (`Int64`) data type. 2. **add_age_column(df: pd.DataFrame) -> pd.DataFrame:** - **Input:** A DataFrame containing a \'birth_year\' column. - **Output:** The same DataFrame with an added \'age\' column calculated based on the current year (assume current year is 2023) where missing values and invalid years are represented as `pd.NA`. 3. **perform_operations(s: pd.Series) -> Dict[str, Any]:** - **Input:** A pandas Series with nullable integers. - **Output:** A dictionary containing: - Sum of the series. - Mean of the series. - Series after adding 5 to each element. - Boolean series indicating which elements are greater than 10. - Results should correctly handle `pd.NA` values. # Constraints - You must use `pd.Int64Dtype()` for creating nullable integer Series. - Ensure calculations propagate `pd.NA` values correctly. # Example ```python data = [20, None, 35, 40] series = create_nullable_int_series(data) print(series) # Output: # 0 20 # 1 <NA> # 2 35 # 3 40 # dtype: Int64 df = pd.DataFrame({\\"birth_year\\": [1980, None, 1990, 2000]}) df = add_age_column(df) print(df) # Output: # birth_year age # 0 1980.0 43.0 # 1 NaN <NA> # 2 1990.0 33.0 # 3 2000.0 23.0 results = perform_operations(series) # Output: # { # \\"sum\\": 95, # \\"mean\\": 31.666666666666668, # \\"add_five\\": [25, <NA>, 40, 45], # \\"greater_than_ten\\": [True, <NA>, True, True] # } ```","solution":"import pandas as pd from typing import List, Union, Dict, Any def create_nullable_int_series(data: List[Union[int, None]]) -> pd.Series: Converts a list of integers which may include None values to a pandas Series with a nullable integer data type. return pd.Series(data, dtype=pd.Int64Dtype()) def add_age_column(df: pd.DataFrame) -> pd.DataFrame: Adds an \'age\' column to the DataFrame based on the current year (assume 2023), with missing values and invalid years handled as pd.NA. current_year = 2023 def calculate_age(year): if pd.isna(year) or year > current_year: return pd.NA return current_year - year df[\'age\'] = df[\'birth_year\'].apply(calculate_age) return df def perform_operations(s: pd.Series) -> Dict[str, Any]: Performs various operations on the Series such as sum, mean, adding 5 to each element, and creating a boolean series indicating which elements are greater than 10. result = { \\"sum\\": s.sum(skipna=True), \\"mean\\": s.mean(skipna=True), \\"add_five\\": s + 5, \\"greater_than_ten\\": s > 10 } return result"},{"question":"# Coding Assessment: Advanced Garbage Collection in Python Objective: You are required to demonstrate your understanding of Python\'s garbage collector by implementing a function that performs a series of operations to monitor and control memory usage in a Python application. Problem Statement: Implement a function `manage_garbage_collection` that performs the following tasks: 1. **Disable automatic garbage collection** using `gc.disable()`. 2. **Set debugging flags** to track collectable and uncollectable objects using `gc.set_debug()` with `gc.DEBUG_COLLECTABLE | gc.DEBUG_UNCOLLECTABLE`. 3. **Run a full garbage collection** (collect generation 2) using `gc.collect()`. 4. **Retrieve and print statistics** for each generation using `gc.get_stats()`. Print these statistics in a readable format. 5. **Freeze all objects currently tracked by the garbage collector** using `gc.freeze()`. 6. **Run another garbage collection** (collect generation 2) after freezing and print the number of unreachable objects found. 7. **Unfreeze the objects** using `gc.unfreeze()`. 8. **Enable automatic garbage collection** using `gc.enable()`. Function Signature: ```python def manage_garbage_collection(): pass ``` Example Output: ``` Garbage collection statistics (before freezing): Generation 0: {\'collections\': 10, \'collected\': 1000, \'uncollectable\': 10} Generation 1: {\'collections\': 5, \'collected\': 500, \'uncollectable\': 5} Generation 2: {\'collections\': 1, \'collected\': 200, \'uncollectable\': 2} Unreachable objects found after second collection: 0 Automatic garbage collection re-enabled. ``` Constraints: 1. You must handle and format the statistics in a readable format. 2. Ensure proper encapsulation of the operations within the function. 3. You can use `print` for displaying outputs as required. Notes: - The function does not take any input and does not return any values. - Focus on utilizing the `gc` module\'s functionalities as described to achieve the tasks. - Consider exceptional cases, such as ensuring that the garbage collector is always re-enabled by the end of function execution.","solution":"import gc def manage_garbage_collection(): try: # Disable automatic garbage collection gc.disable() # Set debugging flags gc.set_debug(gc.DEBUG_COLLECTABLE | gc.DEBUG_UNCOLLECTABLE) # Run a full garbage collection gc.collect() # Retrieve and print statistics for each generation stats = gc.get_stats() print(\\"Garbage collection statistics (before freezing):\\") for gen, stat in enumerate(stats): print(f\\"Generation {gen}: {stat}\\") # Freeze all objects currently tracked by the garbage collector gc.freeze() # Run another garbage collection and print the number of unreachable objects found unreachable_objects = gc.collect() print(f\\"Unreachable objects found after second collection: {unreachable_objects}\\") finally: # Unfreeze the objects gc.unfreeze() # Enable automatic garbage collection gc.enable() print(\\"Automatic garbage collection re-enabled.\\")"},{"question":"You are required to implement a Python function that validates a given snippet of Python code for correct lexical structure and format according to the provided lexical rules. The function should be capable of handling comments, indentation, line joining, string literals, and various other elements as detailed in the documentation. Your task is to implement the function `validate_python_code(code: str) -> bool` that takes a single argument: - `code`: A string containing the Python code to be validated. The function should return: - `True` if the code is lexically valid according to the rules described. - `False` otherwise. # Input A single string containing the Python code snippet. # Output A boolean value: `True` if the code is lexically correct, `False` otherwise. # Constraints - You can assume the input code is not longer than 1000 characters. - Focus on the rules provided in the documentation. - Ignore the actual semantics of the code (you do not need to check if the code would run without exceptions or logical errors). # Example ```python def validate_python_code(code: str) -> bool: # Your implementation here pass # Example usage code_snippet = def perm(l): if len(l) <= 1: return [l] r = [] for i in range(len(l)): s = l[:i] + l[i+1:] p = perm(s) for x in p: r.append(l[i:i+1] + x) return r print(validate_python_code(code_snippet)) # Expected output: True code_snippet_with_error = def perm(l): # error: first line indented for i in range(len(l)): # error: not indented s = l[:i] + l[i+1:] p = perm(l[:i] + l[i+1:]) # error: unexpected indent for x in p: r.append(l[i:i+1] + x) return r # error: inconsistent dedent print(validate_python_code(code_snippet_with_error)) # Expected output: False ``` Guidelines: - Ensure to handle different line endings (`n`, `rn`, `r`). - Properly handle indentation and dedentation tokens. - Comments should not affect the parsing of other tokens. - The code should be able to handle string literals correctly, including escape sequences. - Implicit and explicit line joining should be correctly implemented. **Note**: You can ignore semantic analysis, just focus on lexical correctness according to the provided documentation.","solution":"import ast def validate_python_code(code: str) -> bool: Validates a given snippet of Python code for correct lexical structure. Args: code (str): A string containing the Python code snippet to validate. Returns: bool: True if the code is lexically valid, False otherwise. try: ast.parse(code) return True except SyntaxError: return False"},{"question":"**Problem Statement: Directory Synchronization** You are provided with two directories, `source_dir` and `target_dir`. Your task is to implement a function that synchronizes the `target_dir` with the `source_dir`. Synchronization means that after the function completes: - All files present in `source_dir` should be present in `target_dir` with the same content. - Any extra files in `target_dir` that are not present in `source_dir` should remain untouched. - Subdirectories are recursively synchronized. - If any files are different (same name, different content), the file in `target_dir` should be updated to match the file from `source_dir`. **Function Signature:** ```python from typing import List import filecmp import shutil import os def sync_directories(source_dir: str, target_dir: str) -> List[str]: \'\'\' Synchronizes the `target_dir` with the `source_dir`. Parameters: source_dir (str): The path to the source directory. target_dir (str): The path to the target directory. Returns: List[str]: List of paths to files in `target_dir` that were updated or copied from `source_dir`. \'\'\' pass ``` **Constraints:** - You should use the functionalities provided by the `filecmp` module. - Do not remove any files or directories from `target_dir`. - Ensure that your function is efficient and avoids unnecessary file operations. **Example:** Assume `source_dir` has the following structure: ``` source_dir/ ├── file1.txt ├── file2.txt └── sub_dir1/ └── file3.txt ``` Assume `target_dir` has the following structure: ``` target_dir/ ├── file1.txt (different content from source_dir/file1.txt) └── extra_file.txt ``` After synchronization, `target_dir` should look like: ``` target_dir/ ├── file1.txt (same content as source_dir/file1.txt) ├── file2.txt (copied from source_dir/file2.txt) ├── extra_file.txt └── sub_dir1/ └── file3.txt (copied from source_dir/sub_dir1/file3.txt) ``` The returned list should include paths to `file1.txt`, `file2.txt`, and `sub_dir1/file3.txt` in `target_dir`. **Notes:** - This problem requires deep understanding of file and directory comparison, file operations, and handling of directory structures. - Consider edge cases such as empty directories, nested directories, and files with different contents but same names. Implement the `sync_directories` function and make sure to test it thoroughly.","solution":"from typing import List import filecmp import os import shutil def sync_directories(source_dir: str, target_dir: str) -> List[str]: Synchronizes the `target_dir` with the `source_dir`. Parameters: source_dir (str): The path to the source directory. target_dir (str): The path to the target directory. Returns: List[str]: List of paths to files in `target_dir` that were updated or copied from `source_dir`. updated_files = [] def sync_recursive(src, tgt): if not os.path.exists(tgt): os.makedirs(tgt) for item in os.listdir(src): src_item = os.path.join(src, item) tgt_item = os.path.join(tgt, item) if os.path.isdir(src_item): sync_recursive(src_item, tgt_item) else: if not os.path.exists(tgt_item) or not filecmp.cmp(src_item, tgt_item, shallow=False): shutil.copy2(src_item, tgt_item) updated_files.append(tgt_item) sync_recursive(source_dir, target_dir) return updated_files"},{"question":"# PyTorch MPS Assessment Question **Objective:** Implement and analyze a matrix multiplication operation using the Metal Performance Shaders (MPS) backend in PyTorch. Your task is to: 1. Set up the MPS device. 2. Create large random matrices suitable for MPS computations. 3. Implement the matrix multiplication on the MPS device. 4. Use the MPS profiler to measure the performance of your implementation. 5. Synchronize the device and retrieve the RNG state both before and after the computation. # Requirements: 1. **Implementation**: - Load the MPS device. - Generate two large random matrices (size 1000x1000) on the MPS device. - Perform matrix multiplication on these matrices. 2. **Profiling and Synchronization**: - Use the MPS profiler to capture the time taken for the matrix multiplication. - Collect and print the RNG state of the MPS device before and after the multiplication. - Synchronize the MPS device after the multiplication. # Input and Output: - No external input arguments. The matrices should be generated within the script. - Output the time taken for the matrix multiplication, RNG state before and after the computation, and ensure any resources are appropriately synchronized. # Constraints and Assumptions: - Assume that the machine running the code has a compatible Apple GPU supporting the MPS backend. - The matrices should be large enough (1000x1000) to illustrate the performance advantage of using GPU computations. - Pay attention to memory management to avoid overflow or excessive memory usage warnings. # Example Code Structure: ```python import torch import torch.mps def main(): device = torch.device(\'mps\') # Generate large random matrices on MPS device matrix1 = torch.randn(1000, 1000, device=device) matrix2 = torch.randn(1000, 1000, device=device) # Get RNG state before computation rng_state_before = torch.mps.get_rng_state() # Start profiler torch.mps.profiler.start() # Perform matrix multiplication result = torch.matmul(matrix1, matrix2) # Stop profiler torch.mps.profiler.stop() # Synchronize device torch.mps.synchronize() # Get RNG state after computation rng_state_after = torch.mps.get_rng_state() # Output results print(\\"RNG State Before:\\", rng_state_before) print(\\"RNG State After:\\", rng_state_after) print(\\"Matrix Multiplication Result Shape:\\", result.shape) if __name__ == \\"__main__\\": main() ``` Ensure you understand each component and modify the example as needed to complete the task.","solution":"import torch def mps_matrix_multiplication_and_profiling(): # Check for MPS availability if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS device is not available. Please ensure your environment supports MPS.\\") device = torch.device(\'mps\') # Generate large random matrices on MPS device matrix1 = torch.randn(1000, 1000, device=device) matrix2 = torch.randn(1000, 1000, device=device) # Get RNG state before computation rng_state_before = torch.mps.get_rng_state() # Perform matrix multiplication torch.cuda.profiler.start() result = torch.matmul(matrix1, matrix2) torch.cuda.profiler.stop() # Synchronize device torch.mps.synchronize() # Get RNG state after computation rng_state_after = torch.mps.get_rng_state() results = { \\"rng_state_before\\": rng_state_before, \\"rng_state_after\\": rng_state_after, \\"result_shape\\": result.shape } return results if __name__ == \\"__main__\\": result = mps_matrix_multiplication_and_profiling() print(\\"RNG State Before:\\", result[\'rng_state_before\']) print(\\"RNG State After:\\", result[\'rng_state_after\']) print(\\"Matrix Multiplication Result Shape:\\", result[\'result_shape\'])"},{"question":"# Localization of a Python Application You are tasked with internationalizing a simple Python application using the `gettext` module. The application should be able to display translated messages based on the user\'s choice of language. Application Details You will implement a function `localized_application` that: 1. Accepts a list of languages, a domain name, and a directory for language files. 2. Finds the appropriate translation file for each language using `gettext.translation`. 3. Cycles through the installed languages, printing a greeting message in each language. 4. If a language\'s translation file is missing, provide a fallback to English. Input - `languages`: A list of language codes (e.g., `[\'en\', \'es\', \'fr\']`). - `domain`: The domain name for the translation (e.g., `\'myapp\'`). - `localedir`: The directory where the language files are stored (e.g., `\'/path/to/locale\'`). Output - Print a greeting in each specified language. - If no translation file is found for a language, print a default English greeting. Example Suppose the translation files are located in `/path/to/locale` and we have the following `.mo` files: ``` /path/to/locale/en/LC_MESSAGES/myapp.mo /path/to/locale/es/LC_MESSAGES/myapp.mo /path/to/locale/fr/LC_MESSAGES/myapp.mo ``` The structure in Python may look something like this: ```python def localized_application(languages, domain, localedir): import gettext for lang in languages: try: translation = gettext.translation(domain, localedir, languages=[lang]) translation.install() _ = translation.gettext print(_(\'Hello, world!\')) except OSError: # Fallback to English if translation is missing translation = gettext.translation(domain, localedir, languages=[\'en\'], fallback=True) translation.install() _ = translation.gettext print(_(\'Hello, world!\')) # Example usage localized_application([\'es\', \'de\', \'fr\'], \'myapp\', \'/path/to/locale\') ``` In this example, it will print the greeting in Spanish and French. For German (`de`), which lacks an `.mo` file, it will fall back to English. Constraints - Assume the directory structure for language files follows the format required by the `gettext` module. - Do not use deprecated functions. - Ensure your solution falls back to English if a translation is not available. Implement the function `localized_application` accordingly.","solution":"def localized_application(languages, domain, localedir): import gettext for lang in languages: try: translation = gettext.translation(domain, localedir, languages=[lang]) translation.install() _ = translation.gettext except OSError: # Fallback to English if translation is missing translation = gettext.translation(domain, localedir, languages=[\'en\'], fallback=True) translation.install() _ = translation.gettext print(_(\'Hello, world!\'))"},{"question":"Objective: You are required to design a Python module that employs the idiomatic use of the `__name__ == \\"__main__\\"` construct. This will demonstrate your understanding of how to define entry points for your scripts and how to avoid unintended execution of code when your module is imported. Problem Statement: Your task is to implement a Python module named `calc_operations.py` that performs basic arithmetic operations (addition, subtraction, multiplication, division). It should provide both a command-line interface and importable functions. Implement the following steps: 1. Define four functions (`add(a, b)`, `subtract(a, b)`, `multiply(a, b)`, and `divide(a, b)`) each performing the corresponding arithmetic operation. 2. Design a `main()` function that: - Uses `argparse` to handle command-line arguments for the four operations. - Prints the result of the operation specified by the command-line arguments. 3. Wrap the `main()` function inside the `if __name__ == \\"__main__\\"` block. 4. Ensure that the script’s main behavior can be used as both an importable module and a command-line tool. Requirements: - **Input Format**: Accept command-line arguments specifying the operation and the two operands. (`--operation` should be one of `add`, `subtract`, `multiply`, `divide` and `--a` and `--b` should be the operands). - **Output Format**: Print the result of the operation. Example: - Command-line usage: ``` python3 calc_operations.py --operation add --a 5 --b 3 ``` Output: ``` 8 ``` - Importing and using the functions: ```python from calc_operations import add, subtract result = add(5, 3) print(result) # Output: 8 ``` Constraints: 1. Ensure the code adheres to good programming practices, including proper function definitions and handling of invalid operations or inputs. 2. Use appropriate error handling to manage division by zero and other potential errors. # Submission: Submit your `calc_operations.py` file implementing the described functionality and ensuring both command-line and importable use cases are covered.","solution":"import argparse def add(a, b): return a + b def subtract(a, b): return a - b def multiply(a, b): return a * b def divide(a, b): if b == 0: raise ValueError(\\"Division by zero is not allowed.\\") return a / b def main(): parser = argparse.ArgumentParser(description=\'Perform basic arithmetic operations.\') parser.add_argument(\'--operation\', type=str, choices=[\'add\', \'subtract\', \'multiply\', \'divide\'], required=True, help=\'The operation to perform\') parser.add_argument(\'--a\', type=float, required=True, help=\'First operand\') parser.add_argument(\'--b\', type=float, required=True, help=\'Second operand\') args = parser.parse_args() operation = args.operation a = args.a b = args.b if operation == \'add\': result = add(a, b) elif operation == \'subtract\': result = subtract(a, b) elif operation == \'multiply\': result = multiply(a, b) elif operation == \'divide\': result = divide(a, b) print(result) if __name__ == \\"__main__\\": main()"},{"question":"Question: Implement and Evaluate K-Means Clustering # Background The `KMeans` clustering algorithm is a popular method used across various fields for partitioning data into `k` clusters, where each cluster is represented by its mean value or centroid. The algorithm aims to minimize the within-cluster sum-of-squares (inertia). # Task 1. **Implement the K-Means algorithm**: - Create a function `k_means(X, k, max_iters=100, tol=1e-4)` that takes in: - `X`: A 2D numpy array of shape `(n_samples, n_features)` representing the data. - `k`: An integer representing the number of clusters. - `max_iters`: Maximum number of iterations before stopping. - `tol`: Tolerance for convergence. If the difference in the centroid positions between iterations is less than this value, stop the algorithm. - The function should return: - `centroids`: A 2D numpy array of shape `(k, n_features)` representing the final cluster centers. - `labels`: A 1D numpy array of shape `(n_samples,)` representing the index of the assigned cluster for each sample. 2. **Evaluate your implementation**: - Use the scikit-learn implementation of `KMeans` to cluster the `Iris` dataset. - Compare the results (inertia and cluster assignments) of your implementation with the scikit-learn implementation. # Constraints - Do not use any existing clustering libraries for the implementation of the K-Means algorithm in step 1. - You may use libraries like `numpy` for array manipulations and `matplotlib` for plotting (if needed). # Performance Requirements - Ensure that your implementation converges in a reasonable time for the given dataset (Iris). - The computed centroids and cluster assignments should be reasonably close to those obtained from scikit-learn\'s `KMeans` with the default parameters. # Input and Output Specification - **Input**: - `X`: 2D numpy array (`n_samples, n_features`) - `k`: integer - `max_iters`: integer (default=100) - `tol`: float (default=1e-4) - **Output**: - `centroids`: 2D numpy array (`k, n_features`) - `labels`: 1D numpy array (`n_samples,`) # Example ```python import numpy as np from sklearn import datasets from sklearn.cluster import KMeans as SKLearnKMeans # Your implementation def k_means(X, k, max_iters=100, tol=1e-4): # Initialize centroids by randomly choosing k samples from the data centroids = X[np.random.choice(X.shape[0], k, replace=False)] for i in range(max_iters): # Assign labels based on closest centroid labels = np.array([np.argmin([np.linalg.norm(x - centroid) for centroid in centroids]) for x in X]) # Calculate new centroids new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)]) # Check for convergence if np.all(np.linalg.norm(new_centroids - centroids, axis=1) < tol): break centroids = new_centroids return centroids, labels # Load Iris dataset iris = datasets.load_iris() X = iris.data # Your implementation centroids, labels = k_means(X, 3) # Scikit-learn implementation sklearn_kmeans = SKLearnKMeans(n_clusters=3, random_state=42).fit(X) sklearn_centroids = sklearn_kmeans.cluster_centers_ sklearn_labels = sklearn_kmeans.labels_ print(\\"Your implementation inertia:\\", np.sum((X - centroids[labels])**2)) print(\\"Scikit-learn inertia:\\", sklearn_kmeans.inertia_) ``` # Evaluation Criteria 1. Correctness: - Function correctly partitions the data into `k` clusters. - Results (centroids and labels) are close to the scikit-learn implementation. 2. Code Quality: - Clear, readable, and well-documented code. - Efficient use of numpy functions to minimize computational overhead.","solution":"import numpy as np def k_means(X, k, max_iters=100, tol=1e-4): K-Means clustering algorithm. Parameters: X (numpy.ndarray): Data array of shape (n_samples, n_features). k (int): Number of clusters. max_iters (int): Maximum number of iterations. tol (float): Tolerance for convergence. Returns: centroids (numpy.ndarray): Final cluster centers of shape (k, n_features). labels (numpy.ndarray): Index of the assigned cluster for each sample of shape (n_samples,). n_samples, n_features = X.shape centroids = X[np.random.choice(n_samples, k, replace=False)] for _ in range(max_iters): # Compute distances and assign labels distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2) labels = np.argmin(distances, axis=1) # Compute new centroids new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)]) # Check for convergence if np.linalg.norm(new_centroids - centroids) < tol: break centroids = new_centroids return centroids, labels if __name__ == \\"__main__\\": from sklearn import datasets from sklearn.cluster import KMeans as SKLearnKMeans # Load Iris dataset for testing iris = datasets.load_iris() X = iris.data # Test with k=3 (since there are 3 classes in Iris dataset) centroids, labels = k_means(X, 3) # Scikit-learn implementation for comparison sklearn_kmeans = SKLearnKMeans(n_clusters=3, random_state=42).fit(X) sklearn_centroids = sklearn_kmeans.cluster_centers_ sklearn_labels = sklearn_kmeans.labels_ print(\\"Your implementation inertia:\\", np.sum(np.min(np.linalg.norm(X[:, np.newaxis] - centroids, axis=2), axis=1)**2)) print(\\"Scikit-learn inertia:\\", sklearn_kmeans.inertia_)"},{"question":"# Custom Stream for Memory-Efficient File Merging You are tasked with implementing a custom class `MergeStreams` which merges multiple text files into a single output stream. Your class should be designed to handle large files efficiently, ensuring buffered reads and writes to minimize memory usage and handle text encoding properly. Requirements: 1. **Class Name**: `MergeStreams` 2. **Constructor Parameters**: - `file_paths`: A list of file paths to merge. - `output_path`: The path where the merged content should be written. - `encoding`: Optional text encoding (default is `utf-8`). 3. **Methods**: - `merge()`: Merges the content of the files specified in `file_paths` into the `output_path`. Input: - `file_paths`: - List type, each element being the path to a text file (string). - `output_path`: - Single string path indicating where the merged content should be stored. Output: - A single file at `output_path` containing the combined content of all files in `file_paths`. Constraints: - The class should use efficient, buffered I/O operations. - The class should handle encoding and decoding errors gracefully, using `backslashreplace` for errors. - Files may contain different newline conventions (`n`, `r`, `rn`). Example: ```python file_paths = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] output_path = \'merged_output.txt\' merger = MergeStreams(file_paths, output_path) merger.merge() # Contents of \'merged_output.txt\' should be the concatenation of the contents # of \'file1.txt\', \'file2.txt\', and \'file3.txt\' in that order. ``` Implementation: You should implement the class `MergeStreams` with the required functionality using the `io` module and its classes (`TextIOWrapper`, `BufferedReader`, `BufferedWriter`, etc.). Consider performance implications and ensure that memory usage is optimized for large files. Ensure proper context management to handle file resources efficiently.","solution":"import io class MergeStreams: def __init__(self, file_paths, output_path, encoding=\'utf-8\'): self.file_paths = file_paths self.output_path = output_path self.encoding = encoding def merge(self): # Use \'backslashreplace\' error handler to handle encoding errors gracefully with open(self.output_path, \'w\', encoding=self.encoding, errors=\'backslashreplace\') as output_file: for file_path in self.file_paths: with open(file_path, \'r\', encoding=self.encoding, errors=\'backslashreplace\') as input_file: while True: chunk = input_file.read(4096) # Read in chunks if not chunk: break output_file.write(chunk)"},{"question":"**Multithreaded Producer-Consumer Problem** **Objective:** Implement a multithreaded Producer-Consumer system using Python\'s `threading` module. The system should demonstrate the synchronization of shared resources among multiple threads using locks and condition variables. **Problem Statement:** You need to implement two classes, `Producer` and `Consumer`, to simulate a producer-consumer problem using a shared buffer. The buffer should be thread-safe and have a fixed capacity. Producers and consumers should wait when the buffer is full or empty, respectively. **Details:** 1. The buffer should be implemented as a `Queue` with a fixed capacity. 2. The `Producer` class should: - Produce items and add them to the buffer. - Wait if the buffer is full to avoid overwriting the items. 3. The `Consumer` class should: - Consume items from the buffer. - Wait if the buffer is empty to avoid reading invalid data. 4. Synchronization should be managed using `Lock` and `Condition` objects. 5. Implement methods to start and stop the producer and consumer threads gracefully. **Classes and Methods:** ```python import threading import time import random from collections import deque class Buffer: def __init__(self, capacity): self.capacity = capacity self.buffer = deque(maxlen=capacity) self.lock = threading.Lock() self.not_full = threading.Condition(self.lock) self.not_empty = threading.Condition(self.lock) def add_item(self, item): with self.not_full: while len(self.buffer) == self.capacity: self.not_full.wait() self.buffer.append(item) self.not_empty.notify() def remove_item(self): with self.not_empty: while not self.buffer: self.not_empty.wait() item = self.buffer.popleft() self.not_full.notify() return item class Producer(threading.Thread): def __init__(self, buffer, name, items_to_produce): super().__init__() self.buffer = buffer self.name = name self.items_to_produce = items_to_produce def run(self): for _ in range(self.items_to_produce): item = random.randint(1, 100) self.buffer.add_item(item) print(f\'{self.name} produced {item}\') time.sleep(random.uniform(0.1, 1)) class Consumer(threading.Thread): def __init__(self, buffer, name, items_to_consume): super().__init__() self.buffer = buffer self.name = name self.items_to_consume = items_to_consume def run(self): for _ in range(self.items_to_consume): item = self.buffer.remove_item() print(f\'{self.name} consumed {item}\') time.sleep(random.uniform(0.1, 1)) if __name__ == \\"__main__\\": buffer_capacity = 10 items = 20 buffer = Buffer(buffer_capacity) producer1 = Producer(buffer, \\"Producer-1\\", items) producer2 = Producer(buffer, \\"Producer-2\\", items) consumer1 = Consumer(buffer, \\"Consumer-1\\", items) consumer2 = Consumer(buffer, \\"Consumer-2\\", items) producer1.start() producer2.start() consumer1.start() consumer2.start() producer1.join() producer2.join() consumer1.join() consumer2.join() ``` **Constraints:** - The buffer\'s capacity is fixed and should be taken as input during initialization. - The producer and consumer threads should handle exceptions and terminate gracefully. - Use appropriate thread synchronization mechanisms to ensure data integrity and avoid deadlocks. **Expected Input and Output:** - The buffer capacity, number of items to produce and consume. - Output should display the produced and consumed items with respective thread names. **Notes:** - Ensure the solution handles edge cases like buffer being full or empty properly. - Use thread-safe mechanisms for all operations involving shared resources. **Evaluation Criteria:** - Correctness and efficiency of the solution. - Proper usage of threading concepts such as locks, conditions, and synchronization. - Code readability and adherence to best practices.","solution":"import threading import time import random from collections import deque class Buffer: def __init__(self, capacity): self.capacity = capacity self.buffer = deque(maxlen=capacity) self.lock = threading.Lock() self.not_full = threading.Condition(self.lock) self.not_empty = threading.Condition(self.lock) def add_item(self, item): with self.not_full: while len(self.buffer) == self.capacity: self.not_full.wait() self.buffer.append(item) self.not_empty.notify() def remove_item(self): with self.not_empty: while not self.buffer: self.not_empty.wait() item = self.buffer.popleft() self.not_full.notify() return item class Producer(threading.Thread): def __init__(self, buffer, name, items_to_produce): super().__init__() self.buffer = buffer self.name = name self.items_to_produce = items_to_produce def run(self): for _ in range(self.items_to_produce): item = random.randint(1, 100) self.buffer.add_item(item) print(f\'{self.name} produced {item}\') time.sleep(random.uniform(0.1, 1)) class Consumer(threading.Thread): def __init__(self, buffer, name, items_to_consume): super().__init__() self.buffer = buffer self.name = name self.items_to_consume = items_to_consume def run(self): for _ in range(self.items_to_consume): item = self.buffer.remove_item() print(f\'{self.name} consumed {item}\') time.sleep(random.uniform(0.1, 1)) if __name__ == \\"__main__\\": buffer_capacity = 10 items = 20 buffer = Buffer(buffer_capacity) producer1 = Producer(buffer, \\"Producer-1\\", items) producer2 = Producer(buffer, \\"Producer-2\\", items) consumer1 = Consumer(buffer, \\"Consumer-1\\", items) consumer2 = Consumer(buffer, \\"Consumer-2\\", items) producer1.start() producer2.start() consumer1.start() consumer2.start() producer1.join() producer2.join() consumer1.join() consumer2.join()"},{"question":"# Seaborn Coding Assessment **Objective:** Demonstrate your understanding of Seaborn\'s plotting and data aggregation capabilities. **Problem Statement:** You are given the \\"diamonds\\" dataset from Seaborn, which contains information about diamond prices and attributes. Your task is to create a comparative analysis plot visualizing the distribution of carat weights for different levels of clarity and cut. **Requirements:** 1. Load the diamonds dataset using Seaborn. 2. Create a plot displaying bar charts aggregating the carat weights: - Use the \\"clarity\\" of diamonds along the x-axis. - Aggregate the y-axis values representing the carat weights by their median. 3. Add color distinction among the categories of diamond \\"cut.\\" 4. Apply the `Dodge` transform to separate bars for different cuts within the same clarity level. 5. Create a custom aggregation function to calculate the interquartile range (IQR) of carat weights and overlay it as a separate set of bars over the median bars. 6. Ensure proper labeling and title for the axes and the plot. **Expected Inputs and Outputs:** - **Input:** None. The function should load the dataset internally. - **Output:** A Seaborn plot displaying the described bar charts. **Function Signature:** ```python import seaborn.objects as so from seaborn import load_dataset def plot_diamond_analysis(): # Step 1: Load the dataset diamonds = load_dataset(\\"diamonds\\") # Step 2: Create the plot with median aggregation for clarity p = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") # Step 3: Add bar chart with median aggregation and color by cut p.add(so.Bar(), so.Agg(\\"median\\"), so.Dodge(), color=\\"cut\\") # Step 4: Add bar chart with IQR aggregation p.add(so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25)), color=\\"cut\\", alpha=0.5) # Step 5: Customize axis labels and plot title p.label(x=\\"Clarity\\", y=\\"Carat Weight\\", title=\\"Distribution of Carat Weights by Clarity and Cut\\") # Display plot p.show() ``` **Constraints:** - The dataset should be loaded directly within the function. - Use Seaborn\'s object\'s interface for the entire plot creation and modifications. **Additional Information:** For documentation, refer to [Seaborn.objects API](https://seaborn.pydata.org/).","solution":"import seaborn as sns import seaborn.objects as so def plot_diamond_analysis(): # Step 1: Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Step 2: Create the plot with median aggregation for clarity p = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") # Step 3: Add bar chart with median aggregation and color by cut p.add(so.Bar(), so.Agg(lambda x: x.median()), so.Dodge(), color=\\"cut\\") # Step 4: Add bar chart with IQR aggregation p.add(so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25)), color=\\"cut\\", alpha=0.5) # Step 5: Customize axis labels and plot title p.label(x=\\"Clarity\\", y=\\"Carat Weight\\", title=\\"Distribution of Carat Weights by Clarity and Cut\\") # Display plot p.show()"},{"question":"# Question: Implement a Multi-level Lookup System You are required to create a multi-level lookup system that uses several advanced concepts from the `collections` module. The system should allow the hierarchical collection and retrieval of settings or configurations that might be defined at different levels. Each level should contain some configurations, and looking up a specific configuration should traverse from the most specific to the most general level, using the `ChainMap` class from the `collections` module. Additionally, the system should keep track of how many times each configuration is accessed using the `Counter` class. You need to define three classes: 1. `ConfigurationLevel`: Represents a dictionary storing configurations for a particular level. 2. `ConfigurationManager`: Manages multiple `ConfigurationLevel` instances using a `ChainMap`. 3. `AccessLogger`: Logs and counts each access to a configuration using `Counter`. Class Details: 1. **Class `ConfigurationLevel`**: - Constructor: `__init__(self, name: str, config:dict = None)` - `name`: A unique name for the configuration level. - `config`: A dictionary containing the configurations for this level. - Method: `get_config(self) -> dict` - Returns the dictionary of configurations for this level. - Method: `set_config(self, key: str, value: any) -> None` - Adds or updates a configuration `key` with the given `value`. 2. **Class `ConfigurationManager`**: - Constructor: `__init__(self)` - Initializes an empty `ChainMap` to hold multiple `ConfigurationLevel` instances. - Method: `add_level(self, level: ConfigurationLevel) -> None` - Adds a new `ConfigurationLevel` to the `ChainMap`. - Method: `get_value(self, key: str) -> any` - Returns the value for the given configuration `key`. - Method: `get_combined_config(self) -> dict` - Returns a combined view of all configurations. 3. **Class `AccessLogger`**: - Constructor: `__init__(self)` - Initializes an empty `Counter` to track accesses. - Method: `log_access(self, key: str) -> None` - Logs an access to the given configuration `key`. - Method: `get_access_count(self, key: str) -> int` - Returns the number of times the given configuration `key` has been accessed. Example Usage: ```python # Create ConfigurationLevel instances level1 = ConfigurationLevel(\'Defaults\', {\'timeout\': 30, \'theme\': \'light\'}) level2 = ConfigurationLevel(\'UserSettings\', {\'theme\': \'dark\', \'language\': \'en\'}) # Create a ConfigurationManager and add levels manager = ConfigurationManager() manager.add_level(level1) manager.add_level(level2) # Create an AccessLogger logger = AccessLogger() # Access configurations print(manager.get_value(\'theme\')) # Output: \'dark\' logger.log_access(\'theme\') print(manager.get_value(\'timeout\')) # Output: 30 logger.log_access(\'timeout\') # Number of access logs print(logger.get_access_count(\'theme\')) # Output: 1 print(logger.get_access_count(\'timeout\')) # Output: 1 ``` Input and Output Formats: - Use the methods of the `ConfigurationManager` and `AccessLogger` for configuration management and access logging. - Ensure that any missing configuration raises an appropriate error. Constraints: - Ensure that the solutions handle multiple levels effectively. - Logging should be accurate, counting each access individually. Implement the classes and methods in a way that demonstrates a clear understanding of the `ChainMap`, `Counter`, and dictionary handling in Python.","solution":"from collections import ChainMap, Counter class ConfigurationLevel: def __init__(self, name: str, config: dict = None): Initialize ConfigurationLevel with a name and an optional configuration dictionary. self.name = name self.config = config if config is not None else {} def get_config(self) -> dict: Return the configuration dictionary. return self.config def set_config(self, key: str, value: any) -> None: Add or update a configuration key with the specified value. self.config[key] = value class ConfigurationManager: def __init__(self): Initialize ConfigurationManager with an empty ChainMap. self.chain_map = ChainMap() def add_level(self, level: ConfigurationLevel) -> None: Add a new ConfigurationLevel to the ChainMap. self.chain_map = self.chain_map.new_child(level.get_config()) def get_value(self, key: str) -> any: Return the value for the given configuration key. Raise KeyError if the key is not found in any level. if key in self.chain_map: return self.chain_map[key] else: raise KeyError(f\\"Configuration key \'{key}\' not found\\") def get_combined_config(self) -> dict: Return a combined view of all configurations. combined_config = {} for config in reversed(self.chain_map.maps): combined_config.update(config) return combined_config class AccessLogger: def __init__(self): Initialize AccessLogger with an empty Counter. self.counter = Counter() def log_access(self, key: str) -> None: Log an access to the given configuration key. self.counter[key] += 1 def get_access_count(self, key: str) -> int: Return the number of times the given configuration key has been accessed. return self.counter[key]"},{"question":"**Problem: Normalize and Compare Unicode Strings** # Objective Implement a Python function `normalize_and_compare` that normalizes two given Unicode strings and compares them for equality. Use the normalization form \'NFD\' (Normalization Form D). The function should be able to handle strings containing a mixture of single characters and composed characters. # Function Signature ```python def normalize_and_compare(str1: str, str2: str) -> bool: pass ``` # Input - `str1` (str): The first Unicode string to be compared. - `str2` (str): The second Unicode string to be compared. # Output - `bool`: `True` if the normalized strings are equal, `False` otherwise. # Example ```python assert normalize_and_compare(\'café\', \'cafeu0301\') == True assert normalize_and_compare(\'hello\', \'world\') == False assert normalize_and_compare(\'eu0301\', \'u00e9\') == True ``` # Constraints 1. Both input strings will only contain valid Unicode characters. 2. The function should handle edge cases where the strings are empty. # Additional Information - Use the `unicodedata` module\'s `normalize` function to perform the normalization. - The normalization form \'NFD\' (Normalization Form D) decomposes characters into their composed forms. # Sample Code to Get Started ```python import unicodedata def normalize_and_compare(str1: str, str2: str) -> bool: # Normalize both strings using NFD normalization form norm_str1 = unicodedata.normalize(\'NFD\', str1) norm_str2 = unicodedata.normalize(\'NFD\', str2) # Compare the normalized strings return norm_str1 == norm_str2 # Example usage print(normalize_and_compare(\'café\', \'cafeu0301\')) # Output: True print(normalize_and_compare(\'hello\', \'world\')) # Output: False print(normalize_and_compare(\'eu0301\', \'u00e9\')) # Output: True ```","solution":"import unicodedata def normalize_and_compare(str1: str, str2: str) -> bool: Normalizes two given Unicode strings and compares them for equality using NFD normalization form. Parameters: str1 (str): The first Unicode string to be compared. str2 (str): The second Unicode string to be compared. Returns: bool: True if the normalized strings are equal, False otherwise. # Normalize both strings using NFD normalization form norm_str1 = unicodedata.normalize(\'NFD\', str1) norm_str2 = unicodedata.normalize(\'NFD\', str2) # Compare the normalized strings return norm_str1 == norm_str2"},{"question":"# Question: Exploring and Utilizing Package Import Paths You are tasked with creating a utility function that dynamically discovers and lists all the modules in a given package while ensuring that the package\'s search path is extended properly to include all possible subdirectories. This utility should handle exceptions gracefully. Task 1. Implement a function `discover_modules(package_name: str) -> List[str]` that takes the name of a package as an input and returns a sorted list of all importable modules in that package. 2. The function should use `pkgutil.walk_packages` to recursively find the modules. 3. Ensure that the package\'s search path is extended appropriately using `pkgutil.extend_path`. 4. Handle and log any import errors encountered during the process. Input - `package_name`: A string representing the base package name. Example: `\'ctypes\'`. Output - A sorted list of strings where each string is the name of a module within the package. Example: `[\'ctypes\', \'ctypes._endian\', \'ctypes.util\']`. Constraints - Assume the package name given is a valid and importable package. - You may use the `logging` module to capture and report any errors. - Do not use any external packages apart from those in the standard library. Example ```python import logging from typing import List def discover_modules(package_name: str) -> List[str]: import pkgutil import sys import importlib import logging logging.basicConfig(level=logging.ERROR) logger = logging.getLogger(__name__) try: package = importlib.import_module(package_name) package.__path__ = pkgutil.extend_path(package.__path__, package.__name__) except ImportError as e: logger.error(f\\"Failed to import package {package_name}: {e}\\") return [] module_names = [] for pkg_info in pkgutil.walk_packages(path=package.__path__, prefix=package.__name__ + \'.\'): module_names.append(pkg_info.name) return sorted(module_names) # Example usage print(discover_modules(\'ctypes\')) ``` Notes - Your implementation should dynamically handle and report errors encountered during the import or discovery process. - Aim to efficiently manage the inclusion of valid submodules and avoid redundancy. - Ensure the list of module names is sorted alphabetically for consistent output.","solution":"import logging from typing import List def discover_modules(package_name: str) -> List[str]: import pkgutil import sys import importlib logging.basicConfig(level=logging.ERROR) logger = logging.getLogger(__name__) try: package = importlib.import_module(package_name) package.__path__ = pkgutil.extend_path(package.__path__, package.__name__) except ImportError as e: logger.error(f\\"Failed to import package {package_name}: {e}\\") return [] module_names = [package_name] for pkg_info in pkgutil.walk_packages(path=package.__path__, prefix=package.__name__ + \'.\'): module_names.append(pkg_info.name) return sorted(module_names)"},{"question":"Objective: To assess the understanding of handling asynchronous operations using asyncio\'s `Future` objects in Python 3.10. Problem Statement: Implement a function `wrap_future_result(coro, delay)` that takes a coroutine `coro` and an integer `delay` as input arguments. The function should: 1. Create an `asyncio.Future` object. 2. Use the `ensure_future` function to wrap the given coroutine (`coro`) into a Task and schedule it. 3. Set the result of the Future object after the given `delay` (in seconds). 4. Return the result of the Future object. If the given coroutine raises an exception, the Future object should capture that exception. Explanation: 1. Use the provided `asyncio.Future` methods like `set_result`, `set_exception` and handle exception states. 2. Use the provided functions like `ensure_future` to schedule the coroutine. Expected Function Signature: ```python import asyncio async def wrap_future_result(coro, delay): # Your implementation here pass # Example coroutine to be used for testing async def example_coro(): await asyncio.sleep(0.5) return \\"Hello, Future!\\" # Example Test Case async def main(): result = await wrap_future_result(example_coro(), 2) print(result) # Expected to print \\"Hello, Future!\\" after 2 seconds asyncio.run(main()) ``` Input: - A coroutine `coro` which may return a value or raise an exception. - An integer `delay` which indicates the number of seconds after which the result should be set. Output: - The result of the Future object. Constraints: - The `coro` may raise an exception, which should be properly handled and set in the Future object. - The delay is always a positive integer. Example: ```python async def failing_coro(): await asyncio.sleep(0.1) raise ValueError(\\"Something went wrong!\\") async def success_coro(): await asyncio.sleep(0.1) return \\"It worked!\\" async def main(): try: result = await wrap_future_result(failing_coro(), 1) except Exception as e: print(e) # Expected to print \\"Something went wrong!\\" after 1 second result = await wrap_future_result(success_coro(), 1) print(result) # Expected to print \\"It worked!\\" after 1 second asyncio.run(main()) ``` Good Luck!","solution":"import asyncio async def wrap_future_result(coro, delay): future = asyncio.Future() async def task_wrapper(): try: result = await coro future.set_result(result) except Exception as e: future.set_exception(e) asyncio.ensure_future(task_wrapper()) await asyncio.sleep(delay) # Introduce the specified delay return await future # Example coroutines to be used for testing async def example_coro(): await asyncio.sleep(0.5) return \\"Hello, Future!\\" async def failing_coro(): await asyncio.sleep(0.1) raise ValueError(\\"Something went wrong!\\") async def success_coro(): await asyncio.sleep(0.1) return \\"It worked!\\""},{"question":"Task You are required to implement a command-line utility using the deprecated `optparse` module in Python. The utility will be a simplified version of a task manager that allows users to add, list, and delete tasks. You must demonstrate your understanding of the `optparse` module by implementing the functions described below. Requirements 1. **Define the command-line options**: - `-a`, `--add`: to add a new task. - `-l`, `--list`: to list all tasks. - `-d`, `--delete`: to delete a task by its index (0-based). 2. **Functionality**: - **Add Task**: When invoked with the `-a/--add` option followed by a task description, the program should store the new task. - **List Tasks**: When invoked with the `-l/--list` option, the program should display all stored tasks, each task on a new line preceded by its index. - **Delete Task**: When invoked with the `-d/--delete` option followed by a task index, the program should delete the task at the given index. 3. **Error Handling**: - If the `--delete` option is used with an invalid index, the program should print an appropriate error message. - If a required option argument is missing, the program should print an appropriate error message. 4. **Constraints**: - The task list should be stored in a file named `tasks.txt` to persist data between program invocations. - The program should ensure that tasks are properly loaded from and saved to this file at the start and end of the program, respectively. Input and Output Formats - **Input**: Command-line arguments as described above. - **Output**: Depends on the option used. - For `--list`: List tasks in a readable format. - For `--add` and `--delete`: No output on success, an error message on failure. Example ```sh python task_manager.py --add \\"Buy groceries\\" python task_manager.py --list 0: Buy groceries python task_manager.py --add \\"Read a book\\" python task_manager.py --list 0: Buy groceries 1: Read a book python task_manager.py --delete 0 python task_manager.py --list 0: Read a book ``` Notes - You are allowed to use any Python standard library modules. - Remember to use appropriate exception handling and input validation. Submission Submit a Python script named `task_manager.py` containing your implementation.","solution":"import os import optparse TASK_FILE = \'tasks.txt\' def load_tasks(): Load tasks from the tasks file. if not os.path.exists(TASK_FILE): return [] with open(TASK_FILE, \'r\') as file: tasks = file.readlines() return [task.strip() for task in tasks] def save_tasks(tasks): Save tasks to the tasks file. with open(TASK_FILE, \'w\') as file: file.write(\'n\'.join(tasks)) def add_task(task_description): Add a task to the tasks file. tasks = load_tasks() tasks.append(task_description) save_tasks(tasks) def list_tasks(): List all tasks. tasks = load_tasks() for index, task in enumerate(tasks): print(f\\"{index}: {task}\\") def delete_task(task_index): Delete a task by its index. tasks = load_tasks() try: task_index = int(task_index) if task_index < 0 or task_index >= len(tasks): raise IndexError tasks.pop(task_index) save_tasks(tasks) except ValueError: print(\\"Invalid index: must be an integer.\\") except IndexError: print(f\\"Invalid index: {task_index}. No such task exists.\\") def main(): parser = optparse.OptionParser() parser.add_option(\\"-a\\", \\"--add\\", dest=\\"add\\", help=\\"Add a new task\\") parser.add_option(\\"-l\\", \\"--list\\", action=\\"store_true\\", dest=\\"list\\", help=\\"List all tasks\\") parser.add_option(\\"-d\\", \\"--delete\\", dest=\\"delete\\", help=\\"Delete a task by its index\\") (options, args) = parser.parse_args() if options.add: add_task(options.add) elif options.list: list_tasks() elif options.delete: delete_task(options.delete) else: parser.print_help() if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Python Coding Assessment Problem Statement: You are tasked with implementing a function that handles LZMA-compressed data streams. The function should be capable of reading input data, compressing it with custom filters, and then decompressing it back to its original form. The function should verify that the decompressed data matches the original input data. Function Signature: ```python def handle_compression_decompression(data: bytes, filters: list) -> bool: pass ``` Input: - `data` (bytes): A byte sequence representing the data to be compressed and then decompressed. - `filters` (list): A list of dictionaries specifying the custom filter chain to be used for compression. Output: - Returns `True` if the decompressed data matches the original input data, otherwise returns `False`. Constraints: - You must use the `lzma` module for compression and decompression. - The `filters` list will always have at least one filter with a valid structure. - The total size of the `data` will not exceed 10 MB. - The function should handle potential exceptions gracefully and return `False` in case of any errors. Example: ```python data = b\\"Sample data for compression.\\" filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 6} ] assert handle_compression_decompression(data, filters) == True invalid_filters = [{\\"id\\": \\"INVALID_FILTER\\"}] assert handle_compression_decompression(data, invalid_filters) == False ``` Requirements: 1. Read the input data and compress it using the specified filter chain. 2. Decompress the compressed data back to its original form. 3. Compare the decompressed data with the input data to verify its integrity. 4. Handle any exceptions that may occur during the compression and decompression process. Hints: - Refer to the `lzma.LZMACompressor` and `lzma.LZMADecompressor` classes for implementing custom filter chains. - Use `lzma.compress` and `lzma.decompress` for simpler use-cases if needed. - Ensure to use appropriate exception handling to capture and manage any `LZMAError`.","solution":"import lzma def handle_compression_decompression(data: bytes, filters: list) -> bool: Compresses and then decompresses the given data using the specified filters, and checks if the decompressed data matches the original data. Parameters: - data (bytes): The data to be compressed and decompressed. - filters (list): A list of dictionaries specifying the custom filter chain. Returns: - bool: True if the decompressed data matches the original data, False otherwise. try: # Compress the data with the given filters compressed_data = lzma.compress(data, format=lzma.FORMAT_RAW, filters=filters) # Decompress the data decompressor = lzma.LZMADecompressor(format=lzma.FORMAT_RAW, filters=filters) decompressed_data = decompressor.decompress(compressed_data) # Check if the decompressed data matches the original data return decompressed_data == data except lzma.LZMAError: return False except Exception: return False"},{"question":"# Central User Administration with NIS **Objective**: Demonstrate understanding of the `nis` module by implementing functions that interact with the NIS server and handle potential errors. **Problem Statement**: You are working as a system administrator and need to create a Python utility that interacts with the NIS (Network Information Service) to fetch and manipulate user information centrally. Implement the following functions using the `nis` module: 1. `find_user_in_map(username: str, mapname: str) -> str`: - **Input**: - `username` (str): The username to search for. - `mapname` (str): The NIS map to search within. - **Output**: - Returns the matching entry for the username (as a string) from the NIS map. - **Exceptions to handle**: - Raise a `ValueError` with the message `\\"User not found\\"` if the username is not found in the map. - Raise a `RuntimeError` with the message `\\"NIS Error: {error}\\"` if any other NIS error occurs. 2. `dump_map_to_dict(mapname: str) -> dict`: - **Input**: - `mapname` (str): The NIS map to be dumped. - **Output**: - Returns a dictionary where each key-value pair corresponds to the entries in the NIS map. - **Exceptions to handle**: - Raise a `RuntimeError` with the message `\\"NIS Error: {error}\\"` if any NIS error occurs. 3. `list_all_maps() -> list`: - **Output**: - Returns a list of all valid NIS maps as strings. - **Exceptions to handle**: - Raise a `RuntimeError` with the message `\\"NIS Error: {error}\\"` if any NIS error occurs. **Constraints and Notes**: - These functions are intended to be run on Unix systems where the `nis` module is available. - Handle any necessary encoding and decoding of byte arrays to and from strings. - You do not need to handle the deprecation of the `nis` module for the purpose of this exercise. - Test your functions thoroughly to ensure they handle all edge cases and potential errors gracefully. **Example Usage**: ```python try: user_info = find_user_in_map(\\"jdoe\\", \\"passwd.byname\\") print(user_info) except ValueError as ve: print(ve) except RuntimeError as re: print(re) try: map_dict = dump_map_to_dict(\\"group.bygid\\") print(map_dict) except RuntimeError as re: print(re) try: maps_list = list_all_maps() print(maps_list) except RuntimeError as re: print(re) ``` Implement the specified functions with ample error handling and byte-string conversion.","solution":"import nis def find_user_in_map(username: str, mapname: str) -> str: Returns the matching entry for the username from the NIS map. Raises: ValueError: If the user is not found. RuntimeError: For other NIS errors. try: entry = nis.match(username, mapname) return entry.decode(\'utf-8\') except nis.error as e: if str(e).lower() == \'no such key\': raise ValueError(\\"User not found\\") else: raise RuntimeError(f\\"NIS Error: {e}\\") def dump_map_to_dict(mapname: str) -> dict: Returns a dictionary of the entries in the NIS map. Raises: RuntimeError: For NIS errors. try: map_entries = nis.cat(mapname) return {k.decode(\'utf-8\'): v.decode(\'utf-8\') for k, v in map_entries.items()} except nis.error as e: raise RuntimeError(f\\"NIS Error: {e}\\") def list_all_maps() -> list: Returns a list of all valid NIS maps. Raises: RuntimeError: For NIS errors. try: maps = nis.maps() return [mapname.decode(\'utf-8\') for mapname in maps] except nis.error as e: raise RuntimeError(f\\"NIS Error: {e}\\")"},{"question":"# PyTorch Coding Assessment Objective The objective of this assessment is to evaluate your understanding of PyTorch tensors, including their creation, manipulation, and the use of autograd for differentiation. Problem Statement You are given a multi-dimensional tensor. Your task is to implement a function that performs the following operations: 1. **Initialization:** Create a tensor of shape `(3, 3)` with elements from 1 to 9 (inclusive) of type `torch.float32`. 2. **Modification:** - Square each element in the tensor. - Add `1.5` to each element in the tensor. 3. **Indexing and Slicing:** - Extract a sub-tensor containing the elements in the first two rows and the first two columns. 4. **Auto-differentiation:** - Create a new tensor with `requires_grad=True` by converting the sub-tensor. - Compute the sum of the elements in this new tensor. - Perform backpropagation to compute gradients. 5. **Output:** - Return the modified tensor, the extracted sub-tensor, the summed value, and the gradients of the sub-tensor. Function Signature ```python import torch def tensor_operations(): Performs a series of operations on PyTorch tensors and returns the modified tensor, sub-tensor, summed value, and gradients. Returns: tuple: A tuple containing: - modified_tensor (torch.Tensor): The tensor after squaring each element and adding 1.5. - sub_tensor (torch.Tensor): The sub-tensor extracted from the modified tensor. - summed_value (float): The sum of the elements in the sub-tensor with requires_grad=True. - gradients (torch.Tensor): The gradients of the sub-tensor elements after backpropagation. # Step 1: Initialize a tensor of shape (3, 3) with elements from 1 to 9 inclusive # Your code here # Step 2: Square each element and add 1.5 # Your code here # Step 3: Extract sub-tensor containing the first two rows and columns # Your code here # Step 4: Perform auto-differentiation # Your code here # Return the results # Your code here ``` Constraints - Do not use any loops; leverage PyTorch tensor operations to achieve the tasks. - Ensure the operations handle tensors on GPU optimally. - All operations must be performed using PyTorch\'s in-built functions without using numpy or other external libraries. Example Usage ```python modified_tensor, sub_tensor, summed_value, gradients = tensor_operations() print(\\"Modified Tensor:n\\", modified_tensor) print(\\"Sub-Tensor:n\\", sub_tensor) print(\\"Summed Value:\\", summed_value) print(\\"Gradients:n\\", gradients) ``` Make sure to thoroughly test your function for correctness.","solution":"import torch def tensor_operations(): Performs a series of operations on PyTorch tensors and returns the modified tensor, sub-tensor, summed value, and gradients. Returns: tuple: A tuple containing: - modified_tensor (torch.Tensor): The tensor after squaring each element and adding 1.5. - sub_tensor (torch.Tensor): The sub-tensor extracted from the modified tensor. - summed_value (float): The sum of the elements in the sub-tensor with requires_grad=True. - gradients (torch.Tensor): The gradients of the sub-tensor elements after backpropagation. # Step 1: Initialize a tensor of shape (3, 3) with elements from 1 to 9 inclusive tensor = torch.arange(1, 10, dtype=torch.float32).reshape(3, 3) # Step 2: Square each element and add 1.5 modified_tensor = tensor.pow(2) + 1.5 # Step 3: Extract sub-tensor containing the first two rows and columns sub_tensor = modified_tensor[:2, :2] # Step 4: Perform auto-differentiation sub_tensor.requires_grad_(True) summed_value = sub_tensor.sum() summed_value.backward() gradients = sub_tensor.grad # Return the results return modified_tensor, sub_tensor, summed_value.item(), gradients"},{"question":"# **Advanced Network Server Implementation** **Objective:** Create a robust and efficient network server using the `socketserver` module. The server should be capable of serving multiple client requests simultaneously and must include mechanisms for customizing request handling and server shutdown safely. **Task:** 1. **Design a Custom Request Handler:** - Subclass `socketserver.BaseRequestHandler`. - Override the `handle()` method to read an integer from the client, compute its factorial, and send the result back to the client. 2. **Implement a Threading TCP Server:** - Subclass `socketserver.ThreadingMixIn` and `socketserver.TCPServer` to create a threaded server. - Ensure the server can handle multiple client connections simultaneously. 3. **Server Management:** - Implement server start and shutdown mechanisms. - Ensure the server cleans up all threads properly during shutdown. 4. **Performance Requirement:** - The server should handle at least 50 simultaneous client requests. **Constraints:** - Must properly handle edge cases like non-integer inputs or other invalid data from clients. - Ensure thread safety and proper resource cleanup. **Input:** - Integer values from clients. **Output:** - Factorial of the received integer sent back to the client. **Example:** *Server Code:* ```python import socketserver import threading import math class FactorialRequestHandler(socketserver.BaseRequestHandler): def handle(self): try: data = self.request.recv(1024).strip() num = int(data) response = math.factorial(num) self.request.sendall(str(response).encode(\'utf-8\')) except ValueError: self.request.sendall(b\\"Invalid input. Please send an integer.\\") class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 server = ThreadedTCPServer((HOST, PORT), FactorialRequestHandler) with server: ip, port = server.server_address server_thread = threading.Thread(target=server.serve_forever) server_thread.daemon = True server_thread.start() print(f\\"Server running in thread: {server_thread.name}\\") try: while True: pass except KeyboardInterrupt: server.shutdown() server.server_close() print(\\"Server shut down successfully.\\") ``` *Client Code:* ```python import socket HOST, PORT = \\"localhost\\", 9999 with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: sock.connect((HOST, PORT)) num = \\"5\\" sock.sendall(num.encode(\'utf-8\')) response = str(sock.recv(1024), \'utf-8\') print(f\\"Factorial of {num} is {response}\\") ``` *Output:* ``` Server running in thread: Thread-1 Factorial of 5 is 120 ``` Demonstrate your understanding by designing and implementing the server and client codes following these requirements.","solution":"import socketserver import threading import math class FactorialRequestHandler(socketserver.BaseRequestHandler): def handle(self): try: data = self.request.recv(1024).strip() num = int(data) response = math.factorial(num) self.request.sendall(str(response).encode(\'utf-8\')) except ValueError: self.request.sendall(b\\"Invalid input. Please send an integer.\\") class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass def run_server(host=\\"localhost\\", port=9999): server = ThreadedTCPServer((host, port), FactorialRequestHandler) with server: ip, port = server.server_address server_thread = threading.Thread(target=server.serve_forever) server_thread.daemon = True server_thread.start() print(f\\"Server running in thread: {server_thread.name}\\") try: while True: pass except KeyboardInterrupt: server.shutdown() server.server_close() print(\\"Server shut down successfully.\\")"},{"question":"**File Synchronizer Application** # Problem Statement You are required to implement a Python function `sync_directories(src_dir: str, dst_dir: str) -> None` using the `os` module. The function should synchronize the contents of the source directory (`src_dir`) to the destination directory (`dst_dir`). Specifically, the function will: 1. Recursively copy all files and subdirectories from the source directory to the destination directory. 2. Preserve file attributes such as modification times, ownership, and permissions. 3. Create or update files in the destination directory if they are newer or do not exist in the destination. 4. Remove any files or directories from the destination that do not exist in the source directory. # Requirements 1. **Input:** - `src_dir`: A string representing the path of the source directory. - `dst_dir`: A string representing the path of the destination directory. 2. **Output:** - The function returns `None`. The result is the synchronized destination directory. 3. **Constraints:** - Both `src_dir` and `dst_dir` are valid directories. - The function should handle regular files and directories only (no special devices or symbolic links). - The function should be efficient for dealing with large directory trees. - You must use functionalities provided by the `os` module. # Function Signature ```python def sync_directories(src_dir: str, dst_dir: str) -> None: pass ``` # Example 1. Given `src_dir` containing: ``` src_dir/ ├── file1.txt ├── file2.txt └── subdir └── file3.txt ``` 2. If `dst_dir` initially contains: ``` dst_dir/ ├── file1.txt (older than src_dir/file1.txt) ├── oldfile.txt └── subdir └── oldfile3.txt ``` 3. After `sync_directories(src_dir, dst_dir)`, `dst_dir` should be: ``` dst_dir/ ├── file1.txt (from src_dir, updated) ├── file2.txt (newly copied) └── subdir ├── file3.txt (newly copied) ``` # Guidelines - Use `os.path` module to handle path manipulations. - Use `os.makedirs()`, `os.listdir()`, and related functions to handle directory creation and traversal. - Use `os.stat()`, `os.utime()`, `os.chown()`, and `os.chmod()` to preserve file attributes. - Ensure that you handle exceptions appropriately, such as access permissions errors. Good luck! This task will test your understanding of file handling, directory traversal, and managing file attributes using the `os` module in Python.","solution":"import os import shutil def copy_file_with_metadata(src: str, dst: str): shutil.copy2(src, dst) def sync_directories(src_dir: str, dst_dir: str) -> None: if not os.path.exists(dst_dir): os.makedirs(dst_dir) for root, dirs, files in os.walk(src_dir): relative_path = os.path.relpath(root, src_dir) dst_root = os.path.join(dst_dir, relative_path) # Ensure that the destination directory exists if not os.path.exists(dst_root): os.makedirs(dst_root) # Copy files for file in files: src_file = os.path.join(root, file) dst_file = os.path.join(dst_root, file) if not os.path.exists(dst_file) or os.path.getmtime(dst_file) < os.path.getmtime(src_file): copy_file_with_metadata(src_file, dst_file) # Ensure that subdirectories exist for dir in dirs: dst_sub_dir = os.path.join(dst_root, dir) if not os.path.exists(dst_sub_dir): os.makedirs(dst_sub_dir) # Remove files and directories that don\'t exist in the source for root, dirs, files in os.walk(dst_dir): relative_path = os.path.relpath(root, dst_dir) src_root = os.path.join(src_dir, relative_path) # Remove files for file in files: dst_file = os.path.join(root, file) src_file = os.path.join(src_root, file) if not os.path.exists(src_file): os.remove(dst_file) # Remove directories for dir in dirs: dst_sub_dir = os.path.join(root, dir) src_sub_dir = os.path.join(src_root, dir) if not os.path.exists(src_sub_dir): shutil.rmtree(dst_sub_dir)"},{"question":"**Problem Statement:** You are given a 3-dimensional tensor. Your task is to implement a function `analyze_tensor` that performs the following: 1. Retrieves and returns the size of the given tensor. 2. Returns the size of the second dimension of the given tensor (0-based index). 3. Returns the total number of dimensions of the given tensor. **Function Signature:** ```python def analyze_tensor(tensor: torch.Tensor) -> Tuple[torch.Size, int, int]: pass ``` **Input:** - `tensor`: A 3-dimensional PyTorch tensor. **Output:** - A tuple consisting of: 1. A `torch.Size` object representing the size of the tensor. 2. An integer representing the size of the second dimension of the tensor. 3. An integer representing the total number of dimensions of the tensor. **Example:** ```python import torch # Sample tensor tensor = torch.ones(5, 10, 15) result = analyze_tensor(tensor) print(result) # Output should be: (torch.Size([5, 10, 15]), 10, 3) ``` **Constraints:** - The input tensor will always be 3-dimensional. - You should make use of the `torch.Size` class as described in the documentation. **Notes:** - Remember to import the necessary PyTorch library. - Test your function thoroughly to ensure it handles various 3-dimensional tensor shapes correctly.","solution":"import torch from typing import Tuple def analyze_tensor(tensor: torch.Tensor) -> Tuple[torch.Size, int, int]: Analyzes a 3-dimensional tensor and returns its size, the size of the second dimension, and the total number of dimensions. Parameters: tensor (torch.Tensor): A 3-dimensional PyTorch tensor. Returns: Tuple[torch.Size, int, int]: A tuple containing: - The size of the tensor (torch.Size object) - The size of the second dimension (int) - The total number of dimensions (int) tensor_size = tensor.size() second_dimension_size = tensor.size(1) total_dimensions = tensor.dim() return tensor_size, second_dimension_size, total_dimensions"},{"question":"**Coding Assessment Question:** # XML DOM Manipulation with `xml.dom.minidom` You are required to implement a function called `create_xml_document` using the `xml.dom.minidom` module. This function will create an XML document representing a list of books, each with a title, author, and publication year. The function should also generate a pretty-printed XML string from the created document. # Function Signature ```python def create_xml_document(books: list) -> str: pass ``` # Input - `books`: A list of dictionaries where each dictionary represents a book and has the following structure: ```python [ {\\"title\\": \\"Book Title 1\\", \\"author\\": \\"Author 1\\", \\"year\\": \\"2001\\"}, {\\"title\\": \\"Book Title 2\\", \\"author\\": \\"Author 2\\", \\"year\\": \\"2002\\"}, ... ] ``` # Output - A pretty-printed XML string representing the list of books. # Example ```python books = [ {\\"title\\": \\"The Great Gatsby\\", \\"author\\": \\"F. Scott Fitzgerald\\", \\"year\\": \\"1925\\"}, {\\"title\\": \\"To Kill a Mockingbird\\", \\"author\\": \\"Harper Lee\\", \\"year\\": \\"1960\\"} ] xml_string = create_xml_document(books) print(xml_string) ``` Output: ```xml <?xml version=\\"1.0\\" ?> <library> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> </book> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <year>1960</year> </book> </library> ``` # Constraints - The `books` list will contain at least one book. - Each book dictionary will always contain the keys: `title`, `author`, and `year`. # Requirements 1. Use the `xml.dom.minidom` module to create and manipulate the XML document. 2. Create a root element named \\"library\\". 3. For each book, create a \\"book\\" element with \\"title\\", \\"author\\", and \\"year\\" child elements containing the book\'s data. 4. The output XML string should be pretty-printed with a tab indentation and new lines. # Performance - The solution should handle lists of up to 1000 books efficiently.","solution":"from xml.dom.minidom import Document def create_xml_document(books): Creates an XML document string for the given list of books. Args: books (list of dict): List of books where each book is a dictionary containing \'title\', \'author\', and \'year\'. Returns: str: A pretty-printed XML string representing the list of books. doc = Document() # Create the root element \'library\' library_element = doc.createElement(\\"library\\") doc.appendChild(library_element) for book in books: # Create a \'book\' element book_element = doc.createElement(\\"book\\") # Create \'title\' element and append it to \'book\' element title_element = doc.createElement(\\"title\\") title_text = doc.createTextNode(book[\\"title\\"]) title_element.appendChild(title_text) book_element.appendChild(title_element) # Create \'author\' element and append it to \'book\' element author_element = doc.createElement(\\"author\\") author_text = doc.createTextNode(book[\\"author\\"]) author_element.appendChild(author_text) book_element.appendChild(author_element) # Create \'year\' element and append it to \'book\' element year_element = doc.createElement(\\"year\\") year_text = doc.createTextNode(book[\\"year\\"]) year_element.appendChild(year_text) book_element.appendChild(year_element) # Append the \'book\' element to the \'library\' element library_element.appendChild(book_element) # Get a pretty-printed XML string xml_str = doc.toprettyxml(indent=\\"t\\") return xml_str.strip()"},{"question":"Problem Statement You are required to design a multi-threaded application that processes a list of numerical data, stores it efficiently, logs the processing steps, and formats the final output. Your job is to implement a function `process_data` that meets the following specifications: # Function Signature ```python def process_data(nums: List[float]) -> str: pass ``` # Input - `nums` : A list of floating-point numbers. # Output - A single string formatted to show the sum of the processed numbers. # Requirements 1. **Multi-threading**: Utilize the `threading` module to process the numbers concurrently. You should launch at least two threads: one for performing arithmetic operations and one for logging activities. 2. **Efficient Storage**: Use the `array` module to store the numbers compactly instead of using regular lists. 3. **Logging**: Use the `logging` module to log each step of the arithmetic operations performed on the numbers. The log should include messages for addition, subtraction, multiplication, and division operations. 4. **Output Formatting**: Format the output string using the `pprint` module and `decimal` module to ensure the result is clear and accurate, rounded to two decimal places. 5. **Templating**: Use the `string` module’s `Template` class to create the final output string. # Example ```python import threading from array import array import logging from pprint import pformat from decimal import Decimal, getcontext from string import Template from typing import List def process_data(nums: List[float]) -> str: # Implementation nums = [12.34, 45.67, 78.91, 23.45] print(process_data(nums)) # Expected Output (Format may vary depending on implementation): # \'The processed sum of the numbers is: 160.38\' ``` # Constraints - You must use the specified modules (`threading`, `array`, `logging`, `pprint`, `decimal`, `string.Template`). - The function should handle an empty list by returning `\'The processed sum of the numbers is: 0.00\'`. # Hints - Use `threading.Thread` to create threads. - Use `array(\'d\', nums)` to store the numbers, where `\'d\'` stands for double floating point numbers. - Log each arithmetic operation using `logging.info()` or other appropriate logging levels. - Use `Decimal` for accurate arithmetic operations and rounding. - Use the `Template` object from the `string` module to format the final output string.","solution":"import threading from array import array import logging from pprint import pformat from decimal import Decimal, getcontext from string import Template from typing import List # Configure decimal to 2 decimal places. getcontext().prec = 2 # Configure logging. logging.basicConfig(level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') def process_data(nums: List[float]) -> str: # Create an array to store numbers. num_array = array(\'d\', nums) # Placeholder for the processed sum. result = Decimal(0) def arithmetic_operations(): nonlocal result for num in num_array: result += Decimal(num) logging.info(f\\"Added {num}, current sum: {result}\\") # Define the logging thread (although logging is within the operation in this case). def logging_thread(): pass # Placeholder for any additional logging operation if needed in future. # Creating threads. thread1 = threading.Thread(target=arithmetic_operations) thread2 = threading.Thread(target=logging_thread) # Start threads. thread1.start() thread2.start() # Ensure threads have finished execution. thread1.join() thread2.join() # Format the final output. template = Template(\'The processed sum of the numbers is: sum\') formatted_result = template.substitute(sum=format(result, \'.2f\')) return formatted_result # Example usage: # nums = [12.34, 45.67, 78.91, 23.45] # print(process_data(nums)) # Output: \'The processed sum of the numbers is: 160.37\'"},{"question":"# Email Fetching and Processing with imaplib **Objective:** Your task is to implement a Python function `fetch_and_process_emails` that connects to an IMAP server, logs in, fetches emails from the `INBOX`, and processes them based on specific criteria. **Requirements:** 1. The function `fetch_and_process_emails` should take the following inputs: - `host` (str): The IMAP4 server hostname. - `port` (int): The port to connect to the IMAP4 server. - `username` (str): The username for authentication. - `password` (str): The password for authentication. - `criteria` (list of tuples): Search and fetch criteria, where each tuple contains two elements - a search criterion and a part of message to fetch. Example: `[(\'FROM \\"example@example.com\\"\', \'(BODY[TEXT])\'), (\'SUBJECT \\"Report\\"\', \'(RFC822)\')]` 2. The function should establish a connection to the IMAP server securely using `IMAP4_SSL`. 3. It should handle login using the provided username and password. 4. For each search criterion in the `criteria` list: - Search for emails that match the criterion. - Fetch the specified part of each email. - Return the results as a dictionary where the key is the search criterion and the value is a list of fetched parts of matching emails. 5. The function should handle potential exceptions such as connection issues, authentication failures, and empty search results gracefully. # Function Signature: ```python def fetch_and_process_emails(host: str, port: int, username: str, password: str, criteria: list) -> dict: pass ``` # Example Usage: ```python # Sample criteria: Fetch text bodies of emails sent from \'example@example.com\' # and the full messages with subject containing \'Report\' criteria = [ (\'FROM \\"example@example.com\\"\', \'(BODY[TEXT])\'), (\'SUBJECT \\"Report\\"\', \'(RFC822)\') ] results = fetch_and_process_emails(\'imap.example.com\', 993, \'user\', \'password\', criteria) for criterion, emails in results.items(): print(f\\"Emails for criterion {criterion}:\\") for email in emails: print(email) ``` # Constraints: - Use secure connection methods (`IMAP4_SSL`) for connecting to the server. - Number of search criteria (length of `criteria`) will not exceed 10. - Handle server responses and errors appropriately, ensuring no sensitive data is exposed in error messages. # Notes: - You may use the `imaplib` module as described in the provided documentation. - The provided example usage is only indicative of the expected output format and is not exhaustive of all possible use cases. - Consider best practices for handling sensitive information like passwords and server details.","solution":"import imaplib import email def fetch_and_process_emails(host, port, username, password, criteria): Connect to an IMAP server, log in, fetch emails based on the criteria, and process them. Parameters: host (str): The IMAP4 server hostname. port (int): The port to connect to the IMAP4 server. username (str): The username for authentication. password (str): The password for authentication. criteria (list of tuples): Search and fetch criteria. Returns: dict: Dictionary where the key is the search criterion and the value is a list of fetched parts of matching emails. results = {} try: # Connect to the server with imaplib.IMAP4_SSL(host, port) as mail: # Login to the account mail.login(username, password) mail.select(\\"inbox\\") # Process each criterion for search_criterion, fetch_part in criteria: result, data = mail.search(None, search_criterion) if result == \'OK\': email_ids = data[0].split() fetched_emails = [] for email_id in email_ids: result, data = mail.fetch(email_id, fetch_part) if result == \'OK\': for response_part in data: if isinstance(response_part, tuple): fetched_emails.append(email.message_from_bytes(response_part[1]).get_payload(decode=True)) # Store the fetched emails in the results dictionary results[search_criterion] = fetched_emails else: results[search_criterion] = [] except imaplib.IMAP4.error as e: print(f\\"An error occurred: {str(e)}\\") return results"},{"question":"Using the provided `imghdr` module, write a Python function `identify_image_type(file_path, header_bytes)` that determines the type of image contained in either a file specified by `file_path` or a byte stream provided in `header_bytes`. Your function should: 1. If `header_bytes` is provided (not None), use it to determine the image type. Otherwise, use the file located at `file_path`. 2. If the image type is detected, return the type as a string describing the image format. 3. If the image type cannot be determined, return `\\"unknown\\"`. 4. Extend the standard image type detection by adding a custom test for a fictional new format \\".foo\\". The .foo files start with the byte sequence `b\'x89FOO\'`. The function signature should be: ```python def identify_image_type(file_path: str = None, header_bytes: bytes = None) -> str: pass ``` **Example Usage:** ```python # Assume the presence of some example image data identify_image_type(file_path=\'example.gif\') # Should return \'gif\' identify_image_type(header_bytes=b\'x89PNGrnx1an\') # Should return \'png\' identify_image_type(header_bytes=b\'x89FOOsomeothercontent\') # Should return \'foo\' identify_image_type(file_path=\'unknown.abc\') # Should return \'unknown\' ``` **Constraints:** - Only one of `file_path` or `header_bytes` will be provided to the function in a single call. - File paths provided will be valid paths to readable files. - `header_bytes` if provided, will contain at least 8 bytes. **Performance Requirements:** - Your implementation should handle typical image files efficiently, consistent with common usages of imghdr. # Guidance - Utilize the `imghdr.what()` function as a base for the implementation. - Add a custom test for the \\".foo\\" image format as defined in the requirements. - Ensure the solution is modular and follows best coding practices for readability and maintainability.","solution":"import imghdr def custom_test_foo(h, f): if h.startswith(b\'x89FOO\'): return \'foo\' imghdr.tests.append(custom_test_foo) def identify_image_type(file_path: str = None, header_bytes: bytes = None) -> str: if header_bytes is not None: image_type = imghdr.what(None, h=header_bytes) elif file_path is not None: image_type = imghdr.what(file_path) else: return \\"unknown\\" return image_type if image_type is not None else \\"unknown\\""},{"question":"# Asynchronous Task with Exception Handling Objective Write a Python function that executes a list of asynchronous tasks and handles various exceptions that may arise during execution. Use the `asyncio` package to perform the tasks concurrently. Specifications 1. **Function Signature:** ```python async def run_tasks(tasks: list) -> None: ``` - `tasks`: A list of coroutines that represent the tasks to be executed asynchronously. 2. **Tasks**: Each task in the list may raise one of the following exceptions: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.InvalidStateError` - `asyncio.SendfileNotAvailableError` - `asyncio.IncompleteReadError` (with properties `expected` and `partial`) - `asyncio.LimitOverrunError` (with property `consumed`) 3. **Function Requirements**: - Execute all tasks concurrently using `asyncio`. - Implement comprehensive exception handling for the specified exceptions. - Log an appropriate message for each type of exception caught, including relevant details (like `expected`, `partial`, and `consumed` where applicable). - Re-raise `CancelledError` after logging the custom message. 4. **Example Handling**: - If a `TimeoutError` occurs, log: `\\"TimeoutError: The operation has exceeded the given deadline.\\"` - If a `CancelledError` occurs, log: `\\"CancelledError: The operation has been cancelled.\\"` and re-raise the exception. - If an `InvalidStateError` occurs, log: `\\"InvalidStateError: Invalid internal state of Task or Future.\\"` - If a `SendfileNotAvailableError` occurs, log: `\\"SendfileNotAvailableError: The sendfile syscall is not available.\\"` - If an `IncompleteReadError` occurs, log: `\\"IncompleteReadError: Expected {expected} bytes, read {partial} bytes.\\"` - If a `LimitOverrunError` occurs, log: `\\"LimitOverrunError: Buffer size limit reached with {consumed} bytes to be consumed.\\"` Constraints - The function should manage many tasks efficiently and ensure that all exceptions are handled gracefully. - The function should not terminate execution abruptly unless a `CancelledError` is encountered. Example ```python import asyncio async def example_task(n): # Simulate various exceptions based on task input n if n == 1: raise asyncio.TimeoutError elif n == 2: raise asyncio.CancelledError elif n == 3: raise asyncio.InvalidStateError elif n == 4: raise asyncio.SendfileNotAvailableError elif n == 5: raise asyncio.IncompleteReadError(expected=100, partial=b\'abc\') elif n == 6: raise asyncio.LimitOverrunError(consumed=50) await asyncio.sleep(n) # Simulate some async work async def run_tasks(tasks: list) -> None: try: await asyncio.gather(*tasks) except asyncio.TimeoutError: print(\\"TimeoutError: The operation has exceeded the given deadline.\\") except asyncio.CancelledError: print(\\"CancelledError: The operation has been cancelled.\\") raise except asyncio.InvalidStateError: print(\\"InvalidStateError: Invalid internal state of Task or Future.\\") except asyncio.SendfileNotAvailableError: print(\\"SendfileNotAvailableError: The sendfile syscall is not available.\\") except asyncio.IncompleteReadError as e: print(f\\"IncompleteReadError: Expected {e.expected} bytes, read {e.partial} bytes.\\") except asyncio.LimitOverrunError as e: print(f\\"LimitOverrunError: Buffer size limit reached with {e.consumed} bytes to be consumed.\\") async def main(): tasks = [example_task(i) for i in range(1, 7)] await run_tasks(tasks) # To run the main function # asyncio.run(main()) ``` Note Make sure to test your function with different scenarios and handle exceptions as specified.","solution":"import asyncio async def run_tasks(tasks: list) -> None: async def wrapped_task(task): try: await task except asyncio.TimeoutError: print(\\"TimeoutError: The operation has exceeded the given deadline.\\") except asyncio.CancelledError: print(\\"CancelledError: The operation has been cancelled.\\") raise except asyncio.InvalidStateError: print(\\"InvalidStateError: Invalid internal state of Task or Future.\\") except asyncio.SendfileNotAvailableError: print(\\"SendfileNotAvailableError: The sendfile syscall is not available.\\") except asyncio.IncompleteReadError as e: print(f\\"IncompleteReadError: Expected {e.expected} bytes, read {e.partial} bytes.\\") except asyncio.LimitOverrunError as e: print(f\\"LimitOverrunError: Buffer size limit reached with {e.consumed} bytes to be consumed.\\") await asyncio.gather(*(wrapped_task(task) for task in tasks))"},{"question":"# Question: Implementing Discriminant Analysis and Evaluating Dimensionality Reduction Your task is to implement a Python function that uses scikit-learn\'s Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) to: 1. Perform classification on a given dataset. 2. Conduct dimensionality reduction using LDA and evaluate performance. **Function Signature:** ```python def lda_qda_classification_and_reduction(X_train, y_train, X_test, y_test, n_components): Perform classification using LDA and QDA, and evaluate the effectiveness of LDA-based dimensionality reduction. Parameters: - X_train (numpy array): The training data features. - y_train (numpy array): The training data labels. - X_test (numpy array): The test data features. - y_test (numpy array): The test data labels. - n_components (int): Number of components for LDA dimensionality reduction. Returns: - lda_accuracy (float): Accuracy of the LDA classifier on the test set. - qda_accuracy (float): Accuracy of the QDA classifier on the test set. - lda_reduced_accuracy (float): Accuracy of a classifier trained on LDA-reduced features on the test set. - explained_variance_ratio (numpy array): Array of explained variance ratios of the components after LDA reduction. pass ``` # Steps to Follow: 1. **Classification using LDA and QDA:** - Train an LDA classifier on `X_train` and `y_train`. - Train a QDA classifier on `X_train` and `y_train`. - Predict and evaluate accuracy on the `X_test` and `y_test` data for both classifiers. 2. **Dimensionality Reduction using LDA:** - Perform dimensionality reduction on `X_train` using the `n_components` parameter for LDA. - Train an LDA classifier on the LDA-reduced training data. - Predict and evaluate accuracy on the LDA-reduced `X_test` data. 3. **Output Details:** - `lda_accuracy`: Accuracy of the LDA classifier on the test data. - `qda_accuracy`: Accuracy of the QDA classifier on the test data. - `lda_reduced_accuracy`: Accuracy of a classifier trained on LDA-reduced features. - `explained_variance_ratio`: Explained variance ratio of the components after LDA reduction. # Example Usage: ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Example dataset: Iris data = load_iris() X = data.data y = data.target # Split dataset into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Number of components for LDA reduction n_components = 2 # Function call lda_accuracy, qda_accuracy, lda_reduced_accuracy, explained_variance_ratio = lda_qda_classification_and_reduction(X_train, y_train, X_test, y_test, n_components) print(\\"LDA Accuracy:\\", lda_accuracy) print(\\"QDA Accuracy:\\", qda_accuracy) print(\\"LDA Reduced Accuracy:\\", lda_reduced_accuracy) print(\\"Explained Variance Ratio:\\", explained_variance_ratio) ``` **Constraints:** - Assume `X_train`, `X_test` are numpy arrays of shape `(n_samples, n_features)`. - Assume `y_train`, `y_test` are numpy arrays of shape `(n_samples,)`. - `n_components` should be a valid integer less than the number of features in the dataset. Make sure the output is clearly presented and analysis results are accurate.","solution":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def lda_qda_classification_and_reduction(X_train, y_train, X_test, y_test, n_components): # Classification using LDA lda_clf = LDA() lda_clf.fit(X_train, y_train) y_pred_lda = lda_clf.predict(X_test) lda_accuracy = accuracy_score(y_test, y_pred_lda) # Classification using QDA qda_clf = QDA() qda_clf.fit(X_train, y_train) y_pred_qda = qda_clf.predict(X_test) qda_accuracy = accuracy_score(y_test, y_pred_qda) # Dimensionality reduction using LDA lda_reducer = LDA(n_components=n_components) X_train_reduced = lda_reducer.fit_transform(X_train, y_train) X_test_reduced = lda_reducer.transform(X_test) # Classification on LDA-reduced data using Logistic Regression lr_clf = LogisticRegression() lr_clf.fit(X_train_reduced, y_train) y_reduced_pred = lr_clf.predict(X_test_reduced) lda_reduced_accuracy = accuracy_score(y_test, y_reduced_pred) # Explained variance ratio explained_variance_ratio = lda_reducer.explained_variance_ratio_ return lda_accuracy, qda_accuracy, lda_reduced_accuracy, explained_variance_ratio"},{"question":"**Title: Implement Generic Container Class with Type Hinting** **Objective:** Assess the student\'s ability to implement a generic container class utilizing type hinting using Python’s `types.GenericAlias`. **Instructions:** 1. **Implement a class `GenericContainer` that acts as a container for any type of elements**: - The `GenericContainer` should accept a generic type upon instantiation. - The class should store elements of the specified type and raise an error if any element of a different type is added. 2. **Define the following methods in `GenericContainer`**: - `__init__(self, element_type: type) -> None`: Initializes the container with the specified element type. - `add_element(self, element: Any) -> None`: Adds an element to the container if it matches the specified type; otherwise, raises a `TypeError`. - `get_elements(self) -> list`: Returns a list of all elements in the container. 3. **Use GenericAlias for type hinting**: - Inside the `__init__` method, utilize Python’s `types.GenericAlias` to set up type hinting for the container’s elements. 4. **Constraints**: - Do not use any external libraries apart from the standard library. - The type checking should be enforced both when adding an element and retrieving from the container. 5. **Example**: ```python from types import GenericAlias class GenericContainer: def __init__(self, element_type: type) -> None: # Initialize your container pass def add_element(self, element: Any) -> None: # Add an element if it matches the type pass def get_elements(self) -> list: # Return all elements pass # Usage int_container = GenericContainer(int) int_container.add_element(10) # Should succeed int_container.add_element(\\"a string\\") # Should raise TypeError element_list = int_container.get_elements() # Should return [10] str_container = GenericContainer(str) str_container.add_element(\\"hello\\") # Should succeed str_container.add_element(100) # Should raise TypeError element_list_str = str_container.get_elements() # Should return [\\"hello\\"] ``` **Submit:** - Your implementation of `GenericContainer` class. - Example usage demonstrating the class functionality. - Any assumptions or considerations you made during the implementation. **Evaluation:** - Correctness of the implementation. - Proper use of `GenericAlias` for type hinting. - Proper handling of type constraints within the container. - Clean and readable code with appropriate comments if necessary.","solution":"from typing import Any from types import GenericAlias class GenericContainer: def __init__(self, element_type: type) -> None: Initialize the container with the specified element type. self.element_type = element_type self.elements = [] def add_element(self, element: Any) -> None: Adds an element to the container if it matches the specified type; otherwise, raises a TypeError. if not isinstance(element, self.element_type): raise TypeError(f\\"Element must be of type {self.element_type.__name__}\\") self.elements.append(element) def get_elements(self) -> list: Returns a list of all elements in the container. return self.elements"},{"question":"You are required to implement a function that monitors multiple file descriptors for readiness to read or write using the `select` module in Python. Your function should accept a list of file-like objects which represent files or sockets and continuously check their state for I/O events. # Function Signature ```python def monitor_file_descriptors(read_list: list, write_list: list, error_list: list, timeout: float) -> tuple: ``` # Parameters - `read_list`: A list of file-like objects to be checked for readiness to read. - `write_list`: A list of file-like objects to be checked for readiness to write. - `error_list`: A list of file-like objects to be checked for error conditions. - `timeout`: A float specifying the maximum time in seconds to wait for an event. Use `None` for no timeout. # Returns A tuple of three lists: - `readable`: List of file-like objects that are ready to read. - `writable`: List of file-like objects that are ready to write. - `errored`: List of file-like objects that have an error condition. # Constraints - The function must properly handle any exceptions that may occur during selection. - Use the `selectors` module where possible to write efficient and clear code. - Handle platform-specific limitations like special treatment of sockets on Windows. # Example ```python if __name__ == \\"__main__\\": import os import time # Creating dummy files to simulate file descriptors fd_read_1 = open(\'read_file1.txt\', \'w+\') fd_write_1 = open(\'write_file1.txt\', \'w+\') fd_error_1 = open(\'error_file1.txt\', \'w+\') # Adding some content to simulate readiness fd_read_1.write(\'Test read content\') fd_read_1.seek(0) read_list = [fd_read_1] write_list = [fd_write_1] error_list = [fd_error_1] timeout = 5.0 # seconds readable, writable, errored = monitor_file_descriptors(read_list, write_list, error_list, timeout) for fd in readable: print(f\'Readable: {os.path.basename(fd.name)}\') for fd in writable: print(f\'Writable: {os.path.basename(fd.name)}\') for fd in errored: print(f\'Errored: {os.path.basename(fd.name)}\') # Proper file descriptor clean up after monitoring fd_read_1.close() fd_write_1.close() fd_error_1.close() ``` # Notes - Make sure to properly close all file descriptors after monitoring. - You can simulate file descriptor readiness by creating dummy files and writing content to them. # Required Libraries ```python import os import select import selectors ```","solution":"import selectors import select def monitor_file_descriptors(read_list, write_list, error_list, timeout): Monitors file descriptors for readiness using the select module. Parameters: read_list (list): List of file-like objects to be checked for readiness to read. write_list (list): List of file-like objects to be checked for readiness to write. error_list (list): List of file-like objects to be checked for error conditions. timeout (float): Maximum time in seconds to wait for an event. Use None for no timeout. Returns: tuple: A tuple of three lists (readable, writable, errored) readable, writable, errored = [], [], [] try: readable, writable, errored = select.select(read_list, write_list, error_list, timeout) except Exception as e: print(f\\"An error occurred: {e}.\\") return (readable, writable, errored)"},{"question":"**Problem Statement: Complete Missing Data Using Various Imputation Strategies** You are provided with a dataset containing missing values. Your task is to: 1. Apply three different imputation strategies to handle the missing data. 2. Compare the performance of each imputation strategy using a simple machine learning model. # Input: - A dataset `data` which is a 2D array of shape (n_samples, n_features). - The dataset will contain `NaN` values indicating missing data. # Output: - Print the performance (mean squared error) of a regression model on the dataset after applying each imputation strategy. # Requirements: 1. Use `SimpleImputer` with the mean strategy. 2. Use `IterativeImputer` with the default settings. 3. Use `KNNImputer` with `n_neighbors=3`. 4. Apply these imputation techniques to the dataset `data`. 5. Use a simple Linear Regression model to evaluate the performance of each imputation strategy. # Constraints: - Perform a train-test split of the data (80% train, 20% test). # Example: ```python import numpy as np from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from sklearn.impute import SimpleImputer, KNNImputer from sklearn.experimental import enable_iterative_imputer # Required to enable IterativeImputer from sklearn.impute import IterativeImputer # Provided dataset with missing values data = np.array([ [1, 2, np.nan], [3, np.nan, 3], [np.nan, 6, 5], [8, 8, 7], [7, np.nan, 6] ]) # Split the data into train and test sets X_train, X_test = train_test_split(data, test_size=0.2, random_state=42) # List to store performance results results = {} # SimpleImputer with mean strategy simple_imputer = SimpleImputer(strategy=\'mean\') X_train_simple = simple_imputer.fit_transform(X_train) X_test_simple = simple_imputer.transform(X_test) model = LinearRegression() model.fit(X_train_simple, X_test_simple[:,0]) y_pred_simple = model.predict(X_test_simple) results[\'SimpleImputer\'] = mean_squared_error(X_test_simple[:, 0], y_pred_simple) # IterativeImputer with default settings iterative_imputer = IterativeImputer(random_state=0) X_train_iterative = iterative_imputer.fit_transform(X_train) X_test_iterative = iterative_imputer.transform(X_test) model.fit(X_train_iterative, X_test_iterative[:,0]) y_pred_iterative = model.predict(X_test_iterative) results[\'IterativeImputer\'] = mean_squared_error(X_test_iterative[:, 0], y_pred_iterative) # KNNImputer with n_neighbors=3 knn_imputer = KNNImputer(n_neighbors=3) X_train_knn = knn_imputer.fit_transform(X_train) X_test_knn = knn_imputer.transform(X_test) model.fit(X_train_knn, X_test_knn[:,0]) y_pred_knn = model.predict(X_test_knn) results[\'KNNImputer\'] = mean_squared_error(X_test_knn[:, 0], y_pred_knn) print(results) ``` # Notes: - Ensure to handle any corner cases in the dataset (e.g., entire columns or rows being NaN, varying NaN distributions). - You can assume that the data is non-categorical and numerical.","solution":"import numpy as np from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from sklearn.impute import SimpleImputer, KNNImputer from sklearn.experimental import enable_iterative_imputer # Required to enable IterativeImputer from sklearn.impute import IterativeImputer def evaluate_imputation_strategies(data): # Split the data into train and test sets X_train, X_test = train_test_split(data, test_size=0.2, random_state=42) # List to store performance results results = {} # SimpleImputer with mean strategy simple_imputer = SimpleImputer(strategy=\'mean\') X_train_simple = simple_imputer.fit_transform(X_train) X_test_simple = simple_imputer.transform(X_test) model = LinearRegression() model.fit(X_train_simple[:, 1:], X_train_simple[:, 0]) y_pred_simple = model.predict(X_test_simple[:, 1:]) results[\'SimpleImputer\'] = mean_squared_error(X_test_simple[:, 0], y_pred_simple) # IterativeImputer with default settings iterative_imputer = IterativeImputer(random_state=0) X_train_iterative = iterative_imputer.fit_transform(X_train) X_test_iterative = iterative_imputer.transform(X_test) model.fit(X_train_iterative[:, 1:], X_train_iterative[:, 0]) y_pred_iterative = model.predict(X_test_iterative[:, 1:]) results[\'IterativeImputer\'] = mean_squared_error(X_test_iterative[:, 0], y_pred_iterative) # KNNImputer with n_neighbors=3 knn_imputer = KNNImputer(n_neighbors=3) X_train_knn = knn_imputer.fit_transform(X_train) X_test_knn = knn_imputer.transform(X_test) model.fit(X_train_knn[:, 1:], X_train_knn[:, 0]) y_pred_knn = model.predict(X_test_knn[:, 1:]) results[\'KNNImputer\'] = mean_squared_error(X_test_knn[:, 0], y_pred_knn) return results"},{"question":"Objective: Design and implement a function that calculates the distance from a point to a circle and determines whether the point is inside, on, or outside the circle. Function Signature: ```python def point_circle_relation(px: float, py: float, cx: float, cy: float, r: float) -> str: pass ``` Input: - `px` (float): x-coordinate of the point. - `py` (float): y-coordinate of the point. - `cx` (float): x-coordinate of the circle\'s center. - `cy` (float): y-coordinate of the circle\'s center. - `r` (float): radius of the circle. Output: - Returns a string that indicates the position of the point relative to the circle: - `\\"inside\\"` if the point is inside the circle. - `\\"on\\"` if the point is exactly on the circumference of the circle. - `\\"outside\\"` if the point is outside the circle. Constraints: - The radius `r` will always be a positive number. - The coordinates can be both positive and negative. Example: ```python point_circle_relation(1, 1, 0, 0, 1) # Should return \\"inside\\" point_circle_relation(0, 1, 0, 0, 1) # Should return \\"on\\" point_circle_relation(2, 2, 0, 0, 1) # Should return \\"outside\\" ``` Performance Requirements: - The function should be efficient and perform within reasonable time for typical floating-point operations. Hints: - Use the Euclidean distance formula to compute the distance from the point to the center of the circle. - Compare this distance to the radius of the circle to determine the correct relation.","solution":"import math def point_circle_relation(px: float, py: float, cx: float, cy: float, r: float) -> str: Determines the position of a point relative to a circle. Parameters: px (float): x-coordinate of the point. py (float): y-coordinate of the point. cx (float): x-coordinate of the circle\'s center. cy (float): y-coordinate of the circle\'s center. r (float): radius of the circle. Returns: str: \\"inside\\" if the point is inside the circle, \\"on\\" if the point is exactly on the circumference, \\"outside\\" if the point is outside the circle. distance = math.sqrt((px - cx) ** 2 + (py - cy) ** 2) if distance < r: return \\"inside\\" elif distance == r: return \\"on\\" else: return \\"outside\\""},{"question":"POP3 Email Retrieval and Management Implement a class `EmailClient` using the `poplib` module to manage emails from a POP3 server. The `EmailClient` class should support the following functionalities: 1. **Initialize Connection**: Establish a connection to a specified POP3 server (with or without SSL). 2. **Authenticate User**: Authenticate the user using provided credentials. 3. **Fetch and Display Email Count**: Provide the total count of emails in the mailbox. 4. **Retrieve and Print Emails**: Retrieve and print the contents of all emails. 5. **Delete Email**: Delete a specific email by its number. 6. **Quit Connection**: Properly close the connection to the POP3 server. Write methods for the `EmailClient` class as specified below. You should handle potential errors by raising appropriate exceptions with meaningful error messages. Class Definition: ```python class EmailClient: def __init__(self, host, port=110, use_ssl=False, timeout=None): Initializes the connection to the POP3 server. :param host: The POP3 server hostname. :param port: The port to connect to. Default is 110 for non-SSL, 995 for SSL. :param use_ssl: Boolean indicating whether to use SSL. Default is False. :param timeout: The timeout for connections in seconds. Default is None. pass def authenticate(self, username, password): Authenticates the user to the POP3 server. :param username: The username for the POP3 account. :param password: The password for the POP3 account. pass def get_email_count(self): Fetches the count of emails in the mailbox. :return: The total count of emails. pass def retrieve_emails(self): Retrieves and prints the content of all emails. pass def delete_email(self, email_number): Deletes a specified email from the mailbox. :param email_number: The number of the email to delete. pass def quit(self): Closes the connection to the POP3 server. pass ``` Example Usage: ```python client = EmailClient(\\"pop.example.com\\", use_ssl=True) client.authenticate(\\"user@example.com\\", \\"password123\\") print(f\\"Total emails: {client.get_email_count()}\\") client.retrieve_emails() client.delete_email(3) client.quit() ``` # Constraints: - Use appropriate methods from the `poplib` module for each functionality. - Ensure proper error handling for network issues, authentication failures, and invalid operations. # Performance Requirements: - The implementation should handle standard POP3 operations efficiently. - Ensure the code is readable and well-documented. Good luck!","solution":"import poplib from email.parser import BytesParser class EmailClient: def __init__(self, host, port=110, use_ssl=False, timeout=None): Initializes the connection to the POP3 server. :param host: The POP3 server hostname. :param port: The port to connect to. Default is 110 for non-SSL, 995 for SSL. :param use_ssl: Boolean indicating whether to use SSL. Default is False. :param timeout: The timeout for connections in seconds. Default is None. self.host = host self.port = port if not use_ssl else 995 self.use_ssl = use_ssl self.timeout = timeout try: if self.use_ssl: self.connection = poplib.POP3_SSL(self.host, self.port, timeout=self.timeout) else: self.connection = poplib.POP3(self.host, self.port, timeout=self.timeout) except Exception as e: raise Exception(f\\"Failed to establish connection: {e}\\") def authenticate(self, username, password): Authenticates the user to the POP3 server. :param username: The username for the POP3 account. :param password: The password for the POP3 account. try: self.connection.user(username) self.connection.pass_(password) except Exception as e: raise Exception(f\\"Authentication failed: {e}\\") def get_email_count(self): Fetches the count of emails in the mailbox. :return: The total count of emails. try: email_count, _ = self.connection.stat() return email_count except Exception as e: raise Exception(f\\"Error fetching email count: {e}\\") def retrieve_emails(self): Retrieves and prints the content of all emails. try: email_count, _ = self.connection.stat() for i in range(1, email_count + 1): response, lines, octets = self.connection.retr(i) msg_content = b\'rn\'.join(lines) message = BytesParser().parsebytes(msg_content) print(f\\"Email {i}:\\") print(message) print(\\"-\\" * 50) except Exception as e: raise Exception(f\\"Error retrieving emails: {e}\\") def delete_email(self, email_number): Deletes a specified email from the mailbox. :param email_number: The number of the email to delete. try: self.connection.dele(email_number) except Exception as e: raise Exception(f\\"Error deleting email: {e}\\") def quit(self): Closes the connection to the POP3 server. try: self.connection.quit() except Exception as e: raise Exception(f\\"Error quitting connection: {e}\\")"},{"question":"**Title: Exploring and Manipulating Datasets with `sklearn.datasets`** Objective: You are required to demonstrate your ability to work with various datasets provided by the `sklearn.datasets` package. This task involves loading a dataset, performing basic manipulations and analyses, and extracting useful insights from the data. Instructions: 1. **Loading Data:** - Write a function `load_dataset(dataset_name: str, return_X_y: bool = False) -> Union[Bunch, Tuple[np.ndarray, np.ndarray]]` that takes the name of the dataset (`dataset_name`) to load and a boolean flag (`return_X_y`) to specify if the data and target should be returned separately as a tuple. - The function should handle three datasets: - `\\"iris\\"`: Load the famous Iris dataset. - `\\"wine\\"`: Load the Wine recognition dataset. - `\\"digits\\"`: Load the digits dataset. - For any other dataset name, raise a `ValueError` with an appropriate message. 2. **Data Analysis:** - Write a function `analyze_dataset(data: Union[Bunch, Tuple[np.ndarray, np.ndarray]]) -> Dict[str, Any]` that takes the dataset loaded from `load_dataset` and performs the following analyses: - If the input is a `Bunch` object, extract `data` and `target`. - Calculate and return the following statistics in a dictionary: - `num_samples`: Number of samples in the dataset. - `num_features`: Number of features in the dataset. - `target_classes`: Unique classes in the target. - `feature_means`: Mean value of each feature. - `feature_stddevs`: Standard deviation of each feature. 3. **Data Manipulation:** - Write a function `normalize_data(data: np.ndarray) -> np.ndarray` that normalizes the dataset features (independent variables) to have a mean of 0 and a standard deviation of 1. 4. **Data Splitting:** - Write a function `split_data(data: np.ndarray, target: np.ndarray, test_size: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]` that splits the data into training and testing sets. Use a fixed random seed for reproducibility. Expected Function Definitions: ```python def load_dataset(dataset_name: str, return_X_y: bool = False) -> Union[Bunch, Tuple[np.ndarray, np.ndarray]]: pass def analyze_dataset(data: Union[Bunch, Tuple[np.ndarray, np.ndarray]]) -> Dict[str, Any]: pass def normalize_data(data: np.ndarray) -> np.ndarray: pass def split_data(data: np.ndarray, target: np.ndarray, test_size: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: pass ``` Constraints: - Your functions should handle potential edge cases gracefully. - You must use the `sklearn.datasets` package for dataset loading. - Utilize `numpy` for numerical operations and ensure the results are reproducible by setting a random seed where necessary. - Document any assumptions and considerations in your code. Output: Your code should be well-commented and include examples of usage demonstrating how to load, analyze, normalize, and split datasets.","solution":"import numpy as np from sklearn.datasets import load_iris, load_wine, load_digits from sklearn.utils import Bunch from typing import Union, Tuple, Dict, Any import random def load_dataset(dataset_name: str, return_X_y: bool = False) -> Union[Bunch, Tuple[np.ndarray, np.ndarray]]: Load a dataset from sklearn based on the given dataset name. Parameters: dataset_name (str): The name of the dataset to load (\'iris\', \'wine\', or \'digits\'). return_X_y (bool): If True, return (data, target) tuple. Otherwise, return Bunch object. Returns: Union[Bunch, Tuple[np.ndarray, np.ndarray]]: The dataset loaded in the form requested. Raises: ValueError: If an unsupported dataset name is provided. if dataset_name == \\"iris\\": dataset = load_iris(return_X_y=return_X_y) elif dataset_name == \\"wine\\": dataset = load_wine(return_X_y=return_X_y) elif dataset_name == \\"digits\\": dataset = load_digits(return_X_y=return_X_y) else: raise ValueError(f\\"Unsupported dataset name: {dataset_name}\\") return dataset def analyze_dataset(data: Union[Bunch, Tuple[np.ndarray, np.ndarray]]) -> Dict[str, Any]: Analyze a dataset to extract basic statistics. Parameters: data (Union[Bunch, Tuple[np.ndarray, np.ndarray]]): The dataset to analyze. Returns: Dict[str, Any]: A dictionary containing basic statistics of the dataset. if isinstance(data, Bunch): X, y = data.data, data.target else: X, y = data num_samples = X.shape[0] num_features = X.shape[1] target_classes = np.unique(y) feature_means = np.mean(X, axis=0) feature_stddevs = np.std(X, axis=0) return { \'num_samples\': num_samples, \'num_features\': num_features, \'target_classes\': target_classes, \'feature_means\': feature_means, \'feature_stddevs\': feature_stddevs } def normalize_data(data: np.ndarray) -> np.ndarray: Normalize the dataset features to have a mean of 0 and standard deviation of 1. Parameters: data (np.ndarray): The dataset features to normalize. Returns: np.ndarray: The normalized dataset features. mean = np.mean(data, axis=0) stddev = np.std(data, axis=0) return (data - mean) / stddev def split_data(data: np.ndarray, target: np.ndarray, test_size: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: Split the dataset into training and testing sets. Parameters: data (np.ndarray): The dataset features. target (np.ndarray): The dataset target labels. test_size (float): The proportion of the dataset to include in the test split. Returns: Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: The train-test split of data and target. np.random.seed(42) indices = np.arange(data.shape[0]) np.random.shuffle(indices) split_point = int(len(indices) * (1 - test_size)) train_indices = indices[:split_point] test_indices = indices[split_point:] X_train, X_test = data[train_indices], data[test_indices] y_train, y_test = target[train_indices], target[test_indices] return X_train, X_test, y_train, y_test"},{"question":"Signal Windowing with PyTorch **Objective:** Implement and utilize various window functions from the `torch.signal.windows` module to process a given signal. Demonstrate the effect of different window functions on the signal. **Question:** You are provided with a 1D signal and are required to apply different window functions from the `torch.signal.windows` module to observe their effect. # The Signal: Consider a 1D signal `x` defined as follows: ```python import torch # Generating a sample signal: sum of two sine waves t = torch.linspace(0, 1, 500) x = torch.sin(2 * torch.pi * 5 * t) + 0.5 * torch.sin(2 * torch.pi * 20 * t) ``` # Task: 1. Implement a function `apply_window` that takes in a signal `x` and a window function name (as a string) from the `torch.signal.windows` module, and returns the windowed signal. 2. Plot the original signal and the windowed signal for the following window functions: - \'hann\' - \'hamming\' - \'blackman\' # Expected Function Signature: ```python def apply_window(signal: torch.Tensor, window_func_name: str) -> torch.Tensor: Apply the specified window function to the signal. Parameters: signal (torch.Tensor): The input 1D signal. window_func_name (str): The name of the window function to use. Returns: torch.Tensor: The windowed signal. pass ``` # Constraints: - You must only use the window functions provided within the `torch.signal.windows` module. - Ensure the input signal and the generated window have the same length. - Use `torch` for all operations and ensure your code is efficient. # Example Usage: ```python import matplotlib.pyplot as plt # Generate the sample signal t = torch.linspace(0, 1, 500) x = torch.sin(2 * torch.pi * 5 * t) + 0.5 * torch.sin(2 * torch.pi * 20 * t) # Apply and plot different windowing functions for window_name in [\'hann\', \'hamming\', \'blackman\']: windowed_signal = apply_window(x, window_name) plt.figure() plt.plot(t, x, label=\'Original Signal\') plt.plot(t, windowed_signal, label=f\'Windowed Signal - {window_name}\') plt.legend() plt.title(f\'Effect of {window_name} window\') plt.show() ``` Your solution should clearly demonstrate your ability to: - Use different window functions available in the `torch.signal.windows` module. - Properly handle and apply window functions to signals. - Analyze and visualize the effect of window functions on the signal. Good luck!","solution":"import torch import torch.nn.functional as F def apply_window(signal: torch.Tensor, window_func_name: str) -> torch.Tensor: Apply the specified window function to the signal. Parameters: signal (torch.Tensor): The input 1D signal. window_func_name (str): The name of the window function to use. Returns: torch.Tensor: The windowed signal. if window_func_name == \'hann\': window = torch.hann_window(len(signal)) elif window_func_name == \'hamming\': window = torch.hamming_window(len(signal)) elif window_func_name == \'blackman\': window = torch.blackman_window(len(signal)) else: raise ValueError(f\\"Unknown window function: {window_func_name}\\") return signal * window"},{"question":"# Advanced Coding Assessment Question Objective Implement a simple multi-threaded task scheduler using the `_thread` module from Python\'s standard library. Your implementation should demonstrate thread creation, synchronization using locks, and minimal error handling. Task You are required to implement a function `schedule_tasks` that schedules and runs multiple tasks using separate threads. Each task is represented by a function that prints a message after sleeping for a given number of seconds. You need to ensure that new tasks do not start before all tasks in the current batch have finished executing. Use locks to synchronize this effectively. Function Signature ```python def schedule_tasks(tasks: List[Tuple[Callable, int]]) -> None: pass ``` Input - `tasks`: A list of tuples, where each tuple consists of: - A callback function (`Callable`): This function takes no arguments and returns `None`. - An integer representing the number of seconds the function should sleep before printing its message. Constraints - Each task function should be executed on a separate thread. - Use locks to synchronize task batches to ensure no new batch starts until the current batch is completed. - Ensure minimal error handling for unhandled exceptions in threads, such as printing a simple error message. Example Usage ```python import _thread from time import sleep from typing import List, Tuple, Callable def print_message(message: str): print(message) def dummy_task(): sleep(2) print_message(\\"Task Completed\\") schedule_tasks([(dummy_task, 2), (dummy_task, 1)]) ``` Expected Output (order may vary due to threading): ``` Task Completed Task Completed ``` Implementation Notes - Utilize `_thread.start_new_thread` for creating new threads. - Use `_thread.allocate_lock` to manage synchronization between tasks. - You must ensure that the lock is used properly to synchronize the starting of new batches. - Handle exceptions using the appropriate mechanism provided by the `_thread` module. Performance Requirements - The function should handle at least 100 tasks within a reasonable time frame. - The maximum sleep time for any task is 10 seconds. - The function should not block indefinitely.","solution":"import _thread from time import sleep from typing import List, Tuple, Callable def schedule_tasks(tasks: List[Tuple[Callable, int]]) -> None: def wrapper(task: Callable, duration: int, lock: _thread.LockType): try: sleep(duration) task() finally: lock.release() if not tasks: return lock = _thread.allocate_lock() for task, duration in tasks: lock.acquire() _thread.start_new_thread(wrapper, (task, duration, lock)) # Ensure the main thread waits for all threads to finish while lock.locked(): sleep(0.1)"},{"question":"**Objective**: Implement a function using `seaborn` that demonstrates the ability to customize plot contexts, scales, and individual plot parameters based on different datasets and plot requirements. # Problem Description You are provided with a dataset containing the average temperature data for three different cities over a period of 12 months. Your task is to create a function that plots this data using `seaborn`. The function should demonstrate the following: 1. Setting a context for the plots. 2. Customizing the font scale for the plots. 3. Overriding specific parameters like line width, marker size, and the color palette. # Function Signature ```python def plot_temperature_data(data): \'\'\' This function accepts a dictionary `data` with the average temperature data for three cities and plots this information using seaborn. Parameters: data (dict): A dictionary with keys as city names and values as lists of average temperatures (list of tuples containing month and temperature). Example input: data = { \'CityA\': [(1, 30), (2, 32), (3, 35), (4, 40), (5, 42), (6, 45), (7, 47), (8, 45), (9, 42), (10, 38), (11, 35), (12, 32)], \'CityB\': [(1, 20), (2, 22), (3, 25), (4, 30), (5, 32), (6, 35), (7, 37), (8, 35), (9, 32), (10, 28), (11, 25), (12, 22)], \'CityC\': [(1, 10), (2, 12), (3, 15), (4, 20), (5, 22), (6, 25), (7, 27), (8, 25), (9, 22), (10, 18), (11, 15), (12, 12)] } Returns: A seaborn line plot showing the temperature trends for all three cities. \'\'\' pass ``` # Instructions 1. Use the `sns.set_context()` function to set the context to \\"talk\\". 2. Set the `font_scale` to 1.5. 3. Override specific parameters using `rc` to set: - `lines.linewidth` to 2 - `lines.marker` to \'o\' - `lines.markersize` to 8 - Use a different color palette for the lines. 4. The X-axis should represent the months, and the Y-axis should represent the average temperature. 5. Ensure the plot includes labels for the X-axis (Month) and Y-axis (Average Temperature) and a title (\\"Average Monthly Temperature\\"). 6. Include a legend that clearly differentiates the cities. # Example Usage ```python data = { \'CityA\': [(1, 30), (2, 32), (3, 35), (4, 40), (5, 42), (6, 45), (7, 47), (8, 45), (9, 42), (10, 38), (11, 35), (12, 32)], \'CityB\': [(1, 20), (2, 22), (3, 25), (4, 30), (5, 32), (6, 35), (7, 37), (8, 35), (9, 32), (10, 28), (11, 25), (12, 22)], \'CityC\': [(1, 10), (2, 12), (3, 15), (4, 20), (5, 22), (6, 25), (7, 27), (8, 25), (9, 22), (10, 18), (11, 15), (12, 12)] } plot_temperature_data(data) ``` The function should produce a line plot using `seaborn` that meets the specifications above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_temperature_data(data): This function accepts a dictionary `data` with the average temperature data for three cities and plots this information using seaborn. Parameters: data (dict): A dictionary with keys as city names and values as lists of average temperatures (list of tuples containing month and temperature). # Transform data into a suitable format for seaborn plotting months = range(1, 13) city_names = list(data.keys()) temperatures = [] for city in city_names: temperatures.append([temp for month, temp in data[city]]) # Set the plot context and customize fonts, line width, marker size, and color palette sns.set_context(\\"talk\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2, \\"lines.marker\\": \'o\', \\"lines.markersize\\": 8}) palette = sns.color_palette(\\"husl\\", len(city_names)) # Create the plot plt.figure(figsize=(12, 8)) for i, city in enumerate(city_names): sns.lineplot(x=months, y=temperatures[i], label=city, palette=palette[i]) # Set the labels and title plt.xlabel(\'Month\') plt.ylabel(\'Average Temperature\') plt.title(\'Average Monthly Temperature\') plt.legend(title=\'Cities\') plt.show() data = { \'CityA\': [(1, 30), (2, 32), (3, 35), (4, 40), (5, 42), (6, 45), (7, 47), (8, 45), (9, 42), (10, 38), (11, 35), (12, 32)], \'CityB\': [(1, 20), (2, 22), (3, 25), (4, 30), (5, 32), (6, 35), (7, 37), (8, 35), (9, 32), (10, 28), (11, 25), (12, 22)], \'CityC\': [(1, 10), (2, 12), (3, 15), (4, 20), (5, 22), (6, 25), (7, 27), (8, 25), (9, 22), (10, 18), (11, 15), (12, 12)] } plot_temperature_data(data)"},{"question":"# Comprehensive File Handler and Formatter Problem Statement You are to implement a class `FileHandlerAndFormatter` that provides methods to read and write text files, format the contents, and handle JSON serialization/deserialization efficiently. The class should be able to handle multiple types of data, ensuring proper formatting and error handling. Class Definition ```python class FileHandlerAndFormatter: def __init__(self, filename): Initialize the FileHandlerAndFormatter with a given filename. :param filename: The name of the file to read/write. self.filename = filename def read_file(self): Reads the entire content of the file specified by self.filename. :return: The content of the file as a string. :raises IOError: If the file cannot be read. pass def write_to_file(self, content): Writes the given content to the file specified by self.filename. :param content: The string content to write to the file. :raises IOError: If the file cannot be written. pass def format_file_content(self): Reads the content of the file, formats it by capitalizing each word, and writes it back to the file. :raises IOError: If the file cannot be read or written. pass def serialize_to_json(self, data): Serializes the given data (a Python dictionary) to the file in JSON format. :param data: The dictionary to serialize and write to the file. :raises ValueError: If the data is not a dictionary. :raises IOError: If the file cannot be written. pass def deserialize_from_json(self): Deserializes the JSON content from the file to a Python dictionary. :return: The deserialized dictionary. :raises IOError: If the file cannot be read. :raises ValueError: If the file content cannot be parsed as JSON. pass ``` Implementation Requirements 1. **`read_file` Method**: - Should read the entire content of the file and return it as a string. - Should handle any possible I/O errors. 2. **`write_to_file` Method**: - Should write the provided content to the file. - Should handle any possible I/O errors. 3. **`format_file_content` Method**: - Should read the file content, capitalize each word (e.g., using `str.title()`), and write the formatted content back to the file. - Should handle any possible I/O errors. 4. **`serialize_to_json` Method**: - Should take a dictionary, serialize it to a JSON formatted string, and write it to the file. - Should ensure the input data is a dictionary; otherwise, raise a `ValueError`. 5. **`deserialize_from_json` Method**: - Should read the content of the file, parse it as JSON, and return the resulting dictionary. - Should handle any possible I/O errors and JSON parsing errors. Performance Requirements - All file operations should be efficiently handled using context managers for proper resource management. - Ensure that large files are managed well within memory constraints. Example Usage ```python # Initialize with a filename handler = FileHandlerAndFormatter(\'example.txt\') # Write to file handler.write_to_file(\\"hello world\\") # Read from file print(handler.read_file()) # Output: \\"hello world\\" # Format content and write back to file handler.format_file_content() print(handler.read_file()) # Output: \\"Hello World\\" # Serialize a dictionary to JSON data = {\\"name\\": \\"John\\", \\"age\\": 30, \\"city\\": \\"New York\\"} handler.serialize_to_json(data) # Deserialize JSON content from file print(handler.deserialize_from_json()) # Output: {\\"name\\": \\"John\\", \\"age\\": 30, \\"city\\": \\"New York\\"} ``` Note: Ensure your implementation correctly handles exceptions and edge cases for robust and reliable file operations.","solution":"import os import json class FileHandlerAndFormatter: def __init__(self, filename): Initialize the FileHandlerAndFormatter with a given filename. :param filename: The name of the file to read/write. self.filename = filename def read_file(self): Reads the entire content of the file specified by self.filename. :return: The content of the file as a string. :raises IOError: If the file cannot be read. try: with open(self.filename, \'r\') as file: return file.read() except IOError as e: raise IOError(f\\"Cannot read file {self.filename}: {e}\\") def write_to_file(self, content): Writes the given content to the file specified by self.filename. :param content: The string content to write to the file. :raises IOError: If the file cannot be written. try: with open(self.filename, \'w\') as file: file.write(content) except IOError as e: raise IOError(f\\"Cannot write to file {self.filename}: {e}\\") def format_file_content(self): Reads the content of the file, formats it by capitalizing each word, and writes it back to the file. :raises IOError: If the file cannot be read or written. try: content = self.read_file() formatted_content = content.title() self.write_to_file(formatted_content) except IOError as e: raise IOError(f\\"Error formatting file content: {e}\\") def serialize_to_json(self, data): Serializes the given data (a Python dictionary) to the file in JSON format. :param data: The dictionary to serialize and write to the file. :raises ValueError: If the data is not a dictionary. :raises IOError: If the file cannot be written. if not isinstance(data, dict): raise ValueError(\\"Data must be a dictionary\\") try: with open(self.filename, \'w\') as file: json.dump(data, file) except IOError as e: raise IOError(f\\"Cannot write JSON to file {self.filename}: {e}\\") def deserialize_from_json(self): Deserializes the JSON content from the file to a Python dictionary. :return: The deserialized dictionary. :raises IOError: If the file cannot be read. :raises ValueError: If the file content cannot be parsed as JSON. try: with open(self.filename, \'r\') as file: return json.load(file) except IOError as e: raise IOError(f\\"Cannot read file {self.filename}: {e}\\") except json.JSONDecodeError as e: raise ValueError(f\\"Cannot decode JSON from file {self.filename}: {e}\\")"},{"question":"**Objective**: Implement a PyTorch function that creates a specified tensor, performs various tensor operations, and computes the gradient of a given mathematical function with respect to the tensor. **Task**: Write a function `tensor_operations_and_gradient_calculation` that: 1. Creates a tensor of size `(3, 3)` with `torch.float32` data type, initialized with specified values. 2. Slices the tensor to extract a 2x2 sub-tensor. 3. Computes the element-wise square of the sub-tensor and sums the results. 4. Computes and returns the gradient of this sum with respect to the original tensor. # Function Signature ```python import torch def tensor_operations_and_gradient_calculation(values: list) -> torch.Tensor: pass ``` # Input: - `values`: A list of 9 floating-point values to initialize the tensor. # Output: - A tensor of gradients corresponding to the elements of the original tensor. # Constraints: - The input list `values` will always contain exactly 9 elements. - The function should use PyTorch\'s automatic differentiation capabilities. - Ensure that the tensor is created with `requires_grad=True`. # Example: ```python values = [1.0, -2.0, 3.0, 4.0, -5.0, 6.0, 7.0, -8.0, 9.0] gradients = tensor_operations_and_gradient_calculation(values) # The output should be a tensor of gradients computed with respect to the original tensor. print(gradients) ``` # Explanation: 1. Create a 3x3 tensor with the provided values. 2. Extract the sub-tensor from elements `[0,0]`, `[0,1]`, `[1,0]`, `[1,1]`. 3. Compute the squared value of each element in the sub-tensor and sum these values. 4. Use PyTorch\'s autograd to compute the gradient of this sum with respect to the original tensor. Return this gradient tensor as the output. # Notes: - Use `torch.tensor` for tensor creation and ensure it has `requires_grad=True`. - Use slicing to extract the sub-tensor and PyTorch operations to perform the computations.","solution":"import torch def tensor_operations_and_gradient_calculation(values: list) -> torch.Tensor: # Create a 3x3 tensor from the input list with requires_grad=True tensor = torch.tensor(values, dtype=torch.float32).reshape(3, 3) tensor.requires_grad_(True) # Extract a 2x2 sub-tensor sub_tensor = tensor[:2, :2] # Compute the element-wise square of the sub-tensor and sum the results result = (sub_tensor ** 2).sum() # Compute the gradients result.backward() # Return the gradient of the original tensor return tensor.grad"},{"question":"# Question: Palindrome Checker with Detailed Report A **palindrome** is a word, phrase, number, or other sequence of characters which reads the same backward as forward (ignoring spaces, punctuation, and capitalization). Your task is to write a Python function that checks if a given string is a palindrome. Additionally, your function should return a detailed report containing: 1. The original string. 2. The sanitized string after removing spaces and punctuation and converting to lowercase. 3. The reversed sanitized string. 4. A boolean indicating whether the original string is a palindrome. **Function Signature:** ```python def palindrome_checker(input_string: str) -> dict: pass ``` # Input: - `input_string`: A single string containing alphanumeric characters, spaces, and punctuation. (1 <= len(input_string) <= 1000) # Output: - A dictionary with the following keys: - \'original\': The original input string. - \'sanitized\': The processed string after removing spaces and punctuation and converting to lowercase. - \'reversed\': The reversed version of the sanitized string. - \'is_palindrome\': A boolean value indicating whether the original string is a palindrome. # Constraints: - Ignore case, spaces, and punctuation when checking for palindrome. Only alphanumeric characters should be considered in the check. # Example: ```python input_string = \\"A man, a plan, a canal, Panama!\\" result = palindrome_checker(input_string) print(result) ``` **Expected Output:** ```python { \'original\': \\"A man, a plan, a canal, Panama!\\", \'sanitized\': \\"amanaplanacanalpanama\\", \'reversed\': \\"amanaplanacanalpanama\\", \'is_palindrome\': True } ``` # Notes: - You may use built-in string methods like `.lower()`, `.replace()`, `.isalnum()`, and slicing for string manipulation. - You can also use list comprehensions to filter out non-alphanumeric characters. Write a detailed implementation that adheres to the function signature and requirements outlined above.","solution":"import re def palindrome_checker(input_string: str) -> dict: Checks if the given string is a palindrome and returns a detailed report. Parameters: input_string (str): The original input string. Returns: dict: A dictionary containing: - \'original\': The original input string. - \'sanitized\': The processed string after removing spaces and punctuation and converting to lowercase. - \'reversed\': The reversed version of the sanitized string. - \'is_palindrome\': A boolean value indicating whether the original string is a palindrome. # Remove non-alphanumeric characters and convert to lowercase sanitized = \'\'.join([char.lower() for char in input_string if char.isalnum()]) # Reverse the sanitized string reversed_sanitized = sanitized[::-1] # Check if the sanitized string is a palindrome is_palindrome = sanitized == reversed_sanitized # Create the result dictionary result = { \'original\': input_string, \'sanitized\': sanitized, \'reversed\': reversed_sanitized, \'is_palindrome\': is_palindrome } return result"},{"question":"**Title: Advanced URL Fetching with `urllib.request`** **Objective:** Implement a Python function that fetches data from a given URL using the `urllib.request` module while handling potential errors, managing request headers, and setting specific timeouts. This involves creating custom handlers to manage different scenarios such as basic authentication and proxies. **Task:** Write a function `fetch_url_data(url: str, headers: dict = None, data: dict = None, timeout: int = 10) -> str` that: 1. Fetches data from the provided `url`. 2. Allows custom HTTP headers to be added to the request via the `headers` argument. 3. Supports sending data via POST requests if the `data` argument is provided. 4. Sets a request timeout specified by the `timeout` argument. 5. Handles any HTTP or URL errors, returning a descriptive message for each. 6. Optionally handles basic authentication if `username` and `password` are provided via environment variables `URL_USERNAME` and `URL_PASSWORD`. 7. Optionally disables proxy usage if an environment variable `DISABLE_PROXY` is set to `True`. **Input:** - `url`: (str) The URL to fetch data from. - `headers`: (dict, optional) A dictionary of HTTP headers to include in the request. - `data`: (dict, optional) A dictionary of data to be sent via a POST request. - `timeout`: (int, default=10) The timeout value for the request in seconds. **Output:** - Returns the fetched data as a string if the request is successful. - Returns an appropriate error message if the request fails. **Constraints:** - Do not use external libraries for HTTP requests other than `urllib`. - Handle both GET and POST requests appropriately. - Ensure the function handles network-related exceptions gracefully. **Performance Requirements:** - The function should handle large responses efficiently without excessive memory usage. - The timeout must be enforced to avoid hanging indefinitely on network issues. ```python import os import urllib.request import urllib.parse from urllib.error import URLError, HTTPError def fetch_url_data(url: str, headers: dict = None, data: dict = None, timeout: int = 10) -> str: try: # Prepare data for POST request if provided if data: data = urllib.parse.urlencode(data).encode(\'ascii\') # Build request object with headers if headers: req = urllib.request.Request(url, data=data, headers=headers) else: req = urllib.request.Request(url, data=data) # Handle optional authentication username = os.getenv(\'URL_USERNAME\') password = os.getenv(\'URL_PASSWORD\') if username and password: password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, url, username, password) handler = urllib.request.HTTPBasicAuthHandler(password_mgr) opener = urllib.request.build_opener(handler) urllib.request.install_opener(opener) # Handle optional proxy disabling if os.getenv(\'DISABLE_PROXY\', \'False\') == \'True\': proxy_support = urllib.request.ProxyHandler({}) opener = urllib.request.build_opener(proxy_support) urllib.request.install_opener(opener) # Set the timeout urllib.request.socket.setdefaulttimeout(timeout) # Fetch the URL with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except HTTPError as e: return f\'HTTP Error {e.code}: {e.reason}\' except URLError as e: return f\'URL Error: {e.reason}\' except Exception as e: return f\'An unexpected error occurred: {str(e)}\' # Example usage: # print(fetch_url_data(\'http://example.com\', headers={\'User-Agent\': \'Mozilla/5.0\'})) ``` **Evaluation Criteria:** - **Correctness:** The function should correctly handle both GET and POST requests, including custom headers and timeouts. - **Error Handling:** Proper handling of HTTP and URL errors, with descriptive messages. - **Code Quality:** Clean, readable, and well-documented code following best practices. - **Efficiency:** Efficient handling of large responses and network timeouts. - **Usage of `urllib.request`:** Demonstration of knowledge in using `urllib.request` for advanced URL fetching tasks.","solution":"import os import urllib.request import urllib.parse from urllib.error import URLError, HTTPError def fetch_url_data(url: str, headers: dict = None, data: dict = None, timeout: int = 10) -> str: try: # Prepare data for POST request if provided if data: data = urllib.parse.urlencode(data).encode(\'ascii\') # Build request object with headers if headers: req = urllib.request.Request(url, data=data, headers=headers) else: req = urllib.request.Request(url, data=data) # Handle optional authentication username = os.getenv(\'URL_USERNAME\') password = os.getenv(\'URL_PASSWORD\') if username and password: password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, url, username, password) handler = urllib.request.HTTPBasicAuthHandler(password_mgr) opener = urllib.request.build_opener(handler) urllib.request.install_opener(opener) # Handle optional proxy disabling if os.getenv(\'DISABLE_PROXY\', \'False\') == \'True\': proxy_support = urllib.request.ProxyHandler({}) opener = urllib.request.build_opener(proxy_support) urllib.request.install_opener(opener) # Set the timeout urllib.request.socket.setdefaulttimeout(timeout) # Fetch the URL with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except HTTPError as e: return f\'HTTP Error {e.code}: {e.reason}\' except URLError as e: return f\'URL Error: {e.reason}\' except Exception as e: return f\'An unexpected error occurred: {str(e)}\' # Example usage: # print(fetch_url_data(\'http://example.com\', headers={\'User-Agent\': \'Mozilla/5.0\'}))"},{"question":"# Task Description: You are given a dataset of car attributes. Using Seaborn\'s `sns.kdeplot` function, you are required to: 1. Plot the univariate distribution of the \'mpg\' (miles per gallon) attribute. 2. Plot the bivariate distribution of \'mpg\' and \'horsepower\', with \'origin\' column as the hue parameter. 3. Use the parameter `bw_adjust` to plot the \'mpg\' distribution with less smoothing (0.5) and more smoothing (2.0). 4. Normalize the bivariate distribution of \'mpg\' and \'weight\' at each value in the grid, with \'cylinders\' as the hue parameter. 5. Modify the appearance of the bivariate \'mpg\' and \'displacement\' plot to have filled contours, using the \'origin\' as the hue parameter, with the colormap set to \\"magma\\". # Dataset: Download the `mpg` dataset from the seaborn library using the following code: ```python import seaborn as sns mpg = sns.load_dataset(\\"mpg\\") ``` # Constraints: - Ensure all plots have appropriate labels and titles. - Use consistent color palettes for hue-based plots for better readability. - Save each plot as a `png` file named \'plot_<step>.png\', where <step> is the step number (e.g., \'plot_1.png\' for step 1). # Expected Output: You should submit the Python script containing the code for all the steps mentioned above, and ensure each plot is saved correctly as a PNG file. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Step 1: Plot the univariate distribution of \'mpg\' sns.kdeplot(data=mpg, x=\\"mpg\\") plt.title(\\"Univariate Distribution of MPG\\") plt.savefig(\'plot_1.png\') plt.clf() # Step 2: Plot the bivariate distribution of \'mpg\' and \'horsepower\' with \'origin\' as hue sns.kdeplot(data=mpg, x=\\"mpg\\", y=\\"horsepower\\", hue=\\"origin\\") plt.title(\\"Bivariate Distribution of MPG and Horsepower by Origin\\") plt.savefig(\'plot_2.png\') plt.clf() # Step 3: Plot the \'mpg\' distribution with less and more smoothing sns.kdeplot(data=mpg, x=\\"mpg\\", bw_adjust=0.5) plt.title(\\"Univariate Distribution of MPG with Less Smoothing\\") plt.savefig(\'plot_3.png\') plt.clf() sns.kdeplot(data=mpg, x=\\"mpg\\", bw_adjust=2.0) plt.title(\\"Univariate Distribution of MPG with More Smoothing\\") plt.savefig(\'plot_4.png\') plt.clf() # Step 4: Normalize bivariate distribution of \'mpg\' and \'weight\' with \'cylinders\' as hue sns.kdeplot(data=mpg, x=\\"mpg\\", y=\\"weight\\", hue=\\"cylinders\\", multiple=\\"fill\\") plt.title(\\"Normalized Bivariate Distribution of MPG and Weight by Cylinders\\") plt.savefig(\'plot_5.png\') plt.clf() # Step 5: Modify bivariate \'mpg\' and \'displacement\' plot to have filled contours, with \'origin\' as hue sns.kdeplot(data=mpg, x=\\"mpg\\", y=\\"displacement\\", hue=\\"origin\\", fill=True, cmap=\\"magma\\") plt.title(\\"Filled Contours of Bivariate Distribution of MPG and Displacement by Origin\\") plt.savefig(\'plot_6.png\') plt.clf() ``` # Notes: - Make sure your script runs without errors. - Include comments where necessary to explain your code. - Submit both the script and the PNG files for evaluation.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Step 1: Plot the univariate distribution of \'mpg\' sns.kdeplot(data=mpg, x=\\"mpg\\") plt.title(\\"Univariate Distribution of MPG\\") plt.xlabel(\\"MPG\\") plt.ylabel(\\"Density\\") plt.savefig(\'plot_1.png\') plt.clf() # Step 2: Plot the bivariate distribution of \'mpg\' and \'horsepower\' with \'origin\' as hue sns.kdeplot(data=mpg, x=\\"mpg\\", y=\\"horsepower\\", hue=\\"origin\\") plt.title(\\"Bivariate Distribution of MPG and Horsepower by Origin\\") plt.xlabel(\\"MPG\\") plt.ylabel(\\"Horsepower\\") plt.savefig(\'plot_2.png\') plt.clf() # Step 3: Plot the \'mpg\' distribution with less and more smoothing sns.kdeplot(data=mpg, x=\\"mpg\\", bw_adjust=0.5) plt.title(\\"Univariate Distribution of MPG with Less Smoothing\\") plt.xlabel(\\"MPG\\") plt.ylabel(\\"Density\\") plt.savefig(\'plot_3.png\') plt.clf() sns.kdeplot(data=mpg, x=\\"mpg\\", bw_adjust=2.0) plt.title(\\"Univariate Distribution of MPG with More Smoothing\\") plt.xlabel(\\"MPG\\") plt.ylabel(\\"Density\\") plt.savefig(\'plot_4.png\') plt.clf() # Step 4: Normalize bivariate distribution of \'mpg\' and \'weight\' with \'cylinders\' as hue sns.kdeplot(data=mpg, x=\\"mpg\\", y=\\"weight\\", hue=\\"cylinders\\", multiple=\\"fill\\") plt.title(\\"Normalized Bivariate Distribution of MPG and Weight by Cylinders\\") plt.xlabel(\\"MPG\\") plt.ylabel(\\"Weight\\") plt.savefig(\'plot_5.png\') plt.clf() # Step 5: Modify bivariate \'mpg\' and \'displacement\' plot to have filled contours, with \'origin\' as hue sns.kdeplot(data=mpg, x=\\"mpg\\", y=\\"displacement\\", hue=\\"origin\\", fill=True, cmap=\\"magma\\") plt.title(\\"Filled Contours of Bivariate Distribution of MPG and Displacement by Origin\\") plt.xlabel(\\"MPG\\") plt.ylabel(\\"Displacement\\") plt.savefig(\'plot_6.png\') plt.clf()"},{"question":"**Dynamic Class Creation and Type Verification Using types Module** Objective: Create a dynamic class and instantiate it. Verify the types of its methods and attributes using the `types` module. # Task 1. Use the `types.new_class` function to dynamically create a class `Person` with the following properties: - Attributes: `first_name`, `last_name` - Methods: - `full_name(self)`: Returns the full name in the format `\\"<first_name> <last_name>\\"`. 2. After creating the class, instantiate an object of this class with your name. 3. Verify and print the types of the following elements using the `types` module: - The `Person` class itself. - The `full_name` method. - The `first_name` attribute of the instance. - The `full_name` method bound to the instance. # Requirements 1. **Dynamic Class Creation** - You must use the `types.new_class` function to create the class. - Any additional methods should be dynamically added to the class. 2. **Type Verification** - Use `types.FunctionType` to check the type of the `full_name` method. - Use `types.MethodType` to check the type of the `full_name` method bound to the instance. - Use `type()` to check the type of the `first_name` attribute. # Input and Output - There are no function inputs for this task; create the class and instance within the code. - Print the types of the required elements as: - `Person class type: <type>` - `full_name method type: <type>` - `first_name attribute type: <type>` - `Bound full_name method type: <type>` # Example Your output should look similar to: ``` Person class type: <class \'type\'> full_name method type: <class \'function\'> first_name attribute type: <class \'str\'> Bound full_name method type: <class \'method\'> ``` # Constraints - Ensure that your implementation adheres to Python 3.10 features and structure. - Make sure to use the `types` module for type verification tasks. # Hints - You can define methods dynamically using a lambda function or regular function definition and then add it to the class namespace. - Remember to instantiate your class and populate the attributes while creating the instance.","solution":"import types # Define the full_name method def full_name(self): return f\\"{self.first_name} {self.last_name}\\" # Create the Person class dynamically Person = types.new_class(\\"Person\\", (object, )) # Add attributes and the method to the class Person.__init__ = lambda self, first_name, last_name: setattr(self, \'first_name\', first_name) or setattr(self, \'last_name\', last_name) Person.full_name = full_name # Instantiate an object of the Person class person_instance = Person(\\"John\\", \\"Doe\\") # Verify and print the types of the specified elements print(f\\"Person class type: {type(Person)}\\") print(f\\"full_name method type: {type(Person.full_name)}\\") print(f\\"first_name attribute type: {type(person_instance.first_name)}\\") print(f\\"Bound full_name method type: {type(person_instance.full_name)}\\")"},{"question":"Objective Implement a function using the asyncio library that fetches data from multiple dummy URLs concurrently, processes the fetched data, and combines the results into a single output. Function Signature ```python import asyncio async def fetch_and_process(urls: list, processing_func) -> list: Fetches data asynchronously from a list of URLs and processes it using a provided function. Args: urls: A list of strings, where each string is a URL to fetch data from. processing_func: An asynchronous function that takes a fetched data string and returns a processed result. Returns: A list of processed results corresponding to each URL. ``` Input - `urls` (list of str): A list of URL strings representing the URLs to fetch data from. - `processing_func` (async function): An asynchronous function that takes a single argument (the fetched data as a string) and returns the processed result. Output - A list containing the processed results of the data fetched from each URL. Constraints - The function should make use of asyncio to perform the fetching and processing concurrently. - Assume each fetch operation can be simulated with `asyncio.sleep` and returns the URL as the fetched data. - The number of URLs will not exceed 1000. - Each URL will return a response within 1 to 5 seconds. Example ```python import asyncio async def mock_processing_func(data): await asyncio.sleep(1) return f\\"processed_{data}\\" async def main(): urls = [\\"url1\\", \\"url2\\", \\"url3\\"] results = await fetch_and_process(urls, mock_processing_func) print(results) if __name__ == \\"__main__\\": asyncio.run(main()) # Expected Output (the order may vary depending on the concurrency): # [\'processed_url1\', \'processed_url2\', \'processed_url3\'] ``` Notes - Make use of `asyncio.gather` to run the fetching and processing concurrently. - Remember to use the `await` keyword appropriately when calling asynchronous functions.","solution":"import asyncio async def fetch(url): Simulates fetching data from a URL by sleeping for a random time between 1 and 5 seconds. await asyncio.sleep(1) return url async def fetch_and_process(urls: list, processing_func) -> list: Fetches data asynchronously from a list of URLs and processes it using a provided function. Args: urls: A list of strings, where each string is a URL to fetch data from. processing_func: An asynchronous function that takes a fetched data string and returns a processed result. Returns: A list of processed results corresponding to each URL. # Function to fetch and process a single URL async def fetch_and_process_single(url): fetched_data = await fetch(url) processed_data = await processing_func(fetched_data) return processed_data tasks = [fetch_and_process_single(url) for url in urls] results = await asyncio.gather(*tasks) return results"},{"question":"# Asynchronous Computation with PyTorch Futures You are provided with a list of asynchronous computations that need to be performed. These computations are encapsulated in functions that return `torch.futures.Future` objects. Your task is to write a function to handle these computations using the provided utility functions. Function Signature `def process_async_computations(computation_fns: List[Callable[[], torch.futures.Future]]) -> List[Any]:` Input - `computation_fns`: A list of callable functions, where each function when called returns a `torch.futures.Future` object. Output - A list of results from each computation in the same order as the input list. Requirements 1. All computations should be processed concurrently. 2. You must use the `collect_all` function from `torch.futures` to collect the results of all futures. 3. Ensure that the main function waits for all futures to complete and then returns their results. You can assume that each future, when completed, contains a single result which can be accessed via the `.wait()` method. Example ```python import torch.futures import time # Example computation functions that use torch.futures.Future def async_computation1(): future = torch.futures.Future() time.sleep(2) # Simulate a long computation future.set_result(10) return future def async_computation2(): future = torch.futures.Future() time.sleep(1) # Simulate a longer computation future.set_result(5) return future # Example usage of the function to be implemented def process_async_computations(computation_fns): futures = [fn() for fn in computation_fns] gathered_futures = torch.futures.collect_all(futures).wait() return [fut.wait() for fut in gathered_futures] # Test the function computation_fns = [async_computation1, async_computation2] results = process_async_computations(computation_fns) print(results) # Output should be [10, 5] ``` Constraints - The list of computation functions will have at most 50 functions. - Each computation function will complete within 10 seconds. Performance - The solution should efficiently handle the maximum number of computations within the given time constraints.","solution":"import torch from torch.futures import collect_all from typing import List, Callable, Any def process_async_computations(computation_fns: List[Callable[[], torch.futures.Future]]) -> List[Any]: Processes a list of asynchronous computation functions concurrently using torch.futures.Future. Args: computation_fns (List[Callable[[], torch.futures.Future]]): A list of functions that return a Future object. Returns: List[Any]: A list of results from each computation in the same order as the input list. # Call each function to start the asynchronous computation and collect all futures futures = [fn() for fn in computation_fns] # Wait for all futures to complete and gather their results gathered_futures = collect_all(futures).wait() # Extract and return the result from each future results = [fut.wait() for fut in gathered_futures] return results"},{"question":"# Pattern Matching and Filtering with `fnmatch` You are required to implement a function that takes a list of filenames and a pattern, and returns a list of filenames that match the pattern, excluding those that match an exclusion pattern. Your function should use the `fnmatch` module to perform the pattern matching. Function Signature ```python def filter_filenames(filenames: list, include_pattern: str, exclude_pattern: str) -> list: pass ``` Input - `filenames` (list of str): A list of filenames. - `include_pattern` (str): Pattern that filenames must match to be included. - `exclude_pattern` (str): Pattern that filenames must not match to be included in the result. Output - list of str: A list of filenames that match the `include_pattern` but do not match the `exclude_pattern`. Example ```python filenames = [\'data1.txt\', \'data2.txt\', \'readme.md\', \'script.py\', \'test_data.txt\'] include_pattern = \'*.txt\' exclude_pattern = \'test_*\' result = filter_filenames(filenames, include_pattern, exclude_pattern) # Expected result: [\'data1.txt\', \'data2.txt\'] ``` Constraints - The `filenames` list will contain no more than 10,000 entries. - Each filename will be a non-empty string of length at most 100 characters. - Patterns will be valid Unix shell-style wildcards. Requirements 1. The function should correctly handle the Unix shell-style wildcards as specified. 2. Efficiency is crucial; the implementation should minimize unnecessary processing. 3. The solution must handle case-sensitive and case-insensitive patterns correctly based on the function used from the `fnmatch` module. Use the relevant functions from the `fnmatch` module to implement `filter_filenames` effectively and efficiently.","solution":"import fnmatch def filter_filenames(filenames: list, include_pattern: str, exclude_pattern: str) -> list: Filters a list of filenames based on include and exclude patterns. :param filenames: List of filenames to filter. :param include_pattern: Pattern that filenames must match to be included. :param exclude_pattern: Pattern that filenames must not match to be included in the result. :return: List of filenames that match the include_pattern but do not match the exclude_pattern. included_files = fnmatch.filter(filenames, include_pattern) filtered_files = [file for file in included_files if not fnmatch.fnmatch(file, exclude_pattern)] return filtered_files"},{"question":"In this assessment, you are required to create a Python program that uses the `dataclasses` module to manage a simplified book library system. Requirements: 1. **Define Dataclasses**: - Create a dataclass `Book` with the following fields: - `title`: A string representing the title of the book. - `author`: A string representing the author of the book. - `pages`: An integer representing the number of pages in the book. - `published_year`: An integer representing the year the book was published. - `genres`: A list of strings representing genres of the book. Use a default factory to initialize this list. 2. **Special Features**: - Ensure that instances of `Book` are immutable. - Include `repr` and `eq` methods in the `Book` dataclass. - Define a method `age` within `Book` that returns the age of the book (based on the current year 2023). 3. **Library Dataclass**: - Create another dataclass `Library` with the following fields: - `name`: A string representing the name of the library. - `books`: A list of `Book` instances. Use a default factory to initialize this list. - Add methods to `Library` to: - Add a new book to the library. - Remove a book from the library by title. - Find all books by a specific author. - Get all books in a specific genre. 4. **Functions**: - Write a function `as_dict` that converts a `Library` instance (including its `Book` instances) to a dictionary. - Write a function `replace_book` that takes a `Library` instance, a book\'s title, and a `Book` object, and replaces the book with the given title in the library with the new book. Use the `replace` function from the `dataclasses` module. Constraints: - You must use the `dataclasses` module to implement the above requirements. - The `Book` class should not allow direct modification of its instances once created. - Ensure proper type annotations for all fields and methods. Input and Output Format - Input and output formats are free-form but should demonstrate all functionalities of implemented classes and methods through function calls and print statements. **Example Usage:** ```python from dataclasses import dataclass, field, replace from typing import List # Define Book and Library dataclasses here # Create books book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", pages=328, published_year=1949, genres=[\\"Dystopian\\", \\"Political Fiction\\"]) book2 = Book(title=\\"To Kill a Mockingbird\\", author=\\"Harper Lee\\", pages=281, published_year=1960, genres=[\\"Southern Gothic\\", \\"Bildungsroman\\"]) # Create library library = Library(name=\\"City Library\\") # Add books to library library.add_book(book1) library.add_book(book2) # Remove a book by title library.remove_book(\\"1984\\") # Find books by a specific author books_by_lee = library.find_books_by_author(\\"Harper Lee\\") print(books_by_lee) # Get all books of a specific genre dystopian_books = library.get_books_by_genre(\\"Dystopian\\") print(dystopian_books) # Convert library to dictionary library_dict = as_dict(library) print(library_dict) # Replace a book in the library new_book = Book(title=\\"Animal Farm\\", author=\\"George Orwell\\", pages=112, published_year=1945, genres=[\\"Political Satire\\"]) library = replace_book(library, \\"To Kill a Mockingbird\\", new_book) ``` Implement the above requirements and ensure your code passes various edge cases.","solution":"from dataclasses import dataclass, field, replace from typing import List import datetime @dataclass(frozen=True, repr=True, eq=True) class Book: title: str author: str pages: int published_year: int genres: List[str] = field(default_factory=list) def age(self) -> int: return datetime.datetime.now().year - self.published_year @dataclass class Library: name: str books: List[Book] = field(default_factory=list) def add_book(self, book: Book) -> None: self.books.append(book) def remove_book(self, title: str) -> bool: for book in self.books: if book.title == title: self.books.remove(book) return True return False def find_books_by_author(self, author: str) -> List[Book]: return [book for book in self.books if book.author == author] def get_books_by_genre(self, genre: str) -> List[Book]: return [book for book in self.books if genre in book.genres] def as_dict(library: Library) -> dict: return { \\"name\\": library.name, \\"books\\": [ { \\"title\\": book.title, \\"author\\": book.author, \\"pages\\": book.pages, \\"published_year\\": book.published_year, \\"genres\\": book.genres } for book in library.books ] } def replace_book(library: Library, title: str, new_book: Book) -> Library: new_books = [ new_book if book.title == title else book for book in library.books ] return replace(library, books=new_books)"},{"question":"# Task You are provided with a dataset and a machine learning model. Your objective is to: 1. Train a model on the provided dataset. 2. Generate and plot one-way and two-way Partial Dependence Plots (PDPs) for specified features. 3. Generate and plot Individual Conditional Expectation (ICE) plots for specified features. 4. Save the generated plots to disk. # Dataset An artificial dataset will be generated using the `make_friedman1` function from scikit-learn\'s `datasets` module. # Model Use a Gradient Boosting Regression model from scikit-learn\'s `ensemble` module. # Input and Output - **Input**: - No direct input parameters required. - **Output**: - Save the generated plots to disk in PNG format. - Files: - \\"one_way_pdp.png\\" - \\"two_way_pdp.png\\" - \\"ice_plot.png\\" # Performance Requirements - Ensure that the model training and plotting operations complete within a reasonable time frame. # Constraints - Use only scikit-learn for model training and plotting. - Ensure that the dataset generation and model training steps are reproducible with a fixed random state. # Example Here is the complete task split into steps: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_friedman1 from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay # Step 1: Generate dataset X, y = make_friedman1(n_samples=1000, random_state=0) # Step 2: Train model model = GradientBoostingRegressor(n_estimators=100, random_state=0) model.fit(X, y) # Step 3: Generate and plot one-way PDPs for features 0 and 1 features = [0, 1] fig, ax = plt.subplots(figsize=(12, 5)) PartialDependenceDisplay.from_estimator(model, X, features, ax=ax) plt.savefig(\\"one_way_pdp.png\\") # Step 4: Generate and plot two-way PDP for features (0, 1) features = [(0, 1)] fig, ax = plt.subplots(figsize=(6, 6)) PartialDependenceDisplay.from_estimator(model, X, features, ax=ax) plt.savefig(\\"two_way_pdp.png\\") # Step 5: Generate and plot ICE plot for feature 0 features = [0] fig, ax = plt.subplots(figsize=(6, 5)) PartialDependenceDisplay.from_estimator(model, X, features, kind=\'individual\', ax=ax) plt.savefig(\\"ice_plot.png\\") ``` Ensure you follow all the steps as described and save the plots with the specified file names.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_friedman1 from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay def generate_and_save_plots(): # Generate dataset X, y = make_friedman1(n_samples=1000, random_state=0) # Train model model = GradientBoostingRegressor(n_estimators=100, random_state=0) model.fit(X, y) # Generate and plot one-way PDPs for features 0 and 1 one_way_features = [0, 1] fig, ax = plt.subplots(figsize=(12, 5)) PartialDependenceDisplay.from_estimator(model, X, one_way_features, ax=ax) plt.savefig(\\"one_way_pdp.png\\") plt.close(fig) # Generate and plot two-way PDP for features (0, 1) two_way_features = [(0, 1)] fig, ax = plt.subplots(figsize=(6, 6)) PartialDependenceDisplay.from_estimator(model, X, two_way_features, ax=ax) plt.savefig(\\"two_way_pdp.png\\") plt.close(fig) # Generate and plot ICE plot for feature 0 ice_features = [0] fig, ax = plt.subplots(figsize=(6, 5)) PartialDependenceDisplay.from_estimator(model, X, ice_features, kind=\'individual\', ax=ax) plt.savefig(\\"ice_plot.png\\") plt.close(fig)"},{"question":"Objective Implement a function that demonstrates understanding of reference counting by simulating a Python-like reference management system. Problem Statement You are tasked with simulating a simplified version of Python\'s reference counting mechanism. Implement a class, `PyObjectManager`, which will manage the reference counts of Python objects. For simplicity, the Python objects will be represented by their IDs using the `id()` function. Your task is to implement the following methods: 1. `add_reference(obj_id: int)`: Simulate taking a new strong reference to the object. Increment the reference count of the object with the given ID. 2. `release_reference(obj_id: int)`: Release a strong reference to the object. Decrement the reference count of the object with the given ID. If the reference count reaches zero, remove the object from the managed set. 3. `get_reference_count(obj_id: int)`: Return the current reference count of the object with the given ID. If the object is not being managed, return zero. Input and Output - `add_reference(obj_id)`: - **Input:** `obj_id` (integer): The ID of the object. - **Output:** None - `release_reference(obj_id)`: - **Input:** `obj_id` (integer): The ID of the object. - **Output:** None - `get_reference_count(obj_id)`: - **Input:** `obj_id` (integer): The ID of the object. - **Output:** (integer) The current reference count of the object. Constraints - Object IDs will be positive integers. - You do not need to account for concurrent access to the reference manager. - This is a simulation, so handling of actual Python objects is not required. Example ```python manager = PyObjectManager() # Add references manager.add_reference(42) manager.add_reference(42) # Get reference count print(manager.get_reference_count(42)) # Output: 2 # Release a reference manager.release_reference(42) # Get reference count print(manager.get_reference_count(42)) # Output: 1 # Release the last reference manager.release_reference(42) # Get reference count print(manager.get_reference_count(42)) # Output: 0 ``` Implementation Implement the `PyObjectManager` class with the described methods.","solution":"class PyObjectManager: def __init__(self): self.ref_counts = {} def add_reference(self, obj_id: int): if obj_id in self.ref_counts: self.ref_counts[obj_id] += 1 else: self.ref_counts[obj_id] = 1 def release_reference(self, obj_id: int): if obj_id in self.ref_counts: self.ref_counts[obj_id] -= 1 if self.ref_counts[obj_id] == 0: del self.ref_counts[obj_id] def get_reference_count(self, obj_id: int) -> int: return self.ref_counts.get(obj_id, 0)"},{"question":"Objective You are required to implement a Python function that utilizes the `pkgutil` module to discover all submodules under a specified package and return their names in a sorted list. This task will test your understanding of module handling and the use of the `pkgutil` library. Function Signature ```python def list_submodules(package_name: str) -> list: pass ``` Input - `package_name` (str): The name of the package whose submodules need to be listed. Output - (list): A sorted list of submodule names. Constraints - The input package must be a valid, importable package in the Python environment where this code is executed. If the package is not found, the function should return an empty list. - You are not allowed to use any means other than `pkgutil` for discovering submodules. Example ```python import yourpackage def list_submodules(\'yourpackage\') -> list: return [\'yourpackage.module1\', \'yourpackage.module2\', \'yourpackage.subpackage.module3\'] ``` Instructions 1. Use `pkgutil.iter_modules` to discover all submodules under the specified package. 2. Handle possible exceptions that could arise from invalid package names or import errors. 3. Ensure the returned list is sorted alphabetically. Notes - Consider adding necessary import statements in your function. - You can create extra helper functions if required, but the main functionality should reside in the specified function signature. Good luck, and happy coding!","solution":"import pkgutil import importlib def list_submodules(package_name: str) -> list: try: package = importlib.import_module(package_name) except ModuleNotFoundError: return [] submodules = [] for module_info in pkgutil.iter_modules(package.__path__, package.__name__ + \\".\\"): submodules.append(module_info.name) return sorted(submodules)"},{"question":"# **Coding Assessment Question** **Objective**: Demonstrate your understanding of seaborn\'s palette manipulation and its application to create custom visualizations. **Question**: Write a Python function `create_custom_palette_plot(data, palette_name, num_colors)` that creates a scatter plot using seaborn. The function will take the following inputs: - `data`: a DataFrame containing at least two numerical columns named \'x\' and \'y\'. These columns will be used for the scatter plot\'s x and y axes, respectively. - `palette_name`: a string that represents the name of the palette to be used (e.g., \\"viridis\\", \\"Set2\\"). - `num_colors`: an integer representing the number of distinct colors to use from the palette. The function should: 1. Generate a color palette using `sns.mpl_palette()` based on the `palette_name` and `num_colors`. 2. Create a scatter plot using seaborn\'s `sns.scatterplot()` function, applying the generated palette to color the points. 3. Set appropriate labels for the axes and a title for the plot. **Input Format**: - `data`: A pandas DataFrame with at least two columns named \'x\' and \'y\'. - `palette_name`: A string denoting the palette name. - `num_colors`: An integer specifying the number of colors to retrieve from the palette. **Output Format**: - The function should display a scatter plot using seaborn. **Constraints**: - Ensure that the DataFrame has at least 4 rows. - The palette name should be valid in seaborn; otherwise, raise a ValueError. - `num_colors` should be a positive integer; otherwise, raise a ValueError. **Example**: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Example Data data = pd.DataFrame({ \'x\': [1, 2, 3, 4, 5], \'y\': [2, 3, 5, 7, 11] }) # Example Usage create_custom_palette_plot(data, \\"viridis\\", 5) ``` # **Function Signature** ```python def create_custom_palette_plot(data: pd.DataFrame, palette_name: str, num_colors: int) -> None: pass ``` # **Implementation Hint** - Use `sns.mpl_palette(palette_name, num_colors)` to generate the palette. - Use `sns.scatterplot` to create the scatter plot. - Adjust plot aesthetics such as title and labels for better readability.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_custom_palette_plot(data: pd.DataFrame, palette_name: str, num_colors: int) -> None: Creates a scatter plot using seaborn with a custom palette. Parameters: data (pd.DataFrame): A DataFrame containing at least two numerical columns named \'x\' and \'y\'. palette_name (str): The name of the palette to be used. num_colors (int): The number of distinct colors to use from the palette. # Check if the DataFrame has at least 4 rows if data.shape[0] < 4: raise ValueError(\\"The DataFrame must have at least 4 rows.\\") # Validate num_colors if not isinstance(num_colors, int) or num_colors <= 0: raise ValueError(\\"`num_colors` should be a positive integer.\\") # Try to generate the palette try: palette = sns.color_palette(palette_name, num_colors) except ValueError as e: raise ValueError(f\\"Invalid palette name: {palette_name}\\") from e # Create scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=data, x=\'x\', y=\'y\', palette=palette, legend=None) plt.xlabel(\\"X-axis\\") plt.ylabel(\\"Y-axis\\") plt.title(f\\"Scatter Plot with {palette_name} Palette\\") plt.show()"},{"question":"**Objective:** The goal of this exercise is to assess your understanding of PyTorch\'s tensor storage system by implementing a function that manipulates the storage of a given tensor. **Problem Statement:** You are provided with a tensor `t` of arbitrary dimensions. You need to implement a function `modify_storage(t: torch.Tensor) -> torch.Tensor` that: 1. Clones the underlying storage of the input tensor `t`. 2. Fills the cloned storage with a given pattern. Specifically, the pattern is to fill the storage with a sequence of increasing integers starting from 0, until the storage is full. 3. Uses the modified storage to create a new tensor that has the same shape and dtype as the original tensor `t`. 4. Returns the new tensor. **Function Signature:** ```python import torch def modify_storage(t: torch.Tensor) -> torch.Tensor: pass ``` **Constraints:** 1. The input tensor `t` can be of any dtype and any shape. 2. You must not alter the original tensor `t`. **Example:** ```python import torch # Original tensor t = torch.ones(3, 2) # Modify the storage of the tensor new_tensor = modify_storage(t) print(new_tensor) # Expected Output: # tensor([[0.0, 1.0], # [2.0, 3.0], # [4.0, 5.0]], dtype=torch.float32) ``` **Notes:** - You may use the `torch.UntypedStorage` methods such as `clone`, `fill_`, etc. - Ensure that the original tensor `t` remains unaltered after your function executes. - Remember that the storage is filled as a contiguous, one-dimensional array, so ensure your fill pattern respects this structure. **Performance Requirements:** - Your solution should handle tensors of large sizes efficiently, but real-time performance is not a primary concern. - Focus on correctness and proper manipulation of tensor storages as specified. Good luck!","solution":"import torch def modify_storage(t: torch.Tensor) -> torch.Tensor: # Clone the storage original_storage = t.storage() cloned_storage = original_storage.clone() # Fill the cloned storage with a sequence of increasing integers starting from 0 for i in range(cloned_storage.size()): cloned_storage[i] = i # Create a new tensor with the same shape and dtype as t, using the modified storage new_tensor = torch.tensor(cloned_storage, dtype=t.dtype).view(t.shape) return new_tensor"},{"question":"# HTML Entities Conversion Challenge Using the `html.entities` module in Python 3.10, you are required to implement a set of functions that handle conversions between HTML entities and Unicode characters. Task 1: Convert HTML Entities to Unicode Write a function `html_entities_to_unicode(html_text: str) -> str` that takes an HTML-encoded string and returns a Unicode string with all named HTML entities converted to their corresponding Unicode characters. **Constraints:** - You must handle both named HTML entities with and without the trailing semicolon. - Assume the input string contains only valid HTML entity names. **Input:** - `html_text`: A string containing HTML-encoded text. **Output:** - A Unicode string with HTML entities converted to Unicode characters. **Example:** ```python input: \\"An example with an HTML entity: &gt; and an incomplete one: &lt\\" output: \\"An example with an HTML entity: > and an incomplete one: <\\" ``` Task 2: Convert Unicode to HTML Entities Write a function `unicode_to_html_entities(unicode_text: str) -> str` that takes a Unicode string and returns a string where all characters that have corresponding named HTML entities are replaced by their HTML entity names. **Constraints:** - Only characters that have a named HTML entity representation should be converted. **Input:** - `unicode_text`: A string containing Unicode text. **Output:** - A string with certain Unicode characters replaced by their HTML entity names. **Example:** ```python input: \\"An example with special characters: > and <\\" output: \\"An example with special characters: &gt; and &lt;\\" ``` Performance Considerations: - Aim to make your function efficient in terms of both time and space complexity. - You may assume input strings are reasonably short, i.e., not exceeding a million characters. **Implementation Hints:** - Use the dictionaries `html5`, `name2codepoint`, and `codepoint2name` from the `html.entities` module for your conversions. - Pay attention to edge cases where entities might lack a trailing semicolon. Your implementation will be assessed based on correctness, performance, and code quality.","solution":"import html.entities def html_entities_to_unicode(html_text: str) -> str: Converts HTML entities in a string to their corresponding Unicode characters. named_entities = html.entities.html5 def entity_replacer(match): entity = match.group(1) if entity in named_entities: return named_entities[entity] return match.group(0) # Regex to find HTML entities in the text import re entity_pattern = re.compile(r\'&(#?w+);?\') return entity_pattern.sub(entity_replacer, html_text) def unicode_to_html_entities(unicode_text: str) -> str: Converts Unicode characters in a string to their corresponding named HTML entities. codepoint_to_name = {codepoint: \'&\' + name + \';\' for name, codepoint in html.entities.name2codepoint.items()} def char_replacer(char): codepoint = ord(char) if codepoint in codepoint_to_name: return codepoint_to_name[codepoint] return char return \'\'.join(char_replacer(char) for char in unicode_text)"},{"question":"Configuring PyTorch Logger **Objective:** Utilize PyTorch\'s logging system to dynamically configure logging levels and artifact visibility for different components within a PyTorch application. **Problem Statement:** You are tasked with implementing a class `TorchLoggerConfigurator` that allows for dynamic configuration of PyTorch\'s logging levels and artifact visibility. This class should provide functions to set the logging configuration using both the environment variable method and the direct Python API method. Additionally, it should allow for querying the current logging configuration. **Class Specification:** ```python class TorchLoggerConfigurator: def __init__(self): Initializes the logger configurator. pass def set_logging_via_env(self, config: str): Sets the logging configuration using the environment variable method. Arguments: config -- a string representing the logging configuration, e.g. \\"+dynamo,aot\\" pass def set_logging_via_api(self, component: str, level: int): Sets the logging level of a specific component using the Python API. Arguments: component -- the name of the component (e.g., \\"dynamo\\") level -- the log level as an integer (e.g., logging.DEBUG, logging.WARN) pass def enable_artifact(self, artifact: str): Enables a specific artifact through the environment variable method. Arguments: artifact -- the name of the artifact (e.g., \\"aot_graphs\\") pass def get_current_config(self): Returns the current logging configuration. Returns: A dictionary with components and their current log levels, and enabled artifacts. pass ``` # Constraints: - You must use the `torch._logging.set_logs` function for setting the log levels in `set_logging_via_api`. - The `set_logging_via_env` and `enable_artifact` methods should modify the environment variable `TORCH_LOGS`. - Assume the environment variable changes in the script will not affect the current state of the running program unless handled appropriately. # Expected Behavior: - Creating an instance of `TorchLoggerConfigurator` should initialize the configuration without any changes to the logging behavior. - Calling `set_logging_via_env(config)` should update the logging configuration based on the input string. - Calling `set_logging_via_api(component, level)` should set the logging level of specified components using the direct API method. - Calling `enable_artifact(artifact)` should adjust the environment settings to enable specific artifacts. - `get_current_config()` should return the current logging configurations as a dictionary where keys are component or artifact names, and values represent log levels or enabled status. Example: ```python import logging # Create a TorchLoggerConfigurator instance configurator = TorchLoggerConfigurator() # Set log level using environment variable configurator.set_logging_via_env(\\"+dynamo,-aot\\") # Set log level using the API configurator.set_logging_via_api(\\"dynamo\\", logging.DEBUG) # Enable an artifact configurator.enable_artifact(\\"aot_graphs\\") # Get current configuration print(configurator.get_current_config()) ``` This question will assess the student\'s ability to work with environment variables, use PyTorch\'s internal logging configuration functions, and manage dynamic settings in a Python application.","solution":"import os import logging import torch class TorchLoggerConfigurator: def __init__(self): Initializes the logger configurator. self.artifact_config = set() self.log_config = {} def set_logging_via_env(self, config: str): Sets the logging configuration using the environment variable method. Arguments: config -- a string representing the logging configuration, e.g. \\"+dynamo,aot\\" current_config = os.getenv(\\"TORCH_LOGS\\", \\"\\") new_config = \\",\\".join(filter(None, [current_config, config])) os.environ[\\"TORCH_LOGS\\"] = new_config self._parse_and_set_configs() def set_logging_via_api(self, component: str, level: int): Sets the logging level of a specific component using the Python API. Arguments: component -- the name of the component (e.g., \\"dynamo\\") level -- the log level as an integer (e.g., logging.DEBUG, logging.WARN) torch._logging.set_logs(**{component: level}) self.log_config[component] = level def enable_artifact(self, artifact: str): Enables a specific artifact through the environment variable method. Arguments: artifact -- the name of the artifact (e.g., \\"aot_graphs\\") self.artifact_config.add(artifact) os.environ[\\"TORCH_LOGS\\"] = os.getenv(\\"TORCH_LOGS\\", \\"\\") + f\\",{artifact}\\" def get_current_config(self): Returns the current logging configuration. Returns: A dictionary with components and their current log levels, and enabled artifacts. return {\'log_config\': self.log_config, \'artifact_config\': list(self.artifact_config)} def _parse_and_set_configs(self): Parses the TORCH_LOGS environment variable and updates the configurations. configs = os.getenv(\\"TORCH_LOGS\\", \\"\\").split(\',\') for config in configs: if config.startswith(\'+\'): component = config[1:] self.log_config[component] = logging.DEBUG elif config.startswith(\'-\'): component = config[1:] self.log_config[component] = logging.WARN else: self.artifact_config.add(config)"},{"question":"Objective: To design a Python function that fetches data from a given URL, processes the data, handles possible exceptions, and correctly uses HTTP headers. Problem Statement: You need to write a function `fetch_and_process_url(url: str, params: dict, headers: dict) -> dict` that performs the following actions: 1. Performs an HTTP GET request to the given `url` with the specified query parameters `params` and additional HTTP headers `headers`. 2. If the request is successful (status code 200): * Parse the JSON response body into a dictionary. * If the JSON response contains a key `\\"status\\"` with the value `\\"error\\"`, raise a `ValueError` with the message contained in the `\\"message\\"` key. * If the JSON response contains a key `\\"status\\"` with the value `\\"success\\"`, return the entire JSON response as a dictionary. 3. If the request results in a client error (status codes 400-499) or a server error (status codes 500-599), handle these using custom error messages. * For client errors, raise an `HTTPError` with the accompanying message: `f\\"Client Error {code} - {reason}\\"`. * For server errors, raise an `HTTPError` with the accompanying message: `f\\"Server Error {code} - {reason}\\"`. 4. Use custom User-Agent and Accept headers in the request to specify your application’s identity and accepted response format. Function Signature: ```python def fetch_and_process_url(url: str, params: dict, headers: dict) -> dict: pass ``` Example Usage: ```python url = \\"http://example.com/api\\" params = {\\"query\\": \\"python\\"} headers = { \\"User-Agent\\": \\"MyApp/1.0\\", \\"Accept\\": \\"application/json\\" } try: response_data = fetch_and_process_url(url, params, headers) print(response_data) except ValueError as ve: print(f\\"Value Error: {ve}\\") except HTTPError as he: print(f\\"HTTP Error: {he}\\") except URLError as ue: print(f\\"URL Error: {ue}\\") ``` Constraints: 1. Use the `urllib.request` library to make HTTP requests. 2. Assume the endpoint returns valid JSON responses. 3. Handle timeouts and provide meaningful error messages. 4. Ensure your function is robust and can handle various edge cases. Performance Requirements: - The function should handle network errors gracefully. - The function should not assume the structure of the JSON response beyond the specified keys.","solution":"import urllib.request import urllib.error import json def fetch_and_process_url(url: str, params: dict, headers: dict) -> dict: # Prepare URL with parameters query_string = urllib.parse.urlencode(params) full_url = f\\"{url}?{query_string}\\" # Create and prepare the request request = urllib.request.Request(full_url, headers=headers) try: # Perform the HTTP request with urllib.request.urlopen(request) as response: if response.status == 200: response_data = json.load(response) if response_data.get(\\"status\\") == \\"error\\": raise ValueError(response_data.get(\\"message\\", \\"Unknown error\\")) elif response_data.get(\\"status\\") == \\"success\\": return response_data else: raise urllib.error.HTTPError(full_url, response.status, response.reason, response.headers, None) except urllib.error.HTTPError as e: if 400 <= e.code < 500: raise urllib.error.HTTPError(full_url, e.code, f\\"Client Error {e.code} - {e.reason}\\", e.headers, None) elif 500 <= e.code < 600: raise urllib.error.HTTPError(full_url, e.code, f\\"Server Error {e.code} - {e.reason}\\", e.headers, None) else: raise e except urllib.error.URLError as e: raise urllib.error.URLError(f\\"URL Error: {e.reason}\\") except Exception as e: raise e"},{"question":"# Email Message Parsing and Construction **Objective:** Your task is to write a Python function that reads an email message from a string, splits it into headers and payload, and then reconstructs it properly using the `EmailMessage` class from the `email.message` module. The purpose of this task is to assess your understanding of email message manipulation using the `EmailMessage` class, including handling headers and converting between string and `EmailMessage` object formats. **Function Signature:** ```python def process_email_message(raw_email: str) -> str: This function takes an email message as a single string, parses it to extract headers and payload, manipulates a header, and then reconstructs it using the EmailMessage class. :param raw_email: A string representation of a raw email message. :return: A string representation of the processed email message. ``` **Input:** - `raw_email` (str): A raw email message as a string. **Output:** - Returns a string representation of the processed email message. **Constraints:** - The input email message will have at least one header and a payload. - You must utilize the `EmailMessage` class to parse and reconstruct the email. **Steps:** 1. **Parsing the Email:** - Use appropriate methods from the `email` package to parse the input string into an `EmailMessage` object. 2. **Manipulating Headers:** - Modify the headers as follows: - Add a new header: `X-Processed-By: YourName`. - If a header named `Subject` exists, change its value to `Processed: {original_subject}`. 3. **Reconstructing the Email:** - Convert the modified `EmailMessage` back into a string. 4. **Returning the Result:** - Return the string representation of the processed email message. **Example Usage:** ```python raw_email = From: example@example.com To: recipient@example.com Subject: Original Subject This is the body of the email. processed_email = process_email_message(raw_email) print(processed_email) ``` **Expected Output:** The function should return a string equivalent to: ```plaintext From: example@example.com To: recipient@example.com X-Processed-By: YourName Subject: Processed: Original Subject This is the body of the email. ``` Ensure your implementation adheres to the guidelines and can correctly manipulate and reconstruct email messages as specified.","solution":"from email import message_from_string from email.message import EmailMessage def process_email_message(raw_email: str) -> str: This function takes an email message as a single string, parses it to extract headers and payload, manipulates a header, and then reconstructs it using the EmailMessage class. :param raw_email: A string representation of a raw email message. :return: A string representation of the processed email message. # Parse raw email string into an EmailMessage object email_msg = message_from_string(raw_email) # Add the new header \'X-Processed-By: YourName\' your_name = \'YourName\' email_msg[\'X-Processed-By\'] = your_name # Modify the \'Subject\' header if it exists if \'Subject\' in email_msg: original_subject = email_msg[\'Subject\'] email_msg.replace_header(\'Subject\', f\'Processed: {original_subject}\') # Convert the modified EmailMessage object back into a string return email_msg.as_string()"},{"question":"**Coding Assessment Question: Clustering Analysis using Scikit-Learn** **Objective:** You are required to implement a clustering solution using the scikit-learn library. Your task is to preprocess the data, apply different clustering algorithms, tune their hyperparameters, and evaluate the clustering performance using various evaluation metrics. **Dataset:** We will use the \'Iris\' dataset, which is a well-known dataset in the machine learning community. It contains 150 samples of iris flowers, each described by four features: sepal length, sepal width, petal length, and petal width. The dataset is provided along with scikit-learn. **Tasks:** 1. **Data Preprocessing:** - Load the \'Iris\' dataset using `sklearn.datasets.load_iris`. - Standardize the features using `sklearn.preprocessing.StandardScaler`. 2. **Clustering:** Implement clustering using the following algorithms: - K-Means - DBSCAN - Agglomerative Clustering 3. **Hyperparameter Tuning and Model Training:** - For K-Means, tune the number of clusters (`n_clusters`) from 2 to 6. - For DBSCAN, tune the `eps` parameter from 0.1 to 1.0 with a step size of 0.2 and `min_samples` parameter as 5. - For Agglomerative Clustering, tune the number of clusters (`n_clusters`) from 2 to 6. 4. **Evaluation:** - Use `Adjusted Rand Index (ARI)` to evaluate the clustering performance against the true labels. 5. **Performance Comparison:** - Based on the ARI scores, compare the performance of different algorithms and their optimal hyperparameters. 6. **Visualization:** - Visualize the clustering results of the best-performing model using a scatter plot of the first two principal components obtained using `sklearn.decomposition.PCA`. **Requirements:** - Your solution should include code implementations for all the tasks mentioned above. - The code should be modular, with clear and concise functions for each task. - Include appropriate comments and documentation for better understanding. - Plot the results for better visualization and comparison. **Input and Output Formats:** - **Input:** The code will directly load the \'Iris\' dataset from `sklearn.datasets.load_iris`. - **Output:** ARI scores for different models, optimal hyperparameters, and a scatter plot for the best-performing clustering model. **Constraints:** - Use only the scikit-learn library for implementing clustering algorithms and evaluation metrics. - Pay attention to the scalability and performance of your solution. - Ensure the visualization is clear and informative. **Evaluation Criteria:** - Correctness and efficiency of the implementation. - Proper tuning and selection of hyperparameters. - Clarity of code and documentation. - Quality and interpretability of the visualization. **Example Code Structure:** ```python import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import adjusted_rand_score from sklearn.decomposition import PCA # Load and preprocess the data def load_and_preprocess_data(): # Load the dataset iris = load_iris() X, y = iris.data, iris.target # Standardize the features scaler = StandardScaler() X_std = scaler.fit_transform(X) return X_std, y # Implement clustering algorithms def apply_clustering(X, algorithm, **kwargs): if algorithm == \'kmeans\': model = KMeans(**kwargs) elif algorithm == \'dbscan\': model = DBSCAN(**kwargs) elif algorithm == \'agglomerative\': model = AgglomerativeClustering(**kwargs) else: raise ValueError(\\"Unknown algorithm specified\\") labels = model.fit_predict(X) return labels # Evaluate clustering performance def evaluate_clustering(y_true, y_pred): return adjusted_rand_score(y_true, y_pred) # Visualize clustering results def visualize_clustering(X, y_pred): pca = PCA(n_components=2) X_pca = pca.fit_transform(X) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, cmap=\'viridis\') plt.title(\\"Clustering Results\\") plt.xlabel(\\"Principal Component 1\\") plt.ylabel(\\"Principal Component 2\\") plt.show() # Main function def main(): X, y = load_and_preprocess_data() # Apply and evaluate KMeans best_score = -1 best_labels = None for n_clusters in range(2, 7): labels = apply_clustering(X, \'kmeans\', n_clusters=n_clusters) score = evaluate_clustering(y, labels) if score > best_score: best_score = score best_labels = labels print(f\\"Best K-Means ARI: {best_score}\\") # Apply and evaluate DBSCAN best_score = -1 best_labels = None for eps in [0.1, 0.3, 0.5, 0.7, 0.9]: labels = apply_clustering(X, \'dbscan\', eps=eps, min_samples=5) score = evaluate_clustering(y, labels) if score > best_score: best_score = score best_labels = labels print(f\\"Best DBSCAN ARI: {best_score}\\") # Apply and evaluate Agglomerative Clustering best_score = -1 best_labels = None for n_clusters in range(2, 7): labels = apply_clustering(X, \'agglomerative\', n_clusters=n_clusters) score = evaluate_clustering(y, labels) if score > best_score: best_score = score best_labels = labels print(f\\"Best Agglomerative Clustering ARI: {best_score}\\") # Visualize the best clustering results visualize_clustering(X, best_labels) if __name__ == \\"__main__\\": main() ```","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import adjusted_rand_score from sklearn.decomposition import PCA def load_and_preprocess_data(): Load and preprocess the Iris dataset by standardizing the features. iris = load_iris() X, y = iris.data, iris.target scaler = StandardScaler() X_std = scaler.fit_transform(X) return X_std, y def apply_clustering(X, algorithm, **kwargs): Apply the specified clustering algorithm to the data. if algorithm == \'kmeans\': model = KMeans(**kwargs) elif algorithm == \'dbscan\': model = DBSCAN(**kwargs) elif algorithm == \'agglomerative\': model = AgglomerativeClustering(**kwargs) else: raise ValueError(\\"Unknown algorithm specified\\") labels = model.fit_predict(X) return labels def evaluate_clustering(y_true, y_pred): Evaluate the clustering performance using Adjusted Rand Index (ARI). return adjusted_rand_score(y_true, y_pred) def visualize_clustering(X, y_pred): Visualize the clustering results using PCA. pca = PCA(n_components=2) X_pca = pca.fit_transform(X) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, cmap=\'viridis\') plt.title(\\"Clustering Results\\") plt.xlabel(\\"Principal Component 1\\") plt.ylabel(\\"Principal Component 2\\") plt.show() def main(): X, y = load_and_preprocess_data() # Evaluate K-Means best_kmeans_score = -1 best_kmeans_labels = None for n_clusters in range(2, 7): labels = apply_clustering(X, \'kmeans\', n_clusters=n_clusters) score = evaluate_clustering(y, labels) if score > best_kmeans_score: best_kmeans_score = score best_kmeans_labels = labels print(f\\"Best K-Means ARI: {best_kmeans_score}\\") # Evaluate DBSCAN best_dbscan_score = -1 best_dbscan_labels = None for eps in [0.1, 0.3, 0.5, 0.7, 0.9]: labels = apply_clustering(X, \'dbscan\', eps=eps, min_samples=5) score = evaluate_clustering(y, labels) if score > best_dbscan_score: best_dbscan_score = score best_dbscan_labels = labels print(f\\"Best DBSCAN ARI: {best_dbscan_score}\\") # Evaluate Agglomerative Clustering best_agg_score = -1 best_agg_labels = None for n_clusters in range(2, 7): labels = apply_clustering(X, \'agglomerative\', n_clusters=n_clusters) score = evaluate_clustering(y, labels) if score > best_agg_score: best_agg_score = score best_agg_labels = labels print(f\\"Best Agglomerative Clustering ARI: {best_agg_score}\\") # Visualize the best clustering results visualize_clustering(X, best_kmeans_labels) # for visualization, let\'s use K-Means if __name__ == \\"__main__\\": main()"},{"question":"**Complex Control Flow and Exception Handling in Python** **Objective:** Your task is to implement a function that processes a list of commands using various compound statements. Each command will either be a mathematical operation or a control command. You will need to employ \\"if\\", \\"for\\", \\"try\\", and \\"with\\" statements to handle the commands correctly. **Problem Statement:** You are given a list of commands where each command is a dictionary containing: - An `operation` key that will be either `\\"add\\"`, `\\"subtract\\"`, `\\"divide\\"`, or `\\"control\\"`. - Depending on the operation, the dictionary may contain additional keys: - `\\"value\\"`: A number on which the operation will be performed. - `\\"try\\"`: A boolean that indicates if the operation should be wrapped in a try block to handle exceptions. - `\\"with\\"`: A boolean that indicates if the operation should be wrapped in a with block (use a dummy context manager for this purpose). - `\\"divider\\"`: If the operation is `\\"divide\\"`, this key will hold the divisor. Implement the function `process_commands(commands: list) -> int` that processes each command and returns the final result after completing all commands. The initial result should be `0`. - If `operation` is `\\"add\\"`, add the value to the result. - If `operation` is `\\"subtract\\"`, subtract the value from the result. - If `operation` is `\\"divide\\"`, divide the result by `divider`. - If `operation` is `\\"control\\"` and `value` matches a certain pattern, double the result. Use appropriate exception handling for division by zero and any other errors, and incorporate the \\"with\\" statement as instructed. **Example Input:** ```python commands = [ {\\"operation\\": \\"add\\", \\"value\\": 10}, {\\"operation\\": \\"control\\", \\"value\\": (1, 2)}, {\\"operation\\": \\"subtract\\", \\"value\\": 5, \\"try\\": True}, {\\"operation\\": \\"divide\\", \\"divider\\": 2, \\"try\\": True}, ] ``` **Example Output:** ```python 10 ``` **DummyContextManager Class:** ```python class DummyContextManager: def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): return False ``` **Constraints:** - Use at least one of each compound statement (\\"if\\", \\"for\\", \\"try\\", \\"with\\", and \\"match\\") in your function. - Assume the input list size does not exceed 1000 commands. - Values are guaranteed to be integers and dividers are non-zero integers. **Performance Requirements:** - The implementation should be capable of handling the worst-case scenario within a reasonable time frame. **Hint:** - Use pattern matching to handle the control command pattern.","solution":"class DummyContextManager: def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): return False def process_commands(commands: list) -> int: result = 0 for command in commands: operation = command.get(\\"operation\\") if operation == \\"add\\": value = command.get(\\"value\\", 0) result += value elif operation == \\"subtract\\": value = command.get(\\"value\\", 0) should_try = command.get(\\"try\\", False) if should_try: try: result -= value except Exception as e: print(f\\"Exception encountered in subtract: {e}\\") else: result -= value elif operation == \\"divide\\": divider = command.get(\\"divider\\", 1) should_try = command.get(\\"try\\", False) should_with = command.get(\\"with\\", False) if should_with: with DummyContextManager(): if should_try: try: result //= divider except ZeroDivisionError: print(\\"Division by zero encountered.\\") else: result //= divider else: if should_try: try: result //= divider except ZeroDivisionError: print(\\"Division by zero encountered.\\") else: result //= divider elif operation == \\"control\\": value = command.get(\\"value\\", ()) match value: case (1, 2): result *= 2 return result"},{"question":"Objective You are tasked with implementing a function that utilizes the `glob` module to search for files based on specific patterns and conditions in a directory. The function needs to handle recursive searches and should be efficient in its operation. Function Signature ```python def search_files(pattern: str, root_directory: str = None, recursive: bool = False) -> list: pass ``` Input - `pattern` (str): A string containing the shell-style pattern to match file pathnames. - `root_directory` (str, optional): A string specifying the root directory for searching. Defaults to `None`, meaning the current directory. - `recursive` (bool, optional): A boolean flag indicating whether the search should be recursive. Defaults to `False`. Output - Returns a list of strings where each string is a pathname that matches the specified pattern. Constraints - The function should handle paths containing special characters by escaping them appropriately. - Recursive searches should be efficient, making use of iterators where possible. - The returned list should be sorted in ascending order. Example ```python # Assuming the directory structure: # /example-dir/ # ├── 1.gif # ├── 2.txt # ├── card.gif # ├── .hiddenfile # └── sub/ # └── 3.txt # Example usage: print(search_files(\'*.gif\', \'/example-dir\')) # Expected output: [\'/example-dir/1.gif\', \'/example-dir/card.gif\'] print(search_files(\'[0-9].*\', \'/example-dir\')) # Expected output: [\'/example-dir/1.gif\', \'/example-dir/2.txt\'] print(search_files(\'**/*.txt\', \'/example-dir\', recursive=True)) # Expected output: [\'/example-dir/2.txt\', \'/example-dir/sub/3.txt\'] ``` Notes - Ensure the results are sorted. - Use the `glob` module functions (`glob`, `iglob`, `escape`) to meet these requirements. - Consider the impact of large directory trees and optimize accordingly.","solution":"import glob import os def search_files(pattern: str, root_directory: str = None, recursive: bool = False) -> list: Searches for files matching the specified pattern in the given directory. Args: pattern (str): The pattern to match file pathnames. root_directory (str, optional): The root directory to search in. Defaults to None, which means the current directory. recursive (bool, optional): Whether to search recursively. Defaults to False. Returns: list: A sorted list of file pathnames that match the pattern. root_directory = root_directory or os.getcwd() full_pattern = os.path.join(root_directory, pattern) if recursive: matches = glob.iglob(full_pattern, recursive=True) else: matches = glob.glob(full_pattern) return sorted(matches)"},{"question":"<|Analysis Begin|> The provided documentation discusses how missing values, both numeric (NA) and datetime/timedelta (NaT), are represented in pandas. Understanding and handling missing values is crucial when working with real-world datasets, where such values are common. To craft a challenging yet clear and self-contained question, it is essential to: 1. Ensure the student has to work with both types of missing values. 2. Require implementation of a function that deals with missing values in various forms. 3. Assess the student\'s ability to manipulate and analyze data frames containing such values. 4. Specify input and output formats clearly. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective You are provided with a dataset containing both numeric and datetime data. Your task is to write a function to clean and analyze this dataset, specifically handling missing values properly. Function to Implement ```python def clean_and_analyze_data(df): Cleans the input DataFrame by handling missing values and performs a basic analysis. Parameters: - df (pd.DataFrame): Input pandas DataFrame containing columns \'A\' (numeric), \'B\' (datetime), and other columns. Returns: - result (dict): A dictionary containing: - \'cleaned_df\' (pd.DataFrame): DataFrame after handling missing values. - \'missing_count_A\' (int): Count of missing values in column \'A\'. - \'missing_count_B\' (int): Count of missing values in column \'B\'. - \'mean_A\' (float): Mean of column \'A\' after handling missing values. - \'earliest_B\' (datetime): Earliest date in column \'B\' after handling missing values. pass ``` Input - `df`: A pandas DataFrame with at least the following columns: - `A`: Numeric column which may contain missing values (NA). - `B`: Datetime column which may contain missing values (NaT). - Additional columns may be present but are not relevant for this task. Output - The function should return a dictionary with the following keys: - `cleaned_df`: A DataFrame where missing values in column `A` are replaced with the mean of non-missing values, and missing values in column `B` are replaced with the earliest non-missing date. - `missing_count_A`: An integer count of how many missing values were initially in column `A`. - `missing_count_B`: An integer count of how many missing values were initially in column `B`. - `mean_A`: A float representing the mean value of column `A` after missing values are handled. - `earliest_B`: A datetime representing the earliest date in column `B` after missing values are handled. Constraints - You may assume that the DataFrame input always contains columns \'A\' and \'B\'. - There will be at least one non-missing value in each column to compute the mean or earliest date. - Performance: The function should handle DataFrames with up to 1 million rows efficiently. Example ```python import pandas as pd import numpy as np data = { \'A\': [1, np.nan, 3, np.nan, 5], \'B\': [pd.Timestamp(\'2023-01-01\'), pd.NaT, pd.Timestamp(\'2023-01-03\'), pd.Timestamp(\'2023-01-01\'), pd.NaT], } df = pd.DataFrame(data) result = clean_and_analyze_data(df) print(result) # Expected output: # { # \'cleaned_df\': [1, 3, 3, 3, 5], [2023-01-01, 2023-01-01, 2023-01-03, 2023-01-01, 2023-01-01], # \'missing_count_A\': 2, # \'missing_count_B\': 2, # \'mean_A\': 3.0, # \'earliest_B\': Timestamp(\'2023-01-01 00:00:00\') # } ``` Feel free to add your imports and additional helper functions as needed.","solution":"import pandas as pd import numpy as np def clean_and_analyze_data(df): Cleans the input DataFrame by handling missing values and performs a basic analysis. Parameters: - df (pd.DataFrame): Input pandas DataFrame containing columns \'A\' (numeric), \'B\' (datetime), and other columns. Returns: - result (dict): A dictionary containing: - \'cleaned_df\' (pd.DataFrame): DataFrame after handling missing values. - \'missing_count_A\' (int): Count of missing values in column \'A\'. - \'missing_count_B\' (int): Count of missing values in column \'B\'. - \'mean_A\' (float): Mean of column \'A\' after handling missing values. - \'earliest_B\' (datetime): Earliest date in column \'B\' after handling missing values. # Calculate missing values count missing_count_A = df[\'A\'].isna().sum() missing_count_B = df[\'B\'].isna().sum() # Compute mean of A and earliest date of B mean_A = df[\'A\'].mean() earliest_B = df[\'B\'].min() # Fill missing values df[\'A\'].fillna(mean_A, inplace=True) df[\'B\'].fillna(earliest_B, inplace=True) # Prepare the results result = { \'cleaned_df\': df, \'missing_count_A\': missing_count_A, \'missing_count_B\': missing_count_B, \'mean_A\': mean_A, \'earliest_B\': earliest_B } return result"},{"question":"Objective Assess your understanding of the pandas extension array mechanism by creating a custom `ExtensionArray` and using it within a pandas DataFrame. Problem Statement You are required to implement a custom `ExtensionArray` class that can handle a sequence of prime numbers. Once implemented, you will use this custom array within a pandas DataFrame and perform specific operations. Instructions 1. **Create a Custom Extension Array Class (`PrimeNumberArray`):** - Implement a class `PrimeNumberArray` that extends `pandas.api.extensions.ExtensionArray`. - This class should be designed to store and operate on prime numbers. - Implement the necessary methods that allow this array to integrate smoothly with pandas DataFrames. The methods to implement include: - `_from_sequence` - `_from_factorized` - `astype` - `isna` - `take` - `copy` - (other methods as necessary for smooth functionality) 2. **Use the Custom Array in a DataFrame:** - Create a pandas DataFrame that includes columns with prime numbers stored in `PrimeNumberArray` instances. - Add another column containing some operation on the prime numbers (e.g., the square of the primes). 3. **Perform Operations:** - Demonstrate basic usage by selecting, inserting, and updating rows in the DataFrame. - Perform and display some DataFrame operations such as filtering to select rows based on prime number conditions. Expected Input and Output Formats **Input:** - No explicit input from the user; your code should define the DataFrame schema and fill it with data internally. **Output:** - The code should print the DataFrame after creation. - Display the DataFrame after performing insert and update operations. - Display the result after filtering rows based on some conditions (e.g., prime numbers less than a certain value). Constraints - You should ensure your `PrimeNumberArray` handles only prime numbers and raises appropriate errors for invalid inputs. - You can implement helper functions to check for prime numbers if needed. - Use Python 3.6+ and pandas version 1.0.0 or later. Implementation ```python import pandas as pd from pandas.api.extensions import ExtensionArray, ExtensionDtype class PrimeNumberDtype(ExtensionDtype): name = \'prime\' type = int kind = \'i\' # integer na_value = None @classmethod def construct_array_type(cls): return PrimeNumberArray class PrimeNumberArray(ExtensionArray): dtype = PrimeNumberDtype() def __init__(self, values): self._validate(values) self._data = values def _validate(self, values): # Helper function to check primality def is_prime(n): if n <= 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True for value in values: if not is_prime(value): raise ValueError(f\\"All values must be prime numbers, found {value}\\") def __getitem__(self, item): return self._data[item] def __len__(self): return len(self._data) @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): return cls(scalars) def isna(self): return [x is None for x in self._data] def take(self, indices, allow_fill=False, fill_value=None): # Simplified for the purpose of the example return PrimeNumberArray([self._data[i] for i in indices]) def copy(self): return PrimeNumberArray(self._data[:]) def astype(self, dtype, copy=True): return np.array(self._data, dtype=dtype, copy=copy) # Example usage: # Create a DataFrame using the custom PrimeNumberArray data = PrimeNumberArray([2, 3, 5, 7, 11]) df = pd.DataFrame({\'primes\': data}) df[\'squared_primes\'] = df[\'primes\']**2 # Perform some DataFrame operations print(df) # Insert a new row (note: for demonstration purposes, actual row insertion in pandas may differ) df.loc[5] = [13, 169] print(df) # Update a row df.loc[0, \'primes\'] = 17 print(df) # Filter rows filtered_df = df[df[\'primes\'] < 10] print(filtered_df) ``` Additional Notes - Implement and thoroughly test each method of `PrimeNumberArray`. - Ensure the array can be used naturally with pandas operations. - Handle edge cases, such as empty arrays or invalid prime numbers, gracefully.","solution":"import pandas as pd from pandas.api.extensions import ExtensionArray, ExtensionDtype import numpy as np class PrimeNumberDtype(ExtensionDtype): name = \'prime\' type = int kind = \'i\' # integer na_value = None @classmethod def construct_array_type(cls): return PrimeNumberArray class PrimeNumberArray(ExtensionArray): dtype = PrimeNumberDtype() def __init__(self, values): self._validate(values) self._data = np.array(values, dtype=int) def _validate(self, values): # Helper function to check primality def is_prime(n): if n <= 1: return False for i in range(2, int(n ** 0.5) + 1): if n % i == 0: return False return True for value in values: if not is_prime(value): raise ValueError(f\\"All values must be prime numbers, found {value}\\") def __getitem__(self, item): if isinstance(item, int): return self._data[item] elif isinstance(item, slice): return PrimeNumberArray(self._data[item]) elif isinstance(item, (np.ndarray, pd.api.indexers.BaseIndexer, list)): return PrimeNumberArray(self._data[item]) else: raise IndexError(\\"Invalid index type\\") def __len__(self): return len(self._data) def __repr__(self): return f\\"PrimeNumberArray({self._data})\\" @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): return cls(scalars) def isna(self): return np.zeros(len(self._data), dtype=bool) def take(self, indices, allow_fill=False, fill_value=None): result = self._data.take(indices) if allow_fill: mask = indices == -1 result[mask] = fill_value return PrimeNumberArray(result) def copy(self): return PrimeNumberArray(self._data.copy()) def astype(self, dtype, copy=True): return np.array(self._data, dtype=dtype, copy=copy)"},{"question":"You are required to implement a function that wraps around the built-in `open()` function to create a custom file handler. This custom file handler should read the contents of a file and reverse the order of lines in the file. Function Signature ```python def reverse_lines_in_file(filename: str) -> list: This function opens a file specified by the filename, reads its contents, and returns a list of strings where each string is a line from the file in reverse order. Parameters: filename (str): The name of the file to be opened and read. Returns: list: A list of strings, each string being a line from the file, in reverse order. Example: Let\'s say the file \'sample.txt\' contains the following lines: ``` Hello World Python is amazing Builtins module ``` Then, calling reverse_lines_in_file(\'sample.txt\') should return: ``` [\\"Builtins module\\", \\"Python is amazing\\", \\"Hello World\\"] ``` Constraints: - The file will contain only text lines, without empty lines. - The maximum number of lines in the file will not exceed 1000. - Each line will have a maximum length of 200 characters. Edge Cases: - If the file is empty, return an empty list. - Handle any file-related exceptions gracefully. ``` Example ```python # Assume \'example.txt\' has the following content: # Line one # Line two # Line three result = reverse_lines_in_file(\'example.txt\') print(result) # Output: [\'Line three\', \'Line two\', \'Line one\'] ``` Additional Requirements - Use the `builtins` module to ensure that the built-in `open()` function is used. - Implement error handling to manage potential issues such as file not found, permissions error, etc. - Only read the contents of the file once and process the lines in memory.","solution":"import builtins def reverse_lines_in_file(filename: str) -> list: This function opens a file specified by the filename, reads its contents, and returns a list of strings where each string is a line from the file in reverse order. Parameters: filename (str): The name of the file to be opened and read. Returns: list: A list of strings, each string being a line from the file, in reverse order. try: with builtins.open(filename, \'r\') as file: lines = file.readlines() # Remove newline characters and reverse lines reversed_lines = [line.strip() for line in reversed(lines)] return reversed_lines except FileNotFoundError: print(f\\"Error: The file \'{filename}\' was not found.\\") return [] except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"# Advanced Date and Collections Management You are tasked with developing a Python module for managing events. Each event has a title, date, and priority. You need to develop functionality to add events, fetch upcoming events within a certain number of days, and List all events sorted by their date/time and priority. Requirements: 1. **Event Class**: - Create an `Event` class with the following attributes: - `title` (str): Title of the event. - `event_date` (datetime): Date and time of the event (should handle both aware and naive datetime objects). - `priority` (int): Priority of the event with lower values denoting higher priority. - Implement necessary methods for comparison so that events can be sorted by date first, and by priority next. 2. **EventManager Class**: - Create a class named `EventManager` to manage events. This class should have: - An internal list to store Event objects. - Methods to: - `add_event(event)`: Add an event to the manager. - `upcoming_events(days)`: Return a list of events happening in the next `days` days, sorted by event date and priority, in the format `[(title, event_date, priority), ...]`. - `list_events()`: List all stored events sorted by event_date and priority, in the format `[(title, event_date, priority), ...]`. Constraints: - Event dates should be considered in local time. If the provided `event_date` is naïve, assume it to be in the system\'s local timezone. - Priorities range from 1 (highest) to 5 (lowest). *Performance Requirements:* - The `upcoming_events()` method should be efficient and able to handle up to 10,000 events. Example: ```python from datetime import datetime, timedelta # Create some sample events event1 = Event(\\"Meeting\\", datetime.now() + timedelta(days=1), 2) event2 = Event(\\"Conference\\", datetime.now() + timedelta(days=3), 1) event3 = Event(\\"Hackathon\\", datetime.now(), 3) # Initialize the EventManager and add events manager = EventManager() manager.add_event(event1) manager.add_event(event2) manager.add_event(event3) # List all events print(manager.list_events()) # Fetch upcoming events within the next 2 days print(manager.upcoming_events(2)) ``` Expected Output: ```python [ (\\"Hackathon\\", datetime(2023, 10, 2, 10, 0), 3), (\\"Meeting\\", datetime(2023, 10, 3, 10, 0), 2), (\\"Conference\\", datetime(2023, 10, 5, 10, 0), 1) ] [ (\\"Hackathon\\", datetime(2023, 10, 2, 10, 0), 3), (\\"Meeting\\", datetime(2023, 10, 3, 10, 0), 2) ] ``` Ensure your code is well-structured, properly handles date-time objects, and includes relevant error handling where necessary.","solution":"from datetime import datetime, timedelta import pytz class Event: def __init__(self, title, event_date, priority): self.title = title self.event_date = event_date self.priority = priority def __lt__(self, other): if self.event_date == other.event_date: return self.priority < other.priority return self.event_date < other.event_date def __eq__(self, other): return (self.title == other.title and self.event_date == other.event_date and self.priority == other.priority) class EventManager: def __init__(self): self.events = [] def add_event(self, event): self.events.append(event) self.events.sort() # Maintain sorted order on each addition def upcoming_events(self, days): now = datetime.now(pytz.utc) end_date = now + timedelta(days=days) upcoming = [event for event in self.events if now <= event.event_date <= end_date] return [(e.title, e.event_date, e.priority) for e in sorted(upcoming)] def list_events(self): return [(e.title, e.event_date, e.priority) for e in sorted(self.events)]"},{"question":"# Question: Analyzing Sales Data with pandas You are provided with two datasets: 1. **sales_data.csv**: Contains daily sales records with columns \'date\', \'product_id\', \'store_id\', \'sales_volume\'. 2. **product_info.csv**: Contains product information with columns \'product_id\', \'category\', \'price\'. Your task is to write functions to perform the following operations: 1. **Load Data**: Implement a function `load_data(sales_file: str, product_file: str)`: - Reads CSV files `sales_file` and `product_file` into pandas DataFrames. - Returns two DataFrames: `sales_df` and `product_df`. 2. **Filter Sales Data**: Implement a function `filter_sales_data(sales_df: pd.DataFrame, start_date: str, end_date: str)`: - Filters `sales_df` to include only the records between `start_date` and `end_date` (inclusive). - Returns the filtered DataFrame. 3. **Daily Sales per Category**: Implement a function `calculate_daily_sales_per_category(filtered_sales_df: pd.DataFrame, product_df: pd.DataFrame)`: - Merges `filtered_sales_df` with `product_df` on `product_id` to include product categories in the sales data. - Groups the merged DataFrame by \'date\' and \'category\'. - Calculates the total daily sales volume and total revenue (sales_volume * price) for each category. - Returns a DataFrame with columns \'date\', \'category\', \'total_sales_volume\', \'total_revenue\'. 4. **Top N Products**: Implement a function `get_top_n_products(daily_sales_df: pd.DataFrame, n: int)`: - Groups `daily_sales_df` by \'product_id\' and calculates the total sales volume for each product over the given period. - Returns a DataFrame with the top `n` products based on total sales volume, including columns \'product_id\', \'total_sales_volume\'. # Constraints: - Assume the input CSV files are well-formed and have no missing values. - Dates in the \'date\' column are in the format \'YYYY-MM-DD\'. - The output should be optimized to handle large datasets efficiently. - Utilize appropriate pandas functions and methods to ensure efficient data processing. # Example Usage: ```python sales_df, product_df = load_data(\'data/sales_data.csv\', \'data/product_info.csv\') filtered_sales_df = filter_sales_data(sales_df, \'2023-01-01\', \'2023-01-31\') daily_sales_df = calculate_daily_sales_per_category(filtered_sales_df, product_df) top_products_df = get_top_n_products(daily_sales_df, 5) print(top_products_df) ``` # Expected Output: ```python # Example output format; actual values will vary based on input data product_id total_sales_volume 0 1 15000 1 2 14500 2 3 14000 3 4 13500 4 5 13000 ``` # Implementation Requirements: - Ensure your functions are well-documented and include type hints. - Handle any potential edge cases not covered by the constraints. - Use pandas best practices for efficient data manipulation.","solution":"import pandas as pd from typing import Tuple def load_data(sales_file: str, product_file: str) -> Tuple[pd.DataFrame, pd.DataFrame]: Reads CSV files into pandas DataFrames. Parameters: sales_file (str): Path to the sales data CSV file. product_file (str): Path to the product information CSV file. Returns: Tuple[pd.DataFrame, pd.DataFrame]: The sales and product DataFrames. sales_df = pd.read_csv(sales_file) product_df = pd.read_csv(product_file) return sales_df, product_df def filter_sales_data(sales_df: pd.DataFrame, start_date: str, end_date: str) -> pd.DataFrame: Filters the sales data to include only records between start_date and end_date (inclusive). Parameters: sales_df (pd.DataFrame): The sales DataFrame. start_date (str): The start date in \'YYYY-MM-DD\' format. end_date (str): The end date in \'YYYY-MM-DD\' format. Returns: pd.DataFrame: The filtered sales DataFrame. filtered_df = sales_df[(sales_df[\'date\'] >= start_date) & (sales_df[\'date\'] <= end_date)] return filtered_df def calculate_daily_sales_per_category(filtered_sales_df: pd.DataFrame, product_df: pd.DataFrame) -> pd.DataFrame: Calculates the daily sales volume and revenue per category. Parameters: filtered_sales_df (pd.DataFrame): The filtered sales DataFrame. product_df (pd.DataFrame): The product information DataFrame. Returns: pd.DataFrame: DataFrame with columns \'date\', \'category\', \'total_sales_volume\', and \'total_revenue\'. merged_df = pd.merge(filtered_sales_df, product_df, on=\'product_id\') grouped_df = merged_df.groupby([\'date\', \'category\']).agg( total_sales_volume=(\'sales_volume\', \'sum\'), total_revenue=(\'price\', lambda x: (x * merged_df.loc[x.index, \'sales_volume\']).sum()) ).reset_index() return grouped_df def get_top_n_products(daily_sales_df: pd.DataFrame, n: int) -> pd.DataFrame: Gets the top N products based on total sales volume over the given period. Parameters: daily_sales_df (pd.DataFrame): The daily sales DataFrame with categories included. n (int): The number of top products to return. Returns: pd.DataFrame: DataFrame with the top N products and their total sales volume. total_sales_df = daily_sales_df.groupby(\'product_id\').agg(total_sales_volume=(\'total_sales_volume\', \'sum\')).reset_index() top_products_df = total_sales_df.nlargest(n, \'total_sales_volume\') return top_products_df"},{"question":"You are tasked with developing a Python script to handle both compression and decompression of text files using the `lzma` module. Additionally, you need to utilize custom filter chains for compression to optimize the compression process. Requirements: 1. Write a function `compress_file(input_file: str, output_file: str) -> None` to compress the file specified by `input_file` and write the compressed content to `output_file`. The function should use a custom filter chain that includes: - A delta filter with a distance of 4. - An LZMA2 filter with a preset level of 9. 2. Write a function `decompress_file(input_file: str, output_file: str) -> None` to decompress the file specified by `input_file` and write the decompressed content to `output_file`. Expected Input/Output: - The `input_file` and `output_file` for both functions are expected to be file paths as strings. - The methods should handle any potential exceptions (e.g., `lzma.LZMAError`) and print relevant error messages. - Ensure that the functions appropriately handle files in binary format. Constraints: - The input texts can be quite large, so be mindful of memory usage and performance implications. - Utilize the `lzma.open()` method for file handling and custom filter chains as described in the documentation. Example Usage: ```python # Example usage of the functions compress_file(\\"example.txt\\", \\"example.txt.xz\\") decompress_file(\\"example.txt.xz\\", \\"decompressed_example.txt\\") ``` Notes: - The custom filter chain should ensure efficient use of memory and CPU resources while maximizing compression. - Make sure the script is compatible with Python 3.6 and above. Implement the functions `compress_file` and `decompress_file` as specified. ```python def compress_file(input_file: str, output_file: str) -> None: # Your implementation here pass def decompress_file(input_file: str, output_file: str) -> None: # Your implementation here pass ``` # Assessment Criteria: - Correct implementation of file compression and decompression using the `lzma` module. - Proper handling and application of custom filter chains. - Efficient and clear code with proper exception handling.","solution":"import lzma def compress_file(input_file: str, output_file: str) -> None: try: with open(input_file, \'rb\') as f_in: with lzma.open(output_file, \'wb\', format=lzma.FORMAT_XZ, filters=[{\'id\': lzma.FILTER_DELTA, \'dist\': 4}, {\'id\': lzma.FILTER_LZMA2, \'preset\': 9}]) as f_out: f_out.write(f_in.read()) except Exception as e: print(f\\"An error occurred during compression: {e}\\") def decompress_file(input_file: str, output_file: str) -> None: try: with lzma.open(input_file, \'rb\') as f_in: with open(output_file, \'wb\') as f_out: f_out.write(f_in.read()) except lzma.LZMAError as e: print(f\\"An error occurred during decompression: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"**Objective:** Implement a function that demonstrates your understanding of importing and manipulating modules in Python using the given C-API functions. **Question:** You are required to write a Python function that will: 1. Import a module by name. 2. Reload the module if it is already imported. 3. Access and modify a specific attribute in the module. 4. Return the modified module object. Your task is to implement the function `import_and_modify_module(module_name, attribute_name, new_value)`. **Function Signature:** ```python def import_and_modify_module(module_name: str, attribute_name: str, new_value) -> object: pass ``` **Input:** - `module_name` (str): The name of the module to be imported. - `attribute_name` (str): The name of the attribute within the module that needs to be modified. - `new_value`: The new value to be assigned to the attribute. **Output:** - Returns the modified module object. **Constraints:** 1. If the module cannot be imported, the function should raise an `ImportError`. 2. If the attribute does not exist in the module, the function should raise an `AttributeError`. 3. Use the appropriate C-API functions to handle module imports and reloading. 4. Demonstrate error handling for import and attribute access. 5. The code should properly handle references and memory management through proper API calls. **Example Usage:** ```python # Assume mymodule.py contains: # my_attribute = 10 modified_module = import_and_modify_module(\\"mymodule\\", \\"my_attribute\\", 20) assert modified_module.my_attribute == 20 ``` In this example: - The module named `\\"mymodule\\"` is imported. - If the module was already imported, it would be reloaded. - The attribute `\\"my_attribute\\"` within the module is set to `20`. - The function returns the modified module object. Performance Requirements: - The function should efficiently handle the module importing and reloading process. - Properly manage memory and references to ensure no leaks or dangling pointers. **Note:** Make sure you handle all specified exceptions and provide a clear, functional implementation using the given C-API functionalities.","solution":"import importlib def import_and_modify_module(module_name: str, attribute_name: str, new_value) -> object: Imports a module by name, reloads it if already imported, and modifies a specific attribute in the module with the new value. Parameters: - module_name (str): The name of the module to be imported. - attribute_name (str): The name of the attribute within the module to be modified. - new_value: The new value to assign to the attribute. Returns: - The modified module object. Raises: - ImportError: If the module cannot be imported. - AttributeError: If the attribute does not exist in the module. try: module = importlib.import_module(module_name) module = importlib.reload(module) except ImportError as e: raise ImportError(f\\"Could not import the module \'{module_name}\': {e}\\") if not hasattr(module, attribute_name): raise AttributeError(f\\"Module \'{module_name}\' does not have an attribute \'{attribute_name}\'\\") setattr(module, attribute_name, new_value) return module"},{"question":"You have been provided with a dataset of automobile specifications and their various attributes. Your task is to analyze the relationship between engine size and horsepower through visualizations using seaborn\'s KDE plots. Dataset Use the following code to load the dataset: ```python import seaborn as sns autos = sns.load_dataset(\'mpg\').dropna() ``` Tasks 1. **Basic KDE Plot**: - Plot a basic KDE plot showing the distribution of `horsepower`. Set the title to \\"KDE of Horsepower\\". 2. **Conditional KDE Plot**: - Plot KDEs of `horsepower` conditioned on `origin` (use `hue=\'origin\'`). Set the title to \\"Horsepower Distribution by Origin\\". 3. **Bivariate KDE Plot**: - Create a bivariate KDE plot to show the joint distribution of `horsepower` and `weight`. Apply logarithmic scaling to both axes. Set the title to \\"Bivariate KDE of Horsepower and Weight with Log Scaling\\". 4. **Customized KDE Plot**: - Develop a KDE plot for `horsepower` with: - Hue mapped to `cylinders`. - Normalize the stacked distributions (\\"multiple=\'fill\'\\"). - Adjust the bandwidth to 0.5. - Fill the plot areas and set color palette to \'viridis\'. - Set the title to \\"Customized KDE: Normalized Horsepower by Cylinders\\". 5. **Multivariate KDE Plot with Custom Appearance**: - Create a bivariate KDE plot displaying `horsepower` versus `weight`, with: - Hue mapped to `origin`. - Display filled contours with `levels=30` and `cmap=\'mako\'`. - Set axis labels to \\"Horsepower\\" and \\"Weight\\". - Set the title to \\"Multivariate KDE: Horsepower vs Weight by Origin\\". Input and Output - **Input**: - You\'re provided with the dataset `autos` loaded as described. - **Output**: - Five seaborn KDE plots, saved as images or displayed within your notebook. Constraints - You are required to use seaborn\'s KDE plotting capabilities. - Optimize and customize the plots as specified. Performance - Code must execute efficiently within a standard runtime environment. - Visualizations must be clear, with appropriate titles and labels.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load dataset autos = sns.load_dataset(\'mpg\').dropna() def plot_kde_horsepower(): plt.figure(figsize=(10, 6)) sns.kdeplot(autos[\'horsepower\']) plt.title(\'KDE of Horsepower\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Density\') plt.show() def plot_kde_horsepower_by_origin(): plt.figure(figsize=(10, 6)) sns.kdeplot(data=autos, x=\'horsepower\', hue=\'origin\') plt.title(\'Horsepower Distribution by Origin\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Density\') plt.show() def plot_bivariate_kde_horsepower_weight(): plt.figure(figsize=(10, 6)) sns.kdeplot( data=autos, x=\'horsepower\', y=\'weight\', log_scale=True ) plt.title(\'Bivariate KDE of Horsepower and Weight with Log Scaling\') plt.xlabel(\'Horsepower (log scale)\') plt.ylabel(\'Weight (log scale)\') plt.show() def plot_customized_kde_horsepower_by_cylinders(): plt.figure(figsize=(10, 6)) sns.kdeplot( data=autos, x=\'horsepower\', hue=\'cylinders\', multiple=\'fill\', bw_adjust=0.5, fill=True, palette=\'viridis\' ) plt.title(\'Customized KDE: Normalized Horsepower by Cylinders\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Proportion\') plt.show() def plot_multivariate_kde_horsepower_weight_by_origin(): plt.figure(figsize=(10, 6)) sns.kdeplot( data=autos, x=\'horsepower\', y=\'weight\', hue=\'origin\', levels=30, fill=True, cmap=\'mako\' ) plt.title(\'Multivariate KDE: Horsepower vs Weight by Origin\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Weight\') plt.show()"},{"question":"**Assignment: Implement a Class with Advanced Attribute and Iteration Handling** # Objective Your task is to implement a Python class named `AdvancedObject` that demonstrates advanced attribute handling and supports iteration protocols. # Requirements 1. **Attribute Handling**: - Implement a method, `get_all_attributes`, which returns a dictionary of all attributes and their values for the object. - Implement a method, `delete_attribute`, which deletes a specified attribute. If the attribute does not exist, the method should return `False`, otherwise, return `True`. 2. **Comparison Operations**: - Implement rich comparison methods (`__lt__`, `__le__`, `__eq__`, `__ne__`, `__gt__`, `__ge__`) for the class, comparing objects based on a specified attribute named `value`. 3. **Iteration Handling**: - The class should support synchronous iteration (via `__iter__`), where each iteration yields attributes\' names of the object. - Additionally, implement asynchronous iteration (via `__aiter__` and `__anext__`), where each iteration yields attributes\' names of the object. # Input and Output Formats - For `get_all_attributes`: - **Input**: No input parameter. - **Output**: A dictionary representing all the attributes of the object. - For `delete_attribute`: - **Input**: A string representing the attribute name. - **Output**: A boolean indicating whether the attribute was successfully deleted. # Example Usage ```python class AdvancedObject: def __init__(self, value): self.value = value def get_all_attributes(self): # Implement this method pass def delete_attribute(self, attr_name): # Implement this method pass def __lt__(self, other): # Implement this method pass # Implement other rich comparison methods (__le__, __eq__, __ne__, __gt__, __ge__) def __iter__(self): # Implement this method pass async def __aiter__(self): # Implement this method pass async def __anext__(self): # Implement this method pass # Example demonstrating attribute handling obj = AdvancedObject(10) obj.new_attr = \\"Hello\\" print(obj.get_all_attributes()) # {\'value\': 10, \'new_attr\': \'Hello\'} print(obj.delete_attribute(\'value\')) # True print(obj.delete_attribute(\'missing\')) # False # Example demonstrating comparison operations obj1 = AdvancedObject(5) obj2 = AdvancedObject(10) print(obj1 < obj2) # True # Example demonstrating synchronous iteration for attr in obj: print(attr) # Prints attribute names # Example demonstrating asynchronous iteration import asyncio async def async_iter_demo(): async for attr in obj: print(attr) # Prints attribute names asyncio.run(async_iter_demo()) ``` # Constraints - The `value` attribute will always be an integer. - Each asynchronous iteration through attributes should yield one attribute name per iteration. # Note - You may use `getattr`, `setattr`, `hasattr`, `delattr` in implementing attribute handling. - Use appropriate exception handling for robust implementation.","solution":"class AdvancedObject: def __init__(self, value): self.value = value def get_all_attributes(self): Returns a dictionary of all attributes and their values for the object. return self.__dict__ def delete_attribute(self, attr_name): Deletes a specified attribute. If the attribute does not exist, the method returns False, otherwise returns True. if hasattr(self, attr_name): delattr(self, attr_name) return True return False def __lt__(self, other): return self.value < other.value def __le__(self, other): return self.value <= other.value def __eq__(self, other): return self.value == other.value def __ne__(self, other): return self.value != other.value def __gt__(self, other): return self.value > other.value def __ge__(self, other): return self.value >= other.value def __iter__(self): Yields attribute names of the object. for attr in self.__dict__: yield attr async def __aiter__(self): Returns the asynchronous iterator itself. self._attributes = list(self.__dict__.keys()) self._index = 0 return self async def __anext__(self): Asynchronously yields attribute names of the object. if self._index < len(self._attributes): attr = self._attributes[self._index] self._index += 1 return attr else: raise StopAsyncIteration"},{"question":"# Advanced Coding Assessment: Debugging and Profiling in Python Objective Your task is to implement a function, `find_peak_usage`, which leverages the `tracemalloc` module to monitor and analyze memory usage within a given code snippet. The function should determine the point in the code where the peak memory usage occurs and provide a detailed traceback of the memory allocation leading to this peak. Function Signature ```python def find_peak_usage(code_snippet: str) -> str: Analyzes the given code snippet\'s memory usage and returns the detailed traceback of peak memory usage. Args: - code_snippet (str): A string containing the code to be analyzed. Returns: - str: A formatted string containing the traceback information of the peak memory usage. pass ``` Input - `code_snippet`: A string representing a Python code block. Output - A string containing the formatted traceback of the peak memory usage. Constraints - The `code_snippet` will be a valid Python code string. - The function should make use of the `tracemalloc` module to gather memory usage statistics. Guidelines 1. Use `tracemalloc` to start tracing memory allocations. 2. Execute the `code_snippet` within the context of your function. 3. Stop tracing and get the snapshot of memory allocations. 4. Identify the peak memory usage from the trace. 5. Generate a detailed traceback for the peak memory usage point. 6. Return a formatted string containing the traceback details. Example ```python code = my_list = [] for i in range(10000): my_list.append(i) result = find_peak_usage(code) print(result) ``` This should return a string that contains detailed information about where the peak memory allocation occurred and the chain of events leading up to it. Hints - Consider using the `exec` function to run the `code_snippet` within your function. - Make use of `tracemalloc.take_snapshot()` to get a snapshot of the current memory allocations. - Use the `statistics` and `traceback` functions provided by `tracemalloc` to extract and format the necessary information. Good luck, and happy debugging!","solution":"import tracemalloc def find_peak_usage(code_snippet: str) -> str: Analyzes the given code snippet\'s memory usage and returns the detailed traceback of peak memory usage. Args: - code_snippet (str): A string containing the code to be analyzed. Returns: - str: A formatted string containing the traceback information of the peak memory usage. # Start tracing memory allocations tracemalloc.start() # Execute the code snippet exec(code_snippet) # Stop tracing memory allocations and take a snapshot snapshot = tracemalloc.take_snapshot() # Get the statistics for memory usage and find the peak top_stats = snapshot.statistics(\'traceback\') # Find the biggest memory block biggest_stat = top_stats[0] # Format the output string output_lines = [ f\\"Peak memory usage: {biggest_stat.size / 1024:.1f} KiB\\", \\"Traceback (most recent call last):\\" ] for line in biggest_stat.traceback.format(): output_lines.append(line) return \\"n\\".join(output_lines)"},{"question":"**Objective:** To assess your understanding of scikit-learn\'s dataset loading utilities and your ability to manipulate and utilize datasets in machine learning tasks. **Problem Statement:** Using the `sklearn.datasets` package, write a function called `analyze_iris_dataset` that performs the following tasks: 1. Load the Iris dataset. 2. Split the dataset into training and testing sets (80% training, 20% testing). 3. Train a k-nearest neighbors (KNN) classifier on the training set. 4. Evaluate the classifier on the testing set and return the accuracy score. **Function Signature:** ```python def analyze_iris_dataset() -> float: pass ``` **Instructions:** 1. Use `sklearn.datasets.load_iris` to load the dataset. 2. Use `sklearn.model_selection.train_test_split` to split the dataset. 3. Use `sklearn.neighbors.KNeighborsClassifier` to create and train the KNN model. 4. Use `sklearn.metrics.accuracy_score` to compute the accuracy of the model on the test set. **Detailed Steps:** 1. Load the Iris dataset using `load_iris()`. 2. Separate the dataset into features (X) and target labels (y). 3. Split the features and labels into training and testing sets using an 80%-20% split. 4. Initialize the `KNeighborsClassifier` with `n_neighbors=3`. 5. Train the KNN classifier using the training data. 6. Predict the labels for the testing data. 7. Calculate and return the accuracy of the classifier on the test data. **Example:** ```python accuracy = analyze_iris_dataset() print(f\\"The accuracy of the KNN classifier on the Iris dataset is: {accuracy}\\") ``` **Expected Output:** The function should return a floating-point number representing the accuracy of the KNN classifier on the test set. **Constraints:** - You must use the specified functions and methods from the `sklearn` library. - Ensure that the random state is set to 42 for reproducibility when splitting the dataset.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def analyze_iris_dataset() -> float: # Load the Iris dataset iris = load_iris() X = iris.data y = iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the KNN classifier knn = KNeighborsClassifier(n_neighbors=3) # Train the KNN classifier on the training set knn.fit(X_train, y_train) # Predict the labels for the testing set y_pred = knn.predict(X_test) # Calculate and return the accuracy score accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"Objective: Implement functions to analyze a given model using both validation and learning curves. The goal is to determine if the model is overfitting or underfitting and suggest ways to improve it. Instructions: 1. Implement a function `plot_validation_curve` that takes the following inputs: - `estimator`: The model to analyze. - `X`: Input features, a numpy array or pandas DataFrame. - `y`: Target labels, a numpy array or pandas Series. - `param_name`: The hyperparameter to analyze. - `param_range`: The range of values for the hyperparameter. - `cv`: Cross-validation splitting strategy. The function should plot the validation curve for the given model and parameter. 2. Implement a function `plot_learning_curve` that takes the following inputs: - `estimator`: The model to analyze. - `X`: Input features, a numpy array or pandas DataFrame. - `y`: Target labels, a numpy array or pandas Series. - `train_sizes`: Relative or absolute numbers of training examples that will be used to generate the learning curve. - `cv`: Cross-validation splitting strategy. The function should plot the learning curve for the given model. 3. Using the above functions, analyze the model `SVC(kernel=\\"linear\\")` on the Iris dataset. Identify if the model is overfitting or underfitting and suggest potential improvements. Constraints: - You must use the `validation_curve` and `learning_curve` functions from `sklearn.model_selection`. - The plots should be generated using `matplotlib` or similar plotting libraries. Example Usage: ```python import numpy as np from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.model_selection import ShuffleSplit # Load the Iris dataset X, y = load_iris(return_X_y=True) # Define the model model = SVC(kernel=\\"linear\\") # Define the parameter range for validation curve param_range = np.logspace(-6, -1, 5) # Define cross-validation strategy cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0) # Plot validation curve plot_validation_curve(model, X, y, param_name=\\"C\\", param_range=param_range, cv=cv) # Plot learning curve train_sizes = np.linspace(0.1, 1.0, 5) plot_learning_curve(model, X, y, train_sizes=train_sizes, cv=cv) ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import validation_curve, learning_curve def plot_validation_curve(estimator, X, y, param_name, param_range, cv): Plots the validation curve for the given model and parameter. Parameters: - estimator: The model to analyze. - X: Input features, a numpy array or pandas DataFrame. - y: Target labels, a numpy array or pandas Series. - param_name: The hyperparameter to analyze. - param_range: The range of values for the hyperparameter. - cv: Cross-validation splitting strategy. train_scores, test_scores = validation_curve(estimator, X, y, param_name=param_name, param_range=param_range, cv=cv, scoring=\'accuracy\') train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.figure() plt.title(\\"Validation Curve with {}\\".format(estimator.__class__.__name__)) plt.xlabel(param_name) plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) lw = 2 plt.semilogx(param_range, train_scores_mean, label=\\"Training score\\", color=\\"darkorange\\", lw=lw) plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"darkorange\\", lw=lw) plt.semilogx(param_range, test_scores_mean, label=\\"Cross-validation score\\", color=\\"navy\\", lw=lw) plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\\"navy\\", lw=lw) plt.legend(loc=\\"best\\") plt.show() def plot_learning_curve(estimator, X, y, train_sizes, cv): Plots the learning curve for the given model. Parameters: - estimator: The model to analyze. - X: Input features, a numpy array or pandas DataFrame. - y: Target labels, a numpy array or pandas Series. - train_sizes: Relative or absolute numbers of training examples that will be used to generate the learning curve. - cv: Cross-validation splitting strategy. train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, train_sizes=train_sizes, cv=cv, scoring=\'accuracy\') train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.figure() plt.title(\\"Learning Curve with {}\\".format(estimator.__class__.__name__)) plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) lw = 2 plt.plot(train_sizes, train_scores_mean, label=\\"Training score\\", color=\\"darkorange\\", lw=lw) plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"darkorange\\", lw=lw) plt.plot(train_sizes, test_scores_mean, label=\\"Cross-validation score\\", color=\\"navy\\", lw=lw) plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\\"navy\\", lw=lw) plt.legend(loc=\\"best\\") plt.show()"},{"question":"Objective Implement a class that manages a configuration file using the `configparser` module. Your implementation should demonstrate an understanding of reading, writing, and modifying configuration files, handling different datatypes, and providing fallback values. Problem Statement You are required to implement a `ConfigHandler` class that can read from and write to a configuration file in the INI format. The class should support the following functionalities: 1. **Initialization**: - Initialize the class with the path to the configuration file. 2. **Reading Values**: - A method `get_value(section: str, option: str, fallback: Optional[Union[str, int, float, bool]]=None) -> Union[str, int, float, bool]` that retrieves the value for a given section and option. If the option does not exist, it should return the provided fallback value. 3. **Writing Values**: - A method `set_value(section: str, option: str, value: Union[str, int, float, bool]) -> None` that sets the value for a given section and option. If the section does not exist, it should be created. 4. **Removing Values**: - A method `remove_option(section: str, option: str) -> bool` that removes an option from a section. Should return `True` if the option was successfully removed, `False` otherwise. 5. **Saving Changes**: - A method `save() -> None` that writes any changes made to the configuration back to the file. Constraints - The configuration file will follow standard INI file structure. - Sections and options are case-insensitive. - Values can be of type `str`, `int`, `float`, or `bool`. Input and Output Formats - **Initialization**: ```python config = ConfigHandler(\'path/to/config.ini\') ``` - **Reading Values**: ```python value = config.get_value(\'Section\', \'Option\', fallback=\'default_value\') ``` - **Writing Values**: ```python config.set_value(\'Section\', \'Option\', \'new_value\') ``` - **Removing Values**: ```python success = config.remove_option(\'Section\', \'Option\') ``` - **Saving Changes**: ```python config.save() ``` Example Usage ```python # Initialize ConfigHandler with the path to a configuration file config = ConfigHandler(\'config.ini\') # Setting a value config.set_value(\'Settings\', \'volume\', 75) # Getting a value with a fallback volume = config.get_value(\'Settings\', \'volume\', fallback=50) # Removing an option config.remove_option(\'Settings\', \'volume\') # Saving changes config.save() ``` Performance Requirements - The solution should handle configuration files of moderate size (up to 1MB) efficiently. - Changes to the configuration should be cached in memory until the `save` method is called to minimize file I/O operations. You are not allowed to use any external libraries apart from `configparser`. The use of standard Python libraries is permitted. Implement the `ConfigHandler` class below: ```python from configparser import ConfigParser from typing import Optional, Union class ConfigHandler: def __init__(self, file_path: str): pass def get_value(self, section: str, option: str, fallback: Optional[Union[str, int, float, bool]] = None) -> Union[str, int, float, bool]: pass def set_value(self, section: str, option: str, value: Union[str, int, float, bool]) -> None: pass def remove_option(self, section: str, option: str) -> bool: pass def save(self) -> None: pass # The rest of your implementation goes here ```","solution":"from configparser import ConfigParser from typing import Optional, Union class ConfigHandler: def __init__(self, file_path: str): self.file_path = file_path self.config = ConfigParser() self.config.read(file_path) def get_value(self, section: str, option: str, fallback: Optional[Union[str, int, float, bool]] = None) -> Union[str, int, float, bool]: if not self.config.has_section(section) or not self.config.has_option(section, option): return fallback return self._parse_value(self.config.get(section, option), fallback) def set_value(self, section: str, option: str, value: Union[str, int, float, bool]) -> None: if not self.config.has_section(section): self.config.add_section(section) self.config.set(section, option, str(value)) def remove_option(self, section: str, option: str) -> bool: if self.config.has_section(section) and self.config.has_option(section, option): self.config.remove_option(section, option) return True return False def save(self) -> None: with open(self.file_path, \'w\') as configfile: self.config.write(configfile) def _parse_value(self, value: str, fallback: Optional[Union[str, int, float, bool]]) -> Union[str, int, float, bool]: if isinstance(fallback, bool): return value.lower() in (\'true\', \'yes\', \'1\') elif isinstance(fallback, int): return int(value) elif isinstance(fallback, float): return float(value) return value"},{"question":"Problem Statement You are given a dataset containing information about various cars, including attributes such as fuel efficiency, weight, horsepower, number of cylinders, etc. Your task is to write a Python function using scikit-learn to preprocess the data and predict the fuel efficiency of the cars based on their attributes. Specifically, you need to implement the following steps: 1. **Data Imputation**: Handle missing values by replacing them with the mean value of the respective feature. 2. **Standardization**: Standardize the features to have zero mean and unit variance. 3. **Polynomial Feature Expansion**: Expand the features up to a degree of 2. 4. **Train-Test Split**: Split the data into training (80%) and testing sets (20%). 5. **Linear Regression Model**: Train a linear regression model on the training set and evaluate its performance on the testing set. The dataset is provided as a CSV file named `cars.csv` with the following columns: - `cylinders`: Number of cylinders - `displacement`: Displacement (in cubic centimeters) - `horsepower`: Horsepower - `weight`: Weight (in pounds) - `acceleration`: Acceleration (time to accelerate from 0 to 60 mph) - `model_year`: Model year - `origin`: Origin country - `mpg`: Fuel efficiency in miles per gallon (target variable) # Function Signature ```python def preprocess_and_predict(csv_file: str) -> float: Preprocess the given dataset and predict fuel efficiency using a Linear Regression model. Args: csv_file (str): Path to the input CSV file containing the car dataset. Returns: float: R^2 score of the model evaluated on the test set. ``` # Constraints - Assume that the input CSV file is correctly formatted. - You must use scikit-learn for all preprocessing and model training steps. - Evaluate the model using the R^2 score on the testing set. # Example ```python r2_score = preprocess_and_predict(\'cars.csv\') print(r2_score) # Output: A float representing the R^2 score ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from sklearn.impute import SimpleImputer def preprocess_and_predict(csv_file: str) -> float: # Step 1: Read the data data = pd.read_csv(csv_file) # Step 2: Data Imputation - Replace missing values with mean imputer = SimpleImputer(strategy=\'mean\') imputed_data = imputer.fit_transform(data) # Step 3: Separate features and target X = imputed_data[:, :-1] # all columns except the last one as features y = imputed_data[:, -1] # last column as target variable # Step 4: Standardization scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 5: Polynomial Feature Expansion poly = PolynomialFeatures(degree=2) X_poly = poly.fit_transform(X_scaled) # Step 6: Train-Test Split X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42) # Step 7: Train the Linear Regression Model model = LinearRegression() model.fit(X_train, y_train) # Step 8: Evaluate the model\'s performance using R^2 score y_pred = model.predict(X_test) r2 = r2_score(y_test, y_pred) return r2"},{"question":"**Question: Implement a `LoggedAttribute` Descriptor Class** # Objective: Design and implement a `LoggedAttribute` descriptor class in Python. The descriptor should log attribute access, modifications, and deletions. Additionally, the class should use a custom name for the underlying private attribute. # Description: 1. **LoggedAttribute class**: This descriptor should log whenever the attribute is accessed, modified, or deleted. It should work as follows: - When the attribute is accessed, log a message \\"Accessing: <attr_name>\\". - When the attribute is modified, log a message \\"Modifying: <attr_name> to <new_value>\\". - When the attribute is deleted, log a message \\"Deleting: <attr_name>\\". 2. **Customization of private attribute name**: The `LoggedAttribute` descriptor should use a custom private attribute name to store the actual data. For example, if the public attribute is `foo`, the private attribute could be `_foo`. 3. **Implementation Details**: - Define the `__set_name__`, `__get__`, `__set__`, and `__delete__` methods for the `LoggedAttribute` class. - Use Python\'s built-in `logging` module to log the messages. 4. **Example Usage**: - Create a class `Person` using the `LoggedAttribute` descriptor for attributes like `name` and `age`. - Demonstrate the logging functionality by creating and manipulating an instance of the `Person` class. # Input/Output: - Input: Various attribute operations such as assignment, access, and deletion. - Output: Log messages indicating the operations on the attribute. # Constraints: - Use Python\'s built-in `logging` module for logging. - The descriptor should handle attributes of any type. # Performance Requirements: - The descriptor should have minimal performance overhead during attribute operations. # Sample Code Structure: ```python import logging class LoggedAttribute: def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logging.info(f\'Accessing: {self.public_name}\') return value def __set__(self, obj, value): logging.info(f\'Modifying: {self.public_name} to {value}\') setattr(obj, self.private_name, value) def __delete__(self, obj): logging.info(f\'Deleting: {self.public_name}\') delattr(obj, self.private_name) class Person: name = LoggedAttribute() age = LoggedAttribute() def __init__(self, name, age): self.name = name self.age = age # Example usage logging.basicConfig(level=logging.INFO) p = Person(\'Alice\', 30) print(p.name) p.age = 31 del p.name ``` # Task: 1. Implement the `LoggedAttribute` class as described. 2. Create a `Person` class using the descriptor. 3. Write code to demonstrate the logging of attribute operations.","solution":"import logging class LoggedAttribute: def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logging.info(f\'Accessing: {self.public_name}\') return value def __set__(self, obj, value): logging.info(f\'Modifying: {self.public_name} to {value}\') setattr(obj, self.private_name, value) def __delete__(self, obj): logging.info(f\'Deleting: {self.public_name}\') delattr(obj, self.private_name) class Person: name = LoggedAttribute() age = LoggedAttribute() def __init__(self, name, age): self.name = name self.age = age # Example usage logging.basicConfig(level=logging.INFO) p = Person(\'Alice\', 30) print(p.name) p.age = 31 del p.name"},{"question":"# Question: You are tasked with creating a Python module for a contact management system. The goal is to enforce type safety using the `typing` module. The system should perform the following: 1. **Contact Information**: Define a `TypedDict` named `Contact` that includes the following keys: - `name` (str): The name of the contact. - `age` (Optional[int]): The age of the contact, which can be `None`. - `email` (Optional[str]): The email address of the contact, which can be `None`. 2. **Unique Identifiers**: Use `NewType` to create a distinct type `ContactId` for unique identifiers, which are based on integers. 3. **Contact Manager Class**: Implement a generic class `ContactManager` that uses a type variable `T` to manage a list of contacts where `T` is a subtype of `Contact`. The class should support the following methods: - `add_contact(contact: T) -> ContactId`: Adds a contact and returns its unique identifier. - `get_contact(contact_id: ContactId) -> Optional[T]`: Retrieves a contact by its identifier. - `remove_contact(contact_id: ContactId) -> bool`: Removes a contact by its identifier and returns whether the operation was successful. - `list_contacts() -> List[T]`: Returns a list of all contacts. 4. **Additional Checks**: Implement a function `is_valid_email(email: Optional[str]) -> bool` using `Callable` which checks if the provided email is valid. A valid email should contain an \\"@\\" symbol and a \\".\\" symbol after the \\"@\\". 5. **Protocol for Searchable Contacts**: Create a protocol `Searchable` to define a structural subtyping approach where any class implementing it must have a method `search(query: str) -> List[T]`. # Solution: ```python from typing import TypedDict, NewType, List, Generic, TypeVar, Optional, Callable, Protocol # Define the Contact TypedDict class Contact(TypedDict): name: str age: Optional[int] email: Optional[str] # Define the ContactId NewType ContactId = NewType(\'ContactId\', int) # Create a generic class for managing contacts T = TypeVar(\'T\', bound=Contact) class ContactManager(Generic[T]): def __init__(self) -> None: self._contacts: List[T] = [] self._next_id = 0 def add_contact(self, contact: T) -> ContactId: contact_id = ContactId(self._next_id) self._contacts.append(contact) self._next_id += 1 return contact_id def get_contact(self, contact_id: ContactId) -> Optional[T]: if 0 <= contact_id < len(self._contacts): return self._contacts[contact_id] return None def remove_contact(self, contact_id: ContactId) -> bool: if 0 <= contact_id < len(self._contacts): del self._contacts[contact_id] return True return False def list_contacts(self) -> List[T]: return self._contacts # Implement the email validation function def is_valid_email(email: Optional[str]) -> bool: if email is None: return False if \\"@\\" in email and \\".\\" in email.split(\\"@\\")[1]: return True return False # Define the Searchable protocol class Searchable(Protocol[T]): def search(self, query: str) -> List[T]: ... ``` Your task is to: 1. Implement the `ContactManager` class by filling in the missing method bodies. 2. Complete the `is_valid_email` function using the provided specification. 3. Test your implementation with a few example cases to ensure it works as expected. # Constraints: - The `email` field in `Contact` may be `None`, but if it\'s present, it must follow the validation rule. - The `ContactManager` should manage contacts without keeping duplicate entries based on the `name` field. # Expected Output: - Correctly add, retrieve, remove, and list contacts while maintaining type safety. - Validate emails accurately according to the specified criteria.","solution":"from typing import TypedDict, NewType, List, Generic, TypeVar, Optional, Callable, Protocol # Define the Contact TypedDict class Contact(TypedDict): name: str age: Optional[int] email: Optional[str] # Define the ContactId NewType ContactId = NewType(\'ContactId\', int) # Create a generic class for managing contacts T = TypeVar(\'T\', bound=Contact) class ContactManager(Generic[T]): def __init__(self) -> None: self._contacts: List[T] = [] self._next_id = 0 def add_contact(self, contact: T) -> ContactId: contact_id = ContactId(self._next_id) self._contacts.append(contact) self._next_id += 1 return contact_id def get_contact(self, contact_id: ContactId) -> Optional[T]: if 0 <= contact_id < len(self._contacts): return self._contacts[contact_id] return None def remove_contact(self, contact_id: ContactId) -> bool: if 0 <= contact_id < len(self._contacts): del self._contacts[contact_id] self._contacts.insert(contact_id, None) # Maintain ID consistency. return True return False def list_contacts(self) -> List[T]: return [c for c in self._contacts if c is not None] # Implement the email validation function def is_valid_email(email: Optional[str]) -> bool: if email is None: return False if \\"@\\" in email and \\".\\" in email.split(\\"@\\")[1]: return True return False # Define the Searchable protocol class Searchable(Protocol[T]): def search(self, query: str) -> List[T]: ..."},{"question":"Your task is to create a multi-faceted visualization using seaborn\'s `objects` interface. Specifically, you will: 1. Load the `mpg` dataset provided by seaborn. 2. Create a dot plot visualizing `horsepower` (x-axis) versus `mpg` (y-axis). 3. Color code the dots based on the `origin` of the cars. 4. Adjust the fill color of the dots based on the `weight` of the cars. 5. Use different marker shapes for different values of `origin` and include both filled and unfilled markers. 6. Add jitter to the plot to show local density more effectively. # Input - None. The dataset is loaded directly within the function. # Output - A seaborn plot following the above specifications. # Constraints - You must use seaborn\'s `objects` interface to achieve the task. - The plot should be clear and aesthetically pleasing. # Function Signature ```python def create_custom_mpg_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the plot p = ( so.Plot(mpg, x=\\"horsepower\\", y=\\"mpg\\") .add(so.Dots(), color=\\"origin\\", fillcolor=\\"weight\\", marker=\\"origin\\", stroke=1) .scale(marker=[\\"o\\", \\"x\\", (6, 2, 1)], fillcolor=\\"binary\\") .add(so.Dots(), so.Jitter(0.25)) ) # Show the plot p.show() # Call the function to test it create_custom_mpg_plot() ``` The function must create and display the plot when called correctly. Ensure that all the specified plot customizations are implemented.","solution":"def create_custom_mpg_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the plot p = ( so.Plot(mpg, x=\\"horsepower\\", y=\\"mpg\\") .add(so.Dots(), so.Jitter(0.25), color=\\"origin\\", fillcolor=\\"weight\\", marker=\\"origin\\", stroke=1) .scale(marker=[\\"o\\", \\"x\\", (6, 2, 1)], fillcolor=\\"binary\\") ) # Show the plot p.show() # Call the function to test it create_custom_mpg_plot()"},{"question":"**Pandas Assessment Question** **Problem Statement:** You are given a dataset containing information about sales transactions in a CSV file named `sales_data.csv`. Your task is to perform various data manipulations and generate summary statistics using pandas. Additionally, you will create a custom plot to visualize the sales data. **Dataset Description:** The `sales_data.csv` file contains the following columns: - `TransactionID`: Unique identifier for each transaction. - `Date`: Date of the transaction. - `CustomerID`: Unique identifier for each customer. - `ProductID`: Unique identifier for each product. - `Quantity`: Number of units sold. - `Price`: Price per unit. - `Sales`: Total sales amount (= Quantity * Price). **Instructions:** 1. **Data Loading and Cleaning:** - Load the dataset from the `sales_data.csv` file. - Convert the `Date` column to datetime format. - Handle any missing or erroneous data in the dataset (e.g., replace missing values with appropriate defaults). 2. **Summary Statistics:** - Calculate the total sales amount for each customer. - Identify the top 5 customers by total sales amount. - Calculate the average sales amount per transaction for each day. 3. **Groupby Operations:** - Group the data by `ProductID` and calculate the total quantity sold for each product. - Identify the top 3 products by total quantity sold. - Group the data by `CustomerID` and `ProductID` to create a pivot table that shows the total sales amount for each combination of customer and product. 4. **Custom Plotting:** - Create a line plot that shows the total daily sales over time. - Highlight the days with the highest and lowest total sales. **Function Signatures:** For the implementation, you should create the following functions: ```python import pandas as pd def load_and_clean_data(file_path: str) -> pd.DataFrame: Loads the dataset from the given CSV file, converts the \'Date\' column to datetime format, and handles any missing or erroneous data. Parameters: - file_path: str, the path to the CSV file Returns: - cleaned DataFrame pass def calculate_summary_statistics(df: pd.DataFrame) -> pd.DataFrame: Calculates the total sales amount for each customer and identifies the top 5 customers. Also calculates the average sales amount per transaction for each day. Parameters: - df: pd.DataFrame, the cleaned sales data DataFrame Returns: - DataFrame containing summary statistics pass def perform_groupby_operations(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame): Groups the data by \'ProductID\' to calculate the total quantity sold for each product and identifies the top 3 products by total quantity sold. Also creates a pivot table showing the total sales amount for each combination of customer and product. Parameters: - df: pd.DataFrame, the cleaned sales data DataFrame Returns: - DataFrame with total quantity sold for each product - pivot Table DataFrame with total sales amount for each customer-product combination pass def plot_daily_sales(df: pd.DataFrame) -> None: Creates a line plot that shows the total daily sales over time, highlighting the days with the highest and lowest total sales. Parameters: - df: pd.DataFrame, the cleaned sales data DataFrame Returns: - None pass ``` **Constraints:** - Assume the dataset file (`sales_data.csv`) is correctly formatted. - Ensure that your solution is efficient and makes optimal use of pandas functionalities. **Performance Requirements:** - The solution should handle large datasets efficiently. - The plotting should be clear and informative, using appropriate visualization techniques.","solution":"import pandas as pd import matplotlib.pyplot as plt def load_and_clean_data(file_path: str) -> pd.DataFrame: Loads the dataset from the given CSV file, converts the \'Date\' column to datetime format, and handles any missing or erroneous data. Parameters: - file_path: str, the path to the CSV file Returns: - cleaned DataFrame df = pd.read_csv(file_path) df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Quantity\'] = df[\'Quantity\'].fillna(0) df[\'Price\'] = df[\'Price\'].fillna(0) df[\'Sales\'] = df[\'Quantity\'] * df[\'Price\'] return df def calculate_summary_statistics(df: pd.DataFrame) -> pd.DataFrame: Calculates the total sales amount for each customer and identifies the top 5 customers. Also calculates the average sales amount per transaction for each day. Parameters: - df: pd.DataFrame, the cleaned sales data DataFrame Returns: - DataFrame containing summary statistics total_sales_per_customer = df.groupby(\'CustomerID\')[\'Sales\'].sum().reset_index() top_5_customers = total_sales_per_customer.nlargest(5, \'Sales\') daily_average_sales = df.groupby(df[\'Date\'].dt.date)[\'Sales\'].mean().reset_index() return top_5_customers, daily_average_sales def perform_groupby_operations(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame): Groups the data by \'ProductID\' to calculate the total quantity sold for each product and identifies the top 3 products by total quantity sold. Also creates a pivot table showing the total sales amount for each combination of customer and product. Parameters: - df: pd.DataFrame, the cleaned sales data DataFrame Returns: - DataFrame with total quantity sold for each product - pivot Table DataFrame with total sales amount for each customer-product combination total_quantity_per_product = df.groupby(\'ProductID\')[\'Quantity\'].sum().reset_index() top_3_products = total_quantity_per_product.nlargest(3, \'Quantity\') pivot_table = df.pivot_table(index=\'CustomerID\', columns=\'ProductID\', values=\'Sales\', aggfunc=\'sum\', fill_value=0) return top_3_products, pivot_table def plot_daily_sales(df: pd.DataFrame) -> None: Creates a line plot that shows the total daily sales over time, highlighting the days with the highest and lowest total sales. Parameters: - df: pd.DataFrame, the cleaned sales data DataFrame Returns: - None daily_sales = df.groupby(df[\'Date\'].dt.date)[\'Sales\'].sum().reset_index() highest_sales_day = daily_sales.loc[daily_sales[\'Sales\'].idxmax()] lowest_sales_day = daily_sales.loc[daily_sales[\'Sales\'].idxmin()] plt.figure(figsize=(10, 6)) plt.plot(daily_sales[\'Date\'], daily_sales[\'Sales\'], marker=\'o\', label=\'Daily Sales\') plt.scatter(highest_sales_day[\'Date\'], highest_sales_day[\'Sales\'], color=\'red\', label=\'Highest Sales Day\') plt.scatter(lowest_sales_day[\'Date\'], lowest_sales_day[\'Sales\'], color=\'green\', label=\'Lowest Sales Day\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') plt.title(\'Total Daily Sales Over Time\') plt.legend() plt.grid(True) plt.show()"},{"question":"# Question: Advanced Tensor Operations and Memory Management You have been given the task of implementing a function to perform complex tensor operations that involve managing tensor data types, devices, and memory formats. The function `tensor_operations` should perform the following operations: 1. Create three tensors of shape (3, 3) - one on CPU with `torch.float32`, one on GPU with `torch.int32`, and one on CPU with `torch.complex64`. 2. Add the tensor on GPU to the tensor on CPU after moving the GPU tensor to the CPU and casting it to `torch.float32`. 3. Multiply the resulting tensor from step 2 with the `torch.complex64` tensor. 4. Ensure that the final result tensor is in contiguous memory format. Function Signature ```python def tensor_operations() -> torch.Tensor: pass ``` Expected Output The function should return the final result tensor after performing all the operations. Constraints 1. Perform appropriate type casting and device transfers when necessary. 2. Ensure the final tensor is in a contiguous memory format. 3. Assume that a CUDA-capable GPU is available for tensor operations. You may assume the following imports: ```python import torch ``` Example Usage ```python result = tensor_operations() print(result) # Expected output would be a tensor of shape (3, 3) in `torch.complex64` dtype. ``` Performance Requirements Ensure that tensor transfers between devices and type castings are done efficiently, keeping the operations clear and maintaining the integrity of tensor data.","solution":"import torch def tensor_operations() -> torch.Tensor: # Create tensors with specified shapes, devices, and dtypes tensor_cpu_float32 = torch.randn(3, 3, dtype=torch.float32, device=\'cpu\') tensor_gpu_int32 = torch.randint(low=0, high=10, size=(3, 3), dtype=torch.int32, device=\'cuda\') tensor_cpu_complex64 = torch.randn(3, 3, dtype=torch.complex64, device=\'cpu\') # Move tensor from GPU to CPU and cast to float32 tensor_gpu_to_cpu_float32 = tensor_gpu_int32.to(device=\'cpu\', dtype=torch.float32) # Add tensors (CPU float32 + CPU float32) result_addition = tensor_cpu_float32 + tensor_gpu_to_cpu_float32 # Multiply result with tensor_cpu_complex64 result_multiplication = result_addition * tensor_cpu_complex64 # Ensure the final tensor is in contiguous memory format result_contiguous = result_multiplication.contiguous() return result_contiguous"},{"question":"**Question:** You are tasked with implementing a Python function using the `tarfile` module to create a tar archive from a given directory, including only specific file types, and to extract that archive to a specified destination directory, ensuring certain security measures. # Function signature: ```python def create_and_extract_tar(input_dir: str, output_tar: str, extract_dir: str, allowed_extensions: list[str]) -> None: pass ``` # Description: 1. **Creating the tar archive**: - Traverse the directory specified by `input_dir` and add only files with extensions present in the `allowed_extensions` list to a new tar archive specified by `output_tar`. - The tar archive should use gzip compression. 2. **Extracting the tar archive**: - Extract the archive `output_tar` into the directory specified by `extract_dir` using safe extraction practices. - Ensure that the extraction does not permit files with absolute paths or outside the destination directory, and skips any symbolic links or special files (e.g., devices, pipes). # Input: - `input_dir` (str): The path to the directory to create the tar archive from. - `output_tar` (str): The path to save the created tar archive. - `extract_dir` (str): The path to the directory where the tar archive should be extracted. - `allowed_extensions` (list[str]): A list of allowed file extensions (e.g., [\'.txt\', \'.py\']). # Output: - None. The function should create a tar archive file and extract its contents to the specified directory. # Constraints: - `input_dir` and `extract_dir` are valid directories on the file system. - `output_tar` is a valid path where the tar archive can be written. - File extensions in `allowed_extensions` can vary and may include typical text and code file extensions (e.g., `.txt`, `.py`). # Example usage: ```python create_and_extract_tar(\'/path/to/input_dir\', \'output.tar.gz\', \'/path/to/extract_dir\', [\'.txt\', \'.py\']) ``` This function should: 1. Create a gzipped tar archive named `output.tar.gz` containing only `.txt` and `.py` files from `/path/to/input_dir`. 2. Extract the files from `output.tar.gz` to `/path/to/extract_dir` securely, ensuring no files are extracted outside the specified directory and ignoring any symbolic links or special files. # Hints: - Use `tarfile.TarFile.add` with a filter function to include only allowed files. - Use extraction filters during extraction to enhance security.","solution":"import os import tarfile def is_within_directory(directory, target): Check if a file is within a given directory. abs_directory = os.path.abspath(directory) abs_target = os.path.abspath(target) prefix = os.path.commonprefix([abs_directory, abs_target]) return prefix == abs_directory def create_and_extract_tar(input_dir: str, output_tar: str, extract_dir: str, allowed_extensions: list[str]) -> None: # Creating the tar archive with tarfile.open(output_tar, \\"w:gz\\") as tar: for root, _, files in os.walk(input_dir): for file in files: if any(file.endswith(ext) for ext in allowed_extensions): fullpath = os.path.join(root, file) arcname = os.path.relpath(fullpath, input_dir) tar.add(fullpath, arcname=arcname) # Extracting the tar archive with tarfile.open(output_tar, \\"r:gz\\") as tar: members = [] for member in tar.getmembers(): # Ensure no absolute paths or symbolic links are included if not member.isreg(): continue # Skip non-regular files like links, devices, etc. member_path = os.path.join(extract_dir, member.name) if is_within_directory(extract_dir, member_path): members.append(member) tar.extractall(extract_dir, members)"},{"question":"**Objective:** Demonstrate mastery of the `csv` module by combining reading, processing, and writing CSV data with custom dialect handling. **Question:** You are given a CSV file named `employees.csv` with the following columns: `\\"ID\\"`, `\\"Name\\"`, `\\"Department\\"`, `\\"Salary\\"`. The file uses a semi-colon (`;`) as the delimiter and fields are enclosed in double quotes (`\\"`). Your task is to: 1. Read the `employees.csv` file and convert the salaries to integers. 2. Filter out employees whose salary is less than 3000. 3. Write the filtered data to a new CSV file named `filtered_employees.csv` using a comma (`,`) as the delimiter and enclosing fields in single quotes (`\'`). 4. Generate a summary showing the total number of employees and the average salary of the remaining employees. Implement the function `process_employees` which performs the above tasks. **Function Signature:** ```python def process_employees(input_file: str, output_file: str) -> None: pass ``` **Input:** - `input_file` (str): The path to the input CSV file (`employees.csv`). - `output_file` (str): The path to the output CSV file (`filtered_employees.csv`). **Constraints:** - You can assume the input CSV file exists and is formatted correctly with appropriate headers. - You should handle any potential errors gracefully. **Output:** - The function should write the filtered data to the specified output file. - Print the total number of employees and average salary of the filtered employees. **Example:** For an input file `employees.csv` with the following content: ```csv \\"ID\\";\\"Name\\";\\"Department\\";\\"Salary\\" \\"001\\";\\"Alice Smith\\";\\"Engineering\\";\\"3200\\" \\"002\\";\\"Bob Brown\\";\\"HR\\";\\"2800\\" \\"003\\";\\"Carol Johnson\\";\\"Marketing\\";\\"3500\\" ``` After running `process_employees(\'employees.csv\', \'filtered_employees.csv\')`, the output should be: Contents of `filtered_employees.csv`: ```csv \'ID\',\'Name\',\'Department\',\'Salary\' \'001\',\'Alice Smith\',\'Engineering\',\'3200\' \'003\',\'Carol Johnson\',\'Marketing\',\'3500\' ``` Printed Output: ``` Total Employees: 2 Average Salary: 3350 ``` Use the `csv` module to achieve the desired results.","solution":"import csv def process_employees(input_file, output_file): try: with open(input_file, \'r\', newline=\'\') as infile: csv_reader = csv.DictReader(infile, delimiter=\';\', quotechar=\'\\"\') filtered_employees = [] total_salary = 0 for row in csv_reader: row[\'Salary\'] = int(row[\'Salary\']) if row[\'Salary\'] >= 3000: filtered_employees.append(row) total_salary += row[\'Salary\'] total_employees = len(filtered_employees) avg_salary = total_salary // total_employees if total_employees else 0 with open(output_file, \'w\', newline=\'\') as outfile: fieldnames = [\'ID\', \'Name\', \'Department\', \'Salary\'] csv_writer = csv.DictWriter(outfile, fieldnames=fieldnames, delimiter=\',\', quotechar=\\"\'\\", quoting=csv.QUOTE_NONNUMERIC) csv_writer.writeheader() csv_writer.writerows(filtered_employees) print(f\'Total Employees: {total_employees}\') print(f\'Average Salary: {avg_salary}\') except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Using the `tkinter.font` module, write a Python class `CustomFontManager` that provides functionality to manage and compare fonts. Your class should offer the following methods: 1. **`add_font(name: str, family: str, size: int, weight: str, slant: str, underline: bool, overstrike: bool) -> None`**: Add a new named font with the specified attributes. 2. **`remove_font(name: str) -> None`**: Remove an existing named font. 3. **`list_fonts() -> List[str]`**: Return a list of all font names currently managed. 4. **`get_font_info(name: str) -> Dict[str, Union[str, int, bool]]`**: Return a dictionary containing the attributes of the specified font. 5. **`compare_fonts(font1: str, font2: str) -> bool`**: Compare two fonts by their attributes and return True if they are identical, and False otherwise. # Input and Output Formats - **`add_font` method**: - Input: `name` (string), `family` (string), `size` (integer), `weight` (string, either \\"NORMAL\\" or \\"BOLD\\"), `slant` (string, either \\"ROMAN\\" or \\"ITALIC\\"), `underline` (boolean), `overstrike` (boolean) - Output: `None` - **`remove_font` method**: - Input: `name` (string) - Output: `None` - **`list_fonts` method**: - Input: None - Output: List of font names (list of strings) - **`get_font_info` method**: - Input: `name` (string) - Output: Dictionary with font attributes (`family` (string), `size` (integer), `weight` (string), `slant` (string), `underline` (integer, 0 or 1), `overstrike` (integer, 0 or 1)) - **`compare_fonts` method**: - Input: `font1` (string), `font2` (string) - Output: Boolean (True if fonts are identical, False otherwise) # Constraints - All font attributes provided in the methods should be valid as per the `tkinter.font` module\'s requirements. - Font names are case-sensitive and must be unique. - Raise appropriate errors if operations are attempted on non-existent fonts. # Example ```python manager = CustomFontManager() manager.add_font(\\"TitleFont\\", \\"Courier\\", 14, \\"BOLD\\", \\"ROMAN\\", True, False) manager.add_font(\\"BodyFont\\", \\"Times\\", 12, \\"NORMAL\\", \\"ITALIC\\", False, False) print(manager.list_fonts()) # Output: [\\"TitleFont\\", \\"BodyFont\\"] print(manager.get_font_info(\\"TitleFont\\")) # Output: {\\"family\\": \\"Courier\\", \\"size\\": 14, \\"weight\\": \\"BOLD\\", \\"slant\\": \\"ROMAN\\", \\"underline\\": 1, \\"overstrike\\": 0} print(manager.compare_fonts(\\"TitleFont\\", \\"BodyFont\\")) # Output: False manager.remove_font(\\"BodyFont\\") print(manager.list_fonts()) # Output: [\\"TitleFont\\"] ``` Implement the `CustomFontManager` class according to the specification above.","solution":"from typing import List, Dict, Union import tkinter.font as tkFont class CustomFontManager: def __init__(self): self.fonts = {} def add_font(self, name: str, family: str, size: int, weight: str, slant: str, underline: bool, overstrike: bool) -> None: if name in self.fonts: raise ValueError(f\\"Font with name \'{name}\' already exists.\\") self.fonts[name] = { \\"family\\": family, \\"size\\": size, \\"weight\\": weight, \\"slant\\": slant, \\"underline\\": 1 if underline else 0, \\"overstrike\\": 1 if overstrike else 0 } def remove_font(self, name: str) -> None: if name not in self.fonts: raise ValueError(f\\"Font with name \'{name}\' does not exist.\\") del self.fonts[name] def list_fonts(self) -> List[str]: return list(self.fonts.keys()) def get_font_info(self, name: str) -> Dict[str, Union[str, int, bool]]: if name not in self.fonts: raise ValueError(f\\"Font with name \'{name}\' does not exist.\\") return self.fonts[name] def compare_fonts(self, font1: str, font2: str) -> bool: if font1 not in self.fonts: raise ValueError(f\\"Font with name \'{font1}\' does not exist.\\") if font2 not in self.fonts: raise ValueError(f\\"Font with name \'{font2}\' does not exist.\\") return self.fonts[font1] == self.fonts[font2]"},{"question":"You are required to implement a custom class in TorchScript that models some behavior and is consistent with PyTorch\'s type system and constraints. # Task 1. Implement a TorchScript custom class `SequenceModel` which should inherit from `torch.nn.Module`. 2. The class should have the following functionality: - Initialize with a list of `torch.nn.Module` objects. - Have a `forward` method that takes a tensor as input and sequentially passes the tensor through each module in the list. - Return the resulting tensor. # Class Signature ```python import torch import torch.nn as nn from typing import List @torch.jit.script class SequenceModel(nn.Module): def __init__(self, modules: List[nn.Module]): # Initialize the list of modules def forward(self, x: torch.Tensor) -> torch.Tensor: # Sequentially apply each module to input tensor x # Return the resulting tensor ``` # Input - The `__init__` method should take a list of initialized `torch.nn.Module` objects. - The `forward` method should take a single input tensor `x`. # Output - The `forward` method should return a tensor after passing it through each module in sequence. # Example ```python import torch import torch.nn as nn modules = [nn.Linear(in_features=10, out_features=20), nn.ReLU(), nn.Linear(in_features=20, out_features=5)] model = SequenceModel(modules) input_tensor = torch.randn(1, 10) output_tensor = model(input_tensor) print(output_tensor) ``` # Constraints - Ensure that the implementation adheres to the type constraints of TorchScript. - Use proper type annotations. - Use `torch.jit` annotations where required.","solution":"import torch import torch.nn as nn from typing import List class SequenceModel(nn.Module): def __init__(self, modules: List[nn.Module]): super(SequenceModel, self).__init__() self.modules_list = nn.ModuleList(modules) def forward(self, x: torch.Tensor) -> torch.Tensor: for module in self.modules_list: x = module(x) return x"},{"question":"You are tasked with implementing a secure user authentication system using the deprecated `crypt` module. Your job is to design two main functions for this system. 1. **hash_password(plaintext_password: str, method: crypt.METHOD_* = None, rounds: int = None) -> str** This function should: - Take a plaintext password as input. - An optional `method` from `crypt.METHOD_*` values, which defines the hashing method to be used. If not provided, the strongest available method should be used. - An optional `rounds` parameter which specifies the number of rounds for `METHOD_SHA256`, `METHOD_SHA512`, and `METHOD_BLOWFISH`. - Return the hashed password as a string. 2. **validate_password(plaintext_password: str, hashed_password: str) -> bool** This function should: - Take a plaintext password and a previously hashed password as inputs. - Hash the plaintext password with the salt extracted from the hashed password. - Return `True` if the newly hashed password matches the stored hashed password, and `False` otherwise. # Constraints - The `rounds` parameter, if used, must be: - Between 1000 and 999,999,999 for `METHOD_SHA256` and `METHOD_SHA512`. - A power of two between 16 (2^4) and 2,147,483,648 (2^31) for `METHOD_BLOWFISH`. # Example Usage ```python import crypt hashed = hash_password(\\"mypassword123\\", method=crypt.METHOD_SHA512, rounds=10000) print(hashed) # Should print a SHA-512 hashed password string. is_valid = validate_password(\\"mypassword123\\", hashed) print(is_valid) # Should print True. is_valid_wrong = validate_password(\\"wrongpassword\\", hashed) print(is_valid_wrong) # Should print False. ``` Ensure your solution efficiently handles the password hashing and validation process, adhering to the specified constraints and using the appropriate cryptographic methods.","solution":"import crypt def hash_password(plaintext_password: str, method=crypt.METHOD_SHA512, rounds: int = None) -> str: Hashes a plaintext password using the specified method and rounds. Parameters: - plaintext_password: The plaintext password to hash. - method: The hash method to use. - rounds: The number of rounds to use for hashing (optional). Returns: - The hashed password. if rounds: # Ensure valid rounds for METHOD_SHA256 and METHOD_SHA512 if method in [crypt.METHOD_SHA256, crypt.METHOD_SHA512] and (rounds < 1000 or rounds > 999999999): raise ValueError(\\"Rounds for SHA256/SHA512 must be between 1000 and 999,999,999.\\") # Ensure valid rounds for METHOD_BLOWFISH if method == crypt.METHOD_BLOWFISH and (rounds < 16 or rounds > 2147483648 or (rounds & (rounds - 1)) != 0): raise ValueError(\\"Rounds for BLOWFISH must be a power of 2 between 16 and 2,147,483,648.\\") return crypt.crypt(plaintext_password, crypt.mksalt(method, rounds=rounds)) else: return crypt.crypt(plaintext_password, crypt.mksalt(method)) def validate_password(plaintext_password: str, hashed_password: str) -> bool: Validates a plaintext password against a hashed password. Parameters: - plaintext_password: The plaintext password to validate. - hashed_password: The previously hashed password. Returns: - True if the passwords match, False otherwise. return crypt.crypt(plaintext_password, hashed_password) == hashed_password"},{"question":"# XML Processing with `xml.etree.ElementTree` Problem Statement: You are given an XML document representing a library\'s book collection. Each book has a title, author, year of publication, and genre. Your task is to create a Python function that parses this XML document, performs the following modifications, and returns the modified XML as a string: 1. Add a new book to the library\'s collection. 2. Update the genre of a specified book. 3. Remove a book based on its title. Function Signature: ```python def modify_library_xml(xml_string: str, new_book: dict, update_genre: tuple, remove_title: str) -> str: pass ``` Input: 1. `xml_string` (str): A string containing the XML document. 2. `new_book` (dict): A dictionary representing the new book with keys \'title\', \'author\', \'year\', and \'genre\'. 3. `update_genre` (tuple): A tuple containing the title of the book to update and the new genre `(title, new_genre)`. 4. `remove_title` (str): The title of the book to be removed. Output: - A string representing the modified XML document. Constraints: - The XML structure is consistent, with each book having the elements `<title>`, `<author>`, `<year>`, and `<genre>`. - The input XML string is well-formed. - The `title` field is unique for each book. Example: Given the following input: ```python xml_string = <library> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> <genre>Fiction</genre> </book> <book> <title>1984</title> <author>George Orwell</author> <year>1949</year> <genre>Dystopian</genre> </book> </library> new_book = { \'title\': \'To Kill a Mockingbird\', \'author\': \'Harper Lee\', \'year\': \'1960\', \'genre\': \'Fiction\' } update_genre = (\'1984\', \'Political Fiction\') remove_title = \'The Great Gatsby\' ``` The function call: ```python result = modify_library_xml(xml_string, new_book, update_genre, remove_title) ``` Should produce the following output (formatted as needed): ```xml <library> <book> <title>1984</title> <author>George Orwell</author> <year>1949</year> <genre>Political Fiction</genre> </book> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <year>1960</year> <genre>Fiction</genre> </book> </library> ``` Notes: - Ensure the output XML is correctly formatted. - Handle all edge cases, such as attempting to update or remove a book that does not exist.","solution":"import xml.etree.ElementTree as ET def modify_library_xml(xml_string: str, new_book: dict, update_genre: tuple, remove_title: str) -> str: Modifies the given XML according to the provided new book, update genre, and remove title. Args: - xml_string (str): String containing the original XML data. - new_book (dict): Dictionary with keys \'title\', \'author\', \'year\', and \'genre\'. - update_genre (tuple): Tuple (title, new_genre). - remove_title (str): Title of the book to be removed. Returns: - str: Modified XML as a string. root = ET.fromstring(xml_string) # Remove the book with the given title for book in root.findall(\'book\'): title = book.find(\'title\').text if title == remove_title: root.remove(book) break # Update the genre of the specified book for book in root.findall(\'book\'): title = book.find(\'title\').text if title == update_genre[0]: book.find(\'genre\').text = update_genre[1] break # Add the new book to the library new_book_element = ET.Element(\'book\') title_element = ET.SubElement(new_book_element, \'title\') title_element.text = new_book[\'title\'] author_element = ET.SubElement(new_book_element, \'author\') author_element.text = new_book[\'author\'] year_element = ET.SubElement(new_book_element, \'year\') year_element.text = new_book[\'year\'] genre_element = ET.SubElement(new_book_element, \'genre\') genre_element.text = new_book[\'genre\'] root.append(new_book_element) return ET.tostring(root, encoding=\'unicode\')"},{"question":"Write a Python function `custom_seaborn_plots()` that generates a series of four subplots in a single figure using Seaborn, showcasing the diverse aesthetics control features of Seaborn. Your function should: 1. Create a figure with four subplots arranged in a 2x2 grid. 2. Generate data for plotting: - Use `np.random.normal` to create a dataset of shape (50, 5), where each column is a series representing a different variable. 3. For each subplot, use a different Seaborn style from the following list: `darkgrid`, `whitegrid`, `dark`, `ticks`, and showcase a different functionality of Seaborn: - Top-left: Use `boxplot` with `darkgrid` style. - Top-right: Use `violinplot` with `whitegrid` and remove the left spine. - Bottom-left: Use `lineplot` with `dark` style and plot a custom sinusoidal function. - Bottom-right: Use `stripplot` with `ticks` style and offset the spines. 4. Display the final figure with tight layout. **Function Signature:** ```python def custom_seaborn_plots(): pass ``` # Constraints: - Ensure to set the Seaborn style explicitly at the beginning of each subplot. - You can use any additional Seaborn or Matplotlib functions as needed. - The figure should be displayed correctly without overlapping elements. # Example Output: The function should produce a figure with four subplots, each demonstrating different Seaborn aesthetic capabilities as described. ```python custom_seaborn_plots() ``` *Note: Make sure you import all required libraries like numpy, seaborn, and matplotlib at the beginning of your script.*","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def custom_seaborn_plots(): # Generate data data = np.random.normal(size=(50, 5)) # Setup figure and subplots fig, axes = plt.subplots(2, 2, figsize=(12, 8)) # Top-left subplot: Boxplot with \'darkgrid\' style sns.set_style(\\"darkgrid\\") sns.boxplot(data=data, ax=axes[0, 0]) axes[0, 0].set_title(\\"Boxplot with Darkgrid Style\\") # Top-right subplot: Violin plot with \'whitegrid\' style and no left spine sns.set_style(\\"whitegrid\\") sns.violinplot(data=data, ax=axes[0, 1]) sns.despine(left=True, ax=axes[0, 1]) axes[0, 1].set_title(\\"Violinplot with Whitegrid Style and No Left Spine\\") # Bottom-left subplot: Lineplot with \'dark\' style and custom sinusoidal function sns.set_style(\\"dark\\") x = np.linspace(0, 10, 100) y = np.sin(x) sns.lineplot(x=x, y=y, ax=axes[1, 0]) axes[1, 0].set_title(\\"Lineplot with Dark Style and Sinusoidal Function\\") # Bottom-right subplot: Stripplot with \'ticks\' style and offset spines sns.set_style(\\"ticks\\") sns.stripplot(data=data, ax=axes[1, 1]) sns.despine(offset=10, trim=True, ax=axes[1, 1]) axes[1, 1].set_title(\\"Stripplot with Ticks Style and Offset Spines\\") # Finalize plt.tight_layout() plt.show()"},{"question":"**Question**: You are required to write a PyTorch-based function that will simulate the process of saving and loading a neural network model\'s weights. This function should also allow for the tuning of specific environment variables to observe their effects. # Task: 1. Implement a class `ModelManager` that includes methods for: - Saving a neural network\'s weights. - Loading a neural network\'s weights with the capability to use different environment variable configurations. 2. Provide test cases that: - Demonstrate the loading of weights with `weights_only=True` and `weights_only=False`. - Adjust the autograd shutdown wait limit to different values and observe any differences in behavior. # Details: Class: `ModelManager` - **Methods**: 1. `save_model(model: torch.nn.Module, filepath: str) -> None`: - Saves the weights of the given model to the specified filepath. 2. `load_model(model: torch.nn.Module, filepath: str, weights_only: Optional[bool]=None) -> None`: - Loads the weights of the given model from the specified filepath. - Make use of the environment variables `TORCH_FORCE_WEIGHTS_ONLY_LOAD` and `TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD` to set the `weights_only` parameter. 3. `set_autograd_shutdown_wait_limit(seconds: int) -> None`: - Sets the autograd shutdown wait limit. Example Usage: ```python import torch import torch.nn as nn # Define a simple neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 5) def forward(self, x): return self.fc1(x) # Instantiate the model and ModelManager model = SimpleNN() manager = ModelManager() # Save the model manager.save_model(model, \'model_weights.pth\') # Load the model with weights_only=True manager.load_model(model, \'model_weights.pth\', weights_only=True) # Set autograd shutdown wait limit to 5 seconds manager.set_autograd_shutdown_wait_limit(5) ``` # Constraints and Notes: - Do not use external libraries other than PyTorch. - Provide appropriate exception handling to manage cases where file paths are incorrect or loading fails. - Ensure that the code works for both CPU and GPU. # Performance Requirements: - The implementation should efficiently handle the saving and loading of models with a size of up to 100MB. - The autograd shutdown wait limit should be configurable and reflect any changes in behavior promptly. **Hint**: Refer to `torch.load`, `torch.save`, and manipulating environment variables in Python using the `os` module for completing this task.","solution":"import torch import torch.nn as nn import os class ModelManager: def save_model(self, model: nn.Module, filepath: str) -> None: Saves the weights of the given model to the specified filepath. try: torch.save(model.state_dict(), filepath) print(f\\"Model weights saved to {filepath}\\") except Exception as e: print(f\\"Error saving the model weights: {e}\\") def load_model(self, model: nn.Module, filepath: str, weights_only: bool = None) -> None: Loads the weights of the given model from the specified filepath. Uses environment variables to determine \'weights_only\' parameter if not provided. if weights_only is None: weights_only = os.getenv(\'TORCH_FORCE_WEIGHTS_ONLY_LOAD\', \'false\').lower() == \'true\' weights_only = os.getenv(\'TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\', \'false\').lower() != \'true\' try: checkpoint = torch.load(filepath) if weights_only: model.load_state_dict(checkpoint) print(f\\"Model weights loaded from {filepath}\\") else: model.load_state_dict(checkpoint[\'model_state_dict\']) print(f\\"Full model checkpoint loaded from {filepath}\\") except Exception as e: print(f\\"Error loading the model: {e}\\") def set_autograd_shutdown_wait_limit(self, seconds: int) -> None: Sets the autograd shutdown wait limit. try: os.environ[\'TORCH_AUTODIFF_SHUTDOWN_WAIT_LIMIT\'] = str(seconds) print(f\\"Autograd shutdown wait limit set to {seconds} seconds\\") except Exception as e: print(f\\"Error setting the autograd shutdown wait limit: {e}\\")"},{"question":"# **Question: Implementing a Custom Integer Collection** In this task, you will implement a custom collection class called `IntegerCollection` that emulates a list specifically for integer values. This collection should enforce that only integers are stored within it and should provide some additional functionality as specified below. **Class Requirements** 1. **Initialization**: - The class should be initialized with an optional list of integers. If no list is provided, it initializes with an empty collection. - ```python col = IntegerCollection([1, 2, 3]) col = IntegerCollection() ``` 2. **Adding Elements**: - Implement a method `add(self, value: int)` to add an integer to the collection. Raise a `TypeError` if the value is not an integer. - ```python col.add(4) ``` 3. **Special Methods**: - Implement `__repr__(self)` to return a string representation of the collection. - Implement `__str__(self)` to provide a user-friendly string representation. - Implement `__len__(self)` to return the number of elements in the collection. - Implement `__contains__(self, item: int)` to check if an integer is in the collection. - Implement `__getitem__(self, index: int)` to retrieve an item by its index. - Implement `__setitem__(self, index: int, value: int)` to set the item at a specific index (should only allow integers). 4. **Emulating List Methods**: - Implement the `__iter__(self)` method to iterate over the collection. - Implement the `__add__(self, other)` method to concatenate the collection with another integer collection. - Implement the `__eq__(self, other)` method to compare if two collections have the same elements in the same order. 5. **Mutability and Immutability**: - Add a method `make_immutable(self)` that makes the collection immutable. - Raise appropriate exceptions if mutable operations `add` or `setitem` are attempted on an immutable collection. **Example Usage** ```python col1 = IntegerCollection([1, 2, 3]) col2 = IntegerCollection([4, 5]) print(len(col1)) # Output: 3 print(2 in col1) # Output: True print(col1) # Output: IntegerCollection([1, 2, 3]) col1.add(4) print(col1[3]) # Output: 4 for item in col1: print(item) col3 = col1 + col2 print(col3) # Output: IntegerCollection([1, 2, 3, 4, 4, 5]) col1.make_immutable() col1.add(5) # Raises TypeError: Collection is immutable ``` **Constraints** - Only integers are allowed as elements. - Implement the class methods following Python\'s data model guidelines. **Function Signature** ```python class IntegerCollection: def __init__(self, initial=None): # Your code here def add(self, value: int): # Your code here def __repr__(self): # Your code here def __str__(self): # Your code here def __len__(self): # Your code here def __contains__(self, item: int): # Your code here def __getitem__(self, index: int): # Your code here def __setitem__(self, index: int, value: int): # Your code here def __iter__(self): # Your code here def __add__(self, other): # Your code here def __eq__(self, other): # Your code here def make_immutable(self): # Your code here ```","solution":"class IntegerCollection: def __init__(self, initial=None): if initial is None: self._data = [] else: if not all(isinstance(x, int) for x in initial): raise TypeError(\\"All elements must be integers.\\") self._data = initial[:] self._immutable = False def add(self, value: int): if self._immutable: raise TypeError(\\"Collection is immutable\\") if not isinstance(value, int): raise TypeError(\\"Only integers can be added\\") self._data.append(value) def __repr__(self): return f\\"IntegerCollection({self._data})\\" def __str__(self): return f\\"IntegerCollection contains: {self._data}\\" def __len__(self): return len(self._data) def __contains__(self, item: int): return item in self._data def __getitem__(self, index: int): return self._data[index] def __setitem__(self, index: int, value: int): if self._immutable: raise TypeError(\\"Collection is immutable\\") if not isinstance(value, int): raise TypeError(\\"Only integers can be added\\") self._data[index] = value def __iter__(self): return iter(self._data) def __add__(self, other): if not isinstance(other, IntegerCollection): raise TypeError(\\"Can only add another IntegerCollection\\") return IntegerCollection(self._data + other._data) def __eq__(self, other): if not isinstance(other, IntegerCollection): return False return self._data == other._data def make_immutable(self): self._immutable = True"},{"question":"Objective: Create a function that helps in optimizing the memory usage of a given DataFrame by converting its columns to appropriate data types without affecting the stored data or its operations. Problem Statement: Write a function `optimize_memory_usage(df: pd.DataFrame) -> pd.DataFrame` that takes in a pandas DataFrame `df` and returns a new DataFrame with optimized memory usage. Input: - A pandas DataFrame `df` which may contain columns with different data types including `int64`, `float64`, `datetime64[ns]`, `timedelta64[ns]`, `complex128`, `object`, and `bool`. Output: - A new DataFrame with the same data as the input, but with memory usage optimized by: 1. Converting `object` type columns with few unique values to `category` type. 2. Converting `float64` and `int64` columns to lower precision where possible without data loss. 3. Converting integer columns that can hold null values to nullable integer types like `pd.Int64Dtype()`. Constraints: - You must ensure that the data types are converted in such a way that the data itself remains unaffected. - The function should be capable of handling reasonably large DataFrames efficiently. Example: ```python import pandas as pd import numpy as np # Sample DataFrame dtypes = [ \\"int64\\", \\"float64\\", \\"datetime64[ns]\\", \\"timedelta64[ns]\\", \\"complex128\\", \\"object\\", \\"bool\\" ] n = 10000 data = {t: np.random.choice([1, np.nan], size=n).astype(t) for t in dtypes} df = pd.DataFrame(data) df[\\"categorical\\"] = df[\\"object\\"].astype(\\"category\\") # Function Implementation optimized_df = optimize_memory_usage(df) # Output the original and optimized memory usage print(\\"Original memory usage:\\") print(df.memory_usage(deep=True)) print(\\"nOptimized memory usage:\\") print(optimized_df.memory_usage(deep=True)) ``` Notes: - Manual inspection of the optimized DataFrame to ensure no data or precision is lost is recommended. - Utilize `DataFrame.memory_usage(deep=True)` before and after optimization to show the effectiveness of your function. - Consider edge cases such as mixed data types within the same column or completely null columns. Good luck!","solution":"import pandas as pd import numpy as np def optimize_memory_usage(df: pd.DataFrame) -> pd.DataFrame: Optimize the memory usage of a pandas DataFrame by modifying its column data types. Parameters: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: A DataFrame with optimized memory usage. optimized_df = df.copy() for col in optimized_df.columns: col_type = optimized_df[col].dtypes if col_type == \'object\': # Convert object columns to category if relatively few unique values if len(optimized_df[col].unique()) / len(optimized_df[col]) < 0.5: optimized_df[col] = optimized_df[col].astype(\'category\') elif col_type == \'int64\': # Convert int64 columns to smaller int types if possible if optimized_df[col].min() >= np.iinfo(np.int8).min and optimized_df[col].max() <= np.iinfo(np.int8).max: optimized_df[col] = optimized_df[col].astype(\'int8\') elif optimized_df[col].min() >= np.iinfo(np.int16).min and optimized_df[col].max() <= np.iinfo(np.int16).max: optimized_df[col] = optimized_df[col].astype(\'int16\') elif optimized_df[col].min() >= np.iinfo(np.int32).min and optimized_df[col].max() <= np.iinfo(np.int32).max: optimized_df[col] = optimized_df[col].astype(\'int32\') elif col_type == \'float64\': # Convert float64 columns to float32 if possible optimized_df[col] = optimized_df[col].astype(\'float32\') elif col_type == \'int64\' and optimized_df[col].hasnans: # Convert integer columns with NaNs to nullable integer types optimized_df[col] = optimized_df[col].astype(pd.Int64Dtype()) return optimized_df"},{"question":"**Objective**: Implement a function that validates and processes a list of IP addresses and network definitions using the `ipaddress` module. This function will categorize the inputs, handle invalid entries gracefully, and output summarized information. **Problem Statement**: - Write a function `process_ip_data(ip_data: list) -> dict`. - Given a list `ip_data` containing mixed valid and invalid IP address and network strings, the function should categorize the entries as IPv4 addresses, IPv6 addresses, IPv4 networks, IPv6 networks, and invalid entries. - The function should return a dictionary summarizing the categorized results. **Function Signature**: ```python def process_ip_data(ip_data: list) -> dict: pass ``` **Input**: - `ip_data` (List[str]): A list where each element is a string that may represent an IP address or network. **Output**: - Returns a dictionary with the following structure: ```python { \\"IPv4_addresses\\": List[ipaddress.IPv4Address], \\"IPv6_addresses\\": List[ipaddress.IPv6Address], \\"IPv4_networks\\": List[ipaddress.IPv4Network], \\"IPv6_networks\\": List[ipaddress.IPv6Network], \\"invalid_entries\\": List[str] } ``` **Constraints**: - It is guaranteed that the list will have at least one element. - Strings that cannot be parsed as valid IP addresses or networks should be added to the `invalid_entries` list. **Examples**: ```python data = [ \'192.0.2.1\', \'2001:db8::1\', \'192.0.2.0/24\', \'2001:db8::/96\', \'invalid_ip\', \'192.168.0.256\', \'192.168.0.1/64\' ] result = process_ip_data(data) ``` Expected output: ```python { \\"IPv4_addresses\\": [IPv4Address(\'192.0.2.1\')], \\"IPv6_addresses\\": [IPv6Address(\'2001:db8::1\')], \\"IPv4_networks\\": [IPv4Network(\'192.0.2.0/24\')], \\"IPv6_networks\\": [IPv6Network(\'2001:db8::/96\')], \\"invalid_entries\\": [\'invalid_ip\', \'192.168.0.256\', \'192.168.0.1/64\'] } ``` **Notes**: - Utilize the `ipaddress` module to parse and classify the entries. - Handle exceptions to place invalid entries into the appropriate category. - Ensure the function is efficient and handles edge cases robustly.","solution":"import ipaddress def process_ip_data(ip_data: list) -> dict: Process a list of IP addresses and networks, categorizing them into valid IPv4, IPv6 addresses, IPv4, IPv6 networks, and invalid entries. Args: ip_data: List[str] - A list of strings representing IP addresses or networks. Returns: dict - A dictionary summarizing the categorized results: { \\"IPv4_addresses\\": List[ipaddress.IPv4Address], \\"IPv6_addresses\\": List[ipaddress.IPv6Address], \\"IPv4_networks\\": List[ipaddress.IPv4Network], \\"IPv6_networks\\": List[ipaddress.IPv6Network], \\"invalid_entries\\": List[str] } result = { \\"IPv4_addresses\\": [], \\"IPv6_addresses\\": [], \\"IPv4_networks\\": [], \\"IPv6_networks\\": [], \\"invalid_entries\\": [] } for entry in ip_data: try: if \'/\' in entry: network = ipaddress.ip_network(entry, strict=False) if isinstance(network, ipaddress.IPv4Network): result[\\"IPv4_networks\\"].append(network) elif isinstance(network, ipaddress.IPv6Network): result[\\"IPv6_networks\\"].append(network) else: addr = ipaddress.ip_address(entry) if isinstance(addr, ipaddress.IPv4Address): result[\\"IPv4_addresses\\"].append(addr) elif isinstance(addr, ipaddress.IPv6Address): result[\\"IPv6_addresses\\"].append(addr) except ValueError: result[\\"invalid_entries\\"].append(entry) return result"},{"question":"**Objective:** Demonstrate proficiency in using seaborn to create various kernel density estimate (KDE) plots, manipulate plot aesthetics, and interpret data visualizations. **Problem Statement:** You are provided with a dataset containing information about different species of flowers from the famous Iris dataset. Your task is to create a function that visualizes the distribution of petal lengths for different species using `seaborn`. Additionally, you should provide options to modify the plot\'s appearance and show conditional distributions. **Function Signature:** ```python def plot_petal_length_distribution(hue: str = None, bandwidth_adjust: float = 1.0, fill: bool = False): Visualize the distribution of petal lengths for different species. Parameters: ----------- hue : str, optional (default=None) Variable name to map to different colors for conditional distributions. bandwidth_adjust : float, optional (default=1.0) Factor to adjust the bandwidth of the KDE plot. fill : bool, optional (default=False) Whether to fill the area under the KDE curves. Returns: -------- None # To be implemented by students ``` **Input:** - `hue` (optional): a string representing the column name to map different colors for conditional distributions. It could be \'species\' or `None`. - `bandwidth_adjust` (optional, default=1.0): a float to adjust the bandwidth of the KDE plot. Values less than 1 will reduce the smoothing, values greater than 1 will increase the smoothing. - `fill` (optional, default=False): a boolean to indicate if the area under the KDE curves should be filled. **Output:** - The function should directly plot the KDE of petal lengths, optionally colored by `species`, with adjustments for bandwidth and filled areas if specified. **Constraints:** - Assume the dataset is always available and contains the standard columns [\'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\', \'species\']. - Use seaborn and matplotlib for plotting. **Examples:** ```python # Example 1: # Basic KDE plot of petal_length plot_petal_length_distribution() # Example 2: # KDE plot of petal_length with different colors for each species plot_petal_length_distribution(hue=\\"species\\") # Example 3: # KDE plot of petal_length with increased bandwidth plot_petal_length_distribution(bandwidth_adjust=2.0) # Example 4: # Filled KDE plot of petal_length with different colors for each species plot_petal_length_distribution(hue=\\"species\\", fill=True) ``` **Notes:** - Use the Iris dataset from seaborn\'s built-in datasets. - Ensure the plot is well-labeled with a title, axis labels, and legend (if applicable). **Hints:** - You may find the seaborn documentation on `sns.kdeplot` helpful. - Consider using `sns.load_dataset(\\"iris\\")` to load the dataset.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_petal_length_distribution(hue: str = None, bandwidth_adjust: float = 1.0, fill: bool = False): Visualize the distribution of petal lengths for different species. Parameters: ----------- hue : str, optional (default=None) Variable name to map to different colors for conditional distributions. bandwidth_adjust : float, optional (default=1.0) Factor to adjust the bandwidth of the KDE plot. fill : bool, optional (default=False) Whether to fill the area under the KDE curves. Returns: -------- None # Load the Iris dataset iris = sns.load_dataset(\\"iris\\") # Create KDE plot sns.kdeplot( data=iris, x=\\"petal_length\\", hue=hue, bw_adjust=bandwidth_adjust, fill=fill ) # Set plot labels and title plt.xlabel(\\"Petal Length\\") plt.ylabel(\\"Density\\") plt.title(\\"Distribution of Petal Lengths\\") plt.show()"},{"question":"# Advanced Web Fetching with `urllib`: A Comprehensive Task **Objective**: Demonstrate your proficiency in using the `urllib` package to fetch web resources, handle HTTP headers, manage exceptions, and configure custom openers. **Problem Statement**: Write a Python function `fetch_web_data(url: str, data: dict = None, headers: dict = None, auth: tuple = None, use_proxy: bool = False) -> dict` that performs an HTTP request and returns a dictionary with the response\'s URL, headers, status code, and content. **Function Signature**: ```python def fetch_web_data(url: str, data: dict = None, headers: dict = None, auth: tuple = None, use_proxy: bool = False) -> dict: pass ``` # Detailed Requirements: 1. **Fetch Method**: - If `data` is provided, perform a POST request with the provided data. If not, perform a GET request. - The data, if provided, should be URL-encoded before sending. 2. **Custom Headers**: - If `headers` are provided, include these headers in the request. 3. **Basic Authentication**: - If `auth` is provided, it should be a tuple `(username, password)`. Use HTTP Basic Authentication with these credentials. 4. **Proxy Handling**: - If `use_proxy` is `True`, configure a proxy handler. - If `use_proxy` is `False`, ensure the request bypasses any proxy configurations. 5. **Exception Handling**: - Properly handle `URLError` and `HTTPError`. - For a `URLError`, include the `reason` in the output dictionary under the key `error`. - For an `HTTPError`, include the `code` and `reason` in the output dictionary under the keys `error_code` and `error`. 6. **Timeout**: - Set a global timeout of 10 seconds for all socket connections. 7. **Output**: - The function should return a dictionary with the keys: - `requested_url`: The original URL requested. - `final_url`: The final URL after any redirects. - `status_code`: The HTTP status code of the response. - `headers`: A dictionary of response headers. - `content`: The body content of the response in bytes. - `error`: (If an error occurs) The reason for the error. - `error_code`: (If an HTTPError occurs) The HTTP status code of the error. # Example Usage: ```python url = \\"http://example.com\\" data = {\\"key\\": \\"value\\"} headers = {\\"User-Agent\\": \\"Mozilla/5.0\\"} auth = (\\"user\\", \\"password\\") response = fetch_web_data(url, data=data, headers=headers, auth=auth, use_proxy=False) print(response) ``` # Constraints: - You may use only the `urllib`, `socket`, `shutil`, and `tempfile` modules. - Ensure robust error handling and clear separation of different functionality. # Performance Requirements: - Optimize for readability and maintainability. - Ensure that the function handles varying kinds of HTTP responses adequately. Good luck, and happy coding!","solution":"import urllib.request import urllib.parse import socket from urllib.error import URLError, HTTPError def fetch_web_data(url: str, data: dict = None, headers: dict = None, auth: tuple = None, use_proxy: bool = False) -> dict: # Set a global timeout for socket connections socket.setdefaulttimeout(10) # Encode the data if provided if data is not None: data = urllib.parse.urlencode(data).encode() # Create a request object request = urllib.request.Request(url, data=data) # Add headers if provided if headers: for header, value in headers.items(): request.add_header(header, value) # Handle Basic Authentication if auth: username_password = f\\"{auth[0]}:{auth[1]}\\" encoded_credentials = urllib.request.base64.b64encode(username_password.encode()).decode() request.add_header(\\"Authorization\\", f\\"Basic {encoded_credentials}\\") # Handle proxy settings if use_proxy: proxy_handler = urllib.request.ProxyHandler() else: proxy_handler = urllib.request.ProxyHandler({}) opener = urllib.request.build_opener(proxy_handler) urllib.request.install_opener(opener) response_dict = { \\"requested_url\\": url, \\"final_url\\": None, \\"status_code\\": None, \\"headers\\": None, \\"content\\": None, \\"error\\": None, \\"error_code\\": None } try: with urllib.request.urlopen(request) as response: response_dict[\\"final_url\\"] = response.geturl() response_dict[\\"status_code\\"] = response.getcode() response_dict[\\"headers\\"] = dict(response.info()) response_dict[\\"content\\"] = response.read() except HTTPError as e: response_dict[\\"error\\"] = e.reason response_dict[\\"error_code\\"] = e.code except URLError as e: response_dict[\\"error\\"] = e.reason return response_dict"},{"question":"You have been given the task of writing a script to manage users in a Unix-like system by interfacing with the shadow password database using the `spwd` module. Your task consists of two parts: Part 1: Retrieve Shadow Password Entry for a User Write a function `get_shadow_password_entry(username)` that takes a single parameter: - `username` (str): The user\'s login name. The function should return a dictionary representing the user\'s shadow password entry if retrieved successfully. The dictionary should have the following keys: - `\\"sp_namp\\"` - `\\"sp_pwdp\\"` - `\\"sp_lstchg\\"` - `\\"sp_min\\"` - `\\"sp_max\\"` - `\\"sp_warn\\"` - `\\"sp_inact\\"` - `\\"sp_expire\\"` - `\\"sp_flag\\"` If the entry cannot be found or the user does not have the necessary privileges, raise an appropriate exception with a clear message. Part 2: List All Shadow Password Entries Write a function `list_all_shadow_password_entries()` that does not take any parameters. The function should return a list of dictionaries, with each dictionary representing a shadow password entry in the format specified above. Function Signatures ```python def get_shadow_password_entry(username: str) -> dict: pass def list_all_shadow_password_entries() -> list: pass ``` Examples ```python # Example usage of get_shadow_password_entry try: entry = get_shadow_password_entry(\'user1\') print(entry) except Exception as e: print(e) # Example usage of list_all_shadow_password_entries try: entries = list_all_shadow_password_entries() for entry in entries: print(entry) except Exception as e: print(e) ``` Constraints and Assumptions - The functions will be executed on a Unix-like system where the `spwd` module is available. - The user running the script has the necessary privileges to access the shadow password database. - You should handle exceptions gracefully and provide meaningful error messages. Performance Considerations - The functions should efficiently access and represent the data considering potential large number of entries in the shadow password database. Good luck, and ensure your code is well-documented and tested for various edge cases.","solution":"import spwd def get_shadow_password_entry(username: str) -> dict: Retrieves the shadow password entry for a specific user. Args: username (str): The user\'s login name. Returns: dict: A dictionary representing the user\'s shadow password entry. Raises: KeyError: If the user does not exist. PermissionError: If the user does not have the necessary privileges. try: shadow_entry = spwd.getspnam(username) except KeyError: raise KeyError(f\\"User {username} does not exist.\\") except PermissionError: raise PermissionError(f\\"Insufficient privileges to access the shadow password database.\\") return { \\"sp_namp\\": shadow_entry.sp_nam, \\"sp_pwdp\\": shadow_entry.sp_pwd, \\"sp_lstchg\\": shadow_entry.sp_lstchg, \\"sp_min\\": shadow_entry.sp_min, \\"sp_max\\": shadow_entry.sp_max, \\"sp_warn\\": shadow_entry.sp_warn, \\"sp_inact\\": shadow_entry.sp_inact, \\"sp_expire\\": shadow_entry.sp_expire, \\"sp_flag\\": shadow_entry.sp_flag } def list_all_shadow_password_entries() -> list: Lists all shadow password entries. Returns: list: A list of dictionaries, each representing a shadow password entry. Raises: PermissionError: If the user does not have the necessary privileges. try: all_entries = spwd.getspall() except PermissionError: raise PermissionError(f\\"Insufficient privileges to access the shadow password database.\\") result = [] for entry in all_entries: result.append({ \\"sp_namp\\": entry.sp_nam, \\"sp_pwdp\\": entry.sp_pwd, \\"sp_lstchg\\": entry.sp_lstchg, \\"sp_min\\": entry.sp_min, \\"sp_max\\": entry.sp_max, \\"sp_warn\\": entry.sp_warn, \\"sp_inact\\": entry.sp_inact, \\"sp_expire\\": entry.sp_expire, \\"sp_flag\\": entry.sp_flag }) return result"},{"question":"# Python Debugger (pdb) Coding Assessment # Objective Assess your understanding and ability to use `pdb`, Python\'s built-in debugger, to troubleshoot and analyze code execution. # Task You are given a function `calculate_sum` which calculates the sum of squares of numbers in a given list. However, this function has some issues that need to be debugged. Your task is to: 1. Identify and fix the issues within the `calculate_sum` function using `pdb`. 2. Document the steps taken to debug the function. # Function Definition ```python def calculate_sum(numbers): total = 0 for num in nums: total += num**2 return total ``` # Instructions 1. **Set Breakpoints**: Use `pdb.set_trace()` within the function to set breakpoints. 2. **Debug the Code**: Run the function with `pdb.run()` or any other suitable method, stepping through the code and using `pdb` commands to inspect variables and flow. 3. **Fix the Errors**: Identify and correct any errors found during debugging. 4. **Document Your Process**: Provide a brief documentation of the steps you took to debug the function, the commands you used, and the fixes you made. # Example Usage ```python import pdb def calculate_sum(numbers): total = 0 pdb.set_trace() # Set a breakpoint here for num in nums: # There is a typo here which you need to find total += num**2 return total # Example list numbers = [1, 2, 3, 4] # Use pdb to run the function pdb.run(\'calculate_sum(numbers)\') ``` # Expected Output The function should return `30` for the given example list `[1, 2, 3, 4]`. # Submission Submit your corrected `calculate_sum` function and the documentation of your debugging process. # Constraints - Use `pdb` to actively debug and find issues. - Documenting the process is as important as fixing the code. - Ensure your function handles lists of integers. # Evaluation Your submission will be evaluated on: 1. **Correctness**: Does your final function produce the correct output? 2. **Debugging Steps**: How effectively did you use `pdb` to debug the function? 3. **Documentation**: Clarity and detail of the debugging process documentation.","solution":"def calculate_sum(numbers): Calculates the sum of the squares of the numbers in the input list. Args: numbers (list of int): List of integer numbers. Returns: int: Sum of the squares of the numbers. total = 0 for num in numbers: total += num ** 2 return total Debugging Process: 1. I inserted `pdb.set_trace()` at the beginning of the function to start the debugger. 2. Ran the function using `pdb.run(\'calculate_sum([1, 2, 3, 4])\')` 3. The line `for num in nums:` threw an error because `nums` was not defined. 4. Fixed the typo by changing `nums` to `numbers`. 5. Re-ran the debugger to ensure the function executed correctly. # Example usage # import pdb # pdb.run(\'calculate_sum([1, 2, 3, 4])\') # Output should be 30"},{"question":"**Objective:** To assess your understanding of PyTorch\'s `torch.fx` module by implementing a transformation function that optimizes a given PyTorch model. This transformation will specifically replace `torch.add` operations with `torch.mul` operations in the computational graph of the model. **Task:** Implement a function `transform` that takes in a `torch.nn.Module` and returns a transformed version of the module where all instances of `torch.add` operations are replaced with `torch.mul` operations. **Function Signature:** ```python import torch import torch.fx def transform(m: torch.nn.Module) -> torch.nn.Module: pass ``` **Requirements:** 1. The input to the function is a PyTorch module (`torch.nn.Module`) that you need to transform. 2. The output should be a PyTorch module (`torch.nn.Module`) with all `torch.add` operations replaced by `torch.mul` operations. 3. Use the `torch.fx` module for graph manipulation. 4. Ensure that the transformed model can be run and the replacements are correctly made. **Example:** ```python import torch class MyModule(torch.nn.Module): def forward(self, x, y): return torch.add(x, y) model = MyModule() transformed_model = transform(model) x = torch.tensor([1, 2, 3]) y = torch.tensor([4, 5, 6]) # The transformed model should use torch.mul instead of torch.add print(transformed_model(x, y)) # Should print tensor([ 4, 10, 18]) ``` **Constraints:** - Assume the input model is a valid `torch.nn.Module` instance. - The modified model should retain the same functionality except for the specified transformation. **Performance Requirements:** - The transformation process should have linear complexity with respect to the number of nodes in the computational graph, i.e., O(n), where n is the number of nodes. **Hints:** - Review the provided `torch.fx` documentation on graph manipulation for guidance on how to modify the computational graph. - Use `graph.lint()` to check the integrity of the modified graph. - Recompile the `GraphModule` after making the necessary changes. **Submission:** - Your solution should consist of the `transform` function implementation. - You may include additional helper functions if necessary. - Ensure your code is well-documented and includes comments explaining key steps and decisions.","solution":"import torch import torch.fx def transform(m: torch.nn.Module) -> torch.nn.Module: # Create a symbolic trace of the model using torch.fx traced = torch.fx.symbolic_trace(m) # Iterate through the nodes in the graph for node in traced.graph.nodes: # Check if the node is a call_function and the target is torch.add if node.op == \'call_function\' and node.target == torch.add: # Replace the target to torch.mul node.target = torch.mul # Check graph integrity traced.graph.lint() # Recompile the graph module transformed_model = torch.fx.GraphModule(traced, traced.graph) return transformed_model # Example model for testing class MyModule(torch.nn.Module): def forward(self, x, y): return torch.add(x, y) # Instantiate and transform the model model = MyModule() transformed_model = transform(model) # Verify the transformation x = torch.tensor([1, 2, 3]) y = torch.tensor([4, 5, 6]) print(transformed_model(x, y)) # Should print tensor([ 4, 10, 18])"},{"question":"# Seaborn Coding Assessment Problem Description You are given sales data for different product categories over several months. Your task is to create a clear and aesthetically pleasing visualization to analyze the sales trends of these categories using the seaborn library. The visualization should include a line plot for each product category, properly labeled axes, a legend, and a title. Data Format The input data is provided as a CSV file with the following format: | Month | Category | Sales | |---------|------------|-------| | Jan-2023| Electronics| 1500 | | Feb-2023| Electronics| 1800 | | ... | ... | ... | - **Month:** The month of the observation in \\"MMM-YYYY\\" format (e.g., \\"Jan-2023\\"). - **Category:** The category of the product (e.g., \\"Electronics\\", \\"Furniture\\", etc.). - **Sales:** The sales figure for the given category and month. Input - Path to the CSV file containing the sales data. Output - A seaborn line plot visualizing the sales trends for different categories. Constraints 1. The CSV file will always be correctly formatted. 2. The data will include at least 3 different product categories. 3. The data will cover a period of at least 6 months. Performance Requirements - The solution should efficiently handle datasets up to 1000 rows. Implementation Write a function `plot_sales_trends(filepath: str) -> None` that takes the path to the CSV file as input and generates the required plot. Example Given a CSV file with the following content: ``` Month,Category,Sales Jan-2023,Electronics,1500 Feb-2023,Electronics,1800 Jan-2023,Furniture,800 Feb-2023,Furniture,1100 Jan-2023,Clothing,500 Feb-2023,Clothing,400 ``` The function call: ```python plot_sales_trends(\'sales_data.csv\') ``` Should produce a plot with separate lines for \\"Electronics\\", \\"Furniture\\", and \\"Clothing\\" categories, and display the sales trend over time. **Additional Requirements:** - Use different colors for different categories. - Use seaborn’s `set_theme()` function to set an appropriate theme. - Ensure that the x-axis represents the months and the y-axis represents the sales. - Include a legend that clearly identifies each category. - Add a title to the plot indicating it is a Sales Trend Analysis.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_sales_trends(filepath: str) -> None: Generates a seaborn line plot visualizing the sales trends for different categories. :param filepath: Path to the CSV file containing sales data. # Read the CSV file into a DataFrame data = pd.read_csv(filepath) # Convert the \'Month\' column to datetime data[\'Month\'] = pd.to_datetime(data[\'Month\'], format=\'%b-%Y\') # Set the seaborn theme sns.set_theme(style=\'whitegrid\') # Create a line plot plt.figure(figsize=(12, 6)) sns.lineplot(data=data, x=\'Month\', y=\'Sales\', hue=\'Category\', marker=\'o\') # Label the axes plt.xlabel(\'Month\') plt.ylabel(\'Sales\') # Add a title plt.title(\'Sales Trend Analysis\') # Display the legend plt.legend(title=\'Category\') plt.show()"},{"question":"**Question: Implementing Robust File Operations** # Objective Design a Python function that emulates a lower-level file operation using Python\'s built-in file handling capabilities. This function should demonstrate a fundamental understanding of file I/O, error handling, and resource management. # Function Signature ```python def robust_file_operation(filepath: str, content: str, operation: str) -> str: Perform a specified file operation (read or write) on the given file. Parameters: - filepath (str): Path to the file on which the operation is to be performed. - content (str): Content to be written to the file if the operation is \'write\'. Ignored if the operation is \'read\'. - operation (str): The operation to perform. Must be either \'read\' or \'write\'. Returns: - str: Contents of the file if the operation is \'read\', or a confirmation message if the operation is \'write\'. Raises: - ValueError: If the operation is not \'read\' or \'write\'. - FileNotFoundError: If the file does not exist when attempting to read. - IOError: For any I/O operation failure. pass ``` # Task 1. **File Writing:** - If the `operation` parameter is \'write\', open the file specified by `filepath` in write mode. - Write the `content` to the file. - Close the file and return a confirmation message: \\"Write operation successful.\\" 2. **File Reading:** - If the `operation` parameter is \'read\', open the file specified by `filepath` in read mode. - Read the contents of the file. - Close the file and return the contents as a string. 3. **Error Handling:** - Raise a `ValueError` if the `operation` is not \'read\' or \'write\'. - Handle cases where the file does not exist when attempting to read, raising a `FileNotFoundError`. - Catch and re-raise any other I/O exceptions with an appropriate error message. # Constraints - You must use Python\'s built-in file handling functions and ensure proper closing of file resources. - The function should be robust and handle edge cases appropriately. # Example Usage ```python # Writing to a file result = robust_file_operation(\\"example.txt\\", \\"Hello, World!\\", \\"write\\") print(result) # Output: \\"Write operation successful.\\" # Reading from a file result = robust_file_operation(\\"example.txt\\", \\"\\", \\"read\\") print(result) # Output: \\"Hello, World!\\" # Handling invalid operation try: result = robust_file_operation(\\"example.txt\\", \\"\\", \\"delete\\") except ValueError as e: print(e) # Output: \\"Invalid operation. Only \'read\' and \'write\' are supported.\\" ``` Ensure that your implementation is clean, properly documented, and adheres to Python best practices for file handling and error management.","solution":"def robust_file_operation(filepath: str, content: str, operation: str) -> str: Perform a specified file operation (read or write) on the given file. Parameters: - filepath (str): Path to the file on which the operation is to be performed. - content (str): Content to be written to the file if the operation is \'write\'. Ignored if the operation is \'read\'. - operation (str): The operation to perform. Must be either \'read\' or \'write\'. Returns: - str: Contents of the file if the operation is \'read\', or a confirmation message if the operation is \'write\'. Raises: - ValueError: If the operation is not \'read\' or \'write\'. - FileNotFoundError: If the file does not exist when attempting to read. - IOError: For any I/O operation failure. if operation not in [\'read\', \'write\']: raise ValueError(\\"Invalid operation. Only \'read\' and \'write\' are supported.\\") try: if operation == \'write\': with open(filepath, \'w\') as file: file.write(content) return \\"Write operation successful.\\" elif operation == \'read\': with open(filepath, \'r\') as file: return file.read() except FileNotFoundError: raise FileNotFoundError(f\\"File \'{filepath}\' not found.\\") except IOError as e: raise IOError(f\\"An I/O error occurred: {e}\\")"},{"question":"# Pandas Coding Assessment: Analyzing Sales Data In this assessment, you are required to write a function that processes sales data to extract meaningful insights. The data is provided in a CSV file with the following columns: - `OrderDate`: The date the order was placed. - `Region`: The region where the order was placed. - `Rep`: The salesperson responsible for the order. - `Item`: The item sold. - `Units`: The number of units sold. - `UnitCost`: The cost per unit. - `Total`: The total cost. Your task is to implement the function `process_sales_data(file_path: str) -> pd.DataFrame` that reads the sales data from the given CSV file and returns a DataFrame with the following information for each region and item combination: - `Region`: The region of the sales. - `Item`: The item sold. - `TotalUnitsSold`: The total number of units sold. - `TotalSales`: The total sales amount. - `AvgUnitCost`: The average unit cost. The resulting DataFrame should have `Region` and `Item` as the indices and should be sorted by `Region` and `Item`. Function Signature ```python import pandas as pd def process_sales_data(file_path: str) -> pd.DataFrame: pass ``` Input - `file_path` (str): A string representing the path to the CSV file that contains the sales data. Output - A pandas DataFrame containing the summarized sales data. Constraints - Assume the CSV file is well-formed and follows the above-mentioned structure. Example Consider the following sales data in `sales_data.csv`: ```csv OrderDate,Region,Rep,Item,Units,UnitCost,Total 2023-01-01,East,John,Widget,10,5.00,50.00 2023-01-02,West,Jane,Widget,20,5.00,100.00 2023-01-03,East,John,Thing,30,2.50,75.00 2023-01-04,West,Jane,Thing,40,2.50,100.00 ``` Calling `process_sales_data(\'sales_data.csv\')` should return: ```python TotalUnitsSold TotalSales AvgUnitCost Region Item East Thing 30 75.0 2.50 Widget 10 50.0 5.00 West Thing 40 100.0 2.50 Widget 20 100.0 5.00 ``` Notes - You should use appropriate pandas functions to implement the solution, demonstrating your understanding of data manipulation, grouping, aggregation, and reshaping functionalities in pandas. - Ensure that your solution is efficient and makes use of vectorized operations wherever possible.","solution":"import pandas as pd def process_sales_data(file_path: str) -> pd.DataFrame: # Read the CSV file into a DataFrame df = pd.read_csv(file_path) # Group by \'Region\' and \'Item\' and aggregate the required metrics summary_df = df.groupby([\'Region\', \'Item\']).agg( TotalUnitsSold=(\'Units\', \'sum\'), TotalSales=(\'Total\', \'sum\'), AvgUnitCost=(\'UnitCost\', \'mean\') ).reset_index() # Set \'Region\' and \'Item\' as indices and sort the DataFrame summary_df = summary_df.set_index([\'Region\', \'Item\']).sort_index() return summary_df"},{"question":"You are tasked with creating a Python script that periodically checks the status of multiple URLs and alerts when any of the URLs become inaccessible (i.e., return a status code other than 200). **Requirements**: 1. Your solution should use the `urllib.request` module. 2. Implement a function `check_urls(urls: list, interval: int)` that: - Takes a list of URLs and an interval (in seconds). - Periodically (every `interval` seconds) checks each URL and prints the status code. If a URL returns a status code other than 200, print an alert message. 3. Use appropriate exception handling to manage and log potential errors (e.g., network errors, invalid URLs). 4. Use custom handlers if necessary to manage redirects or proxy settings. **Function Signature**: ```python import urllib.request def check_urls(urls: list, interval: int) -> None: # Your implementation here pass ``` **Example**: ```python urls = [ \'http://www.google.com\', \'http://www.nonexistentwebsite1234.com\', \'http://www.python.org\' ] check_urls(urls, 10) ``` **Expected Output** (this would run indefinitely, checking every 10 seconds): ``` URL: http://www.google.com - Status: 200 URL: http://www.nonexistentwebsite1234.com - Status: URLError ALERT: http://www.nonexistentwebsite1234.com is down! URL: http://www.python.org - Status: 200 ... (repeats every 10 seconds) ``` **Constraints**: - Handle both HTTP and HTTPS URLs. - Manage interval-based execution without blocking other processes if required. Ensure your solution is efficient, handles various edge cases, and adheres to best coding practices.","solution":"import urllib.request import time import logging logging.basicConfig(level=logging.INFO) def check_url(url): try: response = urllib.request.urlopen(url) status_code = response.getcode() if status_code != 200: logging.warning(f\\"ALERT: {url} is down! Status: {status_code}\\") else: logging.info(f\\"URL: {url} - Status: {status_code}\\") except urllib.error.URLError as e: logging.error(f\\"ALERT: {url} is down! URLError: {e.reason}\\") except Exception as e: logging.error(f\\"ALERT: {url} encountered an unexpected error: {str(e)}\\") def check_urls(urls: list, interval: int) -> None: while True: for url in urls: check_url(url) time.sleep(interval)"},{"question":"**Objective:** Create visualizations to analyze relationships in a dataset using Seaborn\'s `seaborn.objects` module. **Context:** You are provided with a dataset \'mpg\', which contains information about various car attributes including \'horsepower\', \'mpg\' (miles per gallon), \'origin\', and \'weight\'. Your task is to create a series of visualizations that explore these relationships effectively using the Seaborn\'s object interface. **Tasks:** 1. **Load the dataset:** Use the Seaborn\'s `load_dataset` function to load the \'mpg\' dataset. 2. **Scatter Plot Visualization:** - Create a scatter plot showing the relationship between \'horsepower\' and \'mpg\'. - Use different colors for points based on the \'origin\' of the car. 3. **Enhanced Scatter Plot with Filled Markers:** - Create a similar scatter plot as above, but this time set the fill color based on \'weight\' and stroke color based on \'origin\'. - Make sure to use different markers to distinguish between \'origin\' values and adjust opacity to 50% for both stroke and fill colors. 4. **Scatter Plot with Jitter:** - Create a scatter plot showing the relationship between \'horsepower\' and \'origin\' with jitter applied to prevent overplotting. **Requirements:** - Utilize `seaborn.objects.Plot` and `seaborn.objects.Dots` for creating the visualizations. - Ensure color differentiation and opacity settings are effectively used. - Apply jitter appropriately to distribute overlapping data points. **Constraints:** - Do not use the basic `seaborn` or `matplotlib` plotting functions. - Utilize only the `seaborn.objects` functionalities as demonstrated. **Expected Input:** No direct input required; using Seaborn\'s `load_dataset` to load \'mpg\'. **Expected Output:** Three distinct scatter plots saved as images or displayed inline in a Jupyter Notebook. ```python # Import necessary libraries import seaborn.objects as so from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # Task 2: Scatter Plot Visualization of \'horsepower\' vs \'mpg\' p1 = so.Plot(mpg, \\"horsepower\\", \\"mpg\\").add(so.Dots(), color=\\"origin\\") p1.show() # Task 3: Enhanced Scatter Plot with filled markers p2 = ( so.Plot(mpg, \\"horsepower\\", \\"mpg\\") .add(so.Dots(fillalpha=0.5), color=\\"origin\\", fillcolor=\\"weight\\") .scale(fillcolor=\\"binary\\") ) p2.show() # Task 4: Scatter Plot with Jitter showing \'horsepower\' vs \'origin\' p3 = ( so.Plot(mpg, \\"horsepower\\", \\"origin\\") .add(so.Dots(), so.Jitter(0.25)) ) p3.show() ```","solution":"# Import necessary libraries import seaborn.objects as so from seaborn import load_dataset def load_data(): Loads the \'mpg\' dataset. return load_dataset(\\"mpg\\") def scatter_plot_hp_vs_mpg(mpg): Creates a scatter plot showing the relationship between \'horsepower\' and \'mpg\', with different colors for points based on the \'origin\' of the car. plot1 = so.Plot(mpg, \\"horsepower\\", \\"mpg\\").add(so.Dots(), color=\\"origin\\") plot1.show() def enhanced_scatter_plot_hp_vs_mpg(mpg): Creates an enhanced scatter plot of \'horsepower\' vs \'mpg\', with fill color based on \'weight\' and stroke color based on \'origin\'. Distinguish between \'origin\' values using different markers and adjust opacity to 50% for both stroke and fill colors. plot2 = ( so.Plot(mpg, \\"horsepower\\", \\"mpg\\") .add(so.Dots(fillalpha=0.5), color=\\"origin\\", fillcolor=\\"weight\\") .scale(fillcolor=\\"binary\\") ) plot2.show() def scatter_plot_with_jitter_hp_vs_origin(mpg): Creates a scatter plot showing the relationship between \'horsepower\' and \'origin\' with jitter applied to prevent overplotting. plot3 = ( so.Plot(mpg, \\"horsepower\\", \\"origin\\") .add(so.Dots(), so.Jitter(0.25)) ) plot3.show()"},{"question":"Objective The goal of this assessment is to evaluate your understanding of the use of the `unittest.mock` module, specifically focusing on advanced mocking features and techniques like chaining calls, handling side effects, and verifying interactions. Task You are tasked with testing a simplified API client for a weather service. The client needs to retrieve weather data, process it, and return a summary. The API client uses an `HTTPFetcher` class for making API requests. The weather data returned by the API is processed to compute the average temperature. Implement the mock tests to verify the behavior of the `WeatherAPIClient`. Initial Code ```python import requests class HTTPFetcher: def fetch_data(self, url): response = requests.get(url) return response.json() class WeatherAPIClient: def __init__(self, fetcher, api_key): self.fetcher = fetcher self.api_key = api_key def get_weather_data(self, city): url = f\\"http://api.weather.com/data?city={city}&apikey={self.api_key}\\" data = self.fetcher.fetch_data(url) return data def process_weather_data(self, data): temperatures = [entry[\'temperature\'] for entry in data[\'weather\']] avg_temp = sum(temperatures) / len(temperatures) return avg_temp def get_average_temperature(self, city): weather_data = self.get_weather_data(city) return self.process_weather_data(weather_data) ``` Task Description Write a test class `TestWeatherAPIClient` using the `unittest` framework and `unittest.mock` functionalities to: 1. **Mock HTTP Responses**: Use mocks to simulate the HTTP responses returned by `fetch_data` method. 2. **Side Effects**: Test the `get_weather_data` method to ensure it appropriately handles cases where the API fails by using side effects to raise exceptions. 3. **Chained Calls**: Verify the correctness of chained calls made within the `get_average_temperature` method. 4. **Assertions**: Use assertions to confirm the interactions with the `fetcher` mock. Input - The `city` string will be passed to the `get_weather_data` method and the `get_average_temperature` method. Output - Your test cases should confirm that all methods of `WeatherAPIClient` behave correctly in various scenarios. - Ensure the tests pass by using mock assertions and handling exceptions appropriately. Constraints - Use `unittest` and `unittest.mock` libraries for creating the tests. - Do not make actual HTTP requests. Example Tests ```python import unittest from unittest.mock import Mock, patch from weather_client import HTTPFetcher, WeatherAPIClient class TestWeatherAPIClient(unittest.TestCase): def setUp(self): self.mock_fetcher = Mock(spec=HTTPFetcher) self.api_client = WeatherAPIClient(self.mock_fetcher, \'test_api_key\') def test_get_weather_data_success(self): # Configure the mock to return predefined weather data sample_weather_data = { \'weather\': [ {\'temperature\': 20}, {\'temperature\': 22}, {\'temperature\': 21} ] } self.mock_fetcher.fetch_data.return_value = sample_weather_data result = self.api_client.get_weather_data(\'test_city\') self.mock_fetcher.fetch_data.assert_called_with(\'http://api.weather.com/data?city=test_city&apikey=test_api_key\') self.assertEqual(result, sample_weather_data) def test_get_weather_data_failure(self): # Configure the mock to raise an exception self.mock_fetcher.fetch_data.side_effect = Exception(\'API failure\') with self.assertRaises(Exception) as context: self.api_client.get_weather_data(\'test_city\') self.assertEqual(str(context.exception), \'API failure\') def test_get_average_temperature(self): # Setup chain of calls sample_weather_data = { \'weather\': [ {\'temperature\': 20}, {\'temperature\': 22}, {\'temperature\': 21} ] } self.mock_fetcher.fetch_data.return_value = sample_weather_data avg_temp = self.api_client.get_average_temperature(\'test_city\') self.mock_fetcher.fetch_data.assert_called_once_with(\'http://api.weather.com/data?city=test_city&apikey=test_api_key\') self.assertEqual(avg_temp, 21) if __name__ == \'__main__\': unittest.main() ``` Provide this code and ensure it covers the following cases: - A successful fetch of weather data. - Handling of exceptions thrown by the fetcher. - Correct processing of weather data to compute the average temperature.","solution":"import requests class HTTPFetcher: def fetch_data(self, url): response = requests.get(url) return response.json() class WeatherAPIClient: def __init__(self, fetcher, api_key): self.fetcher = fetcher self.api_key = api_key def get_weather_data(self, city): url = f\\"http://api.weather.com/data?city={city}&apikey={self.api_key}\\" data = self.fetcher.fetch_data(url) return data def process_weather_data(self, data): temperatures = [entry[\'temperature\'] for entry in data[\'weather\']] avg_temp = sum(temperatures) / len(temperatures) return avg_temp def get_average_temperature(self, city): weather_data = self.get_weather_data(city) return self.process_weather_data(weather_data)"},{"question":"Objective Write a Python function that uses the seaborn library to create a bar plot from a given dataset. Your function will demonstrate your understanding of different aggregation methods and additional transformations. Function Signature ```python def create_seaborn_barplot(data): pass ``` Input - `data` (pandas DataFrame): A DataFrame containing at least two columns: `category` and `value`. Output - The function should display a seaborn bar plot using the following specifications: - Aggregation by the mean value over each `category` group. - Aggregation by the median value over each `category` group. - Aggregation using a custom function that calculates the interquartile range (IQR) over each `category` group. - Include an additional mapping variable and use the `Dodge` transform with it. Constraints - Do not use any default plotting settings beyond those specified. - Ensure all plots are displayed correctly one after another. - The function should handle datasets with at least one numeric column and one categorical column. Example Usage ```python import pandas as pd # Example dataset data = pd.DataFrame({ \\"category\\": [\\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"C\\", \\"C\\"], \\"value\\": [1, 2, 3, 4, 5, 6], \\"sub_category\\": [\\"X\\", \\"Y\\", \\"X\\", \\"Y\\", \\"X\\", \\"Y\\"] }) create_seaborn_barplot(data) ``` Expected Outputs: 1. A bar plot aggregating by mean. 2. A bar plot aggregating by median. 3. A bar plot using a custom function (IQR). 4. A bar plot with an additional sub-category as a mapping variable using the `Dodge` transform. Ensure your solution makes appropriate use of seaborn\'s `Plot`, `Bar`, `Agg`, and `Dodge` capabilities.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_seaborn_barplot(data): Creates and displays bar plots of the data using different aggregation methods. Parameters: - data (pandas DataFrame): DataFrame containing \'category\', \'value\', and optional \'sub_category\' columns. # Mean aggregation sns.catplot(x=\\"category\\", y=\\"value\\", kind=\\"bar\\", data=data, ci=None, estimator=\'mean\') plt.title(\\"Mean Aggregation\\") plt.show() # Median aggregation sns.catplot(x=\\"category\\", y=\\"value\\", kind=\\"bar\\", data=data, ci=None, estimator=\'median\') plt.title(\\"Median Aggregation\\") plt.show() # Custom aggregation: Interquartile Range (IQR) def iqr(x): return x.quantile(0.75) - x.quantile(0.25) sns.catplot(x=\\"category\\", y=\\"value\\", kind=\\"bar\\", data=data, ci=None, estimator=iqr) plt.title(\\"IQR Aggregation\\") plt.show() # Aggregation with dodge using sub_category if \'sub_category\' in data.columns: sns.catplot(x=\\"category\\", y=\\"value\\", hue=\\"sub_category\\", kind=\\"bar\\", data=data, dodge=True) plt.title(\\"Dodge Bar Plot with Sub-category\\") plt.show()"},{"question":"**Background**: You are tasked with developing a simple simulation of a print server. This server handles multiple print jobs sent by multiple clients (threads). Each client thread sends a print job to the server which processes these jobs and prints them in order. Your task is to implement this using Python\'s `threading` module. The focus should be on proper thread management, synchronization to ensure that the print jobs are processed in order, and appropriate handling of shared resources. **Problem Statement**: Implement a `PrintServer` class and a `Client` class in Python using threading. The `PrintServer` class should manage the print queue and handle incoming print jobs from multiple client threads. The `Client` class should represent a client that sends a print job to the server. # Class `PrintServer` Methods: 1. **`__init__()`**: Initialize the print server with necessary attributes. 2. **`add_job(job: str)`**: Adds a print job to the queue. 3. **`process_jobs()`**: Continuously processes print jobs in the queue. This should be run in a separate thread. # Class `Client` Methods: 1. **`__init__(server: PrintServer, job: str)`**: Initializes the client with reference to the server and the print job string. 2. **`send_job()`**: Sends a print job to the server. Make use of appropriate synchronization primitives from the `threading` module to ensure: 1. `print` jobs are added/processed in a thread-safe manner. 2. The main function should initiate and start the print server processing thread and create multiple client threads with some print jobs, sending them to the print server. # Input Format: There is no direct input since classes and methods will be called in the main function script. # Output Format: Print each job from the queue in the order they are added by various client threads. # Constraints: - Ensure that all print jobs are handled without losing any job due to concurrency issues. - Use at least one synchronization primitive (`Lock`, `Semaphore`, etc.). # Example: ```python import threading class PrintServer: def __init__(self): # initialize attributes def add_job(self, job: str): # add job to the queue in a thread-safe manner def process_jobs(self): # continuously process jobs from the queue class Client: def __init__(self, server: PrintServer, job: str): # initialize with server reference and job string def send_job(self): # send job to the server if __name__ == \\"__main__\\": print_server = PrintServer() threading.Thread(target=print_server.process_jobs).start() clients = [Client(print_server, f\\"Job_{i}\\") for i in range(5)] client_threads = [] for client in clients: t = threading.Thread(target=client.send_job) t.start() client_threads.append(t) for t in client_threads: t.join() ``` This example depicts a simple simulated environment where `PrintServer` is continually running, processing jobs added by multiple client threads. Ensure synchronization such that the shared queue resource is handled safely and correctly.","solution":"import threading from queue import Queue class PrintServer: def __init__(self): self.queue = Queue() self.lock = threading.Lock() self.processing_thread = threading.Thread(target=self.process_jobs) self.processing_thread.daemon = True self.processing_thread.start() def add_job(self, job: str): with self.lock: self.queue.put(job) def process_jobs(self): while True: job = self.queue.get() if job is None: # Sentinel to stop processing break print(f\\"Processing job: {job}\\") self.queue.task_done() class Client: def __init__(self, server: PrintServer, job: str): self.server = server self.job = job def send_job(self): self.server.add_job(self.job) # Example usage in the main guard if __name__ == \\"__main__\\": print_server = PrintServer() clients = [Client(print_server, f\\"Job_{i}\\") for i in range(5)] client_threads = [] for client in clients: t = threading.Thread(target=client.send_job) t.start() client_threads.append(t) for t in client_threads: t.join() # Ensure all jobs are processed before exiting print_server.queue.join()"},{"question":"**Question: Implement and Compare Batched vs Non-Batched Matrix Multiplications in PyTorch** You are required to implement a function in PyTorch that performs batched and non-batched matrix multiplications and compares the numerical differences between the results for the first element of the batches. # Function Signature: ```python def compare_batched_non_batched(batch_size: int, dim: int) -> float: ``` # Input: - `batch_size`: An integer representing the number of matrices in each batch. - `dim`: An integer representing the dimension of the square matrices to be multiplied. # Output: - A float representing the maximum absolute difference between the result of the batched multiplication\'s first element and the non-batched multiplication of the first element for multiple random inputs. # Constraints: 1. Use random floating-point values for the elements of the matrices. 2. Use double precision (`torch.float64`) for the computations to minimize precision errors. 3. Ensure reproducibility by setting a random seed (`torch.manual_seed(0)`). 4. The function should handle various sizes for both `batch_size` and `dim`. # Steps: 1. Generate two batches of matrices `A` and `B` of shape `(batch_size, dim, dim)` with random floating-point values. 2. Perform batched matrix multiplication on `A` and `B` using `torch.bmm`. 3. Perform non-batched matrix multiplication for each pair of matrices on `A[i]` and `B[i]` and collect the results. 4. Compare the result of the first element of the batched multiplication with the non-batched multiplication of the first element. 5. Return the maximum absolute difference between these two results. # Example: ```python import torch def compare_batched_non_batched(batch_size: int, dim: int) -> float: torch.manual_seed(0) A = torch.randn(batch_size, dim, dim, dtype=torch.float64) B = torch.randn(batch_size, dim, dim, dtype=torch.float64) # Batched matrix multiplication batched_result = torch.bmm(A, B) # Non-batched matrix multiplication non_batched_results = [torch.mm(A[i], B[i]) for i in range(batch_size)] non_batched_result_first = non_batched_results[0] # Compare the first element of the batch difference = torch.abs(batched_result[0] - non_batched_result_first).max().item() return difference # Example usage print(compare_batched_non_batched(10, 3)) # Output will vary based on random seed ``` # Explanation: This function helps in understanding the differences between batched and non-batched computations, as mentioned in the documentation. It will highlight the potential numerical discrepancies that can arise even when the operations are mathematically identical.","solution":"import torch def compare_batched_non_batched(batch_size: int, dim: int) -> float: torch.manual_seed(0) # Generate random matrices with the given batch size and dimension A = torch.randn(batch_size, dim, dim, dtype=torch.float64) B = torch.randn(batch_size, dim, dim, dtype=torch.float64) # Perform batched matrix multiplication batched_result = torch.bmm(A, B) # Perform non-batched matrix multiplication for each pair of matrices non_batched_results = [torch.mm(A[i], B[i]) for i in range(batch_size)] # Get the first element result from both methods non_batched_result_first = non_batched_results[0] # Calculate the maximum absolute difference difference = torch.abs(batched_result[0] - non_batched_result_first).max().item() return difference"},{"question":"# Coding Assignment Objective: You are required to implement a simulation of an asynchronous job processing system using Python\'s `asyncio.Future` objects and associated functionalities as documented. Problem Statement: Create an asynchronous job manager that handles a series of tasks (`asyncio.Future` objects). The job manager should be able to: 1. Add new jobs. 2. Cancel jobs. 3. Determine the status of jobs. 4. Return the results of completed jobs. 5. Properly handle exceptions in jobs. Function Specifications: You will implement the following class `JobManager`: ```python import asyncio class JobManager: def __init__(self): Initialize the JobManager instance. self.jobs = [] async def add_job(self, coro): Add a new coroutine job to the manager. This coroutine should be wrapped in an asyncio.Future object and added to the jobs list. :param coro: A coroutine function to be added as a job. :return: The asyncio.Future object for the job. # Your Code Here def cancel_job(self, fut): Cancel a specific job. :param fut: The asyncio.Future object representing the job to be cancelled. :return: True if the job was successfully cancelled, False otherwise. # Your Code Here def job_status(self, fut): Get the status of a specific job. :param fut: The asyncio.Future object representing the job. :return: A string indicating the job status: \\"running\\", \\"done\\", \\"cancelled\\". # Your Code Here def job_result(self, fut): Get the result or exception of a completed job. :param fut: The asyncio.Future object representing the job. :return: The result of the job if completed successfully, or the exception if it raised one. # Your Code Here async def manage_jobs(self): Manage all jobs until completion. Ensure all jobs are processed and results are retrieved. Handle exceptions properly. # Your Code Here ``` Constraints: - You are expected to use `asyncio.Future` and relevant methods effectively. - Ensure proper exception handling and job cancellation. - You may assume that the coroutine functions passed to `add_job` do not accept any arguments and simply simulate work using `asyncio.sleep`. Example Usage: ```python import asyncio async def example_job(): await asyncio.sleep(2) return \\"Job Done\\" async def main(): manager = JobManager() # Add jobs job1 = await manager.add_job(example_job()) job2 = await manager.add_job(example_job()) # Check status print(manager.job_status(job1)) # Output: running or done # Cancel a job manager.cancel_job(job2) # Manage jobs to completion await manager.manage_jobs() # Get job results print(manager.job_result(job1)) # Output: Job Done try: print(manager.job_result(job2)) # Should raise CancelledError except asyncio.CancelledError: print(\\"Job was cancelled\\") asyncio.run(main()) ``` Good luck! Your implementation should reflect an understanding of asyncio.Future\'s methods, proper handling of results, exceptions, and cancellations.","solution":"import asyncio class JobManager: def __init__(self): Initialize the JobManager instance. self.jobs = [] async def add_job(self, coro): Add a new coroutine job to the manager. This coroutine should be wrapped in an asyncio.Future object and added to the jobs list. :param coro: A coroutine function to be added as a job. :return: The asyncio.Future object for the job. fut = asyncio.ensure_future(coro) self.jobs.append(fut) return fut def cancel_job(self, fut): Cancel a specific job. :param fut: The asyncio.Future object representing the job to be cancelled. :return: True if the job was successfully cancelled, False otherwise. if not fut.done(): fut.cancel() return True return False def job_status(self, fut): Get the status of a specific job. :param fut: The asyncio.Future object representing the job. :return: A string indicating the job status: \\"running\\", \\"done\\", \\"cancelled\\". if fut.cancelled(): return \\"cancelled\\" elif fut.done(): return \\"done\\" else: return \\"running\\" def job_result(self, fut): Get the result or exception of a completed job. :param fut: The asyncio.Future object representing the job. :return: The result of the job if completed successfully, or the exception if it raised one. if fut.cancelled(): raise asyncio.CancelledError(\\"Job was cancelled.\\") elif fut.done(): if fut.exception(): raise fut.exception() return fut.result() else: raise RuntimeError(\\"Job is not completed yet.\\") async def manage_jobs(self): Manage all jobs until completion. Ensure all jobs are processed and results are retrieved. Handle exceptions properly. await asyncio.gather(*self.jobs, return_exceptions=True)"},{"question":"**Understanding Seaborn Data Visualization** In this assessment, you will use Seaborn to visualize the \\"tips\\" dataset. You are required to demonstrate your understanding of creating scatter plots using various features of Seaborn. Your task is to create a Jupyter Notebook that fulfills the following requirements: # Dataset You will use the \\"tips\\" dataset, which is a part of Seaborn\'s sample datasets. It contains information about the total bill, tip amount, the day of the week, and other relevant information from a restaurant. # Task Requirements 1. Load the \\"tips\\" dataset using Seaborn\'s `load_dataset` function. 2. Display the first 5 rows of the dataset. 3. Create a scatter plot of `total_bill` vs `tip`. 4. Modify the scatter plot to: - Color the points by the `time` of the day (`Lunch` or `Dinner`) using the `hue` parameter. - Vary the markers by `time` of the day using the `style` parameter. - Set the size of points relative to the `size` of the dining party using the `size` parameter. 5. Create a Seaborn relational plot (`relplot`) to show `total_bill` vs `tip`, faceted by `time` with `day` as hue and `style`. 6. Add appropriate titles, axis labels, and a legend to all plots for better readability. # Additional Instructions - Make sure to import all necessary libraries (e.g., `pandas`, `seaborn`, `matplotlib.pyplot`). - Your final notebook should be well-commented, explaining each step and choice of parameters used in the plots. - Ensure that the plots are visually appealing and easy to understand. # Example Output Your visualizations should follow the structure given in the examples below: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt sns.set_theme() # Load dataset tips = sns.load_dataset(\\"tips\\") # Display the first 5 rows print(tips.head()) # Scatter plot of total_bill vs tip sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") # Modified scatter plot with additional parameters sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"time\\", size=\\"size\\", sizes=(20, 200)) # Relational plot faceted by \'time\' with \'day\' as hue and style sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\") # Display the plots plt.show() ``` Your submission should include the Jupyter Notebook file (.ipynb) with all required plots and comments.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_tips_dataset(): Loads the \'tips\' dataset from Seaborn. Returns: DataFrame: The \'tips\' dataset. return sns.load_dataset(\\"tips\\") def display_data_head(data): Displays the first 5 rows of the dataset. Args: data (DataFrame): The dataset to display. print(data.head()) def scatter_plot_total_bill_vs_tip(data): Creates a scatter plot of total_bill vs tip. Args: data (DataFrame): The dataset to visualize. sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Scatter Plot of Total Bill vs Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.show() def scatter_plot_with_modifications(data): Creates a scatter plot of total_bill vs tip with additional parameters. Args: data (DataFrame): The dataset to visualize. sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"time\\", size=\\"size\\", sizes=(20, 200)) plt.title(\\"Scatter Plot of Total Bill vs Tip (with Modifications)\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.legend(title=\\"Time\\") plt.show() def relational_plot_faceted_by_time(data): Creates a Seaborn relational plot (relplot) to show total_bill vs tip, faceted by time with day as hue and style. Args: data (DataFrame): The dataset to visualize. sns.relplot(data=data, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\") plt.show() # Load dataset tips = load_tips_dataset() # Display the first 5 rows display_data_head(tips) # Scatter plot of total_bill vs tip scatter_plot_total_bill_vs_tip(tips) # Modified scatter plot with additional parameters scatter_plot_with_modifications(tips) # Relational plot faceted by \'time\' with \'day\' as hue and style relational_plot_faceted_by_time(tips)"},{"question":"# Advanced Data Visualization with pandas Problem Statement You are given a dataset containing daily stock prices for multiple companies. Your task is to perform the following operations using pandas visualization capabilities: 1. Load the dataset into a Pandas DataFrame. 2. Perform basic data exploration to understand the dataset structure, missing values, and basic statistics. 3. Create various visualizations to gain insights into the data. # Dataset The dataset `stocks.csv` contains daily stock prices of different companies in the following format: ``` Date,Company,Open,High,Low,Close,Volume 2023-01-01,CompanyA,100,110,99,105,1000000 2023-01-01,CompanyB,200,210,195,205,1500000 ... ``` # Tasks 1. **Load the Data:** - Read the CSV file `stocks.csv` into a Pandas DataFrame. - Check for any missing values and handle them appropriately (e.g., use `fillna` or `dropna` depending on the context). 2. **Basic Exploration:** - Get the summary statistics of the `Close` prices for each company. - Plot a time series graph of the `Close` prices for each company. Ensure the x-axis is formatted correctly to display dates. 3. **Monthly Average Close Prices:** - Calculate the monthly average of the `Close` prices for each company. - Plot the monthly average close prices for each company on a single plot. Use different colors for each company and include a legend. 4. **Volume Analysis:** - Plot a bar chart of the total trading volume for each company. - Create a scatter plot with `Volume` on the x-axis and `Close` price on the y-axis for each company. 5. **Subplots:** - Create a 2x2 grid of subplots where each subplot shows the monthly average `Close` prices for a different company. 6. **Handling Missing Data:** - Simulate missing data by randomly inserting `NaN` values in the `Close` prices. - Demonstrate how to plot the data while handling the missing values appropriately (e.g., filling gaps with zeros, leaving gaps, etc.). # Guidelines * **Input:** - A CSV file named `stocks.csv`. * **Output:** - Various plots as described in the tasks. Each plot should have appropriate titles, labels, and legends where necessary. * **Constraints:** - Use pandas built-in plotting functions or methods. - Ensure plots are aesthetically pleasing and informative. * **Performance:** - Your code should handle the dataset efficiently without unnecessary computations. # Example ```python import pandas as pd import numpy as np import matplotlib.pyplot as plt # 1. Load the Data df = pd.read_csv(\'stocks.csv\') df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) # Handle missing data df.fillna(method=\'ffill\', inplace=True) # 2. Basic Exploration close_summary = df.groupby(\'Company\')[\'Close\'].describe() print(close_summary) # Plot time series graph of the Close prices for each company companies = df[\'Company\'].unique() plt.figure(figsize=(10, 6)) for company in companies: company_data = df[df[\'Company\'] == company] company_data[\'Close\'].plot(label=company) plt.title(\'Daily Close Prices\') plt.xlabel(\'Date\') plt.ylabel(\'Close Price\') plt.legend() plt.show() # 3. Monthly Average Close Prices monthly_avg = df.groupby([df.index.to_period(\'M\'), \'Company\'])[\'Close\'].mean().unstack() monthly_avg.plot(figsize=(10, 6)) plt.title(\'Monthly Average Close Prices\') plt.xlabel(\'Month\') plt.ylabel(\'Average Close Price\') plt.legend(title=\'Company\') plt.show() # 4. Volume Analysis total_volume = df.groupby(\'Company\')[\'Volume\'].sum() total_volume.plot(kind=\'bar\') plt.title(\'Total Trading Volume\') plt.xlabel(\'Company\') plt.ylabel(\'Volume\') plt.show() # Scatter plot plt.figure(figsize=(10, 6)) for company in companies: company_data = df[df[\'Company\'] == company] plt.scatter(company_data[\'Volume\'], company_data[\'Close\'], label=company) plt.title(\'Close Price vs Volume\') plt.xlabel(\'Volume\') plt.ylabel(\'Close Price\') plt.legend() plt.show() # 5. Subplots fig, axes = plt.subplots(2, 2, figsize=(15, 10)) axes = axes.flatten() for i, company in enumerate(companies): company_data = monthly_avg[company] company_data.plot(ax=axes[i], title=company) plt.suptitle(\'Monthly Average Close Prices by Company\') plt.tight_layout() plt.show() # 6. Handling Missing Data np.random.seed(0) missing = df.copy() mask = np.random.choice([True, False], size=missing[\'Close\'].shape, p=[0.1, 0.9]) missing.loc[mask, \'Close\'] = np.nan # Leave gaps in line plot missing.plot(y=\'Close\', style=\'.-\') plt.title(\'Close Prices with Missing Data (gaps shown)\') plt.xlabel(\'Date\') plt.ylabel(\'Close Price\') plt.show() # Fill gaps with zero missing.fillna(0).plot(y=\'Close\', style=\'.-\') plt.title(\'Close Prices with Missing Data (filled with zero)\') plt.xlabel(\'Date\') plt.ylabel(\'Close Price\') plt.show() ``` # Submission Submit your code as a Jupyter Notebook (`.ipynb`) or a Python script (`.py`) that can be executed to generate all the required plots.","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt def load_data(filepath): Reads the CSV file into a pandas DataFrame and handles missing values. df = pd.read_csv(filepath) df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) df.fillna(method=\'ffill\', inplace=True) return df def basic_exploration(df): Returns summary statistics for the \'Close\' prices for each company. return df.groupby(\'Company\')[\'Close\'].describe() def plot_time_series(df): Plots the time series graph of the \'Close\' prices for each company. companies = df[\'Company\'].unique() plt.figure(figsize=(10, 6)) for company in companies: company_data = df[df[\'Company\'] == company] company_data[\'Close\'].plot(label=company) plt.title(\'Daily Close Prices\') plt.xlabel(\'Date\') plt.ylabel(\'Close Price\') plt.legend() plt.show() def monthly_avg_close(df): Calculates and plots the monthly average \'Close\' prices for each company. monthly_avg = df.groupby([df.index.to_period(\'M\'), \'Company\'])[\'Close\'].mean().unstack() monthly_avg.plot(figsize=(10, 6)) plt.title(\'Monthly Average Close Prices\') plt.xlabel(\'Month\') plt.ylabel(\'Average Close Price\') plt.legend(title=\'Company\') plt.show() def volume_analysis(df): Plots the total trading volume for each company and a scatter plot of \'Volume\' vs \'Close\' price. # Bar chart of total trading volume total_volume = df.groupby(\'Company\')[\'Volume\'].sum() total_volume.plot(kind=\'bar\') plt.title(\'Total Trading Volume\') plt.xlabel(\'Company\') plt.ylabel(\'Volume\') plt.show() # Scatter plot companies = df[\'Company\'].unique() plt.figure(figsize=(10, 6)) for company in companies: company_data = df[df[\'Company\'] == company] plt.scatter(company_data[\'Volume\'], company_data[\'Close\'], label=company) plt.title(\'Close Price vs Volume\') plt.xlabel(\'Volume\') plt.ylabel(\'Close Price\') plt.legend() plt.show() def subplots_monthly_avg(df): Creates a 2x2 grid of subplots showing the monthly average \'Close\' prices for each company. monthly_avg = df.groupby([df.index.to_period(\'M\'), \'Company\'])[\'Close\'].mean().unstack() companies = monthly_avg.columns fig, axes = plt.subplots(2, 2, figsize=(15, 10)) axes = axes.flatten() for i, company in enumerate(companies): company_data = monthly_avg[company] company_data.plot(ax=axes[i], title=company) plt.suptitle(\'Monthly Average Close Prices by Company\') plt.tight_layout() plt.show() def handle_missing_data(df): Demonstrates handling missing data by plotting with gaps and filling gaps with zeros. np.random.seed(0) missing = df.copy() mask = np.random.choice([True, False], size=missing[\'Close\'].shape, p=[0.1, 0.9]) missing.loc[mask, \'Close\'] = np.nan # Leave gaps in line plot plt.figure(figsize=(10, 6)) for company in missing[\'Company\'].unique(): company_data = missing[missing[\'Company\'] == company] plt.plot(company_data.index, company_data[\'Close\'], label=company, marker=\'.\') plt.title(\'Close Prices with Missing Data (gaps shown)\') plt.xlabel(\'Date\') plt.ylabel(\'Close Price\') plt.legend() plt.show() # Fill gaps with zero missing_filled = missing.fillna(0) plt.figure(figsize=(10, 6)) for company in missing_filled[\'Company\'].unique(): company_data = missing_filled[missing_filled[\'Company\'] == company] plt.plot(company_data.index, company_data[\'Close\'], label=company, marker=\'.\') plt.title(\'Close Prices with Missing Data (filled with zero)\') plt.xlabel(\'Date\') plt.ylabel(\'Close Price\') plt.legend() plt.show()"},{"question":"You are tasked with creating a simple persistence layer for a quiz application using Python\'s `shelve` module. Implement the following functions: 1. **add_quiz(filename: str, quiz_id: str, quiz_data: dict) -> None:** - **Inputs:** - `filename` (str): The name of the shelf file. - `quiz_id` (str): A unique identifier for the quiz. - `quiz_data` (dict): A dictionary containing the details of the quiz (e.g., title, questions). - **Outputs:** - This function does not return anything. - **Description:** - This function should open the shelf with the given filename, add the quiz data to the shelf with the provided quiz_id as the key, and then close the shelf. 2. **view_quiz(filename: str, quiz_id: str) -> dict:** - **Inputs:** - `filename` (str): The name of the shelf file. - `quiz_id` (str): The unique identifier of the quiz to retrieve. - **Outputs:** - Returns a dictionary containing the details of the quiz. - **Description:** - This function should open the shelf with the given filename, retrieve the quiz data using the given quiz_id, and then close the shelf. 3. **delete_quiz(filename: str, quiz_id: str) -> None:** - **Inputs:** - `filename` (str): The name of the shelf file. - `quiz_id` (str): The unique identifier of the quiz to delete. - **Outputs:** - This function does not return anything. - **Description:** - This function should open the shelf with the given filename, delete the quiz from the shelf using the given quiz_id, if it exists, and then close the shelf. 4. **list_quizzes(filename: str) -> list:** - **Inputs:** - `filename` (str): The name of the shelf file. - **Outputs:** - Returns a list of all quiz_ids in the shelf. - **Description:** - This function should open the shelf with the given filename, retrieve a list of all quiz_ids, and then close the shelf. **Constraints:** - Use the `shelve` module to store and retrieve the quiz data. - Ensure that the shelf is properly closed after each operation. - Handle any necessary error cases, such as attempting to retrieve or delete a non-existent quiz. **Example:** ```python # Adding a quiz add_quiz(\'quizzes.db\', \'quiz1\', {\'title\': \'Math Quiz\', \'questions\': [\'Q1\', \'Q2\']}) # Viewing a quiz print(view_quiz(\'quizzes.db\', \'quiz1\')) # Output: {\'title\': \'Math Quiz\', \'questions\': [\'Q1\', \'Q2\']} # Listing all quizzes print(list_quizzes(\'quizzes.db\')) # Output: [\'quiz1\'] # Deleting a quiz delete_quiz(\'quizzes.db\', \'quiz1\') # Trying to view a deleted quiz print(view_quiz(\'quizzes.db\', \'quiz1\')) # Output: KeyError ```","solution":"import shelve def add_quiz(filename: str, quiz_id: str, quiz_data: dict) -> None: with shelve.open(filename, writeback=True) as db: db[quiz_id] = quiz_data def view_quiz(filename: str, quiz_id: str) -> dict: with shelve.open(filename) as db: if quiz_id in db: return db[quiz_id] else: raise KeyError(f\\"No quiz found with id: {quiz_id}\\") def delete_quiz(filename: str, quiz_id: str) -> None: with shelve.open(filename, writeback=True) as db: if quiz_id in db: del db[quiz_id] else: raise KeyError(f\\"No quiz found with id: {quiz_id}\\") def list_quizzes(filename: str) -> list: with shelve.open(filename) as db: return list(db.keys())"},{"question":"You are given a dataset of 2D points with their corresponding labels for a binary classification task. Your objective is to implement a nearest neighbors classifier using scikit-learn\'s `KNeighborsClassifier` and evaluate its performance by calculating the accuracy of the classifier on a given test set. # Task 1. **Read the dataset**: Load a CSV file containing training and testing data. - Training file: `train.csv` with columns `x1`, `x2`, and `label`. - Testing file: `test.csv` with columns `x1`, `x2`, and `label`. 2. **Implement K-nearest neighbors classification**: - Use `KNeighborsClassifier` from `sklearn.neighbors`. - Split the training data into a training set and a validation set (70%-30%). - Optimize the number of nearest neighbors `k` using the validation set by evaluating the accuracy for `k` values ranging from 1 to 15. 3. **Evaluate the final model**: - Train the final model on the entire training set using the optimized `k`. - Predict the labels on the test set and calculate the accuracy. # Input - `train.csv`: A CSV file with columns `x1`, `x2`, and `label` (0 or 1). - `test.csv`: A CSV file with columns `x1`, `x2`, and `label` (0 or 1). # Output - The optimal number of neighbors `k`. - The accuracy of the model on the test set. # Constraints - Ensure the implementation is efficient, particularly when evaluating different values of `k`. - Assume the dataset is reasonably sized and can fit into memory. # Example ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score # Load the data train_data = pd.read_csv(\'train.csv\') test_data = pd.read_csv(\'test.csv\') # Separate features and labels X_train = train_data[[\'x1\', \'x2\']].values y_train = train_data[\'label\'].values X_test = test_data[[\'x1\', \'x2\']].values y_test = test_data[\'label\'].values # Split the training data into training and validation sets X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42) # Optimize the number of neighbors best_k = 1 best_accuracy = 0 for k in range(1, 16): knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train_split, y_train_split) val_predictions = knn.predict(X_val) val_accuracy = accuracy_score(y_val, val_predictions) if val_accuracy > best_accuracy: best_accuracy = val_accuracy best_k = k # Train the final model with the optimal k final_knn = KNeighborsClassifier(n_neighbors=best_k) final_knn.fit(X_train, y_train) test_predictions = final_knn.predict(X_test) test_accuracy = accuracy_score(y_test, test_predictions) print(f\'Optimal number of neighbors: {best_k}\') print(f\'Accuracy on test set: {test_accuracy}\') ``` # Notes - Include appropriate error handling for file reading. - Make sure to randomize the data splitting to avoid bias.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def load_data(train_file, test_file): train_data = pd.read_csv(train_file) test_data = pd.read_csv(test_file) return train_data, test_data def optimize_k(X_train_split, y_train_split, X_val, y_val): best_k = 1 best_accuracy = 0 for k in range(1, 16): knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train_split, y_train_split) val_predictions = knn.predict(X_val) val_accuracy = accuracy_score(y_val, val_predictions) if val_accuracy > best_accuracy: best_accuracy = val_accuracy best_k = k return best_k def evaluate_model(X_train, y_train, X_test, y_test, best_k): knn = KNeighborsClassifier(n_neighbors=best_k) knn.fit(X_train, y_train) test_predictions = knn.predict(X_test) test_accuracy = accuracy_score(y_test, test_predictions) return test_accuracy def main(train_file, test_file): # Load the data train_data, test_data = load_data(train_file, test_file) # Separate features and labels X_train = train_data[[\'x1\', \'x2\']].values y_train = train_data[\'label\'].values X_test = test_data[[\'x1\', \'x2\']].values y_test = test_data[\'label\'].values # Split the training data into training and validation sets X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42) # Optimize the number of neighbors best_k = optimize_k(X_train_split, y_train_split, X_val, y_val) # Evaluate the final model test_accuracy = evaluate_model(X_train, y_train, X_test, y_test, best_k) return best_k, test_accuracy"},{"question":"# Asynchronous Task Handling in Python **Objective**: Implement an asynchronous function that reads data from a stream, processes it, and handles various exceptions defined in the asyncio library. # Problem Statement: You are tasked with creating a function named `read_and_process_stream` that performs the following steps: 1. Tries to read data from an asynchronous stream (simulated by an `async generator` provided). 2. Processes the data by converting it to uppercase. 3. Handles exceptions appropriately: - If a read operation times out, retry up to 3 times and then raise a `TimeoutError`. - If the task is cancelled, log the cancellation and raise a `CancelledError`. - If `InvalidStateError` occurs, report the error and exit. - If `SendfileNotAvailableError` occurs, return a message indicating the issue. - If `IncompleteReadError` occurs, try to use the partial data obtained if `partial` has any content, else raise the error. - If `LimitOverrunError` occurs, discard the current read and continue trying. # Input - `stream` (async generator): A simulated stream generator function that yields data or raises exceptions. # Output - A string representing the processed uppercase data if successful. - If certain exceptions cannot be handled, appropriate error message or handled scenario should be depicted. # Constraints - The function should make use of asyncio\'s exception handling. - You are allowed to use any standard library for logging errors or retries. - Assume the `stream` async generator is already implemented and takes care of yielding values or raising exceptions. # Example async generator: ```python import asyncio import random async def simulated_stream(): data = [ b\\"hello\\", b\\"world\\", asyncio.TimeoutError, b\\"this is\\", b\\"asyncio\\", asyncio.CancelledError, asyncio.InvalidStateError, asyncio.SendfileNotAvailableError, asyncio.IncompleteReadError(expected=10, partial=b\\"incomplete\\"), asyncio.LimitOverrunError(consumed=5), ] for item in data: await asyncio.sleep(1) if isinstance(item, BaseException): raise item yield item ``` # Implementation: Implement the following function: ```python import asyncio import logging async def read_and_process_stream(stream) -> str: retries = 3 while retries > 0: try: data = [] async for chunk in stream: data.append(chunk.upper()) return b\\"\\".join(data).decode(\'utf-8\') except asyncio.TimeoutError: retries -= 1 if retries == 0: raise except asyncio.CancelledError as e: logging.error(\'Task was cancelled\') raise e except asyncio.InvalidStateError as e: logging.error(\'Invalid state encountered in the task\') raise e except asyncio.SendfileNotAvailableError: return \\"Sendfile syscall not available\\" except asyncio.IncompleteReadError as e: if e.partial: return e.partial.upper().decode(\'utf-8\') else: raise e except asyncio.LimitOverrunError: logging.warning(\'Buffer size limit overrun, continuing to read data\') continue except Exception as e: logging.error(f\'Unexpected exception: {e}\') raise e ``` **Note**: Ensure to include appropriate logging for debugging and understanding the exception flow.","solution":"import asyncio import logging logging.basicConfig(level=logging.ERROR) async def read_and_process_stream(stream) -> str: retries = 3 while retries > 0: try: data = [] async for chunk in stream: data.append(chunk.upper()) return b\\"\\".join(data).decode(\'utf-8\') except asyncio.TimeoutError: retries -= 1 if retries == 0: raise except asyncio.CancelledError as e: logging.error(\'Task was cancelled\') raise e except asyncio.InvalidStateError as e: logging.error(\'Invalid state encountered in the task\') raise e except asyncio.SendfileNotAvailableError: return \\"Sendfile syscall not available\\" except asyncio.IncompleteReadError as e: if e.partial: return e.partial.upper().decode(\'utf-8\') else: raise e except asyncio.LimitOverrunError: logging.warning(\'Buffer size limit overrun, continuing to read data\') continue except Exception as e: logging.error(f\'Unexpected exception: {e}\') raise e"},{"question":"Objective You are tasked with creating a utility function that converts a given string containing HTML named character references into its corresponding Unicode string using the mappings provided by the `html.entities` module. Function Signature ```python def convert_html_entities(input_string: str) -> str: pass ``` Inputs - `input_string` (str): A string potentially containing HTML named character references such as `&gt;`, `&lt;`, `&amp;`, etc. Outputs - Returns a `str`: The string with all HTML named character references replaced by their corresponding Unicode characters. Constraints 1. Perform this conversion using the `html5` dictionary from `html.entities`. 2. The function should handle both semicolon-terminated and non-terminated named character references. 3. If an entity is not found in `html5` or is not a valid entity, the function should leave it unchanged. Example ```python input_string = \\"You &gt; me &amp; them\\" output_string = \\"You > me & them\\" input_string = \\"Fruits: &lt;Apple&gt;, &lt;Orange&gt;, &lt;Grape&gt;\\" output_string = \\"Fruits: <Apple>, <Orange>, <Grape>\\" input_string = \\"Identity &unknown;\\" output_string = \\"Identity &unknown;\\" ``` Notes - You can use the `html5` dictionary from the `html.entities` module to look up the named character references and their corresponding Unicode characters. - Consider edge cases such as mixed content and incomplete or invalid entities in the input string. Guidelines 1. Use regular expressions to find potential named character references. 2. Replace the named character references by looking them up in the `html5` dictionary. 3. Ensure your function is optimized for performance, particularly in terms of handling large strings. Implement the `convert_html_entities` function to fulfill the requirements above.","solution":"import re from html.entities import html5 def convert_html_entities(input_string: str) -> str: Converts HTML named character references to their corresponding Unicode characters. Args: input_string (str): The input string containing HTML named character references. Returns: str: A string with all HTML named character references replaced by their corresponding Unicode characters. def replacement(match): entity = match.group(1) return html5.get(entity, f\'&{entity};\') # Regex pattern to match both semicolon-terminated and non-terminated named character references. pattern = re.compile(r\'&([a-zA-Z0-9]+);\') return pattern.sub(replacement, input_string)"},{"question":"**Objective**: Implement a function that operates on nested tensors to perform a sequence of transformations and arithmetic operations. Problem Statement You are given two lists of PyTorch tensors. Each list contains tensors of different lengths but the same inner dimension. Your task is to: 1. Create nested tensors from these lists. 2. Perform an addition operation between corresponding elements of the two nested tensors. 3. Convert the resulting nested tensor into a padded dense tensor with a specified padding value. 4. Return the final padded dense tensor. Function Signature ```python import torch def process_nested_tensors(tensor_list_1, tensor_list_2, padding_value): Args: tensor_list_1 (list of torch.Tensor): A list of tensors with the same inner dimension but different first dimensions. tensor_list_2 (list of torch.Tensor): Another list of tensors with the same inner dimension but different first dimensions. padding_value (float): The value to use for padding the nested tensors before conversion. Returns: torch.Tensor: A padded dense tensor resulting from the operations performed on the nested tensors. pass ``` Example ```python import torch a = torch.tensor([[1, 2], [3, 4], [5, 6]]) b = torch.tensor([[7, 8], [9, 10]]) tensor_list_1 = [a, b] c = torch.tensor([[11, 12], [13, 14], [15, 16]]) d = torch.tensor([[17, 18], [19, 20]]) tensor_list_2 = [c, d] padding_value = 0.0 result = process_nested_tensors(tensor_list_1, tensor_list_2, padding_value) print(result) ``` Expected Output: ```python tensor([[[12., 14.], [16., 18.], [20., 22.]], [[24., 26.], [28., 30.], [ 0., 0.]]]) ``` Requirements: 1. Utilize `torch.nested.nested_tensor` function to create nested tensors. 2. Ensure the addition operation can be performed by transforming the nested tensors accordingly. 3. Convert the resulting nested tensor to a padded dense tensor with the specified padding value. **Constraints**: - The inner dimensions of all tensors across both lists must match. - Tensors within each list may have different first dimensions. - The function should handle potential errors gracefully by raising appropriate exceptions with informative error messages if the inputs are invalid. **Performance Considerations**: - The function should be optimized for performance and avoid unnecessary computations or memory allocations. Converting to contiguous nested tensors when necessary to ensure compatibility. Additional Information: Refer to the PyTorch documentation if needed: https://pytorch.org/docs/stable/nested.html","solution":"import torch def process_nested_tensors(tensor_list_1, tensor_list_2, padding_value): Args: tensor_list_1 (list of torch.Tensor): A list of tensors with the same inner dimension but different first dimensions. tensor_list_2 (list of torch.Tensor): Another list of tensors with the same inner dimension but different first dimensions. padding_value (float): The value to use for padding the nested tensors before conversion. Returns: torch.Tensor: A padded dense tensor resulting from the operations performed on the nested tensors. # Verify input lists are not empty and are of the same length if len(tensor_list_1) == 0 or len(tensor_list_2) == 0: raise ValueError(\\"Input tensor lists must not be empty.\\") if len(tensor_list_1) != len(tensor_list_2): raise ValueError(\\"Input tensor lists must have the same length.\\") # Verify all tensors within each list have the same inner dimension inner_dim = tensor_list_1[0].size(1) for tensor in tensor_list_1 + tensor_list_2: if tensor.size(1) != inner_dim: raise ValueError(\\"All tensors must have the same inner dimension.\\") # Create nested tensors nested_tensor_1 = torch.nested.nested_tensor(tensor_list_1) nested_tensor_2 = torch.nested.nested_tensor(tensor_list_2) # Perform addition operation between nested tensors result_nested_tensor = nested_tensor_1 + nested_tensor_2 # Convert result to a padded dense tensor padded_dense_tensor = result_nested_tensor.to_padded_tensor(padding_value) return padded_dense_tensor"},{"question":"Objective: Implement a set of functions that utilize the `bz2` module to compress and decompress data from files and validate data integrity. Description: You are required to implement three functions: `compress_file`, `decompress_file`, and `validate_integrity`. 1. **compress_file(input_filename: str, output_filename: str, compresslevel: int = 9) -> None**: - This function should read data from `input_filename`, compress it using the specified `compresslevel`, and write the compressed data to `output_filename`. 2. **decompress_file(input_filename: str, output_filename: str) -> None**: - This function should read compressed data from `input_filename`, decompress it, and write the decompressed data to `output_filename`. 3. **validate_integrity(original_filename: str, decompressed_filename: str) -> bool**: - This function should read the data from `original_filename` and `decompressed_filename`, and check if the contents are the same. Return `True` if the contents match, otherwise `False`. Constraints: - The input and output files are in binary mode. - The `compresslevel` should be an integer between 1 and 9. - You may assume that the input filenames are valid and the files exist. Example Usage: ```python # Compress a file compress_file(\'example.txt\', \'example.txt.bz2\', compresslevel=5) # Decompress the file decompress_file(\'example.txt.bz2\', \'example_decompressed.txt\') # Validate the integrity is_valid = validate_integrity(\'example.txt\', \'example_decompressed.txt\') print(is_valid) # Output should be True if the integrity is intact ``` Implementation: ```python import bz2 def compress_file(input_filename: str, output_filename: str, compresslevel: int = 9) -> None: with open(input_filename, \'rb\') as input_file: data = input_file.read() compressed_data = bz2.compress(data, compresslevel=compresslevel) with open(output_filename, \'wb\') as output_file: output_file.write(compressed_data) def decompress_file(input_filename: str, output_filename: str) -> None: with bz2.open(input_filename, \'rb\') as input_file: decompressed_data = input_file.read() with open(output_filename, \'wb\') as output_file: output_file.write(decompressed_data) def validate_integrity(original_filename: str, decompressed_filename: str) -> bool: with open(original_filename, \'rb\') as original_file: original_data = original_file.read() with open(decompressed_filename, \'rb\') as decompressed_file: decompressed_data = decompressed_file.read() return original_data == decompressed_data ``` Make sure to implement these functions and test them with various files and compression levels to ensure they work correctly.","solution":"import bz2 def compress_file(input_filename: str, output_filename: str, compresslevel: int = 9) -> None: Compresses the file specified by input_filename and writes it to output_filename using the given compresslevel. with open(input_filename, \'rb\') as input_file: data = input_file.read() compressed_data = bz2.compress(data, compresslevel=compresslevel) with open(output_filename, \'wb\') as output_file: output_file.write(compressed_data) def decompress_file(input_filename: str, output_filename: str) -> None: Decompresses the file specified by input_filename and writes the decompressed data to output_filename. with open(input_filename, \'rb\') as input_file: compressed_data = input_file.read() decompressed_data = bz2.decompress(compressed_data) with open(output_filename, \'wb\') as output_file: output_file.write(decompressed_data) def validate_integrity(original_filename: str, decompressed_filename: str) -> bool: Validates the integrity of the decompressed file by comparing it with the original file. Returns True if the contents are the same, otherwise False. with open(original_filename, \'rb\') as original_file: original_data = original_file.read() with open(decompressed_filename, \'rb\') as decompressed_file: decompressed_data = decompressed_file.read() return original_data == decompressed_data"},{"question":"Advanced Data Reshaping with Pandas **Objective**: Assess the ability to manipulate and reshape complex data structures using pandas. # Problem Statement: You have been given two datasets in CSV format, representing company sales across various regions and time periods. Your task is to perform multiple reshaping operations to derive insights from this data. **Dataset 1: `sales_data.csv`** ``` Region,Year,Quarter,Sales North,2020,Q1,10000 North,2020,Q2,15000 North,2020,Q3,20000 North,2020,Q4,25000 South,2020,Q1,12000 South,2020,Q2,13000 ... ``` **Dataset 2: `product_data.csv`** ``` ProductID,Category,Product Name,Quarterly Sales 101,Electronics,Smartphone,200,300,400,500 102,Electronics,Laptop,150,250,350,450 103,Furniture,Desk,100,200,300,400 ... ``` **Tasks**: 1. **Load the Datasets**: Read the datasets from CSV files into pandas DataFrames. 2. **Create Pivot Table**: From `sales_data.csv`, create a pivot table showing the total `Sales` for each `Region` per `Year`. - **Input**: DataFrame from `sales_data.csv` - **Output**: DataFrame pivoted with `Region` as index, `Year` as columns, and `Sales` values aggregated by sum. 3. **Melt Product Data**: Use `product_data.csv` to unpivot the `Quarterly Sales` columns into a long format. Assume the columns representing quarterly sales are in the format `Q1, Q2, Q3, Q4`. - **Input**: DataFrame from `product_data.csv` - **Output**: DataFrame transformed to long format with `ProductID`, `Category`, `Product Name`, `Quarter` and `Sales` columns. 4. **Explode Product Categories**: In the melted DataFrame from task 3, assume the `Category` column contains comma-separated values (e.g., \\"Electronics,Mobile\\"). Write code to split these into individual rows. - **Input**: DataFrame from task 3 - **Output**: DataFrame where the `Category` column is exploded into separate rows. 5. **Create Cross-tabulation**: Using the original `sales_data.csv`, create a crosstab to see the frequency distribution of sales across different `Regions` and `Quarters`. - **Input**: DataFrame from `sales_data.csv` - **Output**: Crosstab DataFrame with regions as rows and quarters as columns. **Constraints**: - Use the pandas library for data manipulation. - Ensure efficient code that handles large datasets. - Consider edge cases such as missing values and duplicated records. **Performance Requirements**: - The operations for reshaping should take no more than a few seconds for datasets up to 100,000 rows. **Submission Format**: - Submit a Python script or Jupyter Notebook containing the completed tasks. **Expected Input and Output Formats**: 1. **Task 2 - Pivot Table**: - **Input**: DataFrame from `sales_data.csv` - **Output**: ``` Year 2020 2021 Region North xxxxx yyyyy South xxxxx yyyyy ... ``` 2. **Task 3 - Melt Product Data**: - **Input**: DataFrame from `product_data.csv` - **Output**: ``` ProductID Category Product Name Quarter Sales 101 Electronics Smartphone Q1 200 101 Electronics Smartphone Q2 300 ... ``` 3. **Task 4 - Explode Product Categories**: - **Input**: Melted DataFrame from Task 3 - **Output**: ``` ProductID Category Product Name Quarter Sales 101 Electronics Smartphone Q1 200 101 Mobile Smartphone Q1 200 ... ``` 4. **Task 5 - Cross-tabulation**: - **Input**: DataFrame from `sales_data.csv` - **Output**: ``` Quarter Q1 Q2 Q3 Q4 Region North x y z w South a b c d ... ``` Feel free to define any necessary helper functions to accomplish the tasks.","solution":"import pandas as pd def load_datasets(sales_data_path, product_data_path): sales_data = pd.read_csv(sales_data_path) product_data = pd.read_csv(product_data_path) return sales_data, product_data def create_pivot_table(sales_data): pivot_table = sales_data.pivot_table(values=\'Sales\', index=\'Region\', columns=\'Year\', aggfunc=\'sum\') return pivot_table def melt_product_data(product_data): melted_product_data = product_data.melt(id_vars=[\'ProductID\', \'Category\', \'Product Name\'], value_vars=[\'Q1\', \'Q2\', \'Q3\', \'Q4\'], var_name=\'Quarter\', value_name=\'Sales\') return melted_product_data def explode_product_categories(melted_product_data): melted_product_data[\'Category\'] = melted_product_data[\'Category\'].str.split(\',\') exploded_product_data = melted_product_data.explode(\'Category\').reset_index(drop=True) return exploded_product_data def create_crosstab(sales_data): crosstab = pd.crosstab(index=sales_data[\'Region\'], columns=sales_data[\'Quarter\'], values=sales_data[\'Sales\'], aggfunc=\'size\', dropna=True) return crosstab"},{"question":"# Advanced Coding Assessment - Linear Models with Scikit-Learn Objective: To test your understanding and implementation skills with scikit-learn’s linear models. You are required to create, fit, evaluate, and visualize a linear regression model with Lasso regularization. Additionally, you’ll need to enhance the model by integrating polynomial features to capture non-linear relationships. Problem Statement: You are given a dataset containing two features and one target. Your task is to perform the following steps: 1. Preprocess the data to add polynomial features. 2. Split the data into training and testing sets. 3. Fit a Lasso regression model on the training set. 4. Fine-tune the model by performing cross-validation to select the best regularization parameter. 5. Evaluate the model using the testing set. 6. Visualize the fit of the model to the data. Dataset: Assume you have the following dataset: ```python import numpy as np X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 11]]) y = np.array([3, 6, 7, 11, 14, 14, 20, 23, 25, 27]) ``` Steps: 1. **Preprocess the Data**: - Use `PolynomialFeatures` from `sklearn.preprocessing` to transform `X` into polynomial features of degree 2. 2. **Split the Data**: - Use `train_test_split` from `sklearn.model_selection` to split `X` and `y` into training and testing sets, with 80% for training and 20% for testing. 3. **Fit the Lasso Model**: - Use `LassoCV` from `sklearn.linear_model` to fit the model on the training data and automatically perform cross-validation to find the optimal `alpha`. - Use the polynomial transformed features for this. 4. **Evaluate the Model**: - Calculate and print the Mean Squared Error (MSE) on the testing set using the trained Lasso model. 5. **Visualize the Results**: - Plot the original data points and the model’s predictions on the same graph for visual comparison. - Ensure the plot clearly illustrates the fit of the model to the target data. Expected Output: 1. The optimal `alpha` found via cross-validation. 2. Mean Squared Error on the testing dataset. 3. A plot showing the original data points and the predicted values. Constraints: - Use `PolynomialFeatures` of degree 2. - Use `random_state=42` for `train_test_split` to ensure reproducibility. Example Template: Below is a template to get you started. ```python import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LassoCV from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error # Given data X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 11]]) y = np.array([3, 6, 7, 11, 14, 14, 20, 23, 25, 27]) # Step 1: Preprocess the Data poly = PolynomialFeatures(degree=2) X_poly = poly.fit_transform(X) # Step 2: Split the Data X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42) # Step 3: Fit the Lasso Model lasso_model = LassoCV(cv=5, random_state=42) lasso_model.fit(X_train, y_train) # Step 4: Evaluate the Model y_pred = lasso_model.predict(X_test) mse = mean_squared_error(y_test, y_pred) print(f\'Optimal alpha: {lasso_model.alpha_}\') print(f\'Mean Squared Error: {mse}\') # Step 5: Visualize the Results y_train_pred = lasso_model.predict(X_train) x_plot = np.concatenate((X_train[:,1:3],X_test[:,1:3]), axis=0) y_plot = np.concatenate((y_train_pred,y_pred), axis=0) plt.scatter(x_plot[:,0], y_plot, color=\'blue\', label=\'Predicted\') plt.scatter(X[:, 0], y, color=\'red\', label=\'Original\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Target\') plt.legend() plt.title(\'Lasso Regression with Polynomial Features\') plt.show() ``` Make sure your code follows the given structure and outputs the required results.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LassoCV from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error def lasso_regression_with_poly_features(X, y, degree=2, test_size=0.2, random_state=42): Perform Lasso regression with polynomial features. Parameters: X (np.ndarray): Input features. y (np.ndarray): Target values. degree (int): Degree of the polynomial features. test_size (float): Proportion of the dataset to include in the test split. random_state (int): Random state for reproducibility. Returns: dict: A dictionary containing the fitted model, optimal alpha value, MSE, and visualization. # Step 1: Preprocess the Data poly = PolynomialFeatures(degree=degree) X_poly = poly.fit_transform(X) # Step 2: Split the Data X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=test_size, random_state=random_state) # Step 3: Fit the Lasso Model lasso_model = LassoCV(cv=5, random_state=random_state) lasso_model.fit(X_train, y_train) # Step 4: Evaluate the Model y_pred = lasso_model.predict(X_test) mse = mean_squared_error(y_test, y_pred) print(f\'Optimal alpha: {lasso_model.alpha_}\') print(f\'Mean Squared Error: {mse}\') # Step 5: Visualize the Results plt.scatter(X[:, 0], y, color=\'red\', label=\'Original\') y_train_pred = lasso_model.predict(X_train) x_plot = np.concatenate((X_train[:,1:3],X_test[:,1:3]), axis=0) y_plot = np.concatenate((y_train_pred,y_pred), axis=0) plt.scatter(x_plot[:,0], y_plot, color=\'blue\', label=\'Predicted\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Target\') plt.legend() plt.title(\'Lasso Regression with Polynomial Features\') plt.show() return { \\"lasso_model\\": lasso_model, \\"optimal_alpha\\": lasso_model.alpha_, \\"mse\\": mse }"},{"question":"# Asynchronous File Reader with Timeout and Exception Handling Background You are given the task of implementing a function to read data from a file asynchronously, handle various asyncio-related exceptions, and ensure a timeout for the read operation. Objective Implement a function `read_file_async_with_timeout` that reads data from a file asynchronously and handles exceptions raised by the asyncio module. Function Signature ```python import asyncio from typing import Union async def read_file_async_with_timeout(file_path: str, timeout: float) -> Union[str, bytes]: pass ``` Input - `file_path` (str): A string representing the path to the file to be read. - `timeout` (float): A float representing the maximum number of seconds the read operation is allowed to take. Output - (Union[str, bytes]): Returns the content of the file if the operation completes within the timeout. Exceptions Handling The function should specifically handle the following exceptions: - `asyncio.TimeoutError`: If the read operation exceeds the given timeout, it should raise a custom message. - `asyncio.CancelledError`: If the read operation is cancelled, it should raise the exception again after logging a custom message. - `asyncio.InvalidStateError`: If an invalid state is encountered, it should raise a custom message. - `asyncio.IncompleteReadError`: If the read operation does not complete fully, it should raise a custom message. - `asyncio.LimitOverrunError`: If buffer size limit is reached while reading, it should raise a custom message. Constraints - You may assume that the file exists and is accessible for reading. - Use `asyncio` library for asynchronous reading operations. - The function should ensure proper handling and propagation of the specified exceptions. Example ```python import asyncio from asyncio import IncompleteReadError, LimitOverrunError async def read_file_async_with_timeout(file_path: str, timeout: float) -> Union[str, bytes]: try: await asyncio.wait_for(async_file_read(file_path), timeout) except asyncio.TimeoutError: raise Exception(\\"Read operation exceeded the given timeout.\\") except asyncio.CancelledError: print(\\"Read operation was cancelled.\\") raise except asyncio.InvalidStateError: raise Exception(\\"Encountered an invalid state during read operation.\\") except IncompleteReadError as e: raise Exception(f\\"IncompleteReadError: Expected {e.expected}, got {e.partial}\\") except LimitOverrunError as e: raise Exception(f\\"LimitOverrunError: Consumed bytes {e.consumed}\\") async def async_file_read(file_path: str) -> bytes: # Function to simulate asynchronous file reading. await asyncio.sleep(1) # Simulating I/O operation. with open(file_path, \'rb\') as f: return f.read() # Example usage: # asyncio.run(read_file_async_with_timeout(\\"example.txt\\", 5.0)) ``` Notes - You need to create the `async_file_read` function to simulate the asynchronous read operation. It should use asyncio mechanisms to read the file and return its contents. - Ensure that any custom messages for exceptions are clear and informative.","solution":"import asyncio from typing import Union from asyncio import IncompleteReadError, LimitOverrunError async def async_file_read(file_path: str) -> bytes: Simulates asynchronous file read operation. await asyncio.sleep(1) # Simulate I/O operation with open(file_path, \'rb\') as f: return f.read() async def read_file_async_with_timeout(file_path: str, timeout: float) -> Union[str, bytes]: Reads data from a file asynchronously with a given timeout and handles various asyncio-related exceptions. try: content = await asyncio.wait_for(async_file_read(file_path), timeout) return content except asyncio.TimeoutError: raise Exception(\\"Read operation exceeded the given timeout.\\") except asyncio.CancelledError: print(\\"Read operation was cancelled.\\") raise except asyncio.InvalidStateError: raise Exception(\\"Encountered an invalid state during read operation.\\") except IncompleteReadError as e: raise Exception(f\\"IncompleteReadError: Expected {e.expected}, got {e.partial}\\") except LimitOverrunError as e: raise Exception(f\\"LimitOverrunError: Consumed bytes {e.consumed}\\")"},{"question":"You are given a Python module that includes boolean operations and requires you to handle boolean objects correctly, including type checking, conversion, and ensuring proper reference counting similar to the C API behaviors outlined in the documentation. # Problem Statement Implement the following functions in Python: 1. **is_py_bool(obj):** - **Input:** A single object `obj`. - **Output:** Return `True` if the object is of boolean type (`True` or `False`), otherwise return `False`. 2. **py_bool_from_value(val):** - **Input:** A single integer `val`. - **Output:** Return `True` if `val` is non-zero, otherwise return `False`. 3. **reference_counting_example(obj):** - **Input:** A single boolean object `obj`. - **Output:** Demonstrate a simple reference management by incrementing and returning the reference count of the boolean object (`True` or `False`). Use Python\'s built-in reference management to simulate this. **Note:** - Do NOT import any additional modules. - Constraints: - For `is_py_bool`, `obj` can be any Python object. - For `py_bool_from_value`, `val` can be any integer. - For `reference_counting_example`, use Python\'s built-in mechanisms to demonstrate reference counting. # Example Usage: ```python def is_py_bool(obj): # Your implementation here pass def py_bool_from_value(val): # Your implementation here pass def reference_counting_example(obj): # Your implementation here pass # Example usage: print(is_py_bool(True)) # Output: True print(is_py_bool(\\"test\\")) # Output: False print(py_bool_from_value(1)) # Output: True print(py_bool_from_value(0)) # Output: False bool_obj = True print(reference_counting_example(bool_obj)) # Output: 2 (reference count might vary depending on interpreter) ``` # Requirements: - Handle boolean comparisons and type checking using Python\'s built-in functions and types. - Simulate reference counting by demonstrating the concept, using Python\'s features like `sys.getrefcount` if necessary.","solution":"import sys def is_py_bool(obj): Check if the given object is a boolean (True or False). return isinstance(obj, bool) def py_bool_from_value(val): Convert an integer to a boolean. Returns True if the integer is non-zero, otherwise False. return bool(val) def reference_counting_example(obj): Demonstrate reference counting. Returns the reference count of the given boolean object. return sys.getrefcount(obj)"},{"question":"You are tasked with implementing a custom event scheduling system using Python\'s `sched` module. Specifically, you\'ll be creating a simple task scheduler that can: 1. Schedule tasks to run after a given delay or at a specific time. 2. Execute the scheduled tasks. 3. Manage and prioritize tasks. Implement the function `custom_task_scheduler` with the following specifications: # Function Signature ```python import sched import time def custom_task_scheduler(): # Your implementation here ``` # Requirements 1. **Initialization**: Initialize a `sched.scheduler` instance with `time.time` and `time.sleep`. 2. **Scheduling Methods**: - Implement a method `schedule_task(delay, priority, task, args=(), kwargs={})` to schedule tasks with a delay. - Implement a method `schedule_task_abs(absolute_time, priority, task, args=(), kwargs={})` to schedule tasks at an absolute time. 3. **Task Execution**: Implement a method `run_tasks()` that runs all scheduled tasks. 4. **Cancellation**: Implement a method `cancel_task(event)` to cancel a scheduled task. 5. **Check Empty Queue**: Implement a method `is_empty()` that returns `True` if the event queue is empty, otherwise `False`. # Input/Output - There will be no direct input/output to the function. You should design the class methods so that they can be called and tested independently. - The function should demonstrate usage by scheduling tasks, for example simple print statements or time logging. # Example Usage ```python def custom_task_scheduler(): scheduler = sched.scheduler(time.time, time.sleep) def schedule_task(delay, priority, task, args=(), kwargs={}): return scheduler.enter(delay, priority, task, args, kwargs) def schedule_task_abs(absolute_time, priority, task, args=(), kwargs={}): return scheduler.enterabs(absolute_time, priority, task, args, kwargs) def run_tasks(): scheduler.run() def cancel_task(event): scheduler.cancel(event) def is_empty(): return scheduler.empty() # Example scheduling def print_message(message): print(message, time.time()) # Schedule tasks event1 = schedule_task(5, 1, print_message, args=(\\"Task 1\\",)) event2 = schedule_task(10, 1, print_message, args=(\\"Task 2\\",)) event3 = schedule_task_abs(time.time() + 15, 1, print_message, args=(\\"Absolute Task 3\\",)) # Run all tasks run_tasks() # Cancel a task cancel_task(event2) # Check if queue is empty print(\\"Is queue empty?\\", is_empty()) ``` This task enables students to explore how to: - Initialize and use a scheduler. - Schedule tasks with delays or at specific times. - Cancel tasks and check the event queue. - Understand task prioritization in the scheduler. Ensure to test your implementation thoroughly to handle different scenarios such as task overlap and cancellation.","solution":"import sched import time def custom_task_scheduler(): scheduler = sched.scheduler(time.time, time.sleep) def schedule_task(delay, priority, task, args=(), kwargs={}): Schedules a task to run after a given delay. return scheduler.enter(delay, priority, task, args, kwargs) def schedule_task_abs(absolute_time, priority, task, args=(), kwargs={}): Schedules a task to run at a specific absolute time. return scheduler.enterabs(absolute_time, priority, task, args, kwargs) def run_tasks(): Executes all scheduled tasks. scheduler.run() def cancel_task(event): Cancels a scheduled task. scheduler.cancel(event) def is_empty(): Checks if the event queue is empty. return scheduler.empty() return { \'schedule_task\': schedule_task, \'schedule_task_abs\': schedule_task_abs, \'run_tasks\': run_tasks, \'cancel_task\': cancel_task, \'is_empty\': is_empty }"},{"question":"Objective: Implement and evaluate a neural network using gradient and Jacobian calculations with PyTorch\'s `torch.func` utilities. Description: You are required to implement a small feedforward neural network using `torch.nn.Module` and utilize PyTorch\'s `torch.func` utilities to: 1. Compute the gradient of the network\'s output with respect to its input. 2. Compute the Jacobian of the network\'s output with respect to its parameters. Instructions: 1. Define a neural network `MyNetwork` with: - An input layer of size 2. - One hidden layer of size 3. - An output layer of size 1. - Use ReLU activations for hidden layers. 2. Implement a function `compute_input_gradient(model, x)` that calculates the gradient of the network\'s output with respect to its input `x`. 3. Implement a function `compute_param_jacobian(model, x)` that calculates the Jacobian of the network\'s output with respect to its parameters using `torch.func.functional_call`. Requirements: 1. Use PyTorch\'s automatic differentiation functionalities (`torch.func.grad` and `torch.func.jacrev`). 2. Validate your functions with an example input tensor `x = torch.tensor([0.5, -0.5])`. Constraints: - Ensure your network and functions are compatible with PyTorch\'s `torch.func` utilities. - The functions should handle the forward and backward passes correctly. Input and Output Formats: 1. `MyNetwork` class should take no parameters. 2. `compute_input_gradient(model, x)`: - Input: `model` (instance of `MyNetwork`), `x` (2-dimensional tensor). - Output: Gradient of the model\'s output with respect to `x` (2-dimensional tensor). 3. `compute_param_jacobian(model, x)`: - Input: `model` (instance of `MyNetwork`), `x` (2-dimensional tensor). - Output: Jacobian of the model\'s output with respect to its parameters (dictionary of tensors representing the Jacobian for each parameter). # Example: ```python import torch import torch.nn as nn import torch.func as func class MyNetwork(nn.Module): def __init__(self): super(MyNetwork, self).__init__() self.fc1 = nn.Linear(2, 3) self.relu = nn.ReLU() self.fc2 = nn.Linear(3, 1) def forward(self, x): x = self.relu(self.fc1(x)) return self.fc2(x) def compute_input_gradient(model, x): # Implement function to compute gradient w.r.t input pass def compute_param_jacobian(model, x): # Implement function to compute Jacobian w.r.t parameters pass model = MyNetwork() x = torch.tensor([0.5, -0.5]) input_gradient = compute_input_gradient(model, x) param_jacobian = compute_param_jacobian(model, x) print(\\"Input Gradient:\\", input_gradient) print(\\"Parameter Jacobian:\\", param_jacobian) ``` Notes: - Ensure all gradients and Jacobians are computed using the correct automatic differentiation functions provided in `torch.func`. - Validate the shapes of your outputs to ensure correctness.","solution":"import torch import torch.nn as nn import torch.func as func class MyNetwork(nn.Module): def __init__(self): super(MyNetwork, self).__init__() self.fc1 = nn.Linear(2, 3) self.relu = nn.ReLU() self.fc2 = nn.Linear(3, 1) def forward(self, x): x = self.relu(self.fc1(x)) return self.fc2(x) def compute_input_gradient(model, x): def model_output(x): return model(x).squeeze() gradient = func.grad(model_output)(x) return gradient def compute_param_jacobian(model, x): def model_output(params): data_copy = {name: param.clone() for name, param in params.items()} return func.functional_call(model, data_copy, x).squeeze() params = dict(model.named_parameters()) jacobian = func.jacrev(model_output)(params) return jacobian # Example usage if __name__ == \\"__main__\\": model = MyNetwork() x = torch.tensor([0.5, -0.5], requires_grad=True) input_gradient = compute_input_gradient(model, x) param_jacobian = compute_param_jacobian(model, x) print(\\"Input Gradient:\\", input_gradient) print(\\"Parameter Jacobian:\\", param_jacobian)"},{"question":"Advanced I/O Operations in Python **Objective:** To assess your understanding of Python\'s `io` module by working with text and binary streams and implementing efficient buffered I/O operations. # Problem Statement: You are required to implement a function `process_data(input_data: str, encoding: str = \'utf-8\') -> bytes` that performs the following tasks: 1. **Accepts a string (`input_data`) and an encoding format (`encoding`) as input.** The default encoding should be \'utf-8\'. 2. **Writes the string to a text stream** (`io.StringIO`). 3. **Reads the text back from the stream** and encodes it to bytes using the specified encoding. 4. **Writes this byte data to a binary stream** (`io.BytesIO`). 5. **Reads the binary data back from the stream**. 6. **Returns the binary data** as bytes. # Input: - `input_data` (str): The input text data to be processed. - `encoding` (str): The encoding format to be used for encoding the text data to bytes (default is \'utf-8\'). # Output: - Returns the processed data as a `bytes` object. # Constraints: - You must use `io.StringIO` for text stream operations. - You must use `io.BytesIO` for binary stream operations. - Do not use any external libraries apart from the `io` module and built-in functionalities. # Example Usage: ```python input_data = \\"Hello, World!\\" encoding = \\"utf-16\\" result = process_data(input_data, encoding) print(result) # Output would be the encoded byte string of \\"Hello, World!\\" in utf-16 ``` # Notes: - Ensure proper handling of the streams; close them appropriately. - Handle any encoding issues that may arise. # Solution Template: ```python import io def process_data(input_data: str, encoding: str = \'utf-8\') -> bytes: # Implement your solution here pass ``` Use the provided template to implement your solution. Make sure to thoroughly test your function with different inputs and encodings to ensure correctness and robustness.","solution":"import io def process_data(input_data: str, encoding: str = \'utf-8\') -> bytes: # Create a StringIO object and write the input_data to it text_stream = io.StringIO() text_stream.write(input_data) # Move the cursor to the beginning of the stream to read from it text_stream.seek(0) read_text = text_stream.read() # Encode the read text to bytes using the specified encoding byte_data = read_text.encode(encoding) # Create a BytesIO object and write the byte data to it binary_stream = io.BytesIO() binary_stream.write(byte_data) # Move the cursor to the beginning of the stream to read from it binary_stream.seek(0) read_binary_data = binary_stream.read() # Close the streams text_stream.close() binary_stream.close() return read_binary_data"},{"question":"**Secure Token Management System** Given the necessity to secure sensitive information and manage secure communication, you are tasked to implement a token management system using Python\'s `secrets` module. Your implementation should include the following functionalities: 1. **Generate a Secure Token**: - Implement a function `generate_token(token_type: str, length: int = None) -> str:` that generates a secure token. - `token_type` can be `\'hex\'`, `\'bytes\'`, or `\'urlsafe\'`. - `length` specifies the number of bytes for the token. If it is `None`, the default length should be used. - The function should return: - a hexadecimal string if `token_type` is `\'hex\'`. - a byte string if `token_type` is `\'bytes\'`. - a URL-safe string if `token_type` is `\'urlsafe\'`. 2. **Generate Secure Password**: - Implement a function `generate_password(length: int) -> str:` that generates a secure password. - The password should have at least: - One uppercase letter - One lowercase letter - One digit - Use an appropriate set of characters such that the password is secure. 3. **Temporary Secure URL Generation**: - Implement a function `generate_secure_url(base_url: str, token_type: str = \'urlsafe\', length: int = 32) -> str:` to create a temporary secure URL. - `base_url` is the base URL to which the token is appended. - `token_type` specifies the type of token to append (defaults to `\'urlsafe\'`). - `length` specifies the length of the token in bytes (default is 32 bytes). # Function Signatures ```python def generate_token(token_type: str, length: int = None) -> str: pass def generate_password(length: int) -> str: pass def generate_secure_url(base_url: str, token_type: str = \'urlsafe\', length: int = 32) -> str: pass ``` # Constraints - Use the `secrets` module for any random generation needs. - Ensure that the generated tokens and passwords are sufficiently secure against brute-force attacks. - Valid token types for `generate_token` are `\'hex\'`, `\'bytes\'`, `\'urlsafe\'`. # Examples ```python # Example 1: print(generate_token(\'hex\', 16)) # Output: A random 32-character hexadecimal string, e.g., \'f9bf78b9a18ce6d46a0cd2b0b86df9da\'. # Example 2: print(generate_password(10)) # Output: A random 10-character password, e.g., \'aB3XYe2jFl\'. # Example 3: print(generate_secure_url(\'https://example.com/reset\', \'urlsafe\', 16)) # Output: A URL with a secure token, e.g., \'https://example.com/reset?token=Drmhze6EPcv0fN_81Bj-nA\' ``` # Notes - Ensure all random generation is done using the `secrets` module to guarantee cryptographic security. - You may assume that `base_url` will always be a valid URL without query parameters.","solution":"import secrets import string def generate_token(token_type: str, length: int = None) -> str: Generates a secure token. Args: token_type (str): The type of token (\'hex\', \'bytes\', \'urlsafe\'). length (int): The number of bytes for the token. If None, default length is used. Returns: str: The generated token. if length is None: length = 32 # default length: 32 bytes if token_type == \'hex\': return secrets.token_hex(length) elif token_type == \'bytes\': return secrets.token_bytes(length) elif token_type == \'urlsafe\': return secrets.token_urlsafe(length) else: raise ValueError(\\"Invalid token type. Use \'hex\', \'bytes\', or \'urlsafe\'.\\") def generate_password(length: int) -> str: Generates a secure password with at least one uppercase letter, one lowercase letter, and one digit. Args: length (int): The length of the password. Returns: str: The generated password. if length < 3: raise ValueError(\\"Password length must be at least 3 to include an uppercase letter, a lowercase letter, and a digit.\\") alphabet = string.ascii_letters + string.digits password = [ secrets.choice(string.ascii_uppercase), secrets.choice(string.ascii_lowercase), secrets.choice(string.digits), ] password += [secrets.choice(alphabet) for _ in range(length - 3)] secrets.SystemRandom().shuffle(password) return \'\'.join(password) def generate_secure_url(base_url: str, token_type: str = \'urlsafe\', length: int = 32) -> str: Creates a temporary secure URL with a token. Args: base_url (str): The base URL. token_type (str): The type of token to append (defaults to \'urlsafe\'). length (int): The length of the token in bytes (default is 32 bytes). Returns: str: The secure URL. token = generate_token(token_type, length) return f\\"{base_url}?token={token}\\""},{"question":"Objective: Implement a function that performs a named tensor operation and ensures correct name inference is applied according to PyTorch rules. Problem Statement: You are required to implement a function `custom_named_tensor_operation` that performs matrix multiplication on two named tensors and then adds the result to another named tensor. The function should adhere to the name inference rules provided in the documentation. Function Signature: ```python def custom_named_tensor_operation(tensor1: torch.Tensor, tensor2: torch.Tensor, tensor3: torch.Tensor) -> torch.Tensor: pass ``` Input: - `tensor1` (torch.Tensor): A named tensor of shape (A, B, ...) with proper dimension names. - `tensor2` (torch.Tensor): A named tensor of shape (B, C, ...) with proper dimension names. - `tensor3` (torch.Tensor): A named tensor of compatible shape with `torch.matmul(tensor1, tensor2)`. Output: - Returns a named tensor that is the result of performing `torch.matmul(tensor1, tensor2)` and then adding `tensor3`. The names of the output tensor should follow the name inference rules (`contracts_away_dims` for `torch.matmul` and `unifies_names_from_inputs` for addition). Constraints: - The tensors provided as input will have named dimensions. - The dimensions will be compatible for matrix multiplication and addition. - You should not change the original tensors but instead return a new tensor with the correctly inferred names. Example: ```python # Example input tensor1 = torch.randn(3, 4, names=(\'N\', \'M\')) tensor2 = torch.randn(4, 5, names=(\'M\', \'P\')) tensor3 = torch.randn(3, 5, names=(\'N\', \'P\')) # Expected Output # The output will be a tensor with shape (N, P) and names (\'N\', \'P\') output = custom_named_tensor_operation(tensor1, tensor2, tensor3) print(output.names) # Output should be (\'N\', \'P\') ``` Notes: - Make sure to handle name mismatches and incompatible dimensions appropriately by raising a RuntimeError with an appropriate message. - Use the `torch.matmul` and tensor addition operations as described. - Ensure correct name propagation as per the provided rules.","solution":"import torch def custom_named_tensor_operation(tensor1: torch.Tensor, tensor2: torch.Tensor, tensor3: torch.Tensor) -> torch.Tensor: Performs matrix multiplication on two named tensors and then adds the result to another named tensor. Args: - tensor1 (torch.Tensor): A named tensor of shape (A, B, ...) with proper dimension names. - tensor2 (torch.Tensor): A named tensor of shape (B, C, ...) with proper dimension names. - tensor3 (torch.Tensor): A named tensor of compatible shape with `torch.matmul(tensor1, tensor2)`. Returns: - A named tensor resulting from matrix multiplication and addition. # Perform matrix multiplication matmul_result = torch.matmul(tensor1, tensor2) # Check if the names of the tensors are compatible for addition if matmul_result.names != tensor3.names: raise RuntimeError(f\\"Name mismatch: matmul result names {matmul_result.names} and tensor3 names {tensor3.names} do not match.\\") # Perform the addition result = matmul_result + tensor3 return result"},{"question":"# File Path Manipulation with `os.path` Objective: Write a Python function that processes a list of file paths and normalizes them, ensuring they exist, and then returns some key information about them. Function Signature: ```python def process_file_paths(file_paths: list) -> list: Processes a list of file paths and returns a list of dictionaries with information about each valid path. Args: - file_paths (list): List of file path strings to be processed. Returns: - list: List of dictionaries. Each dictionary corresponds to a valid file path from the input list and contains the following keys: - \'absolute_path\': The normalized absolute path. - \'directory\': The directory name part of the path. - \'file_name\': The base name of the path. - \'size\': The size of the file in bytes. - \'is_directory\': Boolean, True if the path is a directory, otherwise False. If a path does not exist or is inaccessible, it should be ignored. ``` Detailed Requirements: 1. For each path in the input list: - Normalize the path to remove any redundant separators and resolve any symbolic links. - Convert the path to an absolute path. - Ensure the path exists and is accessible. - Extract the directory and the base file name from the path. - Retrieve the size of the file. - Determine if the path is a directory. 2. Collect the extracted information in a dictionary. The dictionary should have the following keys: - `\'absolute_path\'`: The normalized absolute path. - `\'directory\'`: The directory part of the path. - `\'file_name\'`: The base name of the path. - `\'size\'`: The size of the file in bytes. - `\'is_directory\'`: A boolean indicating if the path is a directory. 3. Return a list of such dictionaries, one for each valid path from the input list. If a path does not exist or is inaccessible, it should not be included in the output list. Constraints: - You may assume that the input list contains strings only. - The paths can be a mix of files and directories. - Paths might include symbolic links or other file system quirks. Example Usage: ```python paths = [ \\"/home/user/file1.txt\\", \\"/home/user/dir1\\", \\"/home/user/invalid_path\\" ] result = process_file_paths(paths) print(result) # Output can be: # [ # { # \'absolute_path\': \'/home/user/file1.txt\', # \'directory\': \'/home/user\', # \'file_name\': \'file1.txt\', # \'size\': 1024, # \'is_directory\': False # }, # { # \'absolute_path\': \'/home/user/dir1\', # \'directory\': \'/home/user\', # \'file_name\': \'dir1\', # \'size\': 4096, # \'is_directory\': True # } # ] ``` Notes: - Your solution should make use of the `os.path` functions described in the provided documentation. - Make sure to handle both POSIX and Windows paths properly if running the code on different operating systems.","solution":"import os def process_file_paths(file_paths: list) -> list: Processes a list of file paths and returns a list of dictionaries with information about each valid path. Args: - file_paths (list): List of file path strings to be processed. Returns: - list: List of dictionaries. Each dictionary corresponds to a valid file path from the input list and contains the following keys: - \'absolute_path\': The normalized absolute path. - \'directory\': The directory name part of the path. - \'file_name\': The base name of the path. - \'size\': The size of the file in bytes. - \'is_directory\': Boolean, True if the path is a directory, otherwise False. If a path does not exist or is inaccessible, it should be ignored. result = [] for path in file_paths: if os.path.exists(path): absolute_path = os.path.abspath(path) directory = os.path.dirname(absolute_path) file_name = os.path.basename(absolute_path) is_directory = os.path.isdir(absolute_path) if is_directory: size = sum(os.path.getsize(os.path.join(dirpath, f)) for dirpath, dirnames, filenames in os.walk(absolute_path) for f in filenames) else: size = os.path.getsize(absolute_path) result.append({ \'absolute_path\': absolute_path, \'directory\': directory, \'file_name\': file_name, \'size\': size, \'is_directory\': is_directory }) return result"},{"question":"**Objective:** Write a Python function that analyzes a given pickle string, optimizes it, and provides a detailed report of the pickling operations before and after optimization. **Function Signature:** ```python def analyze_and_optimize_pickle(picklestring: bytes) -> str: pass ``` # Input - `picklestring` (bytes): A byte string representing the pickled data. # Output - `str`: A formatted string report that includes: 1. A disassembly of the original pickle string, with annotations. 2. The optimized pickle string. 3. A disassembly of the optimized pickle string, with annotations. 4. The size comparison between the original and optimized pickle strings. # Constraints - The function should use the `pickletools` module for analyzing and optimizing the pickle string. - The disassemblies should be annotated descriptions of each opcode. - Proper exception handling should be included to manage any issues arising from invalid pickle strings. # Performance requirements - Efficient usage of the `pickletools` functions to minimize redundancy and optimize performance. - Handle large pickle strings effectively. # Example ```python original_pickle = b\'x80x03]qx00(Kx01Kx02Kx03e.\' optimized_pickle = analyze_and_optimize_pickle(original_pickle) print(optimized_pickle) ``` **Expected Output:** ``` Original Pickle Disassembly: 0: x80 PROTO 3 2: ] EMPTY_LIST 3: q BINPUT 0 5: ( MARK 6: K BININT1 1 8: K BININT1 2 10: K BININT1 3 12: e LIST 13: . STOP highest protocol among opcodes = 2 Optimized Pickle String: b\'x80x03]qx00(Kx01Kx02Kx03e.\' Optimized Pickle Disassembly: 0: x80 PROTO 3 2: ] EMPTY_LIST 3: q BINPUT 0 5: ( MARK 6: K BININT1 1 8: K BININT1 2 10: K BININT1 3 12: e LIST 13: . STOP highest protocol among opcodes = 2 Size Comparison: Original Size: 14 bytes Optimized Size: 14 bytes ``` # Notes - Use `pickletools.dis` for disassembling and `pickletools.optimize` for optimizing the pickle string. - Your implementation should print a similar structured report as shown in the example.","solution":"import pickletools def analyze_and_optimize_pickle(picklestring: bytes) -> str: Analyzes and optimizes a given pickle string, providing a detailed report. Args: picklestring (bytes): A byte string representing the pickled data. Returns: str: A formatted string report that includes: 1. Disassembly of the original pickle string with annotations. 2. The optimized pickle string. 3. Disassembly of the optimized pickle string with annotations. 4. Size comparison between the original and optimized pickle strings. try: # Disassemble original pickle original_disassembly = pickletools.dis(picklestring) # Optimize the pickle string optimized_pickle = pickletools.optimize(picklestring) # Disassemble optimized pickle optimized_disassembly = pickletools.dis(optimized_pickle) # Calculate sizes original_size = len(picklestring) optimized_size = len(optimized_pickle) # Generate report report = ( \\"Original Pickle Disassembly:n\\" f\\"{original_disassembly}n\\" \\"nOptimized Pickle String:n\\" f\\"{optimized_pickle}n\\" \\"nOptimized Pickle Disassembly:n\\" f\\"{optimized_disassembly}n\\" \\"nSize Comparison:n\\" f\\"Original Size: {original_size} bytesn\\" f\\"Optimized Size: {optimized_size} bytesn\\" ) return report except Exception as e: return f\\"Error analyzing or optimizing pickle: {str(e)}\\""},{"question":"**Objective:** Write a Python program using the `signal` module that handles multiple types of signals and implements a timer-based execution mechanism for a critical section of code. **Task:** 1. Create a custom signal handler that logs the signal number and the stack frame information. 2. Set up handlers for `SIGINT`, `SIGALRM`, and `SIGUSR1`. 3. Implement a function `critical_section` that performs a long-running task (you can simulate this using `time.sleep`). 4. Use the `alarm` function to set a timer that interrupts the `critical_section` function if it takes longer than 5 seconds by sending a `SIGALRM`. 5. Set a handler for `SIGUSR1` that interrupts the program and logs a specific message. 6. Make sure that the custom handlers set for `SIGINT` and `SIGALRM` do not interfere with each other. **Requirements:** - The program should log each signal reception to a file `signal_log.txt`. - If the `critical_section` exceeds 5 seconds, the program should terminate this section and log the event. - The program should include a mechanism to gracefully handle `KeyboardInterrupt` (from `SIGINT`) and clean up before exiting. - After setting the alarm, the program should wait until the `critical_section` completes or is interrupted by a signal. **Input & Output:** - **Input:** No direct input required. - **Output:** The program should output log entries to `signal_log.txt` detailing signal receptions and any other relevant events (e.g., \\"SIGALRM received, terminating critical section\\"). **Constraints:** - Assume the environment is Unix-based for signal handling compatibility. - Use built-in Python modules only. **Performance Requirements:** - The program should execute efficiently with simulated long-running tasks not exceeding 10 seconds of actual runtime. **Example Output in `signal_log.txt`:** ``` Signal received: 14, Frame: <frame at 0x7f8b1bfbbc80, file..., line 12, code critical_section> SIGALRM received, terminating critical section Signal received: 2, Frame: <frame at 0x7f8b1bfbcdc0, file..., line 18, code main> ``` **Notes:** - Make sure to properly reset the alarm timer after handling each case. - Handle exceptions gracefully and ensure all resources are cleaned up on program termination.","solution":"import signal import time import os import traceback LOG_FILE = \'signal_log.txt\' def log_signal(signum, frame): Logs the signal number and stack frame information to a file. with open(LOG_FILE, \'a\') as f: f.write(f\\"Signal received: {signum}, Frame: {repr(frame)}n\\") if signum == signal.SIGALRM: f.write(\\"SIGALRM received, terminating critical sectionn\\") if signum == signal.SIGUSR1: f.write(\\"SIGUSR1 received, user-defined signaln\\") def handler(signum, frame): log_signal(signum, frame) if signum == signal.SIGALRM: raise TimeoutError(\\"Critical section execution exceeded time limit.\\") def sigusr1_handler(signum, frame): log_signal(signum, frame) os._exit(1) # Exit the program immediately for SIGUSR1 def critical_section(): Simulates a long-running task. try: signal.alarm(5) # Set an alarm for 5 seconds time.sleep(10) # Simulate a long task except TimeoutError: print(\\"Critical section exceeded time limit.\\") finally: signal.alarm(0) # Cancel any existing alarm def main(): # Setting up signal handlers signal.signal(signal.SIGINT, handler) # Handle Ctrl+C signal.signal(signal.SIGALRM, handler) # Handle alarm signal signal.signal(signal.SIGUSR1, sigusr1_handler) # Handle user-defined signal try: critical_section() except KeyboardInterrupt: log_signal(signal.SIGINT, None) print(\\"KeyboardInterrupt received, exiting gracefully.\\") os._exit(0) # Clean up and exit if __name__ == \\"__main__\\": main()"},{"question":"**Advanced Python Coding Assessment Question**: **Objective**: Demonstrate your understanding of the tempfile module in Python by creating a utility that manages temporary files and directories efficiently. # Problem Statement You are required to implement a class `TemporaryStorageManager` which provides an interface to handle temporary files and directories using the tempfile module. This utility should support the following functionalities: 1. **Create a Temporary File**: - Method: `create_temp_file(self, prefix=None, suffix=None, dir=None) -> str` - This method should create a temporary file with the given prefix, suffix, and directory. - The method should return the file path. - The file should be deleted automatically when it is closed. 2. **Create a Named Temporary File**: - Method: `create_named_temp_file(self, prefix=None, suffix=None, dir=None, delete=True) -> str` - This method should create a named temporary file. - The method should return the file path. - The file should be deleted automatically based on the `delete` flag. 3. **Create a Spooled Temporary File**: - Method: `create_spooled_temp_file(self, max_size=0, mode=\'w+b\', prefix=None, suffix=None, dir=None) -> str` - This method should create a spooled temporary file. - Data should be kept in memory until the specified `max_size` is exceeded. - The method should return the memory or file path depending on whether the rollover has happened. 4. **Create a Temporary Directory**: - Method: `create_temp_dir(self, suffix=None, prefix=None, dir=None) -> str` - This method should create a temporary directory. - The method should return the directory path. - The directory and its contents should be cleaned up automatically when it is no longer needed. 5. **Manual Cleanup for Files/Directory**: - Method: `manual_cleanup(self, path: str)` - This method should manually delete the file or directory specified by `path`. # Constraints - Use the tempfile module\'s functionality as appropriate. - Ensure the system resources are released properly after operations. - All paths returned should be absolute paths. - Handle any potential exceptions gracefully with appropriate error messages. # Example Usage ```python manager = TemporaryStorageManager() # Create a temporary file temp_file_path = manager.create_temp_file(prefix=\\"mytemp_\\", suffix=\\".txt\\") print(temp_file_path) # Create a named temporary file named_temp_file_path = manager.create_named_temp_file(prefix=\\"named_\\", suffix=\\".log\\") print(named_temp_file_path) # Create a spooled temporary file spooled_temp_file_path = manager.create_spooled_temp_file(max_size=1024, prefix=\\"spooled_\\") print(spooled_temp_file_path) # Create a temporary directory temp_dir_path = manager.create_temp_dir(prefix=\\"mytempdir_\\") print(temp_dir_path) # Manual cleanup manager.manual_cleanup(temp_file_path) manager.manual_cleanup(temp_dir_path) ``` Implement the `TemporaryStorageManager` class to fulfil the above functionalities.","solution":"import os import tempfile import shutil class TemporaryStorageManager: def create_temp_file(self, prefix=None, suffix=None, dir=None) -> str: try: fd, path = tempfile.mkstemp(prefix=prefix, suffix=suffix, dir=dir) os.close(fd) return path except Exception as e: print(f\\"Error creating temporary file: {e}\\") return None def create_named_temp_file(self, prefix=None, suffix=None, dir=None, delete=True) -> str: try: with tempfile.NamedTemporaryFile(prefix=prefix, suffix=suffix, dir=dir, delete=delete) as tmp: return tmp.name except Exception as e: print(f\\"Error creating named temporary file: {e}\\") return None def create_spooled_temp_file(self, max_size=0, mode=\'w+b\', prefix=None, suffix=None, dir=None) -> str: try: spooled_file = tempfile.SpooledTemporaryFile(max_size=max_size, mode=mode, prefix=prefix, suffix=suffix, dir=dir) return spooled_file except Exception as e: print(f\\"Error creating spooled temporary file: {e}\\") return None def create_temp_dir(self, suffix=None, prefix=None, dir=None) -> str: try: return tempfile.mkdtemp(suffix=suffix, prefix=prefix, dir=dir) except Exception as e: print(f\\"Error creating temporary directory: {e}\\") return None def manual_cleanup(self, path: str): try: if os.path.isfile(path): os.remove(path) elif os.path.isdir(path): shutil.rmtree(path) print(f\\"Cleanup successful for: {path}\\") except Exception as e: print(f\\"Error during cleanup: {e}\\")"},{"question":"Handling NA Values and Boolean Logic in Pandas Objective In this task, you will be working with pandas DataFrames to demonstrate your understanding of nullable boolean arrays, handling NA values, and executing logical operations following Kleene Logic. Description You are given a DataFrame with a nullable boolean column involving some NA values. You need to perform the following tasks: 1. **Fill NA values in a boolean column**: Write a function `fill_na_boolean` that takes in a DataFrame and a column name, then fills the NA values in the nullable boolean column with a specified boolean value (`True` or `False`). 2. **Apply logical operations using nullable boolean logic**: Write a function `apply_logical_operations` that takes in two nullable boolean Series and applies specified logical operations (`and`, `or`, `xor`), returning the result. 3. **Subset DataFrame based on nullable boolean indexing**: Write a function `subset_dataframe` that takes in a DataFrame and a nullable boolean mask, then returns the DataFrame subset based on the mask, ensuring that NA values are treated as False. Function Signatures ```python def fill_na_boolean(df: pd.DataFrame, column: str, fill_value: bool) -> pd.DataFrame: Fills NA values in the specified column with the provided boolean value. Parameters: df (pd.DataFrame): The input DataFrame. column (str): The name of the column to fill. fill_value (bool): The boolean value to use for filling NA. Returns: pd.DataFrame: DataFrame with NA values filled in the specified column. pass def apply_logical_operations(series1: pd.Series, series2: pd.Series, operation: str) -> pd.Series: Applies the specified logical operation on two nullable boolean Series. Parameters: series1 (pd.Series): The first nullable boolean Series. series2 (pd.Series): The second nullable boolean Series. operation (str): The logical operation to apply (\'and\', \'or\', \'xor\'). Returns: pd.Series: The result of the logical operation. pass def subset_dataframe(df: pd.DataFrame, mask: pd.Series) -> pd.DataFrame: Returns a subset of the DataFrame based on the given nullable boolean mask. Parameters: df (pd.DataFrame): The input DataFrame. mask (pd.Series): The nullable boolean mask. Returns: pd.DataFrame: The subsetted DataFrame. pass ``` Example Usage ```python import pandas as pd import numpy as np # Example DataFrame data = {\'values\': [10, 20, 30, 40]} df = pd.DataFrame(data) df[\'bool_mask\'] = pd.array([True, pd.NA, False, pd.NA], dtype=\\"boolean\\") # Task 1: Filling NA values filled_df = fill_na_boolean(df, \'bool_mask\', True) # Task 2: Applying logical operations series1 = pd.Series([True, pd.NA, False], dtype=\\"boolean\\") series2 = pd.Series([pd.NA, True, False], dtype=\\"boolean\\") result_series = apply_logical_operations(series1, series2, \'or\') # Task 3: Subsetting DataFrame subset_df = subset_dataframe(df, df[\'bool_mask\']) ``` Constraints and Requirements - The DataFrame will have at most 10,000 rows. - The nullable boolean Series will have at most 10,000 elements. - Logical operations are limited to \'and\', \'or\', and \'xor\'. Your solution should be efficient and make use of pandas best practices for handling nullable boolean types and logical operations.","solution":"import pandas as pd def fill_na_boolean(df: pd.DataFrame, column: str, fill_value: bool) -> pd.DataFrame: Fills NA values in the specified column with the provided boolean value. Parameters: df (pd.DataFrame): The input DataFrame. column (str): The name of the column to fill. fill_value (bool): The boolean value to use for filling NA. Returns: pd.DataFrame: DataFrame with NA values filled in the specified column. df[column] = df[column].fillna(fill_value) return df def apply_logical_operations(series1: pd.Series, series2: pd.Series, operation: str) -> pd.Series: Applies the specified logical operation on two nullable boolean Series. Parameters: series1 (pd.Series): The first nullable boolean Series. series2 (pd.Series): The second nullable boolean Series. operation (str): The logical operation to apply (\'and\', \'or\', \'xor\'). Returns: pd.Series: The result of the logical operation. if operation == \'and\': return series1 & series2 elif operation == \'or\': return series1 | series2 elif operation == \'xor\': return series1 ^ series2 else: raise ValueError(\\"Invalid operation. Use \'and\', \'or\', or \'xor\'.\\") def subset_dataframe(df: pd.DataFrame, mask: pd.Series) -> pd.DataFrame: Returns a subset of the DataFrame based on the given nullable boolean mask. Parameters: df (pd.DataFrame): The input DataFrame. mask (pd.Series): The nullable boolean mask. Returns: pd.DataFrame: The subsetted DataFrame. return df[mask.fillna(False)]"},{"question":"You are tasked with designing a utility to manage user preferences on an iOS application using Property List (`.plist`) files. The preferences should include settings such as theme, notifications, sounds, and user-specific data. # Part 1: Writing Preferences to a Plist File Design a function `save_preferences(preferences, file_path)` that takes in a dictionary of user preferences and writes them to a specified file in binary format. Input - `preferences` (dict): A dictionary containing user preference settings. Example: ``` { \\"theme\\": \\"dark\\", \\"notifications\\": True, \\"sounds\\": False, \\"userData\\": { \\"name\\": \\"John Doe\\", \\"email\\": \\"john.doe@example.com\\" } } ``` - `file_path` (str): Path to the output binary plist file. Output - None Constraints - All dictionary keys should be strings. - Handle possible exceptions that can arise from invalid data types or failure to write to the file. # Part 2: Loading Preferences from a Plist File Design a function `load_preferences(file_path)` that reads user preferences from the specified binary plist file and returns them as a dictionary. Input - `file_path` (str): Path to the input binary plist file. Output - (dict): A dictionary containing user preferences. Constraints - Handle possible exceptions that can arise from reading the plist file or invalid file formats. # Example Usage ```python preferences = { \\"theme\\": \\"dark\\", \\"notifications\\": True, \\"sounds\\": False, \\"userData\\": { \\"name\\": \\"John Doe\\", \\"email\\": \\"john.doe@example.com\\" } } file_path = \'user_preferences.plist\' # Save the preferences to a file save_preferences(preferences, file_path) # Load the preferences from the file loaded_preferences = load_preferences(file_path) print(loaded_preferences) ``` # Performance Requirements - Your solution should handle the reading and writing of moderately large plist files (up to a few MBs) efficiently. # Implementation Notes - Use the `plistlib.dump()` and `plistlib.load()` functions for writing and reading the plist files, respectively. - Ensure that the file operations are handled gracefully with proper exception handling to avoid crashes.","solution":"import plistlib import os def save_preferences(preferences, file_path): Save user preferences to a plist file in binary format. :param preferences: A dictionary containing user preference settings. :param file_path: Path to the output binary plist file. try: with open(file_path, \'wb\') as fp: plistlib.dump(preferences, fp) except Exception as e: print(f\\"An error occurred while saving preferences: {e}\\") def load_preferences(file_path): Load user preferences from a plist file in binary format. :param file_path: Path to the input binary plist file. :return: A dictionary containing user preferences. try: with open(file_path, \'rb\') as fp: preferences = plistlib.load(fp) return preferences except Exception as e: print(f\\"An error occurred while loading preferences: {e}\\") return {}"},{"question":"**Question: CSV Data Filtering and Transformation** You are provided with a CSV file named `employees.csv` that contains employee data with the following structure: ``` employee_id,first_name,last_name,age,department,salary 1,John,Smith,30,Engineering,70000 2,Jane,Doe,25,Marketing,50000 3,Bob,Johnson,22,Human Resources,45000 4,Alice,Davis,29,Engineering,75000 ... ``` Write a Python function `filter_and_transform_csv(input_filename: str, output_filename: str, department_filter: str, min_age: int, salary_increase: float) -> None` that reads the CSV file, filters the employees based on their department and minimum age, increases their salary by a specified percentage, and writes the modified data to a new CSV file. The output CSV file should have the same structure as the input file. **Function Signature:** ```python def filter_and_transform_csv(input_filename: str, output_filename: str, department_filter: str, min_age: int, salary_increase: float) -> None: pass ``` **Input:** - `input_filename` (str): Path to the input CSV file. - `output_filename` (str): Path to the output CSV file. - `department_filter` (str): The department to filter employees by. - `min_age` (int): The minimum age of employees to include. - `salary_increase` (float): The percentage by which to increase the salaries of the filtered employees (e.g., `10.0` for a 10% increase). **Output:** - The function should not return any value. - A new CSV file should be created at `output_filename` with the filtered and transformed data. **Constraints:** - The input CSV file is well-formed. - The `min_age` is a non-negative integer. - The `salary_increase` is a non-negative float. **Example:** Given an `employees.csv` file with the following data: ``` employee_id,first_name,last_name,age,department,salary 1,John,Smith,30,Engineering,70000 2,Jane,Doe,25,Marketing,50000 3,Bob,Johnson,22,Human Resources,45000 4,Alice,Davis,29,Engineering,75000 ``` Calling the function with these inputs: ```python filter_and_transform_csv(\'employees.csv\', \'filtered_employees.csv\', \'Engineering\', 28, 10.0) ``` The `filtered_employees.csv` file should contain: ``` employee_id,first_name,last_name,age,department,salary 1,John,Smith,30,Engineering,77000 4,Alice,Davis,29,Engineering,82500 ``` Make sure to handle file operations properly, including opening/closing files and using the appropriate csv module functions. The code should be robust and handle any common exceptions that might arise during file operations.","solution":"import csv def filter_and_transform_csv(input_filename: str, output_filename: str, department_filter: str, min_age: int, salary_increase: float) -> None: with open(input_filename, mode=\'r\') as input_file: reader = csv.DictReader(input_file) fieldnames = reader.fieldnames filtered_employees = [] for row in reader: if row[\'department\'] == department_filter and int(row[\'age\']) >= min_age: increased_salary = float(row[\'salary\']) * (1 + salary_increase / 100.0) row[\'salary\'] = f\\"{increased_salary:.2f}\\" filtered_employees.append(row) with open(output_filename, mode=\'w\', newline=\'\') as output_file: writer = csv.DictWriter(output_file, fieldnames=fieldnames) writer.writeheader() writer.writerows(filtered_employees)"},{"question":"# Group Management with `grp` Module You have been provided with access to the Unix group database using the `grp` module in Python. Your task is to implement a function that can fetch group details, add a member to an existing group, and list all groups a user is a member of. The functions to implement are: 1. `get_group_details(group_name: str) -> dict`: This function takes the group name as input, retrieves the group details using `grp.getgrnam()`, and returns a dictionary with keys `group_name`, `group_id`, `password`, and `members`. 2. `add_member_to_group(group_name: str, new_member: str) -> None`: This function adds a new member to an existing group\'s member list. It should update the `gr_mem` attribute of the group named `group_name`, adding the `new_member` to the list if they are not already present. Note that actual updating of the Unix group database is outside the scope of this coding question, so you will simulate this by modifying the retrieved group object. If the group does not exist, the function should raise a `KeyError`. 3. `list_groups_of_user(user_name: str) -> list`: This function returns a list of the names of all groups that the specified user is a member of. It will utilize `grp.getgrall()` to retrieve all groups and check if the user is listed in any of them. # Function Specifications **Function 1: `get_group_details(group_name: str) -> dict`** - **Input**: A string `group_name` representing the name of the group. - **Output**: A dictionary with the following keys: - `group_name`: The name of the group. - `group_id`: The numerical group ID. - `password`: The encrypted group password. - `members`: A list of all the group\'s members. **Function 2: `add_member_to_group(group_name: str, new_member: str) -> None`** - **Input**: - `group_name`: A string representing the name of the group to which the member will be added. - `new_member`: A string representing the user name to be added to the group. - **Output**: None. The function should update the group\'s member list by adding the `new_member`. **Function 3: `list_groups_of_user(user_name: str) -> list`** - **Input**: A string `user_name` representing the user name. - **Output**: A list of strings, each representing a group name the user is a member of. # Constraints - Each group name and user name is a non-empty string. - If a group or user is not found when expected, appropriate exceptions (`KeyError`) should be raised. - Assume that all group and user names are valid and do not need further validation. # Example Usage ```python import grp # Example group entries simulation (the structure of actual grp.getgrall() return values) example_groups = [ grp.struct_group((\'admin\', \'x\', 1000, [\'alice\', \'bob\'])), grp.struct_group((\'staff\', \'x\', 1001, [\'carol\'])), grp.struct_group((\'developers\', \'x\', 1002, [\'alice\', \'dave\', \'eve\'])), ] # Simulate the grp.getgrall() function for testing def getgrall(): return example_groups def getgrgid(id): for group in example_groups: if group.gr_gid == id: return group raise KeyError(\\"Group ID not found\\") def getgrnam(name): for group in example_groups: if group.gr_name == name: return group raise KeyError(\\"Group name not found\\") # Implement your functions using these simulated grp methods # get_group_details example print(get_group_details(\\"admin\\")) # Output: {\'group_name\': \'admin\', \'group_id\': 1000, \'password\': \'x\', \'members\': [\'alice\', \'bob\']} # add_member_to_group example add_member_to_group(\\"staff\\", \\"alice\\") print(get_group_details(\\"staff\\")) # Output: {\'group_name\': \'staff\', \'group_id\': 1001, \'password\': \'x\', \'members\': [\'carol\', \'alice\']} # list_groups_of_user example print(list_groups_of_user(\\"alice\\")) # Output: [\'admin\', \'developers\', \'staff\'] ``` Implement the functions based on the specifications and ensure to handle the relevant exceptions.","solution":"import grp def get_group_details(group_name: str) -> dict: Returns the details of the specified group. group = grp.getgrnam(group_name) return { \'group_name\': group.gr_name, \'group_id\': group.gr_gid, \'password\': group.gr_passwd, \'members\': group.gr_mem } def add_member_to_group(group_name: str, new_member: str) -> None: Adds a new member to an existing group\'s member list. If the member is already present, do nothing. group = grp.getgrnam(group_name) if new_member not in group.gr_mem: grp.getgrnam(group_name).gr_mem.append(new_member) def list_groups_of_user(user_name: str) -> list: Returns a list of all the groups the specified user is a member of. groups = grp.getgrall() user_groups = [group.gr_name for group in groups if user_name in group.gr_mem] return user_groups"},{"question":"**Question: Parsing Command-Line Arguments for a Data Processing Script** You are tasked with writing a Python script that processes data from a list of input files and performs various operations (such as summing or averaging numeric values) based on command-line arguments. You need to use the `argparse` module to handle the command-line arguments. # Requirements: 1. **Input Files**: - The user should specify one or more input files containing numeric data. - Data in each file will be whitespace-separated values. 2. **Operations**: - The script should support two operations: summing and averaging the numeric values in input files. - The operation should be specified using a command-line argument (`--sum` or `--average`). 3. **File Handling**: - Additionally, the script should allow fetching additional arguments from a file specified through a specific prefix (such as `@`). - This feature can help manage long argument lists. 4. **Output**: - The result (either sum or average) should be printed to the console. # Function Signature: ```python def process_arguments(): pass if __name__ == \\"__main__\\": process_arguments() ``` # Example Usage: Summing values from multiple files ```sh python data_processor.py --sum input1.txt input2.txt ``` Averaging values, with additional arguments from a file ```sh echo \\"--average input2.txt input3.txt\\" > arguments.txt python data_processor.py @arguments.txt ``` # Constraints: - Input files will contain only numeric data. - At least one input file should be specified. - The user must specify exactly one operation (`--sum` or `--average`). # Instructions: 1. **Create an `ArgumentParser`** object and add arguments for input files and operations. 2. Ensure only one operation can be specified using mutually exclusive arguments. 3. Implement the functionality to fetch additional arguments from a file. 4. Read numeric data from specified input files, perform the specified operation, and print the result. # Example Implementation: The above code structure should guide you on how to create the function. Focus on leveraging `argparse` features effectively. **Hint:** - Remember to implement argument parsing correctly and validate input constraints. - Use `argparse.FileType` for handling input files and mutual exclusion for operations.","solution":"import argparse import sys def process_file(file): with open(file, \'r\') as f: data = f.read().split() return list(map(float, data)) def process_arguments(argv=None): parser = argparse.ArgumentParser(description=\'Process some numeric data.\') group = parser.add_mutually_exclusive_group(required=True) group.add_argument(\'--sum\', action=\'store_true\', help=\'Sum the values\') group.add_argument(\'--average\', action=\'store_true\', help=\'Average the values\') parser.add_argument(\'files\', metavar=\'FILE\', type=str, nargs=\'+\', help=\'Input files with numeric data\') if argv is None: argv = sys.argv[1:] # Fetch additional arguments from a file new_argv = [] for arg in argv: if arg.startswith(\'@\'): with open(arg[1:], \'r\') as f: new_argv.extend(f.read().split()) else: new_argv.append(arg) args = parser.parse_args(new_argv) values = [] for file in args.files: values.extend(process_file(file)) if args.sum: result = sum(values) elif args.average: result = sum(values) / len(values) if values else 0 print(result) if __name__ == \\"__main__\\": process_arguments()"},{"question":"Coding Assessment Question # Problem Statement You are tasked with implementing a multi-threaded program that processes a list of numbers to generate their cubes and sum them up. The individual cube computations should be handled by separate threads to utilize concurrent execution. # Requirements 1. **Thread Function Implementation:** - Implement a function `cube_number` that takes a list of numbers and a shared dictionary to store the results. Each thread calculates the cube of a portion of the list and stores the result in the shared dictionary with the thread name as the key. 2. **Thread Synchronization:** - Implement synchronization to ensure safe access to the shared dictionary. 3. **Main Function Execution:** - Create a main function that splits the list of numbers into chunks, creates and starts threads for each chunk, and waits for all threads to finish. - The main function should then sum up all the cubed values calculated by the threads. # Input and Output Formats - **Input:** - A list of integers `nums`. - An integer `num_threads`. - **Output:** - An integer representing the sum of cubes of all numbers in the list. # Constraints - The list of integers `nums` can have up to 10^6 elements. - The range of values in the list `nums` is between -10^3 and 10^3. - Use thread synchronization mechanisms (Lock or Condition) to ensure data consistency. # Example ```python nums = [1, 2, 3, 4, 5] num_threads = 3 # Expected Output 225 # 1^3 + 2^3 + 3^3 + 4^3 + 5^3 = 1 + 8 + 27 + 64 + 125 ``` # Solution Template Here is a starting template for your implementation: ```python from threading import Thread, Lock def cube_number(nums, result_dict, thread_name, lock): # Calculate the cube for numbers in the given list chunk # and store the result in the result_dict with thread_name as key. pass def main(nums, num_threads): # Split the nums list into num_threads chunks # Create and start threads to compute the cubes in parallel # Ensure the shared result_dict is accessed safely by threads pass # You can add your test cases to check your solution here if __name__ == \\"__main__\\": nums = [1, 2, 3, 4, 5] num_threads = 3 print(main(nums, num_threads)) # Output should be 225 ```","solution":"from threading import Thread, Lock def cube_number(nums, result_dict, thread_name, lock): cube_sum = sum(x ** 3 for x in nums) with lock: result_dict[thread_name] = cube_sum def main(nums, num_threads): if num_threads > len(nums): num_threads = len(nums) chunk_size = len(nums) // num_threads threads = [] result_dict = {} lock = Lock() for i in range(num_threads): start_index = i * chunk_size end_index = None if i == num_threads - 1 else (i + 1) * chunk_size thread_name = f\'thread_{i}\' thread = Thread(target=cube_number, args=(nums[start_index:end_index], result_dict, thread_name, lock)) threads.append(thread) thread.start() for thread in threads: thread.join() total_sum = sum(result_dict.values()) return total_sum"},{"question":"**System-Level Interaction Challenge** Python provides various utility functions to interact with the underlying system\'s internal states and capabilities considered part of stable ABI. Your task is to implement a Python program that simulates a practical scenario using these utility functions to ensure the proper handling of system-level interactions. # Task Description 1. **Interactive File Check:** Implement a Python function `is_interactive_file(filepath: str) -> bool` that uses the file handling utility function `Py_FdIsInteractive` to check whether a given file is interactive. Create the appropriate bindings to call this C function from Python. 2. **Signal Handling:** Implement a Python function `get_and_set_signal_handler(signal_num: int, new_handler) -> callable` to handle signals. This function should: - Use `PyOS_getsig` to retrieve the current signal handler for a given signal number. - Use `PyOS_setsig` to set a new signal handler and then return the old handler. 3. **Locale Conversion:** Implement two Python functions: - `decode_locale_string(byte_str: bytes) -> str` using `Py_DecodeLocale`. - `encode_locale_string(wide_str: str) -> bytes` using `Py_EncodeLocale`. # Input/Output Specifications: 1. **is_interactive_file:** - **Input:** A file path as a string (`filepath`). - **Output:** A boolean indicating whether the file is interactive. 2. **get_and_set_signal_handler:** - **Input:** A signal number (`signal_num`), a function (`new_handler`) to be used as the new signal handler. - **Output:** The old signal handler function. 3. **decode_locale_string:** - **Input:** A byte string (`byte_str`). - **Output:** A decoded string. 4. **encode_locale_string:** - **Input:** A wide character string (`wide_str`). - **Output:** An encoded byte string. # Constraints: - Ensure the provided file paths are valid and the files exist. - Implement appropriate error handling for signal numbers and invalid input types. - Ensure the locale functions handle surrogateescape error handler scenarios appropriately. # Requirements: - The solution must reflect proper usage of the respective C functions via Python bindings. - Use Python\'s `ctypes` or `cffi` to create bindings for the C functions. # Example Usage: ```python # Example for is_interactive_file assert is_interactive_file(\\"/dev/tty\\") == True assert is_interactive_file(\\"regularfile.txt\\") == False # Example for get_and_set_signal_handler def custom_handler(signum, frame): print(\\"Custom handler called\\") old_handler = get_and_set_signal_handler(signal.SIGINT, custom_handler) signal.raise_signal(signal.SIGINT) assert old_handler is not None # Example for locale functions byte_str = b\\"example\\" decoded_str = decode_locale_string(byte_str) encoded_bytes = encode_locale_string(decoded_str) assert isinstance(decoded_str, str) assert isinstance(encoded_bytes, bytes) ``` # Note: You should assume that the essential system dependencies and configurations are already set up to allow these low-level operations and bindings to execute. Your implementation should also consider thread safety and concurrency wherever applicable.","solution":"import os import signal import ctypes # Using ctypes to bind necessary C functions libc = ctypes.CDLL(None) libc.isatty.argtypes = [ctypes.c_int] libc.isatty.restype = ctypes.c_int libc.fdopen.argtypes = [ctypes.c_int, ctypes.c_char_p] libc.fdopen.restype = ctypes.c_void_p # Interactive file check function def is_interactive_file(filepath: str) -> bool: try: fd = os.open(filepath, os.O_RDONLY) try: return libc.isatty(fd) == 1 finally: os.close(fd) except OSError: return False # Signal handling functions def PyOS_getsig(n): return signal.getsignal(n) def PyOS_setsig(n, handler): return signal.signal(n, handler) def get_and_set_signal_handler(signal_num: int, new_handler) -> callable: old_handler = PyOS_getsig(signal_num) PyOS_setsig(signal_num, new_handler) return old_handler # Locale conversion functions import locale def decode_locale_string(byte_str: bytes) -> str: return byte_str.decode(locale.getpreferredencoding(False), \\"surrogateescape\\") def encode_locale_string(wide_str: str) -> bytes: return wide_str.encode(locale.getpreferredencoding(False), \\"surrogateescape\\")"},{"question":"Objective: Assess the student\'s understanding of rolling, expanding, and exponentially-weighted window functions in pandas. Question: You are given a dataset representing daily temperatures (in degrees Celsius) recorded over a year. Using pandas, write a function that calculates and returns a DataFrame with multiple statistics for various window operations. Function Definition: ```python def calculate_window_statistics(df: pd.DataFrame) -> pd.DataFrame: Given a DataFrame with daily temperatures, calculate and return a DataFrame with multiple window operations. Parameters: df (pd.DataFrame): A DataFrame with a DateTime index and single column \'temperature\' representing daily temperatures. Returns: pd.DataFrame: A DataFrame including the following columns: - \'rolling_mean_7\': 7-day rolling mean of the temperature. - \'expanding_mean\': Expanding mean of the temperature. - \'ewm_mean_0.5\': Exponentially-weighted mean with a span of 2. - \'rolling_std_30\': 30-day rolling standard deviation of the temperature. - \'expanding_max\': The expanding maximum temperature. - \'ewm_std_0.2\': Exponentially-weighted standard deviation with a span of 2. pass ``` Input: - `df`: A pandas DataFrame with a DateTime index and a column named \'temperature\'. Output: - A pandas DataFrame including columns specified in the function definition. Constraints: 1. Ensure input DataFrame has no missing data. 2. Use a window size that makes sense for sufficiently large data (at least 365 rows). Example: ```python import pandas as pd import numpy as np # Example data creation dates = pd.date_range(start=\'1/1/2020\', periods=365, freq=\'D\') temperature = np.random.normal(loc=15, scale=10, size=(365,)) df = pd.DataFrame(data={\'temperature\': temperature}, index=dates) # Function call result_df = calculate_window_statistics(df) print(result_df.head()) ``` Explanation: Given the DataFrame `df`, your function should compute the following: - The 7-day rolling mean. - The expanding mean. - The exponentially-weighted mean with a span of 2. - The 30-day rolling standard deviation. - The expanding maximum value. - The exponentially-weighted standard deviation with a span of 2. Make sure to understand how to use `.rolling()`, `.expanding()`, and `.ewm()` for such operations.","solution":"import pandas as pd def calculate_window_statistics(df: pd.DataFrame) -> pd.DataFrame: Given a DataFrame with daily temperatures, calculate and return a DataFrame with multiple window operations: - 7-day rolling mean of the temperature. - Expanding mean of the temperature. - Exponentially-weighted mean with a span of 2. - 30-day rolling standard deviation of the temperature. - Expanding maximum temperature. - Exponentially-weighted standard deviation with a span of 2. Parameters: df (pd.DataFrame): A DataFrame with a DateTime index and single column \'temperature\' representing daily temperatures. Returns: pd.DataFrame: A DataFrame including the specified columns. result = pd.DataFrame(index=df.index) result[\'rolling_mean_7\'] = df[\'temperature\'].rolling(window=7).mean() result[\'expanding_mean\'] = df[\'temperature\'].expanding().mean() result[\'ewm_mean_0.5\'] = df[\'temperature\'].ewm(span=2).mean() result[\'rolling_std_30\'] = df[\'temperature\'].rolling(window=30).std() result[\'expanding_max\'] = df[\'temperature\'].expanding().max() result[\'ewm_std_0.2\'] = df[\'temperature\'].ewm(span=2).std() return result"},{"question":"Instructions **Context:** You are working on a Python script that needs to interact with the system\'s environment variables. Specifically, you need to: 1. Retrieve the current user\'s home directory. 2. Set a new environment variable. 3. Launch a subprocess that makes use of this new environment variable. **Task:** 1. Write a function `get_home_directory()` that returns the home directory of the current user. 2. Write a function `set_environment_variable(key: str, value: str)` that sets a new environment variable. Ensure this update persists for subprocesses. 3. Write a function `launch_subprocess(env_var_key: str)` that launches a subprocess that prints the value of the environment variable specified by `env_var_key`. **Specifications:** - The functions should be implemented in Python. - You should use the \\"os\\" module, not the \\"posix\\" module directly. - The subprocess should print a message \\"Environment variable {env_var_key} has value: {env_var_value}\\". - Ensure the subprocess correctly inherits the updated environment. **Function Signatures:** ```python import os import subprocess def get_home_directory() -> str: pass def set_environment_variable(key: str, value: str) -> None: pass def launch_subprocess(env_var_key: str) -> None: pass ``` **Example Usage:** ```python # Function to get home directory home_dir = get_home_directory() print(home_dir) # Function to set a new environment variable set_environment_variable(\'MY_VAR\', \'test_value\') # Function to launch a subprocess that prints the environment variable\'s value launch_subprocess(\'MY_VAR\') ``` Expected Output: ``` /home/yourusername # This will vary depending on the user\'s OS and configuration. Environment variable MY_VAR has value: test_value ``` **Constraints:** - Assume the functions are run in sequence in a single script. - Ensure compatibility with both Unix and Windows systems.","solution":"import os import subprocess def get_home_directory() -> str: Returns the home directory of the current user. return os.path.expanduser(\\"~\\") def set_environment_variable(key: str, value: str) -> None: Sets a new environment variable that persists for subprocesses. os.environ[key] = value def launch_subprocess(env_var_key: str) -> None: Launches a subprocess that prints the value of the specified environment variable. value = os.getenv(env_var_key) subprocess.run([\'echo\', f\'Environment variable {env_var_key} has value: {value}\'])"},{"question":"Objective: Assess the understanding of the `dataclasses` module and its advanced functionalities. Question: You are required to manage an inventory system for a store. Implement the following classes using the `dataclasses` module: 1. **Product**: - Represents a generic product in the store. - Fields: - `product_id` (int) - `name` (str) - `price` (float) - Constraints: - `product_id` should be unique for each product. - `price` must be a non-negative value. 2. **InventoryItem**: - Represents an item in the inventory, inheriting from `Product`. - Fields: - `quantity` (int) - default value should be 0. - `supplier` (str, optional) - can be set to `None` if not provided. - Methods: - `total_value(self) -> float`: Calculates and returns the total value of the items in stock (`price * quantity`). 3. **Inventory**: - Manages a collection of `InventoryItem` instances. - Fields: - `items` (List[InventoryItem]) - use a default factory to initialize as an empty list. - Methods: - `add_item(self, item: InventoryItem)`: Adds an `InventoryItem` to the inventory, ensuring the `product_id` is unique. - `remove_item(self, product_id: int)`: Removes an `InventoryItem` from the inventory based on the product id. - `inventory_value(self) -> float`: Returns the total value of the entire inventory. - `as_dict(self) -> dict`: Returns a dictionary representation of the inventory using `dataclasses.asdict()`. - `as_tuple(self) -> tuple`: Returns a tuple representation of the inventory using `dataclasses.astuple()`. 4. **ImmutableProduct**: - Represents an immutable product. - Fields: - `product_id` (int) - `name` (str) - `price` (float) Detailed Requirements: 1. **Product Class**: - Implement with `@dataclass` decorator. - Ensure the `__repr__` method reflects the field values. 2. **InventoryItem Class**: - Implement with `@dataclass` and inherit from `Product`. - Use `field()` to set default values where applicable. 3. **Inventory Class**: - Implement with `@dataclass`. - Ensure methods handle the requirements efficiently. 4. **ImmutableProduct Class**: - Implement with `@dataclass` and set `frozen=True` to make the class immutable. Example Usage: ```python # Example usage # Create products product1 = Product(1, \\"Laptop\\", 999.99) product2 = Product(2, \\"Phone\\", 499.99) # Create inventory items item1 = InventoryItem(1, \\"Laptop\\", 999.99, 10) item2 = InventoryItem(2, \\"Phone\\", 499.99, 5, \\"Supplier A\\") # Create inventory and add items inventory = Inventory() inventory.add_item(item1) inventory.add_item(item2) print(inventory.inventory_value()) # Should print the total value of the inventory print(inventory.as_dict()) # Should print the dict representation of the inventory print(inventory.as_tuple()) # Should print the tuple representation of the inventory # Test ImmutableProduct immutable_product = ImmutableProduct(3, \\"Tablet\\", 299.99) # The following line should raise a dataclasses.FrozenInstanceError # immutable_product.price = 249.99 ``` Constraints: - Ensure proper validations where necessary. - Focus on efficient implementations for the methods. - Use appropriate typing annotations. Submission: Submit your implementation of the classes in a single Python file.","solution":"from dataclasses import dataclass, field, asdict, astuple from typing import List, Optional @dataclass class Product: product_id: int name: str price: float def __post_init__(self): if self.price < 0: raise ValueError(\\"Price must be a non-negative value.\\") # Assuming product_id uniqueness is managed externally from this class @dataclass class InventoryItem(Product): quantity: int = 0 supplier: Optional[str] = None def total_value(self) -> float: return self.price * self.quantity @dataclass class Inventory: items: List[InventoryItem] = field(default_factory=list) def add_item(self, item: InventoryItem): if any(existing_item.product_id == item.product_id for existing_item in self.items): raise ValueError(f\\"Product ID {item.product_id} already exists in the inventory.\\") self.items.append(item) def remove_item(self, product_id: int): self.items = [item for item in self.items if item.product_id != product_id] def inventory_value(self) -> float: return sum(item.total_value() for item in self.items) def as_dict(self) -> dict: return asdict(self) def as_tuple(self) -> tuple: return astuple(self) @dataclass(frozen=True) class ImmutableProduct: product_id: int name: str price: float"},{"question":"Advanced Argument Parsing and Automatic Signature Generation in Python **Objective:** You are tasked with creating a Python utility that mimics some of the features of CPython\'s Argument Clinic for Python functions. This utility will help define functions with specific argument types, default values, and automatic validation, and will generate a clear function signature. **Requirements:** 1. Create a Python decorator `@argument_clinic` that can: - Specify argument types and their default values. - Validate the types of provided arguments. - Automatically generate the function signature. 2. The decorator should be able to handle: - Positional-only arguments. - Keyword-only arguments. - Default values for both positional and keyword arguments. - Type validation for each argument. 3. The decorator must automatically provide a detailed function signature, reflecting the specified arguments, their types, and default values. **Input Format:** - The decorator should be applied to functions, with a metadata dictionary that specifies arguments. - Metadata keys and their expected format: ```python { \'args\': [(\'name\', type, default_value, is_positional_only)], \'kwargs\': [(\'name\', type, default_value)] } ``` **Output Format:** - The function signature string generated by the decorator. - If an argument of incorrect type is passed, raise a `TypeError` with a message indicating the expected type. **Example:** ```python @argument_clinic({ \'args\': [(\'x\', int, 0, True), (\'y\', int, 0, True)], \'kwargs\': [(\'operation\', str, \'add\')] }) def calculate(x, y, *, operation): if operation == \'add\': return x + y elif operation == \'subtract\': return x - y else: raise ValueError(\'Invalid operation\') # The function decorator should ensure: # - Positional-only args `x` and `y` are of type int, with default values 0. # - Keyword-only arg `operation` is of type str, with default value \'add\'. # - The function should raise TypeError if the incorrect type is passed. # - The function signature should be \'calculate(x: int=0, y: int=0, *, operation: str=\'add\')\' ``` 4. Write appropriate test cases to verify that: - Type validation is enforced. - Default values are applied correctly. - Different function signatures are generated appropriately based on provided metadata. **Constraints:** - Focus on standard Python types for validation (int, float, str, bool). - Ensure robust error handling and clear error messages. **Performance Requirements:** - Your solution should optimize for readability and maintainability. - Efficient type checking and signature generation are expected for standard function argument sizes (up to 10 arguments). Implementation Notes: - Use Python\'s `functools.wraps` to preserve original function metadata. - Utilize Python\'s `inspect` module to assist in signature generation. Prepare a complete and cleanly structured implementation for review.","solution":"import functools def argument_clinic(metadata): def decorator(func): # Generate the signature args_signature = \', \'.join( f\\"{name}: {arg_type.__name__}={repr(default_value)}\\" if not is_positional_only else f\\"{name}: {arg_type.__name__}={repr(default_value)}\\" for name, arg_type, default_value, is_positional_only in metadata.get(\'args\', []) ) kwargs_signature = \', \'.join( f\\"{name}: {arg_type.__name__}={repr(default_value)}\\" for name, arg_type, default_value in metadata.get(\'kwargs\', []) ) full_signature = \', \'.join(filter(None, [args_signature, \'*\' if kwargs_signature else \'\', kwargs_signature])) # Create the new function with type validation @functools.wraps(func) def wrapper(*args, **kwargs): # Type validation for positional arguments for i, (name, arg_type, default_value, _) in enumerate(metadata.get(\'args\', [])): if i < len(args): if not isinstance(args[i], arg_type): raise TypeError(f\\"Argument \'{name}\' must be of type {arg_type.__name__}\\") else: kwargs.setdefault(name, default_value) # Type validation for keyword arguments for name, arg_type, default_value in metadata.get(\'kwargs\', []): if name in kwargs and not isinstance(kwargs[name], arg_type): raise TypeError(f\\"Argument \'{name}\' must be of type {arg_type.__name__}\\") kwargs.setdefault(name, default_value) return func(*args, **kwargs) # Attach the generated signature to the function wrapper.__signature__ = full_signature return wrapper return decorator # Example usage @argument_clinic({ \'args\': [(\'x\', int, 0, True), (\'y\', int, 0, True)], \'kwargs\': [(\'operation\', str, \'add\')] }) def calculate(x, y, *, operation): if operation == \'add\': return x + y elif operation == \'subtract\': return x - y else: raise ValueError(\'Invalid operation\')"},{"question":"# Custom BufferedProtocol Implementation for File Transfer Objective: You need to implement a custom `BufferedProtocol` for transferring files over a TCP connection using asyncio. Your protocol should be able to handle file uploads and downloads efficiently. Requirements: 1. **BufferedFileProtocol**: - Implement a class `BufferedFileProtocol` that inherits from `asyncio.BufferedProtocol`. - The protocol should manage the connection lifecycle, handle buffering for incoming file data, and send file data efficiently. 2. **Server Implementation**: - Use the `BufferedFileProtocol` to create a server that can upload and download files. The server should listen on a specified port and handle multiple connections concurrently. - The server should accept commands from the client, specifically: - `UPLOAD <filename>`: The client sends a file to the server. - `DOWNLOAD <filename>`: The client requests a file from the server. 3. **Client Implementation**: - Implement a client using asyncio that can send the above commands to the server. The client should be able to upload and download files to/from the server. 4. **Concurrency**: - Ensure that the server can handle multiple clients concurrently, each potentially uploading or downloading different files simultaneously. Input and Output: - **Input**: - From the client: `UPLOAD <filename>` command followed by the binary data of the file. - From the client: `DOWNLOAD <filename>` command requesting the binary data of the file. - **Output**: - To the client: Acknowledgement of the command or an error message. - To the client: The requested file data for download. Constraints: - Only one connection per client. - The file data should be handled in chunks to optimize memory usage. - Ensure proper error handling, e.g., file not found, permission issues. Example: ```python # BufferedFileProtocol.py import asyncio import os class BufferedFileProtocol(asyncio.BufferedProtocol): def __init__(self): self.buffer = bytearray() self.transport = None def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(\'Connection from {}\'.format(peername)) def get_buffer(self, sizehint): if sizehint < 0: sizehint = 4096 # Default buffer size return self.buffer def buffer_updated(self, nbytes): command = self.buffer[:nbytes].decode() if command.startswith(\\"UPLOAD \\"): filename = command.split(\\" \\", 1)[1] self.handle_upload(filename, nbytes) elif command.startswith(\\"DOWNLOAD \\"): filename = command.split(\\" \\", 1)[1] self.handle_download(filename) self.buffer = bytearray() def handle_upload(self, filename, nbytes): try: with open(filename, \'wb\') as f: f.write(self.buffer[nbytes:]) self.transport.write(b\\"Upload successful\\") except Exception as e: self.transport.write(f\\"Upload failed: {str(e)}\\".encode()) def handle_download(self, filename): try: with open(filename, \'rb\') as f: self.transport.write(f.read()) except Exception as e: self.transport.write(f\\"Download failed: {str(e)}\\".encode()) def eof_received(self): pass def connection_lost(self, exc): print(\'The client closed the connection\') self.transport.close() async def start_server(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: BufferedFileProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() asyncio.run(start_server()) ``` ```python # Client.py import asyncio async def upload_file(filename): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) writer.write(f\\"UPLOAD {filename}\\".encode()) with open(filename, \'rb\') as f: writer.write(f.read()) await writer.drain() writer.close() await writer.wait_closed() async def download_file(filename): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) writer.write(f\\"DOWNLOAD {filename}\\".encode()) data = await reader.read() with open(filename, \'wb\') as f: f.write(data) writer.close() await writer.wait_closed() async def main(): await upload_file(\'example.txt\') await download_file(\'example.txt\') asyncio.run(main()) ``` Implement the above classes and functions to complete the task. Ensure thorough testing under different scenarios, such as multiple clients uploading and downloading files concurrently.","solution":"import asyncio import os class BufferedFileProtocol(asyncio.BufferedProtocol): def __init__(self): self.buffer = bytearray() self.transport = None self.upload_file = None self.upload_bytes_received = 0 def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\'Connection from {peername}\') def get_buffer(self, sizehint): if sizehint < 0: sizehint = 4096 # Default buffer size self.buffer = bytearray(sizehint) return self.buffer def buffer_updated(self, nbytes): data = self.buffer[:nbytes] command = data.decode().strip().split(\' \', 1) if command[0] == \'UPLOAD\': self.upload_bytes_received = 0 self.upload_file = open(command[1], \'wb\') self.transport.write(b\'ACK Upload\') elif command[0] == \'DOWNLOAD\': filename = command[1] try: with open(filename, \'rb\') as f: self.transport.write(f.read()) except Exception as e: self.transport.write(f\\"ERROR: {e}\\".encode()) else: if self.upload_file: self.upload_file.write(data) self.upload_bytes_received += len(data) if len(data) < len(self.buffer): self.upload_file.close() self.transport.write(b\'Upload Complete\') self.upload_file = None def eof_received(self): if self.upload_file: self.upload_file.close() def connection_lost(self, exc): print(\'The client closed the connection\') if self.upload_file: self.upload_file.close() self.transport.close() async def start_server(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: BufferedFileProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(start_server())"},{"question":"**Problem Statement:** You are tasked with creating a secure login interface for a console-based Python application. The interface should prompt users for their login name and password securely, without echoing the password on the screen. You must validate the login credentials against a predefined set of user credentials. # Function Signature ```python def secure_login(user_credentials: dict, prompt_user: str = \'Username: \', prompt_pass: str = \'Password: \') -> bool: pass ``` # Input - `user_credentials`: A dictionary where keys are usernames and values are passwords. - Example: `{\'admin\': \'admin123\', \'user1\': \'password1\'}` - `prompt_user` (optional): The prompt message for entering the username. Defaults to `\'Username: \'`. - `prompt_pass` (optional): The prompt message for entering the password. Defaults to `\'Password: \'`. # Output - Returns `True` if the entered username and password match any entry in `user_credentials`. - Returns `False` otherwise. # Constraints - Your solution must use the `getpass` module for entering the password securely. - The function should handle incorrect login details by returning `False` without revealing any information. - The function should handle missing username or password gracefully by returning `False`. # Example ```python def test_secure_login(): user_credentials = { \'admin\': \'admin123\', \'user1\': \'password1\' } # The actual input would be entered by the user in the console # Here we assume the user enters \'admin\' for username and \'admin123\' for password assert secure_login(user_credentials) == True # Assume the user enters \'admin\' for username and \'wrongpassword\' for password assert secure_login(user_credentials) == False # Assume the user enters \'unknown\' for username and \'password\' for password assert secure_login(user_credentials) == False test_secure_login() ``` In the example, the function should prompt the user for their username and password securely. Based on the predefined `user_credentials`, it should return `True` for valid credentials and `False` for invalid ones. # Notes - Make sure to thoroughly test your function with different user inputs. - Remember to handle the potential exceptions that might be raised by `getpass.getuser()` and `getpass.getpass()` appropriately.","solution":"import getpass def secure_login(user_credentials: dict, prompt_user: str = \'Username: \', prompt_pass: str = \'Password: \') -> bool: try: username = input(prompt_user) password = getpass.getpass(prompt_pass) if username in user_credentials and user_credentials[username] == password: return True else: return False except Exception as e: return False"},{"question":"Nearest Neighbors Classification with Custom Distance Metric # Objective You are required to implement a K-Nearest Neighbors (KNN) classification model using scikit-learn. The model should allow for a custom distance metric and should be capable of evaluating its performance on a given dataset. # Problem Statement 1. **Data Preparation**: - Read the dataset provided as a CSV file. The dataset has `n_features` features and one target variable `label`, which contains the class labels. - Split the dataset into training and test sets. 2. **Model Implementation**: - Implement a K-Nearest Neighbors classifier. Allow the user to specify `k` (number of neighbors) and the `distance_metric` to be used. - Utilize `sklearn.neighbors.KNeighborsClassifier` for the implementation. 3. **Custom Distance Function**: - Define a custom distance function named `custom_distance` which computes the Manhattan distance (L1 distance) between two points. - Allow the KNN model to use this custom distance function. 4. **Model Evaluation**: - Evaluate the model\'s accuracy on the test set and print out the accuracy score. # Input and Output Input: - A CSV file named `data.csv` with `n_features` feature columns and one `label` column. Output: - The accuracy of the KNN model on the test set. # Constraints - You must use `sklearn.neighbors.KNeighborsClassifier`. - Implement and validate your own function to compute Manhattan distance. # Example Suppose the dataset is structured as follows: ```plaintext feature1,feature2,feature3,label ... ,... ,... ,... ``` Your solution must follow this structure: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score # Step 1: Data Preparation data = pd.read_csv(\'data.csv\') X = data.drop(columns=[\'label\']) y = data[\'label\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 2: Custom Distance Function def custom_distance(x1, x2): return sum(abs(a - b) for a, b in zip(x1, x2)) # Step 3: Model Implementation k = 3 knn = KNeighborsClassifier(n_neighbors=k, metric=custom_distance) knn.fit(X_train, y_train) # Step 4: Model Evaluation y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy}\') ``` Fill in the appropriate code to complete the requirements. # Submission Submit a Python script with the implemented solution. Ensure the script reads the dataset, implements the KNN classifier with a custom distance metric, and prints the accuracy score.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score # Step 1: Data Preparation def prepare_data(file_path): data = pd.read_csv(file_path) X = data.drop(columns=[\'label\']) y = data[\'label\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) return X_train, X_test, y_train, y_test # Step 2: Custom Distance Function def custom_distance(x1, x2): return sum(abs(a - b) for a, b in zip(x1, x2)) # Step 3: Model Implementation def knn_classification(X_train, X_test, y_train, y_test, k): knn = KNeighborsClassifier(n_neighbors=k, metric=custom_distance) knn.fit(X_train, y_train) y_pred = knn.predict(X_test) return y_pred # Step 4: Model Evaluation def evaluate_model(y_test, y_pred): accuracy = accuracy_score(y_test, y_pred) return accuracy if __name__ == \\"__main__\\": X_train, X_test, y_train, y_test = prepare_data(\'data.csv\') k = 3 y_pred = knn_classification(X_train, X_test, y_train, y_test, k) accuracy = evaluate_model(y_test, y_pred) print(f\'Accuracy: {accuracy}\')"},{"question":"Background You are tasked with implementing a data processing system that involves compressing and decompressing large datasets. To ensure the system works efficiently, you need to handle both one-shot and incremental compression and decompression. Problem Statement Implement a function `process_data` that takes input data, compresses it, writes it to a file, then reads it back from the file and decompresses it to verify the integrity of the data. You will use the `bz2` module to accomplish this. Function Signature ```python def process_data(data: bytes, filename: str) -> bool: pass ``` Parameters - `data`: A bytes-like object containing the data to be compressed. - `filename`: A string specifying the name of the file to which the compressed data should be written, e.g., `\'data.bz2\'`. Returns - `bool`: Returns `True` if the decompressed data matches the original data, otherwise returns `False`. Constraints - You are not allowed to use any other compression module besides `bz2`. - Ensure the file is correctly closed after processing. - Handle potential exceptions that may arise during file operations. Example ```python data = b\\"Example data that needs to be compressed and decompressed for integrity check.\\" filename = \\"test.bz2\\" assert process_data(data, filename) == True ``` Notes - Use `bz2.open` for file operations involving compressed files. - You may use both one-shot and incremental (de)compression methods where appropriate.","solution":"import bz2 def process_data(data: bytes, filename: str) -> bool: try: # Write the compressed data to the file with bz2.open(filename, \'wb\') as compressed_file: compressed_file.write(data) # Read the compressed data from the file and decompress it with bz2.open(filename, \'rb\') as compressed_file: decompressed_data = compressed_file.read() # Check if the decompressed data matches the original data return decompressed_data == data except IOError: return False"},{"question":"Question: You are provided with two neural networks: a `float_model` (in floating-point precision) and a `quantized_model` (in integer precision) that have been quantized using PyTorch. Your task is to write a function that evaluates these models on a given dataset and computes the following metrics for the output tensors of the models: 1. Signal-to-Quantization-Noise Ratio (SQNR) 2. Normalized L2 error 3. Cosine similarity Use the functions from `torch.ao.ns.fx.utils` to compute these metrics. Your implementation should return a dictionary with the metrics and their values. Function Signature: ```python import torch from typing import Dict def evaluate_models(float_model: torch.nn.Module, quantized_model: torch.nn.Module, data_loader: torch.utils.data.DataLoader) -> Dict[str, float]: Evaluate float_model and quantized_model on the given data_loader using the specified metrics from torch.ao.ns.fx.utils. Parameters: float_model (torch.nn.Module): The floating-point precision model. quantized_model (torch.nn.Module): The quantized model. data_loader (torch.utils.data.DataLoader): DataLoader containing the dataset to evaluate the models on. Returns: Dict[str, float]: Dictionary containing the computed metrics (SQNR, Normalized L2 error, and Cosine similarity). pass # Example usage: # metrics = evaluate_models(float_model, quantized_model, data_loader) # print(metrics) ``` Input Format: - `float_model`: A PyTorch module representing the original floating-point model. - `quantized_model`: A PyTorch module representing the quantized model. - `data_loader`: A DataLoader providing batches of input data for model evaluation. Output Format: - Return a dictionary with the following keys and corresponding float values: - `\'sqnr\'`: Signal-to-Quantization-Noise Ratio - `\'normalized_l2_error\'`: Normalized L2 error - `\'cosine_similarity\'`: Cosine similarity Constraints: - Each tensor should be handled on device (CPU or GPU) as appropriate. - Evaluate the models on the same input data for a fair comparison. Performance Requirements: - Your solution should efficiently handle the provided DataLoader. - Ensure that the models\' evaluations are performed using PyTorch\'s no_grad context to avoid computational overhead associated with gradient computation. ```Example output``` ```python # Assuming float_model, quantized_model, and data_loader are predefined. metrics = evaluate_models(float_model, quantized_model, data_loader) print(metrics) # Output: {\'sqnr\': <float_value>, \'normalized_l2_error\': <float_value>, \'cosine_similarity\': <float_value>} ```","solution":"import torch from typing import Dict from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def evaluate_models(float_model: torch.nn.Module, quantized_model: torch.nn.Module, data_loader: torch.utils.data.DataLoader) -> Dict[str, float]: Evaluate float_model and quantized_model on the given data_loader using the specified metrics from torch.ao.ns.fx.utils. Parameters: float_model (torch.nn.Module): The floating-point precision model. quantized_model (torch.nn.Module): The quantized model. data_loader (torch.utils.data.DataLoader): DataLoader containing the dataset to evaluate the models on. Returns: Dict[str, float]: Dictionary containing the computed metrics (SQNR, Normalized L2 error, and Cosine similarity). # Set the models to evaluation mode float_model.eval() quantized_model.eval() # Initialize variables to accumulate results sqnr_total, l2_error_total, cosine_similarity_total = 0.0, 0.0, 0.0 num_batches = 0 with torch.no_grad(): for data in data_loader: inputs = data[0] # Assuming the dataLoader yields tuples of (inputs, labels) or just inputs float_outputs = float_model(inputs) quantized_outputs = quantized_model(inputs) sqnr_total += compute_sqnr(float_outputs, quantized_outputs).item() l2_error_total += compute_normalized_l2_error(float_outputs, quantized_outputs).item() cosine_similarity_total += compute_cosine_similarity(float_outputs, quantized_outputs).item() num_batches += 1 # Compute average of metrics metrics = { \'sqnr\': sqnr_total / num_batches, \'normalized_l2_error\': l2_error_total / num_batches, \'cosine_similarity\': cosine_similarity_total / num_batches } return metrics"},{"question":"**Coding Assessment Question:** You are tasked with visualizing a dataset using the seaborn objects interface. Specifically, you need to create a faceted grid of plots with customized layout dimensions and constraints. # Task 1. **Generate Synthetic Data:** - Create a DataFrame with the following columns: - \'Category\': A categorical variable with three levels \'A\', \'B\', and \'C\'. - \'X\': Numerical values ranging from 0 to 100. - \'Y\': Numerical values ranging from 0 to 100. 2. **Create a Faceted Plot:** - Use the seaborn objects interface to create a faceted grid of scatter plots. - Facet the plots by \'Category\' along the columns. 3. **Customize the Layout:** - Set the overall size of the figure to 8 inches by 4 inches. - Use the \'constrained\' engine for the layout. - Adjust the size of the individual plots within the figure to ensure they utilize about 80% of the figure\'s extent horizontally and 100% vertically. # Input - You will not receive any input, generate the data within your function. # Output - Display the resulting plot. # Constraints - Use only seaborn\'s objects interface (\'seaborn.objects as so\') for creating the plot. # Example: Here\'s an outline of what your solution might look like: ```python import pandas as pd import numpy as np import seaborn.objects as so import matplotlib.pyplot as plt # Step 1: Generate Data def generate_data(): data = pd.DataFrame({ \'Category\': np.random.choice([\'A\', \'B\', \'C\'], 300), \'X\': np.random.rand(300) * 100, \'Y\': np.random.rand(300) * 100 }) return data # Step 2: Create Plot def create_plot(data): p = so.Plot(data, x=\'X\', y=\'Y\').facet(col=\'Category\').add(so.Dot()) # Step 3: Customize Layout p.layout(size=(8, 4), engine=\'constrained\') p.layout(extent=[0, 0, 0.8, 1]) # Display the plot p.show() # Main execution data = generate_data() create_plot(data) ``` Ensure your solution correctly follows the steps and produces the required plot with the described customizations.","solution":"import pandas as pd import numpy as np import seaborn.objects as so import matplotlib.pyplot as plt # Step 1: Generate Data def generate_data(): data = pd.DataFrame({ \'Category\': np.random.choice([\'A\', \'B\', \'C\'], 300), \'X\': np.random.rand(300) * 100, \'Y\': np.random.rand(300) * 100 }) return data # Step 2: Create Plot def create_plot(data): p = so.Plot(data, x=\'X\', y=\'Y\').facet(col=\'Category\').add(so.Dot()) # Step 3: Customize Layout p.layout(size=(8, 4), engine=\'constrained\') p.layout(extent=[0, 0, 0.8, 1]) # Display the plot p.show() # Main execution data = generate_data() create_plot(data)"},{"question":"Coding Assessment Question # Objective In this task, you will need to create a hierarchical neural network using PyTorch\'s `nn.Module` and then use `torch.utils.module_tracker.ModuleTracker` to track and report the current position inside this hierarchy. # Instructions 1. **Define a Custom Neural Network Module**: - Create a class `CustomNet` that inherits from `torch.nn.Module`. - This class should contain at least three layers: an input layer, a hidden layer, and an output layer. - Each layer can be implemented using `torch.nn.Linear`. 2. **Implement Forward Pass**: - Within the `CustomNet` class, implement the `forward` method to define the forward pass through the network. 3. **Use ModuleTracker**: - Use `torch.utils.module_tracker.ModuleTracker` within the `forward` method to track the current position in the module hierarchy. - Print the current position at each layer during the forward pass. # Constraints - Input to the network should be a tensor with a shape that facilitates passing through the defined layers. - You are required to use `ModuleTracker` to demonstrate the hierarchy at each stage of the forward pass. # Example Assume the following layers and structure in `CustomNet`: - Input layer: 10 units - Hidden layer: 5 units - Output layer: 1 unit The `forward` method should print the following positions (assuming the input tensor size is appropriate): ``` Layer: Input, Position: CustomNet[0] Layer: Hidden, Position: CustomNet[1] Layer: Output, Position: CustomNet[2] ``` # Expected Input and Output Input: - A tensor `x` of appropriate shape to pass through the defined network. Output: - Printed positions tracked by `ModuleTracker` during the forward pass. # Your Implementation ```python import torch import torch.nn as nn import torch.utils.module_tracker class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.input_layer = nn.Linear(10, 5) self.hidden_layer = nn.Linear(5, 3) self.output_layer = nn.Linear(3, 1) self.tracker = torch.utils.module_tracker.ModuleTracker() def forward(self, x): self.tracker.enter_module(self.input_layer) print(f\\"Layer: Input, Position: {self.tracker.current_module()}\\") x = self.input_layer(x) self.tracker.exit_module(self.input_layer) self.tracker.enter_module(self.hidden_layer) print(f\\"Layer: Hidden, Position: {self.tracker.current_module()}\\") x = self.hidden_layer(x) self.tracker.exit_module(self.hidden_layer) self.tracker.enter_module(self.output_layer) print(f\\"Layer: Output, Position: {self.tracker.current_module()}\\") x = self.output_layer(x) self.tracker.exit_module(self.output_layer) return x # Example usage net = CustomNet() x = torch.randn(1, 10) output = net(x) ``` Your task is to implement the above custom module and ensure that the `ModuleTracker` prints the current positions as expected during the forward pass.","solution":"import torch import torch.nn as nn class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.input_layer = nn.Linear(10, 5) self.hidden_layer = nn.Linear(5, 3) self.output_layer = nn.Linear(3, 1) def forward(self, x): print(f\\"Layer: Input, Position: self.input_layer\\") x = self.input_layer(x) print(f\\"Layer: Hidden, Position: self.hidden_layer\\") x = self.hidden_layer(x) print(f\\"Layer: Output, Position: self.output_layer\\") x = self.output_layer(x) return x"},{"question":"**Title: Tensor Validation using PyTorch** **Objective:** Your task is to write a function in PyTorch that performs a series of operations on tensors and validates the results using the `torch.testing` module. Specifically, you will: 1. Create two tensors with specified properties. 2. Perform a mathematical operation on these tensors. 3. Validate the results using the functions provided in `torch.testing`. **Function Signature:** ```python import torch def tensor_operations_and_validation(tensor_size: int, mean_1: float, std_1: float, mean_2: float, std_2: float) -> bool: Create two tensors with normally distributed values, perform a mathematical operation, and validate the result. Args: tensor_size (int): The size of the tensors to create (both tensors will be of shape (tensor_size, tensor_size)). mean_1 (float): The mean value for the first tensor\'s normal distribution. std_1 (float): The standard deviation for the first tensor\'s normal distribution. mean_2 (float): The mean value for the second tensor\'s normal distribution. std_2 (float): The standard deviation for the second tensor\'s normal distribution. Returns: bool: True if the result of the validation passes, False otherwise. pass ``` **Instructions:** 1. **Create two tensors**: - Generate two tensors `tensor_a` and `tensor_b` of shape `(tensor_size, tensor_size)` using a normal distribution. - The first tensor `tensor_a` should have values with a mean of `mean_1` and standard deviation of `std_1`. - The second tensor `tensor_b` should have values with a mean of `mean_2` and standard deviation of `std_2`. - You can use `torch.randn` and adjust the mean and standard deviation accordingly. 2. **Perform Operations**: - Compute the element-wise addition of these two tensors and store the result in `tensor_c`. 3. **Validation**: - Create an expected tensor `expected_result` which can be computed by manually adding the means of the distributions for each corresponding element of `tensor_a` and `tensor_b`. - Use `torch.testing.assert_allclose` to check if `tensor_c` is approximately equal to `expected_result`. 4. **Return the Result**: - If the tensors are approximately equal, return `True`. - If not, return `False`. **Example:** Suppose the function is called with: ```python tensor_operations_and_validation(3, 0, 1, 0, 1) ``` This should: - Create two 3x3 tensors with values from normal distributions (mean 0, std 1). - Add these tensors element-wise. - Validate the result by checking if it matches the expected mean values (0 in this case, since both distributions have mean 0). You are required to write the complete implementation for `tensor_operations_and_validation`. **Constraints:** - Ensure that any random seed settings or tensor generation follow PyTorch\'s conventions. - Make sure the implementation uses the specified `torch.testing` functions correctly.","solution":"import torch def tensor_operations_and_validation(tensor_size: int, mean_1: float, std_1: float, mean_2: float, std_2: float) -> bool: Create two tensors with normally distributed values, perform a mathematical operation, and validate the result. Args: tensor_size (int): The size of the tensors to create (both tensors will be of shape (tensor_size, tensor_size)). mean_1 (float): The mean value for the first tensor\'s normal distribution. std_1 (float): The standard deviation for the first tensor\'s normal distribution. mean_2 (float): The mean value for the second tensor\'s normal distribution. std_2 (float): The standard deviation for the second tensor\'s normal distribution. Returns: bool: True if the result of the validation passes, False otherwise. # Set random seed for reproducibility torch.manual_seed(0) # Create two tensors with normal distributions tensor_a = torch.randn(tensor_size, tensor_size) * std_1 + mean_1 tensor_b = torch.randn(tensor_size, tensor_size) * std_2 + mean_2 # Perform element-wise addition tensor_c = tensor_a + tensor_b # Calculate the expected result tensor expected_result = tensor_a + tensor_b # Validate the result try: torch.testing.assert_allclose(tensor_c, expected_result) return True except AssertionError: return False"},{"question":"# Sound File Type Analysis You are provided with a list of file paths to sound files. Your task is to write a function that utilizes the `sndhdr` module to determine the type of sound data for each file and then summarizes the results. Function Signature ```python from typing import List, Dict, Tuple, Optional from collections import namedtuple def analyze_sound_files(file_paths: List[str]) -> Dict[str, List[Tuple[str, Optional[int], Optional[int], Optional[int], Optional[int]]]]: # Your code here ``` Input - `file_paths`: A list of strings where each string is a path to a sound file. Output - The function should return a dictionary where the keys are the unique sound file types determined by `sndhdr.what`, and the values are lists of tuples. Each tuple should represent the properties of the sound file with the following structure: `(path_to_file, framerate, nchannels, nframes, sampwidth)` Constraints - You should handle cases where the type of sound data cannot be determined by saving an entry with `None` for all its properties except `path_to_file`. - Performance considerations: Assume the list can contain up to 1000 file paths. - The function should use `sndhdr.what` to determine the file type. Example ```python # Example Files (Assuming they exist in the execution environment): file_paths = [ \\"test1.wav\\", \\"test2.aiff\\", \\"corrupt_file.bin\\", \\"test3.wav\\" ] # Example use case result = analyze_sound_files(file_paths) print(result) # Expected Output (This will vary based on the actual content of the sound files): { \'wav\': [(\'test1.wav\', 44100, 2, 1024, 16), (\'test3.wav\', 44100, 2, 1024, 16)], \'aiff\': [(\'test2.aiff\', 48000, 1, -1, 16)], None: [(\'corrupt_file.bin\', None, None, None, None)] } ``` Notes - Pay attention to edge cases where the file might be corrupted or not a recognizable sound file format. - Ensure your implementation is clean, and provides meaningful error handling and logging wherever necessary.","solution":"import sndhdr from typing import List, Dict, Tuple, Optional def analyze_sound_files(file_paths: List[str]) -> Dict[str, List[Tuple[str, Optional[int], Optional[int], Optional[int], Optional[int]]]]: result = {} for file_path in file_paths: info = sndhdr.what(file_path) if info: file_type = info.filetype file_properties = (file_path, info.framerate, info.nchannels, info.nframes, info.sampwidth) else: file_type = None file_properties = (file_path, None, None, None, None) if file_type not in result: result[file_type] = [] result[file_type].append(file_properties) return result"},{"question":"Coding Assessment Question # Problem Description You are tasked with analyzing a series of steps taken by a hypothetical machine. Each step has a corresponding operation and two numerical arguments. Your job is to process these steps efficiently using Python\'s `itertools`, `functools`, and `operator` modules. # Input - A list of tuples `steps`, where each tuple contains: 1. A string representing an operation (`\\"add\\"`, `\\"sub\\"`, `\\"mul\\"`, `\\"truediv\\"`, `\\"pow\\"`, `\\"mod\\"`). 2. Two integers on which the operation is to be performed. # Output - A single integer that is the result of sequentially applying the operations in the order they appear in the list, starting with an initial value of 0. # Constraints - Each operation string is guaranteed to be one of the specified operations. - The list `steps` is non-empty. - Division operations will always involve non-zero denominators. # Example Input: ```python steps = [ (\\"add\\", 5, 3), # 5 + 3 (\\"mul\\", 8, 4), # 8 * 4 (\\"sub\\", 32, 10), # 32 - 10 (\\"truediv\\", 22, 2)# 22 / 2 ] ``` Output: ```python 11 ``` # Explanation: 1. Initial value = 0 2. Add 5 + 3: Result is 8 3. Multiply 8 by 4: Result is 32 4. Subtract 10 from 32: Result is 22 5. Divide 22 by 2: Result is 11 # Requirements 1. Use `functools.reduce` to accumulate the result. 2. Use `operator` functions to perform the arithmetic operations. 3. Use `itertools` where appropriate to enhance loop efficiency. # Function Signature ```python def process_steps(steps: list) -> int: pass ``` # Notes - Ensure your function adheres to the input-output format strictly. - Incorporate error-checking to handle edge cases if any (though the constraints guarantee valid operations in this context).","solution":"from functools import reduce import operator def process_steps(steps): Processes a series of steps, performing operations sequentially and returning the final result. :param steps: A list of tuples containing operation and two integers. :return: The result of sequentially applying the operations. # Define a dictionary to map operation strings to operator functions operations = { \\"add\\": operator.add, \\"sub\\": operator.sub, \\"mul\\": operator.mul, \\"truediv\\": operator.truediv, \\"pow\\": operator.pow, \\"mod\\": operator.mod } # Function to perform an operation given current result and a step def perform_operation(current_result, step): op, a, b = step return operations[op](a, b) # Reduce the steps with initial value as 0 result = reduce(lambda r, step: perform_operation(r, step), steps, 0) return result"},{"question":"# Seaborn Scatter Plot and Facet Grid Objective Your task is to write a Python function using Seaborn to visualize the relationship between the total bill and tip amounts from the \\"tips\\" dataset. You will also add categorical and numerical distinctions within the plots. Function Signature ```python def plot_tips_data(): pass ``` Requirements 1. Load the \\"tips\\" dataset from Seaborn\'s datasets. 2. Create a scatter plot showing the relationship between `total_bill` and `tip`, where: - Points are colored based on the `time` of the day (`Lunch` or `Dinner`). - Different markers are used to distinguish between different `days` of the week. 3. Customize the plot with the following features: - Use a specific color palette for the hues. - Vary the marker size based on the size of the dining party (`size`). 4. Use `relplot` to create faceted scatter plots: - Facet the plots into two columns based on the `time` of the day. - Each subplot should distinguish between the `day` of the week using both color (`hue`) and marker style (`style`). 5. Display the plots on the screen. Example Output The function should generate a visual output in the form of scatter plots and facet grids as described. The points should be well differentiated by color, marker style, and size. Constraints and Notes - You should ensure that the legends are clear and that all unique values are represented. - The plot aesthetics should be clear and informative. Expected Output Running the function should produce the scatter plots as specified with faceted views splitting the data by the `time` of the day. ```python def plot_tips_data(): # Implementation here ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_tips_data(): # Load the tips dataset from Seaborn tips = sns.load_dataset(\\"tips\\") # Create a scatter plot using relplot where `total_bill` vs `tip` # Points are colored based on `time` and different markers for `day` sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\", size=\\"size\\", palette=\\"viridis\\", sizes=(20, 200), col=\\"time\\", col_wrap=2) # Display the plot plt.show()"},{"question":"# Descriptors for Managing and Validating Attributes In this assessment, you will create a set of descriptors to manage and validate attributes of a `Product` class. You are required to implement a system where descriptors will: 1. Ensure attributes are of specific types (e.g., `str`, `int`, `float`). 2. Validate that attributes fall within specific ranges or adhere to certain conditions. 3. Log any access and modification of these attributes. You will need to create three descriptor classes: 1. `TypeValidator`: Ensures that attributes are of a specific type. 2. `RangeValidator`: Ensures that numeric attributes fall within a specified range. 3. `LoggedAccess`: Logs any accesses to the attributes. # Requirements: `TypeValidator` Class - **Input:** - `expected_type` (type): The expected type of the attribute. - **Descriptor Methods:** - `__get__`: Returns the attribute value. - `__set__`: Validates the type of the value before setting it. `RangeValidator` Class - **Input:** - `min_value` (float or int): The minimum allowable value for the attribute. - `max_value` (float or int): The maximum allowable value for the attribute. - **Descriptor Methods:** - `__get__`: Returns the attribute value. - `__set__`: Validates that the value falls within the specified range before setting it. `LoggedAccess` Class - **Descriptor Methods:** - `__get__`: Logs the access and returns the attribute value. - `__set__`: Logs the update and sets the attribute value. # Task: 1. Implement the `TypeValidator`, `RangeValidator`, and `LoggedAccess` classes. 2. Create a `Product` class that uses these descriptors to manage its attributes. # Example Here\'s an example of how the `Product` class should work: ```python import logging logging.basicConfig(level=logging.INFO) class TypeValidator: def __init__(self, expected_type): self.expected_type = expected_type def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if not isinstance(value, self.expected_type): raise TypeError(f\\"Expected {self.private_name} to be of type {self.expected_type.__name__}, got {type(value).__name__}\\") setattr(obj, self.private_name, value) class RangeValidator: def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if not isinstance(value, (int, float)): raise TypeError(f\\"Expected {self.private_name} to be an int or float\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"Expected {self.private_name} to be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"Expected {self.private_name} to be at most {self.max_value}\\") setattr(obj, self.private_name, value) class LoggedAccess: def __set_name__(self, owner, name): self.name = name self.private_name = \'_\' + name def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logging.info(f\'Accessing {self.name} giving {value}\') return value def __set__(self, obj, value): logging.info(f\'Updating {self.name} to {value}\') setattr(obj, self.private_name, value) class Product: name = LoggedAccess() price = RangeValidator(0, 10000) quantity = TypeValidator(int) def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity # Testing the Product class prod = Product(\'Gadget\', 2499.99, 100) print(prod.name) # Logged access prod.price = 2999.99 # Logged update prod.quantity = 200 # Attempting invalid operations # prod.quantity = 2.5 # Should raise TypeError # prod.price = -1000 # Should raise ValueError ``` # Constraints: - Ensure that all attribute accesses and modifications are logged using the `logging` module. - The `price` attribute should support both `int` and `float` types but should be within the range of 0 to 10,000. - The `name` attribute should be logged on access and update. - The `quantity` attribute should only accept integers. # Submission: - Implement the `TypeValidator`, `RangeValidator`, and `LoggedAccess` descriptor classes. - Implement the `Product` class using these descriptors. - Write test cases to verify the functionality of your implementation.","solution":"import logging logging.basicConfig(level=logging.INFO) class TypeValidator: def __init__(self, expected_type): self.expected_type = expected_type def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if not isinstance(value, self.expected_type): raise TypeError(f\\"Expected {self.private_name} to be of type {self.expected_type.__name__}, got {type(value).__name__}\\") setattr(obj, self.private_name, value) class RangeValidator: def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if not isinstance(value, (int, float)): raise TypeError(f\\"Expected {self.private_name} to be an int or float\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"Expected {self.private_name} to be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"Expected {self.private_name} to be at most {self.max_value}\\") setattr(obj, self.private_name, value) class LoggedAccess: def __set_name__(self, owner, name): self.name = name self.private_name = \'_\' + name def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logging.info(f\'Accessing {self.name} giving {value}\') return value def __set__(self, obj, value): logging.info(f\'Updating {self.name} to {value}\') setattr(obj, self.private_name, value) class Product: name = LoggedAccess() price = RangeValidator(0, 10000) quantity = TypeValidator(int) def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity"},{"question":"# Coding Challenge: Build a Custom Threaded TCP Server Objective: Your task is to create a custom threaded TCP server that computes the Fibonacci sequence up to the Nth number sent by the client. The server must handle multiple client requests concurrently using threading. Requirements: 1. **Server-Side:** - Create a class `FibonacciTCPHandler` that inherits from `socketserver.BaseRequestHandler`. - Override the `handle()` method to: - Receive an integer `N` from the client. - Compute the Fibonacci sequence up to the Nth number. - Send the sequence back to the client as a space-separated string. - Create a threaded server class `ThreadedFibonacciTCPServer` using `ThreadingMixIn` and `TCPServer`. - Ensure the server handles multiple client connections concurrently. 2. **Client-Side:** - Create a simple client script that: - Connects to the server. - Sends an integer `N` to the server. - Receives and prints the Fibonacci sequence from the server. 3. **Constraints:** - The server should handle invalid inputs gracefully by sending an error message back to the client. - Use a default port `9999` for the server. - Ensure the server runs effectively by testing it with multiple clients simultaneously. Input and Output Formats: - **Client Input:** An integer `N`. - **Server Output:** A space-separated string of the first `N` numbers in the Fibonacci sequence or an error message. Example: - **Client Input:** `5` - **Server Output:** `0 1 1 2 3` Additional Information: - Fibonacci sequence: `0, 1, 1, 2, 3, 5, 8, ...` Code Structure: 1. **Server Code (server.py):** ```python import socketserver import threading class FibonacciTCPHandler(socketserver.BaseRequestHandler): def handle(self): try: data = self.request.recv(1024).strip() n = int(data) if n < 0: raise ValueError(\\"Negative number\\") fibonacci_seq = self.compute_fibonacci(n) response = \\" \\".join(map(str, fibonacci_seq)) except Exception as e: response = f\\"Error: {e}\\" self.request.sendall(response.encode(\'utf-8\')) def compute_fibonacci(self, n): fib_sequence = [0, 1] for i in range(2, n): fib_sequence.append(fib_sequence[-1] + fib_sequence[-2]) return fib_sequence[:n] class ThreadedFibonacciTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedFibonacciTCPServer((HOST, PORT), FibonacciTCPHandler) as server: server.serve_forever() ``` 2. **Client Code (client.py):** ```python import socket HOST, PORT = \\"localhost\\", 9999 def request_fibonacci(n): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: sock.connect((HOST, PORT)) sock.sendall(str(n).encode(\'utf-8\')) response = sock.recv(1024).decode(\'utf-8\') print(\\"Received:\\", response) if __name__ == \\"__main__\\": n = int(input(\\"Enter the value of N: \\")) request_fibonacci(n) ``` Ensure to run the server script before testing with the client script. This coding challenge tests your understanding of network server creation, concurrency, and basic error handling in Python.","solution":"import socketserver import threading class FibonacciTCPHandler(socketserver.BaseRequestHandler): def handle(self): try: data = self.request.recv(1024).strip() n = int(data) if n < 0: raise ValueError(\\"Negative number\\") fibonacci_seq = self.compute_fibonacci(n) response = \\" \\".join(map(str, fibonacci_seq)) except Exception as e: response = f\\"Error: {e}\\" self.request.sendall(response.encode(\'utf-8\')) def compute_fibonacci(self, n): if n == 0: return [] elif n == 1: return [0] fib_sequence = [0, 1] for i in range(2, n): fib_sequence.append(fib_sequence[-1] + fib_sequence[-2]) return fib_sequence class ThreadedFibonacciTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedFibonacciTCPServer((HOST, PORT), FibonacciTCPHandler) as server: server.serve_forever()"},{"question":"# Question: You are required to use the seaborn library to create a multi-layered plot that visualizes the distribution of a dataset. Specifically, you should load the \\"iris\\" dataset from seaborn and create a plot that uses jitter to distinguish overlapping points and overlay ranges to show the distribution of the \\"sepal_length\\" for different species. Requirements: 1. **Load the iris dataset** from seaborn. 2. **Create a plot** that displays \\"species\\" on the x-axis and \\"sepal_length\\" on the y-axis. 3. **Add jitter** to help distinguish overlapping points. 4. **Overlay ranges** representing the interquartile ranges (25th to 75th percentile) of the \\"sepal_length\\" within each species category. 5. **Customize the plot** with a suitable title and axis labels for clarity. Constraints: - The plot should be clear and informative, distinguishing different species and their sepal lengths effectively. - Utilize seaborn\'s plotting functionalities as demonstrated in the provided documentation. Input and Output: - There is no specific input function. The entire process should be encapsulated in a single function `create_iris_plot()`. - The function should not return any value; it should simply display the plot. Example Implementation: Here is an example signature for your function: ```python import seaborn.objects as so from seaborn import load_dataset def create_iris_plot(): # Load the dataset iris = load_dataset(\\"iris\\") # Create the plot with jitter and range ( so.Plot(iris, x=\\"species\\", y=\\"sepal_length\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) .label(title=\\"Sepal Length Distribution Across Iris Species\\", x=\\"Species\\", y=\\"Sepal Length (cm)\\") ).show() # Call the function to display the plot create_iris_plot() ``` This function loads the iris dataset, creates a seaborn plot, adds jitter and range to the plot, labels it appropriately, and finally displays it.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_iris_plot(): # Load the dataset iris = load_dataset(\\"iris\\") # Create the plot with jitter and range ( so.Plot(iris, x=\\"species\\", y=\\"sepal_length\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) .label(title=\\"Sepal Length Distribution Across Iris Species\\", x=\\"Species\\", y=\\"Sepal Length (cm)\\") ).show() # Call the function to display the plot create_iris_plot()"},{"question":"Problem Statement Consider you are developing a script to sort emails in an \\"mbox\\" mailbox format into multiple \\"Maildir\\" mailboxes based on specific criteria. You have to implement a function that takes an `mbox` mailbox, reads the messages, and distributes them into multiple `Maildir` mailboxes based on the subject line of each message. If an email is spam, it should be moved to a separate `Maildir` mailbox named \\"spam\\". # Function Signature ```python def sort_mbox_to_maildir(mbox_path: str, output_dir: str, spam_keywords: List[str], create: bool = True) -> None: pass ``` # Input - `mbox_path` (str): The path to the `mbox` mailbox. - `output_dir` (str): The directory where the sorted `Maildir` mailboxes will be created. - `spam_keywords` (List[str]): A list of keywords that identify a message as spam if found in the subject line. - `create` (bool): Whether to create the mailbox if it doesn\'t exist. Default is `True`. # Output - None. The function modifies the filesystem by creating mailboxes and adding/removing messages. # Constraints - Each message should be uniquely handled, considering concurrent modifications that might occur. - Handle various exceptions like parsing errors without terminating the program prematurely. # Example ```python sort_mbox_to_maildir(\'mymailbox.mbox\', \'output_maildirs\', [\'buy now\', \'free\', \'click here\']) ``` # Instructions 1. **Initialize Mailboxes**: Create a Maildir mailbox for each category in the output directory. 2. **Process Messages**: Read each message from the `mbox` mailbox and determine its category based on the keywords in the `spam_keywords` list. 3. **Add and Remove Messages**: For each message, add it to the appropriate `Maildir` mailbox and then remove it from the original `mbox` mailbox. 4. **Error Handling**: Properly handle exceptions related to mailbox operations and malformed message parsing. Implement the solution considering performance and memory management while iterating over mailbox messages and file operations. # Notes - The function should be robust, ensuring that no messages are lost during the process. - Use the provided methods (`lock`, `unlock`, `add`, `flush`, `discard`, `get`) smartly to manage concurrency and ensure data integrity.","solution":"import mailbox import os from email import policy from email.parser import BytesParser from typing import List def sort_mbox_to_maildir(mbox_path: str, output_dir: str, spam_keywords: List[str], create: bool = True) -> None: Sorts emails from an mbox mailbox into multiple Maildir mailboxes based on specific criteria. Args: mbox_path (str): The path to the mbox mailbox. output_dir (str): The directory where the sorted Maildir mailboxes will be created. spam_keywords (List[str]): A list of keywords that identify a message as spam if found in the subject line. create (bool): Whether to create the mailbox if it doesn\'t exist. Default is True. # Create the output directory if it doesn\'t exist if not os.path.exists(output_dir): os.makedirs(output_dir) # Open the mbox file mbox = mailbox.mbox(mbox_path, create=create, factory=lambda file: BytesParser(policy=policy.default).parse(file)) # Create Maildir mailboxes maildirs = {} maildirs[\'spam\'] = mailbox.Maildir(os.path.join(output_dir, \'spam\'), create=create) maildirs[\'inbox\'] = mailbox.Maildir(os.path.join(output_dir, \'inbox\'), create=create) try: for message_key in mbox.keys(): message = mbox.get(message_key) # Check if the message is spam subject = message.get(\'subject\', \'\') is_spam = any(keyword in subject.lower() for keyword in spam_keywords) # Determine the target mailbox (spam or inbox) target_mailbox = maildirs[\'spam\'] if is_spam else maildirs[\'inbox\'] target_mailbox.add(message) # Remove the message from mbox mbox.remove(message_key) # Finalize changes for maildir in maildirs.values(): maildir.flush() mbox.flush() finally: # Unlock the mailboxes for maildir in maildirs.values(): maildir.close() mbox.close()"},{"question":"**Complex Number Class Implementation** You are required to implement a `ComplexNumber` class to handle operations on complex numbers. The class should support standard operations such as addition, subtraction, multiplication, and division of complex numbers, as well as their assignment. Additionally, the class should implement custom error handling for invalid operations. Requirements 1. **ComplexNumber Class**: - **Attributes**: Two attributes to represent the real and imaginary parts of a complex number. - **Methods**: - `__init__(self, real, imag)`: Constructor to initialize the complex number. - `__add__(self, other)`: Addition of two complex numbers. - `__sub__(self, other)`: Subtraction of two complex numbers. - `__mul__(self, other)`: Multiplication of two complex numbers. - `__truediv__(self, other)`: Division of two complex numbers. Raise a `ZeroDivisionError` if the denominator is zero. - `conjugate(self)`: Return the conjugate of the complex number. - `__str__(self)`: Return the string representation in the form `a + bi`. - Use augmented assignment statements (`+=`, `-=`, `*=`, `/=`) where appropriate. - Ensure that your implementation correctly handles different data types for the operations (raising a `TypeError` if inappropriate types are used). 2. **Custom Exceptions**: - Create at least one custom exception class to handle an invalid operation scenario not already covered by standard exceptions. Input and Output - **Input**: Creation of complex number objects and calling their methods. - **Output**: Results of operations printed, and custom exception messages where necessary. Constraints - The real and imaginary parts must be numerical values. - Handle type errors gracefully with appropriate messages. Example Usage ```python try: num1 = ComplexNumber(3, 2) num2 = ComplexNumber(1, 7) print(num1 + num2) # Output: 4 + 9i print(num1 - num2) # Output: 2 - 5i print(num1 * num2) # Output: -11 + 23i print(num1 / num2) # Output: 0.3448275862068966 - 0.3793103448275862i print(num1.conjugate()) # Output: 3 - 2i num1 += num2 print(num1) # Output: 4 + 9i num1 -= num2 print(num1) # Output: 3 + 2i num1 *= num2 print(num1) # Output: -11 + 23i num1 /= num2 print(num1) # Output: 0.3448275862068966 - 0.3793103448275862i num3 = ComplexNumber(3, 0) print(num1 / num3) # Should raise ZeroDivisionError num4 = ComplexNumber(3, \'a\') # Should raise TypeError except ZeroDivisionError: print(\\"Cannot divide by zero.\\") except TypeError as e: print(f\\"Type error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") ``` **Note**: Your implementation should handle both the creation and operations of the complex number objects while ensuring compliance with the constraints and appropriate use of assignment statements and custom exception handling.","solution":"class ComplexNumber: def __init__(self, real, imag): if not (isinstance(real, (int, float)) and isinstance(imag, (int, float))): raise TypeError(\\"Real and imaginary parts must be numerical values.\\") self.real = real self.imag = imag def __add__(self, other): if isinstance(other, ComplexNumber): return ComplexNumber(self.real + other.real, self.imag + other.imag) raise TypeError(\\"Operand must be of type ComplexNumber\\") def __sub__(self, other): if isinstance(other, ComplexNumber): return ComplexNumber(self.real - other.real, self.imag - other.imag) raise TypeError(\\"Operand must be of type ComplexNumber\\") def __mul__(self, other): if isinstance(other, ComplexNumber): real = self.real * other.real - self.imag * other.imag imag = self.real * other.imag + self.imag * other.real return ComplexNumber(real, imag) raise TypeError(\\"Operand must be of type ComplexNumber\\") def __truediv__(self, other): if isinstance(other, ComplexNumber): if other.real == 0 and other.imag == 0: raise ZeroDivisionError(\\"Cannot divide by zero.\\") denominator = other.real**2 + other.imag**2 real = (self.real * other.real + self.imag * other.imag) / denominator imag = (self.imag * other.real - self.real * other.imag) / denominator return ComplexNumber(real, imag) raise TypeError(\\"Operand must be of type ComplexNumber\\") def conjugate(self): return ComplexNumber(self.real, -self.imag) def __str__(self): if self.imag < 0: return f\\"{self.real} - {-self.imag}i\\" return f\\"{self.real} + {self.imag}i\\" class InvalidOperationError(Exception): pass # Example Usage try: num1 = ComplexNumber(3, 2) num2 = ComplexNumber(1, 7) print(num1 + num2) # Output: 4 + 9i print(num1 - num2) # Output: 2 - 5i print(num1 * num2) # Output: -11 + 23i print(num1 / num2) # Output: 0.3448275862068966 - 0.3793103448275862i print(num1.conjugate()) # Output: 3 - 2i num1 += num2 print(num1) # Output: 4 + 9i num1 -= num2 print(num1) # Output: 3 + 2i num1 *= num2 print(num1) # Output: -11 + 23i num1 /= num2 print(num1) # Output: 0.3448275862068966 - 0.3793103448275862i num3 = ComplexNumber(3, 0) print(num1 / num3) # Should raise ZeroDivisionError num4 = ComplexNumber(3, \'a\') # Should raise TypeError except ZeroDivisionError: print(\\"Cannot divide by zero.\\") except TypeError as e: print(f\\"Type error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective: Write a C extension module for Python that defines a new type with specified attributes and methods, supports Python\'s cyclic garbage collector, and includes proper memory management techniques. Description: 1. **Type Name**: `Circle` 2. **Attributes**: - `radius` (a float representing the radius of the circle) - `color` (a string representing the color of the circle) 3. **Methods**: - `area()` (returns the area of the circle) - `perimeter()` (returns the perimeter of the circle) Requirements: 1. **Define the Type**: - Create a `CircleObject` structure with `PyObject_HEAD`, `radius`, and `color`. - Define `CircleType` PyTypeObject with appropriate flags and slots. 2. **Implement Methods**: - `area()` method: calculates and returns the area of the circle. - `perimeter()` method: calculates and returns the perimeter of the circle. 3. **Memory Management**: - Implement `tp_dealloc` to properly manage memory. - Use `tp_new` and `tp_init` to initialize the object with default values. - Ensure attributes are properly allocated and deallocated. 4. **Garbage Collection**: - Support Python\'s cyclic garbage collector by implementing `tp_traverse` and `tp_clear`. 5. **Attribute Control**: - Define getters and setters for `radius` and `color` to ensure valid data types and prevent deletion. 6. **Module Initialization**: - Define `PyInit_circle` to initialize the module and add the `Circle` type to the module. Input/Output: - **Input**: The `Circle` object can be initialized with optional `radius` and `color` values. - **Output**: The `area()` and `perimeter()` methods will return the respective calculated values. Constraints: 1. **Radius** must be a float and non-negative. 2. **Color** must be a string. Notes: - Students must follow good memory management practices. - Ensure proper error handling in methods and initialization functions. Example: ```python import circle # Create a new Circle object c = circle.Circle(radius=5.0, color=\\"red\\") # Get the area of the circle print(c.area()) # Output should be ~78.54 # Get the perimeter of the circle print(c.perimeter()) # Output should be ~31.42 # Try setting an invalid radius try: c.radius = -2.0 # Should raise an exception except ValueError as e: print(e) ``` Your task is to implement this C extension following the provided requirements.","solution":"from math import pi class Circle: def __init__(self, radius=0.0, color=\'\'): if not isinstance(radius, (int, float)) or radius < 0: raise ValueError(\\"Radius must be a non-negative float.\\") if not isinstance(color, str): raise ValueError(\\"Color must be a string.\\") self._radius = float(radius) self._color = color @property def radius(self): return self._radius @radius.setter def radius(self, value): if not isinstance(value, (int, float)) or value < 0: raise ValueError(\\"Radius must be a non-negative float.\\") self._radius = float(value) @property def color(self): return self._color @color.setter def color(self, value): if not isinstance(value, str): raise ValueError(\\"Color must be a string.\\") self._color = value def area(self): return pi * (self._radius ** 2) def perimeter(self): return 2 * pi * self._radius"},{"question":"Title: Locale-Aware Formatter and Comparator Objective: Implement a **LocaleFormatter** class in Python that provides functionalities to convert and format numerical data based on user-provided locale settings. This class should handle both numeric and monetary values, and utilize locale-based string comparison. Class: `LocaleFormatter` Methods to Implement: 1. **`__init__(self, locale_name: str)`** - **Parameters:** - `locale_name` (str): Locale identifier string (e.g., \'en_US.UTF-8\'). - **Functionality:** - Initializes the locale for the given `locale_name` for all categories. 2. **`format_number(self, value: float, grouping: bool=False) -> str`** - **Parameters:** - `value` (float): The numeric value to format. - `grouping` (bool, optional): If True, apply grouping (e.g., thousand separators). - **Returns:** - Formatted string representation of the numeric value according to the current locale. 3. **`format_currency(self, value: float, grouping: bool=False, international: bool=False) -> str`** - **Parameters:** - `value` (float): The monetary value to format. - `grouping` (bool, optional): If True, apply grouping (e.g., thousand separators). - `international` (bool, optional): If True, use the international currency symbol. - **Returns:** - Formatted string representation of the monetary value according to the current locale. 4. **`compare_strings(self, string1: str, string2: str) -> int`** - **Parameters:** - `string1` (str): The first string to compare. - `string2` (str): The second string to compare. - **Returns:** - An integer indicating the comparison result: negative if `string1 < string2`, zero if `string1 == string2`, positive if `string1 > string2`. Constraints: - Ensure that locale settings are restored after performing the operations, preserving the thread safety. - The locale settings should default back in case of any failure during the initialization or operation. - Raise appropriate errors if provided locale is not recognized. Example Usage: ```python formatter = LocaleFormatter(\'de_DE.UTF-8\') # Format numeric value number_formatted = formatter.format_number(1234567.89, grouping=True) print(number_formatted) # Output: \'1.234.567,89\' (depending on locale) # Format currency value currency_formatted = formatter.format_currency(1234567.89, grouping=True, international=True) print(currency_formatted) # Output: \'1.234.567,89 EUR\' (depending on locale) # Compare strings comparison_result = formatter.compare_strings(\\"äpfel\\", \\"apfel\\") print(comparison_result) # Output will depend on locale\'s collation rules ``` Performance Requirements: - The implementation should handle large values and frequent calls efficiently. - Locale change operations should minimally impact performance. Notes: - Use the `locale` module functions to handle locale-specific formatting. - Ensure thread safety by managing locale settings within the methods. Evaluation Criteria: - Correctness of locale-dependent formatting. - Efficiency and thread safety in handling locale settings. - Proper error handling and default behavior.","solution":"import locale from functools import cmp_to_key class LocaleFormatter: def __init__(self, locale_name: str): Initializes the locale for the given locale_name for all categories. :param locale_name: Locale identifier string (e.g., \'en_US.UTF-8\'). try: self.original_locale = locale.setlocale(locale.LC_ALL) locale.setlocale(locale.LC_ALL, locale_name) self.locale_name = locale_name except locale.Error: raise ValueError(f\\"Locale {locale_name} not recognized\\") def format_number(self, value: float, grouping: bool=False) -> str: Formats a numeric value according to the current locale. :param value: The numeric value to format. :param grouping: If True, apply grouping (e.g., thousand separators). :return: Formatted string representation of the numeric value. try: result = locale.format_string(\\"%f\\", value, grouping) if \\".\\" in result: result = result.rstrip(\'0\').rstrip(\'.\') return result finally: locale.setlocale(locale.LC_ALL, self.original_locale) def format_currency(self, value: float, grouping: bool=False, international: bool=False) -> str: Formats a monetary value according to the current locale. :param value: The monetary value to format. :param grouping: If True, apply grouping (e.g., thousand separators). :param international: If True, use the international currency symbol. :return: Formatted string representation of the monetary value. try: return locale.currency(value, grouping=grouping, international=international) finally: locale.setlocale(locale.LC_ALL, self.original_locale) def compare_strings(self, string1: str, string2: str) -> int: Compares two strings according to the current locale\'s collation rules. :param string1: The first string to compare. :param string2: The second string to compare. :return: Negative if string1 < string2, zero if string1 == string2, positive if string1 > string2. try: return locale.strcoll(string1, string2) finally: locale.setlocale(locale.LC_ALL, self.original_locale)"},{"question":"**Objective:** Implement a Python script to handle cookies using the `http.cookiejar` module. Your script should demonstrate creating a `CookieJar` object, making HTTP requests while managing cookies, and saving/loading cookies from a file. **Task:** Write a Python function `fetch_url_with_cookies(url, cookie_file)` that: 1. Takes two parameters: - `url` (a string representing the URL to fetch). - `cookie_file` (a string representing the path to save/load cookies). 2. Creates a `MozillaCookieJar` object to store cookies. 3. Loads cookies from the specified file if it exists. 4. Uses an `urllib.request` opener to open the specified URL and handle cookies. 5. Saves the cookies to the specified file after the request. 6. Returns the content of the HTTP response. **Specifications:** - Use the `http.cookiejar` and `urllib.request` modules. - If the `cookie_file` does not exist, your function should still perform the HTTP request but start with an empty `CookieJar`. - Ensure that session cookies and expired cookies are correctly managed according to the module\'s standard behavior. - Handle any exceptions that may occur during the loading or saving of cookies. **Constraints:** - The function should work with URLs returning standard HTTP responses. - The function should manage cookies according to standard HTTP cookie directives. **Example Usage:** ```python response_content = fetch_url_with_cookies(\\"http://example.com\\", \\"cookies.txt\\") print(response_content) ``` **Expected Output:** The content of the HTTP response fetched from the specified URL. **Performance Requirements:** The function should efficiently handle the creation, storage, and retrieval of cookies even with a moderate load of HTTP requests. # Implementation ```python import os import http.cookiejar import urllib.request def fetch_url_with_cookies(url, cookie_file): # Create a MozillaCookieJar object to store cookies cj = http.cookiejar.MozillaCookieJar(cookie_file) # Load cookies from the specified file if it exists if os.path.exists(cookie_file): try: cj.load(ignore_discard=True, ignore_expires=True) except http.cookiejar.LoadError as e: print(f\\"Error loading cookies: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\") # Create an opener to handle cookies opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj)) urllib.request.install_opener(opener) # Open the URL and read the response try: response = opener.open(url) content = response.read() except Exception as e: print(f\\"An error occurred while fetching the URL: {e}\\") return None # Save the cookies to the specified file try: cj.save(ignore_discard=True, ignore_expires=True) except Exception as e: print(f\\"An error occurred while saving the cookies: {e}\\") return content.decode(\'utf-8\') # Example usage if __name__ == \\"__main__\\": response_content = fetch_url_with_cookies(\\"http://example.com\\", \\"cookies.txt\\") print(response_content) ``` **Note:** Ensure to replace `http://example.com` with a valid URL for testing.","solution":"import os import http.cookiejar import urllib.request def fetch_url_with_cookies(url, cookie_file): # Create a MozillaCookieJar object to store cookies cj = http.cookiejar.MozillaCookieJar(cookie_file) # Load cookies from the specified file if it exists if os.path.exists(cookie_file): try: cj.load(ignore_discard=True, ignore_expires=True) except http.cookiejar.LoadError as e: print(f\\"Error loading cookies: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\") # Create an opener to handle cookies opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj)) urllib.request.install_opener(opener) # Open the URL and read the response try: response = opener.open(url) content = response.read() except Exception as e: print(f\\"An error occurred while fetching the URL: {e}\\") return None # Save the cookies to the specified file try: cj.save(ignore_discard=True, ignore_expires=True) except Exception as e: print(f\\"An error occurred while saving the cookies: {e}\\") return content.decode(\'utf-8\')"},{"question":"You are required to implement a function that computes the Jacobian matrix of a given function with respect to its inputs and then uses this Jacobian to find the Hessian matrix. The function to be differentiated will take a single PyTorch Tensor as input and return a Tensor. The Jacobian matrix is a matrix of all first-order partial derivatives of a vector-valued function. The Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function. # Requirements 1. **Input**: - A function `f` that takes a single input of type `torch.Tensor` and returns a `torch.Tensor`. - A tensor `x` of type `torch.Tensor` with `requires_grad=True` that represents the point at which the Jacobian and Hessian are to be evaluated. 2. **Output**: - The Jacobian matrix of `f` evaluated at `x`. - The Hessian matrix of `f` evaluated at `x`. # Constraints - You must use PyTorch\'s autograd package to compute the Jacobian and Hessian matrices. - The function `f` can return either a scalar or a vector. If `f` returns a vector, the Hessian should be computed for each element of the output, resulting in a 3-dimensional tensor. - The solution should efficiently handle reasonably sized inputs, with `x` having dimensions up to `(100, )` or `(10, 10)`. # Function Signature ```python import torch def compute_jacobian_and_hessian(f, x): Compute the Jacobian and Hessian matrices of the function `f` at the point `x`. Parameters: f (function): A function that takes a single tensor input and returns a tensor. x (torch.Tensor): A tensor input with `requires_grad=True`. Returns: tuple: A tuple containing: - torch.Tensor: The Jacobian matrix of `f` at `x`. - torch.Tensor: The Hessian matrix of `f` at `x`. pass ``` # Example ```python import torch def example_function(x): return x.pow(3).sum() x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) jacobian, hessian = compute_jacobian_and_hessian(example_function, x) print(\\"Jacobian:\\", jacobian) print(\\"Hessian:\\", hessian) ``` # Notes - Ensure that your implementation makes proper use of `torch.autograd.functional.jacobian` and `torch.autograd.functional.hessian` to compute the required derivatives. - Pay attention to the dimensions of the output, especially for the Hessian when `f` returns a non-scalar tensor.","solution":"import torch def compute_jacobian_and_hessian(f, x): Compute the Jacobian and Hessian matrices of the function `f` at the point `x`. Parameters: f (function): A function that takes a single tensor input and returns a tensor. x (torch.Tensor): A tensor input with `requires_grad=True`. Returns: tuple: A tuple containing: - torch.Tensor: The Jacobian matrix of `f` at `x`. - torch.Tensor: The Hessian matrix of `f` at `x`. jacobian = torch.autograd.functional.jacobian(f, x) if jacobian.ndim == 1: # f returns a scalar hessian = torch.autograd.functional.hessian(lambda x: f(x).sum(), x) else: # f returns a vector hessian = torch.stack([torch.autograd.functional.hessian(lambda x: f(x).select(0, i), x) for i in range(jacobian.size(0))]) return jacobian, hessian"},{"question":"WAV File Manipulation and Analysis # Objective Write a Python function that reads an input WAV file, extracts its metadata, modifies the audio data by doubling the amplitude of each sample, and writes the modified audio data to a new WAV file. # Function Signature ```python def process_wav(input_file: str, output_file: str) -> dict: Processes a WAV file by reading its content, extracting metadata, modifying audio data, and writing modified audio to a new file. Parameters: input_file (str): The path to the input WAV file. output_file (str): The path to the output WAV file where modified data will be saved. Returns: dict: A dictionary containing metadata information about the input file. Example: { \'num_channels\': 2, \'sampwidth\': 2, \'framerate\': 44100, \'num_frames\': 100000, \'comptype\': \'NONE\', \'compname\': \'not compressed\' } pass ``` # Input - `input_file`: A string representing the path to the input WAV file. - `output_file`: A string representing the path to the output WAV file. # Output - A dictionary containing the metadata of the input WAV file. The keys of the dictionary are: - `num_channels`: Number of audio channels (e.g., 1 for mono, 2 for stereo). - `sampwidth`: Sample width in bytes. - `framerate`: Sampling frequency (e.g., 44100 for 44.1 kHz). - `num_frames`: Number of audio frames. - `comptype`: Compression type (should be \'NONE\'). - `compname`: Human-readable compression name (should be \'not compressed\'). # Constraints and Assumptions - The input WAV file uses the PCM format (not extensible PCM). - Sample width is either 1, 2, or 4 bytes. - The output WAV file should have the same metadata as the input, except for the modified audio data. - Doubling amplitude might cause clipping if the amplitude exceeds the range specified by the sample width. # Example ```python metadata = process_wav(\\"input.wav\\", \\"output.wav\\") print(metadata) # Output: { # \'num_channels\': 2, # \'sampwidth\': 2, # \'framerate\': 44100, # \'num_frames\': 100000, # \'comptype\': \'NONE\', # \'compname\': \'not compressed\' # } ``` # Instructions 1. Use the `wave` module to open the input WAV file, read its metadata and audio frames. 2. Modify the audio frames by doubling the amplitude of each sample. 3. Write the modified audio data to the output WAV file, maintaining the same metadata. 4. Ensure proper exception handling for file I/O and wave-specific errors.","solution":"import wave import numpy as np def process_wav(input_file: str, output_file: str) -> dict: Processes a WAV file by reading its content, extracting metadata, modifying audio data, and writing modified audio to a new file. Parameters: input_file (str): The path to the input WAV file. output_file (str): The path to the output WAV file where modified data will be saved. Returns: dict: A dictionary containing metadata information about the input file. Example: { \'num_channels\': 2, \'sampwidth\': 2, \'framerate\': 44100, \'num_frames\': 100000, \'comptype\': \'NONE\', \'compname\': \'not compressed\' } with wave.open(input_file, \'rb\') as wf: # Extract metadata metadata = { \'num_channels\': wf.getnchannels(), \'sampwidth\': wf.getsampwidth(), \'framerate\': wf.getframerate(), \'num_frames\': wf.getnframes(), \'comptype\': wf.getcomptype(), \'compname\': wf.getcompname() } # Read the audio data audio_data = wf.readframes(metadata[\'num_frames\']) # Convert audio data to numpy array for manipulation dtype_map = {1: np.int8, 2: np.int16, 4: np.int32} audio_np = np.frombuffer(audio_data, dtype=dtype_map[metadata[\'sampwidth\']]) # Double the amplitude audio_np = audio_np * 2 # Clip the values to valid range to avoid overflow if metadata[\'sampwidth\'] == 1: max_val = 127 min_val = -128 elif metadata[\'sampwidth\'] == 2: max_val = 32767 min_val = -32768 elif metadata[\'sampwidth\'] == 4: max_val = 2147483647 min_val = -2147483648 audio_np = np.clip(audio_np, min_val, max_val) # Convert back to bytes doubled_audio_data = audio_np.tobytes() with wave.open(output_file, \'wb\') as wf_out: # Set parameters for the output file wf_out.setnchannels(metadata[\'num_channels\']) wf_out.setsampwidth(metadata[\'sampwidth\']) wf_out.setframerate(metadata[\'framerate\']) wf_out.setnframes(metadata[\'num_frames\']) wf_out.setcomptype(metadata[\'comptype\'], metadata[\'compname\']) # Write the modified audio data to the output file wf_out.writeframes(doubled_audio_data) return metadata"},{"question":"Augmented Assignment and In-Place Modification Objective: Write a Python class that demonstrates the behavior of augmented assignments and in-place modifications to mutable objects such as lists. Implement a class `ListHandler` with the following methods: 1. `__init__(self, initial_list)`: Initializes the object with a list `initial_list`. 2. `mod_list(self, index, value)`: Modifies the list at the specified `index` by adding the `value` to the existing element using augmented assignment. 3. `replace_list(self, new_list)`: Replaces the entire list with `new_list` using regular assignment. 4. `get_list(self)`: Returns the current state of the list. Requirements: - The `mod_list` method must utilize an augmented assignment operation (`+=`) to modify the list element in place. - The `replace_list` method should replace the entire list attribute with a new list. - Ensure that augmented assignment modifies the list in place, reflecting changes outside the method as well. Constraints: - The list will contain integers. - Indexes provided will always be valid. - New lists provided to the `replace_list` method will always be valid lists of integers. Example: ```python # Example usage of ListHandler class lh = ListHandler([1, 2, 3]) lh.mod_list(1, 5) print(lh.get_list()) # Output: [1, 7, 3] lh.replace_list([4, 5, 6]) print(lh.get_list()) # Output: [4, 5, 6] lh.mod_list(2, 5) print(lh.get_list()) # Output: [4, 5, 11] ``` Additional Task: Explain why the augmented assignment operation within `mod_list` modifies the list in place and how this differs from a regular assignment operation that creates a new object.","solution":"class ListHandler: A class to demonstrate augmented assignment and in-place modification on lists. def __init__(self, initial_list): self.lst = initial_list def mod_list(self, index, value): Modifies the list at the specified index by adding the value to the existing element using augmented assignment. self.lst[index] += value def replace_list(self, new_list): Replaces the entire list with new_list using regular assignment. self.lst = new_list def get_list(self): Returns the current state of the list. return self.lst"},{"question":"**Context:** You are provided with a PyTorch model architecture and an incomplete section where the weights need to be initialized using various initialization methods from the `torch.nn.init` module. **Task:** Complete the function `initialize_weights` to initialize the weights and biases of the given model based on the specified initialization method. **Function Signature:** ```python def initialize_weights(model, init_type=\'xavier_uniform\'): Initialize the weights of a model. Parameters: model (torch.nn.Module): The neural network model. init_type (str): The type of initialization to apply. Supported types are: \'uniform\', \'normal\', \'constant\', \'ones\', \'zeros\', \'eye\', \'dirac\', \'xavier_uniform\', \'xavier_normal\', \'kaiming_uniform\', \'kaiming_normal\', \'trunc_normal\', \'orthogonal\', \'sparse\'. Returns: None. The model\'s weights are initialized in place. pass ``` **Requirements:** 1. Implement the `initialize_weights` function to handle different initialization types. 2. Ensure the function correctly applies the chosen initialization method to all the suitable layers of the model. 3. Handle in-place modifications to the model without returning any value. 4. Provide error handling for unsupported initialization types. **Constraints:** - Only use functions from the `torch.nn.init` module for initialization. - Assume that the model consists of a combination of `torch.nn.Linear` and `torch.nn.Conv2d` layers. **Example Usage:** ```python import torch import torch.nn as nn import torch.nn.init as init class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(64*28*28, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.relu(self.conv2(x)) x = x.view(x.size(0), -1) x = torch.relu(self.fc1(x)) x = self.fc2(x) return x model = SampleModel() initialize_weights(model, \'xavier_normal\') # Check initialized weights print(model.conv1.weight) ``` Write your implementation of the function `initialize_weights` that meets the above criteria.","solution":"import torch import torch.nn as nn import torch.nn.init as init def initialize_weights(model, init_type=\'xavier_uniform\'): Initialize the weights of a model. Parameters: model (torch.nn.Module): The neural network model. init_type (str): The type of initialization to apply. Supported types are: \'uniform\', \'normal\', \'constant\', \'ones\', \'zeros\', \'eye\', \'dirac\', \'xavier_uniform\', \'xavier_normal\', \'kaiming_uniform\', \'kaiming_normal\', \'trunc_normal\', \'orthogonal\', \'sparse\'. Returns: None. The model\'s weights are initialized in place. init_fn_map = { \'uniform\': init.uniform_, \'normal\': init.normal_, \'constant\': init.constant_, \'ones\': init.ones_, \'zeros\': init.zeros_, \'eye\': init.eye_, \'dirac\': init.dirac_, \'xavier_uniform\': init.xavier_uniform_, \'xavier_normal\': init.xavier_normal_, \'kaiming_uniform\': init.kaiming_uniform_, \'kaiming_normal\': init.kaiming_normal_, \'trunc_normal\': init.trunc_normal_, \'orthogonal\': init.orthogonal_, \'sparse\': init.sparse_ } if init_type not in init_fn_map: raise ValueError(f\\"Unsupported initialization type: {init_type}\\") init_fn = init_fn_map[init_type] for layer in model.modules(): if isinstance(layer, (nn.Conv2d, nn.Linear)): init_fn(layer.weight) if layer.bias is not None: init.zeros_(layer.bias)"},{"question":"Advanced Unit Testing with Python\'s `test.support` Objective: Demonstrate the ability to create and run advanced unit tests using Python\'s `unittest` framework and `test.support` module. Problem Statement: You are given the following utility function that calculates the factorial of a non-negative integer. This function is critical and needs rigorous testing to ensure its reliability. Implement a comprehensive test suite to cover various edge cases and scenarios using Python\'s `unittest` framework and utilities from `test.support`. **Utility Function:** ```python def factorial(n): Calculate the factorial of a non-negative integer. if n < 0: raise ValueError(\\"Negative input is not allowed.\\") if n == 0 or n == 1: return 1 result = 1 for i in range(2, n+1): result *= i return result ``` Requirements: 1. Implement a test suite in a class named `TestFactorial` using Python\'s `unittest` framework. 2. Ensure that your test suite includes the following: - Tests for typical input values (e.g., `5`, `10`). - Tests for edge cases (e.g., `0`, `1`). - Tests for invalid input values (e.g., negative numbers). 3. Use context managers or decorators from `test.support` to handle specific conditions: - Capture the standard output if needed. - Ensure that invalid inputs raise the correct exceptions. - Use the `gc_collect()` function to ensure garbage collection if needed. - You may use `swap_attr` or similar context managers if any attribute swapping is necessary. 4. Ensure your test suite is clean and does not have unhandled side effects after running (e.g., no leftover open files or connections). 5. Verify your test cases provide adequate coverage for the `factorial` function. # Input and Output: - **Input:** No direct input required from the user. The test cases should invoke the `factorial` function with different parameters. - **Output:** The results from the `unittest` framework indicating the success or failure of each test case. # Example: Here\'s an example of one of the test cases: ```python import unittest from test import support class TestFactorial(unittest.TestCase): def test_valid_input(self): self.assertEqual(factorial(5), 120) self.assertEqual(factorial(10), 3628800) def test_edge_cases(self): self.assertEqual(factorial(0), 1) self.assertEqual(factorial(1), 1) def test_invalid_input(self): with self.assertRaises(ValueError): factorial(-5) def test_large_input(self): with support.adjust_int_max_str_digits(100000): self.assertEqual(factorial(20), 2432902008176640000) if __name__ == \'__main__\': unittest.main() ``` Constraints: - The function `factorial(n)` should not be modified. - Ensure that your tests run efficiently and clean up after they finish. - Use the provided utilities from `test.support` module wherever applicable to enhance your tests. Performance Requirements: - Tests should complete within a reasonable time frame, ensuring no infinite loops or unreasonably large computations within the constraints of typical usage scenarios.","solution":"def factorial(n): Calculate the factorial of a non-negative integer. if n < 0: raise ValueError(\\"Negative input is not allowed.\\") if n == 0 or n == 1: return 1 result = 1 for i in range(2, n + 1): result *= i return result"},{"question":"Objective Create a function that dynamically imports a module, executes a function within that module, and manages the module\'s state in the import system. Problem Statement Implement a function `dynamic_module_importer` that: 1. **Imports** a Python module given its name. 2. **Executes** a specified function within the imported module. 3. **Manages** the module\'s reference properly within the `sys.modules` dictionary to avoid potential issues with incomplete module objects. The function should take the following input parameters: - `module_name` (str): The name of the module to import. - `function_name` (str): The name of the function to execute within the imported module. - `args` (list): A list of arguments to pass to the function. - `kwargs` (dict): A dictionary of keyword arguments to pass to the function. The function should return the result of the executed function if the function exists, or raise an appropriate exception if the module or function cannot be imported or executed. Input - `module_name`: a string representing the name of the module to be imported. - `function_name`: a string representing the name of the function to be executed within the module. - `args`: a list of arguments to be passed to the function. - `kwargs`: a dictionary of keyword arguments to be passed to the function. Output - Return the result of the function execution, or raise an appropriate exception. Constraints - You may assume that the module and function names provided by the user are valid Python identifiers. - The function should handle any exceptions that occur during the import and execution process gracefully. Example ```python def dynamic_module_importer(module_name, function_name, args=[], kwargs={}): # Your implementation here result = dynamic_module_importer(\\"math\\", \\"sqrt\\", [16]) print(result) # Should output: 4.0 result = dynamic_module_importer(\\"os\\", \\"getcwd\\") print(result) # Should output the current working directory as a string ``` **Note**: Your implementation should achieve the following: 1. Import the module dynamically using the appropriate function from the documentation. 2. Retrieve and execute the specified function with the given arguments. 3. Properly handle and manage the module\'s reference within `sys.modules` to avoid any issues with incomplete module objects.","solution":"import sys import importlib def dynamic_module_importer(module_name, function_name, args=[], kwargs={}): Dynamically imports a module, executes a specified function within that module, and returns the result of the function execution. Args: - module_name (str): The name of the module to import. - function_name (str): The name of the function to execute within the imported module. - args (list): A list of arguments to pass to the function. - kwargs (dict): A dictionary of keyword arguments to pass to the function. Returns: - The result of the function execution. Raises: - ModuleNotFoundError: If the module cannot be imported. - AttributeError: If the function does not exist within the imported module. try: module = importlib.import_module(module_name) if not hasattr(module, function_name): raise AttributeError(f\\"The module \'{module_name}\' does not have a function named \'{function_name}\'\\") function = getattr(module, function_name) result = function(*args, **kwargs) return result except ModuleNotFoundError as e: raise e except AttributeError as e: raise e"},{"question":"**Problem Statement: Creating and Using a Custom Operation in PyTorch** In this task, you need to demonstrate your understanding of PyTorch\'s custom operators by implementing a custom mathematical operation. You will create a custom PyTorch module representing a ‘Normalized Squared Difference’ operation, implement its forward and backward passes, and apply this custom operation to a tensor. # Objective - Define a custom PyTorch module `NormalizedSquaredDifference`: - This module receives a tensor `x` and computes the normalized squared difference of each element in `x`. - Normalized Squared Difference ( NSD(x) ) is defined as: [ NSD(x_i) = frac{(x_i - mu)^2}{sigma^2} ] Where (mu) is the mean and (sigma^2) is the variance of the tensor `x`. # Requirements 1. **Custom Module Implementation**: - Implement the `NormalizedSquaredDifference` class which should subclass `torch.autograd.Function`. - Define the `forward` method to compute the normalized squared difference. - Define the `backward` method to compute the gradients necessary for backpropagation. 2. **Integration and Testing**: - Initialize a tensor `x` with values `[1.0, 2.0, 3.0, 4.0, 5.0]`. - Apply the custom module to this tensor. - Verify the implementation by checking the output and its gradients. # Input and Output Formats - **Input**: - A 1-dimensional tensor `x` of arbitrary length with floating point values. - **Output**: - A tensor of the same shape as `x` representing the normalized squared differences. - Prints the output tensor and the gradients. # Constraints - Do not use other external libraries, except for `torch` and `numpy`. # Sample Code Template ```python import torch from torch.autograd import Function import numpy as np class NormalizedSquaredDifference(Function): @staticmethod def forward(ctx, x): # Compute the mean and variance mean = x.mean() var = x.var(unbiased=False) # Save context for backward pass ctx.save_for_backward(x, mean, var) # Compute the normalized squared difference nsd = (x - mean) ** 2 / var return nsd @staticmethod def backward(ctx, grad_output): x, mean, var = ctx.saved_tensors # Compute the gradients for input tensor dx = (2 * (x - mean) / var) * grad_output return dx # Testing the custom operation x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], requires_grad=True) nsd = NormalizedSquaredDifference.apply(x) print(\'Output:\', nsd) # Compute the loss as sum of output loss = nsd.sum() # Backward pass to compute gradients loss.backward() print(\'Gradients:\', x.grad) ``` Make sure your implementation matches the expected output and preserves gradients correctly through the backward pass.","solution":"import torch from torch.autograd import Function class NormalizedSquaredDifference(Function): @staticmethod def forward(ctx, x): # Compute the mean and variance mean = x.mean() var = x.var(unbiased=False) # Save context for backward pass ctx.save_for_backward(x, mean, var) # Compute the normalized squared difference nsd = (x - mean) ** 2 / var return nsd @staticmethod def backward(ctx, grad_output): x, mean, var = ctx.saved_tensors # Compute the gradients for the input tensor dx = (2 * (x - mean) / var) * grad_output return dx # Testing the custom operation if __name__ == \\"__main__\\": x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], requires_grad=True) nsd = NormalizedSquaredDifference.apply(x) print(\'Output:\', nsd) # Compute the loss as the sum of the output loss = nsd.sum() # Backward pass to compute gradients loss.backward() print(\'Gradients:\', x.grad)"},{"question":"**Objective:** Implement a function that reads and processes specific lines from a file using the `linecache` module, respecting cache states and handling any potential errors. **Function Signature:** ```python def process_lines_from_file(filename: str, line_numbers: list, clear_cache: bool = False) -> list: pass ``` **Inputs:** - `filename` (str): The name of the file from which lines will be read. - `line_numbers` (list): A list of integers representing the line numbers to be read from the file. - `clear_cache` (bool): A boolean flag indicating whether to clear the linecache after processing lines. **Outputs:** - `return` (list): A list of strings containing the content of each requested line. If a line does not exist, include an empty string for that line. **Constraints:** - The line numbers in `line_numbers` are 1-indexed. - If `clear_cache` is True, ensure that `linecache`\'s cache is cleared after processing the lines. - Handle non-existent files gracefully by returning an appropriate message for each line, maintaining the position of non-existing lines in the return list. - The solution should demonstrate effective use of the `linecache` module functions: `getline`, `clearcache`, and `checkcache`. **Example:** ```python # Assume \'example.txt\' contains the following lines: # 1: Hello, World! # 2: This is a test file. # 3: Let\'s verify linecache. # 4: Python 310 is amazing! filename = \'example.txt\' line_numbers = [1, 3, 5] clear_cache = True print(process_lines_from_file(filename, line_numbers, clear_cache)) # Should output: # [\'Hello, World!n\', \\"Let\'s verify linecache.n\\", \'\'] ``` **Notes:** - Consider edge cases such as empty files, files without the specified lines, and incorrect file paths. - Use exception handling to manage and log any errors encountered during file access. - Demonstrate understanding of cache management using `linecache`. You are allowed to use any standard Python libraries required to implement this function.","solution":"import linecache def process_lines_from_file(filename: str, line_numbers: list, clear_cache: bool = False) -> list: Reads specific lines from a file using the linecache module. Args: - filename (str): The name of the file from which lines will be read. - line_numbers (list): A list of integers representing the line numbers to be read from the file. - clear_cache (bool): A boolean flag indicating whether to clear the linecache after processing lines. Returns: - list: A list of strings containing the content of each requested line. If a line does not exist, include an empty string for that line. result = [] for line_number in line_numbers: try: line = linecache.getline(filename, line_number) if not line: result.append(\'\') else: result.append(line) except Exception as e: result.append(\'\') if clear_cache: linecache.clearcache() return result"},{"question":"# Question: Unsupervised Dimensionality Reduction with Scikit-Learn You are given a high-dimensional dataset, and your task is to implement a function that reduces its dimensionality using Principal Component Analysis (PCA) and Feature Agglomeration. The function should first apply PCA to capture the majority of the variance in the data and then use Feature Agglomeration to group together features that behave similarly. Function Signature ```python def reduce_dimensionality(data: np.ndarray, n_components_pca: int, n_clusters: int) -> np.ndarray: Reduces the dimensionality of the given high-dimensional dataset using PCA and Feature Agglomeration. Parameters: - data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the high-dimensional dataset. - n_components_pca (int): The number of principal components to keep after applying PCA. - n_clusters (int): The number of clusters to form when applying Feature Agglomeration. Returns: - np.ndarray: A 2D numpy array of shape (n_samples, n_clusters) representing the reduced-dimensionality dataset. ``` Input - `data`: A 2D numpy array of shape (n_samples, n_features), where each row represents a sample and each column represents a feature. - `n_components_pca`: An integer specifying the number of principal components to retain after applying PCA. - `n_clusters`: An integer specifying the number of clusters to form for Feature Agglomeration. Output - A 2D numpy array of shape (n_samples, n_clusters), representing the dataset after dimensionality reduction. Constraints - You must use the PCA and Feature Agglomeration classes from the `sklearn` package. - You should ensure that the data is appropriately scaled before applying Feature Agglomeration. Example ```python import numpy as np # Example Data data = np.array([[1.2, 3.5, 4.2, 0.5], [3.2, 5.1, 0.3, 7.8], [4.8, 8.2, 6.3, 1.0], [9.0, 1.1, 5.9, 2.3]]) # Example usage of the function reduced_data = reduce_dimensionality(data, n_components_pca=2, n_clusters=2) print(reduced_data) ``` The reduced data should have shape `(n_samples, n_clusters)`. Note that the specific values in the reduced data array will depend on the internal calculations of the PCA and Feature Agglomeration mechanisms.","solution":"import numpy as np from sklearn.decomposition import PCA from sklearn.cluster import FeatureAgglomeration from sklearn.preprocessing import StandardScaler def reduce_dimensionality(data: np.ndarray, n_components_pca: int, n_clusters: int) -> np.ndarray: Reduces the dimensionality of the given high-dimensional dataset using PCA and Feature Agglomeration. Parameters: - data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the high-dimensional dataset. - n_components_pca (int): The number of principal components to keep after applying PCA. - n_clusters (int): The number of clusters to form when applying Feature Agglomeration. Returns: - np.ndarray: A 2D numpy array of shape (n_samples, n_clusters) representing the reduced-dimensionality dataset. # Step 1: Apply PCA to reduce the number of components pca = PCA(n_components=n_components_pca) pca_result = pca.fit_transform(data) # Step 2: Standardize the PCA result before Feature Agglomeration scaler = StandardScaler() pca_result_scaled = scaler.fit_transform(pca_result) # Step 3: Apply Feature Agglomeration feature_agglomeration = FeatureAgglomeration(n_clusters=n_clusters) reduced_data = feature_agglomeration.fit_transform(pca_result_scaled) return reduced_data"},{"question":"**Asyncio Programming Assessment** # Objective Create a Python script that demonstrates the use of the asyncio library to manage multiple asynchronous tasks. Your code should include proper handling of coroutines, use of debug mode, logging, and ensuring no coroutines or exceptions are left unhandled. # Task Implement an asynchronous file download manager that downloads files concurrently. Each download operation should run in a separate task, and the results should be logged. If a download fails, the exception should be logged. Use asyncio\'s debug mode to ensure all coroutines are awaited correctly and no exceptions are left unhandled. # Requirements 1. You must use the asyncio library to: - Create and manage an event loop. - Schedule multiple asynchronous tasks/coroutines. - Handle and log any exceptions in the tasks. - Enable debug mode and logging to track coroutine execution and identify unhandled exceptions. 2. Your program should: - Accept a list of URLs to download. - Download the content of each URL concurrently. - Properly handle and log any networking or HTTP errors that occur during the download process. - Ensure the downloaded content is saved to a local file. - Document the location of any files saved. # Input - List of URLs (strings). - Directory to save the downloaded files. # Output - Log messages indicating the start, success, or failure of each download. - Files saved to the specified directory. - Debug messages showing the detection of any never-awaited coroutines or unhandled exceptions. # Constraints - Use Python 3.10’s asyncio package. - Each URL\'s download must run as an individual coroutine. - The number of concurrent downloads should be limited (e.g., to not more than 5 at a time). # Performance - Ensure that the script does not block on any single I/O operation, allowing other tasks to continue. - Properly utilize asyncio\'s concurrency capabilities to maximize efficiency while adhering to the limit on concurrent downloads. # Example Here’s a simple structure to get you started. You should fill in the details: ```python import asyncio import logging import aiohttp # Enable debug mode and configure logging logging.basicConfig(level=logging.DEBUG) logging.getLogger(\'asyncio\').setLevel(logging.DEBUG) async def download_file(session, url, save_dir): try: async with session.get(url) as response: response.raise_for_status() content = await response.read() # Save the content to a file file_path = f\\"{save_dir}/{url.split(\'/\')[-1]}\\" with open(file_path, \'wb\') as f: f.write(content) logging.info(f\\"Downloaded {url} to {file_path}\\") except Exception as e: logging.error(f\\"Error downloading {url}: {e}\\") async def main(urls, save_dir): async with aiohttp.ClientSession() as session: tasks = [download_file(session, url, save_dir) for url in urls] await asyncio.gather(*tasks) if __name__ == \\"__main__\\": urls = [\\"http://example.com/file1.txt\\", \\"http://example.com/file2.txt\\", ...] save_dir = \\"./downloads\\" asyncio.run(main(urls, save_dir), debug=True) ``` Explain any design choices you make and ensure all the requirements are met. Good luck!","solution":"import asyncio import logging from aiohttp import ClientSession, ClientError import os # Enable debug mode and configure logging logging.basicConfig(level=logging.DEBUG) logging.getLogger(\'asyncio\').setLevel(logging.DEBUG) CONCURRENT_DOWNLOADS = 5 async def download_file(session, url, save_dir): try: async with session.get(url) as response: response.raise_for_status() content = await response.read() file_name = os.path.join(save_dir, os.path.basename(url)) with open(file_name, \'wb\') as file: file.write(content) logging.info(f\\"Downloaded {url}\\") except ClientError as e: logging.error(f\\"Error downloading {url}: {e}\\") async def main(urls, save_dir): if not os.path.exists(save_dir): os.makedirs(save_dir) async with ClientSession() as session: sem = asyncio.Semaphore(CONCURRENT_DOWNLOADS) async def bound_download_file(url): async with sem: await download_file(session, url, save_dir) tasks = [bound_download_file(url) for url in urls] await asyncio.gather(*tasks) if __name__ == \\"__main__\\": urls = [\\"http://example.com/file1.txt\\", \\"http://example.com/file2.txt\\"] save_dir = \\"./downloads\\" asyncio.run(main(urls, save_dir), debug=True)"},{"question":"# PyCapsule Manipulation in Python You are provided with several C API functions to create and manipulate PyCapsule objects in Python. Implement the following functions in Python that would simulate the behavior of some of these C API functions while maintaining the encapsulated pointer\'s integrity. Function 1: `create_capsule` ```python def create_capsule(pointer: int, name: str, destructor: callable = None) -> dict: Create a PyCapsule encapsulating the pointer. Parameters: pointer (int): An integer representing the memory address (simulated pointer). name (str): A string name for the capsule. destructor (callable, optional): A destructor function to be called upon capsule deletion. Returns: dict: A dictionary representing the PyCapsule (simulated). pass ``` Function 2: `is_capsule_valid` ```python def is_capsule_valid(capsule: dict, name: str) -> bool: Check if the capsule is valid and its internal name matches the provided name. Parameters: capsule (dict): A dictionary representing the PyCapsule. name (str): The name to match against the capsule’s internal name. Returns: bool: True if the capsule is valid and the names match, False otherwise. pass ``` Function 3: `get_capsule_pointer` ```python def get_capsule_pointer(capsule: dict, name: str) -> int: Retrieve the pointer stored in the capsule. Parameters: capsule (dict): A dictionary representing the PyCapsule. name (str): The name to match against the capsule\'s internal name. Returns: int: The pointer if names match and capsule is valid, raises an exception otherwise. pass ``` Function 4: `set_capsule_name` ```python def set_capsule_name(capsule: dict, name: str) -> None: Set the name inside the capsule to a new name. Parameters: capsule (dict): A dictionary representing the PyCapsule. name (str): The new name to set inside the capsule. pass ``` Constraints - Do not use actual memory management or pointer arithmetic. Use integers to simulate memory addresses. - Ensure that the internal state of the capsule is maintained accurately. - Implement appropriate error handling using exceptions. Example Usage ```python # Create a capsule capsule = create_capsule(123456789, \\"module.attribute\\") # Validate the capsule (should be True) print(is_capsule_valid(capsule, \\"module.attribute\\")) # Retrieve the pointer (should be 123456789) print(get_capsule_pointer(capsule, \\"module.attribute\\")) # Set a new capsule name set_capsule_name(capsule, \\"module.new_attribute\\") # Validate the capsule with new name (should be True) print(is_capsule_valid(capsule, \\"module.new_attribute\\")) # Attempt to retrieve pointer with old name (should raise an exception) try: print(get_capsule_pointer(capsule, \\"module.attribute\\")) except Exception as e: print(e) ``` Each function should be meticulously implemented to mimic the behavior as described in the respective C API functions from the PyCapsule documentation while being tested with the provided example.","solution":"def create_capsule(pointer: int, name: str, destructor: callable = None) -> dict: Create a PyCapsule encapsulating the pointer. Parameters: pointer (int): An integer representing the memory address (simulated pointer). name (str): A string name for the capsule. destructor (callable, optional): A destructor function to be called upon capsule deletion. Returns: dict: A dictionary representing the PyCapsule (simulated). capsule = { \\"pointer\\": pointer, \\"name\\": name, \\"destructor\\": destructor } return capsule def is_capsule_valid(capsule: dict, name: str) -> bool: Check if the capsule is valid and its internal name matches the provided name. Parameters: capsule (dict): A dictionary representing the PyCapsule. name (str): The name to match against the capsule’s internal name. Returns: bool: True if the capsule is valid and the names match, False otherwise. return capsule.get(\\"name\\") == name def get_capsule_pointer(capsule: dict, name: str) -> int: Retrieve the pointer stored in the capsule. Parameters: capsule (dict): A dictionary representing the PyCapsule. name (str): The name to match against the capsule\'s internal name. Returns: int: The pointer if names match and capsule is valid, raises an exception otherwise. if is_capsule_valid(capsule, name): return capsule[\\"pointer\\"] else: raise ValueError(\\"Capsule is invalid or names do not match\\") def set_capsule_name(capsule: dict, name: str) -> None: Set the name inside the capsule to a new name. Parameters: capsule (dict): A dictionary representing the PyCapsule. name (str): The new name to set inside the capsule. capsule[\\"name\\"] = name"},{"question":"# Question: Measuring Code Performance with `timeit` You are given a list of various Python functions, and you need to measure their execution times to determine their performance efficiency. Implement a Python function `compare_execution_times` that takes a list of function definitions as input and returns the execution times of these functions. Function Signature ```python def compare_execution_times(functions: List[Callable[[], None]], number: int = 100000) -> Dict[str, float]: pass ``` Input - `functions`: A list of function objects. Each function takes no arguments and returns nothing. - `number`: An integer specifying the number of times each function should be executed for timing. Default is 100,000. Output A dictionary where the keys are function names (as strings) and the values are the execution times (in seconds) for the given number of executions. Constraints - Each function in the input list will have a unique name. - The `timeit` module must be used to measure execution times. - The function should handle cases where the execution time is minimal by providing higher precision. # Example ```python from typing import Callable, List, Dict import time def func1(): for i in range(100): pass def func2(): for i in range(200): pass def func3(): sum([i for i in range(50)]) functions = [func1, func2, func3] result = compare_execution_times(functions, number=100000) print(result) ``` Expected output (example): ```python { \'func1\': 0.078, \'func2\': 0.157, \'func3\': 0.159 } ``` In this example, `func2` and `func3` have slightly larger execution times than `func1` due to the increased complexity of their operations. # Notes - Use the `timeit` module\'s callable interface for precision. - Ensure the result dictionary contains precise execution times with up to three decimal places.","solution":"from typing import Callable, List, Dict import timeit def compare_execution_times(functions: List[Callable[[], None]], number: int = 100000) -> Dict[str, float]: execution_times = {} for func in functions: # Using timeit to measure the time taken for a single function call, and then multiplying by the number of calls time_taken = timeit.timeit(func, number=number) execution_times[func.__name__] = round(time_taken, 3) return execution_times"},{"question":"Working with Copy-on-Write in pandas As of pandas version 3.0, the Copy-on-Write (CoW) mechanism is the default and only mode. This affects how you can safely manipulate data within DataFrame and Series objects to avoid unintended side effects and improve memory usage. Your task is to implement a function that takes in a DataFrame and performs a series of operations demonstrating proper CoW usage. Specifically, your function should: 1. Create a DataFrame from a dictionary. 2. Perform a data modification using `.loc` without causing side effects. 3. Demonstrate that the original DataFrame remains unchanged after modifications made to a derived DataFrame. 4. Handle a scenario where the underlying NumPy array needs to be modified. # Input - A dictionary of data. ```python { \\"foo\\": [1, 2, 3, 4], \\"bar\\": [5, 6, 7, 8] } ``` # Output - A modified DataFrame with specific values changed, and verification that the original DataFrame remains unchanged. # Constraints - Pandas version 3.0 or higher must be used. - Original DataFrame should not be altered by modifications made to derived DataFrame. # Function Signature ```python import pandas as pd def co_w_safe_modifications(data: dict) -> pd.DataFrame: # Your code here pass ``` # Example ```python def co_w_safe_modifications(data): # Step 1: Create a DataFrame df = pd.DataFrame(data) # Step 2: Create a derived DataFrame (e.g., using .loc to avoid side effects) derived_df = df.loc[:, :] # Step 3: Modify the derived DataFrame derived_df.loc[derived_df[\\"bar\\"] > 6, \\"foo\\"] = 100 # Step 4: Verify original DataFrame remains unchanged assert df[\\"foo\\"].tolist() == [1, 2, 3, 4], \\"Original DataFrame should remain unchanged\\" # Step 5: Modify the underlying NumPy array numpy_arr = derived_df.to_numpy() numpy_arr.flags.writeable = True numpy_arr[0, 0] = 999 return derived_df # Test the function data = { \\"foo\\": [1, 2, 3, 4], \\"bar\\": [5, 6, 7, 8] } resulting_df = co_w_safe_modifications(data) print(resulting_df) ``` Expected Output: ``` foo bar 0 999 5 1 2 6 2 100 7 3 100 8 ``` Note: Ensure the above steps adhere to CoW principles by performing assignments directly on the derived objects without affecting the original DataFrame.","solution":"import pandas as pd def co_w_safe_modifications(data: dict) -> pd.DataFrame: Perform Copy-on-Write safe modifications on a DataFrame. Parameters: data (dict): Input dictionary containing initial data for the DataFrame. Returns: pd.DataFrame: Modified DataFrame with safeguards to ensure the original DataFrame remains unchanged. # Step 1: Create a DataFrame from the given dictionary df = pd.DataFrame(data) # Step 2: Create a derived DataFrame using copy to ensure no side effects on original derived_df = df.copy() # Step 3: Modify the derived DataFrame derived_df.loc[derived_df[\\"bar\\"] > 6, \\"foo\\"] = 100 # Step 4: Verify original DataFrame remains unchanged assert df[\\"foo\\"].tolist() == [1, 2, 3, 4], \\"Original DataFrame should remain unchanged\\" # Step 5: Modify the underlying NumPy array of the derived DataFrame numpy_arr = derived_df.to_numpy() numpy_arr.flags.writeable = True numpy_arr[0, 0] = 999 # Change first element in \'foo\' column return derived_df"},{"question":"# Financial Calculation with Decimal Arithmetic You are tasked with implementing a function to manage a bank account\'s transactions. This bank account should handle deposits, withdrawals, interest application, and balance checking using the `decimal` module to ensure precision in financial calculations. Function Signature ```python from decimal import Decimal, getcontext, Context, ROUND_HALF_UP def bank_account_transactions(transactions: list) -> Decimal: Computes the final account balance after performing a series of transactions. Args: transactions (list): A list of transactions where each transaction is a tuple of the form: (\\"deposit\\" or \\"withdraw\\" or \\"interest\\" or \\"balance\\", Decimal value) Returns: Decimal: The final balance in the account. Raises: ValueError: for invalid transaction types or negative values for deposits and withdrawals. ``` Requirements 1. **Initial Balance**: The account starts with an initial balance of `Decimal(\'0.0\')`. 2. **Transactions**: Transactions are provided as a list of tuples. Each tuple contains: - A string `\\"deposit\\"`, `\\"withdraw\\"`, `\\"interest\\"`, or `\\"balance\\"`. - A `Decimal` representing the transaction amount or interest rate (for \\"interest\\"). 3. **Transaction Types**: - `\\"deposit\\"`: Adds the given amount to the account balance. - `\\"withdraw\\"`: Subtracts the given amount from the account balance. Cannot withdraw more than the current balance. - `\\"interest\\"`: Apply the given interest rate (in percentage) to the current balance. - `\\"balance\\"`: Skips the transaction, shows checkpoint for possible balance query. 4. **Precision and Rounding**: - Set the precision to 7 decimal places using `getcontext().prec = 7`. - Use `ROUND_HALF_UP` for rounding. 5. **Exceptions**: - Raise `ValueError` if an invalid transaction type is encountered. - Raise `ValueError` if a deposit or withdrawal amount is negative. 6. **Handling Special Cases**: - Any operation resulting in a negative balance should be ignored and raise an exception. - Interest application should compound the balance correctly. Example ```python from decimal import Decimal print(bank_account_transactions([ (\\"deposit\\", Decimal(\'100.00\')), (\\"withdraw\\", Decimal(\'30.00\')), (\\"interest\\", Decimal(\'5.00\')), # Apply 5% interest (\\"balance\\", Decimal(\'0.0\')) ])) # Output should be Decimal(\'73.50\') ```","solution":"from decimal import Decimal, getcontext, ROUND_HALF_UP def bank_account_transactions(transactions: list) -> Decimal: Computes the final account balance after performing a series of transactions. Args: transactions (list): A list of transactions where each transaction is a tuple of the form: (\\"deposit\\" or \\"withdraw\\" or \\"interest\\" or \\"balance\\", Decimal value) Returns: Decimal: The final balance in the account. Raises: ValueError: for invalid transaction types or negative values for deposits and withdrawals. # Set the precision and rounding getcontext().prec = 7 getcontext().rounding = ROUND_HALF_UP # Initialize the account balance balance = Decimal(\'0.0\') # Process each transaction for transaction in transactions: txn_type, amount = transaction # Validate the transaction type if txn_type not in [\\"deposit\\", \\"withdraw\\", \\"interest\\", \\"balance\\"]: raise ValueError(f\\"Invalid transaction type: {txn_type}\\") # Process each transaction type if txn_type == \\"deposit\\": if amount < 0: raise ValueError(\\"Deposit amount cannot be negative\\") balance += amount elif txn_type == \\"withdraw\\": if amount < 0: raise ValueError(\\"Withdrawal amount cannot be negative\\") if balance < amount: raise ValueError(\\"Insufficient funds for withdrawal\\") balance -= amount elif txn_type == \\"interest\\": interest_amount = (balance * amount) / Decimal(\'100\') balance += interest_amount return balance"},{"question":"# Task You are provided with three `DataFrame` objects containing sales data from different regions and the respective targets set for those regions. Your task is to: 1. Combine these datasets logically to get a comprehensive view of the sales and targets. 2. Identify and highlight discrepancies between actual sales and targets. 3. Summarize and output a report detailing the differences. # Input You have three dictionaries containing `DataFrame` data: ```python data_north = { \\"region\\": [\\"North\\", \\"North\\", \\"North\\"], \\"product\\": [\\"A\\", \\"B\\", \\"C\\"], \\"sales\\": [150, 200, 300], \\"target\\": [180, 210, 320] } data_south = { \\"region\\": [\\"South\\", \\"South\\"], \\"product\\": [\\"A\\", \\"B\\"], \\"sales\\": [100, 230], \\"target\\": [120, 210] } data_east = { \\"region\\": [\\"East\\", \\"East\\"], \\"product\\": [\\"B\\", \\"C\\"], \\"sales\\": [200, 340], \\"target\\": [190, 310] } ``` Convert these dictionaries into `DataFrame` objects named `df_north`, `df_south`, and `df_east`. # Output 1. Combine the three `DataFrame` objects into a single `DataFrame` named `df_all`. 2. Identify discrepancies where sales did not meet targets and create a `DataFrame` named `df_discrepancy` with the columns `region`, `product`, `sales`, `target`, and `difference` where `difference` is calculated as `sales - target`, and keep only the rows where there is a discrepancy. 3. Summarize by region and product, reporting the total sales, total targets, and the number of discrepancies. # Implementation Implement the function `analyze_sales(data_north, data_south, data_east)` that performs the tasks described above and returns a tuple `(df_all, df_discrepancy, summary)` where: - `df_all` is the combined `DataFrame`. - `df_discrepancy` is the `DataFrame` with identified discrepancies. - `summary` is a dictionary summarizing the information as described. # Constraints - Assume that the length of the data in each dictionary is at most 1000. - Each dictionary contains valid sales data. - Time complexity should be considered if operations need to scale. **Example:** ```python def analyze_sales(data_north, data_south, data_east): # Your code here data_north = { \\"region\\": [\\"North\\", \\"North\\", \\"North\\"], \\"product\\": [\\"A\\", \\"B\\", \\"C\\"], \\"sales\\": [150, 200, 300], \\"target\\": [180, 210, 320] } data_south = { \\"region\\": [\\"South\\", \\"South\\"], \\"product\\": [\\"A\\", \\"B\\"], \\"sales\\": [100, 230], \\"target\\": [120, 210] } data_east = { \\"region\\": [\\"East\\", \\"East\\"], \\"product\\": [\\"B\\", \\"C\\"], \\"sales\\": [200, 340], \\"target\\": [190, 310] } df_all, df_discrepancy, summary = analyze_sales(data_north, data_south, data_east) # Example of the outputs print(df_all) print(df_discrepancy) print(summary) ``` **Note**: Ensure that the combined `DataFrame` retains the order of regions and products as given.","solution":"import pandas as pd def analyze_sales(data_north, data_south, data_east): df_north = pd.DataFrame(data_north) df_south = pd.DataFrame(data_south) df_east = pd.DataFrame(data_east) df_all = pd.concat([df_north, df_south, df_east], ignore_index=True) df_all[\'difference\'] = df_all[\'sales\'] - df_all[\'target\'] df_discrepancy = df_all[df_all[\'difference\'] != 0] summary = df_all.groupby([\'region\', \'product\']).agg( total_sales=(\'sales\', \'sum\'), total_target=(\'target\', \'sum\'), discrepancy_count=(\'difference\', lambda x: (x != 0).sum()) ).reset_index().to_dict(orient=\'list\') return df_all, df_discrepancy, summary # Example Data data_north = { \\"region\\": [\\"North\\", \\"North\\", \\"North\\"], \\"product\\": [\\"A\\", \\"B\\", \\"C\\"], \\"sales\\": [150, 200, 300], \\"target\\": [180, 210, 320] } data_south = { \\"region\\": [\\"South\\", \\"South\\"], \\"product\\": [\\"A\\", \\"B\\"], \\"sales\\": [100, 230], \\"target\\": [120, 210] } data_east = { \\"region\\": [\\"East\\", \\"East\\"], \\"product\\": [\\"B\\", \\"C\\"], \\"sales\\": [200, 340], \\"target\\": [190, 310] } df_all, df_discrepancy, summary = analyze_sales(data_north, data_south, data_east) print(df_all) print(df_discrepancy) print(summary)"},{"question":"Objective: Use `seaborn.objects` to create a plot that demonstrates the use of multiple methods and parameters for handling data visualization, focusing on avoiding overlapping elements and customizing the plot with different semantic variables. Description: You are given a dataset `tips` which contains information about the total bill, tip, gender, smoking status, day, time, and size of the party. Your task is to create a bar plot that visualizes the total bill for different days, separated by gender and smoking status. The bars should be grouped by gender and dodged by smoking status. Additionally, ensure that there is adequate spacing between dodged bars to avoid overlap. Requirements: 1. Load the `tips` dataset from seaborn. 2. Create a bar plot using `seaborn.objects`. 3. Plot the total bill grouped by day and colored by gender. 4. Ensure the bars are dodged by smoking status with a small gap between them. 5. Handle any potential issues with empty spaces or overlapping elements appropriately. Expected Input and Output: - **Input:** No input required; use the provided `tips` dataset. - **Output:** A matplotlib plot displayed with the specified customization. Constraints: - Use only the `seaborn.objects` module for plotting. - Ensure the plot is clear and informative, avoiding overlapping of elements. Example Code: The following example illustrates the expected approach but should be modified to meet the full requirements of the task: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Create and customize the plot p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) # Dodge the bars by smoking status and handle empty spaces p.add(so.Bar(), so.Dodge(by=[\\"smoker\\"], empty=\\"fill\\")) # Display the plot p.show() ``` In the above code, you need to: 1. Integrate multiple semantic variables (`sex` and `smoker`). 2. Customize the gap and handle empty spaces as specified. Submit your complete code that generates the plot as described.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_total_bill_by_day(): # Load the dataset tips = load_dataset(\\"tips\\") # Create and customize the plot p = so.Plot(tips, x=\'day\', y=\'total_bill\', color=\'sex\') # Add bar element and dodge by \'smoker\' with gap to avoid overlap p = p.add(so.Bar(), so.Agg(\'sum\'), so.Dodge(by=\'smoker\', gap=0.1)) # Display the plot p.show() # Call the function to display the plot plot_total_bill_by_day()"},{"question":"Objective The objective of this exercise is to assess your understanding of the pandas options and settings API. You will need to write functions to programmatically manipulate DataFrame display settings and verify their impact. Task Write a function `configure_pandas_display` that takes a dictionary of option-value pairs and applies these settings using pandas\' options API. Additionally, write another function `reset_pandas_display` that resets these options to their default values. Implement the functions with the following signatures: ```python import pandas as pd def configure_pandas_display(options: dict) -> dict: This function takes a dictionary \'options\' where keys are option names (as strings) and values are the values to set for these options. It sets the specified options and returns a dictionary of the old settings before the changes were applied. Parameters: options (dict): A dictionary where keys are pandas options (str) and values are the desired settings. Returns: dict: A dictionary containing old option values before the changes were applied. old_settings = {} for option, value in options.items(): old_settings[option] = pd.get_option(option) pd.set_option(option, value) return old_settings def reset_pandas_display(options: dict): This function takes a dictionary of options and resets them to their default values. Parameters: options (dict): A dictionary where keys are pandas options (str) to be reset. Returns: None for option in options.keys(): pd.reset_option(option) ``` Example Here is an example demonstrating how the functions should work: ```python # Example usage options_to_set = { \\"display.max_rows\\": 10, \\"display.precision\\": 3, \\"display.max_colwidth\\": 50 } # Configure pandas display settings old_settings = configure_pandas_display(options_to_set) print(\\"Old settings:\\", old_settings) # Verify the changes print(pd.get_option(\\"display.max_rows\\")) # Output: 10 print(pd.get_option(\\"display.precision\\")) # Output: 3 print(pd.get_option(\\"display.max_colwidth\\")) # Output: 50 # Reset to default settings reset_pandas_display(options_to_set) # Verify the reset print(pd.get_option(\\"display.max_rows\\")) # Expected output (default): 60 print(pd.get_option(\\"display.precision\\")) # Expected output (default): 6 print(pd.get_option(\\"display.max_colwidth\\")) # Expected output (default): 50 ``` Constraints 1. You should use the pandas options API to get, set, and reset the configurations. 2. Ensure that your functions correctly handle setting and resetting options. 3. Your implementation should handle any valid pandas options. Evaluation You will be evaluated based on the following criteria: 1. Correctness: The functions should correctly set and reset the pandas options. 2. Robustness: The functions should handle various valid pandas options. 3. Clarity: The code should be clear and well-documented.","solution":"import pandas as pd def configure_pandas_display(options: dict) -> dict: This function takes a dictionary \'options\' where keys are option names (as strings) and values are the values to set for these options. It sets the specified options and returns a dictionary of the old settings before the changes were applied. Parameters: options (dict): A dictionary where keys are pandas options (str) and values are the desired settings. Returns: dict: A dictionary containing old option values before the changes were applied. old_settings = {} for option, value in options.items(): old_settings[option] = pd.get_option(option) pd.set_option(option, value) return old_settings def reset_pandas_display(options: dict): This function takes a dictionary of options and resets them to their default values. Parameters: options (dict): A dictionary where keys are pandas options (str) to be reset. Returns: None for option in options.keys(): pd.reset_option(option)"},{"question":"**Question:** As a software engineer, you are tasked to monitor and optimize the memory usage of an application using Python\'s garbage collection interface (`gc` module). You need to perform several tasks to understand how memory is managed and to ensure the system runs efficiently. **Tasks:** 1. **Function: `toggle_gc`** - Create a function `toggle_gc(enable: bool) -> None` that enables or disables automatic garbage collection based on the boolean argument `enable`. - If `enable` is `True`, enable automatic garbage collection. - If `enable` is `False`, disable automatic garbage collection. - Print `\\"Garbage collection enabled\\"` or `\\"Garbage collection disabled\\"` accordingly. 2. **Function: `collect_garbage`** - Create a function `collect_garbage(generation: int = 2) -> int` that forces garbage collection for the specified generation and returns the number of unreachable objects found. - Ensure the function raises a `ValueError` if the generation is not between 0 and 2. - Print the number of unreachable objects found during the collection. 3. **Function: `gc_stats_summary`** - Create a function `gc_stats_summary() -> str` that returns a summary of the garbage collector statistics as a string. - Include the number of collections, collected objects, and uncollectable objects for each generation in the summary. - The summary should be formatted in a readable manner. 4. **Function: `memory_leak_debugging`** - Create a function `memory_leak_debugging(enable: bool) -> None` that sets up debugging for memory leaks. - If `enable` is `True`, set debugging to `gc.DEBUG_LEAK`. - If `enable` is `False`, set debugging to `0` (disable debugging). - Print `\\"Memory leak debugging enabled\\"` or `\\"Memory leak debugging disabled\\"` accordingly. **Example:** ```python # Example usage: toggle_gc(True) # Output: Garbage collection enabled num_unreachable = collect_garbage(1) # Output: (number of unreachable objects found and printed) summary = gc_stats_summary() print(summary) # Output: Collecting statistics summary memory_leak_debugging(True) # Output: Memory leak debugging enabled ``` **Constraints:** - Assume the `gc` module is already imported. - Ensure all print statements provide useful information as indicated in the tasks. Complete these functions to demonstrate your understanding of Python\'s `gc` module and its capabilities in managing and debugging memory.","solution":"import gc def toggle_gc(enable: bool) -> None: Enables or disables automatic garbage collection based on the boolean argument. if enable: gc.enable() print(\\"Garbage collection enabled\\") else: gc.disable() print(\\"Garbage collection disabled\\") def collect_garbage(generation: int = 2) -> int: Forces garbage collection for the specified generation and returns the number of unreachable objects found. Raises a ValueError if the generation is not between 0 and 2. if generation not in (0, 1, 2): raise ValueError(\\"Generation must be between 0 and 2.\\") num_unreachable = gc.collect(generation) print(f\\"Number of unreachable objects found: {num_unreachable}\\") return num_unreachable def gc_stats_summary() -> str: Returns a summary of the garbage collector statistics as a formatted string. stats = gc.get_stats() summary = \\"Garbage Collector Statistics Summary:n\\" for gen, stat in enumerate(stats): summary += (f\\"Generation {gen}: \\" f\\"Collections: {stat[\'collections\']}, \\" f\\"Collected: {stat[\'collected\']}, \\" f\\"Uncollectable: {stat[\'uncollectable\']}n\\") return summary.strip() def memory_leak_debugging(enable: bool) -> None: Sets up debugging for memory leaks. if enable: gc.set_debug(gc.DEBUG_LEAK) print(\\"Memory leak debugging enabled\\") else: gc.set_debug(0) print(\\"Memory leak debugging disabled\\")"},{"question":"# asyncio Server-Client Communication Objective: Your task is to implement a simple server-client architecture using the asyncio package in Python 3.10. The server should handle multiple client connections, and the client should be able to send messages to the server and receive responses. You will also need to implement custom error handling and demonstrate task management using asyncio. Requirements: 1. **Server**: - Should be able to handle multiple client connections concurrently. - Use `asyncio.create_server()` to create the server. - For each connected client, receive a message, process it, and send a response. - Implement custom error handling for client connection issues. - Use a custom protocol that implements `connection_made`, `data_received`, and `connection_lost` methods. 2. **Client**: - Connect to the server using `asyncio.create_connection()`. - Send a message to the server and wait for its response. - Handle server disconnections gracefully. 3. **Task Management**: - Create and manage tasks for handling client connections. - Use `asyncio.gather()` to run multiple tasks concurrently. - Ensure tasks are properly cleaned up on server shutdown. Input and Output Formats: - **Server**: - The server will not take any direct input. It will run continuously, accepting client connections. - For each client message received, the server should prepend \\"Server Response: \\" to the message and send it back. - **Client**: - The client should take a single input message from the user, send it to the server, and print the server\'s response. Constraints: - Ensure the server can handle at least 5 simultaneous client connections. - Properly handle exceptions and ensure resources are freed up correctly. Performance: - The server should be responsive and handle client connections and messages promptly. Example: ```python # Client import asyncio async def tcp_client(message): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') print(\'Close the connection\') writer.close() await writer.wait_closed() asyncio.run(tcp_client(\'Hello, Server!\')) # Server import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport print(\'Connection made:\', transport.get_extra_info(\'peername\')) def data_received(self, data): message = data.decode() print(\'Data received:\', message) response = \'Server Response: \' + message self.transport.write(response.encode()) print(\'Response sent:\', response) def connection_lost(self, exc): print(\'Connection lost:\', exc) async def main(): loop = asyncio.get_running_loop() server = await loop.create_server(lambda: EchoServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() asyncio.run(main()) ``` In this example, the client sends a message to the server, and the server responds with a modified message. Implement this architecture with proper task management and custom error handling.","solution":"import asyncio class CustomServerProtocol(asyncio.Protocol): def __init__(self): super().__init__() self.transport = None def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\'Connection made from {peername}\') def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') response = f\'Server Response: {message}\' self.transport.write(response.encode()) print(f\'Response sent: {response}\') def connection_lost(self, exc): if exc: print(f\'Connection lost with error: {exc}\') else: print(\'Client disconnected normally.\') self.transport.close() async def run_server(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: CustomServerProtocol(), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() async def tcp_client(message): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') print(\'Close the connection\') writer.close() await writer.wait_closed() def start_server(): asyncio.run(run_server()) def start_client(message): asyncio.run(tcp_client(message))"},{"question":"# PyTorch CUDA Device and Memory Management You are working on optimizing the memory usage and device management for a neural network training pipeline using PyTorch and CUDA. In this task, you need to implement functions that manage GPU devices and track memory usage efficiently to prevent out-of-memory errors during training. Task Overview: 1. Implement a function `initialize_cuda_device` to set the current CUDA device. 2. Implement a function `get_memory_status` to retrieve the current memory usage status. 3. Implement a function `optimize_memory_usage` to clear cache and reset peak memory stats. Function Details: 1. **initialize_cuda_device** - **Input**: - `device_index (int)`: The index of the CUDA device to be set as the current device. - **Output**: None - **Description**: This function should set the CUDA device to the provided index and handle initialization if necessary. Ensure the device index is valid. 2. **get_memory_status** - **Input**: None - **Output**: - `memory_status (dict)`: A dictionary containing the following keys - `\'allocated_memory\'`, `\'cached_memory\'`, `\'max_allocated_memory\'`, and `\'max_cached_memory\'`. Each key maps to the corresponding memory value in bytes. - **Description**: This function should return the current GPU memory usage statistics, including allocated memory, cached memory, maximum allocated memory, and maximum cached memory. 3. **optimize_memory_usage** - **Input**: None - **Output**: None - **Description**: This function should clear the GPU memory cache and reset peak memory statistics to optimize memory usage. Constraints: - Your solution should handle cases with multiple CUDA devices and gracefully fall back to CPU if no CUDA devices are available. - Use appropriate PyTorch CUDA functions from the provided documentation. Performance Requirements: - Ensure that memory operations do not disrupt ongoing computations beyond necessary memory management, i.e., clearing cache should be safe within the context of your functions. - Aim to minimize the latencies caused by device queries and memory operations. Example Usage: ```python # Setting the CUDA device to device index 0 initialize_cuda_device(0) # Getting the current memory usage status status = get_memory_status() print(status) # Optimizing memory usage optimize_memory_usage() ``` Implement the functions `initialize_cuda_device`, `get_memory_status`, and `optimize_memory_usage` below: ```python import torch def initialize_cuda_device(device_index: int): Sets the CUDA device to the specified index and initializes if necessary. if torch.cuda.is_available(): device_count = torch.cuda.device_count() if device_index < 0 or device_index >= device_count: raise ValueError(f\\"Invalid device index. Must be between 0 and {device_count-1}.\\") torch.cuda.set_device(device_index) else: print(\\"CUDA is not available. Falling back to CPU.\\") def get_memory_status(): Returns the current memory usage status. if torch.cuda.is_available(): allocated_memory = torch.cuda.memory_allocated() cached_memory = torch.cuda.memory_reserved() max_allocated_memory = torch.cuda.max_memory_allocated() max_cached_memory = torch.cuda.max_memory_reserved() return { \'allocated_memory\': allocated_memory, \'cached_memory\': cached_memory, \'max_allocated_memory\': max_allocated_memory, \'max_cached_memory\': max_cached_memory, } else: return { \'allocated_memory\': 0, \'cached_memory\': 0, \'max_allocated_memory\': 0, \'max_cached_memory\': 0, } def optimize_memory_usage(): Clears the GPU memory cache and resets peak memory statistics. if torch.cuda.is_available(): torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats() else: print(\\"CUDA is not available.\\") ```","solution":"import torch def initialize_cuda_device(device_index: int): Sets the CUDA device to the specified index and initializes if necessary. if torch.cuda.is_available(): device_count = torch.cuda.device_count() if device_index < 0 or device_index >= device_count: raise ValueError(f\\"Invalid device index. Must be between 0 and {device_count-1}.\\") torch.cuda.set_device(device_index) else: print(\\"CUDA is not available. Falling back to CPU.\\") def get_memory_status(): Returns the current memory usage status. if torch.cuda.is_available(): allocated_memory = torch.cuda.memory_allocated() cached_memory = torch.cuda.memory_reserved() max_allocated_memory = torch.cuda.max_memory_allocated() max_cached_memory = torch.cuda.max_memory_reserved() return { \'allocated_memory\': allocated_memory, \'cached_memory\': cached_memory, \'max_allocated_memory\': max_allocated_memory, \'max_cached_memory\': max_cached_memory, } else: return { \'allocated_memory\': 0, \'cached_memory\': 0, \'max_allocated_memory\': 0, \'max_cached_memory\': 0, } def optimize_memory_usage(): Clears the GPU memory cache and resets peak memory statistics. if torch.cuda.is_available(): torch.cuda.empty_cache() torch.cuda.reset_peak_memory_stats() else: print(\\"CUDA is not available.\\")"},{"question":"<|Analysis Begin|> The provided documentation discusses extending the Distutils package, which is used for distributing and installing Python packages. It focuses on two main areas: integrating new commands into the Distutils and adding new distribution types. Key concepts include: 1. Creating new Distutils commands by subclassing the `distutils.cmd.Command` class. 2. Integrating these new commands into the Distutils setup process via the `setup.py` script. 3. Utilizing the `cmdclass` argument in the `setup` function to specify these new or modified commands. 4. Using the `command_packages` option to extend the search for command implementations to additional packages. 5. Modifying the distribution creation process to add new distribution types and ensuring proper support for uploading to PyPI. The document provides enough background to design a Python coding assessment question that involves creating and integrating new Distutils commands. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective Create a custom Distutils command for a Python package\'s `setup.py` script and integrate it properly. This will demonstrate your understanding of extending Distutils by adding new commands. Problem Statement You are tasked with developing a specialized build command for a Python package. This new command should compile all `.example` files in your project directory into a single `output.example` file within the `build` directory. Your task is to: 1. Create a new command class that inherits from `distutils.cmd.Command`. 2. Implement a custom `build_example` command that: - Locates all `.example` files within the project directory recursively. - Concatenates the contents of these `.example` files into a single file named `output.example` in the `build` directory. 3. Update the `setup.py` script to integrate and use this new command. # Requirements 1. **Input and Output Formats:** - Input: The existence of multiple `.example` files within the project directory. - Output: A single file named `output.example` inside the `build` directory containing the concatenated contents of all `.example` files. 2. **Constraints:** - You may assume that the project directory is `.` (current directory). - The directory `build` should be created if it does not exist. - Only files with the `.example` extension should be considered. 3. **Performance Considerations:** - The solution should efficiently handle a large number of `.example` files. - Ensure that opening, reading, and writing files is performed optimally. # Instructions 1. **Step 1: Create the Command Class** ```python from distutils.cmd import Command import os class build_example(Command): A custom command to build example files into output.example description = \'concatenate all .example files into a single output.example file\' user_options = [] def initialize_options(self): pass def finalize_options(self): pass def run(self): # Your code for collecting, reading, and writing .example files goes here # Remember to use self.announce to print informational messages pass ``` 2. **Step 2: Integrate the Command in setup.py** ```python from distutils.core import setup from build_example import build_example setup( name=\'YourPackageName\', version=\'1.0\', packages=[\'your_package\'], cmdclass={ \'build_example\': build_example, }, ) ``` 3. **Step 3: Implement the functionality in the `run` method of your command class.** # Evaluation Criteria - Correct implementation and integration of the custom command. - Efficient and correct reading and writing of `.example` files. - Appropriate handling of directories and file paths. - Clarity and readability of the code. Good luck!","solution":"from distutils.cmd import Command import os class build_example(Command): A custom command to build example files into output.example description = \'concatenate all .example files into a single output.example file\' user_options = [] def initialize_options(self): pass def finalize_options(self): pass def run(self): self.announce(\'Collecting .example files...\', level=3) example_files = [] for root, dirs, files in os.walk(\'.\'): for file in files: if file.endswith(\'.example\'): example_files.append(os.path.join(root, file)) if not example_files: self.announce(\'No .example files found.\', level=2) return self.announce(f\'Found {len(example_files)} .example files.\', level=3) os.makedirs(\'build\', exist_ok=True) output_path = os.path.join(\'build\', \'output.example\') with open(output_path, \'w\') as output_file: for file_path in example_files: self.announce(f\'Reading {file_path}...\', level=3) with open(file_path, \'r\') as example_file: output_file.write(example_file.read()) self.announce(f\'Concatenation complete. Output written to {output_path}\', level=3)"},{"question":"**Objective:** In this exercise, you will create a custom dataclass to model a fictional library system and implement methods for managing the inventory. --- # Problem Statement: You are tasked with designing a `Book` dataclass that represents a book in a library. Your `Book` class should include the following features: 1. **Attributes**: - `title` (str): The title of the book. - `author` (str): The author(s) of the book. - `publication_year` (int): The year the book was published. - `copies_available` (int): The number of copies available in the library. - `genres` (List[str]): A list of genres associated with the book. 2. **Methods**: - `__post_init__(self)`: This method should ensure the `publication_year` is not greater than the current year. If it is, raise a `ValueError`. - `borrow_book(self) -> None`: If there are copies available, decrement the `copies_available` by one; otherwise, raise an `Exception` indicating no copies are available. - `return_book(self) -> None`: Increment the `copies_available` by one. - `is_genre(self, genre: str) -> bool`: Return `True` if the book belongs to the specified genre; otherwise, `False`. 3. **Utility Functions**: - `as_dict(self) -> dict`: Use `dataclasses.asdict` to return the `Book` object as a dictionary. - `Book.from_dict(data: dict) -> Book`: A class method that creates a `Book` instance from a dictionary. --- # Requirements: - Use the `@dataclass` decorator to define the `Book` class. - Use `dataclasses.field` to specify a default factory for the `genres` attribute, ensuring it initializes with an empty list if no genres are provided. - Implement the `__post_init__` method to validate the publication year. - Use `dataclasses.asdict` to implement the `as_dict` method. - Implement the `from_dict` class method to construct a `Book` from a dictionary. # Constraints: - The `publication_year` must be a non-negative integer and should not be greater than the current year. - The `copies_available` should always be non-negative. # Example Usage: ```python from dataclasses import dataclass, field from datetime import datetime from typing import List, Dict @dataclass class Book: title: str author: str publication_year: int copies_available: int = 0 genres: List[str] = field(default_factory=list) def __post_init__(self): current_year = datetime.now().year if self.publication_year > current_year: raise ValueError(f\\"Publication year {self.publication_year} is in the future.\\") def borrow_book(self) -> None: if self.copies_available > 0: self.copies_available -= 1 else: raise Exception(\\"No copies available to borrow.\\") def return_book(self) -> None: self.copies_available += 1 def is_genre(self, genre: str) -> bool: return genre in self.genres def as_dict(self) -> Dict: from dataclasses import asdict return asdict(self) @classmethod def from_dict(cls, data: Dict) -> \\"Book\\": return cls(**data) # Example: book_data = { \'title\': \'Python Programming\', \'author\': \'John Doe\', \'publication_year\': 2020, \'copies_available\': 3, \'genres\': [\'Programming\', \'Technology\'] } book = Book.from_dict(book_data) print(book.as_dict()) book.borrow_book() print(book.copies_available) # Expected: 2 book.return_book() print(book.copies_available) # Expected: 3 ``` # Submission: Save your implementation in a file named `LibrarySystem.py`.","solution":"from dataclasses import dataclass, field, asdict from datetime import datetime from typing import List, Dict @dataclass class Book: title: str author: str publication_year: int copies_available: int = 0 genres: List[str] = field(default_factory=list) def __post_init__(self): current_year = datetime.now().year if self.publication_year > current_year: raise ValueError(f\\"Publication year {self.publication_year} is in the future.\\") def borrow_book(self) -> None: if self.copies_available > 0: self.copies_available -= 1 else: raise Exception(\\"No copies available to borrow.\\") def return_book(self) -> None: self.copies_available += 1 def is_genre(self, genre: str) -> bool: return genre in self.genres def as_dict(self) -> Dict: return asdict(self) @classmethod def from_dict(cls, data: Dict) -> \\"Book\\": return cls(**data)"},{"question":"**Python Coding Assessment Question** # Objective: Implement a Python function that automates the creation of a `setup.py` file for a given module and generates its source distribution. # Problem Statement: Create a function `create_setup_and_distribution(module_name, version, author_name, author_email, module_file_path)` which performs the following tasks: 1. Generates a `setup.py` file with the provided metadata. 2. Creates a source distribution for the module. # Input: - `module_name` (str): The name of the module. - `version` (str): The version of the module. - `author_name` (str): The author\'s name. - `author_email` (str): The author\'s email address. - `module_file_path` (str): The file path to the module file (`.py` file). # Output: - A string indicating success or failure of the operation. # Example: ```python create_setup_and_distribution( module_name=\\"example_module\\", version=\\"0.1\\", author_name=\\"John Doe\\", author_email=\\"john.doe@example.com\\", module_file_path=\\"./example_module.py\\" ) ``` Expected output if successful: ``` \\"Setup and source distribution created successfully for example_module version 0.1.\\" ``` # Constraints: - Ensure the function works on both Unix-based and Windows systems. - Handle exceptions appropriately to ensure robustness. - Use the `distutils` library for creating the setup file and source distribution. # Additional Notes: - Assume Python and necessary tools (like `distutils`) are already installed in the environment where the function will be executed. - If the module file does not exist at the given path, the function should raise an appropriate error. # Your task: 1. Implement the `create_setup_and_distribution` function as described. 2. The function should write to a `setup.py` file dynamically based on the provided input, and subsequently run the necessary `distutils` commands to create the source distribution.","solution":"import os from distutils.core import run_setup def create_setup_and_distribution(module_name, version, author_name, author_email, module_file_path): Creates a setup.py file and source distribution for the provided module information. Parameters: module_name (str): The name of the module. version (str): The version of the module. author_name (str): The author\'s name. author_email (str): The author\'s email address. module_file_path (str): The file path to the module file (.py file). Returns: str: A string indicating the success or failure of the operation. if not os.path.isfile(module_file_path): raise FileNotFoundError(f\\"Module file {module_file_path} does not exist.\\") setup_content = f from distutils.core import setup setup( name=\'{module_name}\', version=\'{version}\', author=\'{author_name}\', author_email=\'{author_email}\', py_modules=[\'{module_name}\'], ) setup_file_path = os.path.join(os.path.dirname(module_file_path), \'setup.py\') try: with open(setup_file_path, \'w\') as setup_file: setup_file.write(setup_content) run_setup(setup_file_path, script_args=[\'sdist\']) return f\\"Setup and source distribution created successfully for {module_name} version {version}.\\" except Exception as e: return str(e)"},{"question":"Objective Assess the ability to use the `seaborn` package for creating complex visualizations with meaningful patterns and customization. Problem Statement You are provided with the \\"tips\\" dataset from the seaborn library. Your task is to write a Python function that creates a multi-faceted scatter plot using this dataset. In this plot, you are required to show relationships between the total bill and tips given by customers, separated by different days, and segmented by the time of the day (Lunch/Dinner). Function Signature ```python def visualize_tips_data(): pass ``` Requirements 1. **Load the dataset**: - Load the \\"tips\\" dataset using seaborn\'s `load_dataset` function. 2. **Create a multi-faceted scatter plot**: - Use the `relplot` function from seaborn to create a scatter plot (`kind=\\"scatter\\"`). - Map the `x` axis to `total_bill` and the `y` axis to `tip`. - Use the `hue` parameter to differentiate data points by the `day` on which they were recorded. - Use the `col` parameter to create columns for different `time` (Lunch/Dinner). - Use the `row` parameter to create rows based on the `sex` of the customers. 3. **Customize the plot**: - Set the theme to `\\"ticks\\"`. - Set the overall figure size to have a `height` of 5 and an `aspect` ratio of 1.2. - Use the `FacetGrid` object returned by `relplot` to: - Set the title for each facet to \\"Time: {col_name}, Sex: {row_name}\\". - Label the x-axis as \\"Total Bill\\" and the y-axis as \\"Tip Given\\". Example Usage The function should not return anything. It should only display the plot when called. ```python visualize_tips_data() ``` **Constraints**: - Ensure that the solution is efficient and avoids redundancy in plot creation and customization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): # Load the \\"tips\\" dataset from seaborn tips = sns.load_dataset(\\"tips\\") # Set the theme to \\"ticks\\" sns.set_theme(style=\\"ticks\\") # Create the multi-faceted scatter plot g = sns.relplot( data=tips, kind=\\"scatter\\", x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", col=\\"time\\", row=\\"sex\\", height=5, aspect=1.2 ) # Set titles and labels g.set_axis_labels(\\"Total Bill\\", \\"Tip Given\\") for ax, col_name, row_name in zip(g.axes.flat, g.col_names * len(g.row_names), g.row_names * len(g.col_names)): ax.set_title(f\\"Time: {col_name}, Sex: {row_name}\\") # Show the plot plt.show()"},{"question":"# Memory Management Practice In this exercise, you\'ll practice implementing a custom memory allocator in Python by simulating the management of a list of various-sized memory blocks. Using the concepts and functions discussed in this document, you will create functions to allocate, reallocate, and free memory blocks correctly. Requirements: You are required to implement three functions using Python\'s C-API memory allocation functions. These functions will manage memory for a simulated list structure (list in Python but implemented using the described memory APIs for practice): 1. **allocate_memory_list:** This function initializes a list by allocating memory for `num_elements`, each of size `element_size`. Use the appropriate allocation strategy and function (`PyMem_*` or `PyObject_*`). 2. **resize_memory_list:** This function resizes the existing list (previously allocated) to a new size `new_size` and retains existing content up to the minimum of the old and new sizes. Use an appropriate reallocation strategy. 3. **free_memory_list:** This function frees the memory allocated for the list. Function Signatures: `allocate_memory_list(num_elements: int, element_size: int) -> list` `resize_memory_list(current_list: list, new_size: int) -> list` `free_memory_list(current_list: list) -> None` Implementations: 1. **allocate_memory_list:** - Input: `num_elements` (number of elements) and `element_size` (size of each element). - Output: returns a list simulating allocated memory. 2. **resize_memory_list:** - Input: `current_list` (list obtained from `allocate_memory_list`), `new_size` (new desired size). - Output: returns a resized list. 3. **free_memory_list:** - Input: `current_list` (list obtained from `allocate_memory_list` or `resize_memory_list`). - Output: returns `None`. Constraints: - You must use `PyMem_*` functions for raw memory and `PyObject_*` functions for object memory allocation. - Ensure memory safety by using appropriate free functions corresponding to their allocate or realloc pairs. Example Usage: ```python elements = 5 size_per_element = 10 # Allocate memory for the list memory_list = allocate_memory_list(elements, size_per_element) # Resize the memory list to a new size memory_list = resize_memory_list(memory_list, 10) # Free the allocated memory free_memory_list(memory_list) ``` **Bonus:** Include error-handling to manage and log issues gracefully if allocation or deallocation fails.","solution":"def allocate_memory_list(num_elements: int, element_size: int) -> list: Initializes a list by allocating memory for `num_elements`, each of size `element_size`. In Python, list elements will be None representing uninitialized memory. return [None] * num_elements def resize_memory_list(current_list: list, new_size: int) -> list: Resizes the existing list (previously allocated) to a new size `new_size` and retains existing content up to the minimum of the old and new sizes. new_list = [None] * new_size for i in range(min(len(current_list), new_size)): new_list[i] = current_list[i] return new_list def free_memory_list(current_list: list) -> None: Frees the memory allocated for the list. In Python, this means clearing the list. current_list.clear()"},{"question":"You are working on a software project that involves user interaction through various message prompts. Your task is to implement a function that will utilize `tkinter.messagebox` to display different kinds of message boxes based on parameters passed to the function. # Task Description Write a function named `display_message` that takes the following parameters: - `message_type` (str): The type of message box to display. It can be one of the following values: \\"info\\", \\"warning\\", \\"error\\", \\"question\\", \\"okcancel\\", \\"retrycancel\\", \\"yesno\\", \\"yesnocancel\\". - `title` (str): The title of the message box. - `message` (str): The message to be displayed within the message box. - `options` (dict, optional): Additional options to customize the message box. Based on the `message_type` parameter, the function should call the corresponding method in the `tkinter.messagebox` module and display the message box with the provided `title` and `message`. The function should return the user\'s response. # Input - `message_type`: A string indicating the type of the message box. - `title`: A string title for the message box. - `message`: A string message for the message box. - `options` (optional): A dictionary of additional options to customize the message box. # Output - The function should return the user\'s response, which will depend on the message type (e.g., `True`, `False`, `OK`, `Yes`, `No`, `None`). # Constraints - The `message_type` should be one of the specified values: \\"info\\", \\"warning\\", \\"error\\", \\"question\\", \\"okcancel\\", \\"retrycancel\\", \\"yesno\\", \\"yesnocancel\\". - If an invalid `message_type` is provided, the function should raise a `ValueError`. - Ensure that the function imports `tkinter.messagebox` and uses it properly to display the message boxes. # Example ```python import tkinter.messagebox def display_message(message_type, title, message, options=None): if options is None: options = {} if message_type == \\"info\\": return tkinter.messagebox.showinfo(title, message, **options) elif message_type == \\"warning\\": return tkinter.messagebox.showwarning(title, message, **options) elif message_type == \\"error\\": return tkinter.messagebox.showerror(title, message, **options) elif message_type == \\"question\\": return tkinter.messagebox.askquestion(title, message, **options) elif message_type == \\"okcancel\\": return tkinter.messagebox.askokcancel(title, message, **options) elif message_type == \\"retrycancel\\": return tkinter.messagebox.askretrycancel(title, message, **options) elif message_type == \\"yesno\\": return tkinter.messagebox.askyesno(title, message, **options) elif message_type == \\"yesnocancel\\": return tkinter.messagebox.askyesnocancel(title, message, **options) else: raise ValueError(\\"Invalid message type provided\\") # Example usage: response = display_message(\\"info\\", \\"Information\\", \\"This is an informational message.\\") print(response) # This will print the user\'s response, e.g., \'ok\' ``` # Notes - Ensure that Tkinter and `tkinter.messagebox` are correctly installed and imported. - Since message boxes are modal, this function will block execution until the user interacts with the message box. Good luck!","solution":"import tkinter import tkinter.messagebox def display_message(message_type, title, message, options=None): if options is None: options = {} if message_type == \\"info\\": return tkinter.messagebox.showinfo(title, message, **options) elif message_type == \\"warning\\": return tkinter.messagebox.showwarning(title, message, **options) elif message_type == \\"error\\": return tkinter.messagebox.showerror(title, message, **options) elif message_type == \\"question\\": return tkinter.messagebox.askquestion(title, message, **options) elif message_type == \\"okcancel\\": return tkinter.messagebox.askokcancel(title, message, **options) elif message_type == \\"retrycancel\\": return tkinter.messagebox.askretrycancel(title, message, **options) elif message_type == \\"yesno\\": return tkinter.messagebox.askyesno(title, message, **options) elif message_type == \\"yesnocancel\\": return tkinter.messagebox.askyesnocancel(title, message, **options) else: raise ValueError(\\"Invalid message type provided\\")"},{"question":"# Color Space Image Manipulator You are tasked with writing a Python function that manipulates an image represented in RGB color space by performing a series of color space conversions and modifications. Problem Statement Create a function `manipulate_image_colors(image: List[List[Tuple[float, float, float]]], modification: Dict[str, Union[str, float]]) -> List[List[Tuple[float, float, float]]]` that modifies the colors of an image. The function should perform the following steps: 1. Convert each pixel from RGB to the specified color space (either \'YIQ\', \'HLS\', or \'HSV\') from the `modification` dictionary. 2. Apply a given modification value to the specified component of this color space. 3. Convert back to RGB and return the modified image. Parameters - `image`: A list of lists representing the image, where each inner list is a row of pixels, and each pixel is represented as a tuple of three floats (r, g, b), where all floats are between 0.0 and 1.0. - `modification`: A dictionary with the following keys: - `\'color_space\'`: A string that is either `\'YIQ\'`, `\'HLS\'`, or `\'HSV\'`, indicating the color space to convert to. - `\'component\'`: A string that is either `\'Y\'`, `\'I\'`, `\'Q\'`, `\'H\'`, `\'L\'`, `\'S\'`, `\'V\'`, indicating the component to be modified. - `\'value\'`: A float representing the amount to add to the specified component. Function Constraints - All images will have at least one pixel. - The modification value will ensure that the modified component remains within valid bounds (i.e., 0.0 to 1.0 for most components). - The input image will always have valid float values between 0.0 and 1.0 for each pixel component. Output Format Return the modified image as a list of lists, where each inner list is a row of pixels, and each pixel is a tuple of three floats (r, g, b), each between 0.0 and 1.0. Example ```python import colorsys from typing import List, Tuple, Dict, Union def manipulate_image_colors(image: List[List[Tuple[float, float, float]]], modification: Dict[str, Union[str, float]]) -> List[List[Tuple[float, float, float]]]: # Your implementation here pass # Sample Usage: image = [ [(0.1, 0.2, 0.3), (0.4, 0.5, 0.6)], [(0.7, 0.8, 0.9), (0.2, 0.3, 0.4)] ] modification = { \'color_space\': \'HSV\', \'component\': \'V\', \'value\': 0.1 } modified_image = manipulate_image_colors(image, modification) print(modified_image) ``` Expected output for this example may include slight adjustments in the \\"V\\" component of the HSV color space, and then converting back to the respective RGB values.","solution":"import colorsys from typing import List, Tuple, Dict, Union def manipulate_image_colors(image: List[List[Tuple[float, float, float]]], modification: Dict[str, Union[str, float]]) -> List[List[Tuple[float, float, float]]]: color_space = modification[\'color_space\'] component = modification[\'component\'] value = modification[\'value\'] def modify_pixel_color(pixel: Tuple[float, float, float]): r, g, b = pixel # Converting RGB to desired color space if color_space == \'YIQ\': y, i, q = colorsys.rgb_to_yiq(r, g, b) if component == \'Y\': y += value elif component == \'I\': i += value elif component == \'Q\': q += value # Correct for possible out of bound values y = max(0.0, min(y, 1.0)) i = max(-1.0, min(i, 1.0)) q = max(-1.0, min(q, 1.0)) r, g, b = colorsys.yiq_to_rgb(y, i, q) elif color_space == \'HLS\': h, l, s = colorsys.rgb_to_hls(r, g, b) if component == \'H\': h += value elif component == \'L\': l += value elif component == \'S\': s += value # Correct for possible out of bound values h = h % 1.0 l = max(0.0, min(l, 1.0)) s = max(0.0, min(s, 1.0)) r, g, b = colorsys.hls_to_rgb(h, l, s) elif color_space == \'HSV\': h, s, v = colorsys.rgb_to_hsv(r, g, b) if component == \'H\': h += value elif component == \'S\': s += value elif component == \'V\': v += value # Correct for possible out of bound values h = h % 1.0 s = max(0.0, min(s, 1.0)) v = max(0.0, min(v, 1.0)) r, g, b = colorsys.hsv_to_rgb(h, s, v) return (r, g, b) # Apply modification to each pixel return [[modify_pixel_color(pixel) for pixel in row] for row in image]"},{"question":"# Mailcap File Handler Implementation Mailcap files are essential for configuring how MIME-aware applications handle files with different MIME types. Python\'s `mailcap` module provides tools to interact with these files. In this task, you are required to work with a subset of this module\'s functionalities to demonstrate your understanding of handling MIME types. Function 1: `get_caps_from_string` You will first implement a function that simulates reading mailcap entries from a string (as opposed to actual files on disk). ```python def get_caps_from_string(mailcap_string): Parses a string containing mailcap entries and returns a dictionary mapping MIME types to lists of action dictionaries. Args: - mailcap_string (str): A string where each line is a mailcap entry, in the format \'MIMEtype; action\' Returns: - dict: A dictionary where each key is a MIME type and the value is a list of dictionaries representing actions. pass ``` - `mailcap_string` is a multiline string where each line corresponds to a mailcap entry in the format `MIMEtype; action`. - The function will return a dictionary mapping MIME types to a list of dictionaries representing the actions. Function 2: `find_match` Using the output of the `get_caps_from_string` function, implement the `find_match` function which mimics `mailcap.findmatch`. ```python def find_match(caps, MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[]): Finds a suitable command line to execute for a given MIME type based on the provided caps. Args: - caps (dict): Dictionary returned by get_caps_from_string. - MIMEtype (str): The MIME type to find a command for. - key (str): The type of activity to perform (default is \'view\'). - filename (str): Filename to replace %s in the command (default is \'/dev/null\'). - plist (list): List of string parameters in the form \'name=value\'. Returns: - tuple: A 2-tuple where the first element is the command line string to be executed, and the second element is the mailcap entry for the given MIME type. Returns (None, None) if no match is found. pass ``` # Requirements: - Your code should parse the given mailcap string correctly. - You should handle edge cases where the mailcap string is empty or malformed. - Parameters in `plist` should correctly replace placeholders like `%{name}` in the command. - Invalid characters in `filename` or `plist` should cause the function to safely handle these cases without causing security issues. # Example Usage ```python mailcap_string = video/mpeg; xmpeg %s text/html; browser %s text/plain; cat %s application/pdf; pdfreader %s caps = get_caps_from_string(mailcap_string) command, entry = find_match(caps, \'video/mpeg\', filename=\'example.mpg\') print(command, entry) # Output: \'xmpeg example.mpg\', {\'view\': \'xmpeg %s\'} ``` # Constraints: 1. The filename should not contain characters other than alphanumerics and \\"@+=:,./-_\\". 2. If such characters are in the filename or plist, `find_match` should return `(None, None)`. # Notes: - You should not use any external libraries other than those in the standard library. - Remember to handle different cases to ensure robustness and security of the implementation.","solution":"import re def get_caps_from_string(mailcap_string): Parses a string containing mailcap entries and returns a dictionary mapping MIME types to lists of action dictionaries. Args: - mailcap_string (str): A string where each line is a mailcap entry, in the format \'MIMEtype; action\' Returns: - dict: A dictionary where each key is a MIME type and the value is a list of dictionaries representing actions. caps = {} for line in mailcap_string.strip().split(\'n\'): if not line.strip(): continue try: mime_type, action = line.split(\';\', 1) mime_type = mime_type.strip() action = action.strip() if mime_type not in caps: caps[mime_type] = [] caps[mime_type].append({\'view\': action}) except ValueError: continue return caps def valid_filename(filename): Validates that the filename contains only allowed characters. return re.match(r\'^[w@+=:,./_-]+\', filename) is not None def find_match(caps, MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[]): Finds a suitable command line to execute for a given MIME type based on the provided caps. Args: - caps (dict): Dictionary returned by get_caps_from_string. - MIMEtype (str): The MIME type to find a command for. - key (str): The type of activity to perform (default is \'view\'). - filename (str): Filename to replace %s in the command (default is \'/dev/null\'). - plist (list): List of string parameters in the form \'name=value\'. Returns: - tuple: A 2-tuple where the first element is the command line string to be executed, and the second element is the mailcap entry for the given MIME type. Returns (None, None) if no match is found. if not valid_filename(filename): return (None, None) params = {item.split(\'=\')[0]: item.split(\'=\')[1] for item in plist} if MIMEtype in caps: for entry in caps[MIMEtype]: if key in entry: command = entry[key] command = command.replace(\'%s\', filename) for param_key, param_value in params.items(): command = command.replace(f\'%{{{param_key}}}\', param_value) return (command, entry) return (None, None)"},{"question":"Async I/O Task Scheduler **Objective**: Implement an asynchronous task scheduler using Python\'s `asyncio` library. Problem Statement You are required to write a function `task_scheduler(tasks: List[Tuple[int, str]]) -> List[str]` that takes a list of tasks as input, where each task is represented as a tuple containing a delay time in seconds and a task name. Your function should asynchronously schedule and execute these tasks, returning the output in the order of their completion. Each task is simply a coroutine that simulates some work by sleeping for the specified delay time. Input: - `tasks` (List[Tuple[int, str]]): A list of tasks, where each task is a tuple. The first element of the tuple is an integer representing the delay time in seconds, and the second element is a string representing the task name. Output: - List of strings: A list of task names in the order they complete. Constraints: - The sum of all delays will not exceed 30 seconds. - The list `tasks` will have at most 10 tasks. - Delay times will be between 1 and 10 seconds. Example: ```python import asyncio from typing import List, Tuple async def task_scheduler(tasks: List[Tuple[int, str]]) -> List[str]: async def execute_task(delay: int, name: str) -> str: await asyncio.sleep(delay) return name coroutines = [execute_task(delay, name) for delay, name in tasks] completed, pending = await asyncio.wait(coroutines) return [task.result() for task in completed] # Example usage: tasks = [(3, \\"task1\\"), (1, \\"task2\\"), (2, \\"task3\\")] output = asyncio.run(task_scheduler(tasks)) print(output) # Output could be: [\'task2\', \'task3\', \'task1\'] ``` Note: - The order of task completion is dependent on their delay times. - Use `asyncio` to handle the execution and scheduling of tasks. **Performance Requirement**: - Your implementation should efficiently utilize asyncio\'s event loop to manage the concurrency. **Evaluation Criteria**: - Correctness of the implemented function. - Proper use of asyncio for concurrency. - Handling of edge cases according to the constraints provided.","solution":"import asyncio from typing import List, Tuple async def task_scheduler(tasks: List[Tuple[int, str]]) -> List[str]: async def execute_task(delay: int, name: str) -> str: await asyncio.sleep(delay) return name coroutines = [execute_task(delay, name) for delay, name in tasks] completed, _ = await asyncio.wait(coroutines) return [task.result() for task in completed]"},{"question":"# Partial Least Squares Regression Implementation and Analysis **Objective**: Implement the `PLSRegression` from the scikit-learn library, use it to perform a regression task on a synthetic dataset, and analyze the results. **Problem Statement**: Write a Python function that performs the following tasks: 1. Generate synthetic datasets for matrices (X) and (Y). 2. Implement a function that fits the `PLSRegression` model from the `sklearn.cross_decomposition` module using the generated data. 3. Evaluate the model\'s performance using R-squared scores. 4. Plot the actual vs. predicted values for the test set. **Function Signature**: ```python def pls_regression_evaluation( n_samples: int = 100, n_features: int = 10, n_targets: int = 1, n_components: int = 2, test_size: float = 0.2, random_state: int = 42 ) -> None: ``` **Inputs**: - `n_samples` (int): Number of samples in the dataset (default is 100). - `n_features` (int): Number of features in matrix (X) (default is 10). - `n_targets` (int): Number of target variables in matrix (Y) (default is 1). - `n_components` (int): Number of PLS components to keep (default is 2). - `test_size` (float): Proportion of the dataset to include in the test split (default is 0.2). - `random_state` (int): Seed used by the random number generator (default is 42). **Outputs**: - Print the R-squared score for the training and test set. - Plot the actual vs. predicted values for the test set. **Constraints**: - Use appropriate data generation methods to create synthetic data for (X) and (Y). - Ensure reproducibility by fixing the random seed where necessary. - Validate the performance using the R-squared score. **Example**: ```python pls_regression_evaluation(n_samples=150, n_features=5, n_targets=2, n_components=3, test_size=0.25, random_state=0) ``` This function call should: - Generate a synthetic dataset with 150 samples, 5 features, and 2 target variables. - Fit a PLS regression model with 3 components. - Split the dataset into training (75%) and testing (25%) sets. - Print the R-squared scores for both training and test sets. - Plot the actual vs. predicted values for the test set. **Additional Notes**: - You may use `numpy` for generating synthetic data. - Utilize `train_test_split` from `sklearn.model_selection` for splitting the data. - Use appropriate plotting libraries such as `matplotlib` or `seaborn` for visualization.","solution":"import numpy as np from sklearn.model_selection import train_test_split from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import r2_score import matplotlib.pyplot as plt def pls_regression_evaluation( n_samples: int = 100, n_features: int = 10, n_targets: int = 1, n_components: int = 2, test_size: float = 0.2, random_state: int = 42 ) -> None: Performs PLSRegression, evaluates and plots the results. :param n_samples: Number of samples in the dataset. :param n_features: Number of features in matrix X. :param n_targets: Number of target variables in matrix Y. :param n_components: Number of PLS components to keep. :param test_size: Proportion of the dataset to include in the test split. :param random_state: Seed used by the random number generator. # Generate synthetic datasets for X and Y np.random.seed(random_state) X = np.random.randn(n_samples, n_features) Y = np.random.randn(n_samples, n_targets) # Split the dataset into training and testing sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state) # Create and fit the PLSRegression model pls = PLSRegression(n_components=n_components) pls.fit(X_train, Y_train) # Predict the response for training and test sets Y_train_pred = pls.predict(X_train) Y_test_pred = pls.predict(X_test) # Evaluate the model using R-squared scores r2_train = r2_score(Y_train, Y_train_pred) r2_test = r2_score(Y_test, Y_test_pred) print(f\'R-squared score for training set: {r2_train:.2f}\') print(f\'R-squared score for test set: {r2_test:.2f}\') # Plot the actual vs. predicted values for the test set for i in range(n_targets): plt.figure() plt.scatter(Y_test[:, i], Y_test_pred[:, i], alpha=0.6, label=\'Predictions\') plt.plot([Y_test[:, i].min(), Y_test[:, i].max()], [Y_test[:, i].min(), Y_test[:, i].max()], \'k--\', lw=2, label=\'Perfect Fit\') plt.xlabel(\'Actual\') plt.ylabel(\'Predicted\') plt.title(f\'Actual vs Predicted Values for Target {i}\') plt.legend() plt.show()"},{"question":"**Objective:** Demonstrate your understanding of using Seaborn\'s object-oriented interface to prepare, plot, and customize data visualizations. Problem Statement You are provided with a dataset containing information about vehicle accidents named `accident_data.csv`. This file includes the following columns: - `datetime`: Timestamp of the accident. - `vehicle_type`: Type of vehicle involved. - `severity`: Severity of the accident, with possible values: \'minor\', \'major\', \'fatal\'. - `location`: City where the accident happened. Your task is to analyze this data and create a visualization using Seaborn\'s object-oriented interface with the following requirements: 1. **Data Preparation**: - Load the dataset and convert the `datetime` column to a `datetime` object. - Extract the hour from the `datetime` column and create a new column `hour`. 2. **Plotting (Using Seaborn objects)**: - Create a `Plot` object using Seaborn, plotting the count of accidents (`y-axis`) against the hour of the day (`x-axis`). - Plot different lines for each `severity` type. - Customize the appearance of the lines: - Set different colors for each severity type. - Adjust the `linewidth` and `alpha` for better visualization. 3. **Final Customizations**: - Set layout size to (10, 6). - Share the x-axis and y-axis among subplots. - Ensure the plot has appropriate title, x-label, y-label, and legend. Input A CSV file named `accident_data.csv` with the described structure and data. Output A customized Seaborn plot as described in the requirements. Example ```python import seaborn.objects as so import pandas as pd # Data Preparation df = pd.read_csv(\'accident_data.csv\') df[\'datetime\'] = pd.to_datetime(df[\'datetime\']) df[\'hour\'] = df[\'datetime\'].dt.hour # Plotting p = ( so.Plot(df) .facet(\\"severity\\") .pair(x=\\"hour\\", y=\\"count\\") .add(so.Paths(linewidth=2), so.Aggregate()) .layout(size=(10, 6)) .share(x=True, y=True) ) # Customizations p.add(so.Paths(), color=\\"severity\\") p.plot() ``` *Note: The example above is a starting point. You need to implement the correct steps to meet all the requirements.* --- # Constraints: - Ensure your solution handles missing data appropriately. - The plot should be clear and readable with appropriately sized labels and legend. # Submission: Submit a well-commented Jupyter notebook (`.ipynb` file) with your complete solution and generated plot.","solution":"# Import necessary packages import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def load_and_prepare_data(file_path): Load the accident data from a CSV file and prepare it for plotting. Parameters: file_path (str): The path to the CSV file containing the accident data. Returns: pd.DataFrame: The prepared DataFrame with an additional \'hour\' column. df = pd.read_csv(file_path) df[\'datetime\'] = pd.to_datetime(df[\'datetime\']) df[\'hour\'] = df[\'datetime\'].dt.hour return df def create_customized_plot(df): Create a customized seaborn plot showing the count of accidents per hour for different severity levels. Parameters: df (pd.DataFrame): The prepared DataFrame containing the accident data. Returns: so.Plot: The customized seaboard Plot object. plot = ( so.Plot(df, x=\\"hour\\", color=\\"severity\\") .add(so.Bar(), so.Count(), alpha=0.7) .layout(size=(10, 6)) .share(x=True, y=True) .scale(color=(\'minor\', \'major\', \'fatal\'), context=\'notebook\') .label(x=\\"Hour of Day\\", y=\\"Count of Accidents\\", title=\\"Accidents Severity by Hour of Day\\") .theme({\'figure.dpi\': 150}) ) return plot def main(file_path): df = load_and_prepare_data(file_path) plot = create_customized_plot(df) plt.figure() plot.plot() plt.show() # Usage of the main function: # main(\'accident_data.csv\')"},{"question":"# Custom Function Implementation and Gradient Check Objective: Implement a custom function in PyTorch and verify its gradient using `torch.autograd.gradcheck`. This exercise will test your understanding of automatic differentiation, both in the real and complex domains, and the use of PyTorch\'s `gradcheck` utility. Problem Statement: 1. **Custom Real Function**: - Implement a custom function `f_real` which takes a real-valued input tensor `x` and computes `y = sin(x) * x^2`. - Define the function in both forward and backward mode. - Use `torch.autograd.gradcheck` to verify the gradients of `f_real`. 2. **Custom Complex Function**: - Implement a custom function `f_complex` which takes a complex-valued input tensor `z` and computes `y = (z.real^2 - z.imag^2) + 1j * (2 * z.real * z.imag)`. - Define the function to handle complex inputs appropriately and ensure gradients are correctly propagated. - Use `torch.autograd.gradcheck` to verify the gradients of `f_complex`. Instructions: 1. **Input and Output Formats**: - `f_real(x: torch.Tensor) -> torch.Tensor`: Input is a real-valued tensor of any shape, output is a tensor of the same shape. - `f_complex(z: torch.Tensor) -> torch.Tensor`: Input is a complex-valued tensor of any shape, output is a tensor of the same shape but with potentially complex numbers. 2. **Constraints**: - Use only PyTorch operations and ensure compatibility with `torch.autograd.gradcheck`. - Ensure the input tensors require gradients (`requires_grad=True`). 3. **Performance Requirements**: - Functions should be efficient and handle evaluation within reasonable time limits. - Gradcheck verification should pass for random inputs within standard floating-point precision limits. Example Implementation: ```python import torch from torch.autograd import Function class FReal(Function): @staticmethod def forward(ctx, x): ctx.save_for_backward(x) return torch.sin(x) * x ** 2 @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors grad_x = grad_output * (2 * x * torch.sin(x) + x ** 2 * torch.cos(x)) return grad_x class FComplex(Function): @staticmethod def forward(ctx, z): real_part = z.real**2 - z.imag**2 imag_part = 2 * z.real * z.imag ctx.save_for_backward(z) return torch.complex(real_part, imag_part) @staticmethod def backward(ctx, grad_output): z, = ctx.saved_tensors grad_z = grad_output * torch.complex(2 * z.real, 2 * z.imag) return grad_z # Verification using gradcheck def main(): real_input = torch.randn(5, 5, dtype=torch.double, requires_grad=True) complex_input = torch.randn(5, 5, dtype=torch.cdouble, requires_grad=True) # Real function gradcheck gradcheck_pass = torch.autograd.gradcheck(FReal.apply, (real_input,)) print(f\'Gradcheck for f_real: {gradcheck_pass}\') # Complex function gradcheck gradcheck_pass = torch.autograd.gradcheck(FComplex.apply, (complex_input,)) print(f\'Gradcheck for f_complex: {gradcheck_pass}\') if __name__ == \'__main__\': main() ``` In this exercise, you should implement similar custom functions `f_real` and `f_complex`, and ensure their gradients pass the `gradcheck` test in PyTorch.","solution":"import torch from torch.autograd import Function class FReal(Function): @staticmethod def forward(ctx, x): ctx.save_for_backward(x) return torch.sin(x) * x ** 2 @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors grad_x = grad_output * (2 * x * torch.sin(x) + x ** 2 * torch.cos(x)) return grad_x class FComplex(Function): @staticmethod def forward(ctx, z): real_part = z.real**2 - z.imag**2 imag_part = 2 * z.real * z.imag ctx.save_for_backward(z) return torch.complex(real_part, imag_part) @staticmethod def backward(ctx, grad_output): z, = ctx.saved_tensors grad_z_real = 2 * z.real * grad_output.real + 2 * z.imag * grad_output.imag grad_z_imag = -2 * z.imag * grad_output.real + 2 * z.real * grad_output.imag grad_z = torch.complex(grad_z_real, grad_z_imag) return grad_z def f_real(x): return FReal.apply(x) def f_complex(z): return FComplex.apply(z) # Function to verify gradients using gradcheck def verify_gradients(): real_input = torch.randn(5, 5, dtype=torch.double, requires_grad=True) complex_input = torch.randn(5, 5, dtype=torch.cdouble, requires_grad=True) real_gradok = torch.autograd.gradcheck(f_real, (real_input,)) complex_gradok = torch.autograd.gradcheck(f_complex, (complex_input,)) return real_gradok, complex_gradok if __name__ == \'__main__\': real_ok, complex_ok = verify_gradients() print(f\'Real gradient check passed: {real_ok}\') print(f\'Complex gradient check passed: {complex_ok}\')"},{"question":"# Seaborn Coding Assessment Question **Objective:** To assess your knowledge and understanding of the seaborn `cubehelix_palette` function and its various parameters, this exercise involves writing a function to generate and visualize several customized palettes based on specific criteria. **Problem Statement:** You are asked to write a Python function named `generate_custom_palettes` that generates four different seaborn cubehelix palettes with the following specifications: 1. **Palette 1:** - Number of colors: 6 - Start: 0 - Rotations: 0.5 - Gamma: 1 - Hue: 0.8 - Dark: 0.1 - Light: 0.9 2. **Palette 2:** - Number of colors: 8 - Start: 2 - Rotations: -0.3 - Gamma: 0.75 - Hue: 0.5 - Dark: 0.2 - Light: 0.8 3. **Palette 3:** - Number of colors: 5 - Start: 3 - Rotations: 0.2 - Gamma: 2 - Hue: 1 - Dark: 0.0 - Light: 1.0 4. **Palette 4:** - Number of colors: 10 - Start: 1 - Rotations: -0.7 - Gamma: 1.5 - Hue: 0.3 - Dark: 0.5 - Light: 0.75 - Reversed direction The function should create and display the palettes using seaborn’s `palplot` function and return a dictionary containing the four palettes. **Input:** There are no input parameters for this function. **Output:** A dictionary where the keys are \\"Palette 1\\", \\"Palette 2\\", \\"Palette 3\\", and \\"Palette 4\\", and the values are the corresponding seaborn color palettes. ```python def generate_custom_palettes() -> dict: # Your code here return palettes ``` **Constraints:** - You must use seaborn\'s `cubehelix_palette` function to generate the palettes. - Use seaborn\'s `palplot` function to display each generated palette. **Example:** ```python palettes = generate_custom_palettes() # This should create and show four different palettes as described and return a dictionary of them. ``` # Notes: - Ensure that seaborn and matplotlib are installed in your Python environment. - Test the function to visualize the palettes correctly. - Do not modify the function signature.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_custom_palettes(): palettes = { \\"Palette 1\\": sns.cubehelix_palette(n_colors=6, start=0, rot=0.5, gamma=1, hue=0.8, dark=0.1, light=0.9), \\"Palette 2\\": sns.cubehelix_palette(n_colors=8, start=2, rot=-0.3, gamma=0.75, hue=0.5, dark=0.2, light=0.8), \\"Palette 3\\": sns.cubehelix_palette(n_colors=5, start=3, rot=0.2, gamma=2, hue=1, dark=0.0, light=1.0), \\"Palette 4\\": sns.cubehelix_palette(n_colors=10, start=1, rot=-0.7, gamma=1.5, hue=0.3, dark=0.5, light=0.75, reverse=True) } for key, palette in palettes.items(): sns.palplot(palette) plt.title(key) plt.show() return palettes"},{"question":"You are responsible for creating a custom Python environment setup script that ensures your development environment has specific directories and modules available. Your task is to write a Python function that uses the `site` module to achieve the following: 1. Print the current global site-packages directories. 2. Print the current user base directory. 3. Print the current user-specific site-packages directory. 4. Add a custom directory to the `sys.path` and ensure that it is included for future Python sessions. 5. Verify if a specific custom directory exists within the `sys.path`; if not, add it, else, skip. Function Signature: ```python def setup_custom_environment(custom_dir: str, verify_dir: str) -> None: pass ``` Input: - `custom_dir` (str): A custom directory path that you want to add to `sys.path`. - `verify_dir` (str): A directory path that you want to check if it exists in `sys.path`. Output: - The function should not return anything but should print: - The list of current global site-packages directories. - The current user base directory. - The current user-specific site-packages directory. - A confirmation message after successfully adding `custom_dir` to `sys.path`. - A message indicating whether `verify_dir` exists in `sys.path` or not. Constraints: - You should not remove any existing paths from `sys.path`. - The function should maintain the order of `sys.path`. Example: ```python setup_custom_environment(\\"/path/to/custom_dir\\", \\"/path/to/verify_dir\\") ``` Expected Output: ``` Global site-packages directories: [...] User base directory: /home/user/.local User-specific site-packages directory: /home/user/.local/lib/python3.10/site-packages Custom directory \'/path/to/custom_dir\' added to sys.path. Directory \'/path/to/verify_dir\' exists in sys.path. ``` Use the functionalities provided by the `site` module for the implementation. Ensure that the path manipulations are accurate, and the function behavior is as described.","solution":"import site import sys def setup_custom_environment(custom_dir: str, verify_dir: str) -> None: Set up a custom Python environment by manipulating sys.path and leveraging the site module. :param custom_dir: A custom directory path to add to sys.path. :param verify_dir: A directory path to check if it exists in sys.path. :return: None # Print the list of current global site-packages directories global_site_packages = site.getsitepackages() print(f\\"Global site-packages directories: {global_site_packages}\\") # Print the current user base directory user_base_dir = site.getusersitepackages() print(f\\"User base directory: {site.getuserbase()}\\") # Print the current user-specific site-packages directory print(f\\"User-specific site-packages directory: {user_base_dir}\\") # Add custom directory to sys.path if it\'s not there if custom_dir not in sys.path: sys.path.append(custom_dir) print(f\\"Custom directory \'{custom_dir}\' added to sys.path.\\") else: print(f\\"Custom directory \'{custom_dir}\' is already in sys.path.\\") # Verify if the verify_dir is in sys.path if verify_dir in sys.path: print(f\\"Directory \'{verify_dir}\' exists in sys.path.\\") else: print(f\\"Directory \'{verify_dir}\' does not exist in sys.path, so it has been added.\\") sys.path.append(verify_dir)"},{"question":"Comprehensive Date and Time Processing **Objective:** Design and implement a Python function that processes date and time information encapsulated in a custom data structure, providing comprehensive manipulation and querying capabilities using the `datetime` module. **Task:** You need to create a class named `DateTimeProcessor` which will handle various operations on datetime objects. The class should manage a collection of datetime objects and provide functionalities to add, remove, and query these objects based on specific criteria. This will involve creating new datetime objects, checking their types, extracting fields, and performing date-time arithmetic. **Requirements:** 1. **Class Definition:** ```python class DateTimeProcessor: def __init__(self): # Initializes an empty list to store datetime objects pass def add_date(self, year: int, month: int, day: int): Adds a new date object to the collection. pass def add_datetime(self, year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int = 0): Adds a new datetime object to the collection. pass def add_timedelta(self, days: int, seconds: int = 0, microseconds: int = 0): Adds a new timedelta object to the collection. pass def remove_date(self, year: int, month: int, day: int): Removes a date object from the collection if it exists. pass def get_all_dates(self) -> list: Returns a list of all stored date objects. pass def filter_dates_by_year(self, year: int) -> list: Returns a list of all dates in the collection that match the given year. pass def get_date_differences(self) -> list: Returns a list of differences between each consecutive pair of dates in the collection. pass def get_date_info(self, year: int, month: int, day: int) -> dict: Returns year, month, day, and weekday information for the specified date. pass ``` 2. **Input and Output Format:** - **add_date:** - *Input:* Integers representing `year`, `month`, and `day`. - *Output:* None. - **add_datetime:** - *Input:* Integers representing `year`, `month`, `day`, `hour`, `minute`, `second`, and optional `microsecond`. - *Output:* None. - **add_timedelta:** - *Input:* Integers representing `days`, and optionally `seconds` and `microseconds`. - *Output:* None. - **remove_date:** - *Input:* Integers representing `year`, `month`, and `day`. - *Output:* None. - **get_all_dates:** - *Input:* None. - *Output:* List of all date objects. - **filter_dates_by_year:** - *Input:* Integer representing `year`. - *Output:* List of date objects matching the given year. - **get_date_differences:** - *Input:* None. - *Output:* List of `timedelta` objects representing differences between consecutive dates. - **get_date_info:** - *Input:* Integers representing `year`, `month`, and `day`. - *Output:* Dictionary containing the `year`, `month`, `day`, and `weekday` information. 3. **Constraints:** - The `year` provided should be a positive integer. - The `month` should be an integer from 1 to 12. - The `day` should be an integer from 1 to 31 (depending on the month and leap year). - The `hour` should be an integer from 0 to 23. - The `minute` should be an integer from 0 to 59. - The `second` should be an integer from 0 to 59. - The `microsecond` should be an integer from 0 to 999999. **Performance Requirements:** - The class methods should efficiently handle the insertion, deletion, and querying of datetime objects. - The `get_date_differences` method should calculate differences in O(n) time complexity. **Notes:** - Utilize the `datetime` module for creating and manipulating datetime objects. - Ensure proper error handling for invalid dates. - Provide meaningful docstrings and comments within your code for clarity.","solution":"from datetime import datetime, date, timedelta class DateTimeProcessor: def __init__(self): Initializes an empty list to store datetime objects self.dates = [] def add_date(self, year: int, month: int, day: int): Adds a new date object to the collection. new_date = date(year, month, day) self.dates.append(new_date) def add_datetime(self, year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int = 0): Adds a new datetime object to the collection. new_datetime = datetime(year, month, day, hour, minute, second, microsecond) self.dates.append(new_datetime) def add_timedelta(self, days: int, seconds: int = 0, microseconds: int = 0): Adds a new timedelta object to the collection. new_timedelta = timedelta(days=days, seconds=seconds, microseconds=microseconds) self.dates.append(new_timedelta) def remove_date(self, year: int, month: int, day: int): Removes a date object from the collection if it exists. try: self.dates.remove(date(year, month, day)) except ValueError: pass def get_all_dates(self) -> list: Returns a list of all stored date objects. return [d for d in self.dates if isinstance(d, (date, datetime))] def filter_dates_by_year(self, year: int) -> list: Returns a list of all dates in the collection that match the given year. return [d for d in self.get_all_dates() if d.year == year] def get_date_differences(self) -> list: Returns a list of differences between each consecutive pair of dates in the collection. sorted_dates = sorted(self.get_all_dates()) differences = [] for i in range(len(sorted_dates) - 1): diff = sorted_dates[i + 1] - sorted_dates[i] differences.append(diff) return differences def get_date_info(self, year: int, month: int, day: int) -> dict: Returns year, month, day, and weekday information for the specified date. target_date = date(year, month, day) date_info = { \\"year\\": target_date.year, \\"month\\": target_date.month, \\"day\\": target_date.day, \\"weekday\\": target_date.strftime(\\"%A\\") } return date_info"},{"question":"Objective: Design a function that reads a configuration file, processes its contents, and generates a new configuration file based on specific criteria. Problem Statement: You are provided with a configuration file in INI format containing various settings. You need to write a Python function `transform_config(input_filepath: str, output_filepath: str) -> None` that reads the given configuration file, modifies its values based on certain rules, and writes the modified settings to a new configuration file. Rules for Modification: 1. Increment all integer values by 10. 2. Append the string \\"_processed\\" to all string values. 3. For boolean values (`True` or `False`), toggle their states. 4. If a section named `[DEBUG]` exists, remove it from the configuration. 5. Add a new section `[Processed]` with a single key-value pair `status=complete`. Input: - `input_filepath`: A string representing the file path to the input configuration file. - `output_filepath`: A string representing the file path where the modified configuration file will be saved. Output: - None Example: **Input Configuration File (`input.ini`):** ```ini [General] timeout = 30 username = user1 enable_feature = True [Database] port = 5432 hostname = localhost [DEBUG] log_level = info ``` **Output Configuration File (`output.ini`):** ```ini [General] timeout = 40 username = user1_processed enable_feature = False [Database] port = 5442 hostname = localhost_processed [Processed] status = complete ``` Constraints: - Only standard libraries are allowed. - Assume the input file is correctly formatted. Performance Requirement: - The function should efficiently handle configuration files up to 1 MB in size. # Implementation: ```python import configparser def transform_config(input_filepath: str, output_filepath: str) -> None: config = configparser.ConfigParser() config.read(input_filepath) for section in config.sections(): if section == \'DEBUG\': config.remove_section(section) continue for key in config[section]: value = config[section][key] if value.isdigit(): config[section][key] = str(int(value) + 10) elif value.lower() in (\'true\', \'false\'): config[section][key] = \'False\' if value == \'True\' else \'True\' else: config[section][key] = value + \'_processed\' config.add_section(\'Processed\') config[\'Processed\'][\'status\'] = \'complete\' with open(output_filepath, \'w\') as configfile: config.write(configfile) # Example usage transform_config(\'input.ini\', \'output.ini\') ``` Complete the implementation of the `transform_config` function and test it using the provided example. Ensure that all modification rules are correctly applied.","solution":"import configparser def transform_config(input_filepath: str, output_filepath: str) -> None: Reads the given configuration file, modifies its values based on certain rules, and writes the modified settings to a new configuration file. config = configparser.ConfigParser() config.read(input_filepath) for section in config.sections(): if section == \'DEBUG\': config.remove_section(section) continue for key in config[section]: value = config[section][key] if value.isdigit(): config[section][key] = str(int(value) + 10) elif value.lower() in (\'true\', \'false\'): config[section][key] = \'False\' if value == \'True\' else \'True\' else: config[section][key] = value + \'_processed\' config.add_section(\'Processed\') config[\'Processed\'][\'status\'] = \'complete\' with open(output_filepath, \'w\') as configfile: config.write(configfile)"},{"question":"# Question: Visualizing Penguin Dataset In this task, you are required to use the Seaborn library to create various visualizations of the Palmer Penguins dataset. This dataset contains the following columns: `species`, `island`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`, `sex`, and `year`. Your tasks are as follows: 1. **ECDF Plot for Flipper Length:** Create an empirical cumulative distribution function (ECDF) plot for the `flipper_length_mm` column. 2. **ECDF Plot with Hue Mapping:** Generate an ECDF plot for the `bill_length_mm` column, using the `species` column for hue mapping to differentiate the distribution for each species. 3. **Histogram for Bill Dimensions:** Plot histograms for the columns that contain `bill_` in their names (i.e., `bill_length_mm` and `bill_depth_mm`). Ensure that the histograms are displayed side by side for comparison. 4. **ECDF Plot with Alternative Statistics:** Create an ECDF plot of the `body_mass_g` column, but display the absolute counts instead of the normalized proportions. 5. **Complementary ECDF for Bill Length:** Plot the complementary empirical cumulative distribution function (1 - CDF) for the `bill_length_mm` column. **Constraints:** - You should use the Seaborn library for all visualizations. - Ensure that the plots are appropriately labeled with titles and axis labels. **Expected Input and Output Format:** - Input: The Palmer Penguins dataset as provided by the Seaborn library. - Output: Five plots as specified above. **Example Code to Load Dataset:** ```python import seaborn as sns sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") ``` Good luck, and happy plotting!","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") # Task 1: ECDF Plot for Flipper Length def plot_ecdf_flipper_length(): plt.figure() sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\'ECDF for Flipper Length (mm)\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'ECDF\') plt.show() # Task 2: ECDF Plot with Hue Mapping def plot_ecdf_bill_length_with_hue(): plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\'ECDF for Bill Length (mm) by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'ECDF\') plt.show() # Task 3: Histogram for Bill Dimensions def plot_histograms_bill_dimensions(): fig, axes = plt.subplots(1, 2, figsize=(14, 5)) sns.histplot(data=penguins, x=\\"bill_length_mm\\", ax=axes[0]) axes[0].set_title(\'Histogram for Bill Length (mm)\') sns.histplot(data=penguins, x=\\"bill_depth_mm\\", ax=axes[1]) axes[1].set_title(\'Histogram for Bill Depth (mm)\') axes[0].set_xlabel(\'Bill Length (mm)\') axes[0].set_ylabel(\'Count\') axes[1].set_xlabel(\'Bill Depth (mm)\') axes[1].set_ylabel(\'Count\') plt.show() # Task 4: ECDF Plot with Alternative Statistics def plot_ecdf_body_mass(): plt.figure() sns.ecdfplot(data=penguins, x=\\"body_mass_g\\", stat=\\"count\\") plt.title(\'ECDF for Body Mass (g)\') plt.xlabel(\'Body Mass (g)\') plt.ylabel(\'Count\') plt.show() # Task 5: Complementary ECDF for Bill Length def plot_complementary_ecdf_bill_length(): plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", complementary=True) plt.title(\'Complementary ECDF for Bill Length (mm)\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'1 - ECDF\') plt.show()"},{"question":"# Python Coding Assessment Question Objective: Demonstrate your understanding of Python\'s `time` module by implementing a function that performs the following tasks. Problem Statement: **Function Name:** `system_time_analysis` **Function Definition:** ```python def system_time_analysis(): pass ``` **Description:** Implement the `system_time_analysis` function to perform the following tasks: 1. Retrieve and print the current system time in the following format: \\"YYYY-MM-DD HH:MM:SS\\". Use the `strftime` function for formatting. 2. Sleep for precisely 1.5 seconds. Use the `sleep` function from the `time` module. 3. After waking from sleep, measure the elapsed time in high-resolution using the `perf_counter` function. 4. Convert the current time to UTC and print it in the format: \\"\'Sun Jun 20 23:21:05 1993\'\\" 5. Convert the current system time to a timestamp, then convert it back to local time and verify it matches the original system time. Print the verification result as \\"Match\\" or \\"Mismatch\\". 6. Retrieve the thread-specific CPU time before and after performing the above operations and print the difference in CPU time spent. **Constraints:** - Ensure that the formatting and conversions are done correctly. - The system time should be handled in the local timezone. - Properly handle leap seconds if encountered. **Example:** ```python system_time_analysis() # Sample output: # Current Time: \\"2023-12-01 12:34:56\\" # Slept for: 1.5 seconds # High Resolution Elapsed Time: 1.500123456 seconds # Current Time in UTC: \\"Sun Jun 20 23:21:05 1993\\" # Time Verification: Match # CPU Time Spent: 0.0004567 seconds ``` Notes: - This function should not take any arguments and should print the results directly. - Thoroughly test the function to ensure all specified outputs are correct. - Using the high-resolution `perf_counter`, ensure you account for different systems\' precision and note any discrepancies. **Performance Requirements:** - The sleep duration should be as close to 1.5 seconds as possible. - High-resolution timing should be accurate to the microsecond level. - The entire function should run in a reasonable time frame considering the 1.5-second sleep. Tip: Refer to the `time` module documentation for detailed function usage and examples.","solution":"import time def system_time_analysis(): # Retrieve and print the current system time in the format \\"YYYY-MM-DD HH:MM:SS\\". current_time = time.localtime() formatted_time = time.strftime(\\"%Y-%m-%d %H:%M:%S\\", current_time) print(\\"Current Time:\\", formatted_time) # Sleep for precisely 1.5 seconds. time.sleep(1.5) # Measure the elapsed time in high-resolution using the perf_counter function. start_perf = time.perf_counter() time.sleep(1.5) # Intentionally sleep again to create a measurable elapsed time. end_perf = time.perf_counter() high_res_elapsed_time = end_perf - start_perf print(\\"High Resolution Elapsed Time: {:.9f} seconds\\".format(high_res_elapsed_time)) # Convert the current time to UTC and print in the format \\"\'Sun Jun 20 23:21:05 1993\'\\" utc_time = time.gmtime() formatted_utc_time = time.strftime(\\"%a %b %d %H:%M:%S %Y\\", utc_time) print(\\"Current Time in UTC:\\", formatted_utc_time) # Convert the current system time to a timestamp, then convert back to local time and verify it matches. timestamp = time.mktime(current_time) verified_time = time.localtime(timestamp) verification_result = \\"Match\\" if current_time == verified_time else \\"Mismatch\\" print(\\"Time Verification:\\", verification_result) # Retrieve the thread-specific CPU time before and after operations and print the difference. cpu_time_before = time.process_time() # Perform the same sleep operation to simulate process time usage. time.sleep(1.5) cpu_time_after = time.process_time() cpu_time_spent = cpu_time_after - cpu_time_before print(\\"CPU Time Spent: {:.7f} seconds\\".format(cpu_time_spent))"},{"question":"Objective: Write a Python function to perform multiple Unicode-related tasks that demonstrate your understanding of Unicode string handling, encoding/decoding, and normalization. Task 1: Normalizing Unicode Strings - Implement a function `normalize_unicode` that accepts a string and returns the string in a specific Unicode normalization form. Use the `unicodedata` module. Task 2: Encoding and Decoding - Implement functions `encode_string` and `decode_bytes`: - `encode_string` takes a Unicode string and an encoding type (\'utf-8\', \'ascii\', etc.) and returns the encoded bytes. - `decode_bytes` takes encoded bytes and an encoding type and returns the decoded Unicode string. Task 3: Unicode Comparison - Implement a function `compare_unicode` that takes two Unicode strings and compares them by normalizing both to a specific normalization form and making a casefolded comparison. Task 4: File I/O with Unicode - Create functions `write_unicode_to_file` and `read_unicode_from_file`: - `write_unicode_to_file` should take a filename, a Unicode string, and an encoding type, then write the string to the file using the specified encoding. - `read_unicode_from_file` should take a filename and an encoding type, then read from the file and return the decoded Unicode string. # Detailed Specifications: 1. **Function: `normalize_unicode`** ```python def normalize_unicode(input_str: str, form: str = \'NFC\') -> str: Normalize a Unicode string to the specified form. Args: input_str (str): The input Unicode string. form (str): The Unicode normalization form (\'NFC\', \'NFKC\', \'NFD\', \'NFKD\'). Returns: str: The normalized Unicode string. pass ``` 2. **Function: `encode_string` and `decode_bytes`** ```python def encode_string(input_str: str, encoding: str = \'utf-8\') -> bytes: Encode a Unicode string into the specified encoding. Args: input_str (str): The Unicode input string. encoding (str): The encoding type (default \'utf-8\'). Returns: bytes: The encoded byte representation. pass def decode_bytes(encoded_bytes: bytes, encoding: str = \'utf-8\') -> str: Decode bytes into a Unicode string using the specified encoding. Args: encoded_bytes (bytes): The encoded byte representation. encoding (str): The encoding type (default \'utf-8\'). Returns: str: The decoded Unicode string. pass ``` 3. **Function: `compare_unicode`** ```python def compare_unicode(str1: str, str2: str, form: str = \'NFC\') -> bool: Compare two Unicode strings by normalizing them and casefolding. Args: str1 (str): The first Unicode string. str2 (str): The second Unicode string. form (str): The normalization form (default \'NFC\'). Returns: bool: True if the normalized and casefolded strings are equal, False otherwise. pass ``` 4. **Function: `write_unicode_to_file` and `read_unicode_from_file`** ```python def write_unicode_to_file(filename: str, content: str, encoding: str = \'utf-8\') -> None: Write a Unicode string to a file with the specified encoding. Args: filename (str): The name of the file. content (str): The Unicode string to write. encoding (str): The encoding type (default \'utf-8\'). pass def read_unicode_from_file(filename: str, encoding: str = \'utf-8\') -> str: Read a Unicode string from a file with the specified encoding. Args: filename (str): The name of the file. encoding (str): The encoding type (default \'utf-8\'). Returns: str: The read Unicode string. pass ``` # Constraints: 1. Ensure correct handling of Unicode normalization forms (`NFC`, `NFKC`, `NFD`, `NFKD`). 2. Handle both strict encoding and decoding, and provide options for error handling (`\'strict\'`, `\'ignore\'`, `\'replace\'`, `\'backslashreplace\'`, etc.). 3. Perform casefolding consistently based on Unicode specifications for valid comparisons. 4. Handle file I/O robustly, ensuring proper handling of encodings and BOM where applicable. # Performance Requirement: Ensure that the implementation is efficient and can handle large UTF-8-encoded files robustly without unnecessary memory overhead. # Example Usage: ```python # Example for normalization print(normalize_unicode(\\"ê\\", \\"NFD\\")) # Output: \'eu0302\' # Example for encoding and decoding encoded = encode_string(\\"Hello, Unicode!\\", \\"utf-8\\") print(encoded) # Output: b\'Hello, Unicode!\' print(decode_bytes(encoded, \\"utf-8\\")) # Output: \'Hello, Unicode!\' # Example for comparison print(compare_unicode(\\"ê\\", \\"u0065u0302\\", \\"NFD\\")) # Output: True # Example for file I/O write_unicode_to_file(\\"test_file.txt\\", \\"This is a Unicode test.\\", \\"utf-8\\") print(read_unicode_from_file(\\"test_file.txt\\", \\"utf-8\\")) # Output: \'This is a Unicode test.\' ```","solution":"import unicodedata def normalize_unicode(input_str: str, form: str = \'NFC\') -> str: return unicodedata.normalize(form, input_str) def encode_string(input_str: str, encoding: str = \'utf-8\') -> bytes: return input_str.encode(encoding) def decode_bytes(encoded_bytes: bytes, encoding: str = \'utf-8\') -> str: return encoded_bytes.decode(encoding) def compare_unicode(str1: str, str2: str, form: str = \'NFC\') -> bool: normalized_str1 = unicodedata.normalize(form, str1).casefold() normalized_str2 = unicodedata.normalize(form, str2).casefold() return normalized_str1 == normalized_str2 def write_unicode_to_file(filename: str, content: str, encoding: str = \'utf-8\') -> None: with open(filename, \'w\', encoding=encoding) as file: file.write(content) def read_unicode_from_file(filename: str, encoding: str = \'utf-8\') -> str: with open(filename, \'r\', encoding=encoding) as file: return file.read()"},{"question":"**Objective**: This task will assess your ability to handle different types of data formats and create visualizations using the seaborn library. Problem Statement You are given a dataset from a memory task experiment. Your goal is to analyze and visualize the data using seaborn. You will need to: 1. Load the dataset. 2. Transform the data from a messy format to a tidy long-form format. 3. Create visualizations based on the tidy data. Dataset The dataset `memory_task.csv` contains the following columns: - `subject`: Unique identifier for each subject. - `attention`: Between-subjects variable (either \'divided\' or \'focused\'). - `sol1`, `sol2`, `sol3`: Scores for memory performance with 1, 2, and 3 possible solutions respectively. Tasks 1. **Load the dataset** using pandas: ```python import pandas as pd data = pd.read_csv(\\"memory_task.csv\\") ``` 2. **Convert the data to tidy long-form format** using the pandas `melt()` function. The resulting DataFrame should have the columns `subject`, `attention`, `solutions`, and `score`. The `solutions` column should contain the number of solutions (1, 2, or 3), and the `score` column should contain the corresponding scores. 3. **Create a point plot** using seaborn to visualize the average score as a function of the number of solutions and attention: - Use `solutions` as the x-axis variable. - Use `score` as the y-axis variable. - Use `attention` as the hue variable. 4. **Create a line plot** using seaborn to show the trend of scores over solutions for each subject, with different lines for each attention condition: - Use `solutions` as the x-axis variable. - Use `score` as the y-axis variable. - Use `subject` as the style variable to differentiate lines for each subject. - Use `attention` as the hue variable. Constraints - Do not use libraries other than `pandas` and `seaborn` for this task. - Ensure your code is optimized for clarity and efficiency. Expected Input and Output Formats **Input**: - A CSV file named `memory_task.csv`. **Output**: - A transformed pandas DataFrame in long-form format. - Two seaborn plots: a point plot and a line plot. Example Here is an example of how to perform the transformation: ```python # Example dataframe data = pd.DataFrame({ \'subject\': [1, 2, 1, 2], \'attention\': [\'divided\', \'focused\', \'divided\', \'focused\'], \'sol1\': [0.8, 0.9, 0.7, 0.85], \'sol2\': [0.6, 0.7, 0.55, 0.75], \'sol3\': [0.5, 0.6, 0.45, 0.65] }) # Melting the dataframe to long-form tidy_data = data.melt(id_vars=[\'subject\', \'attention\'], var_name=\'solutions\', value_name=\'score\') ``` Using the tidy data, create the required visualizations. Your final output should include the code for transforming the data and the code for creating the plots.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_and_transform_data(filepath): Load the dataset and convert it to tidy long-form. Args: - filepath (str): Path to the memory_task.csv file Returns: - pd.DataFrame: Transformed tidy DataFrame # Load dataset data = pd.read_csv(filepath) # Transform the data to tidy long-form tidy_data = data.melt(id_vars=[\'subject\', \'attention\'], var_name=\'solutions\', value_name=\'score\') # Convert \'solutions\' from string (e.g., \'sol1\') to integer (e.g., 1) tidy_data[\'solutions\'] = tidy_data[\'solutions\'].str.extract(\'(d)\').astype(int) return tidy_data def create_point_plot(tidy_data): Create a point plot using the tidy data. Args: - tidy_data (pd.DataFrame): The tidy DataFrame Output: - Point plot displayed sns.pointplot(data=tidy_data, x=\'solutions\', y=\'score\', hue=\'attention\', ci=\'sd\') plt.title(\'Average Score as a Function of Solutions and Attention\') plt.show() def create_line_plot(tidy_data): Create a line plot using the tidy data. Args: - tidy_data (pd.DataFrame): The tidy DataFrame Output: - Line plot displayed sns.lineplot(data=tidy_data, x=\'solutions\', y=\'score\', hue=\'attention\', style=\'subject\', markers=True, dashes=False) plt.title(\'Trends of Scores Over Solutions for Each Subject\') plt.show()"},{"question":"# Unicode Manipulation and Encoding in Python Objective: Write a Python function that demonstrates the understanding of Unicode manipulations, encoding conversions, and proper handling of different Unicode representations and properties. Task: You are required to design and implement a Python function that achieves the following: 1. **Accepts a Unicode string and provides various representations:** - The original Unicode representation. - UTF-8 encoding of the string. - UTF-16 encoding of the string. - Hexadecimal representation of each character in the string. 2. **Counts specific Unicode properties within the string:** - The number of alphabetic characters. - The number of digit characters. - The number of whitespace characters. 3. **Transforms the string:** - Converts all characters to their lowercase. - Converts all characters to their uppercase. - Replaces all digits in the string with their corresponding words (for example, \'1\' becomes \'one\'). 4. **Returns all these results in a structured dictionary.** Function Signature: ```python def unicode_manipulation(unicode_str: str) -> dict: pass ``` Expected Input and Output: - **Input:** - `unicode_str` (Type: str): A string containing Unicode characters. - **Output:** - A dictionary with the following keys: - `\'original\'`: The original Unicode string. - `\'utf8\'`: UTF-8 encoded bytes representation of the string. - `\'utf16\'`: UTF-16 encoded bytes representation of the string. - `\'hex\'`: List of hexadecimal representations of characters in the string. - `\'alphabetic_count\'`: Number of alphabetic characters in the string. - `\'digit_count\'`: Number of digit characters in the string. - `\'whitespace_count\'`: Number of whitespace characters in the string. - `\'lower_case\'`: The string converted to lower case. - `\'upper_case\'`: The string converted to upper case. - `\'digits_to_words\'`: The string with all digit characters replaced by their word equivalents. Constraints: - Implement proper error handling for encoding operations. - Assume the input will be a valid Unicode string. - Use Python\'s built-in libraries and functions to achieve the task. Example: ```python input_str = \\"Hello World! 123\\" result = unicode_manipulation(input_str) # Example output structure (actual values may vary based on the input string) expected_output = { \'original\': \'Hello World! 123\', \'utf8\': b\'Hello World! 123\', \'utf16\': b\'xffxfeHx00ex00lx00lx00ox00 x00Wx00ox00rx00lx00dx00!x00 x001x002x003x00\', \'hex\': [\'0048\', \'0065\', \'006c\', \'006c\', \'006f\', \'0020\', \'0057\', \'006f\', \'0072\', \'006c\', \'0064\', \'0021\', \'0020\', \'0031\', \'0032\', \'0033\'], \'alphabetic_count\': 10, \'digit_count\': 3, \'whitespace_count\': 2, \'lower_case\': \'hello world! 123\', \'upper_case\': \'HELLO WORLD! 123\', \'digits_to_words\': \'Hello World! onetwothree\' } ``` Notes: - Utilize Python’s built-in `str` methods and Unicode handling features. - Encodings can be handled using the `encode` method of a string. - Using list comprehensions and other Pythonic constructs is encouraged for clarity and efficiency.","solution":"def unicode_manipulation(unicode_str: str) -> dict: Takes a Unicode string and provides various representations, counts specific Unicode properties, transforms the string, and returns a structured dictionary with these results. # UTF-8 and UTF-16 encodings utf8_encoded = unicode_str.encode(\'utf-8\') utf16_encoded = unicode_str.encode(\'utf-16\') # Hexadecimal representation of each character hex_representation = [f\\"{ord(char):04x}\\" for char in unicode_str] # Counting specific Unicode properties alphabetic_count = sum(char.isalpha() for char in unicode_str) digit_count = sum(char.isdigit() for char in unicode_str) whitespace_count = sum(char.isspace() for char in unicode_str) # Converting characters to their lowercase and uppercase lower_case = unicode_str.lower() upper_case = unicode_str.upper() # Replacing digits with corresponding words digit_to_word = { \'0\': \'zero\', \'1\': \'one\', \'2\': \'two\', \'3\': \'three\', \'4\': \'four\', \'5\': \'five\', \'6\': \'six\', \'7\': \'seven\', \'8\': \'eight\', \'9\': \'nine\' } digits_to_words = \'\'.join(digit_to_word[char] if char in digit_to_word else char for char in unicode_str) # Result dictionary result = { \'original\': unicode_str, \'utf8\': utf8_encoded, \'utf16\': utf16_encoded, \'hex\': hex_representation, \'alphabetic_count\': alphabetic_count, \'digit_count\': digit_count, \'whitespace_count\': whitespace_count, \'lower_case\': lower_case, \'upper_case\': upper_case, \'digits_to_words\': digits_to_words } return result"},{"question":"PyTorch Model Optimization for Mobile Objective You are given a pre-trained PyTorch model. Your task is to: 1. Load the model. 2. Optimize the model for mobile using the `torch.utils.mobile_optimizer.optimize_for_mobile` function. 3. Evaluate the original and optimized models to demonstrate the improvement in performance (e.g., inference speed). Task Implement a Python function `optimize_mobile_model(model_path: str, optimization_level: list) -> torch.jit.ScriptModule` that: 1. Loads a pre-trained `torch.jit.ScriptModule` model from the file specified by `model_path`. 2. Applies the specified optimizations from the `optimization_level` list. 3. Returns the optimized `torch.jit.ScriptModule`. Input - `model_path` (str): The file path to the pre-trained `torch.jit.ScriptModule` model. - `optimization_level` (list): A list of optimizations to apply. Possible values in this list can be: - `\\"CONV_BN_FUSION\\"` - `\\"INSERT_FOLD_PREPACK_OPS\\"` - `\\"REMOVE_DROPOUT\\"` - `\\"HOIST_CONV_PACKED_PARAMS\\"` - `\\"FUSE_ADD_RELU\\"` Output - Returns the optimized `torch.jit.ScriptModule`. Constraints - Assume the model is in evaluation mode (`model.eval()`). - Ensure to handle the case where the `optimization_level` list is empty or contains invalid optimization levels by applying a default set of optimizations. Example Usage ```python # Example model path and optimizations model_path = \\"path/to/your/model.pt\\" optimization_level = [\\"CONV_BN_FUSION\\", \\"INSERT_FOLD_PREPACK_OPS\\"] # Function call optimized_model = optimize_mobile_model(model_path, optimization_level) # Example: compare performance import time # Load original model original_model = torch.jit.load(model_path) # Dummy input for timing dummy_input = torch.randn(1, 3, 224, 224) # Measure original model inference time start_time = time.time() original_model(dummy_input) original_inference_time = time.time() - start_time # Measure optimized model inference time start_time = time.time() optimized_model(dummy_input) optimized_inference_time = time.time() - start_time print(f\\"Original inference time: {original_inference_time:.6f} seconds\\") print(f\\"Optimized inference time: {optimized_inference_time:.6f} seconds\\") ``` Notes - You may use the `torch.utils.mobile_optimizer.MobileOptimizerType` for specifying the optimization types, e.g., `torch.utils.mobile_optimizer.MobileOptimizerType.CONV_BN_FUSION`. - Ensure your solution handles all possible optimizations mentioned in the documentation.","solution":"import torch from torch.utils.mobile_optimizer import optimize_for_mobile, MobileOptimizerType def optimize_mobile_model(model_path: str, optimization_level: list) -> torch.jit.ScriptModule: Loads a model from the provided path, applies specified mobile optimizations, and returns the optimized model. Args: - model_path (str): The file path to the pre-trained torch.jit.ScriptModule model. - optimization_level (list): A list of optimizations to apply. Returns: - torch.jit.ScriptModule: The optimized model. # Load the pre-trained model model = torch.jit.load(model_path) model.eval() # Convert optimization levels to MobileOptimizerType optimization_types = { \\"CONV_BN_FUSION\\": MobileOptimizerType.CONV_BN_FUSION, \\"INSERT_FOLD_PREPACK_OPS\\": MobileOptimizerType.INSERT_FOLD_PREPACK_OPS, \\"REMOVE_DROPOUT\\": MobileOptimizerType.REMOVE_DROPOUT, \\"HOIST_CONV_PACKED_PARAMS\\": MobileOptimizerType.HOIST_CONV_PACKED_PARAMS, \\"FUSE_ADD_RELU\\": MobileOptimizerType.FUSE_ADD_RELU } # Translate optimization level strings to MobileOptimizerType enums optimizations = [optimization_types[opt] for opt in optimization_level if opt in optimization_types] # Apply default optimizations if no valid optimizations provided if not optimizations: optimizations = [MobileOptimizerType.CONV_BN_FUSION, MobileOptimizerType.REMOVE_DROPOUT] # Optimize the model optimized_model = optimize_for_mobile(model, optimization_blocklist=set(optimizations)) return optimized_model"},{"question":"# Python Object Persistence and Serialization Using `pickle` and `shelve` **Problem Description**: You are required to implement a function that can serialize a given complex Python object to a file and another function that can deserialize the object back from the file. You will use Python\'s `pickle` and `shelve` modules to perform these tasks. Requirements: 1. **Serialization**: - Implement a function `save_object(obj: Any, filename: str) -> None` to serialize the given Python object `obj` and save it to a file with the given `filename`. - The function should use the `pickle` module to perform serialization. 2. **Deserialization**: - Implement a function `load_object(filename: str) -> Any` to deserialize the Python object from the provided `filename`. - The function should retrieve the object and return it. 3. **Using `shelve`**: - Implement a function `save_to_shelve(data: Dict[str, Any], shelve_name: str) -> None` to store a dictionary of objects using the `shelve` module. - Implement a function `load_from_shelve(shelve_name: str) -> Dict[str, Any]` to load and return the dictionary of objects from the shelve database. Input: - For `save_object`, the input parameters will be: - `obj` (any Python object): The object to serialize. - `filename` (str): The name of the file where the object will be serialized. - For `load_object`, the input parameter will be: - `filename` (str): The name of the file from which the object will be deserialized. - For `save_to_shelve`, the input parameters will be: - `data` (Dict[str, Any]): A dictionary containing key-value pairs of objects to store. - `shelve_name` (str): The name of the shelve database file where the objects will be stored. - For `load_from_shelve`, the input parameter will be: - `shelve_name` (str): The name of the shelve database file from which the objects will be loaded. Output: - `save_object` and `save_to_shelve` will have no return values (they should save data to the respective files). - `load_object` should return the deserialized Python object. - `load_from_shelve` should return a dictionary of the loaded objects. Constraints: - Ensure that all file operations handle exceptions gracefully. - Assume that the input objects for serialization are serializable by `pickle`. Performance Requirements: - The solution should efficiently handle the serialization and deserialization of large and nested objects. - Ensure that file operations are performed optimally. Example: ```python import pickle import shelve from typing import Any, Dict def save_object(obj: Any, filename: str) -> None: with open(filename, \'wb\') as file: pickle.dump(obj, file) def load_object(filename: str) -> Any: with open(filename, \'rb\') as file: return pickle.load(file) def save_to_shelve(data: Dict[str, Any], shelve_name: str) -> None: with shelve.open(shelve_name, \'c\') as db: for key, value in data.items(): db[key] = value def load_from_shelve(shelve_name: str) -> Dict[str, Any]: data = {} with shelve.open(shelve_name, \'r\') as db: for key in db: data[key] = db[key] return data # Example usage: complex_object = {\'a\': [1, 2, 3], \'b\': {\'nested\': \'structure\'}} save_object(complex_object, \'my_data.pkl\') loaded_object = load_object(\'my_data.pkl\') assert loaded_object == complex_object data_dict = {\'obj1\': [1, 2, 3], \'obj2\': {\'inner\': \'dict\'}} save_to_shelve(data_dict, \'my_shelve\') loaded_data_dict = load_from_shelve(\'my_shelve\') assert loaded_data_dict == data_dict ```","solution":"import pickle import shelve from typing import Any, Dict def save_object(obj: Any, filename: str) -> None: Serializes the given object and saves it to a file with the specified filename. try: with open(filename, \'wb\') as file: pickle.dump(obj, file) except Exception as e: print(f\\"Error saving object to {filename}: {e}\\") def load_object(filename: str) -> Any: Deserializes the object from the file with the specified filename and returns it. try: with open(filename, \'rb\') as file: return pickle.load(file) except Exception as e: print(f\\"Error loading object from {filename}: {e}\\") def save_to_shelve(data: Dict[str, Any], shelve_name: str) -> None: Stores a dictionary of objects in a shelve database with the specified filename. try: with shelve.open(shelve_name, \'c\') as db: for key, value in data.items(): db[key] = value except Exception as e: print(f\\"Error saving data to shelve {shelve_name}: {e}\\") def load_from_shelve(shelve_name: str) -> Dict[str, Any]: Loads and returns a dictionary of objects from the shelve database with the specified filename. try: data = {} with shelve.open(shelve_name, \'r\') as db: for key in db: data[key] = db[key] return data except Exception as e: print(f\\"Error loading data from shelve {shelve_name}: {e}\\") return {}"},{"question":"# Advanced Caching and Dispatching with functools In this exercise, you\'ll demonstrate your understanding of caching mechanisms and single-dispatch generic functions with Python\'s `functools` module. Objective 1. Implement a function `compute_fibonacci` that efficiently computes Fibonacci numbers with memoization using the `@functools.cache` decorator. 2. Implement a class `Statistics` which computes and caches the results of expensive statistical computations using the `@functools.cached_property`. 3. Implement a generic function `display_info` using `@functools.singledispatch` that handles different data types and a class method `process` using `@functools.singledispatchmethod`. Problem Statement 1. **Fibonacci Calculation**: - Define a function `compute_fibonacci(n: int) -> int` that calculates the nth Fibonacci number. - Use the `@functools.cache` decorator to memoize the function results. - Examples: ```python >>> compute_fibonacci(10) 55 >>> compute_fibonacci(15) 610 ``` 2. **Statistics Class**: - Create a class `Statistics` that accepts a list of numbers. - Implement a method `mean` decorated with `@functools.cached_property` that calculates and caches the mean of the list. - Implement a method `variance` also decorated with `@functools.cached_property` that calculates and caches the variance of the list. - Example: ```python from statistics import Statistics data = [1, 2, 3, 4, 5] stats = Statistics(data) >>> stats.mean 3.0 >>> stats.variance 2.5 ``` 3. **Dispatch Function and Method**: - Write a generic function `display_info` that: - Prints \\"It\'s an integer: [value]\\" if the argument is an integer. - Prints \\"It\'s a string: [value]\\" if the argument is a string. - Prints \\"It\'s a list: [value]\\" if the argument is a list. - Use the `@functools.singledispatch` decorator and the `.register` method to provide specific implementations. - Create a class `Processor` with a method `process` that: - Prints \\"Processing number: [value]\\" if the argument is an integer. - Prints \\"Processing string: [value]\\" if the argument is a string. - Prints \\"Processing data list: [value]\\" if the argument is a list. - Use the `@functools.singledispatchmethod` decorator. - Example: ```python >>> display_info(10) It\'s an integer: 10 >>> display_info(\\"Hello\\") It\'s a string: Hello >>> display_info([1, 2, 3]) It\'s a list: [1, 2, 3] >>> proc = Processor() >>> proc.process(20) Processing number: 20 >>> proc.process(\\"Python\\") Processing string: Python >>> proc.process([\\"A\\", \\"B\\", \\"C\\"]) Processing data list: [\\"A\\", \\"B\\", \\"C\\"] ``` Constraints - The functions you create should be efficient and handle edge cases, such as empty lists and invalid inputs appropriately. - Performance matters, especially for the memoized `compute_fibonacci` and the cached properties of the `Statistics` class. - Adhere to Python\'s best practices for function and variable naming. Submission Submit your Python script containing the complete implementation of `compute_fibonacci`, the `Statistics` class, and the `display_info` generic function along with the `Processor` class. Ensure that your script can be run directly and includes test cases demonstrating the correct behavior of your implementations.","solution":"import functools import math @functools.cache def compute_fibonacci(n: int) -> int: Returns the nth Fibonacci number using memoization for efficiency. if n < 0: raise ValueError(\\"Negative arguments are not allowed.\\") elif n in (0, 1): return n else: return compute_fibonacci(n - 1) + compute_fibonacci(n - 2) class Statistics: def __init__(self, data): self.data = data @functools.cached_property def mean(self): Returns the mean of the data list. if not self.data: raise ValueError(\\"Mean of an empty list is undefined.\\") return sum(self.data) / len(self.data) @functools.cached_property def variance(self): Returns the variance of the data list. if not self.data: raise ValueError(\\"Variance of an empty list is undefined.\\") if len(self.data) == 1: return 0.0 mean = self.mean return sum((x - mean) ** 2 for x in self.data) / (len(self.data) - 1) import functools @functools.singledispatch def display_info(arg): print(f\\"It\'s an unknown type: {arg}\\") @display_info.register def _(arg: int): print(f\\"It\'s an integer: {arg}\\") @display_info.register def _(arg: str): print(f\\"It\'s a string: {arg}\\") @display_info.register def _(arg: list): print(f\\"It\'s a list: {arg}\\") class Processor: def __init__(self): pass @functools.singledispatchmethod def process(self, arg): print(f\\"Processing unknown type: {arg}\\") @process.register(int) def _(self, arg: int): print(f\\"Processing number: {arg}\\") @process.register(str) def _(self, arg: str): print(f\\"Processing string: {arg}\\") @process.register(list) def _(self, arg: list): print(f\\"Processing data list: {arg}\\")"},{"question":"# Question **Concurrency with Python Threading** You are tasked with implementing a simplified multi-threaded simulation of a restaurant with a fixed number of tables and a mix of servers and customers using Python\'s `threading` module. Each table can be used by one customer at a time, and servers must ensure the customers\' orders are taken and processed correctly. Requirements: 1. **Fixed Number of Tables**: The restaurant has a fixed number of tables, say `N`, where `N` is provided as input. 2. **Multiple Customers**: Each customer will wait until a table is available, use the table for a random duration (simulate eating time), and then leave the table. 3. **Servers**: Servers will take orders from customers and ensure tables are marked free after customers finish. You need to implement the following: 1. Create a class `Restaurant` with methods: - `__init__(self, num_tables)`: Initializes the restaurant with a fixed number of tables. - `enter_customer(self)`: Simulates a customer entering the restaurant. This method should block if no table is available. - `leave_customer(self)`: Simulates a customer leaving a table, making it available for others. 2. Create a class `Server` with methods: - `__init__(self, restaurant)`: Initializes the server with a reference to the restaurant instance. - `serve_customer(self)`: Simulates taking an order from a customer and waits for them to finish before cleaning the table and marking it free. 3. Simulate a set number of servers and customers using multiple threads. Use synchronization primitives like `Semaphore`, `Lock`, and `Event` to manage resources and ensure no race conditions occur. Constraints: - Use the `threading` module only. - Make sure the solution handles simultaneous entry and exit of customers while correctly synchronizing access to tables. Expected Input and Output: - **Input**: Number of tables, number of customers. - **Output**: Print statements indicating the status of customers and servers (when a customer enters, eats, and leaves; when a server takes an order and cleans a table). ```python import threading import time import random class Restaurant: def __init__(self, num_tables): Initialize the restaurant with a fixed number of tables. # Your implementation here def enter_customer(self): Simulate a customer entering the restaurant. Blocks if no table is available. # Your implementation here def leave_customer(self): Simulate a customer leaving a table, making it available for others. # Your implementation here class Server: def __init__(self, restaurant): Initialize the server with a reference to the restaurant instance. # Your implementation here def serve_customer(self): Simulate taking an order from a customer and wait for them to finish before cleaning the table and marking it free. # Your implementation here def start_simulation(num_tables, num_customers, num_servers): Start the restaurant simulation with given number of tables, customers, and servers. restaurant = Restaurant(num_tables) threads = [] # Create customer threads for _ in range(num_customers): t = threading.Thread(target=customer_thread, args=(restaurant,)) threads.append(t) t.start() # Create server threads for _ in range(num_servers): server = Server(restaurant) t = threading.Thread(target=server.serve_customer) threads.append(t) t.start() # Wait for all threads to complete for t in threads: t.join() def customer_thread(restaurant): Function representing a customer thread\'s activity. restaurant.enter_customer() eating_duration = random.randint(1, 5) print(f\\"Customer is eating for {eating_duration} seconds.\\") time.sleep(eating_duration) restaurant.leave_customer() print(\\"Customer has left the table.\\") # Example usage if __name__ == \\"__main__\\": start_simulation(num_tables=5, num_customers=10, num_servers=2) ``` Implement the classes and functions above to complete the restaurant simulation.","solution":"import threading import time import random class Restaurant: def __init__(self, num_tables): Initialize the restaurant with a fixed number of tables. self.num_tables = num_tables self.available_tables = num_tables self.lock = threading.Lock() self.table_available = threading.Semaphore(num_tables) def enter_customer(self): Simulate a customer entering the restaurant. Blocks if no table is available. self.table_available.acquire() with self.lock: self.available_tables -= 1 print(f\\"Customer enters the restaurant. Available tables: {self.available_tables}\\") def leave_customer(self): Simulate a customer leaving a table, making it available for others. with self.lock: self.available_tables += 1 print(f\\"Customer leaves the table. Available tables: {self.available_tables}\\") self.table_available.release() class Server: def __init__(self, restaurant): Initialize the server with a reference to the restaurant instance. self.restaurant = restaurant def serve_customer(self): Simulate taking an order from a customer and wait for them to finish before cleaning the table and marking it free. while True: # Adding a delay to simulate the server working in realistic scenario time.sleep(random.randint(1, 3)) print(\\"Server is ready to take an order.\\") # No specific order-handling mechanism required as per task, but it would be implemented here self.restaurant.lock.acquire() if self.restaurant.available_tables < self.restaurant.num_tables: self.restaurant.lock.release() print(\\"Server is cleaning the table.\\") else: self.restaurant.lock.release() break def start_simulation(num_tables, num_customers, num_servers): Start the restaurant simulation with given number of tables, customers, and servers. restaurant = Restaurant(num_tables) threads = [] # Create customer threads for _ in range(num_customers): t = threading.Thread(target=customer_thread, args=(restaurant,)) threads.append(t) t.start() # Create server threads for _ in range(num_servers): server = Server(restaurant) t = threading.Thread(target=server.serve_customer) threads.append(t) t.start() # Wait for all threads to complete for t in threads: t.join() def customer_thread(restaurant): Function representing a customer thread\'s activity. restaurant.enter_customer() eating_duration = random.randint(1, 5) print(f\\"Customer is eating for {eating_duration} seconds.\\") time.sleep(eating_duration) restaurant.leave_customer() print(\\"Customer has left the table.\\") # Example usage if __name__ == \\"__main__\\": start_simulation(num_tables=5, num_customers=10, num_servers=2)"},{"question":"# Objective Demonstrate your understanding of the `seaborn` library, particularly the `sns.blend_palette` function, by creating complex color palettes and applying them to data visualizations. # Question Using the `sns.blend_palette` function, implement a function `create_and_plot_palette` that takes a list of colors, a boolean indicating whether to return a colormap, and a dataset. Your function should do the following: 1. Create a color palette using the `sns.blend_palette` function. 2. Use this palette to generate a heatmap of a dataset. 3. Return the created palette or colormap. The dataset you will work with is the Titanic dataset available through `seaborn`. ```python import seaborn as sns import matplotlib.pyplot as plt def create_and_plot_palette(colors, as_cmap, data): Creates a color palette or colormap using seaborn.blend_palette and plots a heatmap. Parameters: colors (list of str): A list of color codes or names to create the palette. as_cmap (bool): Whether to return a continuous colormap. data (DataFrame): Dataset to visualize using the created color palette. Returns: List of colors or colormap which is used in the heatmap. # Create the color palette or colormap palette = sns.blend_palette(colors, as_cmap=as_cmap) # Plot a heatmap using the created palette plt.figure(figsize=(10, 8)) sns.heatmap(data.corr(), cmap=palette) plt.show() return palette # Example usage titanic = sns.load_dataset(\'titanic\') create_and_plot_palette([\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"], as_cmap=False, data=titanic) ``` # Constraints - The `colors` list must contain at least two color specifications. - The function should correctly use the `as_cmap` parameter to create a colormap if needed. - The `data` parameter should be a Pandas DataFrame suitable for generating a heatmap (with numeric data). # Expected Input and Output - Input: - `colors` (list of str): `[\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"]` - `as_cmap` (bool): `False` - `data` (DataFrame): Titanic dataset from `seaborn` - Output: Either a list of colors or a colormap. # Additional Points - Ensure error handling for invalid color specifications. - Optimize your code for readability and maintainability. - Include comments explaining your logic. # Performance Requirements - Efficient creation of palettes and plotting should not take more than a few seconds, even for larger datasets.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_and_plot_palette(colors, as_cmap, data): Creates a color palette or colormap using seaborn.blend_palette and plots a heatmap. Parameters: colors (list of str): A list of color codes or names to create the palette. as_cmap (bool): Whether to return a continuous colormap. data (DataFrame): Dataset to visualize using the created color palette. Returns: List of colors or colormap which is used in the heatmap. # Error handling for invalid colors if len(colors) < 2: raise ValueError(\\"The \'colors\' list must contain at least two color specifications.\\") # Create the color palette or colormap palette = sns.blend_palette(colors, as_cmap=as_cmap) # Plot a heatmap using the created palette plt.figure(figsize=(10, 8)) sns.heatmap(data.corr(), cmap=palette) plt.show() return palette # Example usage titanic = sns.load_dataset(\'titanic\').select_dtypes(include=\'number\') # Select only numeric data create_and_plot_palette([\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"], as_cmap=False, data=titanic)"},{"question":"**Title**: Implement an XML Content Handler for Custom Parsing **Question**: In this assessment, you will demonstrate your understanding of the `xml.sax.handler` module by writing a custom XML content handler that extracts specific data from an XML document. Specifically, you will develop a `ProductContentHandler` by subclassing `xml.sax.handler.ContentHandler`. **Task**: 1. **Implement a custom content handler class `ProductContentHandler(ContentHandler)`**: - The XML document contains a list of products, where each product is defined within `<product>` tags. - A product has several sub-elements: `<name>`, `<price>`, and `<quantity>`. - Your handler should extract the name, price, and quantity of each product and store them in a list of dictionaries. Each dictionary should represent a product with keys \'name\', \'price\', and \'quantity\'. 2. **Implement the following methods** in your `ProductContentHandler` class: - `startElement(name, attrs)`: This method should identify the start of a `<product>`, `<name>`, `<price>`, or `<quantity>` element and prepare to capture its data. - `characters(content)`: This method should capture the character data within the tags identified in `startElement`. - `endElement(name)`: This method should finalize the data captured for the elements `<name>`, `<price>`, or `<quantity>` and store the completed product information once the `</product>` tag is closed. 3. **Write a function `parse_products(xml_string)`**: - This function should parse the given XML string using your `ProductContentHandler`. - The function should return a list of dictionaries representing the extracted product data. **Input**: - A single string containing XML data. **Output**: - A list of dictionaries where each dictionary contains \'name\', \'price\', and \'quantity\' keys with corresponding values extracted from the XML. **Example XML Input**: ```xml <products> <product> <name>Product A</name> <price>10.99</price> <quantity>100</quantity> </product> <product> <name>Product B</name> <price>12.49</price> <quantity>200</quantity> </product> </products> ``` **Expected Output**: ```python [ {\'name\': \'Product A\', \'price\': \'10.99\', \'quantity\': \'100\'}, {\'name\': \'Product B\', \'price\': \'12.49\', \'quantity\': \'200\'} ] ``` **Constraints**: - Assume that the XML structure is well-formed and follows the given example format. - Your solution should correctly handle multiple products and their corresponding elements. **Notes**: - Use the `xml.sax` module for parsing. - Ensure that your handler captures and stores data appropriately. Use appropriate instance variables to handle the state across method calls of the handler. ```python from xml.sax import handler, make_parser class ProductContentHandler(handler.ContentHandler): def __init__(self): super().__init__() self.current_element = None self.current_product = {} self.products = [] def startElement(self, name, attrs): if name == \\"product\\": self.current_product = {} elif name in (\\"name\\", \\"price\\", \\"quantity\\"): self.current_element = name def characters(self, content): if self.current_element: self.current_product[self.current_element] = content def endElement(self, name): if name == \\"product\\": self.products.append(self.current_product) elif name in (\\"name\\", \\"price\\", \\"quantity\\"): self.current_element = None def parse_products(xml_string): parser = make_parser() handler = ProductContentHandler() parser.setContentHandler(handler) from io import StringIO parser.parse(StringIO(xml_string)) return handler.products # Sample usage xml_input = \'\'\'<products> <product> <name>Product A</name> <price>10.99</price> <quantity>100</quantity> </product> <product> <name>Product B</name> <price>12.49</price> <quantity>200</quantity> </product> </products>\'\'\' print(parse_products(xml_input)) ```","solution":"from xml.sax import handler, make_parser class ProductContentHandler(handler.ContentHandler): def __init__(self): super().__init__() self.current_element = None self.current_product = {} self.products = [] self.content = \\"\\" # Buffer for character data def startElement(self, name, attrs): if name == \\"product\\": self.current_product = {} elif name in (\\"name\\", \\"price\\", \\"quantity\\"): self.current_element = name self.content = \\"\\" def characters(self, content): if self.current_element: self.content += content def endElement(self, name): if name in (\\"name\\", \\"price\\", \\"quantity\\"): self.current_product[self.current_element] = self.content.strip() self.current_element = None elif name == \\"product\\": self.products.append(self.current_product) self.current_product = {} def parse_products(xml_string): parser = make_parser() handler = ProductContentHandler() parser.setContentHandler(handler) from io import StringIO parser.parse(StringIO(xml_string)) return handler.products"},{"question":"Coding Assessment Question Design a Python function that utilizes the `importlib.metadata` library to retrieve and display comprehensive metadata information for an installed package. Your function should accept a package name, and return a dictionary containing the following structured information: 1. **Version number**: The version of the package. 2. **Metadata**: - Name - Summary - Home-page - Author and Author-email - License 3. **Entry points**: - List of all entry points by group and name. 4. **Files**: - List all files included in the package with their size and hash properties. Additionally, implement a custom finder that mimics the default handler but also allows retrieving packages from a specified additional directory. You should then demonstrate using this custom finder by retrieving and displaying metadata for a package located in this additional directory. # Input - `package_name`: A string representing the name of the package. - `additional_path`: A string representing the path to an additional directory where packages might be located. # Output A dictionary containing the structured metadata information described above. # Constraints - You can assume the package names provided as input are always valid and the packages are properly installed or located in the specified additional directory. - Handle any potential exceptions gracefully and provide meaningful error messages. # Example Usage ```python def get_package_metadata(package_name: str, additional_path: str) -> dict: # Your implementation here # Example package_name = \'wheel\' additional_path = \'/path/to/additional/directory\' result = get_package_metadata(package_name, additional_path) print(result) ``` # Expected Dictionary Structure ``` { \\"version\\": \\"0.32.3\\", \\"metadata\\": { \\"name\\": \\"wheel\\", \\"summary\\": \\"A built-package format for Python.\\", \\"home_page\\": \\"https://example.com\\", \\"author\\": \\"Author Name\\", \\"author_email\\": \\"author@example.com\\", \\"license\\": \\"MIT\\" }, \\"entry_points\\": { \\"group1\\": { \\"entry1\\": \\"script1\\", \\"entry2\\": \\"script2\\" }, \\"group2\\": { \\"entry1\\": \\"script3\\", ... } }, \\"files\\": [ {\\"name\\": \\"file1.py\\", \\"size\\": 1234, \\"hash\\": \\"abc123\\"}, {\\"name\\": \\"file2.py\\", \\"size\\": 5678, \\"hash\\": \\"def456\\"}, ... ] } ``` # Additional Information - Refer to the `importlib.metadata` documentation for necessary functions and classes like `version`, `metadata`, `entry_points`, `files`, and custom `MetaPathFinder` for extended search capabilities. - You may need to implement a custom subclass of `importlib.abc.MetaPathFinder` to handle the additional directory.","solution":"import os import importlib_metadata from typing import Dict, List import importlib.util from importlib.abc import MetaPathFinder from importlib.machinery import PathFinder def get_package_metadata(package_name: str, additional_path: str) -> dict: try: if additional_path and os.path.isdir(additional_path): sys.path.insert(0, additional_path) # Retrieve version version = importlib_metadata.version(package_name) # Retrieve metadata metadata_info = importlib_metadata.metadata(package_name) metadata = { \\"Name\\": metadata_info.get(\\"Name\\"), \\"Summary\\": metadata_info.get(\\"Summary\\"), \\"Home-page\\": metadata_info.get(\\"Home-page\\"), \\"Author\\": metadata_info.get(\\"Author\\"), \\"Author-email\\": metadata_info.get(\\"Author-email\\"), \\"License\\": metadata_info.get(\\"License\\"), } # Retrieve entry points entry_points = importlib_metadata.entry_points().select(name=package_name) entry_points_dict = {} for entry_point in entry_points: if entry_point.group not in entry_points_dict: entry_points_dict[entry_point.group] = {} entry_points_dict[entry_point.group][entry_point.name] = entry_point.value # Retrieve files package_dist = importlib_metadata.distribution(package_name) files = [ {\\"name\\": f.name, \\"size\\": f.read_text().encode().decode(\'utf-8\'), \\"hash\\": \\"\\"} # Placeholder for hash, you can add your hash logic here for f in package_dist.files or [] ] return { \\"version\\": version, \\"metadata\\": metadata, \\"entry_points\\": entry_points_dict, \\"files\\": files, } except importlib_metadata.PackageNotFoundError: return {\\"error\\": \\"Package not found\\"} except Exception as e: return {\\"error\\": str(e)} # Example package_name = \'wheel\' additional_path = \'/path/to/additional/directory\' result = get_package_metadata(package_name, additional_path) print(result)"},{"question":"# Complex Assignment and Import Management **Objective:** Demonstrate understanding of Python\'s assignment capabilities and import statement management by writing functions that perform specific types of assignments and another function that manages dynamically imported modules. **Problem Statement:** Implement the following three functions as specified: 1. `tuple_and_list_assignment(data)`: - Input: A list of integers of length `n >= 5`. - Output: A tuple containing: - The first element should be the first item of the list. - The second element should be a sublist of the second through fourth items (inclusive). - The third to last elements should be the rest of the list unpacked. - Example: ```python input: [1, 2, 3, 4, 5, 6] output: (1, [2, 3, 4], 5, 6) ``` 2. `dictionary_assignment_update(dictionary, updates)`: - Input: Two dictionaries, `dictionary` and `updates`. - Output: The resulting dictionary after updating `dictionary` with key-value pairs from `updates`. Ensure the update uses unpacking methodology. - Example: ```python dictionary: {\'a\': 1, \'b\': 2} updates: {\'b\': 3, \'c\': 4} output: {\'a\': 1, \'b\': 3, \'c\': 4} ``` 3. `dynamic_import(module_name)`: - Input: A string `module_name` representing the name of the module to be imported. - Output: The imported module object if the import is successful; otherwise, raise an `ImportError`. - Note: Use appropriate exception handling to capture and raise ImportError if the module doesn\'t exist. - Example: ```python module_name: \'math\' output: <module \'math\' (built-in)> ``` **Implementation Notes:** - Use tuple/list unpacking. - Utilize dictionary unpacking and the `**` operator. - Apply the `importlib.import_module` method for dynamic imports and handle exceptions appropriately. **Constraints:** - Assume the initial list for the first function will always have at least 5 elements. - The `dictionary` and `updates` in the second function will always be dictionaries. - The input string to the third function will always be a valid Python module name (of an existing or non-existing module). **Performance Considerations:** - Ensure efficient handling of data structures for assignments. - The dynamic import function should handle imports gracefully without performance lag due to unnecessary error handling. ```python # Function Templates def tuple_and_list_assignment(data): # Your implementation here pass def dictionary_assignment_update(dictionary, updates): # Your implementation here pass def dynamic_import(module_name): # Your implementation here pass ``` **Good Luck!**","solution":"import importlib def tuple_and_list_assignment(data): Returns a tuple by processing the input list. - The first element is the first item of the list. - The second element is a sublist of the second through fourth items (inclusive). - The third to last elements are the rest of the list unpacked. first_elem, second_elem, third_elem, fourth_elem, *rest = data return (first_elem, [second_elem, third_elem, fourth_elem], *rest) def dictionary_assignment_update(dictionary, updates): Updates the dictionary with key-value pairs from updates using unpacking methodology. return {**dictionary, **updates} def dynamic_import(module_name): Dynamically imports a module by name and returns the module object if successful, otherwise raises an ImportError. try: module = importlib.import_module(module_name) return module except ImportError: raise ImportError(f\\"Module named {module_name} could not be imported\\")"},{"question":"**Question: Simplifying and Approximating Fractions** Implement a function `normalize_fraction(input_value, max_denominator=1000000)` that simplifies a given input to its closest fraction representation with a denominator not exceeding `max_denominator`. This input can be an integer, float, decimal, or string. # Function Signature ```python def normalize_fraction(input_value, max_denominator=1000000): # Your code here ``` # Input - `input_value`: An integer, float, decimal, or string representing the number to be converted to a fraction. - `max_denominator`: An optional integer parameter that specifies the maximum allowed denominator for the fraction (default is 1,000,000). # Output - A tuple of two integers `(numerator, denominator)`, representing the simplified fraction. # Constraints 1. The denominator must be a positive integer. 2. The function should handle improper fractions and mixed numbers properly. 3. The function should raise a `ValueError` if the input value is invalid. 4. You cannot use any external libraries other than the standard Python library. 5. Performance considerations: Your function should be efficient even when provided with large numbers. # Examples ```python >>> normalize_fraction(1.1) (11, 10) >>> normalize_fraction(\'3.141592653589793\') (355, 113) >>> normalize_fraction(0.333333333333333) (1, 3) >>> normalize_fraction(\' 4/16 \') (1, 4) >>> normalize_fraction(\'2e-5\') (1, 50000) ``` # Requirements 1. The function must be able to convert various types of input to a fraction and normalize it. 2. If the input is invalid or conversion isn\'t possible, the function should raise a `ValueError`. # Additional Information for Students - You may find the `fractions.Fraction` module useful to create fractional representations. - Consider edge cases such as zero or negative numbers in your implementation. - Testing your function with various inputs to ensure accuracy and efficiency is critical.","solution":"from fractions import Fraction import decimal def normalize_fraction(input_value, max_denominator=1000000): Simplifies a given input to its closest fraction representation with a denominator not exceeding max_denominator. Args: - input_value: An integer, float, decimal, or string representing the number to be converted to a fraction. - max_denominator: An optional integer parameter that specifies the maximum allowed denominator (default is 1,000,000). Returns: - A tuple of two integers (numerator, denominator), representing the simplified fraction. Raises: - ValueError if the input value is invalid. try: if isinstance(input_value, str): input_value = input_value.strip() if \'/\' in input_value: input_value = Fraction(input_value) elif \'e\' in input_value or \'E\' in input_value: input_value = decimal.Decimal(input_value) else: input_value = float(input_value) elif isinstance(input_value, decimal.Decimal): input_value = float(input_value) fraction = Fraction(input_value).limit_denominator(max_denominator) return (fraction.numerator, fraction.denominator) except (ValueError, ZeroDivisionError, decimal.InvalidOperation) as e: raise ValueError(f\\"Invalid input value: {input_value}\\") from e"},{"question":"**Question: Implementing and Evaluating an Ensemble Learning Model using scikit-learn** Ensemble methods combine multiple machine learning models to improve the overall performance. One popular technique is the Random Forest, which aggregates the predictions of multiple decision trees. Your task is to implement and evaluate a Random Forest classifier using scikit-learn. You are given a dataset with features and labels. Your goal is to build a Random Forest model to classify the data. Follow the instructions below to complete the task. # Instructions: 1. **Data Preparation:** - Load the dataset from the provided CSV file. The path to the CSV file will be given as an input. - Split the data into features (X) and labels (y). Assume the last column in the dataset is the label. 2. **Model Training:** - Split the data into training and testing sets using an 80-20 split. - Implement a Random Forest classifier using scikit-learn with the following parameters: - Number of estimators: 100 - Maximum depth: 10 - Random state: 42 (for reproducibility) - Train the model on the training set. 3. **Model Evaluation:** - Predict the labels for the test set. - Calculate the following evaluation metrics: - Accuracy - Precision - Recall - F1 Score 4. **Function Signature:** ```python def evaluate_random_forest(csv_path: str) -> dict: Trains a Random Forest classifier on the given dataset and evaluates its performance. Parameters: csv_path (str): Path to the CSV file containing the data. Returns: dict: A dictionary containing the evaluation metrics: {\'accuracy\': float, \'precision\': float, \'recall\': float, \'f1_score\': float} pass ``` # Constraints: - You must use `scikit-learn` for this task. - You are expected to handle any missing values in the dataset by removing rows with missing values. - Your implementation should be efficient and make use of the library\'s advanced features. # Example Usage: ```python # Assuming the CSV file is located at \'data/dataset.csv\' metrics = evaluate_random_forest(\'data/dataset.csv\') print(metrics) # Output: {\'accuracy\': 0.85, \'precision\': 0.87, \'recall\': 0.84, \'f1_score\': 0.85} ``` # Notes: - Make sure your solution is well-commented and follows best coding practices. - You may assume that the dataset is not excessively large and can fit into memory. - Use appropriate scikit-learn classes and methods to ensure an efficient solution.","solution":"import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def evaluate_random_forest(csv_path: str) -> dict: Trains a Random Forest classifier on the given dataset and evaluates its performance. Parameters: csv_path (str): Path to the CSV file containing the data. Returns: dict: A dictionary containing the evaluation metrics: {\'accuracy\': float, \'precision\': float, \'recall\': float, \'f1_score\': float} # Load the dataset data = pd.read_csv(csv_path) # Remove rows with missing values data = data.dropna() # Split the data into features (X) and labels (y) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Split the data into training and testing sets using an 80-20 split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a Random Forest classifier with specified parameters model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42) # Train the model on the training set model.fit(X_train, y_train) # Predict the labels for the test set y_pred = model.predict(X_test) # Calculate the evaluation metrics metrics = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred, average=\'binary\', pos_label=y_test[0] if len(set(y_test)) == 2 else \'weighted\'), \'recall\': recall_score(y_test, y_pred, average=\'binary\', pos_label=y_test[0] if len(set(y_test)) == 2 else \'weighted\'), \'f1_score\': f1_score(y_test, y_pred, average=\'binary\', pos_label=y_test[0] if len(set(y_test)) == 2 else \'weighted\') } return metrics"},{"question":"# PyTorch FX Numeric Suite: Compute Tensor Metrics **Objective:** In this assessment, you\'ll demonstrate your understanding of the PyTorch FX numeric suite utilities by implementing functions that utilize core metrics to compare two input tensors. You will be required to: 1. Use the provided utility functions to compute various metrics between two tensors. 2. Implement a new higher-level function that combines multiple metrics. # Instructions: 1. **Function Implementation:** Implement the following three functions utilizing the PyTorch FX utility functions: - `compute_sqnr_metric(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float` - `compute_normalized_l2_metric(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float` - `compute_cosine_similarity_metric(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float` 2. **Composite Function:** Implement a composite function that computes all three metrics and returns a dictionary: - `compute_all_metrics(tensor1: torch.Tensor, tensor2: torch.Tensor) -> Dict[str, float]` # Detailed Requirements: 1. **Function: `compute_sqnr_metric`** - **Inputs:** - `tensor1`, `tensor2`: Two PyTorch tensors of the same shape. - **Output:** - A float representing the signal-to-quantization-noise ratio between the two input tensors. 2. **Function: `compute_normalized_l2_metric`** - **Inputs:** - `tensor1`, `tensor2`: Two PyTorch tensors of the same shape. - **Output:** - A float representing the normalized L2 error between the two input tensors. 3. **Function: `compute_cosine_similarity_metric`** - **Inputs:** - `tensor1`, `tensor2`: Two PyTorch tensors of the same shape. - **Output:** - A float representing the cosine similarity between the two input tensors. 4. **Function: `compute_all_metrics`** - **Inputs:** - `tensor1`, `tensor2`: Two PyTorch tensors of the same shape. - **Output:** - A dictionary with the keys `\\"sqnr\\"`, `\\"normalized_l2\\"`, and `\\"cosine_similarity\\"`, containing the respective metric values as floats. # Sample Implementation Outline: ```python import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity from typing import Dict def compute_sqnr_metric(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float: return compute_sqnr(tensor1, tensor2) def compute_normalized_l2_metric(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float: return compute_normalized_l2_error(tensor1, tensor2) def compute_cosine_similarity_metric(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float: return compute_cosine_similarity(tensor1, tensor2) def compute_all_metrics(tensor1: torch.Tensor, tensor2: torch.Tensor) -> Dict[str, float]: metrics = { \\"sqnr\\": compute_sqnr_metric(tensor1, tensor2), \\"normalized_l2\\": compute_normalized_l2_metric(tensor1, tensor2), \\"cosine_similarity\\": compute_cosine_similarity_metric(tensor1, tensor2), } return metrics ``` # Constraints: - Ensure that the input tensors `tensor1` and `tensor2` have the same shape; raise a ValueError otherwise. - Handle any exception that may occur during the metric computation and return a default value of `0.0`. # Performance Requirements: - The operations should be efficient and handle tensors of typical sizes used in deep learning without significant performance degradation. Good luck, and happy coding!","solution":"import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity from typing import Dict def compute_sqnr_metric(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float: if tensor1.shape != tensor2.shape: raise ValueError(\\"The shapes of tensor1 and tensor2 must match.\\") return compute_sqnr(tensor1, tensor2) def compute_normalized_l2_metric(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float: if tensor1.shape != tensor2.shape: raise ValueError(\\"The shapes of tensor1 and tensor2 must match.\\") return compute_normalized_l2_error(tensor1, tensor2) def compute_cosine_similarity_metric(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float: if tensor1.shape != tensor2.shape: raise ValueError(\\"The shapes of tensor1 and tensor2 must match.\\") return compute_cosine_similarity(tensor1, tensor2) def compute_all_metrics(tensor1: torch.Tensor, tensor2: torch.Tensor) -> Dict[str, float]: if tensor1.shape != tensor2.shape: raise ValueError(\\"The shapes of tensor1 and tensor2 must match.\\") metrics = { \\"sqnr\\": compute_sqnr_metric(tensor1, tensor2), \\"normalized_l2\\": compute_normalized_l2_metric(tensor1, tensor2), \\"cosine_similarity\\": compute_cosine_similarity_metric(tensor1, tensor2), } return metrics"},{"question":"# Question **Objective**: The primary objective of this task is to demonstrate your understanding of the seaborn `ecdfplot` function and your ability to use it effectively for statistical visualization. **Plotting Univariate Distributions**: Given a dataset, implement a function `plot_ecdf`, which will generate and customize empirical cumulative distribution function (ECDF) plots using the seaborn library. The function should meet the following constraints and requirements: # Function Signature: ```python def plot_ecdf(data, column, axis=\'x\', hue=None, stat=\'proportion\', complementary=False): pass ``` # Input: 1. `data`: A pandas DataFrame containing the input data. 2. `column`: A string representing the column name from the DataFrame to plot. 3. `axis`: A string, either \'x\' or \'y\', denoting which axis to assign the data variable (default is \'x\'). 4. `hue`: A string (optional), representing a column name to define the hue semantics (default is None). 5. `stat`: A string, either \'proportion\', \'count\', or \'percent\', to set the ECDF statistic (default is \'proportion\'). 6. `complementary`: A boolean, whether or not to plot the complementary CDF (default is False). # Output: - The function should display the ECDF plot according to the specified parameters. # Example Example 1: **Input:** ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") plot_ecdf(penguins, \'flipper_length_mm\', axis=\'x\') ``` **Output:** The function displays an ECDF plot with flipper length along the x-axis for the penguin dataset. Example 2: **Input:** ```python plot_ecdf(penguins, \'bill_length_mm\', axis=\'y\', hue=\'species\', stat=\'count\', complementary=True) ``` **Output:** The function displays a complementary ECDF plot with bill length on the y-axis, differentiated by the species of penguins, showing the count statistics. # Constraints: 1. The `data` DataFrame must contain numerical data for the column specified. 2. The column specified by `hue` must contain categorical data. 3. The function must handle missing or NaN values gracefully, either by removing them or by an appropriate method suggested by seaborn or pandas. # Notes: - You may customize the plot further by setting additional seaborn or matplotlib parameters as needed to enhance the visualization or to match specific aesthetics. - The function will be tested with various similar datasets and parameters to ensure robustness. Your implementation should effectively demonstrate not only your ability to use the seaborn library but also your understanding of the underlying data distribution and statistical visualization principles.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_ecdf(data, column, axis=\'x\', hue=None, stat=\'proportion\', complementary=False): Generate and customize ECDF plots using seaborn. Parameters: data (DataFrame): A pandas DataFrame containing the input data. column (str): Column name from the DataFrame to plot. axis (str): Either \'x\' or \'y\', denoting which axis to assign the data variable (default \'x\'). hue (str, optional): Column name to define the hue semantics (default None). stat (str): Either \'proportion\', \'count\', or \'percent\', to set the ECDF statistic (default \'proportion\'). complementary (bool): Whether or not to plot the complementary CDF (default False). Returns: None, displays the ECDF plot. # Handle missing or NaN values by dropping them data = data.dropna(subset=[column]) sns.ecdfplot(data=data, x=column if axis == \'x\' else None, y=column if axis == \'y\' else None, hue=hue, stat=stat, complementary=complementary) plt.title(f\\"ECDF Plot for {column}\\") plt.xlabel(column if axis == \'x\' else \'ECDF\') plt.ylabel(\'ECDF\' if axis == \'x\' else column) plt.grid(True) plt.show()"},{"question":"# XML Document Manipulation with `xml.etree.ElementTree` **Objective**: Write a Python function that parses a given XML string, modifies certain elements based on specified criteria, and returns the modified XML as a string. **Function Signature**: ```python def modify_xml(xml_str: str) -> str: pass ``` **Input**: - `xml_str` (str): A string representing a well-formed XML document. **Output**: - Returns a string representing the modified XML document. **Constraints**: - The input `xml_str` will always be a well-formed XML document. - The XML document will contain a predefined structure as shown below. **Example XML Document Structure**: ```xml <library> <book id=\\"1\\"> <title>Book Title 1</title> <author>Author 1</author> <year>2000</year> </book> <book id=\\"2\\"> <title>Book Title 2</title> <author>Author 2</author> <year>2005</year> </book> <book id=\\"3\\"> <title>Book Title 3</title> <author>Author 3</author> <year>2015</year> </book> </library> ``` **Required Modifications**: 1. Update the `year` element of each `<book>` to the current year if it is earlier than 2010. 2. Add a `<genre>` element with the text \\"Unknown\\" to every `<book>` that does not already have a `<genre>`. **Example**: ```python xml_content = \'\'\' <library> <book id=\\"1\\"> <title>Book Title 1</title> <author>Author 1</author> <year>2000</year> </book> <book id=\\"2\\"> <title>Book Title 2</title> <author>Author 2</author> <year>2005</year> </book> <book id=\\"3\\"> <title>Book Title 3</title> <author>Author 3</author> <year>2015</year> </book> </library> \'\'\' modified_xml = modify_xml(xml_content) print(modified_xml) ``` **Expected Output**: ```xml <library> <book id=\\"1\\"> <title>Book Title 1</title> <author>Author 1</author> <year>2023</year> <genre>Unknown</genre> </book> <book id=\\"2\\"> <title>Book Title 2</title> <author>Author 2</author> <year>2023</year> <genre>Unknown</genre> </book> <book id=\\"3\\"> <title>Book Title 3</title> <author>Author 3</author> <year>2015</year> <genre>Unknown</genre> </book> </library> ``` **Notes**: - Ensure you handle the XML parsing and modification using the `xml.etree.ElementTree` module. - Consider the timezone and date settings of your environment for the current year. - Aim for efficient traversal and modification of the XML tree.","solution":"import xml.etree.ElementTree as ET from datetime import datetime def modify_xml(xml_str: str) -> str: Parses the given XML string, updates the year for books that are earlier than 2010 to the current year, and adds a \'genre\' element with text \'Unknown\' to books without one. Args: - xml_str (str): A string representing a well-formed XML document. Returns: - str: A string representing the modified XML document. current_year = datetime.now().year # Parse the XML string root = ET.fromstring(xml_str) # Iterate over all book elements for book in root.findall(\'book\'): # Update year if it is earlier than 2010 year_elem = book.find(\'year\') if year_elem is not None: year = int(year_elem.text) if year < 2010: year_elem.text = str(current_year) # Add genre element with \'Unknown\' if it does not exist genre_elem = book.find(\'genre\') if genre_elem is None: genre_elem = ET.SubElement(book, \'genre\') genre_elem.text = \'Unknown\' # Return the modified XML as a string return ET.tostring(root, encoding=\'unicode\')"},{"question":"**Problem Statement:** You are required to create a utility in Python using the `http.client` module that performs the following tasks: 1. Make a GET request to a specified URL and retrieve the response. 2. Make a POST request to a specified URL with given payload data and headers, and retrieve the response. 3. Handle various HTTP exceptions and provide appropriate error messages. You need to implement the following functions: 1. `make_get_request(url: str) -> str`: - This function takes a URL as input and makes a GET request to the server. - It should return the response body as a string. - Handle any exceptions and return an appropriate error message. 2. `make_post_request(url: str, data: dict, headers: dict) -> str`: - This function takes a URL, a dictionary of data to be sent as payload, and a dictionary of headers as input. - It should make a POST request to the server with the given data and headers. - Return the response body as a string. - Handle any exceptions and return an appropriate error message. **Constraints:** - You are allowed to use only the `http.client` module for making HTTP requests. - The functions should handle common HTTP exceptions such as connection errors, invalid URLs, and bad status lines. - You should ensure that the connection is properly closed after making the request. **Examples:** 1. **GET Request:** ```python url = \\"http://www.example.com\\" response = make_get_request(url) print(response) # Output will be the response body of the GET request ``` 2. **POST Request:** ```python url = \\"http://www.example.com/api\\" data = {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"} headers = {\\"Content-type\\": \\"application/json\\"} response = make_post_request(url, data, headers) print(response) # Output will be the response body of the POST request ``` Notes: - You can use the `json` module to convert the dictionary data to JSON format when making a POST request. - Ensure you convert the response body to a string format before returning it. **Hint:** Refer to the examples provided in the `http.client` documentation to understand how to make GET and POST requests using the `HTTPConnection` class.","solution":"import http.client import json from urllib.parse import urlparse def make_get_request(url: str) -> str: try: parsed_url = urlparse(url) conn = http.client.HTTPConnection(parsed_url.netloc) conn.request(\'GET\', parsed_url.path or \'/\') response = conn.getresponse() if response.status != 200: return f\\"HTTP Error: {response.status} {response.reason}\\" data = response.read().decode(\'utf-8\') conn.close() return data except Exception as e: return f\\"Error: {str(e)}\\" def make_post_request(url: str, data: dict, headers: dict) -> str: try: parsed_url = urlparse(url) conn = http.client.HTTPConnection(parsed_url.netloc) json_data = json.dumps(data) conn.request(\'POST\', parsed_url.path or \'/\', body=json_data, headers=headers) response = conn.getresponse() if response.status != 200: return f\\"HTTP Error: {response.status} {response.reason}\\" data = response.read().decode(\'utf-8\') conn.close() return data except Exception as e: return f\\"Error: {str(e)}\\""},{"question":"You need to develop a process to classify data using a supervised machine learning model. Your task includes data preprocessing, dimensionality reduction, model training, and evaluation. Follow these steps: 1. **Data Preprocessing**: - Load a dataset of your choice (like the Iris dataset from sklearn). - Standardize the features using `StandardScaler` from `sklearn.preprocessing`. 2. **Dimensionality Reduction**: - Apply Principal Component Analysis (PCA) to reduce the dataset to 2 principal components using `sklearn.decomposition.PCA`. 3. **Model Training**: - Use a supervised learning method like `LogisticRegression` from `sklearn.linear_model` for classification. - Create a pipeline that combines the `StandardScaler`, `PCA`, and `LogisticRegression`. 4. **Model Evaluation**: - Use cross-validation to evaluate the model\'s performance. The metric for evaluation should be accuracy. - Output the cross-validation accuracy scores and their mean. 5. **Implementation Details**: - Use the `make_pipeline` function from `sklearn.pipeline`. # Constraints: - You should not use any other preprocessing, transformation, or model except those specified. - Ensure your solution is efficient and leverages the pipeline feature provided by scikit-learn. # Expected Input and Output - **Input**: Dataset (you can use the predefined datasets like Iris from `sklearn.datasets`). - **Output**: Cross-validation accuracy scores and their mean. # Example ```python from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score from sklearn.pipeline import make_pipeline # Load the Iris dataset data = load_iris() X, y = data.data, data.target # Create the pipeline pipeline = make_pipeline( StandardScaler(), PCA(n_components=2), LogisticRegression() ) # Evaluate the model using cross-validation scores = cross_val_score(pipeline, X, y, cv=5) # Output the results print(\\"Cross-validation scores:\\", scores) print(\\"Mean cross-validation score:\\", scores.mean()) ``` In the code provided: - The Iris dataset is loaded and split into features (`X`) and targets (`y`). - A pipeline is created which standardizes the data, reduces it to 2 principal components, and finally applies logistic regression. - Cross-validation is performed, and the accuracy scores are printed along with their mean. Ensure your code meets the outlined steps and constraints.","solution":"from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score from sklearn.pipeline import make_pipeline def classify_data(): # Load the Iris dataset data = load_iris() X, y = data.data, data.target # Create the pipeline pipeline = make_pipeline( StandardScaler(), PCA(n_components=2), LogisticRegression() ) # Evaluate the model using cross-validation scores = cross_val_score(pipeline, X, y, cv=5) return scores, scores.mean()"},{"question":"# **Coding Assessment Question** **Objective** Implement a function that applies a specified window function from the `torch.signal.windows` module to a 1-dimensional tensor and computes the Fast Fourier Transform (FFT) of the windowed signal. **Function Signature** ```python import torch def windowed_fft(signal: torch.Tensor, window_name: str) -> torch.Tensor: Apply a window function to the input signal and compute its Fast Fourier Transform (FFT). Parameters: signal (torch.Tensor): A 1-dimensional tensor representing the signal. window_name (str): The name of the window function to apply. This must be one of the following: \'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\'. Returns: torch.Tensor: The FFT of the windowed signal. Raises: ValueError: If an invalid window name is provided. # Your code here ``` **Input** 1. `signal`: A 1-dimensional tensor of type `torch.Tensor` representing the signal. 2. `window_name`: A string representing the name of the window function to apply. The window function names available are: - \'bartlett\' - \'blackman\' - \'cosine\' - \'exponential\' - \'gaussian\' - \'general_cosine\' - \'general_hamming\' - \'hamming\' - \'hann\' - \'kaiser\' - \'nuttall\' **Output** - The FFT of the windowed signal as a 1-dimensional tensor of type `torch.Tensor`. **Constraints** - The input signal tensor will have a length between 2 and 10,000. - The function should handle incorrect window names by raising a `ValueError` with a descriptive message. **Example** ```python import torch signal = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]) window_name = \'hamming\' fft_result = windowed_fft(signal, window_name) print(fft_result) # Expected Output: tensor([...]) # FFT result of the windowed signal ``` **Notes** - You may utilize the `torch.fft` module to compute the FFT. - You need to import the appropriate window function from `torch.signal.windows` based on the `window_name` parameter. - Ensure the length of the window matches the length of the signal for element-wise multiplication before computing the FFT.","solution":"import torch import torch.fft import torch.signal.windows as windows WINDOW_FUNCTIONS = { \'bartlett\': windows.bartlett, \'blackman\': windows.blackman, \'cosine\': windows.cosine, \'exponential\': windows.exponential, \'gaussian\': windows.gaussian, \'general_cosine\': windows.general_cosine, \'general_hamming\': windows.general_hamming, \'hamming\': windows.hamming, \'hann\': windows.hann, \'kaiser\': windows.kaiser, \'nuttall\': windows.nuttall } def windowed_fft(signal: torch.Tensor, window_name: str) -> torch.Tensor: Apply a window function to the input signal and compute its Fast Fourier Transform (FFT). Parameters: signal (torch.Tensor): A 1-dimensional tensor representing the signal. window_name (str): The name of the window function to apply. Returns: torch.Tensor: The FFT of the windowed signal. Raises: ValueError: If an invalid window name is provided. if window_name not in WINDOW_FUNCTIONS: raise ValueError(f\\"Invalid window name: {window_name}\\") window_function = WINDOW_FUNCTIONS[window_name] window = window_function(len(signal)) windowed_signal = signal * window fft_result = torch.fft.fft(windowed_signal) return fft_result"},{"question":"# Customizing Object Serialization with copyreg Problem Statement You are tasked with creating a customized pickling process for a class that represents a complex object. This class, `Project`, holds information about a project, including its name, deadline, and a list of tasks. Each task is represented by a `Task` class, which includes a description and a flag indicating if the task is completed. Implement the classes `Project` and `Task`, a custom pickling function for `Project` that serializes its attributes, and register this function using the `copyreg` module. Ensure that your implementation allows these objects to be pickled and unpickled correctly. Requirements 1. Define a `Task` class with the following attributes and methods: - `description` (str): a description of the task. - `completed` (bool): a flag indicating whether the task is completed. - `__init__(self, description: str, completed: bool = False)`: initializes the attributes. 2. Define a `Project` class with the following attributes and methods: - `name` (str): the name of the project. - `deadline` (str): the deadline for the project. - `tasks` (list of `Task` instances): a list of tasks associated with the project. - `__init__(self, name: str, deadline: str, tasks: list)`: initializes the attributes. 3. Implement a custom pickling function `pickle_project` for the `Project` class that serializes its attributes. 4. Use `copyreg.pickle` to register the custom pickling function for the `Project` class. 5. Demonstrate the functionality by: - Creating an instance of `Project` with some tasks. - Pickling and unpickling the `Project` instance. - Verifying that the unpickled object is identical to the original. Constraints - Assume description and name strings will not exceed 100 characters. - Serialization should handle empty task lists correctly. - The unpickled object should preserve the exact state (including the completion status of tasks) of the original object. Example Usage ```python import pickle # Define the Task and Project classes here # Create some tasks task1 = Task(\\"Design module\\", False) task2 = Task(\\"Implement module\\", True) task3 = Task(\\"Test module\\", False) # Create a project with these tasks project = Project(\\"Python Project\\", \\"2023-12-31\\", [task1, task2, task3]) # Register the custom pickling function for Project class # (Implementation details for pickle_project and copyreg.pickle registration) # Pickle the Project instance pickled_project = pickle.dumps(project) # Unpickle the Project instance unpickled_project = pickle.loads(pickled_project) # Verify the unpickled project instance assert unpickled_project.name == project.name assert unpickled_project.deadline == project.deadline assert len(unpickled_project.tasks) == len(project.tasks) assert all( utask.description == otask.description and utask.completed == otask.completed for utask, otask in zip(unpickled_project.tasks, project.tasks) ) ``` Implement the `Task` and `Project` classes, the custom pickling function `pickle_project`, and the registration process described above.","solution":"import copyreg import pickle class Task: def __init__(self, description: str, completed: bool = False): self.description = description self.completed = completed class Project: def __init__(self, name: str, deadline: str, tasks: list): self.name = name self.deadline = deadline self.tasks = tasks def pickle_project(project): return Project, (project.name, project.deadline, project.tasks) copyreg.pickle(Project, pickle_project) # Demonstrate the functionality # Create some tasks task1 = Task(\\"Design module\\", False) task2 = Task(\\"Implement module\\", True) task3 = Task(\\"Test module\\", False) # Create a project with these tasks project = Project(\\"Python Project\\", \\"2023-12-31\\", [task1, task2, task3]) # Pickle the Project instance pickled_project = pickle.dumps(project) # Unpickle the Project instance unpickled_project = pickle.loads(pickled_project) # Verify the unpickled project instance matches the original assert unpickled_project.name == project.name assert unpickled_project.deadline == project.deadline assert len(unpickled_project.tasks) == len(project.tasks) assert all( utask.description == otask.description and utask.completed == otask.completed for utask, otask in zip(unpickled_project.tasks, project.tasks) ) print(\\"Custom serialization and deserialization works correctly.\\")"},{"question":"**Coding Assessment Question:** Given the instructions below, write a Python script that demonstrates your understanding of creating and managing virtual environments, and handling packages with `pip`. # Instructions: 1. **Create a Virtual Environment:** - Write a Python function `create_virtual_env` which takes a directory name as input and creates a virtual environment in that directory using the `venv` module. 2. **Activate the Virtual Environment:** - Write a Python function `activate_virtual_env` which takes the directory name as input and activates the virtual environment for the current session. - Note: Just simulate activation by printing the command to activate the environment, as actual activation cannot be performed through a script. 3. **Install Packages:** - Write a Python function `install_packages` which takes a list of package names and installs them in the activated virtual environment using `pip`. 4. **List Installed Packages:** - Write a Python function `list_installed_packages` which prints all the packages installed in the virtual environment. 5. **Show Package Details:** - Write a Python function `show_package_details` which takes a package name as input and prints its details using `pip show`. # Expected Input and Output Formats: Input: - `create_virtual_env` should take a string representing the directory name. - `activate_virtual_env` should take a string representing the directory name. - `install_packages` should take a list of package names. - `list_installed_packages` takes no input. - `show_package_details` should take a string representing the package name. Output: - `create_virtual_env`: Create a virtual environment in the specified directory. - `activate_virtual_env`: Print the command to activate the environment. - `install_packages`: Install the specified packages. - `list_installed_packages`: Print the installed packages. - `show_package_details`: Print the details of the specified package. # Constraints: - Ensure the scripts are compatible with Python 3.10. - Handle any possible errors such as missing packages, invalid directory names, or incorrect package names gracefully. # Example Usage: ```python # Example directory name \\"myenv\\" create_virtual_env(\\"myenv\\") # Simulated activation: activate_virtual_env(\\"myenv\\") # Install packages: install_packages([\\"requests==2.7.0\\", \\"numpy\\"]) # List installed packages: list_installed_packages() # Show details of a specific package: show_package_details(\\"requests\\") ``` # Notes: - Ensure to handle all edge cases and provide proper error messages. - The script should be self-contained and executable independently. - Comment your code appropriately for better understanding. This question assesses the student\'s ability to manipulate virtual environments and handle packages using pip, which are essential skills for efficient Python development and project management.","solution":"import subprocess import os def create_virtual_env(directory_name): Create a virtual environment in the specified directory. try: subprocess.run([\'python3\', \'-m\', venv, directory_name], check=True) print(f\\"Virtual environment created at: {directory_name}\\") except Exception as e: print(f\\"Error creating virtual environment: {e}\\") def activate_virtual_env(directory_name): Print the command to activate the virtual environment. activate_script = f\\"{directory_name}/bin/activate\\" if os.name == \'nt\': # Windows activate_script = f\\"{directory_name}Scriptsactivate\\" print(f\\"To activate the virtual environment, run: source {activate_script}\\") def install_packages(packages): Install the specified packages in the virtual environment. try: for package in packages: subprocess.run([\'pip\', \'install\', package], check=True) print(f\\"Package {package} installed successfully\\") except Exception as e: print(f\\"Error installing package {package}: {e}\\") def list_installed_packages(): List all the packages installed in the virtual environment. try: result = subprocess.run([\'pip\', \'list\'], check=True, capture_output=True, text=True) print(\\"Installed packages:\\") print(result.stdout) except Exception as e: print(f\\"Error listing installed packages: {e}\\") def show_package_details(package_name): Show details of the specified package using pip show. try: result = subprocess.run([\'pip\', \'show\', package_name], check=True, capture_output=True, text=True) print(f\\"Details for package {package_name}:\\") print(result.stdout) except Exception as e: print(f\\"Error showing details for package {package_name}: {e}\\")"},{"question":"# Python Coding Assessment Problem Statement You are tasked with writing a Python function that will analyze a list of numeric error codes and provide a comprehensive report in a structured format. The report should include the following details for each error code: 1. The error code number. 2. The error code name (as a string). 3. The error message (using `os.strerror()`). 4. The corresponding exception class (if applicable) or \\"No direct mapping\\" if there is no direct exception class. Input - A list of integers, where each integer represents a numeric error code. Output - A list of dictionaries, each containing the following keys: * `error_code`: The numeric error code. * `error_name`: The name of the error code. * `error_message`: The error message corresponding to the error code. * `exception_class`: The exception class if directly mapped, otherwise \\"No direct mapping\\". Function Signature ```python def analyze_error_codes(error_codes: list[int]) -> list[dict]: pass ``` Constraints - Each error code in the input will be a valid error code present in `errno.errorcode.keys()`. - The list of error codes will contain at least one and no more than 100 error codes. - You may use the `errno` module, the `os` module, and any standard library modules required. Example ```python error_codes = [1, 2, 3, 4] report = analyze_error_codes(error_codes) # Example output [ { \\"error_code\\": 1, \\"error_name\\": \\"EPERM\\", \\"error_message\\": \\"Operation not permitted\\", \\"exception_class\\": \\"PermissionError\\" }, { \\"error_code\\": 2, \\"error_name\\": \\"ENOENT\\", \\"error_message\\": \\"No such file or directory\\", \\"exception_class\\": \\"FileNotFoundError\\" }, { \\"error_code\\": 3, \\"error_name\\": \\"ESRCH\\", \\"error_message\\": \\"No such process\\", \\"exception_class\\": \\"ProcessLookupError\\" }, { \\"error_code\\": 4, \\"error_name\\": \\"EINTR\\", \\"error_message\\": \\"Interrupted system call\\", \\"exception_class\\": \\"InterruptedError\\" } ] ``` Notes - Use the `errno.errorcode` dictionary to get the error name from the error code. - Use the `os.strerror()` function to get the error message. - Map error codes to their exceptions if they exist; otherwise, indicate no direct mapping.","solution":"import errno import os def analyze_error_codes(error_codes: list[int]) -> list[dict]: exception_mapping = { errno.EPERM: \\"PermissionError\\", errno.ENOENT: \\"FileNotFoundError\\", errno.ESRCH: \\"ProcessLookupError\\", errno.EINTR: \\"InterruptedError\\", errno.EIO: \\"OSError\\", errno.ENXIO: \\"OSError\\", errno.E2BIG: \\"OSError\\", errno.ENOEXEC: \\"OSError\\", errno.EBADF: \\"OSError\\", errno.ECHILD: \\"OSError\\", errno.EAGAIN: \\"OSError\\", errno.ENOMEM: \\"MemoryError\\", errno.EACCES: \\"PermissionError\\", errno.EFAULT: \\"OSError\\", errno.EBUSY: \\"OSError\\", errno.EEXIST: \\"FileExistsError\\", errno.EXDEV: \\"OSError\\", errno.ENODEV: \\"OSError\\", errno.ENOTDIR: \\"NotADirectoryError\\", errno.EISDIR: \\"IsADirectoryError\\", errno.EINVAL: \\"ValueError\\", errno.ENFILE: \\"OSError\\", errno.EMFILE: \\"OSError\\", errno.ENOTTY: \\"OSError\\", errno.ETXTBSY: \\"OSError\\", errno.EFBIG: \\"OSError\\", errno.ENOSPC: \\"OSError\\", errno.ESPIPE: \\"OSError\\", errno.EROFS: \\"OSError\\", errno.EMLINK: \\"OSError\\", errno.EPIPE: \\"BrokenPipeError\\", errno.EDOM: \\"OSError\\", errno.ERANGE: \\"OSError\\", errno.EDEADLK: \\"OSError\\", errno.ENAMETOOLONG: \\"OSError\\", errno.ENOLCK: \\"OSError\\", errno.ENOSYS: \\"OSError\\", errno.ENOTEMPTY: \\"OSError\\", errno.ELOOP: \\"OSError\\", errno.ENOMSG: \\"OSError\\", errno.EIDRM: \\"OSError\\", errno.ECHRNG: \\"OSError\\", errno.EL2NSYNC: \\"OSError\\", errno.EL3HLT: \\"OSError\\", errno.EL3RST: \\"OSError\\", errno.ELNRNG: \\"OSError\\", errno.EUNATCH: \\"OSError\\", errno.ENOCSI: \\"OSError\\", errno.EL2HLT: \\"OSError\\", errno.EBADE: \\"OSError\\", errno.EBADR: \\"OSError\\", errno.EXFULL: \\"OSError\\", errno.ENOANO: \\"OSError\\", errno.EBADRQC: \\"OSError\\", errno.EBADSLT: \\"OSError\\", errno.EBFONT: \\"OSError\\", errno.ENOSTR: \\"OSError\\", errno.ENODATA: \\"OSError\\", errno.ETIME: \\"OSError\\", errno.ENOSR: \\"OSError\\", errno.ENONET: \\"OSError\\", errno.ENOPKG: \\"OSError\\", errno.EREMOTE: \\"OSError\\", errno.ENOLINK: \\"OSError\\", errno.EADV: \\"OSError\\", errno.ESRMNT: \\"OSError\\", errno.ECOMM: \\"OSError\\", errno.EPROTO: \\"OSError\\", errno.EMULTIHOP: \\"OSError\\", errno.EDOTDOT: \\"OSError\\", errno.EREMCHG: \\"OSError\\", errno.EOWNERDEAD: \\"OSError\\", errno.ENOTRECOVERABLE: \\"OSError\\" } report = [] for code in error_codes: error_info = { \\"error_code\\": code, \\"error_name\\": errno.errorcode.get(code, \\"UNKNOWN\\"), \\"error_message\\": os.strerror(code), \\"exception_class\\": exception_mapping.get(code, \\"No direct mapping\\") } report.append(error_info) return report"},{"question":"You are tasked with visualizing the relationships between various characteristics of penguins from the Palmer Archipelago. The dataset `penguins` contains the following columns: - `species`: Species of the penguin (`Adelie`, `Chinstrap`, `Gentoo`) - `island`: Island where the penguin is located - `bill_length_mm`: Bill length of the penguin - `bill_depth_mm`: Bill depth of the penguin - `flipper_length_mm`: Flipper length of the penguin - `body_mass_g`: Body mass of the penguin - `sex`: Sex of the penguin Your task is to use seaborn\'s `PairGrid` class to create a visualization grid with the following specifications: 1. The grid should include the variables `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. 2. The diagonal elements should display a histogram (`histplot`) without hue differentiation. 3. The off-diagonal elements should display scatter plots (`scatterplot`) colored by the penguin\'s `species`. 4. Add an appropriate legend for the species. Write a function `visualize_penguins()` that: - Loads the `penguins` dataset. - Creates the specified `PairGrid` visualization. - Displays the plot. ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Specify the variables to include in the grid variables = [\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\", \\"body_mass_g\\"] # Create the PairGrid with the specified variables and hue g = sns.PairGrid(penguins, vars=variables, hue=\\"species\\") # Map the diagonal plots to histograms without hue g.map_diag(sns.histplot, hue=None, color=\\".3\\") # Map the off-diagonal plots to scatter plots with hue differentiation g.map_offdiag(sns.scatterplot) # Add a legend for the species g.add_legend() # Display the plot plt.show() # Run the function to visualize the penguins dataset visualize_penguins() ``` # Input: - No inputs required as the function should load the dataset internally. # Output: - The function should display a `PairGrid` plot with the specified configurations. # Constraints: - The function should utilize seaborn for all plotting. - Ensure correct handling of any missing data in the dataset by seaborn\'s default behavior. # Performance Requirements: - The function should execute efficiently without unnecessary computations. - The plot rendering should be efficient and not overly complex.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins(): Loads the penguins dataset and creates a PairGrid visualization with histograms on the diagonal and scatter plots off-diagonal. # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Filter the dataset to include only rows with no missing values in the specified variables penguins = penguins.dropna(subset=[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\", \\"body_mass_g\\"]) # Specify the variables to include in the grid variables = [\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\", \\"body_mass_g\\"] # Create the PairGrid with the specified variables and hue g = sns.PairGrid(penguins, vars=variables, hue=\\"species\\") # Map the diagonal plots to histograms without hue g.map_diag(sns.histplot, hue=None, color=\\".3\\") # Map the off-diagonal plots to scatter plots with hue differentiation g.map_offdiag(sns.scatterplot) # Add a legend for the species g.add_legend() # Display the plot plt.show() # Run the function to visualize the penguins dataset visualize_penguins()"},{"question":"# Question You are given a class called `ExportedProgram` from PyTorch. This class encapsulates an Export IR graph, which represents a computational graph of a PyTorch model. Your task is to implement a function `transform_graph` that takes an `ExportedProgram` object and a transformation function, and applies this transformation to every `call_function` node in the graph. The transformation function should take a `Node` as input and return a modified `Node`. Function Signature ```python def transform_graph(exported_program: torch.export.ExportedProgram, transform_fn: Callable[[torch.fx.Node], torch.fx.Node]) -> None: pass ``` Input - `exported_program` (torch.export.ExportedProgram): The exported program containing the computational graph. - `transform_fn` (Callable[[torch.fx.Node], torch.fx.Node]): A function that takes a `call_function` node as input and returns a transformed `Node`. Output - The function does not return anything. It modifies the graph in-place by applying the transformation to all `call_function` nodes. Example Given the following transformation function: ```python def transform_fn(node: torch.fx.Node) -> torch.fx.Node: # Transform every `call_function` node by changing its name to \\"transformed_<original_name>\\" node.name = f\\"transformed_{node.name}\\" return node ``` And the graph of the exported program: ``` graph(): %x : [num_users=1] = placeholder[target=x] %y : [num_users=1] = placeholder[target=y] %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%x, %y), kwargs = {}) return (add,) ``` After applying `transform_graph(exported_program, transform_fn)`, the graph should be modified to: ``` graph(): %x : [num_users=1] = placeholder[target=x] %y : [num_users=1] = placeholder[target=y] %transformed_add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%x, %y), kwargs = {}) return (transformed_add,) ``` Constraints - You may assume that the `ExportedProgram` object is well-formed (i.e., it contains a valid directed acyclic graph structure with the appropriate nodes). - The graph may contain multiple types of nodes (`placeholder`, `call_function`, `output`, `get_attr`), but you only need to transform `call_function` nodes. Note - You can use the provided documentation to understand the structure of the graph and the nodes. - Make sure to handle all potential edge cases and ensure that the program\'s graph structure remains valid after transformation.","solution":"import torch from torch.fx import GraphModule, Node from typing import Callable def transform_graph(exported_program: torch.fx.GraphModule, transform_fn: Callable[[Node], Node]) -> None: Applies a transformation to every `call_function` node in the graph. Parameters: - exported_program : torch.fx.GraphModule : The exported program containing the computational graph. - transform_fn : Callable[[Node], Node] : A function that takes a `call_function` node as input and returns a transformed `Node`. for node in exported_program.graph.nodes: if node.op == \'call_function\': # Transform the node using the provided transformation function transformed_node = transform_fn(node) # Replace the original node with the transformed node node.name = transformed_node.name # Recompile the graph to apply changes exported_program.recompile()"},{"question":"**Abstract Data Structure Implementation** # Objective Implement a custom abstract data structure called `CustomCollection` using Python\'s `abc` module. You will also create a concrete subclass that implements the necessary abstract methods. # Requirements 1. **Abstract Base Class: `CustomCollection`** - The class should be an abstract base class derived from `abc.ABC`. - Define the following abstract methods: - `add(self, item)`: Adds an item to the collection. - `remove(self, item)`: Removes an item from the collection. - `contains(self, item)`: Checks if an item is present in the collection. - `size(self)`: Returns the number of items in the collection. - Implement a concrete method: - `clear(self)`: Clears all items from the collection using the abstract methods. 2. **Concrete Subclass: `SimpleListCollection`** - This class should inherit from `CustomCollection`. - Implement all abstract methods defined in `CustomCollection`. - Use a list for storing items internally. # Constraints - Do not use built-in collections like `set` or `dict` directly to implement methods. - The `SimpleListCollection` should not allow duplicate items. # Input and Output Format - You do not need to provide a main function to handle inputs and outputs. Focus on class implementation. # Example Usage ```python from abc import ABC, abstractmethod # Abstract Base Class class CustomCollection(ABC): @abstractmethod def add(self, item): pass @abstractmethod def remove(self, item): pass @abstractmethod def contains(self, item): pass @abstractmethod def size(self): pass def clear(self): while self.size() > 0: item = next(iter(self)) self.remove(item) class SimpleListCollection(CustomCollection): def __init__(self): self._items = [] def add(self, item): if not self.contains(item): self._items.append(item) def remove(self, item): if self.contains(item): self._items.remove(item) def contains(self, item): return item in self._items def size(self): return len(self._items) def __iter__(self): return iter(self._items) # Demo collection = SimpleListCollection() collection.add(1) collection.add(2) print(collection.size()) # Output: 2 print(collection.contains(1)) # Output: True collection.remove(1) print(collection.contains(1)) # Output: False collection.clear() print(collection.size()) # Output: 0 ``` # Note - Ensure that the `SimpleListCollection` class properly handles duplicate checks and maintains the constraints mentioned above. - Your solution will be evaluated based on correctness, efficiency, and adherence to object-oriented principles.","solution":"from abc import ABC, abstractmethod class CustomCollection(ABC): @abstractmethod def add(self, item): pass @abstractmethod def remove(self, item): pass @abstractmethod def contains(self, item): pass @abstractmethod def size(self): pass def clear(self): while self.size() > 0: # To remove each item, we iterate using list copy to avoid modification during iteration for item in list(self): self.remove(item) class SimpleListCollection(CustomCollection): def __init__(self): self._items = [] def add(self, item): if not self.contains(item): self._items.append(item) def remove(self, item): if self.contains(item): self._items.remove(item) def contains(self, item): return item in self._items def size(self): return len(self._items) def __iter__(self): return iter(self._items)"},{"question":"# Question You are provided with a deep learning model that you need to run on a CUDA-enabled GPU. To optimize the performance of the model, you need to configure several CUDA backend settings in PyTorch. Your task is to write code that does the following: 1. Check if CUDA is available. 2. If CUDA is available, enable the use of TensorFloat-32 tensor cores for matrix multiplications. 3. Allow reduced precision reductions for both fp16 and bf16 GEMMs. 4. Optimize the cuFFT plan cache by setting its maximum size to 4096 and clearing any existing plans. 5. Ensure all changes were correctly applied by printing out the current configurations. Expected Input and Output Formats: - **Input:** No input is required from the user. The task is to implement the required configurations in PyTorch. - **Output:** ```plaintext CUDA available: True TF32 tensor cores enabled: True FP16 reduced precision reduction allowed: True BF16 reduced precision reduction allowed: True cuFFT plan cache max size: 4096 cuFFT plan cache size after clear: 0 ``` If CUDA is not available, the output should be: ```plaintext CUDA available: False ``` Constraints: - Ensure the code can run on both environments where CUDA might or might not be available. Performance Requirements: - The solution should optimize CUDA settings efficiently without unnecessary performance overhead. Implementation Notes: - Use relevant PyTorch backend functions and attributes to achieve the tasks. - Make sure to handle scenarios where CUDA is not available by not attempting settings that only apply if CUDA is present. ```python import torch def configure_cuda_backend(): is_cuda_available = torch.backends.cuda.is_available() print(f\\"CUDA available: {is_cuda_available}\\") if is_cuda_available: # Enable TensorFloat-32 (TF32) tensor cores for matrix multiplications torch.backends.cuda.matmul.allow_tf32 = True print(f\\"TF32 tensor cores enabled: {torch.backends.cuda.matmul.allow_tf32}\\") # Allow reduced precision reductions torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True print(f\\"FP16 reduced precision reduction allowed: {torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction}\\") torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True print(f\\"BF16 reduced precision reduction allowed: {torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction}\\") # Optimize cuFFT plan cache cufft_plan_cache = torch.backends.cuda.cufft_plan_cache cufft_plan_cache.max_size = 4096 cufft_plan_cache.clear() print(f\\"cuFFT plan cache max size: {cufft_plan_cache.max_size}\\") print(f\\"cuFFT plan cache size after clear: {cufft_plan_cache.size}\\") # Run the configuration function configure_cuda_backend() ```","solution":"import torch def configure_cuda_backend(): is_cuda_available = torch.cuda.is_available() print(f\\"CUDA available: {is_cuda_available}\\") if is_cuda_available: # Enable TensorFloat-32 (TF32) tensor cores for matrix multiplications torch.backends.cuda.matmul.allow_tf32 = True print(f\\"TF32 tensor cores enabled: {torch.backends.cuda.matmul.allow_tf32}\\") # Allow reduced precision reductions torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True print(f\\"FP16 reduced precision reduction allowed: {torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction}\\") torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True print(f\\"BF16 reduced precision reduction allowed: {torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction}\\") # Optimize cuFFT plan cache cufft_plan_cache = torch.backends.cuda.cufft_plan_cache cufft_plan_cache.max_size = 4096 cufft_plan_cache.clear() print(f\\"cuFFT plan cache max size: {cufft_plan_cache.max_size}\\") print(f\\"cuFFT plan cache size after clear: {cufft_plan_cache.size}\\") # Run the configuration function configure_cuda_backend()"},{"question":"# Seaborn Assessment Question Objective Write a Python function to generate a comprehensive visualization using seaborn, which demonstrates your understanding of advanced features. Your function should: 1. Load the `fmri` dataset from seaborn. 2. Create a grid of line plots separated by the `region` column. 3. Each line plot should show the `signal` over `timepoint`. 4. Differentiate lines in each plot with both `hue` and `style` semantics using the `event` column. 5. Customize the appearance of the lines by adding markers, removing dashes, and setting a distinct color palette. 6. Include error bars representing the standard error of the mean. Function Signature ```python def create_complex_fmri_plot(): pass ``` Expected Output A grid of line plots showing `signal` over `timepoint` for each `region`. Each line should be color and style coded by `event`, with markers for points, no dashes, and a custom color palette applied. Error bars should represent the standard error of the mean. Implementation Notes * Use the `sns.set_theme()` to set the seaborn theme at the beginning. * Use the `sns.load_dataset(\\"fmri\\")` to load the dataset. * Use `sns.relplot` to create the grid of plots. * Use `sns.color_palette` to create a distinct color palette. * Set markers and remove dashes in the plot. Constraints * Ensure that the code is clear, concise, and correctly commented. * Handle plotting without any errors even if there are NaN values in the data.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_complex_fmri_plot(): # Set the seaborn theme for the plot aesthetics sns.set_theme(style=\\"darkgrid\\") # Load the fmri dataset from seaborn fmri = sns.load_dataset(\\"fmri\\") # Create a custom color palette custom_palette = sns.color_palette(\\"viridis\\", as_cmap=False) # Create a line plot with facets for different regions g = sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", col=\\"region\\", hue=\\"event\\", style=\\"event\\", kind=\\"line\\", markers=True, dashes=False, ci=\\"sd\\", palette=custom_palette ) # Adjust the titles and labels for clarity g.set_axis_labels(\\"Timepoint\\", \\"Signal\\") g.set_titles(col_template=\\"{col_name} Region\\") # Display the plot plt.show()"},{"question":"# Question: Custom Serialization and Deserialization with `json` You have been provided with a custom Python class `Person` which represents a person\'s information. Your task is to implement custom serialization and deserialization of instances of this class using the `json` module. Requirements: 1. **Person Class**: - `Person` has the following attributes: - `name` (str) - `age` (int) - `birthdate` (datetime.date) 2. **Custom Serializer**: - Implement a custom encoder by subclassing `json.JSONEncoder` to handle the serialization of `Person` objects. The JSON representation should be an object with keys \\"name\\", \\"age\\", and \\"birthdate\\" (formatted as \\"YYYY-MM-DD\\"). 3. **Custom Deserializer**: - Implement a custom decoder that converts a JSON object back into a `Person` object. Use the `object_hook` parameter of `json.loads()` for this purpose. The JSON object will have keys \\"name\\", \\"age\\", and \\"birthdate\\" (formatted as \\"YYYY-MM-DD\\"). 4. **Validation**: - Write a function `validate_serialization()` that: - Creates an instance of `Person`. - Serializes it to a JSON string using your custom serializer. - Deserializes the JSON string back to a `Person` object. - Asserts that the original and deserialized `Person` instances are equivalent. You can use the provided `datetime` module for handling date conversions. Solution Template: ```python import json from datetime import datetime, date from typing import Any class Person: def __init__(self, name: str, age: int, birthdate: date): self.name = name self.age = age self.birthdate = birthdate def __eq__(self, other): return (self.name == other.name and self.age == other.age and self.birthdate == other.birthdate) class PersonEncoder(json.JSONEncoder): def default(self, obj: Any) -> Any: if isinstance(obj, Person): return { \\"name\\": obj.name, \\"age\\": obj.age, \\"birthdate\\": obj.birthdate.strftime(\'%Y-%m-%d\') } return super().default(obj) def person_decoder(dct: dict) -> Any: if \\"name\\" in dct and \\"age\\" in dct and \\"birthdate\\" in dct: return Person( name=dct[\\"name\\"], age=dct[\\"age\\"], birthdate=datetime.strptime(dct[\\"birthdate\\"], \'%Y-%m-%d\').date() ) return dct def validate_serialization(): original_person = Person(name=\\"John Doe\\", age=30, birthdate=date(1992, 1, 1)) json_str = json.dumps(original_person, cls=PersonEncoder) deserialized_person = json.loads(json_str, object_hook=person_decoder) assert original_person == deserialized_person, \\"Serialization/Deserialization failed\\" # Call validate_serialization to run validation validate_serialization() ``` Instructions: - Complete the implementation of the `PersonEncoder` class and the `person_decoder` function. - Call the `validate_serialization` function to test your implementation. Tip: Make sure to thoroughly test your implementation with various `Person` instances to ensure robust functionality.","solution":"import json from datetime import datetime, date from typing import Any class Person: def __init__(self, name: str, age: int, birthdate: date): self.name = name self.age = age self.birthdate = birthdate def __eq__(self, other): return (self.name == other.name and self.age == other.age and self.birthdate == other.birthdate) class PersonEncoder(json.JSONEncoder): def default(self, obj: Any) -> Any: if isinstance(obj, Person): return { \\"name\\": obj.name, \\"age\\": obj.age, \\"birthdate\\": obj.birthdate.strftime(\'%Y-%m-%d\') } return super().default(obj) def person_decoder(dct: dict) -> Any: if \\"name\\" in dct and \\"age\\" in dct and \\"birthdate\\" in dct: return Person( name=dct[\\"name\\"], age=dct[\\"age\\"], birthdate=datetime.strptime(dct[\\"birthdate\\"], \'%Y-%m-%d\').date() ) return dct def validate_serialization(): original_person = Person(name=\\"John Doe\\", age=30, birthdate=date(1992, 1, 1)) json_str = json.dumps(original_person, cls=PersonEncoder) deserialized_person = json.loads(json_str, object_hook=person_decoder) assert original_person == deserialized_person, \\"Serialization/Deserialization failed\\" # Call validate_serialization to run validation validate_serialization()"},{"question":"# Complex Tensor Operations and Automatic Differentiation in PyTorch Objective: Write a function that performs a series of operations on complex tensors and returns specific results. The function should demonstrate proficiency in handling complex numbers, linear algebra operations, and PyTorch\'s autograd. Function Signature: ```python import torch def complex_tensor_operations(): This function performs the following operations: 1. Creates a complex tensor `A` of shape (3, 3) with values of your choice. 2. Creates another complex tensor `B` of shape (3, 3) with values of your choice. 3. Computes the product `C` of tensors `A` and `B` using matrix multiplication. 4. Computes the element-wise absolute values and angles of `C`. 5. Calculates the gradient of the sum of absolute values of `C` with respect to `A` using autograd. Returns: - C_abs: Tensor of element-wise absolute values of `C`. - C_angle: Tensor of element-wise angles of `C`. - grad_A: Gradient of the sum of absolute values of `C` with respect to `A`. pass ``` Constraints: - You are free to choose the values for tensors `A` and `B`. - Use the appropriate complex dtype for the tensors. - The function should utilize PyTorch\'s autograd for gradient computation. Expected Output: The function should return three tensors: 1. `C_abs` - Tensor of element-wise absolute values of `C`. 2. `C_angle` - Tensor of element-wise angles of `C`. 3. `grad_A` - Gradient of the sum of absolute values of `C` with respect to `A`. Example: ```python # Example usage of the function C_abs, C_angle, grad_A = complex_tensor_operations() print(\\"Absolute values of C:\\", C_abs) print(\\"Angles of C:\\", C_angle) print(\\"Gradient of the sum of absolute values of C with respect to A:\\", grad_A) ``` The implementation should make proper use of complex tensor operations, matrix multiplication, element-wise operations, and autograd capabilities in PyTorch.","solution":"import torch def complex_tensor_operations(): This function performs the following operations: 1. Creates a complex tensor `A` of shape (3, 3) with values of your choice. 2. Creates another complex tensor `B` of shape (3, 3) with values of your choice. 3. Computes the product `C` of tensors `A` and `B` using matrix multiplication. 4. Computes the element-wise absolute values and angles of `C`. 5. Calculates the gradient of the sum of absolute values of `C` with respect to `A` using autograd. Returns: - C_abs: Tensor of element-wise absolute values of `C`. - C_angle: Tensor of element-wise angles of `C`. - grad_A: Gradient of the sum of absolute values of `C` with respect to `A`. # Step 1: Create complex tensor A A_real = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], dtype=torch.float32) A_imag = torch.tensor([[1.0, -1.0, 1.0], [-1.0, 1.0, -1.0], [1.0, -1.0, 1.0]], dtype=torch.float32) A = torch.complex(A_real, A_imag).clone().detach().requires_grad_(True) # Step 2: Create complex tensor B B_real = torch.tensor([[1.0, -2.0, 1.5], [2.0, 3.0, -1.0], [1.0, 4.0, 0.5]], dtype=torch.float32) B_imag = torch.tensor([[0.5, 2.0, -1.5], [-2.0, -1.0, 2.0], [1.0, 0.5, 2.5]], dtype=torch.float32) B = torch.complex(B_real, B_imag) # Step 3: Compute the product of tensors A and B using matrix multiplication C = torch.matmul(A, B) # Step 4: Compute the element-wise absolute values and angles of C C_abs = torch.abs(C) C_angle = torch.angle(C) # Step 5: Calculate the gradient of the sum of absolute values of C with respect to A using autograd sum_C_abs = C_abs.sum() sum_C_abs.backward() grad_A = A.grad return C_abs, C_angle, grad_A"},{"question":"**Objective**: Demonstrate your understanding of the `zoneinfo` module by writing a function that works with time zones and handles typical operations involving `ZoneInfo`. # Problem Statement You need to implement a function `schedule_meeting(meeting_times, zones)`, which schedules meetings across different time zones and adjusts for daylight saving time where applicable. Function Signature ```python def schedule_meeting(meeting_times: List[Tuple[str, str]], zones: List[str]) -> List[str]: ``` Input - `meeting_times`: A list of tuples, where each tuple contains two strings: - `meeting_times[i][0]`: A datetime string in the format \\"YYYY-MM-DD HH:MM\\". - `meeting_times[i][1]`: A string representing the time zone key (e.g., \\"America/New_York\\"). - `zones`: A list of time zone keys to which the meeting times should be converted (e.g., [\\"America/Los_Angeles\\", \\"Europe/London\\"]). Output - Returns a list of strings, where each string represents the meeting time adjusted to one of the time zones in the `zones` list. Each converted meeting time should be in the format \\"YYYY-MM-DD HH:MM [Zone Name]\\". Constraints - Assume all input datetime strings are valid and all keys represent valid IANA time zones. - Handle scenarios with daylight saving time transitions appropriately. Example ```python meeting_times = [(\\"2023-03-12 09:00\\", \\"America/New_York\\"), (\\"2023-11-05 09:00\\", \\"America/New_York\\")] zones = [\\"America/Los_Angeles\\", \\"Europe/London\\"] print(schedule_meeting(meeting_times, zones)) # Expected Output: # [ # \\"2023-03-12 06:00 [America/Los_Angeles]\\", # \\"2023-03-12 13:00 [Europe/London]\\", # \\"2023-11-05 06:00 [America/Los_Angeles]\\", # \\"2023-11-05 14:00 [Europe/London]\\" # ] ``` # Notes 1. Use the `ZoneInfo` class from the `zoneinfo` module to handle time zones. 2. Each meeting time should be correctly adjusted for the time zone shifts, including handling of daylight saving time transitions. 3. Ensure the function handles the correct `tzinfo` for each meeting time, converts them, and outputs the converted times in the correct format. # Instructions 1. Import necessary modules (`zoneinfo` and `datetime`). 2. Create `datetime` objects with the correct `ZoneInfo` for each input time zone. 3. Use appropriate methods to convert these datetime objects to the desired time zones. 4. Return the formatted datetime strings for the converted meeting times.","solution":"from datetime import datetime from zoneinfo import ZoneInfo from typing import List, Tuple def schedule_meeting(meeting_times: List[Tuple[str, str]], zones: List[str]) -> List[str]: result = [] for meeting_time, original_zone in meeting_times: # Parse the original meeting time with its timezone dt = datetime.strptime(meeting_time, \\"%Y-%m-%d %H:%M\\").replace(tzinfo=ZoneInfo(original_zone)) for zone in zones: # Convert the time to the target timezone converted_dt = dt.astimezone(ZoneInfo(zone)) # Format the datetime in the required format formatted_time = converted_dt.strftime(\\"%Y-%m-%d %H:%M\\") + f\\" [{zone}]\\" result.append(formatted_time) return result"},{"question":"# Quantizing a Convolutional Neural Network with PyTorch **Objective:** You need to implement and demonstrate static post-training quantization or quantization-aware training on a simple convolutional neural network (CNN) using PyTorch. Your goal is to showcase proficiency with PyTorch’s quantization APIs and accurately apply quantization techniques to a neural network model. **Task:** 1. Define a CNN model class that supports quantization. The model should include: - QuantStub and DeQuantStub to handle tensor conversion. - At least one convolutional layer followed by a ReLU activation. 2. Implement static quantization on the defined model: - Set your model to evaluation mode. - Attach a suitable qconfig. - Perform module fusion (if applicable). - Prepare the model for static quantization and calibrate it using sample data. - Convert the model to the quantized version. 3. Alternatively, implement quantization-aware training: - Define your model similarly with QuantStub and DeQuantStub. - Attach a QAT-specific qconfig. - Prepare the model for QAT. - Simulate a training loop (no need for actual training). - Convert the model to the quantized version. 4. Finally, run inference on quantized data and ensure it produces outputs without errors. **Constraints:** - Use PyTorch’s `torch.ao.quantization` or `torch.ao.quantization.fx` modules. - Ensure your quantized model outputs correct tensor shapes/types without runtime errors. - For calibration and QAT simulation, use dummy data. This data can be purely random tensors compatible with your CNN model’s expected input shape. **Input/Output:** - Input: None directly from the user; model architecture, and dummy data should be defined in-line. - Output: The quantized model should perform inference demonstrating both forward-pass computation and quantization efficacy. **Performance Requirements:** - The quantized model should be efficient and compatible with the selected backend, defaulting to applicable PyTorch settings. **Example Starter Code:** ```python import torch import torch.nn as nn import torch.ao.quantization as quantization class QuantizedCNN(nn.Module): def __init__(self): super(QuantizedCNN, self).__init__() self.quant = quantization.QuantStub() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.relu = nn.ReLU() self.dequant = quantization.DeQuantStub() def forward(self, x): x = self.quant(x) x = self.conv1(x) x = self.relu(x) x = self.dequant(x) return x # Static Quantization Example: def static_quantization_example(): # 1. Create and evaluate the model instance model = QuantizedCNN().eval() # 2. Attach a qconfig model.qconfig = quantization.get_default_qconfig(\'x86\') # 3. Fuse modules if needed fused_model = torch.ao.quantization.fuse_modules(model, [[\'conv1\', \'relu\']]) # 4. Prepare the model for static quantization prepared_model = quantization.prepare(fused_model) # 5. Calibrate the model dummy_input = torch.randn(1, 3, 224, 224) prepared_model(dummy_input) # Model calibration # 6. Convert to quantized model quantized_model = quantization.convert(prepared_model) # 7. Test quantized model with a dummy input output = quantized_model(dummy_input) print(\\"Quantized model output shape:\\", output.shape) # Optional: Define Quantization Aware Training Example def qat_example(): # 1. Create and train the model instance (simulated) model = QuantizedCNN().train() model.qconfig = quantization.get_default_qat_qconfig(\'x86\') # 2. Fuse modules if needed fused_model = torch.ao.quantization.fuse_modules(model, [[\'conv1\', \'relu\']]) # 3. Prepare the model for QAT prepared_model = quantization.prepare_qat(fused_model) # 4. Simulate training dummy_input = torch.randn(1, 3, 224, 224) prepared_model(dummy_input) # Forward pass # 5. Convert to quantized model prepared_model.eval() quantized_model = quantization.convert(prepared_model) # 6. Test quantized model with a dummy input output = quantized_model(dummy_input) print(\\"Quantized model output shape:\\", output.shape) # Uncomment to execute one of the examples # static_quantization_example() # qat_example() ``` Provide a code implementation and ensure it runs without errors while producing an output tensor in the final quantized inference.","solution":"import torch import torch.nn as nn import torch.ao.quantization as quantization class QuantizedCNN(nn.Module): def __init__(self): super(QuantizedCNN, self).__init__() self.quant = quantization.QuantStub() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.relu = nn.ReLU() self.dequant = quantization.DeQuantStub() def forward(self, x): x = self.quant(x) x = self.conv1(x) x = self.relu(x) x = self.dequant(x) return x # Static Quantization Example: def static_quantization_example(): # 1. Create and evaluate the model instance model = QuantizedCNN().eval() # 2. Attach a qconfig model.qconfig = quantization.get_default_qconfig(\'fbgemm\') # 3. Fuse modules model_fused = torch.ao.quantization.fuse_modules(model, [[\'conv1\', \'relu\']]) # 4. Prepare the model for static quantization model_prepared = quantization.prepare(model_fused) # 5. Calibrate the model dummy_input = torch.randn(1, 3, 224, 224) model_prepared(dummy_input) # Model calibration # 6. Convert to quantized model model_quantized = quantization.convert(model_prepared) # 7. Test quantized model with a dummy input output = model_quantized(dummy_input) return output.shape # Quantization-Aware Training Example: def qat_example(): # 1. Create and train the model instance (simulated) model = QuantizedCNN().train() # 2. Attach a qconfig model.qconfig = quantization.get_default_qat_qconfig(\'fbgemm\') # 3. Fuse modules model_fused = torch.ao.quantization.fuse_modules(model, [[\'conv1\', \'relu\']]) # 4. Prepare the model for QAT model_prepared = quantization.prepare_qat(model_fused) # 5. Simulate training dummy_input = torch.randn(1, 3, 224, 224) model_prepared(dummy_input) # Forward pass # 6. Convert to quantized model model_prepared.eval() model_quantized = quantization.convert(model_prepared) # 7. Test quantized model with a dummy input output = model_quantized(dummy_input) return output.shape # Uncomment to run static quantization example # print(static_quantization_example()) # Uncomment to run QAT example # print(qat_example())"},{"question":"# Semi-Supervised Learning with scikit-learn **Objective** You are provided with a dataset containing both labeled and unlabeled data points. Your task is to implement and compare the performance of different semi-supervised learning algorithms available in scikit-learn: `SelfTrainingClassifier`, `LabelPropagation`, and `LabelSpreading`. **Dataset** You will use the Iris dataset, which contains 150 samples with 4 features each. This dataset includes 3 classes: Setosa, Versicolour, and Virginica. However, for this task, some of the labels will be hidden. **Instructions** 1. **Load and Prepare Data:** - Load the Iris dataset using `sklearn.datasets.load_iris`. - Randomly remove the labels of 50% of the samples, replacing them with `-1` to denote unlabeled data. 2. **Implement Semi-Supervised Learning:** - Train three models: `SelfTrainingClassifier`, `LabelPropagation`, and `LabelSpreading` on the prepared data. - Use `sklearn\'s` `DecisionTreeClassifier` as the base estimator for `SelfTrainingClassifier`. 3. **Evaluate Models:** - Evaluate the performance of each model using accuracy on the labeled portion of the dataset. - Output predictions for the entire dataset and calculate accuracy only for the originally labeled points. **Expected Output** - Accuracy scores of `SelfTrainingClassifier`, `LabelPropagation`, and `LabelSpreading` on labeled data after semi-supervised learning. **Constraints** - Use the default parameters for all classifiers unless specified. - Ensure that the random seed is set for reproducibility. - Labels for unlabeled data should be assigned as `-1`. **Performance Requirements** - The solution should run efficiently on a standard machine within acceptable time limits. **Code Template** ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation, LabelSpreading from sklearn.metrics import accuracy_score # Set random seed for reproducibility np.random.seed(42) # Load dataset data = load_iris() X = data.data y = data.target # Randomly replace 50% of the labels with -1 n_labeled = len(y) // 2 indices_unlabeled = np.random.choice(len(y), size=n_labeled, replace=False) y_unlabeled = y.copy() y_unlabeled[indices_unlabeled] = -1 # Train SelfTrainingClassifier base_classifier = DecisionTreeClassifier() self_training_model = SelfTrainingClassifier(base_classifier) self_training_model.fit(X, y_unlabeled) # Train LabelPropagation label_propagation_model = LabelPropagation() label_propagation_model.fit(X, y_unlabeled) # Train LabelSpreading label_spreading_model = LabelSpreading() label_spreading_model.fit(X, y_unlabeled) # Evaluate models on the labeled portion of the dataset y_true = y[indices_unlabeled == False] # SelfTrainingClassifier predictions and accuracy y_pred_self_training = self_training_model.predict(X[indices_unlabeled == False]) accuracy_self_training = accuracy_score(y_true, y_pred_self_training) # LabelPropagation predictions and accuracy y_pred_label_propagation = label_propagation_model.predict(X[indices_unlabeled == False]) accuracy_label_propagation = accuracy_score(y_true, y_pred_label_propagation) # LabelSpreading predictions and accuracy y_pred_label_spreading = label_spreading_model.predict(X[indices_unlabeled == False]) accuracy_label_spreading = accuracy_score(y_true, y_pred_label_spreading) # Output the accuracies print(f\\"SelfTrainingClassifier Accuracy: {accuracy_self_training}\\") print(f\\"LabelPropagation Accuracy: {accuracy_label_propagation}\\") print(f\\"LabelSpreading Accuracy: {accuracy_label_spreading}\\") ``` **Notes** - You are allowed to use helper functions to make the code modular. - Add comments to explain your code for clarity.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation, LabelSpreading from sklearn.metrics import accuracy_score def load_and_prepare_data(random_seed=42): Load the Iris dataset and randomly remove 50% of the labels. Args: random_seed: int, the seed for the random number generator (default: 42) Returns: X: ndarray, the feature matrix y: ndarray, the labels including some -1 values for unlabeled points y_full: ndarray, the complete original labels np.random.seed(random_seed) data = load_iris() X = data.data y = data.target # Randomly replace 50% of the labels with -1 n_labeled = len(y) // 2 indices_unlabeled = np.random.choice(len(y), size=n_labeled, replace=False) y_unlabeled = y.copy() y_unlabeled[indices_unlabeled] = -1 return X, y_unlabeled, y def train_and_evaluate_models(X, y_unlabeled, y_true, indices_unlabeled): Train the semi-supervised learning models and evaluate their performance. Args: X: ndarray, the feature matrix y_unlabeled: ndarray, the labels including some -1 values for unlabeled points y_true: ndarray, the true labels for the labeled subset indices_unlabeled: ndarray, indices that denote which points were unlabeled Returns: accuracies: dict, contains accuracy scores of SelfTrainingClassifier, LabelPropagation, and LabelSpreading accuracies = {} # Train SelfTrainingClassifier base_classifier = DecisionTreeClassifier() self_training_model = SelfTrainingClassifier(base_classifier) self_training_model.fit(X, y_unlabeled) y_pred_self_training = self_training_model.predict(X[indices_unlabeled == False]) accuracies[\'SelfTrainingClassifier\'] = accuracy_score(y_true, y_pred_self_training) # Train LabelPropagation label_propagation_model = LabelPropagation() label_propagation_model.fit(X, y_unlabeled) y_pred_label_propagation = label_propagation_model.predict(X[indices_unlabeled == False]) accuracies[\'LabelPropagation\'] = accuracy_score(y_true, y_pred_label_propagation) # Train LabelSpreading label_spreading_model = LabelSpreading() label_spreading_model.fit(X, y_unlabeled) y_pred_label_spreading = label_spreading_model.predict(X[indices_unlabeled == False]) accuracies[\'LabelSpreading\'] = accuracy_score(y_true, y_pred_label_spreading) return accuracies # Load and prepare the data X, y_unlabeled, y = load_and_prepare_data() # Identify the labeled indices indices_unlabeled = y_unlabeled == -1 # Get the original labels for the remaining data points y_true = y[~indices_unlabeled] # Train the models and evaluate their performance accuracies = train_and_evaluate_models(X, y_unlabeled, y_true, ~indices_unlabeled) # Output the accuracies accuracies"},{"question":"# Kernel Approximation Comparison **Objective:** Implement and compare the performance of the Nystroem method and RBFSampler for kernel approximation in a machine learning pipeline. You will use these approximations to train a classifier and evaluate its accuracy. **Problem Statement:** 1. Load the `digits` dataset from `sklearn.datasets`. 2. Split the data into training (80%) and testing (20%) sets. 3. Implement two separate pipelines using the following kernel approximation techniques: - Nystroem method with RBF kernel. - RBFSampler. 4. Train an `SGDClassifier` in both pipelines. 5. Evaluate and compare the accuracy of both models on the test set. **Details:** - You can use the default parameters for the kernel approximation techniques where not specified. - You must use `SGDClassifier` for training in both pipelines. - Output both the training time and accuracy for each model. **Input:** No input required. **Output:** The output should be a dictionary with keys `nystroem_accuracy`, `rbf_sampler_accuracy`, `nystroem_training_time`, and `rbf_sampler_training_time`. The values should be the accuracy on the test set and training times for each method. Example format: ```python { \\"nystroem_accuracy\\": <accuracy>, \\"rbf_sampler_accuracy\\": <accuracy>, \\"nystroem_training_time\\": <time in seconds>, \\"rbf_sampler_training_time\\": <time in seconds> } ``` **Constraints:** - Use `random_state=1` for reproducibility where applicable. - The dataset has fixed characteristics (8x8 images of digits). **Performance Requirements:** - The solution should be performant enough to run within a reasonable time (few seconds) on a standard machine. **Here is a skeleton to help you get started:** ```python import time from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score def kernel_approximation_comparison(): # Load dataset digits = load_digits() X, y = digits.data, digits.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # Nystroem approximation pipeline nystroem_start = time.time() nystroem_model = make_pipeline(Nystroem(random_state=1), SGDClassifier(random_state=1)) nystroem_model.fit(X_train, y_train) nystroem_end = time.time() nystroem_accuracy = accuracy_score(y_test, nystroem_model.predict(X_test)) nystroem_training_time = nystroem_end - nystroem_start # RBFSampler approximation pipeline rbf_sampler_start = time.time() rbf_sampler_model = make_pipeline(RBFSampler(random_state=1), SGDClassifier(random_state=1)) rbf_sampler_model.fit(X_train, y_train) rbf_sampler_end = time.time() rbf_sampler_accuracy = accuracy_score(y_test, rbf_sampler_model.predict(X_test)) rbf_sampler_training_time = rbf_sampler_end - rbf_sampler_start # Output results results = { \\"nystroem_accuracy\\": nystroem_accuracy, \\"rbf_sampler_accuracy\\": rbf_sampler_accuracy, \\"nystroem_training_time\\": nystroem_training_time, \\"rbf_sampler_training_time\\": rbf_sampler_training_time } return results # Example of function call print(kernel_approximation_comparison()) ```","solution":"import time from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.kernel_approximation import Nystroem, RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score def kernel_approximation_comparison(): # Load dataset digits = load_digits() X, y = digits.data, digits.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # Nystroem approximation pipeline nystroem_start = time.time() nystroem_model = make_pipeline(Nystroem(random_state=1), SGDClassifier(random_state=1)) nystroem_model.fit(X_train, y_train) nystroem_end = time.time() nystroem_accuracy = accuracy_score(y_test, nystroem_model.predict(X_test)) nystroem_training_time = nystroem_end - nystroem_start # RBFSampler approximation pipeline rbf_sampler_start = time.time() rbf_sampler_model = make_pipeline(RBFSampler(random_state=1), SGDClassifier(random_state=1)) rbf_sampler_model.fit(X_train, y_train) rbf_sampler_end = time.time() rbf_sampler_accuracy = accuracy_score(y_test, rbf_sampler_model.predict(X_test)) rbf_sampler_training_time = rbf_sampler_end - rbf_sampler_start # Output results results = { \\"nystroem_accuracy\\": nystroem_accuracy, \\"rbf_sampler_accuracy\\": rbf_sampler_accuracy, \\"nystroem_training_time\\": nystroem_training_time, \\"rbf_sampler_training_time\\": rbf_sampler_training_time } return results"},{"question":"# Custom Kernel Density Estimation with Scikit-Learn Problem Statement You are provided with a dataset of 2D points and your task is to implement a custom kernel function for Kernel Density Estimation (KDE) using scikit-learn. Specifically, you will implement a triangular kernel and use it with the `KernelDensity` class to estimate the density of the given points. Custom Kernel Definition The custom triangular kernel is defined as follows: [ K(x; h) = max(1 - frac{|x|}{h}, 0) ] where ( h ) is the bandwidth parameter. Input - A 2D numpy array `data` of shape (n_samples, 2) representing the dataset of 2D points. - A floating-point number `bandwidth` representing the bandwidth parameter for KDE. Output - A 1D numpy array of shape (n_samples,) containing the log density estimates for each point in the input dataset. Constraints 1. Implement the custom kernel function and integrate it with the scikit-learn `KernelDensity` class. 2. Ensure that your code efficiently handles datasets with up to 10,000 points. Example ```python import numpy as np from sklearn.neighbors import KernelDensity from sklearn.base import BaseEstimator class TriangularKernelDensity(KernelDensity): def __init__(self, bandwidth=1.0): super().__init__(bandwidth=bandwidth) def _kernel(self, distances): return np.maximum(1 - (distances / self.bandwidth), 0) def custom_kernel_density_estimation(data, bandwidth): kde = TriangularKernelDensity(bandwidth=bandwidth) kde.fit(data) log_density = kde.score_samples(data) return log_density # Example Usage data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]) bandwidth = 1.0 log_density_estimates = custom_kernel_density_estimation(data, bandwidth) print(log_density_estimates) ``` In the example above, the custom_kernel_density_estimation function computes the log-density estimates for the provided dataset using the triangular kernel. # Notes 1. You are expected to override/inherit necessary methods from the `KernelDensity` class to implement the custom kernel. 2. Make sure to test your implementation with different bandwidth values and input datasets to ensure correctness.","solution":"import numpy as np from sklearn.neighbors import KernelDensity from sklearn.base import BaseEstimator class TriangularKernelDensity(KernelDensity): def __init__(self, bandwidth=1.0): # bandwidth is required to be stored and passed to the base class KernelDensity super().__init__(bandwidth=bandwidth, kernel=\'custom\') self.bandwidth = bandwidth def _kernel(self, distances): return np.maximum(1 - np.abs(distances) / self.bandwidth, 0) def score_samples(self, X): # Compute the distance between each pair of points distances = self._compute_distances(X) kernel_values = self._kernel(distances) log_density = np.log(np.sum(kernel_values, axis=1)) - np.log(self.bandwidth) return log_density def _compute_distances(self, X): Compute the pairwise distances between the points in X distances = np.sqrt(((X[:, np.newaxis] - self.tree_.data) ** 2).sum(axis=2)) return distances def custom_kernel_density_estimation(data, bandwidth): kde = TriangularKernelDensity(bandwidth=bandwidth) kde.fit(data) log_density = kde.score_samples(data) return log_density # Example usage data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]) bandwidth = 1.0 log_density_estimates = custom_kernel_density_estimation(data, bandwidth) print(log_density_estimates)"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},F={class:"card-container"},q={key:0,class:"empty-state"},O=["disabled"],N={key:0},R={key:1};function L(n,e,l,m,r,s){const h=_("PoemCard");return a(),i("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>r.searchQuery=o),placeholder:"Search..."},null,512),[[y,r.searchQuery]]),r.searchQuery?(a(),i("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>r.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),i(b,null,v(s.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),s.displayedPoems.length===0?(a(),i("div",q,' No results found for "'+c(r.searchQuery)+'". ',1)):d("",!0)]),s.hasMorePoems?(a(),i("button",{key:0,class:"load-more-button",disabled:r.isLoading,onClick:e[2]||(e[2]=(...o)=>s.loadMore&&s.loadMore(...o))},[r.isLoading?(a(),i("span",R,"Loading...")):(a(),i("span",N,"See more"))],8,O)):d("",!0)])}const j=p(z,[["render",L],["__scopeId","data-v-7f536507"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"quotes/62.md","filePath":"quotes/62.md"}'),M={name:"quotes/62.md"},X=Object.assign(M,{setup(n){return(e,l)=>(a(),i("div",null,[x(j)]))}});export{Y as __pageData,X as default};
