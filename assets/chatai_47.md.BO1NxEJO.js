import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as x,q as w}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},S={class:"review-title"},E={class:"review-content"};function P(i,e,l,m,n,o){return a(),s("div",T,[t("div",C,[t("div",S,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",P],["__scopeId","data-v-34abdfd9"]]),A=JSON.parse('[{"question":"# **Python Input Handling Challenge** In this assessment, you are required to write a Python function that simulates the various modes of input handling as described by the Python interpreter: complete program execution, file input parsing, interactive input parsing, and expression evaluation. # **Objective** Build a function `simulate_input_handling(input_type: str, input_content: str) -> str` that processes input based on different input types and returns the result of execution. # **Function Signature** ```python def simulate_input_handling(input_type: str, input_content: str) -> str: ``` # **Input** - `input_type`: a string, one of `\\"complete_program\\"`, `\\"file_input\\"`, `\\"interactive_input\\"`, and `\\"eval_input\\"`. This specifies the type of input handling to simulate. - `input_content`: a string representing the content to be executed or evaluated based on the input type. # **Output** - A string representing the result of execution or evaluation. # **Constraints** - Assume valid Python code is provided in `input_content`. - Handle exceptions gracefully and return error messages as strings. - Do not use `exec` for \\"complete_program\\" and \\"interactive_input\\" modes for security reasons. - Safeguard against potential infinite loops in the input content. # **Requirements** 1. For `input_type == \\"complete_program\\"`, simulate calling a complete program, ensuring all built-in and standard modules are minimally initialized except `sys`, `builtins`, and `__main__`. 2. For `input_type == \\"file_input\\"`, parse the input as if it was a file execution, maintaining its form. 3. For `input_type == \\"interactive_input\\"`, handle it as interactive mode execution, executing one statement at a time. 4. For `input_type == \\"eval_input\\"`, use the `eval()` function to parse and evaluate the expression. # **Example** ```python assert simulate_input_handling(\\"complete_program\\", \'print(\\"Hello World\\")\') == \\"Hello Worldn\\" assert simulate_input_handling(\\"file_input\\", \'x = 10ny = 20nz = x + ynprint(z)\') == \\"30n\\" assert simulate_input_handling(\\"interactive_input\\", \'x = 10ny = 20nz = x + ynprint(z)\') == \\"30n\\" assert simulate_input_handling(\\"eval_input\\", \'3 + 5*2\') == \\"13\\" ``` # **Note** This function requires thorough understanding of Python\'s code execution environment and careful handling of various input types. Focus on ensuring that result consistency reflects what would be the actual output by Python interpreter in respective modes.","solution":"import io import sys def simulate_input_handling(input_type: str, input_content: str) -> str: old_stdout = sys.stdout redirected_output = sys.stdout = io.StringIO() try: if input_type == \\"complete_program\\": # We use a similar approach as running the script itself global_namespace = { \\"__name__\\": \\"__main__\\", \\"__doc__\\": None, \\"__package__\\": None, \\"__loader__\\": None, \\"__spec__\\": None, \\"__annotations__\\": {}, \\"__builtins__\\": __builtins__, } exec(input_content, global_namespace) elif input_type == \\"file_input\\": exec(input_content) elif input_type == \\"interactive_input\\": for line in input_content.splitlines(): exec(line) elif input_type == \\"eval_input\\": result = eval(input_content) print(result) else: raise ValueError(\\"Invalid input type\\") except Exception as e: return str(e) finally: sys.stdout = old_stdout return redirected_output.getvalue()"},{"question":"**Objective**: Write a python function `perform_operations` that takes a list of operations to be performed on a list of numbers and returns the result. # Function Signature ```python def perform_operations(numbers: list, operations: list) -> list: pass ``` # Input 1. `numbers` (list of integers): A list of integers on which operations are to be performed. 2. `operations` (list of tuples): Each tuple corresponds to an operation. The first element of the tuple is a string that specifies the operation (`\'add\'`, `\'sub\'`, `\'mul\'`, `\'truediv\'`, `\'floordiv\'`, `\'mod\'`, `\'pow\'`, `\'lshift\'`, `\'rshift\'`, `\'and\'`, `\'or\'`, `\'xor\'`). The second and third elements are the indices of the numbers in the list on which the operation should be performed. # Output - A list of the same length as `numbers` where each element is the result of applying the specified operations. # Constraints - 2 <= len(numbers) <= 1000 - 1 <= len(operations) <= 1000 - The indices specified in each operation tuple will be valid indices within the list `numbers`. # Example ```python numbers = [1, 2, 3, 4, 5] operations = [ (\'add\', 0, 1), (\'sub\', 3, 2), (\'mul\', 2, 4), (\'truediv\', 4, 1) ] assert perform_operations(numbers, operations) == [3, -1, 15, 2.5] ``` # Note - You can import the required functions from the `operator` module to implement the solution. - Use exception handling to manage any potential division by zero or invalid operations gracefully by ignoring the invalid operation (do not perform the operation and proceed with the next). # Hints - Utilize the functions from the `operator` module to perform the operations. - Use list comprehensions and functional programming approaches where appropriate to keep the code clean and efficient.","solution":"import operator def perform_operations(numbers: list, operations: list) -> list: # Mapping of string operation to function from operator module ops_map = { \'add\': operator.add, \'sub\': operator.sub, \'mul\': operator.mul, \'truediv\': operator.truediv, \'floordiv\': operator.floordiv, \'mod\': operator.mod, \'pow\': operator.pow, \'lshift\': operator.lshift, \'rshift\': operator.rshift, \'and\': operator.and_, \'or\': operator.or_, \'xor\': operator.xor } results = [] for op, idx1, idx2 in operations: try: func = ops_map[op] result = func(numbers[idx1], numbers[idx2]) results.append(result) except ZeroDivisionError: # When a division by zero occurs, skip the operation results.append(\\"undefined\\") # Placeholder for division by zero except (KeyError, IndexError, TypeError): # Invalid operation, index out of range, or invalid type continue return results"},{"question":"**Objective:** To assess the student\'s understanding of the `contextvars` module in Python, specifically in asynchronous programming contexts. Problem Statement: You are required to create a concurrent data processing system where multiple tasks operate independently and use context variables to maintain and reset their state. Implement a system that allows the following: 1. Create a context variable to keep track of a task-specific identifier. 2. Create a context variable to maintain a counter for operations performed by a task. 3. Write an asynchronous function `process_data` that accepts task identifier and data list. It should: - Set the task-specific context variables. - Perform increment on the counter for each data item processed. - Retrieve and return the task identifier and operation count at the end of processing. 4. Use the `copy_context` method to create a new context for each task. Run multiple tasks concurrently using `asyncio.gather`. Input: - A list of tasks, where each task is represented as a tuple containing a task identifier (string) and a data list (list of integers). Output: - A list of task results, where each result is a dictionary with task identifier and operation count. Constraints: - Each task must operate in its own context independently. - No two tasks should interfere with each other\'s context. **Performance Requirements:** - Ensure the solution efficiently manages asynchronous operations. ```python import asyncio import contextvars # Declare context variables task_id_var = contextvars.ContextVar(\'task_id\') operation_counter_var = contextvars.ContextVar(\'operation_counter\', default=0) async def process_data(task_id, data): # Set context variables task_id_var.set(task_id) operation_counter_var.set(0) # Process data for _ in data: # Increment the counter for each data item processed current_count = operation_counter_var.get() operation_counter_var.set(current_count + 1) # Retrieve and return the context variable values final_id = task_id_var.get() final_count = operation_counter_var.get() return {\\"task_id\\": final_id, \\"operation_count\\": final_count} async def main(tasks): results = [] ctx = contextvars.copy_context() async def run_task_in_context(task): task_ctx = ctx.copy() return await task_ctx.run(process_data, task[0], task[1]) # Run all tasks concurrently results = await asyncio.gather(*(run_task_in_context(task) for task in tasks)) return results # Example usage: tasks = [ (\\"task1\\", [1, 2, 3]), (\\"task2\\", [4, 5]), (\\"task3\\", [6, 7, 8, 9]) ] asyncio.run(main(tasks)) ``` Test your implementation against the provided example usage to ensure correctness.","solution":"import asyncio import contextvars # Declare context variables task_id_var = contextvars.ContextVar(\'task_id\') operation_counter_var = contextvars.ContextVar(\'operation_counter\', default=0) async def process_data(task_id, data): Processes data asynchronously, updating task-specific context variables. Args: task_id (str): The identifier for the task. data (list): The list of data items to process. Returns: dict: A dictionary containing the task identifier and operation count. # Set context variables task_id_var.set(task_id) operation_counter_var.set(0) # Process data for _ in data: # Increment the counter for each data item processed current_count = operation_counter_var.get() operation_counter_var.set(current_count + 1) # Retrieve and return the context variable values final_id = task_id_var.get() final_count = operation_counter_var.get() return {\\"task_id\\": final_id, \\"operation_count\\": final_count} async def main(tasks): Runs multiple tasks concurrently, each in its own context. Args: tasks (list): A list of tasks, where each task is represented as a tuple of task identifier and data list. Returns: list: A list of dictionaries containing the task identifier and operation count for each task. ctx = contextvars.copy_context() async def run_task_in_context(task): task_ctx = ctx.copy() return await task_ctx.run(process_data, task[0], task[1]) # Run all tasks concurrently results = await asyncio.gather(*(run_task_in_context(task) for task in tasks)) return results"},{"question":"# Student Management System You are tasked with implementing a simplified student management system using Python. Your system should allow you to store student records, each containing a unique ID, name, list of grades, and compute some aggregate data about the grades. Requirements: 1. Define a `Student` class with the following attributes and methods: - **Attributes**: - `student_id` (unique identifier, integer) - `name` (string) - `grades` (a list of integers, each between 0 and 100) - **Methods**: - `add_grade(grade: int)`: Adds a new grade to the student\'s list of grades. - `average_grade() -> float`: Returns the average of the student\'s grades. - `highest_grade() -> int`: Returns the highest grade. - `lowest_grade() -> int`: Returns the lowest grade. - `grade_summary() -> str`: Returns a string summarizing the student\'s grades in the format: \\"Student [student_id] - [name]: Grades = [grades], Average = [average], Highest = [highest], Lowest = [lowest]\\". 2. Create a function `create_student(student_id: int, name: str, grades: list[int] = []) -> Student` that returns a new student instance. 3. Implement a `manage_students()` function that: - Reads commands from the user to manage student records. The available commands should be: - `ADD_STUDENT <id> <name>`: Adds a new student with the given ID and name. - `ADD_GRADE <id> <grade>`: Adds a grade to the student with the given ID. - `SHOW_SUMMARY <id>`: Displays the grade summary for the student with the given ID. - `EXIT`: Ends the program. - Uses appropriate control flow statements and exception handling to manage inputs and possible errors. # Constraints: - You must use the `match` statement for pattern matching in command parsing. - The `Student` class should handle invalid grades (below 0 or above 100) by raising a `ValueError` with an appropriate message. - Student IDs should be unique, and attempting to add a student with an existing ID should raise a `ValueError`. # Example usage: ```python # Initialize a management system manage_students() # User commands: ADD_STUDENT 1 Alice ADD_GRADE 1 95 ADD_GRADE 1 85 SHOW_SUMMARY 1 # Output: Student 1 - Alice: Grades = [95, 85], Average = 90.0, Highest = 95, Lowest = 85 # Continue with more commands or end the program ``` Expectations: Your solution should showcase understanding of Python\'s control flow tools, function definitions, use of classes, and error handling. Ensure your code is well-organized, follows proper naming conventions, and includes comments where necessary.","solution":"class Student: def __init__(self, student_id: int, name: str, grades: list[int] = None): if grades is None: grades = [] self.student_id = student_id self.name = name self.grades = grades def add_grade(self, grade: int): if not (0 <= grade <= 100): raise ValueError(\\"Grade must be between 0 and 100.\\") self.grades.append(grade) def average_grade(self) -> float: if not self.grades: return 0.0 return sum(self.grades) / len(self.grades) def highest_grade(self) -> int: if not self.grades: return 0 return max(self.grades) def lowest_grade(self) -> int: if not self.grades: return 0 return min(self.grades) def grade_summary(self) -> str: avg = self.average_grade() highest = self.highest_grade() lowest = self.lowest_grade() return (f\\"Student {self.student_id} - {self.name}: Grades = {self.grades}, \\" f\\"Average = {avg}, Highest = {highest}, Lowest = {lowest}\\") def create_student(student_id: int, name: str, grades: list[int] = None) -> Student: return Student(student_id, name, grades) def manage_students(): students = {} while True: command = input(\\"Enter command: \\").strip() tokens = command.split() if not tokens: continue action = tokens[0] match action: case \\"ADD_STUDENT\\": if len(tokens) != 3: print(\\"Invalid command format. Use: ADD_STUDENT <id> <name>\\") continue student_id = int(tokens[1]) name = tokens[2] if student_id in students: print(f\\"Student with ID {student_id} already exists.\\") else: students[student_id] = create_student(student_id, name) print(f\\"Added student {name} with ID {student_id}\\") case \\"ADD_GRADE\\": if len(tokens) != 3: print(\\"Invalid command format. Use: ADD_GRADE <id> <grade>\\") continue student_id = int(tokens[1]) grade = int(tokens[2]) if student_id not in students: print(f\\"Student with ID {student_id} not found.\\") else: try: students[student_id].add_grade(grade) print(f\\"Added grade {grade} to student ID {student_id}\\") except ValueError as e: print(e) case \\"SHOW_SUMMARY\\": if len(tokens) != 2: print(\\"Invalid command format. Use: SHOW_SUMMARY <id>\\") continue student_id = int(tokens[1]) if student_id not in students: print(f\\"Student with ID {student_id} not found.\\") else: print(students[student_id].grade_summary()) case \\"EXIT\\": print(\\"Exiting program.\\") break case _: print(\\"Unknown command.\\")"},{"question":"# Exception Handling and Custom Exceptions in Python **Objective:** Implement a Python function that reads a list of integers from a file and performs specific operations while handling exceptions appropriately. You will also implement a custom exception. **Function Specification:** - Implement a function `process_numbers(file_path: str) -> Tuple[int, int, int]` that processes integers from a file. - The function should: - Read integers from a file specified by `file_path` (one integer per line). - Calculate: - The total sum of numbers. - The number of elements. - The maximum number. **Constraints:** - Raise a `FileNotFoundError` if the file does not exist. - Raise a custom exception `EmptyFileError` if the file is empty. - Raise a `ValueError` if the file contains non-integer values. **Custom Exception:** - Define a custom exception `EmptyFileError` inheriting from `Exception`. - The exception should display the message \\"The file is empty\\". # Example: Suppose the content of the file `numbers.txt` is: ``` 10 20 30 ``` Calling `process_numbers(\\"numbers.txt\\")` should return `(60, 3, 30)`. If the file is empty or contains non-integer values, appropriate exceptions should be raised. # Expected Input and Output: ```python def process_numbers(file_path: str) -> Tuple[int, int, int]: # Your implementation here # Example function call try: result = process_numbers(\\"numbers.txt\\") print(result) # Output: (60, 3, 30) except FileNotFoundError: print(\\"File not found.\\") except EmptyFileError: print(\\"The file is empty.\\") except ValueError: print(\\"File contains non-integer values.\\") ``` Implement the function and define the custom exception class as specified. Ensure your solution is robust and handles all specified edge cases.","solution":"from typing import Tuple class EmptyFileError(Exception): Custom exception raised when the file is empty. def __init__(self): super().__init__(\\"The file is empty\\") def process_numbers(file_path: str) -> Tuple[int, int, int]: try: with open(file_path, \\"r\\") as file: numbers = file.readlines() if not numbers: raise EmptyFileError() int_numbers = [] for line in numbers: stripped_line = line.strip() if stripped_line: try: number = int(stripped_line) int_numbers.append(number) except ValueError: raise ValueError(\\"File contains non-integer values.\\") if not int_numbers: raise EmptyFileError() total_sum = sum(int_numbers) num_elements = len(int_numbers) max_number = max(int_numbers) return (total_sum, num_elements, max_number) except FileNotFoundError: raise FileNotFoundError(f\\"The file \'{file_path}\' does not exist.\\")"},{"question":"You are required to design a class that mimics the functionality of the `chunk.Chunk` class described below. Your implementation should be able to read files that use EA IFF 85 chunks, such as AIFF, AIFF-C, RMFF, and WAVE file formats. # Class Definition class `Chunk` # Initialization Define the `__init__` method with the following parameters: - `file`: A file-like object to read from. - `align` (default: `True`): If `True`, assume chunks are aligned on 2-byte boundaries. - `bigendian` (default: `True`): If `True`, assume the chunk size is in big-endian byte order; if `False`, in little-endian order. - `inclheader` (default: `False`): If `True`, the size in the chunk header includes the size of the header itself. # Methods - `getname()`: Returns the 4-byte string ID of the chunk. - `getsize()`: Returns the size of the chunk data (excluding the header). - `close()`: Closes the chunk and skips to the end. - `isatty()`: Returns `False` (always). - `seek(pos, whence=0)`: Sets the chunk\'s current position. - `tell()`: Returns the chunk\'s current position. - `read(size=-1)`: Reads up to `size` bytes from the chunk; if `size` is negative or omitted, reads all data until the end of the chunk. - `skip()`: Skips to the end of the chunk. Should be used if the chunk\'s data is not needed. # Example Usage ```python # Assuming you have a file-like object \'file_obj\' opened in binary mode chunk = Chunk(file_obj) # Get the chunk ID chunk_id = chunk.getname() # Should return a 4-byte string # Get the chunk size chunk_size = chunk.getsize() # Returns the size of the data portion # Read some data data = chunk.read(10) # Reads the first 10 bytes of data # Skip to the end of the chunk chunk.skip() # Ensure to close the chunk when done chunk.close() ``` # Constraints - Your `Chunk` class should handle proper error checking for the file operations. - Efficiency is important: your methods should not perform unnecessary reads or seeks. - You may use standard Python libraries like `struct` for handling binary data. - Ensure that the class handles both big-endian and little-endian sizes correctly. - Account for alignment and padding as specified. # Submission Submit your implementation of the `Chunk` class as well as a short explanation of how you tested your implementation with different file formats. Ensure the class works with the provided usage example.","solution":"import struct class Chunk: def __init__(self, file, align=True, bigendian=True, inclheader=False): self.file = file self.align = align self.bigendian = bigendian self.inclheader = inclheader self.start_pos = self.file.tell() # Read the header self.chunk_id = self.file.read(4) if bigendian: self.chunk_size = struct.unpack(\'>I\', self.file.read(4))[0] else: self.chunk_size = struct.unpack(\'<I\', self.file.read(4))[0] if inclheader: self.chunk_size -= 8 self.current_pos = 0 def getname(self): return self.chunk_id def getsize(self): return self.chunk_size def close(self): self.skip() def isatty(self): return False def seek(self, pos, whence=0): if whence == 0: self.current_pos = pos elif whence == 1: self.current_pos += pos elif whence == 2: self.current_pos = self.chunk_size + pos self.file.seek(self.start_pos + 8 + self.current_pos) def tell(self): return self.current_pos def read(self, size=-1): if size < 0: size = self.chunk_size - self.current_pos size = min(size, self.chunk_size - self.current_pos) data = self.file.read(size) self.current_pos += len(data) return data def skip(self): if self.current_pos < self.chunk_size: self.file.seek(self.start_pos + 8 + self.chunk_size) self.current_pos = self.chunk_size if self.align and self.chunk_size % 2 == 1: self.file.seek(self.file.tell() + 1)"},{"question":"Objective: Implement a feature selection strategy on a dataset using scikit-learn\'s feature selection module. You will use both `VarianceThreshold` and `SelectKBest` to select the most relevant features and evaluate the performance of a classifier using these selected features. Instructions: 1. **Load the Dataset:** - Use the `load_wine` function from `sklearn.datasets` to load the wine dataset. 2. **Feature Selection with VarianceThreshold:** - Initialize a `VarianceThreshold` method to remove all features with a variance lower than 0.1. - Transform the dataset using this initialized method. 3. **Feature Selection with SelectKBest:** - On the transformed dataset from step 2, implement `SelectKBest` with the `f_classif` scoring function to select the 5 best features. 4. **Classifier Evaluation:** - Train a `RandomForestClassifier` on the original dataset and on the datasets obtained from steps 2 and 3. - Use 5-fold cross-validation and calculate the mean accuracy for each scenario. 5. **Output the Results:** - Print the shape of the datasets after each feature selection step. - Print the mean accuracy from the cross-validation for each scenario. Requirements: 1. Use the scikit-learn library for feature selection and classification. 2. Ensure reproducibility by setting a random seed where applicable. 3. Comment your code to explain the steps clearly. Starting Code: ```python from sklearn.datasets import load_wine from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score import numpy as np # Step 1: Load the dataset # Your code here # Step 2: Initialize and apply VarianceThreshold # Your code here # Step 3: Initialize and apply SelectKBest # Your code here # Step 4: Train and evaluate RandomForestClassifier # Original dataset # Your code here # After VarianceThreshold # Your code here # After VarianceThreshold and SelectKBest # Your code here # Step 5: Output the shapes and mean accuracies # Your code here ``` When you have completed the task, your output should include the following: 1. The shape of the dataset after applying `VarianceThreshold`. 2. The shape of the dataset after applying `SelectKBest`. 3. The mean cross-validation accuracy of the classifier trained on: - The original dataset - The dataset after `VarianceThreshold` - The dataset after `VarianceThreshold` and `SelectKBest` This will demonstrate your understanding of different feature selection techniques in sklearn and their impact on model performance.","solution":"# Importing necessary libraries from sklearn.datasets import load_wine from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score import numpy as np # Step 1: Load the dataset data = load_wine() X = data.data y = data.target # Step 2: Initialize and apply VarianceThreshold var_thresh = VarianceThreshold(threshold=0.1) X_var_thresh = var_thresh.fit_transform(X) # Step 3: Initialize and apply SelectKBest select_k_best = SelectKBest(score_func=f_classif, k=5) X_k_best = select_k_best.fit_transform(X_var_thresh, y) # Step 4: Train and evaluate RandomForestClassifier # Initialize Random Forest Classifier clf = RandomForestClassifier(random_state=42) # Original dataset scores_original = cross_val_score(clf, X, y, cv=5) mean_accuracy_original = scores_original.mean() # After VarianceThreshold scores_var_thresh = cross_val_score(clf, X_var_thresh, y, cv=5) mean_accuracy_var_thresh = scores_var_thresh.mean() # After VarianceThreshold and SelectKBest scores_k_best = cross_val_score(clf, X_k_best, y, cv=5) mean_accuracy_k_best = scores_k_best.mean() # Step 5: Output the shapes and mean accuracies result = { \\"Original shape\\": X.shape, \\"VarianceThreshold shape\\": X_var_thresh.shape, \\"SelectKBest shape\\": X_k_best.shape, \\"Original mean accuracy\\": mean_accuracy_original, \\"VarianceThreshold mean accuracy\\": mean_accuracy_var_thresh, \\"SelectKBest mean accuracy\\": mean_accuracy_k_best } result"},{"question":"**Objective:** Implement a PyTorch model that utilizes the `torch.cond` operator to dynamically choose between two functions based on specific criteria. **Problem Statement:** You are required to design a PyTorch module `SumOrProductModel` which, given an input tensor `x`, will: - Compute the sum of `x` if the mean of `x` is greater than a given threshold. - Compute the product of `x` if the mean of `x` is less than or equal to the threshold. You must use the `torch.cond` operator to implement this dynamic control flow within the forward method of the model. **Function Signature:** ```python import torch import torch.nn as nn class SumOrProductModel(nn.Module): def __init__(self, threshold: float): super(SumOrProductModel, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: pass # Example usage model = SumOrProductModel(threshold=0.5) input_tensor = torch.tensor([1.0, 2.0, 3.0, 4.0]) output = model(input_tensor) ``` **Requirements:** 1. The model should have an `__init__` method that initializes the threshold value. 2. Implement two functions, `sum_fn` and `product_fn`, that compute the sum and the product of the elements of the input tensor respectively. 3. In the `forward` method, use `torch.cond` to decide which function to execute based on whether the mean of `x` is greater than the threshold. 4. Ensure the output is a tensor containing a single value (sum or product). **Constraints:** - The input tensor `x` will have at least one element. - You are not allowed to use explicit if-else statements outside the scope of `torch.cond`. **Example:** ```python model = SumOrProductModel(threshold=2.5) input_tensor1 = torch.tensor([1.0, 2.0, 3.0, 4.0]) output1 = model(input_tensor1) print(output1) # Should compute the sum since the mean (2.5) is equal to the threshold input_tensor2 = torch.tensor([1.0, 1.5, 2.0]) output2 = model(input_tensor2) print(output2) # Should compute the product since the mean (1.5) is less than the threshold ``` **Notes:** - Make sure to handle any potential edge cases such as tensors with a single element. - Document your code appropriately for clarity.","solution":"import torch import torch.nn as nn class SumOrProductModel(nn.Module): def __init__(self, threshold: float): super(SumOrProductModel, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def sum_fn(): return torch.sum(x) def product_fn(): return torch.prod(x) mean_x = torch.mean(x) result = torch.sum(x) if mean_x > self.threshold else torch.prod(x) return result"},{"question":"**Objective:** Demonstrate understanding and application of `sklearn.preprocessing` to preprocess a given dataset. **Problem Statement:** You are given a dataset containing both numerical and categorical features. Your task is to preprocess this dataset in two stages: 1. Apply standard scaling to the numerical features. 2. Encode the categorical features using one-hot encoding. **Input:** - `df` (pandas DataFrame): A DataFrame containing the dataset with both numerical and categorical features. **Output:** - `transformed_df` (pandas DataFrame): A DataFrame where numerical features are standardized to have zero mean and unit variance, and categorical features are encoded using one-hot encoding. **Constraints:** - You are restricted to using the `StandardScaler` for scaling and `OneHotEncoder` for encoding. - Handle any missing values appropriately by assuming they are encoded as `np.nan` in the pandas DataFrame. - The order of columns should remain the same as in the original DataFrame. **Additional Information:** - Assume all categorical features are of dtype `object` and all numerical features are of dtype `int64` or `float64`. - You are required to fit the scalers and encoders on the data before transforming it. **Example:** ```python import pandas as pd import numpy as np # Sample input data = { \'age\': [25, 30, np.nan, 35, 40], \'salary\': [50000, 55000, 60000, np.nan, 65000], \'gender\': [\'male\', \'female\', \'female\', \'male\', \'male\'], \'city\': [\'New York\', \'Los Angeles\', \'New York\', np.nan, \'Chicago\'] } df = pd.DataFrame(data) # Function implementation def preprocess_data(df): from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer import pandas as pd # Identifying numerical and categorical columns numerical_cols = df.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_cols = df.select_dtypes(include=[\'object\']).columns # Defining the transformers for the pipeline numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combining transformers into a single preprocessor preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_cols), (\'cat\', categorical_transformer, categorical_cols) ] ) # Applying transformations transformed_array = preprocessor.fit_transform(df) # Converting transformed_array to a DataFrame column_names = list(numerical_cols) + preprocessor.transformers_[1][1][\'onehot\'].get_feature_names_out(categorical_cols).tolist() transformed_df = pd.DataFrame(transformed_array, columns=column_names) return transformed_df transformed_df = preprocess_data(df) print(transformed_df) ``` # Your Task Implement the `preprocess_data` function according to the problem statement and constraints given above.","solution":"def preprocess_data(df): from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer import pandas as pd # Identifying numerical and categorical columns numerical_cols = df.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_cols = df.select_dtypes(include=[\'object\']).columns # Defining the transformers for the pipeline numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combining transformers into a single preprocessor preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_cols), (\'cat\', categorical_transformer, categorical_cols) ] ) # Applying transformations transformed_array = preprocessor.fit_transform(df) # Converting transformed_array to a DataFrame column_names = list(numerical_cols) + preprocessor.transformers_[1][1][\'onehot\'].get_feature_names_out(categorical_cols).tolist() transformed_df = pd.DataFrame(transformed_array, columns=column_names) return transformed_df"},{"question":"# Advanced Coding Assessment: Writing Comprehensive Unit Tests **Objective:** Demonstrate your understanding of writing unit tests using Python\'s `unittest` framework and the `test.support` module. Problem Statement: You are given a utility module `utils.py` that contains the following functions: ```python # utils.py def add(x, y): Adds two numbers. return x + y def subtract(x, y): Subtracts second number from the first number. return x - y def multiply(x, y): Multiplies two numbers. return x * y def divide(x, y): Divides the first number by the second number. if y == 0: raise ValueError(\\"Cannot divide by zero.\\") return x / y ``` Your task is to write a comprehensive unit test suite for the `utils.py` module. Your tests should exhaustively test every function, including edge and corner cases. Requirements: 1. Create a test suite following the guidelines: - Each function in `utils.py` should have its own test method within the appropriate test class. - Include tests for both valid and invalid inputs. - Use the `test.support` module utilities where appropriate to enhance your tests. 2. Your tests must: - Verify correct functionality. - Handle exceptions. - Check for boundary conditions. 3. Use utilities from `test.support` such as: - `captured_stdout` or `captured_stderr` for functions that print to stdout or stderr (if needed). - `gc_collect` and `disable_gc` for managing garbage collection. - `swap_attr` or `swap_item` if any function relies on modifying object attributes or dictionary items (simulate scenarios). - Ensure that your tests can clean up resources as needed. Example Structure: ```python import unittest from test import support from utils import add, subtract, multiply, divide class TestUtils(unittest.TestCase): def test_add(self): # Tests for add function # Test with positive numbers self.assertEqual(add(2, 3), 5) # Test with negative numbers self.assertEqual(add(-1, -1), -2) # Test with mixed numbers self.assertEqual(add(-1, 1), 0) # More tests... def test_subtract(self): # Tests for subtract function # Test with positive numbers self.assertEqual(subtract(5, 3), 2) # Test with negative numbers self.assertEqual(subtract(-1, -1), 0) # Test with mixed numbers self.assertEqual(subtract(-1, 1), -2) # More tests... def test_multiply(self): # Test code for multiply function # Include normal, edge, and special cases. def test_divide(self): # Test code for divide function # Include normal, edge, and special cases. # Test division by zero with self.assertRaises(ValueError): divide(1, 0) # More tests... if __name__ == \'__main__\': unittest.main() ``` Submission: Submit your test suite code by implementing it within a single file `test_utils.py`, ensuring all tests run successfully with the `unittest` framework. **Note:** Ensure to use `test.support` utilities wherever you see fit to showcase their utility in testing scenarios.","solution":"def add(x, y): Adds two numbers. return x + y def subtract(x, y): Subtracts second number from the first number. return x - y def multiply(x, y): Multiplies two numbers. return x * y def divide(x, y): Divides the first number by the second number. if y == 0: raise ValueError(\\"Cannot divide by zero.\\") return x / y"},{"question":"# Question: Implement and Manipulate a Custom SQLite Database with Custom Type Conversion You are tasked with creating and managing a SQLite database to store and manipulate geometrical shapes. Each shape will be either a `Circle` or a `Rectangle`, with specific attributes for each: - `Circle`: - `radius` (a float) - `Rectangle`: - `length` (a float) - `width` (a float) The types of the shapes will be stored in a SQLite database along with their attributes. Your implementation should involve adapting these custom shape types for storage in the database and converting them back to custom Python types upon retrieval. You will need to: 1. Create a `Shape` class as the base class for `Circle` and `Rectangle`. 2. Implement `Circle` and `Rectangle` classes inheriting from the `Shape` class. 3. Adapt these classes to be stored in SQLite. 4. Create a SQLite table to store the shapes along with a column for shape type differentiation. 5. Insert multiple `Circle` and `Rectangle` instances into the database. 6. Query the database to retrieve shapes and convert them back to their respective Python types. 7. Implement error handling for any database operations. # Requirements: 1. Implement a base `Shape` class and `Circle` and `Rectangle` classes. 2. Register adapters and converters for `Circle` and `Rectangle` to store and retrieve them in the database. 3. Create a SQLite database table `shapes` that contains: - an `id` column (primary key), - a `type` column (to differentiate between `Circle` and `Rectangle`), - an `attributes` column (to store attributes specific to the shape type). 4. Write a function `insert_shape(connection, shape)` to insert a shape into the database. 5. Write a function `retrieve_shapes(connection)` to retrieve all shapes from the database as their respective types. 6. Handle transactions and ensure that database operations are correctly committed or rolled back in case of errors. # Input and Output Formats: - **Input:** Various instances of `Circle` and `Rectangle` classes. - **Output:** A list of shape instances retrieved from the database. ```python # Your code starts here import sqlite3 class Shape: pass class Circle(Shape): def __init__(self, radius): self.radius = radius def __repr__(self): return f\\"Circle(radius={self.radius})\\" class Rectangle(Shape): def __init__(self, length, width): self.length = length self.width = width def __repr__(self): return f\\"Rectangle(length={self.length}, width={self.width})\\" def adapt_circle(circle): return f\\"{circle.radius}\\" def convert_circle(s): radius = float(s.decode()) return Circle(radius) def adapt_rectangle(rect): return f\\"{rect.length};{rect.width}\\" def convert_rectangle(s): length, width = map(float, s.decode().split(\\";\\")) return Rectangle(length, width) sqlite3.register_adapter(Circle, adapt_circle) sqlite3.register_converter(\\"circle\\", convert_circle) sqlite3.register_adapter(Rectangle, adapt_rectangle) sqlite3.register_converter(\\"rectangle\\", convert_rectangle) def create_shapes_table(connection): with connection: connection.execute(\'\'\'CREATE TABLE IF NOT EXISTS shapes ( id INTEGER PRIMARY KEY, type TEXT NOT NULL, attributes TEXT NOT NULL)\'\'\') def insert_shape(connection, shape): with connection: if isinstance(shape, Circle): connection.execute(\\"INSERT INTO shapes (type, attributes) VALUES (?, ?)\\", (\\"circle\\", adapt_circle(shape))) elif isinstance(shape, Rectangle): connection.execute(\\"INSERT INTO shapes (type, attributes) VALUES (?, ?)\\", (\\"rectangle\\", adapt_rectangle(shape))) else: raise ValueError(\\"Unsupported shape type\\") def retrieve_shapes(connection): shapes = [] cur = connection.cursor() cur.execute(\\"SELECT type, attributes FROM shapes\\") rows = cur.fetchall() for shape_type, attributes in rows: if shape_type == \\"circle\\": shapes.append(convert_circle(attributes.encode())) elif shape_type == \\"rectangle\\": shapes.append(convert_rectangle(attributes.encode())) else: raise ValueError(\\"Unknown shape type\\") return shapes # Your code ends here # Sample usage (You can uncomment this part to test your solution) if __name__ == \\"__main__\\": con = sqlite3.connect(\\":memory:\\") con.text_factory = bytes create_shapes_table(con) # Insert shapes circle_1 = Circle(5.0) rectangle_1 = Rectangle(4.0, 2.0) insert_shape(con, circle_1) insert_shape(con, rectangle_1) # Retrieve shapes shapes = retrieve_shapes(con) print(shapes) # Output: [Circle(radius=5.0), Rectangle(length=4.0, width=2.0)] con.close() ``` # Notes: - Make sure to test your code with different shapes and database operations. - Handle any potential exceptions that might occur during database interactions. - Ensure your solution handles both `Circle` and `Rectangle` types correctly.","solution":"import sqlite3 class Shape: pass class Circle(Shape): def __init__(self, radius): self.radius = radius def __repr__(self): return f\\"Circle(radius={self.radius})\\" class Rectangle(Shape): def __init__(self, length, width): self.length = length self.width = width def __repr__(self): return f\\"Rectangle(length={self.length}, width={self.width})\\" def adapt_circle(circle): return f\\"{circle.radius}\\" def convert_circle(s): radius = float(s.decode()) return Circle(radius) def adapt_rectangle(rect): return f\\"{rect.length};{rect.width}\\" def convert_rectangle(s): length, width = map(float, s.decode().split(\\";\\")) return Rectangle(length, width) sqlite3.register_adapter(Circle, adapt_circle) sqlite3.register_converter(\\"circle\\", convert_circle) sqlite3.register_adapter(Rectangle, adapt_rectangle) sqlite3.register_converter(\\"rectangle\\", convert_rectangle) def create_shapes_table(connection): with connection: connection.execute(\'\'\'CREATE TABLE IF NOT EXISTS shapes ( id INTEGER PRIMARY KEY, type TEXT NOT NULL, attributes TEXT NOT NULL)\'\'\') def insert_shape(connection, shape): with connection: if isinstance(shape, Circle): connection.execute(\\"INSERT INTO shapes (type, attributes) VALUES (?, ?)\\", (\\"circle\\", adapt_circle(shape))) elif isinstance(shape, Rectangle): connection.execute(\\"INSERT INTO shapes (type, attributes) VALUES (?, ?)\\", (\\"rectangle\\", adapt_rectangle(shape))) else: raise ValueError(\\"Unsupported shape type\\") def retrieve_shapes(connection): shapes = [] cur = connection.cursor() cur.execute(\\"SELECT type, attributes FROM shapes\\") rows = cur.fetchall() for shape_type, attributes in rows: if shape_type == \\"circle\\": shapes.append(convert_circle(attributes.encode())) elif shape_type == \\"rectangle\\": shapes.append(convert_rectangle(attributes.encode())) else: raise ValueError(\\"Unknown shape type\\") return shapes"},{"question":"# Coding Challenge: Python Code Executor Objective Implement a Python function that takes a Python source code snippet as a string, compiles it, and executes it within a provided global and local scope. The function should handle and report any errors that occur during the compilation or execution process. Function Signature ```python def execute_python_code(code: str, globals_dict: dict, locals_dict: dict) -> dict: pass ``` Input - `code` (str): A string containing the Python source code to execute. - `globals_dict` (dict): A dictionary representing the global scope for the code execution. - `locals_dict` (dict): A dictionary representing the local scope for the code execution. Output - A dictionary with the following structure: - `\\"success\\"` (bool): `True` if the code executed successfully, `False` otherwise. - `\\"result\\"`: If `success` is `True`, this key contains the return value of the executed code; if `False`, it contains the error message. Constraints - You must use the `PyRun_StringFlags` function to achieve the execution. - Handle possible exceptions and ensure that appropriate error messages are captured. - Your solution should work for complex expressions and multiple statements included in the `code` string. Example Usage ```python globals_dict = {\\"x\\": 10} locals_dict = {} code = y = x + 5 print(y) result = y * 2 output = execute_python_code(code, globals_dict, locals_dict) # Expected Output # { # \\"success\\": True, # \\"result\\": 30 # } code = print(z) output = execute_python_code(code, globals_dict, locals_dict) # Expected Output # { # \\"success\\": False, # \\"result\\": \\"NameError: name \'z\' is not defined\\" # } ``` Requirements - The provided globals and locals dictionaries should be modifiable by the executed code. - Ensure that the function is efficient and handles multiple execution scenarios properly. - You are required to use appropriate methods from the documentation provided. Tip Refer to the `PyRun_StringFlags` method in the documentation to understand how to compile and execute Python code within specific contexts.","solution":"def execute_python_code(code: str, globals_dict: dict, locals_dict: dict) -> dict: Executes a given Python code string using the provided global and local dictionaries. Parameters: code (str): The Python code to be executed. globals_dict (dict): Dictionary representing the global scope. locals_dict (dict): Dictionary representing the local scope. Returns: dict: A dictionary containing \'success\' key indicating whether execution was successful, and \'result\' key containing the return value or error message. try: exec(code, globals_dict, locals_dict) return { \\"success\\": True, \\"result\\": locals_dict.get(\'result\', None) } except Exception as e: return { \\"success\\": False, \\"result\\": f\\"{type(e).__name__}: {e}\\" }"},{"question":"# Advanced Python Function Object Manipulation Problem Statement You are tasked with creating a Python module `function_utils.py` that provides utility functions to manipulate Python function objects created at the C level. Your module should include the following functionalities: 1. **Create Function**: - `create_function(code_str: str, globals_dict: dict, qualname: str = None) -> types.FunctionType` - Creates and returns a new function object. - `code_str` is the string representation of the function\'s code. - `globals_dict` is the dictionary containing the global variables accessible to the function. - `qualname` is the optional qualified name of the function. 2. **Get Function Details**: - `get_function_details(func: types.FunctionType) -> dict` - Returns a dictionary containing details about the function object. - The dictionary should include the keys: `\'code\'`, `\'globals\'`, `\'module\'`, `\'defaults\'`, `\'closure\'`, and `\'annotations\'`. Each key should map to the respective attribute\'s value. 3. **Set Function Defaults**: - `set_function_defaults(func: types.FunctionType, defaults: tuple) -> None` - Sets the default values for the function\'s arguments. - `defaults` is a tuple containing default values for the function\'s arguments. Constraints - Use the provided C functions in the documentation to implement the required functionalities. - Ensure robust error handling where appropriate. Example Usage ```python import types from function_utils import create_function, get_function_details, set_function_defaults # Global variables globals_dict = {\'x\': 10, \'y\': 20} # Function code as a string code_str = def my_dynamic_func(a, b=5): return a + b + x + y # Creating the function dynamic_func = create_function(code_str, globals_dict, qualname=\\"dynamic.my_dynamic_func\\") # Getting function details details = get_function_details(dynamic_func) print(details) # Setting function defaults set_function_defaults(dynamic_func, (10,)) print(dynamic_func(5)) # Should output 45 considering globals x=10, y=20, a=5, b=10 ``` Submission Submit the `function_utils.py` file containing the implementations of the required functions.","solution":"import types def create_function(code_str, globals_dict, qualname=None): Creates and returns a new function object from the given code string, globals dictionary, and qualified name. Parameters: - code_str (str): The string representation of the function\'s code. - globals_dict (dict): The dictionary containing the global variables accessible to the function. - qualname (str): The optional qualified name of the function. Returns: - types.FunctionType: The created function object. # Extract the function code from code_str exec(code_str, globals_dict) func_name = list(globals_dict.keys())[-1] # The new function gets added to the end of the dictionary func = globals_dict[func_name] if qualname: func.__qualname__ = qualname return func def get_function_details(func): Returns a dictionary containing details about the function object. Parameters: - func (types.FunctionType): The function object to get details of. Returns: - dict: A dictionary containing details about the function object. return { \'code\': func.__code__, \'globals\': func.__globals__, \'module\': func.__module__, \'defaults\': func.__defaults__, \'closure\': func.__closure__, \'annotations\': func.__annotations__, } def set_function_defaults(func, defaults): Sets the default values for the function\'s arguments. Parameters: - func (types.FunctionType): The function object to set the default values for. - defaults (tuple): A tuple containing default values for the function\'s arguments. Returns: - None func.__defaults__ = defaults"},{"question":"# Question: Implement and Train a Model Using PyTorch on Intel GPU **Objective**: The goal of this assessment is to evaluate your ability to implement and train a neural network using PyTorch on an Intel GPU. You will use the provided instructions and examples to set up the training environment and optimize the training process using AMP and `torch.compile`. **Requirements**: 1. **Model Implementation**: Define a simple convolutional neural network (CNN) for classifying the MNIST dataset. 2. **Training Setup**: - Load the MNIST dataset and transform it as needed. - Prepare the training and validation data loaders. 3. **GPU Integration**: Ensure the model and data are transferred to the Intel GPU. 4. **Training Loop**: - Implement the training loop using FP32. - Implement the training loop using AMP. - Optimize the training process using `torch.compile`. 5. **Validation**: Implement the validation loop to evaluate the model during training. **Constraints and Performance Requirements**: - You must use PyTorch\'s `torch.xpu` API for Intel GPU support. - The training should showcase the speed improvements when using AMP and `torch.compile`. - Your code should be modular and well-documented, making it clear which sections handle data loading, model definition, training, and validation. **Input and Output**: - **Input**: Path to the MNIST dataset. - **Output**: Training loss and validation accuracy printed after each epoch. **Example**: ```python import torch import torch.nn as nn import torch.optim as optim from torchvision import datasets, transforms from torch.utils.data import DataLoader # Define a simple CNN model class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3, 1) self.conv2 = nn.Conv2d(32, 64, 3, 1) self.fc1 = nn.Linear(9216, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.relu(self.conv2(x)) x = torch.flatten(x, 1) x = torch.relu(self.fc1(x)) x = self.fc2(x) return torch.log_softmax(x, dim=1) def main(): # Set random seed for reproducibility torch.manual_seed(42) # Define data transformations and load the dataset transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) train_dataset = datasets.MNIST(root=\'./data\', train=True, download=True, transform=transform) val_dataset = datasets.MNIST(root=\'./data\', train=False, transform=transform) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False) # Initialize the model, loss function, and optimizer model = SimpleCNN().to(\'xpu\') criterion = nn.CrossEntropyLoss().to(\'xpu\') optimizer = optim.Adam(model.parameters()) # Training loop for epoch in range(1, 11): model.train() total_loss = 0 for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(\'xpu\'), target.to(\'xpu\') optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() total_loss += loss.item() if batch_idx % 10 == 0: print(f\'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \' f\'({100. * batch_idx / len(train_loader):.0f}%)]tLoss: {loss.item():.6f}\') # Validation loop def validate(model, val_loader): model.eval() val_loss = 0 correct = 0 with torch.no_grad(): for data, target in val_loader: data, target = data.to(\'xpu\'), target.to(\'xpu\') output = model(data) val_loss += criterion(output, target).item() pred = output.argmax(dim=1, keepdim=True) correct += pred.eq(target.view_as(pred)).sum().item() val_loss /= len(val_loader.dataset) accuracy = 100. * correct / len(val_loader.dataset) print(f\'Validation set: Average loss: {val_loss:.4f}, Accuracy: {correct}/{len(val_loader.dataset)}\' f\' ({accuracy:.2f}%)\') return accuracy # Validate the model validate(model, val_loader) if __name__ == \'__main__\': main() ``` **Note**: Ensure the Intel GPU and necessary drivers are correctly installed on your system. Use the `torch.xpu.is_available()` function to check GPU availability before proceeding with the training.","solution":"import torch import torch.nn as nn import torch.optim as optim from torchvision import datasets, transforms from torch.utils.data import DataLoader # Define a simple CNN model class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, 3, 1) self.conv2 = nn.Conv2d(32, 64, 3, 1) self.fc1 = nn.Linear(9216, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.relu(self.conv2(x)) x = torch.flatten(x, 1) x = torch.relu(self.fc1(x)) x = self.fc2(x) return torch.log_softmax(x, dim=1) def main(): # Ensure availability of Intel GPU if not torch.xpu.is_available(): print(\\"Intel GPU not available\\") return device = \'xpu\' # Set random seed for reproducibility torch.manual_seed(42) # Define data transformations and load the dataset transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) train_dataset = datasets.MNIST(root=\'./data\', train=True, download=True, transform=transform) val_dataset = datasets.MNIST(root=\'./data\', train=False, transform=transform) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False) # Initialize the model, loss function, and optimizer model = SimpleCNN().to(device) criterion = nn.CrossEntropyLoss().to(device) optimizer = optim.Adam(model.parameters()) # Training loop for epoch in range(1, 11): model.train() total_loss = 0 for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() total_loss += loss.item() if batch_idx % 10 == 0: print(f\'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \' f\'({100. * batch_idx / len(train_loader):.0f}%)]tLoss: {loss.item():.6f}\') # Validation loop def validate(model, val_loader): model.eval() val_loss = 0 correct = 0 with torch.no_grad(): for data, target in val_loader: data, target = data.to(device), target.to(device) output = model(data) val_loss += criterion(output, target).item() pred = output.argmax(dim=1, keepdim=True) correct += pred.eq(target.view_as(pred)).sum().item() val_loss /= len(val_loader.dataset) accuracy = 100. * correct / len(val_loader.dataset) print(f\'Validation set: Average loss: {val_loss:.4f}, Accuracy: {correct}/{len(val_loader.dataset)}\' f\' ({accuracy:.2f}%)\') return accuracy # Validate the model validate(model, val_loader) if __name__ == \'__main__\': main()"},{"question":"# Persistent Shopping Cart using `shelve` Objective: Design a persistent shopping cart system using the `shelve` module. Your task is to create a set of functions that manage the shopping cart data persistently. Requirements: Implement the following functions: 1. **add_item(filename: str, item: str, quantity: int) -> None**: - Adds the specified quantity of an item to the cart. If the item already exists, increment its quantity by the specified amount. - Parameters: - `filename` (str): The name of the shelf file where the cart data is stored. - `item` (str): The name of the item to add. - `quantity` (int): The quantity of the item to add. 2. **remove_item(filename: str, item: str, quantity: int) -> None**: - Decreases the quantity of an item in the cart by the specified amount. If the resulting quantity is zero or less, remove the item from the cart. - Parameters: - `filename` (str): The name of the shelf file where the cart data is stored. - `item` (str): The name of the item to remove. - `quantity` (int): The quantity of the item to remove. 3. **view_cart(filename: str) -> dict**: - Returns the entire shopping cart as a dictionary where the keys are item names and the values are their respective quantities. - Parameter: - `filename` (str): The name of the shelf file where the cart data is stored. - Returns: - dict: A dictionary representation of the shopping cart. 4. **checkout(filename: str) -> None**: - Empties the shopping cart (i.e., removes all items). - Parameter: - `filename` (str): The name of the shelf file where the cart data is stored. Constraints: - The shelf file should be properly closed after each operation to ensure data integrity. - Ensure concurrent write accesses are handled appropriately. - Handle exceptions that might arise from file operations. Example Usage: ```python # Add items to the cart add_item(\'cart.db\', \'apple\', 10) add_item(\'cart.db\', \'banana\', 5) # View cart print(view_cart(\'cart.db\')) # Output: {\'apple\': 10, \'banana\': 5} # Remove items from the cart remove_item(\'cart.db\', \'apple\', 3) # View cart print(view_cart(\'cart.db\')) # Output: {\'apple\': 7, \'banana\': 5} # Checkout checkout(\'cart.db\') # View cart (should be empty) print(view_cart(\'cart.db\')) # Output: {} ``` **Note**: Ensure to manage the shelf operations correctly to prevent data loss or corruption.","solution":"import shelve def add_item(filename: str, item: str, quantity: int) -> None: with shelve.open(filename, writeback=True) as cart: if item in cart: cart[item] += quantity else: cart[item] = quantity def remove_item(filename: str, item: str, quantity: int) -> None: with shelve.open(filename, writeback=True) as cart: if item in cart: cart[item] -= quantity if cart[item] <= 0: del cart[item] def view_cart(filename: str) -> dict: with shelve.open(filename) as cart: return dict(cart) def checkout(filename: str) -> None: with shelve.open(filename, writeback=True) as cart: cart.clear()"},{"question":"# Derangement Problem In combinatorics, a derangement is a permutation of a set such that none of the elements appear in their original positions. For example, the derangements of `[1, 2, 3]` are `[2, 3, 1]`, `[3, 1, 2]`. Using the functional programming modules `itertools`, `functools`, and `operator`, write a function `find_derangements` that takes a list of distinct integers as input and returns a list of all possible derangements of this list. Function Signature ```python def find_derangements(arr: list) -> list: ``` Input * `arr` (List[int]): A list of distinct integers where `1 <= len(arr) <= 8`. Output * `List[List[int]]`: A list containing all possible derangements of the input list. Constraints * The solution should leverage the `itertools` module for generating permutations. * The function should employ `functools` partially applied functions or similar higher-order functions to build helper utilities if necessary. * The `operator` module should be used to replace anonymous lambda expressions where applicable, making the code more readable. Example ```python # Example 1 input_list = [1, 2, 3] output = find_derangements(input_list) # Possible output # [ # [2, 3, 1], # [3, 1, 2] # ] # Example 2 input_list = [1, 2, 3, 4] output = find_derangements(input_list) # Possible output (one of the correct permutations) # [ # [2, 1, 4, 3], # [2, 3, 4, 1], # [2, 4, 1, 3], # [3, 1, 4, 2], # [3, 4, 1, 2], # [3, 4, 2, 1], # [4, 1, 3, 2], # [4, 2, 1, 3], # [4, 3, 1, 2], # ] ``` Notes - The length of the derangement output may vary as they depend on the permutations where no element remains in the same position as the input list. - Optimize the solution to handle the maximum constraint (`len(arr) = 8`) efficiently.","solution":"import itertools from functools import partial def is_derangement(perm, original): return all(map(lambda i: perm[i] != original[i], range(len(original)))) def find_derangements(arr): all_permutations = itertools.permutations(arr) return list(filter(partial(is_derangement, original=arr), all_permutations)) # Example usage: # input_list = [1, 2, 3] # print(find_derangements(input_list)) # Expected output: [(2, 3, 1), (3, 1, 2)]"},{"question":"You are building an email utility script to assist in processing and managing email headers for an email client application. Your task is to implement the following functions using the `email.utils` module provided in Python 3.10: 1. **`normalize_email_addresses(header_values: List[str]) -> List[Tuple[str, str]]`**: - **Input**: A list of header field values (strings) that might contain email addresses such as \\"To\\", \\"Cc\\", etc. - **Output**: A list of 2-tuples (realname, email_address) containing normalized email addresses extracted from the header values. - **Constraints**: - Use a strict parser to ensure only valid email addresses are included. - Ensure the output list does not contain duplicate email addresses. 2. **`compose_email_header(addresses: List[Tuple[str, str]], charset: str = \'utf-8\') -> str`**: - **Input**: A list of 2-tuples (realname, email_address) and an optional charset (defaults to \'utf-8\'). - **Output**: A single string suitable for use in an email header. - **Constraints**: - The realnames and email addresses should be properly encoded according to the given charset. - If the charset is not specified, default to \'utf-8\'. 3. **`generate_unique_message_id(domain: Optional[str] = None) -> str`**: - **Input**: An optional domain string to generate the message ID. If no domain is provided, use the local hostname. - **Output**: A string that represents a unique Message-ID suitable for an RFC 2822-compliant email. # Examples ```python header_values = [ \\"Alice <alice@example.com>, Bob <bob@example.com>\\", \\"Carol <carol@sample.com>, Bob <bob@example.com>\\" ] # Example usage of normalize_email_addresses function normalized_addresses = normalize_email_addresses(header_values) # normalized_addresses should be: # [(\'Alice\', \'alice@example.com\'), (\'Bob\', \'bob@example.com\'), (\'Carol\', \'carol@sample.com\')] # Example usage of compose_email_header function header_string = compose_email_header(normalized_addresses, charset=\'utf-8\') # header_string should be: # \'Alice <alice@example.com>, Bob <bob@example.com>, Carol <carol@sample.com>\' # Example usage of generate_unique_message_id function msg_id = generate_unique_message_id(\\"mydomain.com\\") # msg_id should be a unique string such as: # \'<randompart@mydomain.com>\' ``` # Notes - Ensure you handle edge cases such as empty input lists. - You may assume all input header values are valid email address headers. - The solution should be efficient and concise. Happy coding!","solution":"import email.utils import hashlib import socket import time from typing import List, Tuple, Optional def normalize_email_addresses(header_values: List[str]) -> List[Tuple[str, str]]: Normalizes email addresses from header values, ensuring unique and valid emails. Parameters: - header_values (List[str]): List of header field values. Returns: - List[Tuple[str, str]]: List of 2-tuples containing (realname, email_address). seen = set() result = [] for header in header_values: addresses = email.utils.getaddresses([header]) for realname, email_address in addresses: if email_address not in seen: seen.add(email_address) result.append((realname, email_address)) return result def compose_email_header(addresses: List[Tuple[str, str]], charset: str = \'utf-8\') -> str: Composes a single string suitable for use in an email header. Parameters: - addresses (List[Tuple[str, str]]): List of 2-tuples containing (realname, email_address). - charset (str): Charset encoding for the header. Returns: - str: A single string suitable for use in an email header. formatted_addresses = [] for realname, email_address in addresses: formatted_address = email.utils.formataddr((realname, email_address)) formatted_addresses.append(formatted_address) return \', \'.join(formatted_addresses) def generate_unique_message_id(domain: Optional[str] = None) -> str: Generates a unique Message-ID suitable for an email. Parameters: - domain (Optional[str]): Domain to be used for the Message-ID. If not provided, local hostname will be used. Returns: - str: A string that represents a unique Message-ID. if domain is None: domain = socket.getfqdn() unique_part = hashlib.md5(f\\"{time.time()}-{socket.gethostname()}\\".encode()).hexdigest() return f\\"<{unique_part}@{domain}>\\""},{"question":"# Pattern Matching with `fnmatch` Module Objective: You are required to write a function that filters and organizes a list of filenames based on specified patterns. You will use the `fnmatch` module to perform this task. Problem Statement: Implement a Python function `organize_files(file_list: List[str], pattern_list: List[str]) -> Dict[str, List[str]]` that accepts two lists: 1. `file_list` (List of strings): A list containing filenames. 2. `pattern_list` (List of strings): A list containing Unix shell-style wildcard patterns. The function should return a dictionary where the keys are the patterns from `pattern_list`, and the values are lists of filenames from `file_list` that match each pattern. Constraints: - The function should use `fnmatch.filter()` to match filenames to patterns. - Each filename should only appear in the list corresponding to the first pattern it matches (from the top of the `pattern_list`). - If a filename does not match any pattern, it should not be included in the resulting dictionary. Inputs: - `file_list`: A list of strings with a maximum length of 5000. - `pattern_list`: A list of strings with a maximum length of 100. - Each filename and pattern is a string with a maximum length of 255 characters. Output: - A dictionary where keys are patterns, and values are lists of filenames matching those patterns. Example: ```python file_list = [\\"report1.txt\\", \\"data.csv\\", \\"script.py\\", \\"log.txt\\", \\"image.png\\", \\"notes.md\\"] pattern_list = [\\"*.txt\\", \\"*.py\\", \\"*.md\\"] result = organize_files(file_list, pattern_list) # Expected Output: # { # \\"*.txt\\": [\\"report1.txt\\", \\"log.txt\\"], # \\"*.py\\": [\\"script.py\\"], # \\"*.md\\": [\\"notes.md\\"] # } ``` Code Template: ```python from typing import List, Dict import fnmatch def organize_files(file_list: List[str], pattern_list: List[str]) -> Dict[str, List[str]]: result = {} # Your implementation here return result ``` Additional Notes: - Remember to import necessary modules (`fnmatch`). - Think about the efficiency and readability of your code.","solution":"from typing import List, Dict import fnmatch def organize_files(file_list: List[str], pattern_list: List[str]) -> Dict[str, List[str]]: result = {pattern: [] for pattern in pattern_list} for filename in file_list: matched = False for pattern in pattern_list: if fnmatch.fnmatch(filename, pattern) and not matched: result[pattern].append(filename) matched = True # Remove empty lists if no files matched a particular pattern result = {pattern: files for pattern, files in result.items() if files} return result"},{"question":"**Objective:** The goal of this exercise is to assess your understanding of Python\'s `asyncio` module, including how to manage concurrency, handle scheduling, and debug coroutines. You will implement an asynchronous function and demonstrate proper exception handling and logging. **Question:** You are tasked with writing an asynchronous application that simulates data processing from various sources. The application needs to: 1. **Fetch data asynchronously** from three simulated data sources. 2. **Process the fetched data** concurrently without blocking the event loop. 3. **Log important events** and handle any exceptions properly. 4. **Run in debug mode** with detailed logging to help identify any issues. Implement the following function: ```python import asyncio import logging async def fetch_data(source: str) -> str: Simulate fetching data asynchronously from a data source. # Code to simulate data fetching pass async def process_data(data: str) -> str: Simulate processing data asynchronously. # Code to simulate data processing pass async def main(): Main function to manage fetching and processing data concurrently. # Implementation here pass if __name__ == \\"__main__\\": # Setup logging and debug mode logging.basicConfig(level=logging.DEBUG) asyncio.run(main(), debug=True) ``` **Detailed Requirements:** 1. **Fetching Data**: - Implement `fetch_data` that takes a source identifier (string) and simulates an I/O-bound operation that takes 1 second to complete. - The function should return a string indicating the data fetched from the source. 2. **Processing Data**: - Implement `process_data` that takes the fetched data (string) and simulates a CPU-bound operation that takes 1 second to complete. - The function should return a string indicating the processed result. 3. **Main Function**: - Implement the `main` function to create three separate tasks for fetching data from sources \\"Source A\\", \\"Source B\\", and \\"Source C\\" concurrently. - Once data from all sources have been fetched, create three new tasks to process the fetched data concurrently. - Use `asyncio.gather` to wait for all tasks to complete. - Log the events: data fetching start, data fetching complete, data processing start, data processing complete. - Ensure your code properly handles any exceptions that may be raised during data fetching or processing. 4. **Debug Mode**: - Ensure that the application runs in debug mode by enabling the `asyncio` debug mode. - Configure the logging level to `DEBUG` to capture all events. 5. **Expected Output**: - The output should consist of logging statements indicating the progress of data fetching and processing. - Handle any potential exceptions gracefully and log appropriate error messages. ```python import asyncio import logging async def fetch_data(source: str) -> str: logging.debug(f\\"Starting to fetch data from {source}\\") await asyncio.sleep(1) # Simulate network delay logging.debug(f\\"Completed fetching data from {source}\\") return f\\"Data from {source}\\" async def process_data(data: str) -> str: logging.debug(f\\"Starting to process {data}\\") await asyncio.sleep(1) # Simulate processing delay logging.debug(f\\"Completed processing {data}\\") return f\\"Processed {data}\\" async def main(): sources = [\\"Source A\\", \\"Source B\\", \\"Source C\\"] try: # Fetch data concurrently fetch_tasks = [asyncio.create_task(fetch_data(source)) for source in sources] fetched_data = await asyncio.gather(*fetch_tasks) # Process data concurrently process_tasks = [asyncio.create_task(process_data(data)) for data in fetched_data] processed_data = await asyncio.gather(*process_tasks) for data in processed_data: logging.info(f\\"Result: {data}\\") except Exception as e: logging.error(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": logging.basicConfig(level=logging.DEBUG) asyncio.run(main(), debug=True) ``` **Constraints:** - All tasks (fetching and processing) must be executed concurrently. - Ensure that debug mode is enabled and logging is set to `DEBUG` level. - Use sleep to simulate I/O and processing delays. - Handle and log exceptions properly. **Additional Note:** - Submitting your solution should include code implementation and output of the logging information to demonstrate the correctness of your application.","solution":"import asyncio import logging async def fetch_data(source: str) -> str: logging.debug(f\\"Starting to fetch data from {source}\\") await asyncio.sleep(1) # Simulate network delay logging.debug(f\\"Completed fetching data from {source}\\") return f\\"Data from {source}\\" async def process_data(data: str) -> str: logging.debug(f\\"Starting to process {data}\\") await asyncio.sleep(1) # Simulate processing delay logging.debug(f\\"Completed processing {data}\\") return f\\"Processed {data}\\" async def main(): sources = [\\"Source A\\", \\"Source B\\", \\"Source C\\"] try: # Fetch data concurrently fetch_tasks = [asyncio.create_task(fetch_data(source)) for source in sources] fetched_data = await asyncio.gather(*fetch_tasks) # Process data concurrently process_tasks = [asyncio.create_task(process_data(data)) for data in fetched_data] processed_data = await asyncio.gather(*process_tasks) for data in processed_data: logging.info(f\\"Result: {data}\\") except Exception as e: logging.error(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": logging.basicConfig(level=logging.DEBUG) asyncio.run(main(), debug=True)"},{"question":"# Question: Using the MPS Backend in PyTorch You are provided with an existing neural network class `Net` defined as follows: ```python import torch import torch.nn as nn import torch.optim as optim class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x ``` Your task is to: 1. Check if the MPS backend is available on your machine and handle the case if it is not. 2. Create a random input tensor of shape `(64, 10)` on the MPS device if available. 3. Instantiate the `Net` model and move it to the MPS device if available. 4. Perform a forward pass of the model on the input tensor and compute the mean squared error loss with respect to a random target tensor of the same shape as the model\'s output. 5. Move the input tensor, target tensor, and model back to the CPU. 6. Output the final computed loss. # Constraints: - If the MPS device is not available, perform all operations on the CPU. - You should handle the case where the model and tensors need to be appropriately moved between devices. - Random seed should be set to 0 for reproducibility. # Input: - There is no direct input to the function. # Output: - A single floating number representing the computed mean squared error loss. # Function Signature: ```python def run_model_mps(): pass ``` # Example Usage: ```python loss = run_model_mps() print(loss) ``` You may assume that PyTorch and the necessary libraries are installed in the environment where this script will run.","solution":"import torch import torch.nn as nn import torch.optim as optim import numpy as np class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def run_model_mps(): # Set random seed for reproducibility torch.manual_seed(0) np.random.seed(0) # Check if MPS is available device = torch.device(\\"mps\\") if torch.has_mps else torch.device(\\"cpu\\") # Generate random input tensor input_tensor = torch.randn(64, 10, device=device) # Instantiate and move model to device model = Net().to(device) # Perform forward pass output = model(input_tensor) # Generate random target tensor target_tensor = torch.randn(64, 1, device=device) # Compute mean squared error loss criterion = nn.MSELoss() loss = criterion(output, target_tensor) # Move tensors and model back to CPU input_tensor_cpu = input_tensor.to(\\"cpu\\") target_tensor_cpu = target_tensor.to(\\"cpu\\") model_cpu = model.to(\\"cpu\\") loss_cpu = loss.item() # Get the loss value, it\'s already on the CPU # Return the final computed loss return loss_cpu"},{"question":"You are given a class called `DatabaseClient` that interacts with a database. The class has three methods: 1. `connect()`: Establishes a connection to the database. 2. `fetch_data(query)`: Executes a query on the database and returns the result. 3. `close_connection()`: Closes the connection to the database. Your task is to write a unit test for the `DatabaseClient` class using the `unittest.mock` module to ensure that: 1. The connection to the database is established. 2. A specific query is executed. 3. The connection is closed after the query is executed. 4. The result of the query is correctly returned. Here is the `DatabaseClient` class for reference: ```python class DatabaseClient: def connect(self): pass def fetch_data(self, query): pass def close_connection(self): pass ``` # Requirements: 1. Use the `patch.object` function to mock the `connect`, `fetch_data`, and `close_connection` methods of the `DatabaseClient` class. 2. Ensure that the `fetch_data` method is called with a specific query string. 3. Validate the order of method calls: `connect` -> `fetch_data` -> `close_connection`. 4. Set a return value for the `fetch_data` method and assert that it is returned correctly. # Input and Output Format: - **Input**: None - **Output**: Test cases should pass successfully. # Constraints: - Use the `unittest` framework for structuring your test cases. # Example Usage: ```python import unittest from unittest.mock import patch class TestDatabaseClient(unittest.TestCase): @patch.object(DatabaseClient, \'connect\') @patch.object(DatabaseClient, \'fetch_data\') @patch.object(DatabaseClient, \'close_connection\') def test_database_client(self, mock_close, mock_fetch, mock_connect): # Your test implementation here if __name__ == \'__main__\': unittest.main() ``` Implement the necessary test methods within the `TestDatabaseClient` class to validate the behavior of the `DatabaseClient` class according to the requirements mentioned.","solution":"class DatabaseClient: def connect(self): pass def fetch_data(self, query): pass def close_connection(self): pass def use_database(client, query): client.connect() result = client.fetch_data(query) client.close_connection() return result"},{"question":"Objective Implement a Python function that utilizes memoryview objects to read and manipulate buffer data from different sources. Ensure you understand how to create memoryview objects and utilize them for manipulating in-memory data efficiently. Problem Statement You are provided with several byte buffers representing different data sources. Your task is to create a function that accumulates data from these buffers using memoryview objects and performs specific manipulations. Function Signature ```python def accumulate_and_transform(buffers: List[bytes], transform: Callable[[memoryview], memoryview]) -> bytes: Accumulates data from the given list of byte buffers, applies the transformation function on memoryview objects, and returns the concatenated transformed data as bytes. Parameters: buffers (List[bytes]): A list of byte buffers to accumulate and transform. transform (Callable[[memoryview], memoryview]): A transformation function to apply to each memoryview. Returns: bytes: The concatenated, transformed data. pass ``` Input - `buffers`: A list of byte arrays (`List[bytes]`). Each byte array can be of arbitrary length. - `transform`: A function that takes a memoryview object as input, performs a specific transformation, and returns a memoryview object. Output - Returns the concatenated byte arrays after applying the transformation function to each memoryview representation of the buffers. Example ```python def example_transform(mv: memoryview) -> memoryview: # Example transformation: Reverse the memory content return memoryview(bytes(mv[::-1])) buffers = [b\'hello\', b\' \', b\'world\'] result = accumulate_and_transform(buffers, example_transform) print(result) # Output: b\'olleh dlrow\' ``` Constraints 1. You must use memoryview objects to handle and manipulate the buffers. 2. Your solution should be efficient, avoiding unnecessary data copying or conversions. 3. Ensure that the transformation function can handle varying buffer lengths. Notes 1. Memoryview objects provide an efficient way to manipulate byte data. Utilize them to avoid unnecessary copies. 2. You must ensure that the manipulated data is correctly accumulated and returned as a byte object.","solution":"from typing import List, Callable def accumulate_and_transform(buffers: List[bytes], transform: Callable[[memoryview], memoryview]) -> bytes: Accumulates data from the given list of byte buffers, applies the transformation function on memoryview objects, and returns the concatenated transformed data as bytes. Parameters: buffers (List[bytes]): A list of byte buffers to accumulate and transform. transform (Callable[[memoryview], memoryview]): A transformation function to apply to each memoryview. Returns: bytes: The concatenated, transformed data. result = bytearray() for buffer in buffers: mv = memoryview(buffer) transformed_mv = transform(mv) result.extend(transformed_mv) return bytes(result)"},{"question":"# Advanced File and Directory Management with Python **Objective**: Demonstrate understanding and efficient usage of multiple Python Standard Library modules to manage files and directories, process command-line arguments, and handle data. **Question**: You are required to write a Python script that performs the following tasks using the Python Standard Library: 1. **Directory Management**: - Accept a directory path from the command line. - If the directory does not exist, create it. 2. **File Operations**: - Within the provided directory, create a new file named `summary.txt`. - Write the current date and time into this file. - Create another file named `data.txt` and populate it with a list of random numbers. 3. **Processing Data**: - Read the numbers from `data.txt` and calculate their mean and variance using the `statistics` module. - Append these statistics to `summary.txt`. 4. **Error Handling**: - Use appropriate error handling to manage potential errors (e.g., permission issues, invalid directory paths). **Input and Output Specifications**: - **Input**: The script should accept a directory path as a command-line argument. - **Output**: The directory will contain two files, `data.txt` and `summary.txt`. `data.txt` will contain a list of random numbers. `summary.txt` will contain the current date and time along with the mean and variance of the numbers in `data.txt`. **Constraints**: - You must use the `os`, `shutil`, `sys`, `argparse`, `datetime`, `random`, and `statistics` modules where appropriate. - The script should handle at least 100 random numbers in `data.txt`. **Performance Requirements**: - The script should be efficient and avoid unnecessary computations or file operations. **Example Usage**: ```sh python manage_files.py /path/to/directory ``` After running the script, `/path/to/directory` should contain `data.txt` populated with random numbers, and `summary.txt` containing the current date and time along with the calculated mean and variance of the numbers. **Guidelines**: 1. Use `argparse` to handle command-line arguments. 2. Use `os` to handle directory and file creation. 3. Use `datetime` to get the current date and time. 4. Use `random` to generate random numbers. 5. Use `statistics` to compute mean and variance. 6. Handle exceptions gracefully and provide meaningful error messages. **Sample Code Structure**: ```python import os import sys import argparse import datetime import random import statistics def create_directory(path): # Your code here def write_current_datetime(file_path): # Your code here def generate_random_numbers(file_path, count=100): # Your code here def compute_statistics(file_path): # Your code here def main(): # Your code here if __name__ == \\"__main__\\": main() ``` **Bonus**: For bonus points, after performing the above tasks, compress the directory into a `.zip` file using the `zipfile` module.","solution":"import os import argparse import datetime import random import statistics import zipfile def create_directory(path): if not os.path.exists(path): os.makedirs(path) def write_current_datetime(file_path): with open(file_path, \'w\') as f: current_datetime = datetime.datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\") f.write(f\\"Current Date and Time: {current_datetime}n\\") def generate_random_numbers(file_path, count=100): numbers = [random.randint(0, 100) for _ in range(count)] with open(file_path, \'w\') as f: for number in numbers: f.write(f\\"{number}n\\") return numbers def compute_statistics(numbers): mean = statistics.mean(numbers) variance = statistics.variance(numbers) return mean, variance def append_statistics(file_path, mean, variance): with open(file_path, \'a\') as f: f.write(f\\"Mean: {mean}n\\") f.write(f\\"Variance: {variance}n\\") def compress_directory(directory): archive_name = f\\"{directory}.zip\\" with zipfile.ZipFile(archive_name, \'w\') as zipf: for root, _, files in os.walk(directory): for file in files: zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), directory)) return archive_name def main(): parser = argparse.ArgumentParser(description=\\"Manage files and directories\\") parser.add_argument(\'directory\', type=str, help=\\"Directory path for operations\\") args = parser.parse_args() directory = args.directory create_directory(directory) summary_file = os.path.join(directory, \'summary.txt\') data_file = os.path.join(directory, \'data.txt\') write_current_datetime(summary_file) numbers = generate_random_numbers(data_file) mean, variance = compute_statistics(numbers) append_statistics(summary_file, mean, variance) compress_directory(directory) if __name__ == \\"__main__\\": main()"},{"question":"# Coding Assessment: Hyperparameter Tuning using GridSearchCV and RandomizedSearchCV Objective The objective of this assignment is to assess your understanding of hyperparameter tuning using `GridSearchCV` and `RandomizedSearchCV` in scikit-learn. You will implement both methods to find the best hyperparameters for a given dataset and model. Problem Statement You are provided with the popular Iris dataset. Your task is to: 1. Implement hyperparameter tuning using `GridSearchCV` to optimize the hyperparameters of an SVM classifier. 2. Implement hyperparameter tuning using `RandomizedSearchCV` to optimize the hyperparameters of the same SVM classifier. 3. For both cases, identify the best hyperparameters and the corresponding model performance. 4. Compare the results and explain the advantages and disadvantages of each method. Requirements 1. Load the Iris dataset from `sklearn.datasets`. 2. Implement `GridSearchCV` with a predefined parameter grid for hyperparameters `C`, `kernel`, and `gamma`. 3. Implement `RandomizedSearchCV` with a predefined distribution for hyperparameters `C`, `kernel`, and `gamma`. 4. Use 5-fold cross-validation during the search for both methods. 5. Report the best hyperparameters and the best score for both methods. 6. Provide a brief explanation comparing `GridSearchCV` and `RandomizedSearchCV` based on your findings. Input - None (the Iris dataset should be loaded within your code) Output - The best hyperparameters and best score found by `GridSearchCV`. - The best hyperparameters and best score found by `RandomizedSearchCV`. - A brief explanation comparing the two methods (in comments). Constraints - Use `scipy.stats.expon` or similar suitable distributions for `RandomizedSearchCV`. Example Solution Outline ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV from sklearn.svm import SVC from scipy.stats import expon # Load the Iris dataset data = load_iris() X, y = data.data, data.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the parameter grid for GridSearchCV param_grid = [ {\'C\': [1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] # Implement GridSearchCV grid_search = GridSearchCV(SVC(), param_grid, cv=5) grid_search.fit(X_train, y_train) # Output grid search results print(\\"GridSearchCV Best Params:\\", grid_search.best_params_) print(\\"GridSearchCV Best Score:\\", grid_search.best_score_) # Define the parameter distribution for RandomizedSearchCV param_dist = { \'C\': expon(scale=100), \'gamma\': expon(scale=0.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } # Implement RandomizedSearchCV random_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=100, cv=5, random_state=42) random_search.fit(X_train, y_train) # Output random search results print(\\"RandomizedSearchCV Best Params:\\", random_search.best_params_) print(\\"RandomizedSearchCV Best Score:\\", random_search.best_score_) # Explanation # GridSearchCV evaluates all parameter combinations, resulting in more computation time. # RandomizedSearchCV samples parameter combinations, offering a potentially faster approach with a large parameter space. ```","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV from sklearn.svm import SVC from scipy.stats import expon def get_best_hyperparameters(): # Load the Iris dataset data = load_iris() X, y = data.data, data.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the parameter grid for GridSearchCV param_grid = [ {\'C\': [1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] # Implement GridSearchCV grid_search = GridSearchCV(SVC(), param_grid, cv=5) grid_search.fit(X_train, y_train) # Capture grid search results grid_best_params = grid_search.best_params_ grid_best_score = grid_search.best_score_ # Define the parameter distribution for RandomizedSearchCV param_dist = { \'C\': expon(scale=100), \'gamma\': expon(scale=0.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } # Implement RandomizedSearchCV random_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=100, cv=5, random_state=42) random_search.fit(X_train, y_train) # Capture random search results random_best_params = random_search.best_params_ random_best_score = random_search.best_score_ # Explanation explanation = ( \\"GridSearchCV evaluates all parameter combinations provided in the grid, resulting in more computation time. \\" \\"It guarantees that the optimal parameters within the grid are found, but it can be computationally expensive.n\\" \\"RandomizedSearchCV randomly samples a given number of parameter combinations, which can be much faster, especially \\" \\"when the parameter space is large. However, it does not guarantee finding the absolute best parameters but often finds \\" \\"fairly good ones in less time.\\" ) return (grid_best_params, grid_best_score, random_best_params, random_best_score, explanation) grid_best_params, grid_best_score, random_best_params, random_best_score, explanation = get_best_hyperparameters() print(\\"GridSearchCV Best Params:\\", grid_best_params) print(\\"GridSearchCV Best Score:\\", grid_best_score) print(\\"RandomizedSearchCV Best Params:\\", random_best_params) print(\\"RandomizedSearchCV Best Score:\\", random_best_score) print(\\"Explanation:\\", explanation)"},{"question":"Challenging Coding Assessment Question: # Problem Statement You are tasked with creating a configuration management utility in Python that reads from a configuration file, performs some modifications, and writes the updated configurations back to a new file. The configuration file follows the INI format and your task is to use the `configparser` module to achieve this. # Function to Implement Implement a function `update_config(filename: str, changes: dict, output_filename: str) -> None` that performs the following operations: 1. **Reads** the configuration from the file specified by `filename`. 2. **Applies** the changes specified in the `changes` dictionary. The dictionary is structured with section names as keys and another dictionary as values. The inner dictionary contains key-value pairs to be updated or added within the respective section. 3. **Writes** the updated configuration to a file specified by `output_filename`. # Input and Output Formats - **Input:** - `filename` (str): The path to the input configuration file (in INI format). - `changes` (dict): A dictionary containing the sections and corresponding updates that need to be applied. - `output_filename` (str): The path where the updated configuration file will be saved. - **Output:** - The function shall not return any value. It should write the updated configurations to the `output_filename`. # Constraints - The configuration file will be in a valid INI format. - If a section or a key does not exist in the original configuration file, it should be added. - The function should handle cases where the configuration file is large efficiently. # Examples ```python # Example configuration file content (input filename: \'config.ini\'): [General] version = 1.0 name = Sample Application [Settings] theme = light timeout = 30 [Database] type = sqlite host = localhost # Changes to be applied to the configuration changes = { \'General\': { \'version\': \'2.0\', # Update existing key \'new_feature\': \'enabled\' # Add new key }, \'Settings\': { \'theme\': \'dark\' # Update existing key }, \'Database\': { \'type\': \'postgresql\', # Update existing key \'port\': \'5432\' # Add new key }, \'NewSection\': { \'new_key\': \'new_value\' # Completely new section and key } } # expected output filename: \'updated_config.ini\' # Calling the function update_config(\'config.ini\', changes, \'updated_config.ini\') # The updated \'updated_config.ini\' file should have the content: [General] version = 2.0 name = Sample Application new_feature = enabled [Settings] theme = dark timeout = 30 [Database] type = postgresql host = localhost port = 5432 [NewSection] new_key = new_value ``` # Additional Notes - To start with, you may find the `configparser` module documentation useful: https://docs.python.org/3/library/configparser.html","solution":"import configparser def update_config(filename: str, changes: dict, output_filename: str) -> None: Reads the configuration from the file specified by `filename`, applies the changes specified in the `changes` dictionary, and writes the updated configurations to `output_filename`. # Create a config parser object config = configparser.ConfigParser() # Read the existing configuration file config.read(filename) # Apply the changes for section, params in changes.items(): if not config.has_section(section): config.add_section(section) for key, value in params.items(): config.set(section, key, value) # Write the updated configuration to the output file with open(output_filename, \'w\') as configfile: config.write(configfile)"},{"question":"<|Analysis Begin|> The \\"chunk\\" module in Python provides an interface for reading files that adhere to the EA IFF 85 file format, which is used by file formats such as AIFF/AIFF-C and RMFF. The core element described in this documentation is the `Chunk` class, which facilitates reading data from chunks within a file. Key elements and methods of the `Chunk` class include: - **Constructor `__init__`**: Initializes a chunk object with parameters including file-like object, align, bigendian, and inclheader. - **Methods**: - `getname()`: Retrieves the chunk ID. - `getsize()`: Retrieves the chunk size. - `close()`: Closes the chunk and skips to its end without closing the file itself. - `seek(pos, whence)`: Repositions within the chunk. - `tell()`: Returns the current position. - `read(size)`: Reads data from the chunk. - `skip()`: Skips to the end of the chunk. Given this detailed functionality, a challenging question can be designed that requires students to implement functionality that interacts with this `Chunk` class to perform specific operations on IFF files, ensuring they understand file handling, byte order, and alignment. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Problem Statement:** You are provided with an IFF file that contains several chunks. Each chunk has a unique ID and consists of various data that may need to be processed differently based on the ID. Your task is to implement a function `process_iff_file(file_path)` that reads this file and extracts data from specific chunks whose IDs are provided in a list. The function should handle chunk alignments, byte orders, and potential header inclusions correctly. You are required to: 1. Open the file and read through each chunk. 2. For each chunk whose ID is in the provided list of IDs, read its data and store the data in a dictionary with the chunk ID as the key. 3. Ensure you handle big-endian and little-endian formats as per the given argument. 4. Handle errors gracefully, ensuring all file and chunk operations do not leave the file object in an inconsistent state. **Function Signature:** ```python def process_iff_file(file_path: str, chunk_ids: list, bigendian: bool = True) -> dict: pass ``` **Input:** - `file_path` (str): Path to the IFF file. - `chunk_ids` (list): List of chunk IDs (4-character strings) which need to be processed. - `bigendian` (bool): If True, chunk size is in big-endian order. If False, it is in little-endian. Default is True. **Output:** - A dictionary where the keys are chunk IDs specified in `chunk_ids`, and the values are bytes objects containing the data of the corresponding chunk. **Constraints:** - The file can be large, so the implementation should avoid loading the entire file into memory. - The function should only read and store chunks listed in `chunk_ids`. - You must use the `chunk.Chunk` class to read the chunks from the file. - The chunk size does not include the 8-byte header unless `inclheader` is specified otherwise in any provided arguments. **Example:** ```python # Assuming we have a file_path and chunk IDs like \'FORM\', \'AIFF\', \'COMM\' # and the file consists of chunks with these IDs file_path = \\"sample.iff\\" chunk_ids = [\'FORM\', \'AIFF\', \'COMM\'] result = process_iff_file(file_path, chunk_ids) for chunk_id, data in result.items(): print(f\\"Chunk ID: {chunk_id}, Data: {data[:50]}\\") # Print first 50 bytes of each chunk data ``` **Notes:** - Ensure to handle the alignment if required (2-byte boundaries). - Test with both big-endian and little-endian files. - Handle padding correctly when size of data is odd. Implement the function `process_iff_file()` based on the requirements given above.","solution":"import chunk def process_iff_file(file_path: str, chunk_ids: list, bigendian: bool = True) -> dict: result = {} try: with open(file_path, \'rb\') as file: while True: try: current_chunk = chunk.Chunk(file, bigendian=bigendian) chunk_id = current_chunk.getname().decode(\'ascii\') chunk_size = current_chunk.getsize() if chunk_id in chunk_ids: data = current_chunk.read(chunk_size) result[chunk_id] = data current_chunk.skip() except EOFError: break except Exception as e: print(f\\"Error processing file: {e}\\") return result"},{"question":"# Pandas Assessment Problem Statement You are provided with a dataset containing information about a fleet of vehicles. The dataset consists of columns such as `vehicle_id`, `vehicle_type`, `model_year`, `mileage`, `fuel_type`, and `last_serviced_date`. Your task is to implement a function `analyze_fleet_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, float, pd.DataFrame]` that performs the following analyses: 1. **Summarize Data**: - Generate a summary of the dataset with descriptive statistics for each numerical column and include the count, mean, standard deviation, min, 25%, 50%, 75%, and max values. 2. **Missing Data Handling**: - Identify which columns have missing data and the proportion of missing data in those columns. 3. **Calculate Average Mileage**: - Calculate the average mileage for each type of fuel used (`fuel_type`). 4. **Vehicle Service Interval Analysis**: - Determine the average number of days between `last_serviced_date` and the current date. Assume today\'s date is October 1, 2023. 5. **Outlier Detection**: - Identify vehicles whose mileage is more than 3 standard deviations away from their mean mileage. 6. **String Operations on `vehicle_type`**: - Convert the `vehicle_type` column to lowercase to ensure consistency, and strip any leading or trailing whitespace. Input - `df`: A pandas DataFrame containing the fleet data with columns `vehicle_id`, `vehicle_type`, `model_year`, `mileage`, `fuel_type`, and `last_serviced_date`. Output - A tuple containing: 1. A DataFrame with the descriptive statistics summary. 2. A Series indicating the proportion of missing data in columns with missing values. 3. The average mileage for each `fuel_type`. 4. A DataFrame with two columns: `vehicle_id` and `days_since_last_service`, indicating the number of days since each vehicle was last serviced. Constraints - Assume the dataset is reasonably large and the function should be efficient in handling it. - Handle data types carefully, ensuring date operations are performed correctly. Example Given the following sample DataFrame: | vehicle_id | vehicle_type | model_year | mileage | fuel_type | last_serviced_date | |------------|--------------|------------|---------|-----------|--------------------| | 1 | Car | 2015 | 75000 | Diesel | 2023-01-15 | | 2 | Truck | 2017 | 120000 | Petrol | 2022-12-10 | | 3 | Car | 2016 | | Diesel | | | 4 | Motorbike | 2020 | 20000 | Electric | 2023-07-22 | | 5 | Car | | 90000 | Hybrid | 2023-08-30 | Your implementation should handle this data and return the corresponding outputs as specified above. Hint - Utilize pandas\' vectorized operations for efficiency. - Pay attention to handling missing data and date operations properly. ```python from typing import Tuple import pandas as pd def analyze_fleet_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, float, pd.DataFrame]: # Your implementation here pass # Below is a small dataset to test the function data = { \'vehicle_id\': [1, 2, 3, 4, 5], \'vehicle_type\': [\'Car\', \' Truck\', \'Car\', \'Motorbike\', \'Car\'], \'model_year\': [2015, 2017, None, 2020, None], \'mileage\': [75000, 120000, None, 20000, 90000], \'fuel_type\': [\'Diesel\', \'Petrol\', \'Diesel\', \'Electric\', \'Hybrid\'], \'last_serviced_date\': [\'2023-01-15\', \'2022-12-10\', None, \'2023-07-22\', \'2023-08-30\'] } df = pd.DataFrame(data) df[\'last_serviced_date\'] = pd.to_datetime(df[\'last_serviced_date\'], errors=\'coerce\') # Testing the function output = analyze_fleet_data(df) ```","solution":"from typing import Tuple import pandas as pd from datetime import datetime def analyze_fleet_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, pd.Series, pd.DataFrame]: # 1. Summarize Data summary = df.describe(include=\'all\') # 2. Missing Data Handling missing_data = df.isnull().mean() missing_data = missing_data[missing_data > 0] # 3. Calculate Average Mileage for each fuel type average_mileage = df.dropna(subset=[\'mileage\']).groupby(\'fuel_type\')[\'mileage\'].mean() # 4. Vehicle Service Interval Analysis today = pd.Timestamp(\'2023-10-01\') df[\'last_serviced_date\'] = pd.to_datetime(df[\'last_serviced_date\']) df[\'days_since_last_service\'] = (today - df[\'last_serviced_date\']).dt.days service_interval = df[[\'vehicle_id\', \'days_since_last_service\']].dropna() # 5. Outlier Detection mileage_mean = df[\'mileage\'].mean() mileage_std = df[\'mileage\'].std() outliers = df[(df[\'mileage\'] > mileage_mean + 3 * mileage_std) | (df[\'mileage\'] < mileage_mean - 3 * mileage_std)] # 6. String Operations on `vehicle_type` df[\'vehicle_type\'] = df[\'vehicle_type\'].str.lower().str.strip() return summary, missing_data, average_mileage, service_interval"},{"question":"# Question: Shell Command String Manipulator You are required to implement a few utility functions utilizing the `shlex` module to handle shell command manipulations. These utilities should help in splitting command strings, joining command tokens back into a string, and safely quoting portions of command strings. Implement the following functions: 1. `shell_split(command: str) -> list`: This function should take a single string `command` representing a shell command and split it into a list of tokens using shell-like syntax. The comments should be disabled, and it should operate in POSIX mode. - **Input**: A string `command` (1<= len(command) <= 1000) - **Output**: A list of strings which are the tokens of the command. 2. `shell_join(tokens: list) -> str`: This function should take a list of command tokens and join them back into a shell command string. - **Input**: A list `tokens` of strings (0 <= len(tokens) <= 100) - **Output**: A string which is the concatenated command with proper shell escaping for security. 3. `shell_quote(token: str) -> str`: This function should take a single string `token` and return a shell-escaped version of that string that can be used safely as one token in a shell command line. - **Input**: A string `token` (0 <= len(token) <= 100) - **Output**: A shell-escaped string. # Constraints: - All strings will contain printable ASCII characters only. - Ensure that your code is efficient and handles edge cases, such as empty strings or lists. # Example: ```python # Example 1: command = \\"echo \'Hello, World!\'\\" assert shell_split(command) == [\'echo\', \'Hello, World!\'] assert shell_join([\'echo\', \'Hello, World!\']) == \\"echo \'Hello, World!\'\\" assert shell_quote(\\"echo \'Hello, World!\'\\") == \\"\'echo \'\\"\'\\"\'Hello, World!\'\\"\'\\"\'\'\\" # Example 2: command = \\"grep \\"foo\\" bar.txt\\" assert shell_split(command) == [\'grep\', \'foo\', \'bar.txt\'] assert shell_join([\'grep\', \'foo\', \'bar.txt\']) == \\"grep \'foo\' bar.txt\\" assert shell_quote(\\"foo\\") == \\"\'foo\'\\" ``` Implement these functions using the `shlex` module. Validate your solution with the provided examples and ensure you cover edge cases.","solution":"import shlex def shell_split(command: str) -> list: Split the command string into a list of tokens using shell-like syntax. return shlex.split(command, posix=True) def shell_join(tokens: list) -> str: Join a list of command tokens back into a string with proper shell escaping. return \' \'.join(shlex.quote(token) for token in tokens) def shell_quote(token: str) -> str: Return a shell-escaped version of the string token. return shlex.quote(token)"},{"question":"# Custom Exception Handling and Chaining in Python **Objective:** Write a Python program that demonstrates your understanding of creating custom exceptions, raising them, and properly chaining exceptions. # Task: 1. **Define a Custom Exception:** - Create a custom exception `InvalidAgeError` which inherits from `ValueError`. - This exception should include an attribute `age` to store the invalid age value. 2. **Function Implementation:** - Implement a function `check_age(age)` that: - Takes an integer `age` (representing a person\'s age) as input. - Raises `InvalidAgeError` if the provided age is less than 0 or more than 150. - Returns a message \\"Valid age\\" if the age is valid. 3. **Chaining Exceptions:** - Create a second function called `validate_age_input(input_data)` that: - Takes a single parameter `input_data`. - Tries to convert `input_data` to an integer. - If a `ValueError` occurs, raises a custom `InvalidInputError`, keeping the original `ValueError` as context. - Calls `check_age(age)` for validated age input. - Returns the result of `check_age(age)` if successful. 4. **Implement the `InvalidInputError` class:** - This should be a custom exception that inherits from `Exception`. - Should include an attribute `input_data` to store invalid input. # Input and Output - For the `check_age(age)` function: - **Input:** An integer `age`. - **Output:** Returns \\"Valid age\\" if the age is valid else raises `InvalidAgeError`. - For the `validate_age_input(input_data)` function: - **Input:** Any data type as `input_data`. - **Output:** Returns the result of `check_age(age)` if input is valid, otherwise raises `InvalidInputError`. # Constraints: - Ensure that any exception details are informative and useful for debugging. - The provided functions should not terminate execution abruptly when an exception occurs; it should provide proper feedback. # Example Usage: ```python # Custom exceptions definition class InvalidAgeError(ValueError): def __init__(self, age, message=\\"Invalid age provided\\"): self.age = age self.message = message super().__init__(f\\"{message}: {age}\\") class InvalidInputError(Exception): def __init__(self, input_data, message=\\"Invalid input data\\"): self.input_data = input_data self.message = message super().__init__(f\\"{message}: {input_data}\\") # Function Implementations def check_age(age): if age < 0 or age > 150: raise InvalidAgeError(age) return \\"Valid age\\" def validate_age_input(input_data): try: age = int(input_data) except ValueError as e: raise InvalidInputError(input_data) from e return check_age(age) # Example calls try: print(validate_age_input(\\"28\\")) # Should print: Valid age print(validate_age_input(\\"-5\\")) # Should raise InvalidAgeError print(validate_age_input(\\"abc\\")) # Should raise InvalidInputError except InvalidInputError as e: print(e) except InvalidAgeError as e: print(e) ``` # Submission Please submit a single Python file that includes the custom exception classes and the function implementations as described above.","solution":"# Custom exceptions definition class InvalidAgeError(ValueError): def __init__(self, age, message=\\"Invalid age provided\\"): self.age = age self.message = message super().__init__(f\\"{message}: {age}\\") class InvalidInputError(Exception): def __init__(self, input_data, message=\\"Invalid input data\\"): self.input_data = input_data self.message = message super().__init__(f\\"{message}: {input_data}\\") # Function Implementations def check_age(age): Checks if the given age is valid. Parameters: age (int): The age to check. Returns: str: \\"Valid age\\" if the age is between 0 and 150. Raises: InvalidAgeError: If the age is less than 0 or more than 150. if age < 0 or age > 150: raise InvalidAgeError(age) return \\"Valid age\\" def validate_age_input(input_data): Validates the age from the input data. Parameters: input_data: The input data to validate and convert to age. Returns: str: The result of check_age if the input is valid. Raises: InvalidInputError: If input_data cannot be converted to an integer. try: age = int(input_data) except ValueError as e: raise InvalidInputError(input_data) from e return check_age(age)"},{"question":"You are asked to implement a basic curses application that uses panels to manage multiple overlapping windows. Your task is to create a Python script using the `curses` and `curses.panel` modules that performs the following actions: 1. Initialize the curses environment and create three windows with the following properties: - Window 1: Positioned at (5, 5) with dimensions 10 rows and 20 columns. - Window 2: Positioned at (7, 10) with dimensions 10 rows and 20 columns. - Window 3: Positioned at (12, 15) with dimensions 10 rows and 20 columns. 2. Create corresponding panel objects for each window. Use these panels to: - Stack the panels such that Window 1 is at the bottom, Window 2 is in the middle, and Window 3 is at the top. - Hide Window 2 temporarily and then show it again after 3 seconds. - Move Window 3 to a new position (3, 3) and update the stack order so that Window 3 is now at the top. - After another 3 seconds, push Window 1 to the top and move Window 2 to position (0, 0). 3. Throughout this sequence of actions, ensure that the screen updates correctly to reflect the changes in panel visibility and order. 4. Properly terminate the curses environment when the script exits. **Expected Input and Output:** - **Input:** No user input is required. - **Output:** The script should display the three windows on the terminal, managing their visibility and stacking order according to the specified sequence. **Performance Requirements:** - Ensure the application runs efficiently without excessive flickering or lag during updates. - Handle any exceptions to properly clean up the curses environment in case of errors. **Implementation Constraints:** - You should use the `curses` and `curses.panel` modules exclusively for managing the windows and panels. - Make sure to import necessary modules and handle the initialization and termination of the curses environment properly. **Sample Code Structure:** ```python import curses import curses.panel import time def main(stdscr): # Initialize windows and panels here # Perform actions on windows and panels # Ensure the screen updates properly # Remember to handle exit and cleanup pass if __name__ == \\"__main__\\": curses.wrapper(main) ``` **Note:** - Use `curses.panel.update_panels()` to update the virtual screen after changes. - Use `curses.doupdate()` to refresh the screen. # Additional Notes: - This question requires a good understanding of both the `curses` and `curses.panel` modules. - Focus on proper usage of panel methods to manipulate stacking order and visibility.","solution":"import curses import curses.panel import time def main(stdscr): # Hide cursor curses.curs_set(0) # Create windows win1 = curses.newwin(10, 20, 5, 5) win2 = curses.newwin(10, 20, 7, 10) win3 = curses.newwin(10, 20, 12, 15) # Add content to windows win1.addstr(1, 1, \\"Window 1\\") win2.addstr(1, 1, \\"Window 2\\") win3.addstr(1, 1, \\"Window 3\\") # Create panels panel1 = curses.panel.new_panel(win1) panel2 = curses.panel.new_panel(win2) panel3 = curses.panel.new_panel(win3) # Update panels initially curses.panel.update_panels() curses.doupdate() # Hide second window temporarily panel2.hide() curses.panel.update_panels() curses.doupdate() time.sleep(3) # Show second window again panel2.show() curses.panel.update_panels() curses.doupdate() time.sleep(3) # Move third window and bring to top panel3.move(3, 3) panel3.top() curses.panel.update_panels() curses.doupdate() time.sleep(3) # Bring first window to top and move second window panel1.top() panel2.move(0, 0) curses.panel.update_panels() curses.doupdate() time.sleep(3) if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"# Advanced PyTorch Coding Assessment **Objective**: Implement a custom function in PyTorch and verify its gradient computation using `torch.autograd.gradcheck` and `torch.autograd.gradgradcheck`. This exercise will assess your understanding and ability to work with PyTorch\'s gradient checking utilities, ensuring correctness in both first and second-order derivatives. **Problem Statement**: You are to implement a custom function that involves both real and complex-valued inputs and outputs. Then, you would verify its correctness using PyTorchs gradient checking utilities `gradcheck` and `gradgradcheck`. **Tasks**: 1. **Implement Custom Function**: Implement a custom function `complex_function` that takes in a complex-valued PyTorch tensor and returns a real-valued tensor. Ensure the function is differentiable. ```python import torch def complex_function(z: torch.Tensor) -> torch.Tensor: Custom complex function which takes a complex tensor z and returns a real tensor. Parameters: z (torch.Tensor): Complex-valued input tensor. Returns: torch.Tensor: Real-valued output tensor. a = z.real b = z.imag return a**2 + b**2 ``` 2. **Gradient Check**: - Write code to check the gradients of the custom function using `torch.autograd.gradcheck` and `torch.autograd.gradgradcheck`. ```python def verify_gradients(): Verify the gradients of the custom complex function using gradcheck and gradgradcheck. # Create a random tensor of complex values z = torch.randn(3, dtype=torch.cdouble, requires_grad=True) # Check gradients using gradcheck gradcheck_result = torch.autograd.gradcheck(complex_function, (z,)) print(f\\"Gradcheck Passed: {gradcheck_result}\\") # Check second-order gradients using gradgradcheck gradgradcheck_result = torch.autograd.gradgradcheck(complex_function, (z,)) print(f\\"Gradgradcheck Passed: {gradgradcheck_result}\\") verify_gradients() ``` **Input Format**: - The input tensor `z` is a one-dimensional tensor of type `torch.cdouble` with `requires_grad=True`. **Output Format**: - Print statements indicating whether `gradcheck` and `gradgradcheck` passed or not. **Constraints**: - Ensure that the custom function is differentiable and correctly handles complex numbers. - Use double precision for tensor inputs for better gradient checking accuracy. **Performance Requirements**: - The implementation should efficiently handle gradient checks and should not exceed a reasonable computational time for small tensors. **Evaluation Criteria**: - Correct implementation of the custom function. - Proper usage of `gradcheck` and `gradgradcheck` functions. - Correct verification of gradients with informative output.","solution":"import torch def complex_function(z: torch.Tensor) -> torch.Tensor: Custom complex function which takes a complex tensor z and returns a real tensor. Parameters: z (torch.Tensor): Complex-valued input tensor. Returns: torch.Tensor: Real-valued output tensor representing the squared magnitude of z. a = z.real b = z.imag return a**2 + b**2"},{"question":"**Objective**: Write a Python program that demonstrates your understanding of sequence iterators and callable iterators as described in the provided documentation. **Task**: 1. Implement a function named `sequence_iterator` that takes a sequence (e.g., list, tuple) as input and returns a list of elements iterated over using `PySeqIter_New`. 2. Implement a function named `callable_iterator` that takes a callable (a function with no parameters) and a sentinel value as input and returns a list of elements produced by the callable until the sentinel value is encountered using `PyCallIter_New`. **Function Signatures**: ```python def sequence_iterator(seq) -> list: pass def callable_iterator(callable_func, sentinel) -> list: pass ``` **Input/Output**: - For `sequence_iterator(seq)`, the input is a sequence (e.g., list, tuple), and the output is a list of elements iterated over from the sequence. - For `callable_iterator(callable_func, sentinel)`, the input is a callable (e.g., a function) and a sentinel value, and the output is a list of elements produced by calling the callable until it returns the sentinel value. **Example**: ```python # Example for sequence_iterator seq = [1, 2, 3, 4, 5] print(sequence_iterator(seq)) # Output: [1, 2, 3, 4, 5] # Example for callable_iterator def func(): func.counter += 1 return func.counter func.counter = 0 sentinel = 5 print(callable_iterator(func, sentinel)) # Output: [1, 2, 3, 4, 5] ``` **Constraints**: - Ensure the sequence passed to `sequence_iterator` is a valid iterable. - Ensure the callable passed to `callable_iterator` is a valid callable that can be executed with no parameters. **Notes**: - Use the `PySeqIter_New` and `PyCallIter_New` functions as described in the documentation to create the iterators. - Handle the iterations appropriately within the Python program to yield the expected results. Good luck!","solution":"def sequence_iterator(seq): Takes a sequence as input and returns a list of elements iterated over using PySeqIter_New equivalent. # Using the built-in iter function to create an iterator it = iter(seq) result = [] try: while True: result.append(next(it)) except StopIteration: pass return result def callable_iterator(callable_func, sentinel): Takes a callable and a sentinel value as input and returns a list of elements produced by the callable until the sentinel value is encountered. # Using the iter function with sentinel it = iter(callable_func, sentinel) result = [] try: while True: result.append(next(it)) except StopIteration: pass return result"},{"question":"# Platform Report Generator Problem Statement You are tasked with implementing a function that generates a comprehensive report about the platform on which a Python script is being executed. This report should include details about the operating system, the hardware, and the Python interpreter. Function Signature ```python def generate_platform_report() -> str: ``` Requirements - The function should gather information using multiple methods from the `platform` module. - The information that should be included in the report are: - System and release details (using `platform.system()` and `platform.release()`). - Platform version (using `platform.version()`). - Machine type (using `platform.machine()`). - Processor information (using `platform.processor()`). - Python version and build details (using `platform.python_version()`, `platform.python_build()`, and `platform.python_compiler()`). - Node (network name) (using `platform.node()`). - A detailed platform string (using `platform.platform()`). The report should be formatted as a multi-line string with each piece of information on a new line as follows: ``` System: <system> Release: <release> Platform Version: <platform_version> Machine: <machine> Processor: <processor> Python Version: <python_version> Python Build: <python_build_number> on <python_build_date> Python Compiler: <python_compiler> Node: <node> Platform: <platform> ``` Example An example of the output might be: ``` System: Linux Release: 5.8.0-59-generic Platform Version: #66-Ubuntu SMP Wed Jun 2 23:20:19 UTC 2021 Machine: x86_64 Processor: x86_64 Python Version: 3.10.0 Python Build: 3.10.0 (default, Oct 3 2021, 04:10:20) [GCC 7.5.0] Python Compiler: GCC 7.5.0 Node: hostname Platform: Linux-5.8.0-59-generic-x86_64-with-glibc2.29 ``` Constraints - Assume that the `platform` module is imported and available. - The function should handle cases where some information might not be available, using \\"N/A\\" (Not Available) as a placeholder for such cases. Notes This task is intended to test your ability to use the Python `platform` module effectively and to combine the gathered information into a well-formatted report string.","solution":"import platform def generate_platform_report() -> str: try: system = platform.system() except: system = \\"N/A\\" try: release = platform.release() except: release = \\"N/A\\" try: platform_version = platform.version() except: platform_version = \\"N/A\\" try: machine = platform.machine() except: machine = \\"N/A\\" try: processor = platform.processor() except: processor = \\"N/A\\" try: python_version = platform.python_version() except: python_version = \\"N/A\\" try: python_build = platform.python_build() python_build_str = f\\"{python_build[0]} on {python_build[1]}\\" except: python_build_str = \\"N/A\\" try: python_compiler = platform.python_compiler() except: python_compiler = \\"N/A\\" try: node = platform.node() except: node = \\"N/A\\" try: platform_str = platform.platform() except: platform_str = \\"N/A\\" report = ( f\\"System: {system}n\\" f\\"Release: {release}n\\" f\\"Platform Version: {platform_version}n\\" f\\"Machine: {machine}n\\" f\\"Processor: {processor}n\\" f\\"Python Version: {python_version}n\\" f\\"Python Build: {python_build_str}n\\" f\\"Python Compiler: {python_compiler}n\\" f\\"Node: {node}n\\" f\\"Platform: {platform_str}\\" ) return report"},{"question":"# Color Conversion Utility **Problem Description:** You are tasked with creating a color conversion utility using the \\"colorsys\\" module that can convert between various color coordinate systems (RGB, YIQ, HLS, HSV). Your utility should be able to handle random inputs in one coordinate system and convert them to another coordinate system. **Function Signature:** ```python def convert_color(color, input_space, output_space): Convert a color from one color space to another using colorsys module. Parameters: - color: a tuple representing the color in the input space (e.g., (r, g, b) for RGB) - input_space: a string representing the input color space (e.g., \'rgb\', \'yiq\', \'hls\', \'hsv\') - output_space: a string representing the output color space (e.g., \'rgb\', \'yiq\', \'hls\', \'hsv\') Returns: - a tuple representing the color in the output space ``` **Input:** - `color` - A tuple of 3 floating point numbers representing the color in the input space. Each value will be in the range [0, 1] for all spaces except YIQ where some values can be negative. - `input_space` and `output_space` - Strings indicating the input and output color spaces. Possible values are \'rgb\', \'yiq\', \'hls\', and \'hsv\'. **Output:** - A tuple of 3 floating point numbers representing the color in the specified output space. **Constraints:** - You must use the `colorsys` module for the conversions. - You must handle invalid input gracefully (i.e., if the input_space or output_space is not one of \'rgb\', \'yiq\', \'hls\', or \'hsv\'). **Examples:** 1. Convert from RGB to HSV: ```python color = (0.2, 0.4, 0.4) input_space = \'rgb\' output_space = \'hsv\' result = convert_color(color, input_space, output_space) # Expected output: (0.5, 0.5, 0.4) ``` 2. Convert from HLS to RGB: ```python color = (0.5, 0.5, 0.4) input_space = \'hls\' output_space = \'rgb\' result = convert_color(color, input_space, output_space) # Expected output: (0.7, 0.3, 0.6) ``` Your task is to implement the `convert_color` function using the given module functions to handle the conversions appropriately. Ensure you include error handling for unsupported input or output color spaces, and return `None` for such cases.","solution":"import colorsys def convert_color(color, input_space, output_space): Convert a color from one color space to another using colorsys module. Parameters: - color: a tuple representing the color in the input space (e.g., (r, g, b) for RGB) - input_space: a string representing the input color space (e.g., \'rgb\', \'yiq\', \'hls\', \'hsv\') - output_space: a string representing the output color space (e.g., \'rgb\', \'yiq\', \'hls\', \'hsv\') Returns: - a tuple representing the color in the output space if not (isinstance(color, tuple) and len(color) == 3): return None color_spaces = [\'rgb\', \'yiq\', \'hls\', \'hsv\'] if input_space not in color_spaces or output_space not in color_spaces: return None # Convert input to RGB first if input_space == \'rgb\': rgb = color elif input_space == \'yiq\': rgb = colorsys.yiq_to_rgb(*color) elif input_space == \'hls\': rgb = colorsys.hls_to_rgb(*color) elif input_space == \'hsv\': rgb = colorsys.hsv_to_rgb(*color) # Convert from RGB to the output space if output_space == \'rgb\': return rgb elif output_space == \'yiq\': return colorsys.rgb_to_yiq(*rgb) elif output_space == \'hls\': return colorsys.rgb_to_hls(*rgb) elif output_space == \'hsv\': return colorsys.rgb_to_hsv(*rgb)"},{"question":"# Advanced Logging Configuration You are required to set up a logging system for a Python application that runs various operations and needs to record logged events to different destinations based on their severity. The setup should distinguish between normal execution logs, warnings, errors, and critical issues. This will involve using different handlers, formatters, and loggers. Implement a function `setup_logging` that configures the logging system according to the following specifications: 1. **Loggers**: - There should be two loggers: one for the main application (`app_logger`) and another for a library part (`lib_logger`). - The `app_logger` should log all levels of messages to a file named `app.log`. - The `lib_logger` should log warning and above levels to a file named `lib.log`. 2. **Handlers**: - `app_logger` should have two handlers: - A `FileHandler` that writes all logs (DEBUG and above) to `app.log`. - A `StreamHandler` that writes ERROR and CRITICAL logs to the console (stdout). - `lib_logger` should have one `FileHandler` that writes WARNING and above logs to `lib.log`. 3. **Formatters**: - File logs should include the timestamp, logger name, severity level, and the log message. - Console logs should include only the severity level and the log message. 4. **Example Usage**: - After setting up logging using `setup_logging`, demonstrate how to log messages using both `app_logger` and `lib_logger`. Input: - No input parameters for the `setup_logging` function. Output: - The function doesn\'t return anything. Problem Constraints: - Use `logging.basicConfig()` and other related methods to set up the logging configuration. - The configuration should work correctly when the following example code is executed. ```python setup_logging() app_logger = logging.getLogger(\'my_application\') lib_logger = logging.getLogger(\'my_library\') app_logger.debug(\'This is a debug message\') app_logger.info(\'This is an info message\') app_logger.warning(\'This is a warning message\') app_logger.error(\'This is an error message\') app_logger.critical(\'This is critical\') lib_logger.debug(\'This debug message should not appear\') lib_logger.info(\'This info message should not appear\') lib_logger.warning(\'This is a warning from the library\') lib_logger.error(\'This is an error from the library\') lib_logger.critical(\'This is critical from the library\') ``` The above code should result in: - `app.log` containing all the app-related log messages. - `lib.log` containing only the warning and above messages from `lib_logger`. - The console displaying only the error and critical messages from `app_logger`.","solution":"import logging def setup_logging(): # Create formatters file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_formatter = logging.Formatter(\'%(levelname)s - %(message)s\') # Create handlers for app_logger app_file_handler = logging.FileHandler(\'app.log\') app_file_handler.setLevel(logging.DEBUG) app_file_handler.setFormatter(file_formatter) app_console_handler = logging.StreamHandler() app_console_handler.setLevel(logging.ERROR) app_console_handler.setFormatter(console_formatter) # Create app_logger and attach handlers app_logger = logging.getLogger(\'my_application\') app_logger.setLevel(logging.DEBUG) app_logger.addHandler(app_file_handler) app_logger.addHandler(app_console_handler) # Create handler for lib_logger lib_file_handler = logging.FileHandler(\'lib.log\') lib_file_handler.setLevel(logging.WARNING) lib_file_handler.setFormatter(file_formatter) # Create lib_logger and attach handler lib_logger = logging.getLogger(\'my_library\') lib_logger.setLevel(logging.DEBUG) lib_logger.addHandler(lib_file_handler)"},{"question":"# Python Assessment: Serialization and Deserialization with `marshal` Objective Implement functions to serialize and deserialize Python objects using the `marshal` module. Your solutions should handle supported types and manage errors for unsupported types appropriately. Task 1. **Function 1: `serialize_object`** - **Input**: A single Python object (`value`) of a supported type and an optional integer `version` (default to `marshal.version`). - **Output**: A bytes object representing the serialized form of the input value. - **Constraints**: - Raise a `ValueError` if the input objects type is unsupported. - Ensure the version is an integer between 0 and `marshal.version`. - **Example Usage**: ```python serialized_data = serialize_object([1, 2, 3]) ``` 2. **Function 2: `deserialize_object`** - **Input**: A bytes-like object (`data`) that represents the serialized form of a Python object. - **Output**: The original Python object if deserialization is successful, otherwise `None`. - **Constraints**: - Handle exceptions (`EOFError`, `ValueError`, `TypeError`) that may arise during deserialization. - **Example Usage**: ```python original_object = deserialize_object(serialized_data) ``` Function Implementations ```python import marshal def serialize_object(value, version=marshal.version): try: if not isinstance(version, int) or version < 0 or version > marshal.version: raise ValueError(\\"Invalid version specified\\") serialized_data = marshal.dumps(value, version) return serialized_data except ValueError: raise ValueError(\\"Unsupported type for serialization\\") def deserialize_object(data): try: return marshal.loads(data) except (EOFError, ValueError, TypeError): return None ``` Example ```python if __name__ == \\"__main__\\": obj = {\'a\': 1, \'b\': 2.5, \'c\': (3, 4, 5)} try: # Serialize object serialized_data = serialize_object(obj) print(\\"Serialized Data:\\", serialized_data) # Deserialize object deserialized_data = deserialize_object(serialized_data) print(\\"Deserialized Object:\\", deserialized_data) except ValueError as e: print(\\"Error:\\", e) ``` Notes - Ensure that your code handles possible exceptions gracefully. - Test your functions with various supported and unsupported types to validate behavior.","solution":"import marshal def serialize_object(value, version=marshal.version): Serialize a Python object using marshal with an optional version. Parameters: value: The Python object to serialize. version (int): The optional version for serialization (default: marshal.version). Returns: bytes: The serialized representation of the object. Raises: ValueError: If the object type is unsupported or version is invalid. if not isinstance(version, int) or not (0 <= version <= marshal.version): raise ValueError(\\"Invalid version specified\\") try: serialized_data = marshal.dumps(value, version) return serialized_data except ValueError: raise ValueError(\\"Unsupported type for serialization\\") def deserialize_object(data): Deserialize a bytes object back into a Python object using marshal. Parameters: data: The bytes object containing the serialized data. Returns: The deserialized Python object or None if deserialization fails. try: return marshal.loads(data) except (EOFError, ValueError, TypeError): return None"},{"question":"**Problem Statement:** You are tasked with developing a financial reporting tool that utilizes the Python \\"decimal\\" module for precise monetary calculations. Implement several functions ensuring strict control over arithmetic precision and proper handling of exceptional cases. # Function Specifications 1. **set_precision** - Set the precision for decimal calculations. - **Input**: A single integer `n` where `1 <= n <= decimal.MAX_PREC`. - **Output**: None. ```python def set_precision(n: int) -> None: pass ``` 2. **create_decimal** - Convert a string or integer to a `Decimal` object using the current context. - **Input**: A string or integer `value`. - **Output**: A `Decimal` object representing the given value. - **Constraints**: If the input is an invalid string for a decimal, raise `InvalidOperation`. ```python def create_decimal(value: Union[str, int]) -> Decimal: pass ``` 3. **calculate_interest** - Calculate the interest for a given principal, rate, and time using the formula `P * (1 + rt)`, where `P` is the principal, `r` is the rate, and `t` is the time period. - **Input**: - `P` (principal) as a `Decimal` object. - `r` (rate) as a `Decimal` object representing the interest rate (e.g., `0.05` for 5%). - `t` (time) as a `Decimal` object representing the time period. - **Output**: The interest as a `Decimal` object. ```python def calculate_interest(P: Decimal, r: Decimal, t: Decimal) -> Decimal: pass ``` 4. **normalize_report_value** - Normalize a `Decimal` value by stripping the unnecessary trailing zeros. - **Input**: A `Decimal` object `value`. - **Output**: A normalized `Decimal` object. ```python def normalize_report_value(value: Decimal) -> Decimal: pass ``` 5. **format_currency** - Format a `Decimal` value as a string in a readable currency format with thousands separator and two decimal places. - **Input**: A `Decimal` object `value`. - **Output**: A formatted string. ```python def format_currency(value: Decimal) -> str: pass ``` # Example Usage ```python from decimal import Decimal, InvalidOperation # Setting precision set_precision(10) # Creating decimals try: value1 = create_decimal(\\"100.25\\") value2 = create_decimal(200) except InvalidOperation: print(\\"Invalid decimal string\\") # Calculating interest interest = calculate_interest(value1, Decimal(\\"0.05\\"), Decimal(\\"3\\")) # Normalizing value normalized_value = normalize_report_value(Decimal(\\"100.2500\\")) # Formatting currency formatted_value = format_currency(Decimal(\\"1234567.89\\")) print(formatted_value) # Should print \\"1,234,567.89\\" ``` # Evaluation Criteria 1. **Correctness**: Solutions that cover the mentioned requirements. 2. **Exception Handling**: Proper handling of invalid values. 3. **Code Quality**: Clean and readable code following Python conventions.","solution":"from decimal import Decimal, getcontext, InvalidOperation from typing import Union def set_precision(n: int) -> None: Sets the precision for decimal calculations. getcontext().prec = n def create_decimal(value: Union[str, int]) -> Decimal: Converts a string or integer to a Decimal object using the current context. try: return Decimal(value) except InvalidOperation: raise InvalidOperation(\\"Invalid decimal string\\") def calculate_interest(P: Decimal, r: Decimal, t: Decimal) -> Decimal: Calculates the interest for a given principle, rate, and time. return P * (1 + r * t) def normalize_report_value(value: Decimal) -> Decimal: Normalize a Decimal value by stripping the unnecessary trailing zeros. return value.normalize() def format_currency(value: Decimal) -> str: Format a Decimal value in a readable currency format with thousands separator and two decimal places. return f\\"{value:,.2f}\\""},{"question":"# Question: Implement a Custom Python Object Type You are required to implement a custom Python object type that represents a mathematical vector in 3D space. Follow these guidelines: 1. **Object Structure**: Create a custom object type `Vector3D` with the following attributes: - `x` (float): The x-coordinate of the vector. - `y` (float): The y-coordinate of the vector. - `z` (float): The z-coordinate of the vector. 2. **Methods**: Implement the following methods for the `Vector3D` type: - `__init__(self, x, y, z)`: Initialize the vector with coordinates `x`, `y`, and `z`. - `__repr__(self)`: Return a string representation of the vector in the format `Vector3D(x, y, z)`. - `__add__(self, other)`: Add two vectors and return the resulting vector. - `__sub__(self, other)`: Subtract one vector from another and return the resulting vector. - `__mul__(self, scalar)`: Multiply the vector by a scalar and return the resulting vector. - `__abs__(self)`: Return the magnitude (length) of the vector. - `dot(self, other)`: Compute the dot product of two vectors. - `cross(self, other)`: Compute the cross product of two vectors. 3. **Constraints**: - Your implementation must manage memory allocation and deallocation for the vector objects. - Ensure your implementation is efficient in terms of performance. 4. **Performance Requirements**: - Vector operations should have O(1) complexity. - Memory management should be handled using efficient allocation techniques. # Example Input and Output **Example 1** ```python v1 = Vector3D(1.0, 2.0, 3.0) v2 = Vector3D(4.0, 5.0, 6.0) print(v1 + v2) # Vector3D(5.0, 7.0, 9.0) print(v1 - v2) # Vector3D(-3.0, -3.0, -3.0) print(v1 * 2) # Vector3D(2.0, 4.0, 6.0) print(abs(v1)) # 3.7416573867739413 print(v1.dot(v2)) # 32.0 print(v1.cross(v2)) # Vector3D(-3.0, 6.0, -3.0) ``` Make sure to provide complete and functional code for the `Vector3D` type, demonstrating your understanding of the internal structures and memory management in Python 3.10.","solution":"import math class Vector3D: def __init__(self, x, y, z): self.x = float(x) self.y = float(y) self.z = float(z) def __repr__(self): return f\\"Vector3D({self.x}, {self.y}, {self.z})\\" def __add__(self, other): return Vector3D(self.x + other.x, self.y + other.y, self.z + other.z) def __sub__(self, other): return Vector3D(self.x - other.x, self.y - other.y, self.z - other.z) def __mul__(self, scalar): return Vector3D(self.x * scalar, self.y * scalar, self.z * scalar) def __abs__(self): return math.sqrt(self.x**2 + self.y**2 + self.z**2) def dot(self, other): return self.x * other.x + self.y * other.y + self.z * other.z def cross(self, other): return Vector3D(self.y * other.z - self.z * other.y, self.z * other.x - self.x * other.z, self.x * other.y - self.y * other.x)"},{"question":"**Question:** Working with `.plist` files using the `plistlib` module **Problem Statement:** Given a binary property list file (`input_file.plist`), you are to perform the following operations: 1. Read the property list file. 2. Add a new key-value pair to the top-level dictionary, where the key is `\\"newKey\\"` and the value is `\\"newValue\\"`. 3. Update the value of an existing key `\\"aString\\"` to `\\"UpdatedString\\"`. 4. Write the modified dictionary back to a new property list file (`output_file.plist`) in XML format. **Function Signature:** ```python def modify_plist(input_filepath: str, output_filepath: str) -> None: pass ``` **Input:** - `input_filepath` (string): Path to the input binary property list file. - `output_filepath` (string): Path to write the output XML property list file. **Output:** - The function should not return anything. It should write the modified `.plist` file to the specified output file path. **Constraints:** - You can assume the input file always exists and contains a valid property list. - The input property list is always in binary format and should be parsed as such. - The modifications should be made in-memory and the final dictionary should be written to the output file in XML format. **Example:** If `input_file.plist` contains: ```plist <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>aString</key> <string>Doodah</string> <key>aList</key> <array> <string>A</string> <string>B</string> <integer>12</integer> <real>32.1</real> <array> <integer>1</integer> <integer>2</integer> <integer>3</integer> </array> </array> <key>aFloat</key> <real>0.1</real> <key>anInt</key> <integer>728</integer> <key>aDict</key> <dict> <key>anotherString</key> <string>&lt;hello &amp; hi there!&gt;</string> <key>aThirdString</key> <string>Mssig, Ma</string> <key>aTrueValue</key> <true/> <key>aFalseValue</key> <false/> </dict> <key>someData</key> <data> PGJpbmFyeSBndW5rPg== </data> <key>someMoreData</key> <data> PGxvdHMgb2YgYmluYXJ5IGd1bms+PGxvdHMgb2YgYmluYXJ5IGd1bms+PGxvdHMgb2YgYmluYXJ5IGd1bms+PGxvdHMgb2YgYmluYXJ5IGd1bms+... </data> <key>aDate</key> <date>2019-08-24T14:35:20Z</date> </dict> </plist> ``` After running `modify_plist(\'input_file.plist\', \'output_file.plist\')`, the `output_file.plist` should look like: ```plist <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>aString</key> <string>UpdatedString</string> <key>aList</key> <array> <string>A</string> <string>B</string> <integer>12</integer> <real>32.1</real> <array> <integer>1</integer> <integer>2</integer> <integer>3</integer> </array> </array> <key>aFloat</key> <real>0.1</real> <key>anInt</key> <integer>728</integer> <key>aDict</key> <dict> <key>anotherString</key> <string>&lt;hello &amp; hi there!&gt;</string> <key>aThirdString</key> <string>Mssig, Ma</string> <key>aTrueValue</key> <true/> <key>aFalseValue</key> <false/> </dict> <key>someData</key> <data> PGJpbmFyeSBndW5rPg== </data> <key>someMoreData</key> <data> PGxvdHMgb2YgYmluYXJ5IGd1bms+PGxvdHMgb2YgYmluYXJ5IGd1bms+PGxvdHMgb2YgYmluYXJ5IGd1bms+PGxvdHMgb2YgYmluYXJ5IGd1bms+... </data> <key>aDate</key> <date>2019-08-24T14:35:20Z</date> <key>newKey</key> <string>newValue</string> </dict> </plist> ``` **Implementation Notes:** Make sure to handle file reading in binary mode properly and ensure the dictionary is modified in-memory before writing the final plist back to the output file in XML format.","solution":"import plistlib def modify_plist(input_filepath: str, output_filepath: str) -> None: Reads a binary property list file, modifies its content, and writes it back as an XML property list file. Parameters: input_filepath (str): Path to the binary input property list file. output_filepath (str): Path to the XML output property list file. # Reading the binary property list file with open(input_filepath, \'rb\') as f: plist_data = plistlib.load(f) # Modifying the property list content plist_data[\'newKey\'] = \'newValue\' if \'aString\' in plist_data: plist_data[\'aString\'] = \'UpdatedString\' # Writing the modified plist back in XML format with open(output_filepath, \'wb\') as f: plistlib.dump(plist_data, f, fmt=plistlib.FMT_XML)"},{"question":"**Objective:** To assess the student\'s understanding of the `datetime` module, focusing on datetime creation, manipulation, time zone handling, and performing arithmetic operations. **Question:** You are provided with birthdates and meeting schedules in different time zones. Your task is to write a Python function that: 1. Converts all birthdates to UTC. 2. Calculates the age of each individual as of an input reference date (also provided in UTC). 3. Schedules a meeting for each individual at a given local time in their respective time zones and converts these times to UTC. 4. Calculates the duration of each meeting and ensures no overlap between any two meetings in UTC. **Function Signature:** ```python from datetime import datetime, timedelta, timezone def process_schedules_and_birthdates(birthdates, meeting_times, reference_date): Args: - birthdates (list of tuples): Each tuple contains the name (str) of the individual, the birthdate (str) in the format \'YYYY-MM-DD\', and the timezone offset (str) in the format \'+HH:MM\' or \'-HH:MM\'. - meeting_times (list of tuples): Each tuple contains the name (str) of the individual, the meeting start time (str) in the format \'HH:MM\', the duration (int) representing minutes, and the timezone offset (str) in the format \'+HH:MM\' or \'-HH:MM\'. - reference_date (str): Date in UTC in the format \'YYYY-MM-DD\' Returns: - dict: A dictionary where the key is the individual\'s name, and the value is another dictionary with: - \'age\': Current age as of the reference_date. - \'utc_meeting_start\': The meeting start time in UTC. - \'utc_meeting_end\': The meeting end time in UTC. pass ``` # Constraints: 1. Birthdates are given as strings in the \'YYYY-MM-DD\' format. 2. All times (birthdates and meeting times) include timezone information and need to be converted to UTC. 3. Meeting durations are given in minutes. 4. The reference date is given in \'YYYY-MM-DD\' format (UTC). # Example: ```python birthdates = [ (\'Alice\', \'1990-05-21\', \'+02:00\'), (\'Bob\', \'1985-08-15\', \'-05:00\'), ] meeting_times = [ (\'Alice\', \'15:00\', 45, \'+02:00\'), (\'Bob\', \'09:30\', 30, \'-05:00\'), ] reference_date = \'2023-10-02\' output = process_schedules_and_birthdates(birthdates, meeting_times, reference_date) print(output) # Expected output: # { # \'Alice\': { # \'age\': 33, # \'utc_meeting_start\': \'2023-10-02T13:00:00Z\', # \'utc_meeting_end\': \'2023-10-02T13:45:00Z\' # }, # \'Bob\': { # \'age\': 38, # \'utc_meeting_start\': \'2023-10-02T14:30:00Z\', # \'utc_meeting_end\': \'2023-10-02T15:00:00Z\' # } # } ``` # Notes: - You can leverage the `datetime.strptime` method for converting strings to datetime objects and `datetime.isoformat()` for converting datetime objects to strings. - Ensure that the function handles and adjusts for different time zones correctly. - Use the `datetime.timezone` class for managing timezone offsets.","solution":"from datetime import datetime, timedelta, timezone def process_schedules_and_birthdates(birthdates, meeting_times, reference_date): def str_to_tz_offset(tz_str): sign = 1 if tz_str[0] == \'+\' else -1 hours, minutes = map(int, tz_str[1:].split(\':\')) return timezone(timedelta(hours=sign * hours, minutes=sign * minutes)) reference_date = datetime.strptime(reference_date, \'%Y-%m-%d\').replace(tzinfo=timezone.utc) results = {} for name, birthdate_str, tz_str in birthdates: birthdate = datetime.strptime(birthdate_str, \'%Y-%m-%d\') tz = str_to_tz_offset(tz_str) birthdate = birthdate.replace(tzinfo=tz).astimezone(timezone.utc) age = (reference_date - birthdate).days // 365 results[name] = {\'age\': age} for name, meeting_start_str, duration, tz_str in meeting_times: meeting_time = datetime.strptime(meeting_start_str, \'%H:%M\').time() tz = str_to_tz_offset(tz_str) meeting_start = datetime.combine(reference_date, meeting_time).replace(tzinfo=tz) meeting_start_utc = meeting_start.astimezone(timezone.utc) meeting_end_utc = meeting_start_utc + timedelta(minutes=duration) results[name][\'utc_meeting_start\'] = meeting_start_utc.isoformat() results[name][\'utc_meeting_end\'] = meeting_end_utc.isoformat() return results"},{"question":"# Question: Customizing Plot Styles in Seaborn Using the seaborn library, create a function `custom_plot` that: 1. Takes in two lists: `x` (categorical data) and `y` (numerical data). 2. Takes an additional parameter `style` to set the seaborn style for the plot (default to `\\"whitegrid\\"` if not provided). 3. Optionally takes a dictionary `style_params` to customize style parameters like `\\"grid.color\\"` and `\\"grid.linestyle\\"`. 4. Creates and displays a bar plot with the given data and styles. The function signature should look like this: ```python def custom_plot(x, y, style=\\"whitegrid\\", style_params=None): pass ``` # Input - `x`: List of strings (categorical data for the x-axis). - `y`: List of numbers (numerical data for the y-axis). - `style`: String indicating the seaborn style to apply (`\\"whitegrid\\"`, `\\"darkgrid\\"`, etc.). - `style_params`: Dictionary for additional style parameters (e.g., `{\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"}`). # Output - The function should display the bar plot with the specified styles applied. # Constraints - Both `x` and `y` lists should be of the same length and non-empty. - `style` should be one of seaborn\'s valid styles (`\\"whitegrid\\"`, `\\"darkgrid\\"`, `\\"dark\\"`, `\\"white\\"`, `\\"ticks\\"`). # Example Usage ```python x = [\\"A\\", \\"B\\", \\"C\\"] y = [1, 3, 2] style = \\"darkgrid\\" style_params = {\\"grid.color\\": \\".3\\", \\"grid.linestyle\\": \\"--\\"} custom_plot(x, y, style, style_params) ``` This will set the seaborn style to \\"darkgrid\\" and customize the grid color and linestyle before displaying a bar plot with the data provided.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_plot(x, y, style=\\"whitegrid\\", style_params=None): Creates and displays a bar plot with the given data and styles. Parameters: x (list): Categorical data for the x-axis. y (list): Numerical data for the y-axis. style (str): Seaborn style to apply. style_params (dict): Additional style parameters for customization. # Validate input lengths if len(x) != len(y): raise ValueError(\\"The length of x and y must be the same.\\") if style not in [\\"whitegrid\\", \\"darkgrid\\", \\"dark\\", \\"white\\", \\"ticks\\"]: raise ValueError(\\"Invalid style provided.\\") # Set the seaborn style sns.set_style(style) # Apply additional style parameters if provided if style_params is not None: sns.set_context(rc=style_params) # Create the bar plot sns.barplot(x=x, y=y) plt.show()"},{"question":"**Question:** # Exceptional File Processor You are required to write a Python function `process_file(filename: str) -> None` that processes a text file and performs several actions, including handling various possible exceptions that could arise during file operations. This function should demonstrate effective exception handling and cleanup actions. Function Requirements 1. **Reading the File:** - The function should attempt to open and read the entire file specified by `filename`. - If the file does not exist, raise a `FileNotFoundError`. - If there are issues with file permissions, raise an `OSError`. 2. **Processing the Content:** - The file content should contain lines with integers. If any line is not an integer, catch a `ValueError` and log an error message. - Ignore any empty lines in the file. - Sum all the valid integers found in the file. 3. **Reporting and Cleanup:** - If the file is read and processed without any issues, print the sum of integers. - Use a `finally` block to ensure the file is properly closed. - Use a custom exception to signal if the file is empty (i.e., contains no valid integers). Custom Exception Define a custom exception `EmptyFileError` which should be raised if the provided file contains no valid integers. Input: - `filename` (str): The name of the file to process. Output: - None. The function should only print results or error messages. Function Signature: ```python def process_file(filename: str) -> None: pass ``` # Example Usage ```python # Example content of \'example.txt\' 10 20 abc 30 # Expected output: 60 # Example content of \'empty.txt\' # Expected output: should raise EmptyFileError process_file(\'example.txt\') process_file(\'empty.txt\') ``` Constraints: - You are required to handle exceptions using `try`, `except`, `else`, and `finally` blocks. - You should define and use at least one custom exception. - You should perform appropriate clean-up actions using `finally` or `with` statements. # Notes: - Handle all exceptions gracefully and provide useful error messages where appropriate. - Ensure the file is always properly closed, regardless of whether an exception occurs or not.","solution":"class EmptyFileError(Exception): Custom exception raised when the file contains no valid integers. pass def process_file(filename: str) -> None: Processes a file, sums up integers, and handles exceptions. sum_of_integers = 0 try: with open(filename, \'r\') as file: lines = file.readlines() if not lines: raise EmptyFileError(\\"The file is empty.\\") valid_integers_found = False for line in lines: line = line.strip() if line: try: num = int(line) sum_of_integers += num valid_integers_found = True except ValueError: print(f\\"Error: \'{line}\' is not a valid integer.\\") if not valid_integers_found: raise EmptyFileError(\\"The file contains no valid integers.\\") print(f\\"The sum of integers in the file is: {sum_of_integers}\\") except FileNotFoundError: print(f\\"Error: The file {filename} does not exist.\\") except OSError: print(f\\"Error: An issue occurred while accessing the file {filename}.\\") except EmptyFileError as e: print(f\\"Error: {e}\\") finally: print(\\"File processing completed.\\")"},{"question":"# **Data Marshalling in Python: Function Implementation** Objective: You are required to implement a Python program that demonstrates the use of data marshalling by serializing and deserializing various data types, including integers and Python objects, using functions that mimic the C functions provided in the documentation. Task: 1. **Write a function** `write_data_to_file(data, file_path, version)`, which: - Accepts a Python object `data`, a string `file_path` indicating where to save the file, and an integer `version` specifying the marshalling version. - Uses the marshalling functions to save the data into the specified file. 2. **Write a function** `read_data_from_file(file_path, version)`, which: - Accepts a string `file_path` indicating from where to read the file and an integer `version` specifying the marshalling version. - Reads the marshalled data from the specified file and returns the Python object. 3. **Error Handling**: - Ensure that your implementation properly handles errors that might occur during the read/write process (e.g., file not found, read/write errors). 4. **Testing Functionality**: - Implement a test function `test_data_marshalling()` which: - Creates a variety of Python objects (e.g., integers, strings, lists, dictionaries). - Uses `write_data_to_file` to serialize those objects. - Uses `read_data_from_file` to deserialize the objects. - Verifies the integrity of the original and deserialized objects by comparing them. Constraints: - Use the standard Python `marshal` module for marshalling and unmarshalling data. - Only use Python\'s standard library for file operations. - Handle different object types (integers, strings, lists, dictionaries). Example: ```python import marshal def write_data_to_file(data, file_path, version): try: with open(file_path, \'wb\') as file: if isinstance(data, int): marshal.dump(data, file, version) else: marshal.dump(data, file, version) except Exception as e: print(f\\"Error writing data to file: {e}\\") def read_data_from_file(file_path, version): try: with open(file_path, \'rb\') as file: data = marshal.load(file) return data except Exception as e: print(f\\"Error reading data from file: {e}\\") def test_data_marshalling(): test_objects = [ 42, \\"Hello, World!\\", [1, 2, 3, 4, 5], {\'a\': 1, \'b\': 2, \'c\': 3} ] for i, obj in enumerate(test_objects): file_path = f\\"test_file_{i}.dat\\" write_data_to_file(obj, file_path, 2) read_obj = read_data_from_file(file_path, 2) assert obj == read_obj, f\\"Test failed for object: {obj}\\" test_data_marshalling() ``` Ensure your code handles exceptions gracefully, and add validations to improve robustness.","solution":"import marshal def write_data_to_file(data, file_path, version): Serializes the specified data into a file using the marshal module. Args: data: The Python object to be serialized. file_path: The path to the file where data will be written. version: The version of the marshalling format. try: with open(file_path, \'wb\') as file: marshal.dump(data, file, version) except Exception as e: print(f\\"Error writing data to file: {e}\\") def read_data_from_file(file_path, version): Deserializes data from the specified file using the marshal module. Args: file_path: The path to the file from which data will be read. version: The version of the marshalling format (not used in reading). Returns: The deserialized Python object. try: with open(file_path, \'rb\') as file: data = marshal.load(file) return data except Exception as e: print(f\\"Error reading data from file: {e}\\") return None"},{"question":"# PLS Regression for Predicting House Prices You are given a dataset containing various features of houses (`X`) and their corresponding prices (`Y`). Your task is to implement a PLS regression model to predict house prices using the Scikit-learn `cross_decomposition` module. You will be required to: 1. Preprocess the data (normalize features). 2. Implement the PLS regression model to fit `X` and `Y`. 3. Transform the data using the fitted model. 4. Predict house prices for new data. 5. Evaluate the performance of your model using Mean Squared Error (MSE). Here are the steps to follow: 1. **Data Preprocessing**: - Normalize the features of `X` using StandardScaler from `sklearn.preprocessing`. 2. **Model Implementation**: - Use `PLSRegression` from `sklearn.cross_decomposition` to fit the model on the training dataset. Use `n_components=2`. 3. **Data Transformation**: - Transform the datasets `X` and `Y` using the fitted model. 4. **Prediction**: - Predict the targets `Y` for new test data `X_test`. 5. **Evaluation**: - Compute the Mean Squared Error (MSE) of the predicted values against the actual house prices in the test set. # Implementation ```python import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error # Data Loading (example data, replace with actual data) # X_train, X_test, Y_train, Y_test should be numpy arrays X_train = np.array([[...], [...], ...]) # shape (n_samples, n_features) Y_train = np.array([...]) # shape (n_samples,) X_test = np.array([[...], [...], ...]) # shape (n_samples, n_features) Y_test = np.array([...]) # shape (n_samples,) # Step 1: Normalize the Features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 2: Fit PLSRegression Model pls = PLSRegression(n_components=2) pls.fit(X_train_scaled, Y_train) # Step 3: Transform the Data X_train_transformed = pls.transform(X_train_scaled) X_test_transformed = pls.transform(X_test_scaled) # Step 4: Predict House Prices for Test Data Y_pred = pls.predict(X_test_scaled) # Step 5: Evaluate the Model mse = mean_squared_error(Y_test, Y_pred) print(f\'Mean Squared Error: {mse:.4f}\') ``` # Input - `X_train`, `Y_train`: Training features and targets (numpy arrays). - `X_test`, `Y_test`: Testing features and targets (numpy arrays). # Output - Print the Mean Squared Error (MSE) of the prediction. # Constraints 1. Ensure the data has more rows than columns for the features matrix `X`. 2. Handle any exceptions or errors gracefully during the implementation. 3. Explain each step and function used with appropriate comments in the code. # Performance Requirement - The implementation should efficiently handle large datasets with several thousand samples and features. - Aim for an MSE as low as possible. **Note:** This task evaluates your ability to use Scikit-learn\'s `cross_decomposition` module, understand PLS regression, and your overall data preprocessing and model evaluation skills.","solution":"import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error def pls_regression(X_train, Y_train, X_test, Y_test): PLS Regression for Predicting House Prices Parameters: X_train (numpy array): Training features Y_train (numpy array): Training targets X_test (numpy array): Testing features Y_test (numpy array): Testing targets Returns: float: Mean Squared Error (MSE) of the prediction # Step 1: Normalize the Features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 2: Fit PLSRegression Model pls = PLSRegression(n_components=2) pls.fit(X_train_scaled, Y_train) # Step 3: Transform the Data X_train_transformed = pls.transform(X_train_scaled) X_test_transformed = pls.transform(X_test_scaled) # Step 4: Predict House Prices for Test Data Y_pred = pls.predict(X_test_scaled) # Step 5: Evaluate the Model mse = mean_squared_error(Y_test, Y_pred) return mse"},{"question":"PyTorch and TorchScript Compatibility **Objective:** The goal of this assessment is to evaluate your understanding of PyTorch and TorchScript, particularly how to handle constructs that may not be directly supported when converting PyTorch code to TorchScript. **Problem Statement:** You are tasked with implementing a custom neural network model in PyTorch that can be converted to TorchScript. However, you must avoid using constructs that are listed as unsupported or have diverging behavior in TorchScript. Specifically, implement a PyTorch model and a function to convert it to TorchScript format that performs the following operations: 1. Initializes a linear layer followed by a ReLU activation. 2. Uses tensor initialization techniques that are compatible with TorchScript. 3. Demonstrates how to handle a tensor operation that requires special handling due to divergent schema (e.g., creating a tensor without the `requires_grad` argument). **Requirements:** 1. **Model Class:** - Create a class `SimpleNet` that inherits from `torch.nn.Module`. - The class should contain one linear layer followed by a ReLU activation. - The forward method should perform a matrix multiplication followed by the ReLU activation. 2. **Initialization:** - Initialize the weights of the linear layer using `torch.nn.init.kaiming_normal_`. - Make sure the initialization is compatible with TorchScript. 3. **Tensor Operations:** - Create a function `create_custom_tensor` that constructs a tensor without using the `requires_grad` argument using a compatible function (e.g., `torch.ones`). 4. **TorchScript Conversion:** - Write a function `convert_to_torchscript` that takes an instance of `SimpleNet` and converts it to TorchScript using `torch.jit.script`. **Input Format:** No specific input format. You will define the model within the script. **Output Format:** The final output should be a serialized TorchScript model file saved with the name `simple_net_scripted.pt`. **Constraints:** - Do not use any of the functions or modules listed as unsupported by TorchScript in the provided documentation. - Ensure all tensor creation operations within the functions and methods do not use the `requires_grad` argument unless using `torch.tensor`. **Evaluation Criteria:** - Correct implementation of `SimpleNet` with appropriate layers and initialization. - Successful creation of a tensor using an appropriate function without the `requires_grad` argument. - Successful conversion of the model to TorchScript and saving the serialized model. - Code should be clear, well-documented, and follow best practices. **Example Code Outline:** ```python import torch import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear = nn.Linear(10, 5) torch.nn.init.kaiming_normal_(self.linear.weight) def forward(self, x): x = self.linear(x) return torch.relu(x) def create_custom_tensor(size): return torch.ones(size) def convert_to_torchscript(model): scripted_model = torch.jit.script(model) scripted_model.save(\'simple_net_scripted.pt\') # Example usage model = SimpleNet() tensor = create_custom_tensor((2, 10)) output = model(tensor) convert_to_torchscript(model) ``` Provide the complete implementation based on the outline and ensure it meets all the requirements.","solution":"import torch import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear = nn.Linear(10, 5) torch.nn.init.kaiming_normal_(self.linear.weight) def forward(self, x): x = self.linear(x) return torch.relu(x) def create_custom_tensor(size): Creates a tensor with the given size filled with ones. This approach ensures compatibility with TorchScript. return torch.ones(size) def convert_to_torchscript(model): Converts the given model to TorchScript and saves it as \'simple_net_scripted.pt\'. scripted_model = torch.jit.script(model) scripted_model.save(\'simple_net_scripted.pt\') return scripted_model # Example usage: model = SimpleNet() tensor = create_custom_tensor((2, 10)) output = model(tensor) scripted_model = convert_to_torchscript(model)"},{"question":"# Boolean Object Manipulation Objective Write a Python function that converts elements of an input list into Boolean objects based on their truthiness and then manipulates these Boolean objects according to specific rules. Problem Description Write a function `process_bools(input_list)` that takes a list of various elements (integers, strings, lists, etc.) as input and returns a new list of Boolean objects based on the following rules: 1. Convert each element of the input list to its Boolean equivalent. 2. If the Boolean equivalent of an element is `True`, append `Py_True` to the result list. 3. If the Boolean equivalent of an element is `False`, append `Py_False` to the result list. 4. Ensure to handle the reference count properly when appending `Py_True` or `Py_False`. The `process_bools` function should mimic the functionality using Python constructs due to the limitations of direct implementation of reference counting in high-level Python code. Function Signature ```python def process_bools(input_list: list) -> list: pass ``` Input - A list `input_list` containing various elements. The elements can be of any type. Output - A list of Boolean objects `Py_True` or `Py_False` based on the truthiness of elements of the input list. Constraints - Each element should be evaluated for its truthiness. - The function should work efficiently even for large input lists. Example ```python input_list = [0, 1, \\"\\", \\"hello\\", [], [1, 2, 3], None, True, False] result = process_bools(input_list) # Expected Output: [Py_False, Py_True, Py_False, Py_True, Py_False, Py_True, Py_False, Py_True, Py_False] ``` Notes You do not need to handle reference counting explicitly as it is managed by the Python interpreter. Just ensure to use correct Boolean evaluations.","solution":"def process_bools(input_list): Convert elements of input_list to their Boolean equivalents and append either \'Py_True\' or \'Py_False\' to the result list based on the truthiness of each element. result_list = [] for element in input_list: if bool(element): result_list.append(\'Py_True\') else: result_list.append(\'Py_False\') return result_list"},{"question":"You are tasked with creating a Python function to analyze log files from a server. Each log file entry contains a timestamp, log level, and a message. Using regular expressions, your function needs to extract specific types of entries and summarize some statistics. Input: - A string containing the log file data. - A string representing the log level to filter (e.g., \\"ERROR\\", \\"INFO\\", \\"DEBUG\\"). Output: - A list of tuples, each containing the timestamp and message of the filtered log entries. - The number of occurrences of each log level in the log file. Specifications: 1. The log file entries follow the format: `[<TIMESTAMP>] <LOG_LEVEL>: <MESSAGE>`. 2. Each entry is on a new line. 3. Timestamps use the format `YYYY-MM-DD HH:MM:SS`. 4. Log levels are always in uppercase and can include \\"ERROR\\", \\"INFO\\", \\"DEBUG\\", \\"WARNING\\", \\"CRITICAL\\". 5. Your function should handle cases where the input log level does not exist in the log file. Constraints: - You may assume the log file string is well-formed and respects the specified format. Performance: Ensure your implementation can effectively handle large log strings (up to 10MB). Function Signature: ```python def analyze_log_file(log_data: str, log_level: str) -> Tuple[List[Tuple[str, str]], Dict[str, int]]: pass ``` Example: ```python log_data = [2023-04-01 12:30:45] INFO: Server started [2023-04-01 12:31:12] ERROR: Failed to connect to the database [2023-04-01 12:32:33] DEBUG: Retrying connection [2023-04-01 12:33:01] INFO: Connection established [2023-04-01 12:34:43] WARNING: High memory usage [2023-04-01 12:35:21] ERROR: Lost connection analyze_log_file(log_data, \\"ERROR\\") ``` Expected Output: ```python ( [ (\\"2023-04-01 12:31:12\\", \\"Failed to connect to the database\\"), (\\"2023-04-01 12:35:21\\", \\"Lost connection\\") ], { \\"INFO\\": 2, \\"ERROR\\": 2, \\"DEBUG\\": 1, \\"WARNING\\": 1 } ) ``` Hints: - Use `re.findall` to extract log entries based on the specified format. - Use capturing groups to separately extract timestamps, log levels, and messages. - You might find `re.compile` and `re.finditer` useful for efficient regex operations.","solution":"import re from typing import List, Tuple, Dict def analyze_log_file(log_data: str, log_level: str) -> Tuple[List[Tuple[str, str]], Dict[str, int]]: Analyzes a log file to extract entries of a specific log level and count occurrences of each log level. Args: - log_data: A string containing the log file data. - log_level: The log level to filter (e.g., \\"ERROR\\", \\"INFO\\", \\"DEBUG\\"). Returns: - A tuple where: - The first element is a list of tuples containing the timestamp and message of the filtered log entries. - The second element is a dictionary with the count of occurrences of each log level. # Define regex pattern for log entries pattern = re.compile(r\'[(d{4}-d{2}-d{2} d{2}:d{2}:d{2})] (ERROR|INFO|DEBUG|WARNING|CRITICAL): (.+)\') # Find all matches matches = pattern.findall(log_data) # Initialize result structures filtered_entries = [] log_level_counts = {\\"ERROR\\": 0, \\"INFO\\": 0, \\"DEBUG\\": 0, \\"WARNING\\": 0, \\"CRITICAL\\": 0} # Process matches for timestamp, level, message in matches: # Increment log level count if level in log_level_counts: log_level_counts[level] += 1 # Add to filtered entries if the log level matches the input log level if level == log_level: filtered_entries.append((timestamp, message)) return filtered_entries, log_level_counts"},{"question":"You have been tasked with creating a Python extension module that exposes various list operations leveraging the sequence protocol functions described in the provided documentation. You need to implement the following function: **Function: concat_repeat** ```python def concat_repeat(sequence1, sequence2, n): Concatenate two sequences and repeat the result n times. Args: - sequence1: A sequence object (list or tuple). - sequence2: A sequence object (list or tuple). - n: An integer specifying the number of times to repeat the concatenated result. Returns: - A sequence (list or tuple) which is the result of concatenating sequence1 and sequence2, repeated n times. If the input sequences are lists, the result should be a list. If the input sequences are tuples, the result should be a tuple. Raises: - ValueError: If either sequence1 or sequence2 is not a sequence (list or tuple), or if n is not a positive integer. pass ``` # Constraints: 1. `sequence1` and `sequence2` will have at most 10^5 elements. 2. `n` will be in the range [1, 1000]. 3. You should use the sequence protocol functions provided in the documentation wherever possible. 4. Handle exceptions appropriately and efficiently. # Example: ```python # Example usage: result = concat_repeat([1, 2], [3, 4], 3) print(result) # Output: [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4] result = concat_repeat((1, 2), (3, 4), 2) print(result) # Output: (1, 2, 3, 4, 1, 2, 3, 4) ``` # Performance: Ensure your implementation is efficient in both time and space complexity. Given the constraints, the total operations should be manageable within the sequence protocol limits. # Additional Notes: 1. Review the sequence protocol functions described in the documentation and use functions such as `PySequence_Concat`, `PySequence_Repeat`, and others as appropriate. 2. Ensure the return type matches the input type (if inputs are lists, return a list; if inputs are tuples, return a tuple).","solution":"def concat_repeat(sequence1, sequence2, n): Concatenate two sequences and repeat the result n times. Args: - sequence1: A sequence object (list or tuple). - sequence2: A sequence object (list or tuple). - n: An integer specifying the number of times to repeat the concatenated result. Returns: - A sequence (list or tuple) which is the result of concatenating sequence1 and sequence2, repeated n times. If the input sequences are lists, the result should be a list. If the input sequences are tuples, the result should be a tuple. Raises: - ValueError: If either sequence1 or sequence2 is not a sequence (list or tuple), or if n is not a positive integer. if not isinstance(sequence1, (list, tuple)): raise ValueError(\\"sequence1 must be a list or tuple\\") if not isinstance(sequence2, (list, tuple)): raise ValueError(\\"sequence2 must be a list or tuple\\") if not isinstance(n, int) or n <= 0: raise ValueError(\\"n must be a positive integer\\") concatenated_seq = sequence1 + sequence2 result = concatenated_seq * n if isinstance(sequence1, list): return list(result) else: return tuple(result)"},{"question":"**Question: Implement a Unit Test for a Simple Application Using `unittest` and `unittest.mock` in Python** # Objective Your task is to implement a Python function and write corresponding unit tests using the `unittest` framework. You will also use `unittest.mock` for simulating certain parts of the functionality. This challenge will assess your ability to write clean code, tests, and utilize Python\'s testing libraries effectively. # Function to Implement Implement a simple booking system for a library. The booking system should have the following functionalities: 1. **Add a Book**: Add a new book to the collection. 2. **Remove a Book**: Remove a book from the collection. 3. **Check Availability**: Check if a book is available. 4. **Book a Book**: Book a book for a user. 5. **Return a Book**: Return a booked book. Here\'s the class definition to be implemented: ```python class LibraryBookingSystem: def __init__(self): self.books = [] self.booked_books = {} def add_book(self, book): Adds a new book to the collection. self.books.append(book) def remove_book(self, book): Removes a book from the collection. if book in self.books: self.books.remove(book) def check_availability(self, book): Checks if a book is available in the collection. return book in self.books and book not in self.booked_books def book_book(self, book, user): Books a book for a user if available. if self.check_availability(book): self.booked_books[book] = user return True return False def return_book(self, book): Returns a booked book. if book in self.booked_books: del self.booked_books[book] ``` # Unit Tests Write a test suite using `unittest` and `unittest.mock` to validate the above functionalities. Your test cases should cover: 1. Adding a book. 2. Removing a book. 3. Checking the availability of a book. 4. Booking a book. 5. Returning a booked book. Include at least one test case using `unittest.mock` to simulate the booking system behavior, such as a user interaction or booking confirmation step. # Example Test Suite Structure ```python import unittest from unittest.mock import MagicMock from your_module import LibraryBookingSystem class TestLibraryBookingSystem(unittest.TestCase): def setUp(self): self.library = LibraryBookingSystem() def test_add_book(self): self.library.add_book(\'1984\') self.assertIn(\'1984\', self.library.books) def test_remove_book(self): self.library.add_book(\'1984\') self.library.remove_book(\'1984\') self.assertNotIn(\'1984\', self.library.books) def test_check_availability(self): self.library.add_book(\'1984\') self.assertTrue(self.library.check_availability(\'1984\')) self.library.book_book(\'1984\', \'user1\') self.assertFalse(self.library.check_availability(\'1984\')) def test_book_book(self): self.library.add_book(\'1984\') result = self.library.book_book(\'1984\', \'user1\') self.assertTrue(result) self.assertIn(\'1984\', self.library.booked_books) self.assertEqual(self.library.booked_books[\'1984\'], \'user1\') def test_return_book(self): self.library.add_book(\'1984\') self.library.book_book(\'1984\', \'user1\') self.library.return_book(\'1984\') self.assertNotIn(\'1984\', self.library.booked_books) def test_mock_booking_system(self): self.library.add_book = MagicMock(return_value=None) self.library.add_book(\'Mocked Book\') self.library.add_book.assert_called_once_with(\'Mocked Book\') if __name__ == \'__main__\': unittest.main() ``` # Constraints - You must use the `unittest` library for writing your tests. - You must utilize `unittest.mock` to mock at least one method or interaction. # Deliverables Submit a Python file containing the `LibraryBookingSystem` class and the complete set of unit tests.","solution":"class LibraryBookingSystem: def __init__(self): self.books = [] self.booked_books = {} def add_book(self, book): Adds a new book to the collection. self.books.append(book) def remove_book(self, book): Removes a book from the collection. if book in self.books: self.books.remove(book) def check_availability(self, book): Checks if a book is available in the collection. return book in self.books and book not in self.booked_books def book_book(self, book, user): Books a book for a user if available. if self.check_availability(book): self.booked_books[book] = user return True return False def return_book(self, book): Returns a booked book. if book in self.booked_books: del self.booked_books[book]"},{"question":"You are tasked with creating a utility function to gather and present detailed information about the current Python environment using the `sysconfig` module. Your function will output a summary that includes specific configuration variables, installation paths, the current installation scheme, and platform-specific details. Function Signature ```python def get_python_environment_info() -> dict: pass ``` Input - No input arguments Output - A dictionary with the following structure: ```python { \\"platform\\": str, # The string identifying the current platform \\"python_version\\": str, # The MAJOR.MINOR Python version number \\"default_scheme\\": str, # The name of the default installation scheme \\"installation_paths\\": dict, # Dictionary containing all installation paths (expanded) \\"config_vars\\": dict # Dictionary containing specific configuration variables } ``` Requirements 1. **Platform Information**: Retrieve the current platform using `sysconfig.get_platform()`. 2. **Python Version**: Retrieve the MAJOR.MINOR Python version using `sysconfig.get_python_version()`. 3. **Default Scheme**: Retrieve the name of the default installation scheme using `sysconfig.get_default_scheme()`. 4. **Installation Paths**: Retrieve and include all installation paths using `sysconfig.get_paths()`. Expand the paths. 5. **Configuration Variables**: Retrieve and include specific configuration variables (`\'AR\'`, `\'CXX\'`, `\'LIBDIR\'`) using `sysconfig.get_config_vars()`. Constraints - You must use the `sysconfig` module to interact and retrieve the required information. - The information should be returned in the specified dictionary format. - If a configuration variable is not found, its value should be `None`. Example ```python { \\"platform\\": \\"linux-x86_64\\", \\"python_version\\": \\"3.10\\", \\"default_scheme\\": \\"posix_prefix\\", \\"installation_paths\\": { \\"stdlib\\": \\"/usr/local/lib/python3.10\\", \\"platstdlib\\": \\"/usr/local/lib/python3.10\\", \\"purelib\\": \\"/usr/local/lib/python3.10/site-packages\\", \\"platlib\\": \\"/usr/local/lib/python3.10/site-packages\\", \\"include\\": \\"/usr/local/include/python3.10\\", \\"scripts\\": \\"/usr/local/bin\\", \\"data\\": \\"/usr/local\\" }, \\"config_vars\\": { \\"AR\\": \\"ar\\", \\"CXX\\": \\"g++\\", \\"LIBDIR\\": \\"/usr/local/lib\\" } } ``` Implement the `get_python_environment_info` function to meet the requirements and support the given example.","solution":"import sysconfig def get_python_environment_info() -> dict: Retrieves detailed information about the current Python environment. info = { \\"platform\\": sysconfig.get_platform(), \\"python_version\\": sysconfig.get_python_version(), \\"default_scheme\\": sysconfig.get_default_scheme(), \\"installation_paths\\": sysconfig.get_paths(), \\"config_vars\\": {key: sysconfig.get_config_var(key) for key in [\'AR\', \'CXX\', \'LIBDIR\']} } return info"},{"question":"# Python Coding Assessment Question Objective: To assess your understanding of file handling, pattern matching, and script execution in Python. Problem Statement: You are provided with a source directory containing various types of files. Your task is to write a Python function `manage_source_files` that will process this directory based on a set of given commands and patterns. The function should categorize files to either include or exclude from the distribution based on the following commands: - `include pat1 pat2 ...` - `exclude pat1 pat2 ...` - `recursive-include dir pat1 pat2 ...` - `recursive-exclude dir pat1 pat2 ...` - `global-include pat1 pat2 ...` - `global-exclude pat1 pat2 ...` - `prune dir` - `graft dir` Input: 1. `source_dir` (str): Path to the source directory containing various files. 2. `commands` (List[str]): A list of commands for managing file inclusion and exclusion. Each command is a string. Output: - A dictionary with two keys: `\\"included_files\\"` and `\\"excluded_files\\"`. - `\\"included_files\\"` contains a list of files to be included in the distribution. - `\\"excluded_files\\"` contains a list of files to be excluded from the distribution. Constraints: - The function should properly handle nested directories. - Patterns will follow Unix-style \\"glob\\" patterns: `*`, `?`, `[range]`. - Performance should be optimized to handle directories with a large number of files. Example: ```python def manage_source_files(source_dir: str, commands: List[str]) -> Dict[str, List[str]]: pass # Example usage: source_dir = \\"/path/to/source\\" commands = [ \\"include *.py README.md\\", \\"exclude *.tmp\\", \\"recursive-include subdir *.txt *.md\\", \\"recursive-exclude subdir *.log\\", \\"global-include *.cfg\\", \\"global-exclude *.bak\\", \\"prune build\\", \\"graft assets\\" ] result = manage_source_files(source_dir, commands) print(\\"Included files:\\", result[\\"included_files\\"]) print(\\"Excluded files:\\", result[\\"excluded_files\\"]) ``` Expected output structure: ``` { \\"included_files\\": [\\"file_list1\\", \\"file_list2\\", ...], \\"excluded_files\\": [\\"file_list8\\", \\"file_list9\\", ...] } ``` **Notes:** - Ensure to respect the command precedence: later commands should override earlier ones when deciding inclusion/exclusion. - `prune dir` should exclude all files under the specified `dir`. - `graft dir` should include all files under the specified `dir`. Write the function implementation and include relevant test cases to demonstrate its functionality.","solution":"import os import fnmatch from typing import List, Dict def manage_source_files(source_dir: str, commands: List[str]) -> Dict[str, List[str]]: included_files = [] excluded_files = [] def apply_patterns(directory, patterns, include=True): for root, _, files in os.walk(directory): for pattern in patterns: matched_files = fnmatch.filter(files, pattern) for file in matched_files: full_path = os.path.join(root, file) if include: if full_path not in included_files: included_files.append(full_path) if full_path in excluded_files: excluded_files.remove(full_path) else: if full_path not in excluded_files: excluded_files.append(full_path) if full_path in included_files: included_files.remove(full_path) def prune_dir(directory): for root, dirs, files in os.walk(directory, topdown=True): for name in files: full_path = os.path.join(root, name) if full_path not in excluded_files: excluded_files.append(full_path) if full_path in included_files: included_files.remove(full_path) dirs[:] = [] # don\'t recurse into more subdirectories for command in commands: parts = command.split() cmd = parts[0] if cmd == \\"include\\": apply_patterns(source_dir, parts[1:], include=True) elif cmd == \\"exclude\\": apply_patterns(source_dir, parts[1:], include=False) elif cmd == \\"recursive-include\\": subdir = os.path.join(source_dir, parts[1]) apply_patterns(subdir, parts[2:], include=True) elif cmd == \\"recursive-exclude\\": subdir = os.path.join(source_dir, parts[1]) apply_patterns(subdir, parts[2:], include=False) elif cmd == \\"global-include\\": apply_patterns(source_dir, parts[1:], include=True) elif cmd == \\"global-exclude\\": apply_patterns(source_dir, parts[1:], include=False) elif cmd == \\"prune\\": prune_dir(os.path.join(source_dir, parts[1])) elif cmd == \\"graft\\": apply_patterns(os.path.join(source_dir, parts[1]), [\\"*\\"], include=True) return { \\"included_files\\": sorted(included_files), \\"excluded_files\\": sorted(excluded_files) }"},{"question":"# HTTP Status Codes Analysis You are tasked with creating a function that analyzes HTTP status codes and returns formatted information about them. Function Signature ```python def analyze_http_status(codes: list) -> list: ``` Input - `codes` (list): A list of integers representing HTTP status codes. Output - A list of strings, each string providing detailed information about each HTTP status code in the following format: ``` \\"HTTP Status: <CODE>, Name: <NAME>, Description: <DESCRIPTION>\\" ``` Constraints 1. Only valid HTTP status codes as per the `http.HTTPStatus` enum are allowed in the input list. 2. The input list can have a maximum of 100 status codes. 3. You should handle duplicate codes by listing them once in the output. Example ```python from http import HTTPStatus def analyze_http_status(codes): # Your implementation here # Test cases print(analyze_http_status([200, 404, 418])) # Output: [ # \\"HTTP Status: 200, Name: OK, Description: Request fulfilled, document follows\\", # \\"HTTP Status: 404, Name: NOT_FOUND, Description: Requested resource could not be found\\", # \\"HTTP Status: 418, Name: IM_A_TEAPOT, Description: I\'m a teapot\\" # ] print(analyze_http_status([100, 200, 500, 200])) # Output: [ # \\"HTTP Status: 100, Name: CONTINUE, Description: Continue with the request\\", # \\"HTTP Status: 200, Name: OK, Description: Request fulfilled, document follows\\", # \\"HTTP Status: 500, Name: INTERNAL_SERVER_ERROR, Description: Server encountered an unexpected condition\\" # ] ``` Implement the `analyze_http_status` function to fulfill the requirements, using the provided `http.HTTPStatus` enum. Notes - You can assume that the imported `http.HTTPStatus` is available for use. - Ensure that the output list only contains unique status codes, even if duplicates are present in the input list.","solution":"from http import HTTPStatus def analyze_http_status(codes: list) -> list: Analyzes HTTP status codes and returns formatted information about them. Parameters: codes (list): A list of integers representing HTTP status codes. Returns: list: A list of strings, each string providing detailed information about each HTTP status code. seen = set() result = [] for code in codes: if code in seen: continue if code in HTTPStatus.__members__.values(): seen.add(code) status = HTTPStatus(code) result.append(f\\"HTTP Status: {code}, Name: {status.name}, Description: {status.description}\\") return result"},{"question":"**Objective**: Assess the ability to work with Python\'s \\"webbrowser\\" module to create a flexible tool for managing and opening URLs in different web browsers. Task You are required to implement a Python script that: 1. Registers a custom browser under the name \\"my-custom-browser\\" using a dummy browser command. 2. Provides a command-line interface to: - Open a specified URL in the default browser. - Open a specified URL in the \\"my-custom-browser\\" browser. - List all currently registered browsers. Constraints 1. You should simulate the execution of the \\"my-custom-browser\\" using the `webbrowser` module\'s registration mechanism. 2. Assume the \\"my-custom-browser\\" command is `dummy-browser-command %s`. 3. You must handle exceptions that occur when attempting to open URLs and provide meaningful error messages. 4. Your script should work on both Windows and Unix-based platforms. Input and Output - **Command-Line Options**: - `-o <URL>` : Opens the URL in the default browser. - `-m <URL>` : Opens the URL in \\"my-custom-browser\\". - `-l` : Lists all registered browsers. - **Expected Outputs**: - URLs should be opened as specified by the command options. - The list of registered browsers should be printed to the console. Example Usages 1. Open a URL in the default browser: ``` python webbrowser_tool.py -o https://www.example.com ``` 2. Open a URL in the \\"my-custom-browser\\": ``` python webbrowser_tool.py -m https://www.example.com ``` 3. List all registered browsers: ``` python webbrowser_tool.py -l ``` Implementation Details Implement the following functions: - **register_custom_browser()**: Registers the \\"my-custom-browser\\" with a dummy command. - **open_in_default_browser(url)**: Opens a given URL in the default browser using `webbrowser.open`. - **open_in_custom_browser(url)**: Opens a given URL in the custom registered browser using `webbrowser.get` and the controller. - **list_registered_browsers()**: List all registered browsers. - **main()**: Handles command-line arguments and invokes the appropriate functions. **Note**: Ensure you handle cases where URLs might be incorrectly formatted or where the browser fails to open them. ```python import webbrowser import sys def register_custom_browser(): webbrowser.register(\'my-custom-browser\', None, webbrowser.BackgroundBrowser(\'dummy-browser-command %s\')) def open_in_default_browser(url): try: webbrowser.open(url) print(f\\"Opened {url} in the default browser.\\") except webbrowser.Error as e: print(f\\"Failed to open {url} in the default browser: {e}\\") def open_in_custom_browser(url): try: browser = webbrowser.get(\'my-custom-browser\') browser.open(url) print(f\\"Opened {url} in \'my-custom-browser\'.\\") except webbrowser.Error as e: print(f\\"Failed to open {url} in \'my-custom-browser\': {e}\\") def list_registered_browsers(): browsers = webbrowser._browsers.keys() print(\\"Registered browsers:\\") for browser in browsers: print(browser) def main(): if len(sys.argv) < 2: print(\\"Usage: python webbrowser_tool.py [-o URL | -m URL | -l]\\") return register_custom_browser() option = sys.argv[1] if option == \'-o\' and len(sys.argv) == 3: url = sys.argv[2] open_in_default_browser(url) elif option == \'-m\' and len(sys.argv) == 3: url = sys.argv[2] open_in_custom_browser(url) elif option == \'-l\': list_registered_browsers() else: print(\\"Usage: python webbrowser_tool.py [-o URL | -m URL | -l]\\") if __name__ == \\"__main__\\": main() ```","solution":"import webbrowser import sys def register_custom_browser(): webbrowser.register(\'my-custom-browser\', None, webbrowser.BackgroundBrowser(\'dummy-browser-command %s\')) def open_in_default_browser(url): try: webbrowser.open(url) print(f\\"Opened {url} in the default browser.\\") except webbrowser.Error as e: print(f\\"Failed to open {url} in the default browser: {e}\\") def open_in_custom_browser(url): try: browser = webbrowser.get(\'my-custom-browser\') browser.open(url) print(f\\"Opened {url} in \'my-custom-browser\'.\\") except webbrowser.Error as e: print(f\\"Failed to open {url} in \'my-custom-browser\': {e}\\") def list_registered_browsers(): browsers = webbrowser._browsers.keys() print(\\"Registered browsers:\\") for browser in browsers: print(browser) def main(): if len(sys.argv) < 2: print(\\"Usage: python webbrowser_tool.py [-o URL | -m URL | -l]\\") return register_custom_browser() option = sys.argv[1] if option == \'-o\' and len(sys.argv) == 3: url = sys.argv[2] open_in_default_browser(url) elif option == \'-m\' and len(sys.argv) == 3: url = sys.argv[2] open_in_custom_browser(url) elif option == \'-l\': list_registered_browsers() else: print(\\"Usage: python webbrowser_tool.py [-o URL | -m URL | -l]\\") if __name__ == \\"__main__\\": main()"},{"question":"Documented Function Implementation with Doctests **Objective:** Create a Python function to solve a specific problem, document it with examples of its usage and expected outputs using doctests, and ensure that the examples are validated correctly by the `doctest` module. **Problem Statement:** Implement a Python function `is_palindrome(s: str) -> bool` that checks whether a given string `s` is a palindrome. A palindrome is a word, phrase, number, or other sequences of characters that read the same forward and backward (ignoring spaces, capitalization, and punctuation). Document the function with detailed docstring examples demonstrating: 1. Basic palindrome checks. 2. Edge cases such as empty strings or strings with only spaces. 3. Strings with mixed casing and punctuation. Ensure these examples are verified using the `doctest` module. **Requirements:** 1. **Function Definition:** - **Input:** A single string `s` (0 <= len(s) <= 10^4). - **Output:** A boolean value `True` if the input string is a palindrome, `False` otherwise. 2. **Docstring:** - Include at least five examples with a mix of positive and negative cases. - Ensure different character scenarios (e.g., upper and lower case, spaces, punctuation). 3. **Constraints:** - Ignore spaces, capitalization, and punctuation when checking for palindromes. 4. **Testing:** - Use the `doctest` module to run tests on the examples provided in the docstring. - Ensure the doctests are self-contained and pass by simply running the script. **Example Implementation Structure:** ```python def is_palindrome(s: str) -> bool: Check if the given string s is a palindrome. A palindrome is a word, phrase, number, or other sequences of characters that read the same forward and backward (ignoring spaces, capitalization, and punctuation). >>> is_palindrome(\\"A man, a plan, a canal, Panama\\") True >>> is_palindrome(\\"No lemon, no melon\\") True >>> is_palindrome(\\"Hello, World!\\") False >>> is_palindrome(\\"\\") True >>> is_palindrome(\\"Racecar\\") True >>> is_palindrome(\\"Able was I ere I saw Elba\\") True >>> is_palindrome(\\"Madam, in Eden, I\'m Adam\\") True >>> is_palindrome(\\"This isn\'t a palindrome\\") False # Your implementation goes here if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` **Submission:** - Implement the function. - Add the required docstring with examples. - Validate the examples with `doctest`. Good luck!","solution":"import re def is_palindrome(s: str) -> bool: Check if the given string s is a palindrome. A palindrome is a word, phrase, number, or other sequences of characters that read the same forward and backward (ignoring spaces, capitalization, and punctuation). >>> is_palindrome(\\"A man, a plan, a canal, Panama\\") True >>> is_palindrome(\\"No lemon, no melon\\") True >>> is_palindrome(\\"Hello, World!\\") False >>> is_palindrome(\\"\\") True >>> is_palindrome(\\"Racecar\\") True >>> is_palindrome(\\"Able was I ere I saw Elba\\") True >>> is_palindrome(\\"Madam, in Eden, I\'m Adam\\") True >>> is_palindrome(\\"This isn\'t a palindrome\\") False # Remove non-alphanumeric characters and lowercase the string cleaned = re.sub(r\'[^A-Za-z0-9]\', \'\', s).lower() # Check if cleaned string is equal to its reverse return cleaned == cleaned[::-1] if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"# Advanced Email MIME Construction You are required to create an email MIME structure using the `email.mime` module in Python 3.10. This structure should support the addition of multipart and individual MIME objects. Your task involves constructing an email with the following properties: 1. The email should be a **multipart** type, with the major subtype `mixed`. 2. The email should contain: - A **text** part with HTML content. - An **image** part containing an image with automatic mime-type detection. - An **application** part containing an arbitrary binary file, encoded in Base64. 3. The email should include all necessary headers for each part and the main message. # Requirements: 1. **Function Definition**: ```python def create_mime_email(html_content: str, image_bytes: bytes, binary_data: bytes, image_subtype: str = None) -> email.mime.multipart.MIMEMultipart: ``` - `html_content`: A string containing the HTML content to be included in the email. - `image_bytes`: A bytes object containing the image data. - `binary_data`: A bytes object containing arbitrary binary data. - `image_subtype`: An optional string specifying the image subtype (e.g., \'jpeg\'). If not provided, it should be inferred automatically. 2. **Expected Output**: - Return an `email.mime.multipart.MIMEMultipart` object representing the constructed email. 3. **Constraints**: - Use appropriate MIME classes from the `email.mime` module. - The text part should be added as `text/html`. - Use the default Base64 encoding for binary and image data. - Ensure proper error handling if the supplied image data cannot be automatically determined and no subtype is given. - If the minor type for the image could not be guessed and `_subtype` was not given, raise a `TypeError`. # Example Usage: ```python html_content = \\"<h1>This is an HTML Email</h1>\\" image_bytes = open(\'example_image.png\', \'rb\').read() binary_data = open(\'example_document.pdf\', \'rb\').read() try: mime_email = create_mime_email(html_content, image_bytes, binary_data) except Exception as e: print(f\\"Error: {str(e)}\\") print(mime_email.as_string()) ``` The goal of this question is to assess your ability to work with the `email.mime` module to create complex MIME email structures, and handle various data types and potential exceptions in MIME processing. Ensure your implementation adheres to the guidelines provided.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication from email import encoders import imghdr def create_mime_email(html_content: str, image_bytes: bytes, binary_data: bytes, image_subtype: str = None) -> MIMEMultipart: # Create the root message msg = MIMEMultipart(\'mixed\') msg[\'Subject\'] = \'Test Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' # Add text part text_part = MIMEText(html_content, \'html\') msg.attach(text_part) # Determine image subtype if not provided if not image_subtype: image_subtype = imghdr.what(None, h=image_bytes) if not image_subtype: raise TypeError(\\"Cannot determine the image subtype. Please specify the image subtype manually.\\") # Add image part image_part = MIMEImage(image_bytes, _subtype=image_subtype) image_part.add_header(\'Content-Disposition\', \'attachment\', filename=\'image.{}\'.format(image_subtype)) msg.attach(image_part) # Add binary data part binary_part = MIMEApplication(binary_data) binary_part.add_header(\'Content-Disposition\', \'attachment\', filename=\'binary_data.bin\') encoders.encode_base64(binary_part) msg.attach(binary_part) return msg"},{"question":"# Customized Interactive Console Implementation You are provided with the `code` module that allows for creating Python\'s read-eval-print loops. Your task is to implement a customized interactive console class that adds extra functionalities to the existing `InteractiveConsole` class from the `code` module. Task 1. **Class Inheritance:** - Create a class `CustomInteractiveConsole` that inherits from `code.InteractiveConsole`. 2. **Enhanced Input Prompting:** - Override the `raw_input` method to add a custom prefix to the input prompt. The prefix should be `[Custom Prompt] -> `. 3. **Command Logging:** - Enhance the console to log each command executed into a list named `command_log`. This includes both complete and incomplete commands, but excludes syntax errors. 4. **Restricted Commands:** - Implement a feature in the `CustomInteractiveConsole` to block certain commands that you specify. For this task, block the `exit()` and `quit()` commands. If a blocked command is entered, the console should raise a `RuntimeError` with the message \\"Blocked command: {command}\\". 5. **Undo Last Command:** - Add a method named `undo_last_command` which removes the last executed command from the `command_log` and also removes the last command from the input buffer if it hasn\'t been executed yet. Constraints - You must use the provided methods and structure of the original `InteractiveConsole` class without using any external libraries for extending functionalities. - Ensure that your class handles all exceptions gracefully and prints appropriate messages using the `write` method. - Your implementation should not use global variables. - The input prompt and logging functionality should work seamlessly when using the `interact` method of your `CustomInteractiveConsole` class. Example Given the following usage: ```python import code custom_console = CustomInteractiveConsole() custom_console.interact(banner=\\"Welcome to Custom Interactive Console\\") ``` Expected behavior (inputs and outputs shown for the interactive console): ```plaintext Welcome to Custom Interactive Console [Custom Prompt] -> 2 + 2 4 [Custom Prompt] -> x = 1 [Custom Prompt] -> exit() Blocked command: exit() [Custom Prompt] -> x 1 ``` Additionally: ```python print(custom_console.command_log) ``` Should output: ```plaintext [\'2 + 2\', \'x = 1\', \'x\'] ``` # Submission: Submit your implementation of the `CustomInteractiveConsole` class in a Python file.","solution":"import code class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals, filename) self.command_log = [] self.buffer = [] def raw_input(self, prompt=\\"\\"): return input(f\\"[Custom Prompt] -> \\") def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): if source.strip() in [\\"exit()\\", \\"quit()\\"]: self.write(\\"Blocked command: {}n\\".format(source.strip())) raise RuntimeError(\\"Blocked command: {}\\".format(source.strip())) self.command_log.append(source.strip()) return super().runsource(source, filename, symbol) def undo_last_command(self): if self.command_log: self.command_log.pop() if self.buffer: self.buffer.pop()"},{"question":"**Coding Assessment Question:** # Problem: Managing and Manipulating ZIP Archives You are tasked with creating a Python function to manage ZIP archives. This function will: 1. Create a new ZIP archive from a provided list of files. 2. Extract specific files from an existing ZIP archive. 3. List the contents of a ZIP archive with detailed information. # Requirements: Implement a function `manage_zip_archive(zip_path: str, mode: str, file_paths: list = None, extract_files: list = None) -> list` that performs the following operations based on the provided `mode`: - `\'create\'`: Create a new ZIP archive at `zip_path` containing the files specified in `file_paths`. - `\'extract\'`: Extract the files specified in `extract_files` from the ZIP archive located at `zip_path` into the current working directory. Return a list of the paths of the extracted files. - `\'list\'`: List all the files in the ZIP archive located at `zip_path` along with their sizes and the date/time of last modification. Return this list as tuples in the format (filename, size, date_time). # Input: - `zip_path` (str): The path of the ZIP archive. - `mode` (str): The mode of operation (`\'create\'`, `\'extract\'`, `\'list\'`). - `file_paths` (list): A list of file paths to include in the ZIP archive when `mode` is `\'create\'`. Default is `None`. - `extract_files` (list): A list of file names within the ZIP archive to extract when `mode` is `\'extract\'`. Default is `None`. # Output: - When `mode` is `\'extract\'`: A list of extracted file paths. - When `mode` is `\'list\'`: A list of tuples containing file information `(filename, size, date_time)`. # Constraints: - The function should handle errors gracefully and indicate any issues through appropriate exceptions or error messages. - Assume all file paths provided are valid and accessible. - If the specified `mode` does not match any valid operation, raise a `ValueError`. # Example Usage: ```python # Example usage of manage_zip_archive function # Creating a ZIP archive manage_zip_archive(\\"example.zip\\", \\"create\\", file_paths=[\\"file1.txt\\", \\"file2.txt\\"]) # Listing contents of the ZIP archive contents = manage_zip_archive(\\"example.zip\\", \\"list\\") print(contents) # Output: [(\\"file1.txt\\", 123, (2023, 10, 3, 12, 0, 0)), (\\"file2.txt\\", 456, (2023, 10, 3, 12, 0, 0))] # Extracting specific files from the ZIP archive extracted_files = manage_zip_archive(\\"example.zip\\", \\"extract\\", extract_files=[\\"file1.txt\\"]) print(extracted_files) # Output: [\\"./file1.txt\\"] ``` # Note: - Remember to use `with` statements for handling file operations to ensure that files are properly closed after their operations are complete. - Handle any potential exceptions, such as `FileNotFoundError` or `zipfile.BadZipFile`, appropriately. Good luck!","solution":"import zipfile import os from datetime import datetime def manage_zip_archive(zip_path: str, mode: str, file_paths: list = None, extract_files: list = None) -> list: if mode == \'create\': if not file_paths: raise ValueError(\\"file_paths must be provided when mode is \'create\'\\") with zipfile.ZipFile(zip_path, \'w\') as zipf: for file in file_paths: zipf.write(file, os.path.basename(file)) return [] elif mode == \'extract\': if not extract_files: raise ValueError(\\"extract_files must be provided when mode is \'extract\'\\") extracted_files = [] with zipfile.ZipFile(zip_path, \'r\') as zipf: for file in extract_files: zipf.extract(file, \'.\') extracted_files.append(os.path.join(\'.\', file)) return extracted_files elif mode == \'list\': with zipfile.ZipFile(zip_path, \'r\') as zipf: file_info = [] for info in zipf.infolist(): file_info.append((info.filename, info.file_size, info.date_time)) return file_info else: raise ValueError(f\\"Unsupported mode: {mode}\\")"},{"question":"**Problem Statement** You are tasked with working with JSON data that includes complex numbers. The standard JSON encoder and decoder in Python do not support complex numbers out of the box. You need to implement a custom solution to handle complex numbers using the `json` module. # Requirements 1. **Custom Encoder:** Create a custom JSON encoder that can serialize complex numbers. In the JSON representation, complex numbers should be serialized as a dictionary with the format: ```json { \\"__complex__\\": true, \\"real\\": <real part>, \\"imag\\": <imaginary part> } ``` 2. **Custom Decoder:** Create a custom JSON decoder that can deserialize the JSON representation of complex numbers back into Python complex number objects. 3. **Handling Regular Data:** Ensure that the custom encoder and decoder can still handle the usual JSON data types (string, int, float, list, dict, etc.) alongside complex numbers. # Function Specifications Implement the following two functions: 1. `encode_complex_obj(obj: Any) -> str` - **Input:** A Python object (`obj`) which may include complex numbers. - **Output:** JSON string representation of the object with complex numbers supported. 2. `decode_complex_obj(json_data: str) -> Any` - **Input:** A JSON string (`json_data`) which may include complex number representations. - **Output:** A Python object with complex numbers reconstituted. # Constraints - The implementation should use `json.JSONEncoder` and `json.JSONDecoder` classes for encoding and decoding. - Ensure the solution does not exceed the default recursion limit in Python. - It should handle nested data structures containing complex numbers. # Example ```python data = { \\"name\\": \\"Alice\\", \\"value\\": 10, \\"complex_num\\": 3 + 4j } # Encoding json_str = encode_complex_obj(data) print(json_str) # Expected Output (dict order may vary): # \'{\\"name\\": \\"Alice\\", \\"value\\": 10, \\"complex_num\\": {\\"__complex__\\": true, \\"real\\": 3.0, \\"imag\\": 4.0}}\' # Decoding decoded_data = decode_complex_obj(json_str) print(decoded_data) # Expected Output: # {\'name\': \'Alice\', \'value\': 10, \'complex_num\': (3+4j)} ``` # Implementation Make sure to implement the `ComplexEncoder` class that inherits from `json.JSONEncoder` and override the `default()` method to handle complex numbers. Similarly, implement a function that will act as the `object_hook` to deserialize complex number dictionaries back into Python complex numbers.","solution":"import json class ComplexEncoder(json.JSONEncoder): Custom encoder for complex numbers. def default(self, obj): if isinstance(obj, complex): return {\\"__complex__\\": True, \\"real\\": obj.real, \\"imag\\": obj.imag} return super().default(obj) def as_complex(dct): Custom decoder function for complex numbers. if \\"__complex__\\" in dct: return complex(dct[\\"real\\"], dct[\\"imag\\"]) return dct def encode_complex_obj(obj): Encodes a Python object with potential complex numbers into a JSON string. return json.dumps(obj, cls=ComplexEncoder) def decode_complex_obj(json_data): Decodes a JSON string into a Python object, interpreting complex number formats. return json.loads(json_data, object_hook=as_complex)"},{"question":"# Question You are tasked with creating a WSGI application that serves a simple web page. Your application should meet the following requirements: 1. **Function Implementation**: - Implement a function named `greeting_app` that takes two arguments: - `environ`: A dictionary containing CGI-style environment variables. - `start_response`: A callable to start the HTTP response. - The function should do the following: - Determine the request method and path from the `environ` dictionary. - If the request method is `GET` and the path is `/greet`, respond with a status of `200 OK` and a greeting message \\"Hello, WSGI!\\". - If the request path is anything else, respond with a status of `404 Not Found` and a message \\"Page not found\\". 2. **Header Manipulation**: - Use the `wsgiref.headers.Headers` class to handle the response headers. Ensure the `Content-type` header is set to `text/plain; charset=utf-8`. 3. **Validation**: - Wrap your WSGI application with the `wsgiref.validate.validator` to ensure compliance with the WSGI specification. 4. **Server Setup**: - Use `wsgiref.simple_server.make_server` to create a simple server that runs your WSGI application. It should listen on port 8000. 5. **Testing**: - Set up a testing environment using `wsgiref.util.setup_testing_defaults` to verify your application. # Constraints - You must use the `wsgiref` module\'s functionalities as specified. - Ensure your application is compliant with the WSGI specification by validating it using `wsgiref.validate`. # Input Your `greeting_app` does not take any direct input from the user, but processes HTTP GET requests. # Output The output should be the HTTP responses described in the implementation details above. # Example Running the server and accessing `http://localhost:8000/greet` should display: ``` Hello, WSGI! ``` Accessing any other path should display: ``` Page not found ``` ```python from wsgiref.simple_server import make_server from wsgiref.validate import validator from wsgiref.headers import Headers from wsgiref.util import setup_testing_defaults def greeting_app(environ, start_response): # Set up testing defaults for the environment setup_testing_defaults(environ) # Extract path and method path = environ[\'PATH_INFO\'] method = environ[\'REQUEST_METHOD\'] # Determine response based on path and method if method == \'GET\' and path == \'/greet\': status = \'200 OK\' response_body = \'Hello, WSGI!\' else: status = \'404 Not Found\' response_body = \'Page not found\' # Set headers headers = Headers([(\'Content-type\', \'text/plain; charset=utf-8\')]) start_response(status, headers.items()) return [response_body.encode(\'utf-8\')] # Validate the WSGI application validated_app = validator(greeting_app) # Create the server with make_server(\'\', 8000, validated_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` **Note**: When running your code, make sure to test it via a web browser or a tool like `curl` to verify the correct responses based on different paths.","solution":"from wsgiref.simple_server import make_server from wsgiref.validate import validator from wsgiref.headers import Headers from wsgiref.util import setup_testing_defaults def greeting_app(environ, start_response): A simple WSGI application to greet the user or show a 404 message. # Set up testing defaults for the environment setup_testing_defaults(environ) # Extract path and method path = environ[\'PATH_INFO\'] method = environ[\'REQUEST_METHOD\'] # Determine response based on path and method if method == \'GET\' and path == \'/greet\': status = \'200 OK\' response_body = \'Hello, WSGI!\' else: status = \'404 Not Found\' response_body = \'Page not found\' # Set headers headers = Headers([(\'Content-type\', \'text/plain; charset=utf-8\')]) start_response(status, headers.items()) return [response_body.encode(\'utf-8\')] # Validate the WSGI application validated_app = validator(greeting_app) # Create the server if __name__ == \\"__main__\\": with make_server(\'\', 8000, validated_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"# TypeSafe Container In this task, you will create a type-safe container that can store and retrieve items while ensuring type safety using Python\'s `typing` module. Requirements: 1. **Define a generic type container `TypeSafeContainer`**: - The container should use a `typing.Generic` base class to accept any type `T`. - The container should have an internal list to store items of type `T`. 2. **Implement the following methods in the container**: - `add_item(item: T) -> None`: Adds an item to the container. - `get_item(index: int) -> T`: Retrieves an item at a specified index. - `get_all_items() -> list[T]`: Returns a list of all items in the container. 3. **Type hints**: - Ensure all methods are annotated with appropriate type hints using `typing`. - Use `List` and `Generic` from the `typing` module. 4. **Type constraint**: - Enforce that instances of the container can only store items of a single, consistent type. Input and Output Formats: - `add_item(item: T) -> None`: Adds an item to the container. No return value. - `get_item(index: int) -> T`: Retrieves an item at a specified index and returns it. - `get_all_items() -> list[T]`: Returns a list of all items in the container. Example: ```python from typing import TypeVar, Generic, List T = TypeVar(\'T\') class TypeSafeContainer(Generic[T]): def __init__(self) -> None: self._items: List[T] = [] def add_item(self, item: T) -> None: self._items.append(item) def get_item(self, index: int) -> T: return self._items[index] def get_all_items(self) -> List[T]: return self._items # Example usage container = TypeSafeContainer[int]() container.add_item(1) container.add_item(2) print(container.get_item(0)) # Output: 1 print(container.get_all_items()) # Output: [1, 2] ``` In this example, `TypeSafeContainer` is a generic container that only accepts and stores items of a specified type (in this case, `int`). The methods are type-hinted to ensure type safety. Create a similar implementation for the `TypeSafeContainer` class and ensure that type safety is maintained throughout.","solution":"from typing import TypeVar, Generic, List T = TypeVar(\'T\') class TypeSafeContainer(Generic[T]): def __init__(self) -> None: self._items: List[T] = [] def add_item(self, item: T) -> None: self._items.append(item) def get_item(self, index: int) -> T: return self._items[index] def get_all_items(self) -> List[T]: return self._items"},{"question":"Objective You are required to demonstrate your understanding of creating and customizing color palettes using Seaborn\'s `husl_palette` function. Problem Statement Write a function `create_custom_palette` that generates a color palette based on the given parameters and applies it to a seaborn plot. The function should: 1. Create an `n`-color HUSL palette with specified lightness and saturation. 2. Use the generated palette to color a distribution plot. 3. Return both the palette and the plot. Function Signature ```python def create_custom_palette(n: int, l: float, s: float) -> tuple: pass ``` Parameters - `n` (int): The number of colors to include in the palette. - `l` (float): The lightness of the colors in the palette (range from 0.0 to 1.0). - `s` (float): The saturation of the colors in the palette (range from 0.0 to 1.0). Constraints - The function should handle invalid inputs gracefully, such as: - `n` should be a positive integer greater than zero. - `l` and `s` should be float values between 0.0 and 1.0 inclusive. - Use an assert statement to validate `n`, `l`, and `s` constraints. Example ```python palette, plot = create_custom_palette(n=5, l=0.5, s=0.7) # The function should return a list of 5 colors and display the distribution plot colored using the custom palette. ``` Notes - You can use sample data to demonstrate the plot. For example, seaborn\'s built-in `titanic` dataset or any other dataset of your choice that is suitable for a distribution plot. - Ensure your function includes docstrings explaining the purpose of the function and its parameters. - Comment your code where necessary to improve readability.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette(n: int, l: float, s: float) -> tuple: Creates an n-color HUSL palette with specified lightness and saturation and applies it to a seaborn plot. Parameters: n (int): The number of colors to include in the palette. l (float): The lightness of the colors in the palette (range from 0.0 to 1.0). s (float): The saturation of the colors in the palette (range from 0.0 to 1.0). Returns: tuple: A tuple containing the palette list and the plot object. # Validate inputs assert isinstance(n, int) and n > 0, \\"n should be a positive integer greater than zero.\\" assert isinstance(l, float) and 0.0 <= l <= 1.0, \\"l should be a float between 0.0 and 1.0 inclusive.\\" assert isinstance(s, float) and 0.0 <= s <= 1.0, \\"s should be a float between 0.0 and 1.0 inclusive.\\" # Create the palette palette = sns.husl_palette(n, l=l, s=s) # Create a distribution plot with the palette plot = sns.displot(data=sns.load_dataset(\\"titanic\\"), x=\\"age\\", palette=palette) return palette, plot"},{"question":"Objective: Demonstrate comprehension of Python\'s `asyncio.Future` class and related asynchronous programming concepts. Implement a function to perform multiple asynchronous operations using `Future` objects and handle their results or exceptions. Problem Statement: You are required to implement an asynchronous function `perform_operations` that asynchronously performs a list of given operations, each represented by a coroutine. The function should do the following: 1. Create a `Future` object for each coroutine in the operations list. 2. Schedule each coroutine using the event loop and set the result of the corresponding `Future` object when the coroutine completes. 3. If a coroutine raises an exception, set the exception of the corresponding `Future` object. 4. Await all the `Future` objects and aggregate their results or exceptions in a list in the same order as the input list of coroutines. 5. Return this list of results or exceptions. Function Specification: ```python import asyncio async def perform_operations(operations: list) -> list: Parameters: operations (list): A list of coroutine functions to be performed. Returns: list: A list containing the results or exceptions of the executed coroutines. results = [] # Your implementation goes here. return results # You can define additional helper functions if needed. ``` Example: ```python async def coro_success(val): await asyncio.sleep(1) return val async def coro_fail(): await asyncio.sleep(1) raise ValueError(\\"An error occurred\\") # Sample Usage # asyncio.run is used to run the main function if it\'s executed in the main scope. async def main(): operations = [ coro_success(10), coro_fail(), coro_success(20) ] results = await perform_operations(operations) print(results) # Example output: # [10, \\"An error occurred\\", 20] asyncio.run(main()) ``` Constraints: 1. At least one coroutine will always be provided in the operations list. 2. Ensure all futures are awaited, and the order of results in the final list matches the order of the input coroutines. 3. Handle exceptions properly to avoid stopping the execution of other coroutines. Evaluation Criteria: 1. Correctness: Does the code handle all coroutines and accumulate results/exceptions as specified? 2. Asynchronous Programming: Does the code efficiently use asyncio features like `Future`, event loops, and task scheduling? 3. Error Handling: Are exceptions being handled correctly without disrupting other operations?","solution":"import asyncio async def perform_operations(operations: list) -> list: Parameters: operations (list): A list of coroutine functions to be performed. Returns: list: A list containing the results or exceptions of the executed coroutines. futures = [asyncio.ensure_future(op) for op in operations] results = [] for future in futures: try: result = await future results.append(result) except Exception as e: results.append(e) return results # Example usage: # asyncio.run(main())"},{"question":"Problem Statement You are given a dataset containing survey responses collected from a group of participants on various aspects. Each participant\'s responses are represented in a DataFrame where certain columns represent categorical data. The objective is to manipulate and optimize this categorical data efficiently. Steps to follow: 1. **Loading Data** - Create a DataFrame `df` with the following data: ```python data = { \\"Participant\\": [101, 102, 103, 104, 105, 106], \\"Gender\\": [\\"Male\\", \\"Female\\", \\"Female\\", \\"Male\\", \\"Female\\", \\"Male\\"], \\"AgeGroup\\": [\\"18-25\\", \\"26-35\\", \\"18-25\\", \\"46-55\\", \\"36-45\\", \\"36-45\\"], \\"Satisfaction\\": [\\"Very Satisfied\\", \\"Satisfied\\", \\"Neutral\\", \\"Dissatisfied\\", \\"Neutral\\", \\"Very Satisfied\\"] } ``` 2. **Convert Columns to Categorical Data** - Convert the `Gender`, `AgeGroup`, and `Satisfaction` columns to categorical data types. - Ensure the `AgeGroup` is ordered. 3. **Manipulation and Operations** - Rename the levels of the `Satisfaction` column to: - \\"Very Satisfied\\" -> \\"VS\\" - \\"Satisfied\\" -> \\"S\\" - \\"Neutral\\" -> \\"N\\" - \\"Dissatisfied\\" -> \\"D\\" - Add a new category \\"Very Dissatisfied\\" to the `Satisfaction` levels. - Set the categories for the `AgeGroup` to [\\"18-25\\", \\"26-35\\", \\"36-45\\", \\"46-55\\"] maintaining the order. 4. **Missing Data Handling** - Simulate missing data by setting the satisfaction level of the participant with ID 105 to `np.nan`. - Fill all missing satisfaction levels with \\"Neutral\\". 5. **Memory Optimization** - Compare and print the memory usage of the DataFrame before and after converting relevant columns to categorical. 6. **Combine DataFrames** - Assume another survey data `data2` from new participants is: ```python data2 = { \\"DataFrame\\": { \\"Participant\\": [107, 108], \\"Gender\\": [\\"Female\\", \\"Male\\"], \\"AgeGroup\\": [\\"26-35\\", \\"26-35\\"], \\"Satisfaction\\": [\\"Satisfied\\", \\"Dissatisfied\\"] } } ``` - Load this new dataset and combine it with the original DataFrame ensuring the `Gender`, `AgeGroup`, and `Satisfaction` columns remain as categorical types. Input Format - Data for the new survey participants as mentioned above. Output Format - Print the final combined DataFrame. - Print the memory usage before and after categorical conversion as specified in step 5. Constraints - Ensure to handle potential datatype conversions gracefully. - Maintain the order and levels of categories throughout the operations. - Avoid any hard-coded values or assumptions beyond what is specified in the problem statement. Sample Output ```python Final DataFrame: Participant Gender AgeGroup Satisfaction 0 101 Male 18-25 VS 1 102 Female 26-35 S 2 103 Female 18-25 N 3 104 Male 46-55 D 4 105 Female 36-45 N 5 106 Male 36-45 VS 6 107 Female 26-35 S 7 108 Male 26-35 D Memory Usage Before Conversion: XXXX bytes Memory Usage After Conversion: YYYY bytes ``` Write functions to implement the operations as described, ensuring clear and efficient handling of categorical data with pandas.","solution":"import pandas as pd import numpy as np def create_dataframe(): data = { \\"Participant\\": [101, 102, 103, 104, 105, 106], \\"Gender\\": [\\"Male\\", \\"Female\\", \\"Female\\", \\"Male\\", \\"Female\\", \\"Male\\"], \\"AgeGroup\\": [\\"18-25\\", \\"26-35\\", \\"18-25\\", \\"46-55\\", \\"36-45\\", \\"36-45\\"], \\"Satisfaction\\": [\\"Very Satisfied\\", \\"Satisfied\\", \\"Neutral\\", \\"Dissatisfied\\", \\"Neutral\\", \\"Very Satisfied\\"] } df = pd.DataFrame(data) return df def convert_to_categorical(df): df[\'Gender\'] = df[\'Gender\'].astype(\'category\') df[\'AgeGroup\'] = pd.Categorical(df[\'AgeGroup\'], categories=[\\"18-25\\", \\"26-35\\", \\"36-45\\", \\"46-55\\"], ordered=True) df[\'Satisfaction\'] = df[\'Satisfaction\'].astype(\'category\') return df def manipulate_data(df): df[\'Satisfaction\'] = df[\'Satisfaction\'].cat.rename_categories({ \\"Very Satisfied\\": \\"VS\\", \\"Satisfied\\": \\"S\\", \\"Neutral\\": \\"N\\", \\"Dissatisfied\\": \\"D\\", }) df[\'Satisfaction\'] = df[\'Satisfaction\'].cat.add_categories(\\"Very Dissatisfied\\") return df def handle_missing_data(df): df.loc[df[\'Participant\'] == 105, \'Satisfaction\'] = np.nan df[\'Satisfaction\'] = df[\'Satisfaction\'].fillna(\'N\') return df def memory_optimization(df): memory_before = df.memory_usage(deep=True).sum() df_categorical = convert_to_categorical(df) memory_after = df_categorical.memory_usage(deep=True).sum() return df_categorical, memory_before, memory_after def combine_dataframes(df1, df2): df_combined = pd.concat([df1, df2], ignore_index=True) df_combined[\'Gender\'] = df_combined[\'Gender\'].astype(\'category\') df_combined[\'AgeGroup\'] = pd.Categorical(df_combined[\'AgeGroup\'], categories=[\\"18-25\\", \\"26-35\\", \\"36-45\\", \\"46-55\\"], ordered=True) df_combined[\'Satisfaction\'] = pd.Categorical(df_combined[\'Satisfaction\'], categories=[\\"VS\\", \\"S\\", \\"N\\", \\"D\\", \\"Very Dissatisfied\\"]) return df_combined if __name__ == \\"__main__\\": df = create_dataframe() df = convert_to_categorical(df) df = manipulate_data(df) df = handle_missing_data(df) df_categorical, memory_before, memory_after = memory_optimization(df) data2 = { \\"Participant\\": [107, 108], \\"Gender\\": [\\"Female\\", \\"Male\\"], \\"AgeGroup\\": [\\"26-35\\", \\"26-35\\"], \\"Satisfaction\\": [\\"Satisfied\\", \\"Dissatisfied\\"] } df2 = pd.DataFrame(data2) df_final = combine_dataframes(df_categorical, df2) print(\\"Final DataFrame:\\") print(df_final) print(\\"nMemory Usage Before Conversion: {} bytes\\".format(memory_before)) print(\\"Memory Usage After Conversion: {} bytes\\".format(memory_after))"},{"question":"Coding Assessment Question # Objective The goal is to assess your understanding of coroutines in Python, which are functions declared with the \\"async\\" keyword and managed using \\"await.\\" # Problem Statement You are required to implement a coroutine-based system for handling asynchronous tasks in a fictional task processing scenario. The system should allow scheduling, running, and monitoring tasks. # Function to Implement Implement a class `TaskManager` that includes the following methods: 1. **add_task(coro)**: Adds a coroutine to the list of pending tasks. 2. **run_all()**: Runs all the added coroutines asynchronously and waits for all of them to complete. It should return a list of the results of the tasks in the order they were added. 3. **get_status()**: Returns a dictionary with the statuses of all tasks (either \\"pending,\\" \\"running,\\" or \\"completed\\"). The keys should be the task indices (starting from 0). You may assume that the tasks can be either coroutines or normal functions that are wrapped to run as coroutines. # Input Format There are no direct inputs to implement the `TaskManager` class and its methods. The coroutines and tasks will be added and run by interacting with instances of this class. # Output Format - The `run_all` method should return a list of results. - The `get_status` method should return a dictionary mapping task indices to their statuses. # Constraints - The tasks added could take different amounts of time to complete. - You must use asynchronous programming primitives (`async`, `await`, and possibly `asyncio` module functions) to ensure non-blocking behavior. # Example ```python import asyncio async def task1(): await asyncio.sleep(1) return \\"task1 completed\\" async def task2(): await asyncio.sleep(2) return \\"task2 completed\\" def main(): manager = TaskManager() manager.add_task(task1()) manager.add_task(task2()) statuses_before = manager.get_status() print(statuses_before) # Output: {0: \'pending\', 1: \'pending\'} results = asyncio.run(manager.run_all()) print(results) # Output: [\'task1 completed\', \'task2 completed\'] statuses_after = manager.get_status() print(statuses_after) # Output: {0: \'completed\', 1: \'completed\'} if __name__ == \\"__main__\\": main() ```","solution":"import asyncio class TaskManager: def __init__(self): self.tasks = [] self.status = {} def add_task(self, coro): task_index = len(self.tasks) self.tasks.append(coro) self.status[task_index] = \'pending\' async def _run_task(self, index, coro): self.status[index] = \'running\' result = await coro self.status[index] = \'completed\' return result async def run_all(self): results = await asyncio.gather( *[self._run_task(i, task) for i, task in enumerate(self.tasks)] ) return results def get_status(self): return self.status"},{"question":"# Challenge: Implement a Compressed Data Archiver Objective Write a Python function that accepts a list of strings, compresses each string using different compression levels, and returns a dictionary containing the original and compressed data. Then, decompress the data and verify that it matches the original input. This will demonstrate your understanding of the `zlib` module\'s compression and decompression functionalities. Function Signature ```python def compress_and_verify_data(data_list: list[str]) -> dict: pass ``` Input - `data_list`: A list of strings where each string represents data that needs to be compressed. Output - A dictionary containing: - The original list of strings under the key `\\"original\\"`. - Lists of compressed data using compression levels 1, 5, and 9 under keys `\\"compressed_level_1\\"`, `\\"compressed_level_5\\"`, and `\\"compressed_level_9\\"`. - Decompressed data that should match the original data under keys `\\"decompressed_level_1\\"`, `\\"decompressed_level_5\\"`, and `\\"decompressed_level_9\\"`. Example ```python data_list = [\\"example string\\", \\"another example\\", \\"yet another example\\"] result = compress_and_verify_data(data_list) expected = { \\"original\\": data_list, \\"compressed_level_1\\": [b\\"...\\", b\\"...\\", b\\"...\\"], # Actual compressed bytes will vary \\"compressed_level_5\\": [b\\"...\\", b\\"...\\", b\\"...\\"], # Actual compressed bytes will vary \\"compressed_level_9\\": [b\\"...\\", b\\"...\\", b\\"...\\"], # Actual compressed bytes will vary \\"decompressed_level_1\\": [\\"example string\\", \\"another example\\", \\"yet another example\\"], \\"decompressed_level_5\\": [\\"example string\\", \\"another example\\", \\"yet another example\\"], \\"decompressed_level_9\\": [\\"example string\\", \\"another example\\", \\"yet another example\\"], } assert result == expected ``` Constraints - Each string in `data_list` has a length between 1 and 1000 characters. - The list `data_list` contains between 1 and 100 strings. Notes - Ensure that you handle any possible compression or decompression errors. - You may use the `zlib.compress` and `zlib.decompress` functions to work with compression and decompression. - Validate the integrity of the data by checking if decompressed data matches the original input.","solution":"import zlib def compress_and_verify_data(data_list): Compresses and decompresses a list of strings using zlib at different compression levels and returns a dictionary containing the original, compressed, and decompressed data. Parameters: data_list (list of str): List of strings to be compressed and decompressed. Returns: dict: A dictionary with the original list, compressed data at levels 1, 5, 9, and decompressed data that should match the original. compression_levels = [1, 5, 9] result = { \\"original\\": data_list, \\"compressed_level_1\\": [], \\"compressed_level_5\\": [], \\"compressed_level_9\\": [], \\"decompressed_level_1\\": [], \\"decompressed_level_5\\": [], \\"decompressed_level_9\\": [] } for level in compression_levels: for string in data_list: if level == 1: compressed = zlib.compress(string.encode(\'utf-8\'), level) result[\\"compressed_level_1\\"].append(compressed) decompressed = zlib.decompress(compressed).decode(\'utf-8\') result[\\"decompressed_level_1\\"].append(decompressed) elif level == 5: compressed = zlib.compress(string.encode(\'utf-8\'), level) result[\\"compressed_level_5\\"].append(compressed) decompressed = zlib.decompress(compressed).decode(\'utf-8\') result[\\"decompressed_level_5\\"].append(decompressed) elif level == 9: compressed = zlib.compress(string.encode(\'utf-8\'), level) result[\\"compressed_level_9\\"].append(compressed) decompressed = zlib.decompress(compressed).decode(\'utf-8\') result[\\"decompressed_level_9\\"].append(decompressed) return result"},{"question":"**Question: Manipulating Environment Variables in Python** Using the `os` module in Python, write a function `modify_and_execute(cmd: str, env_changes: dict) -> str` that modifies the environment variables, executes a shell command, and returns the output. # Input - `cmd`: A string representing the shell command to be executed. - `env_changes`: A dictionary where keys are the names of environment variables and values are the new values for these variables. # Output - The function should return the output of the executed shell command as a string. # Constraints - The function should first update the environment variables using `os.environ`. - Then, it should execute the command `cmd` using the `subprocess` module. - Ensure that the changes in `env_changes` do not affect the environment variables of the Python process after the function completes. # Example ```python import os import subprocess def modify_and_execute(cmd: str, env_changes: dict) -> str: # Your implementation here # Example usage: cmd = \\"printenv MY_VAR\\" env_changes = {\\"MY_VAR\\": \\"HelloWorld\\"} print(modify_and_execute(cmd, env_changes)) # Should output: HelloWorld ``` # Notes - Consider how to temporarily set environment variables within the function. - Think about the security implications of executing shell commands and handling environment variables.","solution":"import os import subprocess def modify_and_execute(cmd: str, env_changes: dict) -> str: Modifies the environment variables, executes a shell command, and returns the output. Parameters: cmd (str): The shell command to be executed. env_changes (dict): A dictionary with environment variable names as keys and their corresponding values. Returns: str: The output of the executed shell command. # Make a copy of the current environment variables env = os.environ.copy() # Update with the changes env.update(env_changes) # Execute the command with the modified environment result = subprocess.run(cmd, shell=True, env=env, capture_output=True, text=True) # Return the standard output of the command return result.stdout"},{"question":"**Objective**: Demonstrate your understanding of seaborn\'s plotting capabilities including histogram bin settings, normalization, faceting, and handling grouped data with advanced visualization techniques. **Instructions**: 1. Load the \\"penguins\\" dataset from seaborn. 2. Create a histogram to display the distribution of penguin \\"flipper_length_mm\\". 3. Customize the histogram to have 20 bins. 4. Normalize the histogram to show proportions instead of counts. 5. Facet the histogram by \\"species\\". 6. Normalize the histogram within each species group independently. 7. Visualize overlapping distributions using area plots. 8. Use stacks to represent a part-whole relationship for the sex of penguins within each species. **Requirements**: - Use seaborn\'s object interface for creating plots. - Provide appropriate labels and titles for the plots. - Ensure all steps are combined into a single, coherent visualization process. **Input**: You don\'t need to take any input other than loading the dataset since the dataset is a built-in seaborn dataset. **Output**: Your code should output the required plots. Example: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # 1. Create a histogram for flipper_length_mm p = so.Plot(penguins, \\"flipper_length_mm\\") # 2. Customize the histogram to have 20 bins p.add(so.Bars(), so.Hist(bins=20)) # 3. Normalize the histogram to show proportions p.add(so.Bars(), so.Hist(stat=\\"proportion\\")) # 4. Facet the histogram by species p = p.facet(\\"species\\") # 5. Normalize within each species group independently p.add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False)) # 6. Visualize using area plots with overlapping distributions p.add(so.Area(), so.Hist(stat=\\"proportion\\"), color=\\"species\\") # 7. Use stacks to represent part-whole relationship for the sex of penguins within each species p.add(so.Bars(), so.Hist(stat=\\"proportion\\"), so.Stack(), color=\\"sex\\") # Showing the plot - this line is implicit in a script/notebook environment p.show() ``` **Constraints**: - Ensure the solution is efficient and concise. - Utilize seaborn functionalities as much as possible without over-relying on manual pandas manipulations. **Notes**: - You are expected to create plots that are visually informative and correctly labeled. - Make sure your code is well-documented and easy to understand.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_flipper_distribution(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a FacetGrid to facet by species g = sns.FacetGrid(penguins, col=\\"species\\", hue=\\"species\\", margin_titles=True, sharex=True, sharey=True) # Map a histogram visualization to the FacetGrid g.map_dataframe(sns.histplot, x=\\"flipper_length_mm\\", bins=20, stat=\\"density\\", kde=False) # Create a subplot for the overall distribution of the flipper lengths, overlaid with area plots plt.figure(figsize=(10, 6)) sns.histplot(penguins, x=\\"flipper_length_mm\\", bins=20, hue=\\"species\\", kde=True, element=\\"step\\", stat=\\"density\\") # Use stacks to represent part-whole relationship for the sex of penguins within each species plt.figure(figsize=(10, 6)) sns.histplot(penguins, x=\\"flipper_length_mm\\", bins=20, hue=\\"species\\", multiple=\\"stack\\", stat=\\"density\\") plt.title(\\"Distribution of Flipper Length by Species and Sex\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Proportion\\") # Display all plots plt.show()"},{"question":"**File Organizer Utility** **Objective:** Create a Python utility that organizes files in a specified directory into subdirectories based on file extensions. **Task:** Implement a function `organize_files(directory_path: str) -> None` that rearranges files in the given `directory_path` into subdirectories. Each subdirectory will represent a file extension category (e.g., `.txt`, `.jpg`, etc.). **Requirements:** 1. If a file has no extension, move it to a subdirectory named `no_extension`. 2. If a subdirectory for a particular file extension already exists, move the file into that subdirectory. 3. If a subdirectory does not exist, create it before moving the file. 4. Maintain a log of all operations (creation of directories and movement of files) in a file named `log.txt` in the root of the given directory. 5. Skip directories within the given `directory_path`  only organize files. **Function Signature:** `def organize_files(directory_path: str) -> None:` **Input:** - `directory_path` (str): The path to the directory that needs to be organized. **Output:** - The function does not return anything but performs file and directory operations. **Constraints:** - `directory_path` is a valid directory path. - You have read/write permissions for the specified directory. - There may be nested directories, but they should not be organized. **Example:** Assume the directory `/example` has the following files: ``` /example/file1.txt /example/file2.jpg /example/file3 /example/file4.txt /example/subdirectory/file5.png ``` After running `organize_files(\\"/example\\")`, the directory structure should be: ``` /example/log.txt /example/no_extension/file3 /example/txt/file1.txt /example/txt/file4.txt /example/jpg/file2.jpg /example/subdirectory/file5.png ``` The `log.txt` file should contain entries similar to: ``` Created directory /example/no_extension/ Moved file /example/file3 to /example/no_extension/file3 Created directory /example/txt/ Moved file /example/file1.txt to /example/txt/file1.txt Moved file /example/file4.txt to /example/txt/file4.txt Created directory /example/jpg/ Moved file /example/file2.jpg to /example/jpg/file2.jpg ``` **Notes:** - Ensure to handle exceptions and edge cases, such as permission issues or non-existent directories. - Utilize appropriate modules and functionalities described in the provided documentation for efficient and Pythonic file operations.","solution":"import os import shutil def organize_files(directory_path: str) -> None: Organizes files in a specified directory into subdirectories based on file extensions. log_entries = [] # Iterate over items in the given directory for item in os.listdir(directory_path): item_path = os.path.join(directory_path, item) # Skip directories if os.path.isdir(item_path): continue # Determine file extension file_name, file_extension = os.path.splitext(item) file_extension = file_extension[1:] # Remove leading dot # Define destination directory if file_extension == \\"\\": dest_dir = os.path.join(directory_path, \'no_extension\') else: dest_dir = os.path.join(directory_path, file_extension) # Create destination directory if it does not exist if not os.path.exists(dest_dir): os.mkdir(dest_dir) log_entries.append(f\\"Created directory {dest_dir}/\\") # Move the file dest_path = os.path.join(dest_dir, item) shutil.move(item_path, dest_path) log_entries.append(f\\"Moved file {item_path} to {dest_path}\\") # Write log entries to log.txt log_path = os.path.join(directory_path, \'log.txt\') with open(log_path, \'w\') as log_file: for entry in log_entries: log_file.write(entry + \'n\')"},{"question":"# Python Coding Assessment **Objective**: Demonstrate the ability to dynamically create and manage classes and use special object types using the `types` module in Python. # Problem Statement: You are required to create a dynamic class using the `types.new_class` function. This class should meet the following specifications: 1. The class should be able to dynamically accept and store any number of attributes. 2. Include a method to return the type names of all attributes stored. 3. Use the `types.ModuleType` to create a custom module dynamically and add the newly created class to this module. 4. Implement a utility function `create_dynamic_class` to perform the above tasks. # Function Signature ```python def create_dynamic_class(class_name: str, attributes: dict) -> types.ModuleType: pass ``` # Input - `class_name`: A string representing the name of the dynamic class to be created. - `attributes`: A dictionary where the keys are attribute names and the values are their respective values to be initialized in the class. # Output - Return an instance of `types.ModuleType` containing the dynamically created class. # Example Usage ```python # Example input class_name = \'Person\' attributes = { \'name\': \'John Doe\', \'age\': 30, \'email\': \'john.doe@example.com\' } # Create dynamic class and custom module custom_module = create_dynamic_class(class_name, attributes) # Accessing the dynamically created class from the module PersonClass = getattr(custom_module, \'Person\') # Creating an instance of the dynamically created class person = PersonClass() # Accessing attributes print(person.name) # Output: John Doe print(person.age) # Output: 30 print(person.email) # Output: john.doe@example.com # Getting types of attributes print(person.get_attribute_types()) # Output: {\'name\': \'str\', \'age\': \'int\', \'email\': \'str\'} ``` # Constraints 1. The `create_dynamic_class` function should handle any attribute names and values provided in the dictionary. 2. The attribute values should have appropriate type hints. 3. The function should ensure the dynamically created class and its methods work correctly and return expected results. # Notes 1. Use the `types.new_class` function to create the class dynamically. 2. Use the `types.ModuleType` to create a module and attach the dynamically created class to it. 3. Ensure that the `get_attribute_types` method inside the dynamically created class returns a dictionary where keys are attribute names and values are their corresponding type names.","solution":"import types def create_dynamic_class(class_name: str, attributes: dict) -> types.ModuleType: # Define initializer method for the class def __init__(self, **kwargs): for key, value in kwargs.items(): setattr(self, key, value) # Define a method to get attribute types def get_attribute_types(self): return {key: type(value).__name__ for key, value in self.__dict__.items()} # Creating the dynamic class cls = types.new_class(class_name) cls.__init__ = __init__ cls.get_attribute_types = get_attribute_types # Initialize __dict__ for attributes in the dynamic class cls.__init__.__annotations__ = {key: type(value) for key, value in attributes.items()} # Create a custom module and add the dynamic class to it module = types.ModuleType(\'custom_module\') setattr(module, class_name, cls) # Return the module containing the dynamic class return module"},{"question":"**Objective:** To evaluate the student\'s understanding of data preprocessing using Scikit-Learn, focusing on standardization, handling categorical features, and applying custom transformations. Problem Statement: You are provided with a dataset that contains numerical as well as categorical features. Your task is to preprocess the dataset for better performance in machine learning models. The preprocessing steps should include: 1. **Standardizing numerical features**: Use `StandardScaler` to standardize all numerical features. 2. **Handling categorical features**: Encode categorical features using `OneHotEncoder`. 3. **Custom Transformation on Numerical Features**: Implement a custom transformation using `FunctionTransformer` to apply log transformation on specific numerical features. Dataset Description: Assume you have a dataset in the form of a pandas DataFrame `df` with the following columns: - `age` (numerical) - `salary` (numerical) - `city` (categorical) - `job` (categorical) Requirements: 1. Implement a function `preprocess_data(df: pd.DataFrame) -> pd.DataFrame` that takes in a pandas DataFrame and returns the preprocessed DataFrame. 2. Standardize the `age` and `salary` columns using `StandardScaler`. 3. Apply one-hot encoding to the `city` and `job` columns using `OneHotEncoder`. 4. Create a custom transformation using `FunctionTransformer` that applies the natural logarithm to the `salary` column, but only after ensuring that any zero or negative values are set to a small positive value to avoid errors during the log transformation. Input: - `df` (pandas DataFrame): Input DataFrame containing the columns `age`, `salary`, `city`, and `job`. Output: - Returns a pandas DataFrame with `age` and `salary` columns standardized, `city` and `job` columns one-hot encoded, and the `salary` column transformed using the logarithm. Constraints: - Ensure numerical features are standardized. - Handle the possibility of zero or negative values in the `salary` column before applying the log transformation. Function Signature ```python import pandas as pd from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline import numpy as np def preprocess_data(df: pd.DataFrame) -> pd.DataFrame: # Ensure that all numerical features (age and salary) are standardized # Create a custom transformer for the salary column to apply log transformation # Apply one-hot encoding to categorical features (city and job) # Combine all transformations using ColumnTransformer and Pipeline # Return the transformed DataFrame pass ``` *Example:* ```python # Example usage: data = { \'age\': [25, 32, 47, 51], \'salary\': [50000, 54000, 83000, -1000], \'city\': [\'New York\', \'Los Angeles\', \'Chicago\', \'New York\'], \'job\': [\'Engineer\', \'Artist\', \'Doctor\', \'Artist\'] } df = pd.DataFrame(data) print(preprocess_data(df)) # Expected Output: # transformed DataFrame with standardized `age` and `salary` (post-log transformation) columns # and one-hot encoded `city` and `job` columns ``` **Note:** - Make sure to handle edge cases for the log transformation. - The transformed DataFrame should have a clear and intuitive column naming convention.","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline import numpy as np def preprocess_data(df: pd.DataFrame) -> pd.DataFrame: # Define numerical and categorical columns numerical_features = [\'age\', \'salary\'] categorical_features = [\'city\', \'job\'] # Prepare custom transformer for log transforming the salary def log_transform(x): return np.log1p(np.where(x < 1, 1, x)) log_transformer = FunctionTransformer(log_transform, validate=True) # Create transformations for numerical features numerical_transformer = Pipeline(steps=[ (\'log\', log_transformer), (\'scaler\', StandardScaler()) ]) # Create transformation for categorical features categorical_transformer = OneHotEncoder(handle_unknown=\'ignore\') # Combine all transformations preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Apply transformations to the dataframe transformed_data = preprocessor.fit_transform(df) # Extract transformed feature names for numerical and categorical columns num_features_names = [\'age\', \'log_transformed_salary\'] cat_features_names = preprocessor.transformers_[1][1].get_feature_names_out(categorical_features) # Combine column names for the final DataFrame feature_names = np.concatenate([num_features_names, cat_features_names]) # Return the processed DataFrame return pd.DataFrame(transformed_data, columns=feature_names)"},{"question":"# Data Visualization with Custom Diverging Palettes Objective Write a Python function named `visualize_custom_palette` that creates a heatmap of a given dataset using a custom diverging palette. The function should allow for customization of the palette\'s parameters. Requirements 1. The function should generate a diverging palette with customizable parameters such as the starting color, ending color, center color, and additional options specified in the function `sns.diverging_palette`. 2. Plot a heatmap using the generated diverging palette with the given dataset. Input - Two integers, `start` and `end`, that represent the start and end hues for the diverging palette. - A string `center` to specify the center color (`\'light\'` or `\'dark\'`). - A boolean `as_cmap` to specify if the palette should be returned as a colormap. - A pandas DataFrame `data` that contains the numerical data to visualize. Output - The function should display a heatmap using the generated diverging palette. Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_custom_palette(start, end, center, as_cmap, data): Create and visualize a heatmap using a custom diverging palette. Parameters: start (int): Starting hue value for the diverging palette. end (int): Ending hue value for the diverging palette. center (str): Center color, \'light\' or \'dark\'. as_cmap (bool): Whether to return the palette as a colormap. data (pd.DataFrame): Dataset to visualize as a heatmap. Returns: None: Displays the heatmap. # Generate the custom diverging palette palette = sns.diverging_palette(start, end, center=center, as_cmap=as_cmap) # Create the heatmap sns.heatmap(data, cmap=palette) # Display the plot plt.show() # Sample usage df = pd.DataFrame({ \'A\': [1, 2, 3], \'B\': [4, 5, 6], \'C\': [7, 8, 9] }) visualize_custom_palette(240, 20, \'dark\', True, df) ``` Constraints - `start` and `end` must be valid hue values (0 to 360). - `center` should only be `\'light\'` or `\'dark\'`. - `as_cmap` should be either `True` or `False`. - `data` should be a numerical pandas DataFrame with no missing values. You can use the above example as a reference to develop your solution. Ensure you handle different scenarios by testing with various inputs and data sizes.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_custom_palette(start, end, center, as_cmap, data): Create and visualize a heatmap using a custom diverging palette. Parameters: start (int): Starting hue value for the diverging palette. end (int): Ending hue value for the diverging palette. center (str): Center color, \'light\' or \'dark\'. as_cmap (bool): Whether to return the palette as a colormap. data (pd.DataFrame): Dataset to visualize as a heatmap. Returns: None: Displays the heatmap. # Validate inputs if not (0 <= start <= 360 and 0 <= end <= 360): raise ValueError(\\"start and end must be between 0 and 360.\\") if center not in [\'light\', \'dark\']: raise ValueError(\\"center must be \'light\' or \'dark\'.\\") if not isinstance(as_cmap, bool): raise ValueError(\\"as_cmap must be a boolean.\\") if not isinstance(data, pd.DataFrame) or data.isnull().values.any(): raise ValueError(\\"data must be a pandas DataFrame with no missing values.\\") # Generate the custom diverging palette palette = sns.diverging_palette(start, end, center=center, as_cmap=as_cmap) # Create the heatmap sns.heatmap(data, cmap=palette) # Display the plot plt.show()"},{"question":"**XML Data Manipulation and Querying** **Problem Statement:** You are provided with an XML document that represents a simplified library system. Each book entry includes details such as title, author, year of publication, genre, and some have a list of reviews. Here is an example structure of the XML data: ```xml <library> <book id=\\"1\\"> <title>Python Programming</title> <author>John Doe</author> <year>2015</year> <genre>Programming</genre> <reviews> <review> <user>Alice</user> <rating>5</rating> <comment>Excellent book!</comment> </review> <review> <user>Bob</user> <rating>4</rating> <comment>Very informative.</comment> </review> </reviews> </book> <book id=\\"2\\"> <title>Data Science Essentials</title> <author>Jane Smith</author> <year>2018</year> <genre>Data Science</genre> </book> <!-- More book entries --> </library> ``` **Tasks:** 1. **Parse the XML Data:** Write a function `parse_xml(data: str) -> ET.Element` that takes an XML string `data` and returns the root `Element` of the parsed XML tree. 2. **Find Books by Author:** Write a function `find_books_by_author(root: ET.Element, author_name: str) -> List[str]` that takes the root `Element` and an author\'s name, and returns a list of book titles by that author. 3. **Add a New Book:** Write a function `add_new_book(root: ET.Element, book_info: dict) -> None` that takes the root `Element` and a dictionary `book_info` containing details of the new book, and adds this book as a new element in the library XML tree. ```python book_info = { \'id\': \'3\', \'title\': \'Machine Learning Basics\', \'author\': \'David Lee\', \'year\': \'2020\', \'genre\': \'Machine Learning\' } ``` 4. **Update Review Ratings:** Write a function `update_review_ratings(root: ET.Element, book_id: str, user: str, new_rating: int) -> bool` that takes the root `Element`, a book ID, a user name, and a new rating. It updates the rating for the specified user\'s review on the specified book. Returns `True` if the review was found and updated, `False` otherwise. 5. **Serialize Updated XML:** Write a function `serialize_xml(root: ET.Element) -> str` that takes the root `Element` of the modified XML tree and returns it as a pretty-printed XML string. **Constraints:** - All book IDs are unique strings. - Review ratings are integers between 1 and 5. - If multiple reviews by the same user exist for a book, update only the first one found. **Example Usage:** ```python # Example XML data xml_data = <library> <book id=\\"1\\"> <title>Python Programming</title> <author>John Doe</author> <year>2015</year> <genre>Programming</genre> <reviews> <review> <user>Alice</user> <rating>5</rating> <comment>Excellent book!</comment> </review> <review> <user>Bob</user> <rating>4</rating> <comment>Very informative.</comment> </review> </reviews> </book> <book id=\\"2\\"> <title>Data Science Essentials</title> <author>Jane Smith</author> <year>2018</year> <genre>Data Science</genre> </book> </library> import xml.etree.ElementTree as ET # Parse XML root = parse_xml(xml_data) # Find books by author print(find_books_by_author(root, \'John Doe\')) # Add a new book new_book_info = { \'id\': \'3\', \'title\': \'Machine Learning Basics\', \'author\': \'David Lee\', \'year\': \'2020\', \'genre\': \'Machine Learning\' } add_new_book(root, new_book_info) # Update review ratings print(update_review_ratings(root, \'1\', \'Alice\', 4)) # Serialize updated XML print(serialize_xml(root)) ``` Expected output: ```python [\'Python Programming\'] True <library> <book id=\\"1\\"> <title>Python Programming</title> <author>John Doe</author> <year>2015</year> <genre>Programming</genre> <reviews> <review> <user>Alice</user> <rating>4</rating> <comment>Excellent book!</comment> </review> <review> <user>Bob</user> <rating>4</rating> <comment>Very informative.</comment> </review> </reviews> </book> <book id=\\"2\\"> <title>Data Science Essentials</title> <author>Jane Smith</author> <year>2018</year> <genre>Data Science</genre> </book> <book id=\\"3\\"> <title>Machine Learning Basics</title> <author>David Lee</title> <year>2020</year> <genre>Machine Learning</genre> </book> </library> ``` **Note:** Ensure the serialized XML is well-indented for readability.","solution":"import xml.etree.ElementTree as ET from typing import List, Optional, Dict def parse_xml(data: str) -> ET.Element: Parses the XML string and returns the root element. root = ET.fromstring(data) return root def find_books_by_author(root: ET.Element, author_name: str) -> List[str]: Finds and returns a list of book titles by the specified author. books = [] for book in root.findall(\'book\'): author = book.find(\'author\') if author is not None and author.text == author_name: title = book.find(\'title\').text if book.find(\'title\') is not None else \'\' books.append(title) return books def add_new_book(root: ET.Element, book_info: Dict[str, str]) -> None: Adds a new book element to the library with the given book information. new_book = ET.SubElement(root, \'book\', id=book_info[\'id\']) for key, value in book_info.items(): if key != \'id\': element = ET.SubElement(new_book, key) element.text = value def update_review_ratings(root: ET.Element, book_id: str, user: str, new_rating: int) -> bool: Updates the rating of the specified user\'s review for the specified book. for book in root.findall(\'book\'): if book.get(\'id\') == book_id: reviews = book.find(\'reviews\') if reviews: for review in reviews.findall(\'review\'): review_user = review.find(\'user\') if review_user is not None and review_user.text == user: rating = review.find(\'rating\') if rating is not None: rating.text = str(new_rating) return True return False def serialize_xml(root: ET.Element) -> str: Serializes the given XML root to a pretty-printed XML string. return ET.tostring(root, encoding=\'unicode\', method=\'xml\')"},{"question":"Objective: Assess your understanding of creating and customizing plots using seaborn\'s `seaborn.objects` module by designing a complex plot with specific transformations. Problem Statement: You are given the dataset `healthexp`, which contains health expenditure data for multiple countries over various years. Your task is to create a line plot that displays the health spending in USD for each country as a percentage of the spending in the baseline year (1970) over time. Instructions: 1. Load the dataset `healthexp` using seaborn\'s `load_dataset` function. 2. Create a line plot using the `Plot` class from `seaborn.objects` to visualize the health spending in USD (`Spending_USD`) for each country over the years, as a percentage of the spending in the baseline year 1970. 3. Scale the Y-axis to show the percentage change relative to the 1970 values. 4. Label the Y-axis as \\"Percent change in spending from 1970 baseline\\". 5. Ensure the plot displays different lines for each country, with each line uniquely colored. Input: - No input parameters are required. You will be using the internal `healthexp` dataset. Output: - The function should create and display a line plot with the required specifications. Example Usage: ```python create_health_exp_plot() ``` This should display a line plot with the required specifications. Constraints: - Use the `seaborn.objects.Plot` class and related methods. - Ensure that your code does not modify the original dataset. Function Signature: ```python def create_health_exp_plot(): pass ``` Implement the `create_health_exp_plot` function to create and display the plot.","solution":"import seaborn as sns import seaborn.objects as so import pandas as pd def create_health_exp_plot(): # Load the dataset healthexp = sns.load_dataset(\\"healthexp\\") # Create a pivot table to properly calculate the percentage change pivot_df = healthexp.pivot(index=\'Year\', columns=\'Country\', values=\'Spending_USD\') # Calculate the percentage change from the year 1970 baseline = pivot_df.loc[1970] pct_change = (pivot_df.div(baseline) - 1) * 100 # Melt the DataFrame back to long-format required for seaborn.objects melt_pct_change = pct_change.reset_index().melt(id_vars=\'Year\', var_name=\'Country\', value_name=\'Percent Change\') # Create the line plot using seaborn.objects plot = so.Plot(melt_pct_change, x=\\"Year\\", y=\\"Percent Change\\", color=\\"Country\\").add(so.Line()) # Adjust y-axis label plot.label(y=\\"Percent change in spending from 1970 baseline\\") # Display the plot plot.show()"},{"question":"Overview Your task is to simulate some of the functionalities provided by the Python C API for managing function objects using pure Python. You will need to create a class in Python that can mimic some of the functionalities of `PyFunctionObject`. Task Implement a class `FunctionObject` in Python that should include methods to: 1. Create a function object from a given code object and globals dictionary. 2. Retrieve the code object associated with the function object. 3. Retrieve the globals dictionary associated with the function object. 4. Set and retrieve default argument values. 5. Set and retrieve function annotations. Specifications 1. **Class Name**: `FunctionObject` 2. **Methods and Attributes**: ```python class FunctionObject: def __init__(self, code, globals_dict): # Initializes the function object with a code object and a globals dictionary def get_code(self): # Returns the code object associated with the function object def get_globals(self): # Returns the globals dictionary associated with the function object def get_defaults(self): # Returns the default argument values of the function object def set_defaults(self, defaults): # Sets the default argument values for the function object def get_annotations(self): # Returns the annotations of the function object def set_annotations(self, annotations): # Sets the annotations for the function object ``` 3. **Input Constraints**: - The `code` parameter should be a valid code object created using Python\'s built-in `compile` function. - The `globals_dict` parameter should be a dictionary. - The `defaults` parameter should be a tuple or `None`. - The `annotations` parameter should be a dictionary or `None`. 4. **Example**: ```python # Sample code object code = compile(\'def example(x, y): return x + y\', \'\', \'exec\') # Globals dictionary globals_dict = {\'__name__\': \'__main__\'} # Create function object func_obj = FunctionObject(code, globals_dict) # Setting defaults func_obj.set_defaults((1, 2)) # Getting defaults print(func_obj.get_defaults()) # Output: (1, 2) # Setting annotations func_obj.set_annotations({\'x\': int, \'y\': int, \'return\': int}) # Getting annotations print(func_obj.get_annotations()) # Output: {\'x\': int, \'y\': int, \'return\': int} ``` **Note**: Ensure that your implementation adheres to the data structures and method behavior specified above. This challenge tests your understanding of Python function objects and handling code objects dynamically.","solution":"class FunctionObject: def __init__(self, code, globals_dict): Initializes the function object with a code object and a globals dictionary. self.code = code self.globals_dict = globals_dict self.defaults = None self.annotations = {} def get_code(self): Returns the code object associated with the function object. return self.code def get_globals(self): Returns the globals dictionary associated with the function object. return self.globals_dict def get_defaults(self): Returns the default argument values of the function object. return self.defaults def set_defaults(self, defaults): Sets the default argument values for the function object. self.defaults = defaults def get_annotations(self): Returns the annotations of the function object. return self.annotations def set_annotations(self, annotations): Sets the annotations for the function object. self.annotations = annotations"},{"question":"Objective: Demonstrate your understanding of the `importlib.metadata` module in Python 3.10 by implementing a set of functions to extract and display metadata information for a given Python package. Requirements: 1. **Retrieve the Version of a Package**: - **Function Signature**: `def get_package_version(package_name: str) -> str:` - **Input**: A string, `package_name`, representing the name of the package. - **Output**: A string representing the version of the specified package. 2. **List All Entry Points for a Package**: - **Function Signature**: `def list_entry_points(package_name: str) -> Dict[str, List[str]]:` - **Input**: A string, `package_name`, representing the name of the package. - **Output**: A dictionary where keys are entry point groups and values are lists of entry point names. 3. **Display Package Metadata**: - **Function Signature**: `def display_package_metadata(package_name: str) -> None:` - **Input**: A string, `package_name`, representing the name of the package. - **Output**: None. Print the metadata of the specified package in a human-readable format. 4. **List All Files in a Package**: - **Function Signature**: `def list_package_files(package_name: str) -> List[str]:` - **Input**: A string, `package_name`, representing the name of the package. - **Output**: A list of strings where each string is a file path of a file included in the package. 5. **Get All Package Requirements**: - **Function Signature**: `def get_package_requirements(package_name: str) -> List[str]:` - **Input**: A string, `package_name`, representing the name of the package. - **Output**: A list of strings representing the requirements of the specified package. Constraints: - Assume the package specified by `package_name` is installed in the Python environment where the code is executed. Example: ```python # Example usage: version = get_package_version(\\"wheel\\") print(version) # Output: \'0.32.3\' (as an example) entry_points = list_entry_points(\\"wheel\\") print(entry_points) # Output: {\'console_scripts\': [\'pip\', \'pytest\', ...]} display_package_metadata(\\"wheel\\") # Output: Will print metadata like \'Name: wheel | Version: 0.32.3 | ...\' files = list_package_files(\\"wheel\\") print(files) # Output: [\'wheel/util.py\', \'wheel/__init__.py\', ...] requirements = get_package_requirements(\\"wheel\\") print(requirements) # Output: [\\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\"] ``` Performance Requirements: - The functions should efficiently handle packages with a large number of files and metadata properties. Implement the functions adhering to the specifications provided.","solution":"from importlib.metadata import version, entry_points, metadata, files, requires from typing import Dict, List def get_package_version(package_name: str) -> str: Returns the version of the specified package. return version(package_name) def list_entry_points(package_name: str) -> Dict[str, List[str]]: Lists all entry points for the specified package. eps = entry_points() package_entry_points = {group: [point.name for point in eps.select(group=group, name=package_name)] for group in eps.groups} return package_entry_points def display_package_metadata(package_name: str) -> None: Displays metadata for the specified package. meta = metadata(package_name) for key, value in meta.items(): print(f\\"{key}: {value}\\") def list_package_files(package_name: str) -> List[str]: Lists all files in the specified package. return [str(file) for file in files(package_name)] def get_package_requirements(package_name: str) -> List[str]: Retrieves all requirements for the specified package. return requires(package_name) or []"},{"question":"# Asyncio Exception Handling Exercise **Objective:** Write a Python function that demonstrates how to handle various asyncio exceptions effectively within an asynchronous context. Your function should simulate an asynchronous operation that can trigger the different exceptions documented above. **Function Signature:** ```python import asyncio async def simulate_async_operations(): pass ``` **Requirements:** 1. **Timeout Handling**: Simulate an operation that times out using `asyncio.TimeoutError`. Provide a custom message indicating the timeout. 2. **Cancellation Handling**: Simulate an operation that gets cancelled using `asyncio.CancelledError`. Provide a custom message indicating the cancellation. 3. **Invalid State Handling**: Simulate an invalid state in an asyncio Task or Future using `asyncio.InvalidStateError`. Provide a custom message indicating the invalid state. 4. **Sendfile Not Available Handling**: Simulate the unavailability of the `sendfile` syscall using `asyncio.SendfileNotAvailableError`. Provide a custom message indicating the unavailability. 5. **Incomplete Read Handling**: Simulate an incomplete read operation using `asyncio.IncompleteReadError`. Provide a custom message indicating the read was incomplete and the number of expected and partial bytes. 6. **Limit Overrun Handling**: Simulate reaching a buffer size limit using `asyncio.LimitOverrunError`. Provide a custom message indicating the buffer overrun and the number of consumed bytes. **Constraints:** - You should use the `asyncio` library to simulate the asynchronous operations. - The function should be well-structured and handle each exception case separately within the function. - Properly document each part of the code for clarity. **Expected Output:** Your function should print out custom messages for each exception encountered, demonstrating the appropriate handling mechanism for each asyncio-specific exception. **Example:** ```python async def simulate_async_operations(): try: raise asyncio.TimeoutError except asyncio.TimeoutError: print(\\"Operation timed out.\\") try: raise asyncio.CancelledError except asyncio.CancelledError: print(\\"Operation was cancelled.\\") # Continue similarly for the other exceptions # Handle InvalidStateError, SendfileNotAvailableError, IncompleteReadError, and LimitOverrunError # Run the coroutine to see the exception handling in action asyncio.run(simulate_async_operations()) ``` Write the complete function `simulate_async_operations` as described.","solution":"import asyncio async def simulate_async_operations(): Simulates various asynchronous operations and handles several asyncio exceptions. # Timeout Handling try: raise asyncio.TimeoutError(\\"Operation timed out.\\") except asyncio.TimeoutError as e: print(f\\"TimeoutError: {e}\\") # Cancellation Handling try: raise asyncio.CancelledError(\\"Operation was cancelled.\\") except asyncio.CancelledError as e: print(f\\"CancelledError: {e}\\") # Invalid State Handling try: raise asyncio.InvalidStateError(\\"Invalid state in asyncio Task or Future.\\") except asyncio.InvalidStateError as e: print(f\\"InvalidStateError: {e}\\") # Sendfile Not Available Handling try: raise asyncio.SendfileNotAvailableError(\\"Sendfile syscall is not available.\\") except asyncio.SendfileNotAvailableError as e: print(f\\"SendfileNotAvailableError: {e}\\") # Incomplete Read Handling try: raise asyncio.IncompleteReadError(partial=b\\"data\\", expected=10) except asyncio.IncompleteReadError as e: print(f\\"IncompleteReadError: {e}. Expected {e.expected}, got {len(e.partial)}\\") # Limit Overrun Handling try: raise asyncio.LimitOverrunError(\\"Buffer size limit exceeded.\\", consumed=5) except asyncio.LimitOverrunError as e: print(f\\"LimitOverrunError: {e}. Consumed {e.consumed} bytes\\") # Run the coroutine to see the exception handling in action # Uncomment the line below to run the simulation if running interactively # asyncio.run(simulate_async_operations())"},{"question":"**Objective:** Implement a custom exception handler in Python using the traceback module. This handler must capture exceptions, format the stack trace, and write the formatted stack trace to a log file. Additionally, print a user-friendly message to the console. **Function Signature:** ```python def custom_exception_handler(exc_type, exc_value, exc_traceback): ``` **Requirements:** 1. The function `custom_exception_handler` should accept three parameters: - `exc_type`: The type of exception. - `exc_value`: The value of the exception. - `exc_traceback`: The traceback object. 2. The function should perform the following tasks: - Capture and format the stack trace from the `exc_traceback` using the `traceback.format_tb` function. - Capture and format the exception information using the `traceback.format_exception_only` function. - Combine the formatted stack trace and exception information into a single string. - Write this string to a log file named `error_log.txt`. - Print a user-friendly message to the console, indicating that an error has occurred and it has been logged. The message should not include the actual stack trace or exception details. **Example Usage:** ```python import traceback import sys def custom_exception_handler(exc_type, exc_value, exc_traceback): # Task: Implement the custom exception handler as described above # Setting the custom exception handler as the global exception hook sys.excepthook = custom_exception_handler # Example code to raise an exception, triggering the custom handler def cause_error(): raise ValueError(\\"This is a sample error.\\") try: cause_error() except Exception as e: # Manually triggering the custom handler for demonstration custom_exception_handler(type(e), e, e.__traceback__) ``` **Constraints:** - Ensure that the function works for any type of exception. - The log file should be appended with each new error instead of overwriting previous errors. - You are not allowed to use any external logging libraries; only standard Python libraries are permitted. - You are expected to handle file operations and potential file I/O errors gracefully. **Expected Output:** 1. When an exception occurs, a message like the following should be printed to the console: ``` An error has occurred and has been logged. Please check the log file for details. ``` 2. The log file `error_log.txt` should contain the detailed stack trace and exception information in a readable format. **Performance Consideration:** - The solution should be efficient in terms of both time and space complexity, considering the typical use-case scenarios. **Assessment Criteria:** - Correctness: The function must handle and log the exception correctly. - Robustness: The function should gracefully handle potential errors during file operations. - Clarity: Code should be clean, well-documented, and easy to understand. - Efficiency: The function should efficiently manage resources like file handles and memory.","solution":"import traceback def custom_exception_handler(exc_type, exc_value, exc_traceback): Custom exception handler that logs the stack trace to a file and prints a user-friendly message. try: with open(\\"error_log.txt\\", \\"a\\") as log_file: # Capture and format stack trace trace_lines = traceback.format_tb(exc_traceback) trace_text = \\"\\".join(trace_lines) # Capture and format exception information exc_info_lines = traceback.format_exception_only(exc_type, exc_value) exc_info_text = \\"\\".join(exc_info_lines) # Combine into a single string log_entry = f\\"Exception type: {exc_type.__name__}n{trace_text}n{exc_info_text}n{\'=\'*60}n\\" # Write to log file log_file.write(log_entry) except Exception as e: print(f\\"Failed to log error due to: {e}\\") # Print user-friendly message to the console print(\\"An error has occurred and has been logged. Please check the log file for details.\\")"},{"question":"# Coding Assessment: Understanding Python Built-in Constants Objective Demonstrate understanding of Python 3.10 built-in constants by implementing a function that showcases their usage and constraints. Problem Statement Write a function `constant_checker()` that takes no parameters and returns a dictionary with the following keys and corresponding boolean values: - `\'false_assignment\'`: `True` if the statement `False = 1` raises a `SyntaxError`, otherwise `False`. - `\'true_assignment\'`: `True` if the statement `True = 0` raises a `SyntaxError`, otherwise `False`. - `\'none_assignment\'`: `True` if the statement `None = object()` raises a `SyntaxError`, otherwise `False`. - `\'notimplemented_usage\'`: `True` if `NotImplemented` is evaluated in a boolean context and raises a `TypeError`, otherwise `False`. - `\'ellipsis_type\'`: `True` if `Ellipsis` corresponds to the `types.EllipsisType` type, otherwise `False`. - `\'debug_status\'`: `True` if `__debug__` is `True` when Python runs without optimization (-O flag), otherwise `False`. Constraints: 1. You should not actually assign values to `False`, `True`, or `None` directly in your code, as this will raise a `SyntaxError` and halt execution. 2. Use appropriate exception handling to determine if `NotImplemented` raises a `TypeError` in a boolean context. 3. Use Python\'s built-in `types` module to check the type of `Ellipsis`. Function Signature ```python def constant_checker() -> dict: pass ``` Example Output ```python print(constant_checker()) # Possible output: {\'false_assignment\': True, \'true_assignment\': True, \'none_assignment\': True, # \'notimplemented_usage\': True, \'ellipsis_type\': True, \'debug_status\': True} ``` Notes: - The constraints are there to ensure that you can check for syntax errors and type errors without causing the interpreter to stop your program. - Use a combination of try-except blocks and type checks where necessary. Good luck, and ensure your solution adheres to the constraints provided!","solution":"import types def constant_checker(): results = {} # Check if assigning to False raises a SyntaxError try: exec(\'False = 1\') results[\'false_assignment\'] = False except SyntaxError: results[\'false_assignment\'] = True # Check if assigning to True raises a SyntaxError try: exec(\'True = 0\') results[\'true_assignment\'] = False except SyntaxError: results[\'true_assignment\'] = True # Check if assigning to None raises a SyntaxError try: exec(\'None = object()\') results[\'none_assignment\'] = False except SyntaxError: results[\'none_assignment\'] = True # Check if NotImplemented raises a TypeError in a boolean context try: notimplemented_usage = bool(NotImplemented) results[\'notimplemented_usage\'] = False except TypeError: results[\'notimplemented_usage\'] = True # Check if Ellipsis corresponds to types.EllipsisType results[\'ellipsis_type\'] = isinstance(Ellipsis, types.EllipsisType) # Check the debug status results[\'debug_status\'] = __debug__ return results"},{"question":"# Seaborn Context and Customization Challenge You are provided with a dataset that contains information about the average performance of different algorithms over various sample sizes. Your task is to create a function that visualizes this data using Seaborn, with specific requirements for context and customization. Dataset Description The dataset is a CSV file with three columns: - `algorithm`: Name of the algorithm (string). - `sample_size`: Size of the sample (integer). - `performance`: Average performance (float). Here is an example of the dataset: ``` algorithm,sample_size,performance Algorithm_A,10,0.85 Algorithm_A,50,0.90 Algorithm_B,10,0.80 Algorithm_B,50,0.88 ... ``` Function Specification Define a function `visualize_performance` that receives a file path to the CSV dataset and generates a line plot with the following customizations: 1. Set the Seaborn context to `\\"notebook\\"` with a `font_scale` of `1.5`. 2. Override the default line width to `2.5`. 3. The x-axis should represent `sample_size`. 4. The y-axis should represent `performance`. 5. Lines should be colored by `algorithm`. # Input - `file_path`: A string representing the file path to the CSV dataset. # Output - The function should generate and display the line plot as specified. Use the following template for your implementation: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_performance(file_path): # Read the CSV data into a DataFrame data = pd.read_csv(file_path) # Set the desired Seaborn context with custom font scale sns.set_context(\\"notebook\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2.5}) # Create the line plot sns.lineplot(data=data, x=\\"sample_size\\", y=\\"performance\\", hue=\\"algorithm\\") # Display the plot plt.show() # Example usage # visualize_performance(\\"path/to/your/dataset.csv\\") ``` Constraints 1. The dataset file will always be correctly formatted as specified. 2. There will be no missing or NaN values in the dataset. 3. The function should handle datasets with any number of algorithms and sample sizes. Notes - Ensure you use the Seaborn library and adhere to the context and customization requirements. - The line plot should be clear, with appropriately scaled fonts and lines as specified.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_performance(file_path): Reads dataset from a CSV file and generates a line plot with specified context and customization. Args: file_path (str): Path to the CSV file containing the dataset. Returns: None: Displays the plot. # Read the CSV data into a DataFrame data = pd.read_csv(file_path) # Set the desired Seaborn context with custom font scale sns.set_context(\\"notebook\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2.5}) # Create the line plot sns.lineplot(data=data, x=\\"sample_size\\", y=\\"performance\\", hue=\\"algorithm\\") # Display the plot plt.show()"},{"question":"**Problem Statement: Advanced File Locking and Control with `fcntl`** You are tasked with implementing a Python function that will handle file locking and control operations on a given file. Specifically, you will: 1. Open a file for reading and writing. 2. Apply an exclusive lock to the file to prevent other processes from modifying it. 3. Check the current status flags on the file and modify them to set the file status as non-blocking. 4. Unlock the file after performing these operations. **Function Signature:** ```python def advanced_file_control(file_path: str) -> int: pass ``` **Input:** - `file_path` (str): The path to the file that you will operate on. **Output:** - Returns an integer representing the final status flags of the file after modification. **Constraints and Notes:** - You may assume the file exists and is accessible with appropriate permissions. - Use the `fcntl` module functions as described in the provided documentation. - The function should handle exceptions appropriately and ensure the file is unlocked in case of errors. **Example:** ```python file_path = \\"/tmp/testfile.txt\\" result = advanced_file_control(file_path) print(result) ``` **Explanation:** In the function implementation, you should: 1. Open the specified file in read-write mode. 2. Apply an exclusive lock using `fcntl.lockf()` or `fcntl.flock()`. 3. Retrieve the current file status flags using `fcntl.fcntl()` with `F_GETFL`. 4. Modify the flags to set the file status as non-blocking using `fcntl.fcntl()` with `F_SETFL`. 5. Return the modified status flags. 6. Ensure that in case of any exception, the exclusive lock is released to avoid deadlocks.","solution":"import fcntl import os def advanced_file_control(file_path: str) -> int: Handles file locking and control operations on the given file. Args: file_path (str): The path to the file to operate on. Returns: int: The file status flags after modification. try: fd = os.open(file_path, os.O_RDWR) # Apply an exclusive lock fcntl.flock(fd, fcntl.LOCK_EX) # Get the current file status flags flags = fcntl.fcntl(fd, fcntl.F_GETFL) # Modify the flags to set the file status as non-blocking new_flags = flags | os.O_NONBLOCK fcntl.fcntl(fd, fcntl.F_SETFL, new_flags) # Unlock the file fcntl.flock(fd, fcntl.LOCK_UN) return new_flags except Exception as e: if \'fd\' in locals(): fcntl.flock(fd, fcntl.LOCK_UN) raise e finally: if \'fd\' in locals(): os.close(fd)"},{"question":"# Audio Device Control and Playback in Python You are required to develop a Python program using the `ossaudiodev` module (though deprecated, use it for this exercise) to interact with an OSS audio device and perform the following tasks: 1. Open the default audio device in write-only mode. 2. Set the audio format to `AFMT_S16_LE` (Signed, 16-bit audio, little-endian byte order). 3. Set the audio device to use 2 channels (stereophonic sound). 4. Set the sampling rate to 44100 Hz (CD quality audio). 5. Generate a 5-second sine wave tone of 440 Hz (A4 note) and write it to the audio device. 6. Ensure the tone plays correctly before the program exits. 7. Properly handle any potential errors using the appropriate exceptions like `OSSAudioError` and `OSError`. # Specifications - **Input/Output**: There is no input from the user. The program generates and plays a 440 Hz sine wave. - **Constraints**: Ensure the generated audio is of high quality and plays without interruptions. - **Performance**: The playback should not have noticeable lags or delays. # Hints - Use the `math` module to generate sine wave values. - The `write()` method of the OSS audio device can be used to send audio data. - Handle device configuration and playback in a context management block for proper resource management. # Expected Behavior - The program should open the audio device, configure it correctly, and then play the generated sine wave without any audio glitches. - Any errors should be caught and a meaningful message should be printed, without crashing the program. **Sample Code Skeleton**: ```python import ossaudiodev import math SAMPLING_RATE = 44100 DURATION = 5 FREQUENCY = 440 try: # Open the audio device in write-only mode dsp = ossaudiodev.open(\'w\') # Set audio parameters dsp.setfmt(ossaudiodev.AFMT_S16_LE) dsp.channels(2) dsp.speed(SAMPLING_RATE) # Generate sinewave tone num_samples = int(DURATION * SAMPLING_RATE) amplitude = 32767 sine_wave = bytearray() for i in range(num_samples): sample = amplitude * math.sin(2 * math.pi * FREQUENCY * i / SAMPLING_RATE) sine_wave.extend(int(sample).to_bytes(2, byteorder=\'little\', signed=True)) sine_wave.extend(int(sample).to_bytes(2, byteorder=\'little\', signed=True)) # Write the generated audio to the device dsp.writeall(sine_wave) # Close the device dsp.close() except (ossaudiodev.OSSAudioError, OSError) as e: print(f\\"An error occurred: {e}\\") ``` **Note**: You might need to adjust the method calls and error handling as appropriate based on testing and specific runtime behaviors.","solution":"import ossaudiodev import math SAMPLING_RATE = 44100 DURATION = 5 FREQUENCY = 440 def generate_sine_wave(frequency, duration, sampling_rate): num_samples = int(duration * sampling_rate) amplitude = 32767 sine_wave = bytearray() for i in range(num_samples): sample_value = int(amplitude * math.sin(2 * math.pi * frequency * i / sampling_rate)) sine_wave.extend(sample_value.to_bytes(2, byteorder=\'little\', signed=True)) sine_wave.extend(sample_value.to_bytes(2, byteorder=\'little\', signed=True)) return sine_wave try: # Open the audio device in write-only mode dsp = ossaudiodev.open(\'w\') # Set audio parameters dsp.setfmt(ossaudiodev.AFMT_S16_LE) dsp.channels(2) dsp.speed(SAMPLING_RATE) # Generate sinewave tone sine_wave = generate_sine_wave(FREQUENCY, DURATION, SAMPLING_RATE) # Write the generated audio to the device dsp.writeall(sine_wave) # Close the device dsp.close() except (ossaudiodev.OSSAudioError, OSError) as e: print(f\\"An error occurred: {e}\\")"},{"question":"Playing a Simple Audio Tone Using `ossaudiodev` Objective Implement a Python function that sets up an OSS audio device and plays a simple audio tone (e.g., a sine wave) for a short duration. The function should demonstrate comprehension of opening and configuring audio devices, handling audio formats, writing audio data, and ensuring proper resource cleanup. Detailed Requirements 1. **Function Signature**: ```python def play_tone(frequency: int, duration: float, samplerate: int = 44100) -> None: ``` 2. **Input Parameters**: - `frequency` (int): Frequency of the sine wave in Hertz (e.g., 440 for the A4 note). - `duration` (float): Duration of the tone in seconds (e.g., 1.0 for one second). - `samplerate` (int, optional): Sampling rate in samples per second. Default is 44100, which is CD-quality audio. 3. **Functionality Requirements**: - Open the default audio device (e.g., `/dev/dsp`). - Set the audio format to 16-bit signed little-endian (`AFMT_S16_LE`). - Set the number of channels to 1 (mono). - Set the sampling rate as specified by the `samplerate` parameter. - Generate audio data for a sine wave with the given `frequency` and `duration`. - Write the generated audio data to the audio device ensuring all data is written. - Close the audio device properly after playing the tone. 4. **Constraints**: - Ensure the function handles different samplerates but defaults to 44100 Hz. - Assume that valid frequencies are in the range [20, 20000] Hz. - Handle any potential `OSError` or `OSSAudioError` exceptions appropriately. - Use blocking mode for writing data to ensure the entire tone is played. Example Usage ```python if __name__ == \\"__main__\\": # Play a 440 Hz tone for 2 seconds play_tone(440, 2.0) ``` Hints - Use numpy to generate the sine wave data easily. - Remember to convert the generated samples to the appropriate byte format before writing. Boilerplate Code You can start with the following boilerplate code: ```python import numpy as np import ossaudiodev import os import struct def play_tone(frequency: int, duration: float, samplerate: int = 44100) -> None: # Open the audio device dsp = ossaudiodev.open(\'w\') # Set audio parameters dsp.setfmt(ossaudiodev.AFMT_S16_LE) dsp.channels(1) dsp.speed(samplerate) # Length of the sample array length = int(duration * samplerate) # Generate sine wave samples t = np.linspace(0, duration, length, endpoint=False) samples = (np.sin(2 * np.pi * frequency * t) * 32767).astype(np.int16) # Write audio data to the device dsp.write(samples.tobytes()) # Close the device dsp.close() # Example to play a 440 Hz sine wave for 2 seconds if __name__ == \\"__main__\\": play_tone(440, 2.0) ```","solution":"import numpy as np import ossaudiodev import os import struct def play_tone(frequency: int, duration: float, samplerate: int = 44100) -> None: if not 20 <= frequency <= 20000: raise ValueError(\\"Frequency must be between 20 and 20000 Hz\\") if duration <= 0: raise ValueError(\\"Duration must be positive\\") try: # Open the default audio device dsp = ossaudiodev.open(\'w\') # Set audio format dsp.setfmt(ossaudiodev.AFMT_S16_LE) # 16-bit signed little-endian dsp.channels(1) # Mono dsp.speed(samplerate) # Set sample rate # Generate the sine wave length = int(duration * samplerate) t = np.linspace(0, duration, length, endpoint=False) samples = (np.sin(2 * np.pi * frequency * t) * 32767).astype(np.int16) # Write audio data to the device dsp.write(samples.tobytes()) except (ossaudiodev.OSSAudioError, OSError) as e: print(f\\"An error occurred while playing the tone: {e}\\") finally: if \'dsp\' in locals(): dsp.close()"},{"question":"# Coding Assessment Objective Your task is to implement a Python function that hashes a password and verifies it using the `crypt` module. The purpose of this assessment is to test your understanding of hashing methods, salt generation, and password validation using the `crypt` module. # Problem Statement Implement a class `PasswordManager` with the following methods: 1. **`hash_password(plaintext: str, method: str = None, rounds: int = None) -> str`**: - **Input**: - `plaintext` (str): The plain text password to hash. - `method` (str, optional): The hashing method to use. If `None`, the strongest method available should be used. Valid choices are `SHA512`, `SHA256`, `BLOWFISH`, `MD5`, `CRYPT`. - `rounds` (int, optional): The number of rounds for `METHOD_SHA256`, `METHOD_SHA512`, and `METHOD_BLOWFISH`. If not provided or not applicable to the method, default values should be used. - **Output**: A hashed password string. - **Constraints**: - Ensure the `method` (if provided) is one of the valid choices or `None`. - `rounds` should be within the allowed range for the selected method. 2. **`verify_password(plaintext: str, hashed: str) -> bool`**: - **Input**: - `plaintext` (str): The plain text password to verify. - `hashed` (str): The hashed password to compare against. - **Output**: A boolean value indicating whether the plain text password matches the hashed password. The hashing process should make use of the strongest available method if none is specified. # Example Usage ```python # Example inputs and usage manager = PasswordManager() hashed_password = manager.hash_password(\\"my_password\\", method=\\"SHA256\\") print(hashed_password) # Output will be a hashed password string # Verify the password is_valid = manager.verify_password(\\"my_password\\", hashed_password) print(is_valid) # Output will be True # Example with default method and rounds for stronger hashing mechanism strong_hashed_password = manager.hash_password(\\"another_password\\") print(strong_hashed_password) # Verify using default method and rounds is_valid_strong = manager.verify_password(\\"another_password\\", strong_hashed_password) print(is_valid_strong) # Output will be True # Invalid case is_invalid = manager.verify_password(\\"wrong_password\\", hashed_password) print(is_invalid) # Output will be False ``` # Requirements - Your solution must use the `crypt` module for the hash generation and verification. - You must handle invalid inputs gracefully. - Write appropriate validations for method types and bounds for rounds, raising `ValueError` for invalid cases. - Ensure to document your code and provide comments for better understanding. Good luck!","solution":"import crypt import random import string class PasswordManager: VALID_METHODS = { \'SHA512\': crypt.METHOD_SHA512, \'SHA256\': crypt.METHOD_SHA256, \'BLOWFISH\': crypt.METHOD_BLOWFISH, \'MD5\': crypt.METHOD_MD5, \'CRYPT\': crypt.METHOD_CRYPT } def hash_password(self, plaintext: str, method: str = None, rounds: int = None) -> str: Hashes a plaintext password using the specified method. Args: plaintext (str): The plain text password to hash. method (str, optional): The hashing method to use. Defaults to the strongest available method. rounds (int, optional): The number of rounds to use for methods that support it. Default rounds will be used if not provided or not applicable. Returns: str: The hashed password string. Raises: ValueError: If the method is not in the valid methods. if method: if method not in self.VALID_METHODS: raise ValueError(f\\"Invalid method. Valid methods are: {list(self.VALID_METHODS.keys())}\\") selected_method = self.VALID_METHODS[method] else: selected_method = crypt.METHOD_SHA512 # Default to the strongest method if rounds: if selected_method in [crypt.METHOD_SHA256, crypt.METHOD_SHA512] and (1000 <= rounds <= 999999999): salt = crypt.mksalt(selected_method, rounds=rounds) elif selected_method == crypt.METHOD_BLOWFISH and (4 <= rounds <= 31): salt = crypt.mksalt(selected_method, rounds=rounds) else: raise ValueError(\\"Invalid rounds for the selected method.\\") else: salt = crypt.mksalt(selected_method) hashed_password = crypt.crypt(plaintext, salt) return hashed_password def verify_password(self, plaintext: str, hashed: str) -> bool: Verifies if a plaintext password matches its hashed version. Args: plaintext (str): The plain text password. hashed (str): The hashed password to verify against. Returns: bool: True if the password matches, False otherwise. return crypt.crypt(plaintext, hashed) == hashed"},{"question":"# Question: Comprehensive Error Handling with `errno` **Objective:** You need to write a Python function that demonstrates understanding and usage of the `errno` module to handle and report file-related errors. **Function Signature:** ```python def handle_file_errors(file_path: str) -> str: pass ``` **Description:** Implement the `handle_file_errors` function that takes a single parameter: - `file_path` (a string): The path to a file the function should attempt to access. 1. Attempt to perform the following operations on the specified `file_path`: - Check if the file exists. - Open the file for reading. - Read the first line of the file. 2. Handle the following potential errors using the `errno` module: - If the file does not exist, return the error message corresponding to `errno.ENOENT`. - If the file is not readable due to permission issues, return the error message corresponding to `errno.EACCES`. - Handle any other `OSError` and return its corresponding error message using `os.strerror()`. **Constraints:** - You may assume the input `file_path` is always a valid string. - You should only handle errors specific to file operations as described above. **Examples:** ```python # Example 1: # Assuming \'/path/to/file_that_exists.txt\' exists and is readable handle_file_errors(\'/path/to/file_that_exists.txt\') # Possible Output: \'First line of the file: This is the content of the first line.\' # Example 2: # Assuming \'/path/to/missing_file.txt\' does not exist handle_file_errors(\'/path/to/missing_file.txt\') # Output: \'No such file or directory\' # Example 3: # Assuming \'/path/to/unreadable_file.txt\' exists, but is not readable due to permissions handle_file_errors(\'/path/to/unreadable_file.txt\') # Output: \'Permission denied\' # Example 4: # Handling any other error that arises handle_file_errors(\'/path/to/other_error_file.txt\') # Output: \'<Corresponding error message based on os.strerror()>\' ``` **Assessment Criteria:** - Correct usage of the `errno` module and handling of specified errors. - Clear and descriptive handling of error messages. - Robustness of the function to handle unexpected `OSError` instances appropriately.","solution":"import os import errno def handle_file_errors(file_path: str) -> str: try: # Check if the file exists and can be opened with open(file_path, \'r\') as file: # Read the first line of the file first_line = file.readline().strip() return f\'First line of the file: {first_line}\' except OSError as e: if e.errno == errno.ENOENT: return \'No such file or directory\' elif e.errno == errno.EACCES: return \'Permission denied\' else: return os.strerror(e.errno)"},{"question":"You are tasked with writing an asynchronous Python program using the `asyncio` module. The goal is to simulate a set of I/O-bound tasks (such as API calls) that must run concurrently, handle timeouts, and appropriately handle cancellations. # Problem Statement Implement a function `run_concurrent_tasks_with_timeout`, which takes a list of I/O-bound coroutine functions `tasks` and a `timeout` in seconds. The function should: 1. Run all the provided `tasks` concurrently. 2. Ensure that all tasks complete within the specified `timeout`. 3. Cancel and handle any tasks that do not complete within the `timeout`. 4. Return a list of results of the completed tasks and a list of tasks that were cancelled due to the timeout. # Function Signature ```python import asyncio from typing import List, Tuple, Any async def run_concurrent_tasks_with_timeout(tasks: List[asyncio.Task], timeout: float) -> Tuple[List[Any], List[asyncio.Task]]: pass ``` # Input - `tasks` (List[asyncio.Task]): A list of tasks (coroutines) to run concurrently. - `timeout` (float): The maximum time allowed for the tasks to complete. # Output - Tuple[List[Any], List[asyncio.Task]]: A tuple containing: - A list of results of the successfully completed tasks. - A list of tasks that were cancelled due to the timeout. # Constraints - Each task in `tasks` will be an awaitable coroutine. - The `timeout` parameter will be a positive float representing seconds. # Example ```python import asyncio async def task1(): await asyncio.sleep(1) return \\"Task 1 completed\\" async def task2(): await asyncio.sleep(2) return \\"Task 2 completed\\" async def task3(): await asyncio.sleep(3) return \\"Task 3 completed\\" tasks = [task1(), task2(), task3()] result, cancelled = await run_concurrent_tasks_with_timeout(tasks, 2.5) print(result) # Output: [\\"Task 1 completed\\", \\"Task 2 completed\\"] print(cancelled) # Output: [<Task pending name=\'...\'>] (containing task3) ``` # Notes - Make use of `asyncio.gather()`, `asyncio.wait()`, and `asyncio.shield()` to manage concurrency and cancellations. - Ensure appropriate exception handling for cancellations and timeouts. # Hints - Use `await asyncio.wait_for` combined with a try-except block to handle timeouts. - Remember to handle cancelled tasks gracefully and provide meaningful output for both successful and cancelled tasks.","solution":"import asyncio from typing import List, Any, Tuple async def run_concurrent_tasks_with_timeout(tasks: List[asyncio.Task], timeout: float) -> Tuple[List[Any], List[asyncio.Task]]: try: results = await asyncio.wait_for(asyncio.gather(*tasks), timeout=timeout) return results, [] except asyncio.TimeoutError: # If a TimeoutError occurs, cancel all running tasks for task in tasks: if not task.done(): task.cancel() # Gather the results of completed tasks and list of cancelled tasks completed_tasks = [] cancelled_tasks = [] for task in tasks: if task.done() and not task.cancelled(): completed_tasks.append(task.result()) else: cancelled_tasks.append(task) return completed_tasks, cancelled_tasks"},{"question":"**Objective:** Demonstrate your understanding of the `scikit-learn` package by implementing and evaluating SGD-based classifiers and regressors. # Problem Statement You are provided with two distinct datasets: 1. **Classification Data:** A dataset containing images of handwritten digits (0-9) with pixel values as features. 2. **Regression Data:** A dataset containing information about house prices with various numerical and categorical features. **Tasks:** 1. **Data Preparation:** - Load the classification and regression datasets. - Scale the feature values for both datasets using StandardScaler. 2. **Model Training:** - Train a `SGDClassifier` on the classification dataset using the following configurations: - `loss=\\"hinge\\"` with `penalty=\\"l2\\"` - `loss=\\"log_loss\\"` with `penalty=\\"elasticnet\\"` - Train a `SGDRegressor` on the regression dataset using the following configurations: - `loss=\\"squared_error\\"` with `penalty=\\"l2\\"` - `loss=\\"huber\\"` with `penalty=\\"l1\\"` 3. **Model Evaluation:** - For the classification tasks, evaluate the models using accuracy and present the confusion matrix. - For the regression tasks, evaluate the models using the mean squared error (MSE). 4. **Comparison and Analysis:** - Compare the performance of the different configurations of the classifier and regressor. - Discuss the effect of scaling the data and using different loss functions and penalties. # Input / Output Format **Input:** - You are provided with two datasets (details will be provided in the actual assessment environment). **Output:** - Report on classification performance with accuracy and confusion matrix. - Report on regression performance with MSE. - Discussion on the effects of scaling, loss functions, and penalties. # Constraints: - Use `scikit-learn` version 0.24 or later. - Ensure that you use pipelines wherever appropriate. # Example Code: ```python from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix from sklearn.datasets import load_digits, fetch_california_housing from sklearn.model_selection import train_test_split # Load datasets (Use appropriate dataset loading functions) digits = load_digits() X_digits, y_digits = digits.data, digits.target housing = fetch_california_housing() X_housing, y_housing = housing.data, housing.target # Split data into training and test sets X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_digits, y_digits, test_size=0.2, random_state=42) X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_housing, y_housing, test_size=0.2, random_state=42) # Create and train SGDClassifier with hinge loss clf_hinge = make_pipeline(StandardScaler(), SGDClassifier(loss=\\"hinge\\", penalty=\\"l2\\", max_iter=1000, tol=1e-3)) clf_hinge.fit(X_train_clf, y_train_clf) y_pred_clf_hinge = clf_hinge.predict(X_test_clf) # Create and train SGDClassifier with log_loss and elasticnet penalty clf_log = make_pipeline(StandardScaler(), SGDClassifier(loss=\\"log_loss\\", penalty=\\"elasticnet\\", max_iter=1000, tol=1e-3)) clf_log.fit(X_train_clf, y_train_clf) y_pred_clf_log = clf_log.predict(X_test_clf) # Evaluation for classification accuracy_hinge = accuracy_score(y_test_clf, y_pred_clf_hinge) accuracy_log = accuracy_score(y_test_clf, y_pred_clf_log) print(f\\"Classification Accuracy (hinge): {accuracy_hinge}\\") print(f\\"Confusion Matrix (hinge):n{confusion_matrix(y_test_clf, y_pred_clf_hinge)}\\") print(f\\"Classification Accuracy (log_loss): {accuracy_log}\\") print(f\\"Confusion Matrix (log_loss):n{confusion_matrix(y_test_clf, y_pred_clf_log)}\\") # Create and train SGDRegressor with squared_error loss reg_squared = make_pipeline(StandardScaler(), SGDRegressor(loss=\\"squared_error\\", penalty=\\"l2\\", max_iter=1000, tol=1e-3)) reg_squared.fit(X_train_reg, y_train_reg) y_pred_reg_squared = reg_squared.predict(X_test_reg) # Create and train SGDRegressor with huber loss reg_huber = make_pipeline(StandardScaler(), SGDRegressor(loss=\\"huber\\", penalty=\\"l1\\", max_iter=1000, tol=1e-3)) reg_huber.fit(X_train_reg, y_train_reg) y_pred_reg_huber = reg_huber.predict(X_test_reg) # Evaluation for regression mse_squared = mean_squared_error(y_test_reg, y_pred_reg_squared) mse_huber = mean_squared_error(y_test_reg, y_pred_reg_huber) print(f\\"Regression MSE (squared_error): {mse_squared}\\") print(f\\"Regression MSE (huber): {mse_huber}\\") # Discussion # Write your analysis and comparison here based on the results. ``` Take care to explain each step with appropriate comments and ensure your code is clean and well-documented.","solution":"from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix from sklearn.datasets import load_digits, fetch_california_housing from sklearn.model_selection import train_test_split # Load datasets digits = load_digits() X_digits, y_digits = digits.data, digits.target housing = fetch_california_housing() X_housing, y_housing = housing.data, housing.target # Split data into training and test sets X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_digits, y_digits, test_size=0.2, random_state=42) X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_housing, y_housing, test_size=0.2, random_state=42) # Create and train SGDClassifier with hinge loss clf_hinge = make_pipeline(StandardScaler(), SGDClassifier(loss=\\"hinge\\", penalty=\\"l2\\", max_iter=1000, tol=1e-3)) clf_hinge.fit(X_train_clf, y_train_clf) y_pred_clf_hinge = clf_hinge.predict(X_test_clf) # Create and train SGDClassifier with log_loss and elasticnet penalty clf_log = make_pipeline(StandardScaler(), SGDClassifier(loss=\\"log_loss\\", penalty=\\"elasticnet\\", max_iter=1000, tol=1e-3)) clf_log.fit(X_train_clf, y_train_clf) y_pred_clf_log = clf_log.predict(X_test_clf) # Evaluation for classification accuracy_hinge = accuracy_score(y_test_clf, y_pred_clf_hinge) accuracy_log = accuracy_score(y_test_clf, y_pred_clf_log) conf_matrix_hinge = confusion_matrix(y_test_clf, y_pred_clf_hinge) conf_matrix_log = confusion_matrix(y_test_clf, y_pred_clf_log) # Create and train SGDRegressor with squared_error loss reg_squared = make_pipeline(StandardScaler(), SGDRegressor(loss=\\"squared_error\\", penalty=\\"l2\\", max_iter=1000, tol=1e-3)) reg_squared.fit(X_train_reg, y_train_reg) y_pred_reg_squared = reg_squared.predict(X_test_reg) # Create and train SGDRegressor with huber loss reg_huber = make_pipeline(StandardScaler(), SGDRegressor(loss=\\"huber\\", penalty=\\"l1\\", max_iter=1000, tol=1e-3)) reg_huber.fit(X_train_reg, y_train_reg) y_pred_reg_huber = reg_huber.predict(X_test_reg) # Evaluation for regression mse_squared = mean_squared_error(y_test_reg, y_pred_reg_squared) mse_huber = mean_squared_error(y_test_reg, y_pred_reg_huber) # Outputs for necessary metrics classification_results = { \\"accuracy_hinge\\": accuracy_hinge, \\"conf_matrix_hinge\\": conf_matrix_hinge, \\"accuracy_log\\": accuracy_log, \\"conf_matrix_log\\": conf_matrix_log } regression_results = { \\"mse_squared\\": mse_squared, \\"mse_huber\\": mse_huber } classification_results, regression_results"},{"question":"# Question 1: Working with CUDA Streams and Synchronization in PyTorch Objective: Demonstrate the ability to manage GPU resources and asynchronous operations using CUDA streams and synchronization in PyTorch. Problem Statement: Implement a function `run_cuda_operations` that performs the following steps: 1. Create three tensors `a`, `b`, and `c` of size (1024, 1024) with random values, placing them on different GPUs: - `a` on GPU 0 - `b` on GPU 1 - `c` on GPU 2 2. Create a CUDA stream for each tensor: - Stream `stream0` for `a` on GPU 0 - Stream `stream1` for `b` on GPU 1 - Stream `stream2` for `c` on GPU 2 3. Perform a matrix multiplication (`a @ a.T`, `b @ b.T`, and `c @ c.T`) in parallel using the respective streams. Ensure synchronization correctly so that the multiplication is performed only when previous operations on that stream has finished. 4. Transfer the resulting matrices back to the CPU, and return them as a tuple `(result_a, result_b, result_c)`. Constraints: - Use the `torch.cuda.stream` context manager to manage streams. - Ensure that the operations and synchronizations are correctly handled to avoid errors related to concurrent GPU usage. Function Signature: ```python def run_cuda_operations() -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: pass ``` Input: - No input parameters are required for this function. Output: - A tuple of three tensors `(result_a, result_b, result_c)` containing the results of the matrix multiplications performed on different GPUs and transferred back to the CPU. Example: ```python result_a, result_b, result_c = run_cuda_operations() print(result_a.shape) # Expected output: torch.Size([1024, 1024]) print(result_b.shape) # Expected output: torch.Size([1024, 1024]) print(result_c.shape) # Expected output: torch.Size([1024, 1024]) ``` Additional Notes: - Consider using `torch.cuda.synchronize` to handle the synchronization between streams and transfers. - The matrices should be allocated on three different GPUs using `torch.device(\'cuda:x\')` where `x` is the device index. - Using `torch.matmul` for matrix multiplication is recommended.","solution":"import torch from typing import Tuple def run_cuda_operations() -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: # Ensure that the user\'s system has at least 3 GPUs if torch.cuda.device_count() < 3: raise RuntimeError(\\"This operation requires at least 3 CUDA-capable GPUs\\") # Data allocation on different GPUs device0 = torch.device(\'cuda:0\') device1 = torch.device(\'cuda:1\') device2 = torch.device(\'cuda:2\') a = torch.randn(1024, 1024, device=device0) b = torch.randn(1024, 1024, device=device1) c = torch.randn(1024, 1024, device=device2) # Creating CUDA streams stream0 = torch.cuda.Stream(device0) stream1 = torch.cuda.Stream(device1) stream2 = torch.cuda.Stream(device2) with torch.cuda.stream(stream0): result_a = torch.matmul(a, a.t()) with torch.cuda.stream(stream1): result_b = torch.matmul(b, b.t()) with torch.cuda.stream(stream2): result_c = torch.matmul(c, c.t()) # Synchronize to ensure all operations are complete before transferring back to CPU torch.cuda.synchronize(device=device0) torch.cuda.synchronize(device=device1) torch.cuda.synchronize(device=device2) result_a_cpu = result_a.cpu() result_b_cpu = result_b.cpu() result_c_cpu = result_c.cpu() return result_a_cpu, result_b_cpu, result_c_cpu"},{"question":"**Custom Serialization and Deserialization with Python\'s `pickle` Module** # Problem Statement You are required to serialize and deserialize a complex Python data structure using custom serialization behavior. Specifically, you need to handle an external object referenced within a class instance and ensure it is restored correctly upon deserialization. Your task involves creating a custom `Pickler` and `Unpickler` to achieve this. # Detailed Instructions 1. Define a `User` class that holds user information such as `id`, `name`, and a `profile` object. 2. Define a `Profile` class containing fields `profile_id`, `email`, and `address`. 3. Implement a custom `Pickler` class (`UserPickler`) that can handle serialization of `User` instances, where it creates persistent references for `Profile` instances instead of serializing them directly. 4. Implement a custom `Unpickler` class (`UserUnpickler`) that can correctly restore the `User` instances by resolving the persistent references to `Profile` instances using an external database or a lookup dictionary. 5. Ensure that `Profile` instances are not duplicated during the deserialization process and are shared correctly if multiple `User` objects refer to the same `Profile`. # Constraints - You can use a simple in-memory dictionary as the storage for `Profile` objects during the deserialization process. - Handle any necessary updates to the `Profile` class instances upon deserialization. # Example Given the following user data structure: ```python users = [ User(id=1, name=\'Alice\', profile=Profile(profile_id=101, email=\'alice@example.com\', address=\'123 Maple Street\')), User(id=2, name=\'Bob\', profile=Profile(profile_id=102, email=\'bob@example.com\', address=\'456 Oak Avenue\')), User(id=3, name=\'Charlie\', profile=Profile(profile_id=101, email=\'alice@example.com\', address=\'123 Maple Street\')), # Shared Profile ] ``` Upon serialization and deserialization, ensure the shared `Profile` object (profile_id=101) is not duplicated. # Implementation You need to implement the following: 1. **Classes for User and Profile**: ```python class Profile: def __init__(self, profile_id, email, address): self.profile_id = profile_id self.email = email self.address = address class User: def __init__(self, id, name, profile): self.id = id self.name = name self.profile = profile ``` 2. **Custom Pickler and Unpickler**: ```python import pickle class UserPickler(pickle.Pickler): def __init__(self, file, profile_lookup): super().__init__(file) self.profile_lookup = profile_lookup def persistent_id(self, obj): if isinstance(obj, Profile): return obj.profile_id return None class UserUnpickler(pickle.Unpickler): def __init__(self, file, profile_lookup): super().__init__(file) self.profile_lookup = profile_lookup def persistent_load(self, pid): if pid in self.profile_lookup: return self.profile_lookup[pid] raise pickle.UnpicklingError(f\\"Profile with id {pid} not found\\") ``` 3. **Serialization and Deserialization Functions**: ```python def serialize_users(users, profile_lookup, file_path): with open(file_path, \'wb\') as f: pickler = UserPickler(f, profile_lookup) pickler.dump(users) def deserialize_users(profile_lookup, file_path): with open(file_path, \'rb\') as f: unpickler = UserUnpickler(f, profile_lookup) return unpickler.load() ``` # Example Usage ```python # Define the profiles and users profile_lookup = { 101: Profile(profile_id=101, email=\'alice@example.com\', address=\'123 Maple Street\'), 102: Profile(profile_id=102, email=\'bob@example.com\', address=\'456 Oak Avenue\') } users = [ User(id=1, name=\'Alice\', profile=profile_lookup[101]), User(id=2, name=\'Bob\', profile=profile_lookup[102]), User(id=3, name=\'Charlie\', profile=profile_lookup[101]), # Shared Profile ] # Serialize serialize_users(users, profile_lookup, \'users.pkl\') # Deserialize loaded_users = deserialize_users(profile_lookup, \'users.pkl\') # Validate the shared profile is not duplicated assert loaded_users[0].profile is loaded_users[2].profile ``` Implement the above classes and functions to demonstrate custom serialization and deserialization with `pickle`. # Note Make sure to handle errors gracefully and add appropriate error messages if the deserialization process encounters issues such as missing profiles.","solution":"import pickle class Profile: def __init__(self, profile_id, email, address): self.profile_id = profile_id self.email = email self.address = address class User: def __init__(self, id, name, profile): self.id = id self.name = name self.profile = profile class UserPickler(pickle.Pickler): def __init__(self, file, profile_lookup): super().__init__(file) self.profile_lookup = profile_lookup def persistent_id(self, obj): if isinstance(obj, Profile): return obj.profile_id return None class UserUnpickler(pickle.Unpickler): def __init__(self, file, profile_lookup): super().__init__(file) self.profile_lookup = profile_lookup def persistent_load(self, pid): if pid in self.profile_lookup: return self.profile_lookup[pid] raise pickle.UnpicklingError(f\\"Profile with id {pid} not found\\") def serialize_users(users, profile_lookup, file_path): with open(file_path, \'wb\') as f: pickler = UserPickler(f, profile_lookup) pickler.dump(users) def deserialize_users(profile_lookup, file_path): with open(file_path, \'rb\') as f: unpickler = UserUnpickler(f, profile_lookup) return unpickler.load()"},{"question":"**Problem Description:** You need to organize and manage a large set of files in different directories based on specific patterns. Your task is to write a function that, given a list of directory paths and a file pattern, finds and lists all files in those directories and their subdirectories that match the given pattern using Unix shell-style wildcards as supported by the `fnmatch` module. **Function Signature:** ```python def find_matching_files(directories: list, pattern: str) -> list: pass ``` **Input:** - `directories`: A list of strings where each string is a path to a directory. - `pattern`: A string representing the Unix shell-style wildcard pattern to match filenames. **Output:** - Returns a list of strings, where each string is the full path to a file that matches the given pattern. **Example:** ```python # Given the directory structure: # /home/user/docs/ #  project1 #   file1.txt #   file2.doc #   notes.md #  project2 #  file3.txt #  readme.md directories = [\'/home/user/docs/project1\', \'/home/user/docs/project2\'] pattern = \'*.txt\' result = find_matching_files(directories, pattern) # Expected Output: # [ # \'/home/user/docs/project1/file1.txt\', # \'/home/user/docs/project1/file2.txt\', # \'/home/user/docs/project2/file3.txt\' # ] ``` **Constraints:** - The directories in the input list might contain subdirectories, and the function should perform a recursive search. - Throws a ValueError if any directory in the list does not exist. - Consider using built-in libraries like `os` and `fnmatch` to implement this function. **Performance Requirements:** - The solution should efficiently handle a large number of files and deep directory structures. **Notes:** - Ensure that your solution is robust and handles cases where directories might not contain any matching files. - The function should be tested with various file patterns to ensure full coverage of functionality.","solution":"import os import fnmatch def find_matching_files(directories: list, pattern: str) -> list: if not directories: raise ValueError(\\"The list of directories should not be empty.\\") matching_files = [] for directory in directories: if not os.path.isdir(directory): raise ValueError(f\\"The directory \'{directory}\' does not exist.\\") for root, _, files in os.walk(directory): for filename in fnmatch.filter(files, pattern): matching_files.append(os.path.join(root, filename)) return matching_files"},{"question":"Objective To assess your understanding of functional programming tools in Python, you are required to write a function that processes a list of transactions to generate a summary report. You\'ll make use of the `itertools`, `functools`, and `operator` modules to achieve this. Problem Statement You are given a list of transactions where each transaction is represented as a tuple `(user_id, transaction_amount)`. Implement a function `summarize_transactions(transactions)` that computes and returns a summary report. The summary report should include: 1. The total amount of all transactions combined. 2. A dictionary of total transaction amounts per user, sorted by user_id. 3. The top 3 highest individual transaction amounts. **Input:** - A list of transactions where each transaction is a tuple `(user_id, transaction_amount)`. Each `user_id` is a unique integer and `transaction_amount` is a float. - The list can contain up to `10^6` transactions. **Output:** - A tuple containing: 1. Total transaction amount (float). 2. Dictionary with keys as `user_id` and values as their total transaction amounts, sorted by `user_id`. 3. A list of the top 3 highest transaction amounts (list of floats). **Constraints:** - The function should handle up to `10^6` transactions efficiently. - Use functional programming constructs such as those in `itertools`, `functools`, and `operator`. **Example:** ```python transactions = [ (1, 150.0), (2, 200.0), (1, 350.0), (3, 100.0), (2, 50.0), (4, 400.0) ] summarize_transactions(transactions) ``` **Output:** ```python (1250.0, {1: 500.0, 2: 250.0, 3: 100.0, 4: 400.0}, [400.0, 350.0, 200.0]) ``` Requirements: 1. **Use `itertools` to efficiently handle the transactions list as an iterator to avoid memory issues.** 2. **Use `functools.partial` or similar functionalities to create helper functions if necessary.** 3. **Use `operator` to perform sorting and other operations cleanly.** # Note Make sure your implementation can handle large inputs efficiently by leveraging iterators and functional programming paradigms effectively.","solution":"from collections import defaultdict from heapq import nlargest from functools import reduce, partial def summarize_transactions(transactions): # Initialize a dictionary to store total amounts per user user_totals = defaultdict(float) total_amount = 0.0 # Process each transaction for user_id, amount in transactions: user_totals[user_id] += amount total_amount += amount # Sorted user totals dictionary sorted_user_totals = dict(sorted(user_totals.items())) # Calculate the top 3 transaction amounts top_3_transactions = nlargest(3, (amount for _, amount in transactions)) return total_amount, sorted_user_totals, top_3_transactions"},{"question":"**Problem Statement: PyTorch Serialization Challenge** **Objective:** You are required to demonstrate your understanding of PyTorch serialization by performing the following tasks: 1. Create tensor objects and save them, preserving their view relationships. 2. Implement a custom PyTorch module, save its state_dict, and restore it. 3. Serialize the module using TorchScript and ensure it can be loaded back. **Task Details:** 1. **Tensor Serialization with View Relationships:** - Create a tensor of integers from 1 to 20 and another tensor that slices this tensor. - Save both tensors to a file. - Load the tensors back and modify the sliced tensor. Verify the modifications in the original tensor. 2. **Custom Module Serialization:** - Implement a custom PyTorch module named `CustomModel` with at least two layers. - Save the state_dict of `CustomModel` to a file. - Load the state_dict back into a new instance of `CustomModel` and confirm the state_dict matches. 3. **TorchScript Serialization:** - Script the `CustomModel` using TorchScript. - Save the scripted model to a file. - Load the scripted model back and demonstrate it performs inference correctly. **Constraints:** - Use only PyTorch and built-in Python libraries. - Ensure the tensor view relationships are maintained during serialization. - Document the steps clearly and provide sufficient comments in your code. **Function Signature:** ```python import torch from torch import nn def serialize_and_test_tensors(): # Task 1: Implement tensor serialization with view relationships pass class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.Linear(5, 2) def forward(self, x): x = torch.relu(self.layer1(x)) return self.layer2(x) def save_and_load_model_state(model: CustomModel, filepath: str): # Task 2: Implement saving and loading model state_dict pass def script_and_test_model(model: CustomModel, filepath: str): # Task 3: Implement TorchScript serialization pass if __name__ == \\"__main__\\": # Instantiate the model model = CustomModel() # Run tensor serialization test serialize_and_test_tensors() # Run model state_dict serialization test save_and_load_model_state(model, \'custom_model_state.pt\') # Run TorchScript serialization test script_and_test_model(model, \'custom_model_scripted.pt\') ``` **Expected Output:** - Demonstrate tensor view relationships are maintained after loading. - Confirm the state_dict is correctly saved and restored in a new instance of `CustomModel`. - Verify the scripted model can be saved, loaded, and used for inference.","solution":"import torch from torch import nn def serialize_and_test_tensors(): # Task 1: Implement tensor serialization with view relationships original_tensor = torch.arange(1, 21) sliced_tensor = original_tensor[5:15] # Save tensors to a file torch.save((original_tensor, sliced_tensor), \'tensors.pt\') # Load tensors back from the file loaded_original_tensor, loaded_sliced_tensor = torch.load(\'tensors.pt\') # Modify the sliced tensor loaded_sliced_tensor[0] = 100 return loaded_original_tensor, loaded_sliced_tensor class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.Linear(5, 2) def forward(self, x): x = torch.relu(self.layer1(x)) return self.layer2(x) def save_and_load_model_state(model: CustomModel, filepath: str): # Save the state_dict of CustomModel to a file torch.save(model.state_dict(), filepath) # Load the state_dict back into a new instance of CustomModel new_model = CustomModel() new_model.load_state_dict(torch.load(filepath)) # Confirm the state_dict matches return model.state_dict(), new_model.state_dict() def script_and_test_model(model: CustomModel, filepath: str): # Script the model using TorchScript scripted_model = torch.jit.script(model) # Save the scripted model to a file scripted_model.save(filepath) # Load the scripted model back loaded_scripted_model = torch.jit.load(filepath) # Test inference on loaded model test_input = torch.randn(1, 10) original_output = model(test_input) loaded_output = loaded_scripted_model(test_input) return original_output, loaded_output"},{"question":"# Threaded Task Manager in Python You are tasked with creating a simple threaded task manager using Python\'s `threading` module. This manager should be able to handle multiple tasks concurrently, ensuring that no more than a specified number of tasks are running at the same time. Each task will just print a message saying it has started, wait for a random short duration, and then announce its completion. Your implementation should involve creating a `TaskManager` class with the following requirements: 1. **Constructor (`__init__`) of TaskManager:** - Takes an integer `max_concurrent_tasks` specifying the maximum number of concurrent tasks. - Initializes any necessary synchronization mechanisms (like `Semaphore`). 2. **add_task method:** - Takes a callable `task` that performs some operation. - Adds the task to an internal queue and starts it if the number of running tasks is below the specified limit. 3. **run method:** - Continuously checks for and runs tasks until all tasks are completed. - Ensures that no more than `max_concurrent_tasks` tasks are running at any given time. - Utilizes `Semaphore` for managing the number of concurrently running tasks. 4. **ensure_task_completed method:** - Accepts a thread and waits for it to complete using `join()`. 5. **Example Usage:** - The code snippet provided shows an example of how to use the `TaskManager` to run multiple tasks. Please ensure to follow proper coding practices and handle any necessary synchronization within the code. Here is the function signature you must use: ```python import threading import time import random class TaskManager: def __init__(self, max_concurrent_tasks): # Your implementation here def add_task(self, task): # Your implementation here def run(self): # Your implementation here def ensure_task_completed(self, thread): # Your implementation here # Example task function to be managed def example_task(task_name): print(f\\"Task {task_name} started.\\") time.sleep(random.uniform(0.5, 2.0)) print(f\\"Task {task_name} completed.\\") # Example usage if __name__ == \\"__main__\\": manager = TaskManager(max_concurrent_tasks=3) tasks = [lambda: example_task(f\\"Task-{i}\\") for i in range(10)] for task in tasks: manager.add_task(task) manager.run() ``` # Constraints: - Do not modify the given function signatures. - Use synchronization mechanisms appropriately to ensure thread safety. - Your solution should work for any number of tasks and any valid `max_concurrent_tasks` value. # Expected Output: The program should start tasks and print their start and end in a non-deterministic order but should limit the number of concurrently running tasks to `max_concurrent_tasks`. # Example: ``` Task Task-0 started. Task Task-1 started. Task Task-2 started. Task Task-0 completed. Task Task-3 started. Task Task-1 completed. Task Task-4 started. ... ``` Ensure your implementation follows these instructions accurately.","solution":"import threading import time import random class TaskManager: def __init__(self, max_concurrent_tasks): self.max_concurrent_tasks = max_concurrent_tasks self.semaphore = threading.Semaphore(max_concurrent_tasks) self.tasks = [] self.task_threads = [] def add_task(self, task): self.tasks.append(task) def run(self): while self.tasks or any(thread.is_alive() for thread in self.task_threads): if len(self.task_threads) < self.max_concurrent_tasks and self.tasks: task = self.tasks.pop(0) thread = threading.Thread(target=self._run_task, args=(task,)) thread.start() self.task_threads.append(thread) self.task_threads = [thread for thread in self.task_threads if thread.is_alive()] time.sleep(0.1) def _run_task(self, task): with self.semaphore: task() def ensure_task_completed(self, thread): thread.join() def example_task(task_name): print(f\\"Task {task_name} started.\\") time.sleep(random.uniform(0.5, 2.0)) print(f\\"Task {task_name} completed.\\") if __name__ == \\"__main__\\": manager = TaskManager(max_concurrent_tasks=3) tasks = [lambda: example_task(f\\"Task-{i}\\") for i in range(10)] for task in tasks: manager.add_task(task) manager.run()"},{"question":"**Email Parsing and Content Extraction** As a mail server developer, you are tasked with implementing a utility that can process raw email data and extract specific information from it. The raw data can be in the form of bytes, a string, or a file. The utility must handle common email structures, including MIME-encoded messages. # Objectives 1. Implement a function `extract_email_details()` that parses raw email data. 2. Extract the sender\'s email address, the email subject, and the plain text body of the message. 3. Handle MIME-encoded messages, ensuring the plain text body is correctly extracted even if it\'s within a multipart message. 4. Log any defects or anomalies found during the parsing process. # Function Signature ```python def extract_email_details(email_data, data_format): Extract details from an email message. Parameters: email_data (bytes|string|file): The raw email data to be parsed. data_format (str): The format of the input data. It can be \'bytes\', \'string\', or \'file\'. Returns: dict: A dictionary with the keys \'sender\', \'subject\', \'body\', and \'defects\'. ``` # Input - `email_data`: The raw email data. The format can be bytes, a string, or a file object. - `data_format`: A string indicating the format of `email_data`. It can be: - `\'bytes\'` if `email_data` is a bytes-like object. - `\'string\'` if `email_data` is a string. - `\'file\'` if `email_data` is a file-like object. # Output - Return a dictionary containing: - `\'sender\'`: The email address of the sender. - `\'subject\'`: The subject of the email. - `\'body\'`: The plain text body of the email message. - `\'defects\'`: A list of any defects found during parsing. # Constraints 1. The email headers will always include `From` and `Subject`. 2. The message body can be plain text or multipart MIME encoded. 3. Ensure to handle different line endings (r, n, rn) in the raw message data. 4. If the MIME type is not `multipart`, the body directly contains the message text. # Example ```python from io import BytesIO # Example usage of extract_email_details function email_data_bytes = b\\"From: example@example.comrnSubject: Test EmailrnrnHello, this is a test email.\\" result = extract_email_details(email_data_bytes, \'bytes\') # Expected result: # { # \'sender\': \'example@example.com\', # \'subject\': \'Test Email\', # \'body\': \'Hello, this is a test email.\', # \'defects\': [] # } email_data_string = \\"From: example@example.comnSubject: Another Test EmailnnThis is another test message.\\" result = extract_email_details(email_data_string, \'string\') # Expected result: # { # \'sender\': \'example@example.com\', # \'subject\': \'Another Test Email\', # \'body\': \'This is another test message.\', # \'defects\': [] # } ``` # Notes - Use the appropriate classes (`BytesParser`, `Parser`) and methods (`parse`, `parsestr`, `parsebytes`) for parsing the email data based on the format specified in `data_format`. - For MIME messages, ensure to navigate the parts properly and extract the plain text content. - Log all defects found during the parsing process using the `defects` attribute of the message object.","solution":"import email from email.policy import default from email.parser import BytesParser, Parser def extract_email_details(email_data, data_format): Extract details from an email message. Parameters: email_data (bytes|string|file): The raw email data to be parsed. data_format (str): The format of the input data. It can be \'bytes\', \'string\', or \'file\'. Returns: dict: A dictionary with the keys \'sender\', \'subject\', \'body\', and \'defects\'. if data_format == \'bytes\': email_message = BytesParser(policy=default).parsebytes(email_data) elif data_format == \'string\': email_message = Parser(policy=default).parsestr(email_data) elif data_format == \'file\': email_message = BytesParser(policy=default).parse(email_data) else: raise ValueError(\\"Unsupported data format\\") sender = email_message.get(\'From\', \'\') subject = email_message.get(\'Subject\', \'\') defects = email_message.defects body = \'\' if email_message.is_multipart(): for part in email_message.iter_parts(): if part.get_content_type() == \'text/plain\': body = part.get_payload(decode=True).decode(part.get_content_charset(failobj=\'utf-8\')) break else: body = email_message.get_payload(decode=True).decode(email_message.get_content_charset(failobj=\'utf-8\')) return { \'sender\': sender, \'subject\': subject, \'body\': body, \'defects\': [str(defect) for defect in defects] }"},{"question":"Using **seaborn\'s blend_palette** function, write a function `create_custom_palette` that meets the following specifications: Function Signature ```python def create_custom_palette(colors: list, n_colors: int, as_cmap: bool = False): Create a custom color palette by blending between the given list of colors. Parameters: colors (list): A list of colors to blend. Colors can be specified in any format supported by seaborn (e.g., \\"b\\", \\"#123456\\", \\"xkcd:colorname\\"). n_colors (int): Number of colors to generate in the palette. as_cmap (bool): If True, return as a continuous colormap. Default is False (discrete palette). Returns: palette: A seaborn color palette or colormap. pass ``` Input - **colors**: A list containing at least two colors. Colors can be specified in the form of standard color names, hex codes, or xkcd color names. - **n_colors**: An integer specifying the number of colors to generate in the final palette. - **as_cmap**: A boolean to return the output as a continuous colormap if set to True, otherwise returns a list of discrete color values. Output - Returns a seaborn color palette if `as_cmap` is False, or a colormap if `as_cmap` is True. # Example ```python custom_palette = create_custom_palette([\\"#ff0000\\", \\"#00ff00\\", \\"#0000ff\\"], 5) # Expected output: A seaborn palette blended between the specified colors with 5 colors. custom_cmap = create_custom_palette([\\"b\\", \\"#ffdd44\\", \\"xkcd:sky blue\\"], 7, as_cmap=True) # Expected output: A seaborn continuous colormap blending between the specified colors. ``` # Constraints - The list of colors should have at least two colors. - The `n_colors` should be a positive integer greater than or equal to the length of the color list. - Ensure your function works correctly by handling various color formats. # Performance Requirements - The function should be optimized to handle typical usage within data visualization tasks, supporting practical values for `n_colors` (e.g., up to 256). Notes - Utilize the seaborn `blend_palette` function to achieve the implementation. - Ensure error handling for invalid inputs like color names that are not recognized by seaborn.","solution":"import seaborn as sns def create_custom_palette(colors, n_colors, as_cmap=False): Create a custom color palette by blending between the given list of colors. Parameters: colors (list): A list of colors to blend. Colors can be specified in any format supported by seaborn (e.g., \\"b\\", \\"#123456\\", \\"xkcd:colorname\\"). n_colors (int): Number of colors to generate in the palette. as_cmap (bool): If True, return as a continuous colormap. Default is False (discrete palette). Returns: palette: A seaborn color palette or colormap. if not isinstance(colors, list) or len(colors) < 2: raise ValueError(\\"Colors must be a list with at least two colors.\\") if not isinstance(n_colors, int) or n_colors < len(colors): raise ValueError(\\"n_colors must be an integer >= the length of the colors list.\\") try: blend_palette = sns.color_palette(colors).as_hex() except ValueError as e: raise ValueError(f\\"One or more colors are not recognized: {e}\\") palette = sns.blend_palette(blend_palette, n_colors=n_colors, as_cmap=as_cmap) return palette"},{"question":"Objective Write a Python program that uses the `ipaddress` module to analyze IP addresses and networks. Your task is to implement a function that takes a list of IP addresses and their corresponding network masks, and then groups these addresses into the smallest possible set of non-overlapping networks. The function should also determine and return whether any address from the input list falls within private IP ranges. Function Specification Implement the following function in Python: ```python def analyze_ip_addresses(ip_list): Analyze and group IP addresses into the smallest set of non-overlapping networks. Parameters: ip_list (list): A list of tuples, each containing an IP address (str) and a network mask (str). Returns: dict: A dictionary containing: - \\"networks\\": A list of grouped non-overlapping networks (as strings). - \\"has_private\\": A boolean indicating if any of the input IP addresses fall within private IP ranges. pass ``` Input - `ip_list`: A list of tuples where each tuple consists of an IP address and a network mask (as strings). For example: ```python [ (\\"192.168.0.1\\", \\"255.255.255.0\\"), (\\"10.0.0.1\\", \\"255.0.0.0\\"), (\\"172.16.0.1\\", \\"255.240.0.0\\") ] ``` Output - The function should return a dictionary with two keys: - `\\"networks\\"`: A list of strings representing the smallest set of non-overlapping networks. - `\\"has_private\\"`: A boolean indicating whether any of the input IP addresses fall within private IP ranges. Constraints - The input list will contain valid IP addresses and network masks. - The function should handle both IPv4 and IPv6 addresses. - The size of the input list will be in the range [1, 1000]. Example ```python ip_list = [ (\\"192.168.0.1\\", \\"255.255.255.0\\"), (\\"10.0.0.1\\", \\"255.0.0.0\\"), (\\"172.16.0.1\\", \\"255.240.0.0\\"), (\\"192.168.1.1\\", \\"255.255.255.0\\") ] result = analyze_ip_addresses(ip_list) # Expected output: # { # \\"networks\\": [\\"10.0.0.0/8\\", \\"172.16.0.0/12\\", \\"192.168.0.0/23\\"], # \\"has_private\\": True # } ``` Performance Requirements - The implementation should efficiently handle the grouping of networks and detection of private IP ranges. Use of efficient algorithms and data structures is encouraged to ensure the solution scales for the maximum input size. Notes - The `ipaddress` module provides functions and classes to facilitate IP address manipulation, such as `ip_network`, `collapse_addresses`, and checking properties like `is_private`. - Consider edge cases where networks might overlap or fall within each other. Hints - Use `ipaddress.ip_network` to create network objects from the IPs and masks. - Utilize `ipaddress.collapse_addresses` to combine overlapping networks. - Check each IP address\'s `is_private` attribute to determine the presence of private IPs.","solution":"import ipaddress def analyze_ip_addresses(ip_list): Analyze and group IP addresses into the smallest set of non-overlapping networks. Parameters: ip_list (list): A list of tuples, each containing an IP address (str) and a network mask (str). Returns: dict: A dictionary containing: - \\"networks\\": A list of grouped non-overlapping networks (as strings). - \\"has_private\\": A boolean indicating if any of the input IP addresses fall within private IP ranges. networks = [] has_private = False for ip, mask in ip_list: net = ipaddress.ip_network(f\\"{ip}/{mask}\\", strict=False) networks.append(net) if net.is_private: has_private = True # Collapse overlapping networks into the smallest possible set of non-overlapping networks collapsed_networks = ipaddress.collapse_addresses(networks) collapsed_networks_str = [str(network) for network in collapsed_networks] return {\\"networks\\": collapsed_networks_str, \\"has_private\\": has_private}"},{"question":"# PyTorch Broadcasting Challenge Objective: Create a function that takes two tensors as inputs and returns their broadcasted sum. Additionally, ensure that the function correctly handles broadcasting semantics and raises an appropriate error if the tensors are not broadcastable. Function Signature: ```python def broadcasted_sum(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: pass ``` Input: - `tensor_a` (torch.Tensor): A PyTorch tensor of any shape. - `tensor_b` (torch.Tensor): Another PyTorch tensor of any shape. Output: - Returns a new PyTorch tensor which is the result of adding `tensor_a` and `tensor_b` after applying broadcasting. Constraints: - The function should internally check if tensors are broadcastable according to PyTorch broadcasting rules. - Raise a `ValueError` with a descriptive message if the tensors are not broadcastable. - Do not use any loops. Utilize PyTorch\'s built-in operations to achieve the result. Examples: ```python >>> import torch >>> a = torch.empty(5, 3, 4, 1) >>> b = torch.empty(3, 1, 1) >>> result = broadcasted_sum(a, b) >>> result.size() torch.Size([5, 3, 4, 1]) >>> a = torch.empty(5, 2, 4, 1) >>> b = torch.empty(3, 1, 1) >>> result = broadcasted_sum(a, b) ValueError: The dimensions of tensor_a and tensor_b are not broadcastable. ``` Notes: - Make sure to include comprehensive error handling for cases where broadcasting is not possible. - You may use `torch.add` to perform the addition and size alignment operations. Performance: Ensure your solution is efficient and leverages PyTorch\'s optimized operations without unnecessarily duplicating data.","solution":"import torch def broadcasted_sum(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: Returns the sum of tensor_a and tensor_b after broadcasting. Raises a ValueError if the tensors are not broadcastable. try: result = tensor_a + tensor_b except RuntimeError: raise ValueError(\\"The dimensions of tensor_a and tensor_b are not broadcastable.\\") return result"},{"question":"# Seaborn Bar Plot Visualization and Grouping You are provided with the \\"tips\\" dataset from the Seaborn library, which contains information about tips given in a restaurant, including variables such as the day of the week, the sex of the person, and the size of the party. Your task is to write a function that: 1. Loads the \\"tips\\" dataset from Seaborn. 2. Creates a bar plot showing the count of observations for each day of the week (`day`). 3. Creates a grouped bar plot showing the count of observations for each day of the week (`day`), grouped by the sex of the person (`sex`). 4. Creates a bar plot showing the count of observations for different party sizes (`size`). # Function Signature ```python def create_seaborn_plots() -> None: pass ``` # Requirements 1. Use the `seaborn.objects` module and specifically the `so.Plot` object to create the plots. 2. Ensure that all plots are displayed along with an appropriate title for each plot. 3. You do not need to return anything from the function. The function should simply display the plots. # Example Output The function should, when executed, display three plots: 1. A bar plot showing the count of observations for each day of the week. 2. A grouped bar plot showing the count of observations for each day of the week, grouped by the sex. 3. A bar plot showing the count of observations for different party sizes. **Note**: You can use additional customization options from Seaborn to enhance the appearance of the plots if desired. # Hints - Refer to the Seaborn documentation for additional customization options for plots. - Use the `so.Plot` syntax shown in the provided documentation snippets to create the required plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_seaborn_plots() -> None: # Load the tips dataset from Seaborn tips = sns.load_dataset(\\"tips\\") # Create a bar plot showing the count of observations for each day of the week plt.figure(figsize=(8,6)) sns.countplot(data=tips, x=\'day\').set(title=\'Count of Observations for Each Day of the Week\') plt.show() # Create a grouped bar plot showing the count of observations for each day of the week, grouped by the sex of the person plt.figure(figsize=(8,6)) sns.countplot(data=tips, x=\'day\', hue=\'sex\').set(title=\'Count of Observations for Each Day of the Week Grouped by Sex\') plt.show() # Create a bar plot showing the count of observations for different party sizes plt.figure(figsize=(8,6)) sns.countplot(data=tips, x=\'size\').set(title=\'Count of Observations for Different Party Sizes\') plt.show()"},{"question":"Problem You have been given a list of employee records, where each record is represented as a dictionary with the following keys: - \'name\': A string representing the employee\'s name. - \'department\': A string representing the department the employee belongs to. - \'salary\': An integer representing the employee\'s salary. - \'age\': An integer representing the employee\'s age. You need to implement a function `sort_employees` that sorts this list based on the following criteria: 1. First, by \'department\' alphabetically. 2. Within each department, by \'salary\' in ascending order. 3. For records with the same salary, by \'age\' in descending order. 4. Finally, for records with the same age, by \'name\' alphabetically. The function should use both `sorted` and `list.sort` methods where appropriate and demonstrate the usage of custom key functions and the `operator` module functions. # Input - A list of dictionaries, where each dictionary contains the following key-value pairs: - \'name\': a string - \'department\': a string - \'salary\': an integer - \'age\': an integer # Output - A sorted list of dictionaries, based on the criteria specified. # Example ```python employees = [ {\'name\': \'Alice\', \'department\': \'HR\', \'salary\': 50000, \'age\': 30}, {\'name\': \'Bob\', \'department\': \'HR\', \'salary\': 70000, \'age\': 34}, {\'name\': \'Charlie\', \'department\': \'IT\', \'salary\': 60000, \'age\': 28}, {\'name\': \'David\', \'department\': \'IT\', \'salary\': 60000, \'age\': 32}, {\'name\': \'Eve\', \'department\': \'Finance\', \'salary\': 70000, \'age\': 40} ] sorted_employees = sort_employees(employees) print(sorted_employees) ``` Expected Output: ```python [ {\'name\': \'Eve\', \'department\': \'Finance\', \'salary\': 70000, \'age\': 40}, {\'name\': \'Alice\', \'department\': \'HR\', \'salary\': 50000, \'age\': 30}, {\'name\': \'Bob\', \'department\': \'HR\', \'salary\': 70000, \'age\': 34}, {\'name\': \'David\', \'department\': \'IT\', \'salary\': 60000, \'age\': 32}, {\'name\': \'Charlie\', \'department\': \'IT\', \'salary\': 60000, \'age\': 28} ] ``` # Constraints - The number of records in the input list will not exceed 1000. - Employee names, department names, and ages are guaranteed to be unique. # Implementation ```python from operator import itemgetter def sort_employees(employees): # First sort by name to enforce final stability on name as we move right-to-left in criteria employees.sort(key=itemgetter(\'name\')) # Next sort by age in descending order employees.sort(key=itemgetter(\'age\'), reverse=True) # Next sort by salary in ascending order employees.sort(key=itemgetter(\'salary\')) # Finally sort by department in ascending order employees.sort(key=itemgetter(\'department\')) return employees ```","solution":"from operator import itemgetter def sort_employees(employees): Sorts the given list of employee records based on specified criteria: 1. By \'department\' alphabetically. 2. By \'salary\' in ascending order within each department. 3. By \'age\' in descending order for same salary within each department. 4. By \'name\' alphabetically for same age and salary within each department. # First sort by name to enforce final stability on name as we move right-to-left in criteria employees.sort(key=itemgetter(\'name\')) # Next sort by age in descending order employees.sort(key=itemgetter(\'age\'), reverse=True) # Next sort by salary in ascending order employees.sort(key=itemgetter(\'salary\')) # Finally sort by department in ascending order employees.sort(key=itemgetter(\'department\')) return employees"},{"question":"# Custom Iterator Implementation and Usage In Python, iterators must implement two methods: `__iter__()` to return the iterator object itself and `__next__()` to return the next item from the sequence. We will extend this concept by asking you to create a class that behaves like a custom iterator. Task You are required to implement a custom iterator class `CustomRangeIterator`: 1. **Initialization**: - The iterator should be initialized with two parameters: `start` (mandatory) and `end` (mandatory). 2. **Functionality**: - The `__iter__` method should return the iterator object itself. - The `__next__` method should return the next integer in the range from `start` to `end`, inclusive. - If the iterator reaches `end`, it should raise a `StopIteration` exception. - There should be a method `reset()` which reinitializes the iterator to start from the beginning. 3. **Constraints**: - You may assume `start` and `end` are integers where `start <= end`. Implement the `CustomRangeIterator` class as specified and demonstrate usage with example code. # Input/Output Specifications * **Input**: No direct input. * **Output**: - Implementation of the class `CustomRangeIterator`. - Demonstration code that shows: - Iterating through the range. - Resetting the iterator. - Handling the `StopIteration` correctly. # Example ```python # Example usage iterator = CustomRangeIterator(1, 5) for num in iterator: print(num) # Should print numbers 1 through 5 iterator.reset() for num in iterator: print(num) # Should again print numbers 1 through 5 ``` Provide the implementation of the `CustomRangeIterator` class and the demonstration code.","solution":"class CustomRangeIterator: def __init__(self, start, end): self.start = start self.end = end self.current = start def __iter__(self): # The __iter__ method returns the iterator object itself return self def __next__(self): # If current is greater than end, raise StopIteration if self.current > self.end: raise StopIteration # Store the current value to return current_value = self.current # Increment the current value self.current += 1 return current_value def reset(self): # Reinitialize the current value to start self.current = self.start # Demonstration code iterator = CustomRangeIterator(1, 5) for num in iterator: print(num) # Should print numbers 1 through 5 iterator.reset() for num in iterator: print(num) # Should again print numbers 1 through 5"},{"question":"**Python Coding Assessment Question:** # Serialization and Deserialization with Marshal The `marshal` module in Python allows for the serialization (marshaling) and deserialization (unmarshaling) of Python objects into a specific binary format. This is particularly useful for saving intermediate states of computations or for transferring certain object structures between different execution contexts while preserving their Python-specific representation. In this task, you are required to implement a function that saves a dictionary of computational results to a binary file and subsequently reads it back, ensuring data integrity through the process. # Function Requirements 1. **Function Name**: `save_and_load_results` 2. **Parameters**: - `results_dict` (dict): A dictionary where the keys are string identifiers and the values are tuples consisting of: - An integer timestamp, - A floating point computational result, - A string description of the computation. - `file_path` (str): Path to the binary file where the results should be saved. 3. **Expected Behavior**: - The function should serialize (`marshal.dump`) the given dictionary (`results_dict`) into the specified binary file (`file_path`). - The function should then deserialize (`marshal.load`) the data back from the file. - The function should ensure that the data read back from the file matches the original dictionary. 4. **Return Value**: - The function should return the deserialized data structure from the binary file, which should match the original input dictionary. 5. **Constraints**: - Do not use any modules other than `marshal` and Python\'s standard file I/O functions. - Handle any potential exceptions that could arise during file operations, ensuring the function handles errors gracefully and provides meaningful error messages. # Example Usage: ```python def save_and_load_results(results_dict, file_path): Save the results dictionary to a binary file and load it back. :param results_dict: dict - Dictionary with computational results. Example: {\'task1\': (1618033988, 3.14, \'Pi Computation\')} :param file_path: str - Path to the binary file. :return: dict - The dictionary loaded from the binary file. import marshal try: # Step 1: Serialize the dictionary to the file with open(file_path, \'wb\') as file: marshal.dump(results_dict, file) # Step 2: Deserialize the dictionary from the file with open(file_path, \'rb\') as file: loaded_results = marshal.load(file) return loaded_results except (ValueError, EOFError, TypeError) as e: raise RuntimeError(f\\"Error during marshaling operations: {e}\\") # Example usage results = {\\"comp1\\": (1622547802, 7.389, \\"Exponential Computation\\"), \\"comp2\\": (1622547803, 2.718, \\"Euler\'s Number Computation\\")} file_path = \\"results.bin\\" restored_results = save_and_load_results(results, file_path) assert results == restored_results ``` # Additional Notes: - Ensure that the deserialized output matches the original input dictionary exactly. - Provide error messages for failed serialization/deserialization attempts.","solution":"import marshal def save_and_load_results(results_dict, file_path): Save the results dictionary to a binary file and load it back. :param results_dict: dict - Dictionary with computational results. Example: {\'task1\': (1618033988, 3.14, \'Pi Computation\')} :param file_path: str - Path to the binary file. :return: dict - The dictionary loaded from the binary file. try: # Serialize the dictionary to the file with open(file_path, \'wb\') as file: marshal.dump(results_dict, file) # Deserialize the dictionary from the file with open(file_path, \'rb\') as file: loaded_results = marshal.load(file) return loaded_results except (ValueError, EOFError, TypeError) as e: raise RuntimeError(f\\"Error during marshaling operations: {e}\\")"},{"question":"Your task is to create a Python function called `analyze_robots_txt(url, useragent)`. This function should take in two parameters: 1. `url` (string): The URL of the website whose `robots.txt` file you want to analyze. 2. `useragent` (string): The user agent for which you want to retrieve information about access permissions, crawl delay, request rate, and sitemaps from the `robots.txt` file. The function should perform the following steps: 1. Instantiate a `RobotFileParser` object and set its URL to the `robots.txt` file of the provided website URL. 2. Read and parse the `robots.txt` file. 3. Retrieve the crawl delay, request rate, and sitemap URLs for the provided user agent. 4. Return a dictionary with the following keys and corresponding values: - `\\"can_fetch\\"`: A boolean indicating whether the provided user agent is allowed to fetch the base URL. - `\\"crawl_delay\\"`: The crawl delay for the provided user agent (or `None` if not specified). - `\\"request_rate\\"`: A tuple with the request rate (requests, seconds) for the provided user agent (or `None` if not specified). - `\\"sitemaps\\"`: A list of sitemap URLs specified in the `robots.txt` file (or `None` if not specified). Input Format - `url` (string): The base URL of the website (e.g., \\"http://www.example.com\\"). - `useragent` (string): The user agent string (e.g., \\"Googlebot\\"). Output Format - A dictionary with the keys `\\"can_fetch\\"`, `\\"crawl_delay\\"`, `\\"request_rate\\"`, and `\\"sitemaps\\"` as described above. Constraints - The `robots.txt` file may not always contain all the parameters. - Handle any potential errors in fetching and parsing the `robots.txt` file gracefully. Example ```python def analyze_robots_txt(url, useragent): # Your implementation here pass # Example usage: result = analyze_robots_txt(\\"http://www.musi-cal.com\\", \\"*\\") print(result) ``` Expected output (based on the given example in the documentation): ```python { \\"can_fetch\\": True, \\"crawl_delay\\": 6, \\"request_rate\\": (3, 20), \\"sitemaps\\": None } ```","solution":"import urllib.robotparser import urllib.parse def analyze_robots_txt(url, useragent): Analyze the robots.txt file for the given URL and user agent. parsed_url = urllib.parse.urlparse(url) robots_url = urllib.parse.urljoin(f\\"{parsed_url.scheme}://{parsed_url.netloc}\\", \\"/robots.txt\\") rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_url) try: rp.read() except: return { \\"can_fetch\\": False, \\"crawl_delay\\": None, \\"request_rate\\": None, \\"sitemaps\\": None } can_fetch = rp.can_fetch(useragent, url) crawl_delay = rp.crawl_delay(useragent) request_rate = rp.request_rate(useragent) sitemaps = rp.site_maps() return { \\"can_fetch\\": can_fetch, \\"crawl_delay\\": crawl_delay, \\"request_rate\\": request_rate, \\"sitemaps\\": sitemaps }"},{"question":"**Coding Assessment Question:** # Task Write a Python function using Seaborn named `enhanced_regression_analysis` that takes a DataFrame, and visualizes various regression fits based on the specified parameters. # Function Signature ```python def enhanced_regression_analysis(df: pd.DataFrame, x: str, y: str, hue: str = None, col: str = None, row: str = None, order: int = 1, robust: bool = False, logistic: bool = False, lowess: bool = False, y_jitter: float = 0, x_jitter: float = 0, ci: float = 95, kind: str = \\"scatter\\") -> None: pass ``` # Inputs - `df` (pd.DataFrame): The input DataFrame containing the data. - `x` (str): The name of the independent variable column. - `y` (str): The name of the dependent variable column. - `hue` (str, optional): The name of a categorical variable column to group by color. Default is None. - `col` (str, optional): The name of a categorical variable column to create columns of plots. Default is None. - `row` (str, optional): The name of a categorical variable column to create rows of plots. Default is None. - `order` (int): The order of the polynomial to fit. Default is 1 (linear regression). - `robust` (bool): Whether to use robust regression. Default is False. - `logistic` (bool): Whether to use logistic regression for binary outcomes. Default is False. - `lowess` (bool): Whether to use lowess smoothing for non-parametric regression. Default is False. - `y_jitter` (float): Amount of jitter to add to the `y` variable for better visualization of discrete values. Default is 0. - `x_jitter` (float): Amount of jitter to add to the `x` variable for better visualization of discrete values. Default is 0. - `ci` (float): Confidence interval width. Default is 95%. - `kind` (str): The type of plot to draw for the relationships (\'scatter\', \'reg\', \'resid\', \'kde\', \'hex\'). Default is \'scatter\'. # Outputs - The function should not return anything. Instead, it should display the appropriate plot(s) based on the provided inputs. # Constraints - You must use Seaborn for creating the plots. - Ensure that the function allows for the combination of `hue`, `col`, and `row` parameters using `lmplot`. # Example Usage ```python import seaborn as sns import pandas as pd # Load example dataset tips = sns.load_dataset(\\"tips\\") # Example call to the function enhanced_regression_analysis( df=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", col=\\"time\\", row=\\"sex\\", order=2, robust=True, logistic=False, lowess=False, y_jitter=0.1, x_jitter=0.1, ci=90, kind=\\"reg\\" ) ``` # Evaluation Criteria - **Correctness**: The function should correctly interpret and visualize the inputs. - **Efficiency**: The function should handle large datasets efficiently. - **Clarity**: The visualizations should be clear and accurately represent the data. - **Flexibility**: The function should handle various types of regression (linear, polynomial, robust, logistic, non-parametric) as specified by the parameters. Happy coding!","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def enhanced_regression_analysis( df: pd.DataFrame, x: str, y: str, hue: str = None, col: str = None, row: str = None, order: int = 1, robust: bool = False, logistic: bool = False, lowess: bool = False, y_jitter: float = 0, x_jitter: float = 0, ci: float = 95, kind: str = \\"scatter\\" ) -> None: Visualizes various regression fits using seaborn\'s lmplot based on the specified parameters. Parameters ---------- df : pd.DataFrame The input DataFrame containing the data. x : str The name of the independent variable column. y : str The name of the dependent variable column. hue : str, optional The name of a categorical variable column to group by color. Default is None. col : str, optional The name of a categorical variable column to create columns of plots. Default is None. row : str, optional The name of a categorical variable column to create rows of plots. Default is None. order : int The order of the polynomial to fit. Default is 1 (linear regression). robust : bool Whether to use robust regression. Default is False. logistic : bool Whether to use logistic regression for binary outcomes. Default is False. lowess : bool Whether to use lowess smoothing for non-parametric regression. Default is False. y_jitter : float Amount of jitter to add to the y variable for better visualization of discrete values. Default is 0. x_jitter : float Amount of jitter to add to the x variable for better visualization of discrete values. Default is 0. ci : float Confidence interval width. Default is 95%. kind : str The type of plot to draw for the relationships (scatter, reg, resid, kde, hex). Default is scatter. sns.lmplot( data=df, x=x, y=y, hue=hue, col=col, row=row, order=order, robust=robust, logistic=logistic, lowess=lowess, y_jitter=y_jitter, x_jitter=x_jitter, ci=ci, palette=\\"muted\\", scatter_kws={\\"s\\": 50, \\"alpha\\": 1}, markers=\\"o\\", height=5, aspect=1.2, kind=kind ) plt.show()"},{"question":"# Question: XML Parsing using `xml.sax` You are required to parse XML data using the `xml.sax` module in Python. Your task is to implement a SAX parser that reads data from an XML string and extracts specific information. Particularly, you need to count the number of occurrences of each element in the XML content and handle any parsing errors gracefully. Requirements: 1. Implement a class `ElementCounterHandler` that inherits from `xml.sax.handler.ContentHandler`. 2. Within the `ElementCounterHandler` class, override the `startElement` and `endElement` methods to maintain a count of elements. 3. Implement a class `CustomErrorHandler` that inherits from `xml.sax.handler.ErrorHandler` to handle any parsing errors. 4. Write a function `count_elements(xml_string)` which: - Accepts an XML string as input. - Uses the `ElementCounterHandler` and `CustomErrorHandler` to parse the XML string. - Returns a dictionary with element names as keys and their occurrence counts as values. - Handles any SAX parsing exceptions by returning a string message indicating an error occurred. Constraints: - The XML string will always be well-formed and valid. - You should not rely on external libraries other than `xml.sax`. Input: - An XML string containing multiple nested elements. Output: - A dictionary with element names as keys and their occurrence counts as values. - If a parsing error occurs, return a string message: `\\"Parsing Error\\"`. Example: ```python xml_string = <root> <child> <subchild>Data</subchild> <subchild>Data</subchild> </child> <child> <subchild>Data</subchild> </child> </root> result = count_elements(xml_string) print(result) # Output should be {\'root\': 1, \'child\': 2, \'subchild\': 3} ``` Implementation Notes: - Use `xml.sax.make_parser()` to create the parser. - Implement the `startElement` and `endElement` methods to count the elements. - Implement the `error`, `fatalError`, and `warning` methods in `CustomErrorHandler` to handle errors appropriately.","solution":"import xml.sax from xml.sax.handler import ContentHandler, ErrorHandler from collections import defaultdict class ElementCounterHandler(ContentHandler): def __init__(self): self.element_counts = defaultdict(int) def startElement(self, name, attrs): self.element_counts[name] += 1 def endElement(self, name): pass # Not used, but required to override class CustomErrorHandler(ErrorHandler): def error(self, exception): pass # You can log or handle errors here def fatalError(self, exception): pass # You can log or handle fatal errors here def warning(self, exception): pass # You can log or handle warnings here def count_elements(xml_string): handler = ElementCounterHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.setErrorHandler(CustomErrorHandler()) try: xml.sax.parseString(xml_string, handler) return dict(handler.element_counts) except xml.sax.SAXException: return \\"Parsing Error\\""},{"question":"**Complexity Level**: Advanced **Title**: Customizing and Analyzing Seaborn Color Palettes **Question**: You are provided with a dataset and you need to create a visualization using seaborn while demonstrating your understanding of custom color palettes. # Problem Statement 1. Given a list of sales data for different products over a period of 12 months: ```python data = { \\"Product_A\\": [10, 23, 13, 45, 67, 78, 56, 65, 45, 34, 23, 12], \\"Product_B\\": [15, 25, 20, 35, 45, 55, 75, 85, 95, 105, 65, 50], \\"Product_C\\": [5, 7, 15, 25, 27, 30, 40, 35, 25, 15, 5, 2] } ``` 2. You need to write code that: - Uses seaborn to create a line plot showing the sales trend of each product over the months. - Customizes the color palette for the line plot using a gradient between two colors. - Generates and prints the hex codes for the colors used in your palette. - Ensures the custom palette is used consistently within the context of the plot. # Function Implementation Expected Input - The input is a dictionary containing sales data for different products. Expected Output - A line plot visualizing the sales trends of all products. - A list of hex codes representing the colors used in the plot. # Constraints - You must use seaborn to create the line plot. - The color palette should be a blend gradient between two specified colors. - The range of months (x-axis) should be labeled from \\"Month 1\\" to \\"Month 12\\". # Performance Requirements - The code should execute efficiently and generate the plot and color palette within a reasonable time. # Example Assuming the function is named `visualize_sales(data)`: ```python sales_data = { \\"Product_A\\": [10, 23, 13, 45, 67, 78, 56, 65, 45, 34, 23, 12], \\"Product_B\\": [15, 25, 20, 35, 45, 55, 75, 85, 95, 105, 65, 50], \\"Product_C\\": [5, 7, 15, 25, 27, 30, 40, 35, 25, 15, 5, 2] } visualize_sales(sales_data) ``` Expected Output - A line plot showing the sales trend for each product with a custom gradient color palette. - A list of hex codes for colors used in the plot. ```python [\'#0072B2\', \'#56B4E9\', \'#E69F00\', \'#F0E442\', \'#009E73\', \'#D55E00\', \'#CC79A7\'] ``` You are expected to write the `visualize_sales` function to accomplish the specified requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_sales(data): Visualizes the sales data for different products over a period of 12 months using seaborn and custom color palettes. Parameters: data (dict): Dictionary containing sales data for different products. Keys are product names and values are lists of sales over 12 months. Returns: list: List of hex codes of colors used in the plot. # Convert dictionary to DataFrame df = pd.DataFrame(data) df[\'Month\'] = [\'Month \' + str(i) for i in range(1, 13)] # Define custom palette from two colors custom_palette = sns.color_palette(\\"coolwarm\\", n_colors=len(data)) # Generate hex codes for the colors used hex_codes = custom_palette.as_hex() # Plotting using seaborn with the custom palette plt.figure(figsize=(12, 6)) sns.set_palette(custom_palette) melted_df = df.melt(\'Month\', var_name=\'Product\', value_name=\'Sales\') ax = sns.lineplot(data=melted_df, x=\'Month\', y=\'Sales\', hue=\'Product\', marker=\'o\') ax.set_title(\\"Sales Trends Over 12 Months\\") ax.set_xlabel(\\"Month\\") ax.set_ylabel(\\"Sales\\") plt.xticks(rotation=45) plt.tight_layout() plt.legend(title=\\"Product\\") plt.show() return hex_codes"},{"question":"# Advanced Coding Assessment Question **Objective:** Write a Python function using the `smtplib` module to send an email with both plain text and HTML content. The function should also handle different types of connections (regular SMTP, SSL, and TLS). Additionally, implement proper error handling for different possible exceptions. **Function Specification:** ```python def send_custom_email( smtp_server: str, port: int, sender_email: str, recipient_emails: list, subject: str, plain_text_body: str, html_body: str, username: str = None, password: str = None, use_ssl: bool = False, use_tls: bool = False ) -> dict: Sends an email with both plain text and HTML content using the specified SMTP server. Parameters: - smtp_server (str): The SMTP server address. - port (int): The port to use for the SMTP server. - sender_email (str): Sender\'s email address. - recipient_emails (list): List of recipient email addresses. - subject (str): Subject of the email. - plain_text_body (str): Plain text body of the email. - html_body (str): HTML body of the email. - username (str): SMTP authentication username (optional). - password (str): SMTP authentication password (optional). - use_ssl (bool): Whether to use SSL for the connection (default is False). - use_tls (bool): Whether to use STARTTLS for encryption (default is False). Returns: - dict: A dictionary indicating the result of the email sending operation. Keys are email addresses, values are either an error message or \'Sent\' (indicating that the email was successfully sent). pass ``` **Expected Input and Output:** - Input: - `smtp_server`: as a string - `port`: as an integer - `sender_email`: as a string - `recipient_emails`: as a list of strings - `subject`: as a string - `plain_text_body`: as a string - `html_body`: as a string - `username`: as a string (optional) - `password`: as a string (optional) - `use_ssl`: as a boolean (optional) - `use_tls`: as a boolean (optional) - Output: - A dictionary where the keys are recipient email addresses and the values are \'Sent\' if the email was successfully sent or an error message if an exception was raised. **Constraints:** - Ensure that at least one recipient is specified. - If both `use_ssl` and `use_tls` are set to `True`, prioritize SSL. - Handle edge cases where the SMTP server may not support TLS or authentication failures might occur. - Use Python\'s `email` module for building the email with both plain text and HTML components. **Performance Requirements:** - The function should efficiently handle connections and exceptions. - Make sure to close the connection after sending the email to avoid resource leaks. **Example Usage:** ```python result = send_custom_email( smtp_server=\'smtp.example.com\', port=587, sender_email=\'sender@example.com\', recipient_emails=[\'recipient1@example.com\', \'recipient2@example.com\'], subject=\'Test Email\', plain_text_body=\'This is a test email sent with plain text.\', html_body=\'<html><body><p>This is a test email with <b>HTML</b> content.</p></body></html>\', username=\'user\', password=\'password\', use_tls=True ) print(result) ``` **Expected Output:** ```python { \'recipient1@example.com\': \'Sent\', \'recipient2@example.com\': \'Sent\' } ``` or ```python { \'recipient1@example.com\': \'SMTPAuthenticationError: Authentication failed\', \'recipient2@example.com\': \'SMTPAuthenticationError: Authentication failed\' } ``` **Note:** - For testing purposes, you can use mock SMTP servers. - Make sure to implement comprehensive error handling as specified in the documentation.","solution":"import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText def send_custom_email( smtp_server: str, port: int, sender_email: str, recipient_emails: list, subject: str, plain_text_body: str, html_body: str, username: str = None, password: str = None, use_ssl: bool = False, use_tls: bool = False ) -> dict: Sends an email with both plain text and HTML content using the specified SMTP server. Parameters: - smtp_server (str): The SMTP server address. - port (int): The port to use for the SMTP server. - sender_email (str): Sender\'s email address. - recipient_emails (list): List of recipient email addresses. - subject (str): Subject of the email. - plain_text_body (str): Plain text body of the email. - html_body (str): HTML body of the email. - username (str): SMTP authentication username (optional). - password (str): SMTP authentication password (optional). - use_ssl (bool): Whether to use SSL for the connection (default is False). - use_tls (bool): Whether to use STARTTLS for encryption (default is False). Returns: - dict: A dictionary indicating the result of the email sending operation. Keys are email addresses, values are either an error message or \'Sent\' (indicating that the email was successfully sent). results = {} if not recipient_emails: raise ValueError(\\"At least one recipient email address must be specified.\\") try: if use_ssl: server = smtplib.SMTP_SSL(smtp_server, port) else: server = smtplib.SMTP(smtp_server, port) if use_tls: server.starttls() if username and password: server.login(username, password) # Create the email content for recipient in recipient_emails: msg = MIMEMultipart(\'alternative\') msg[\'Subject\'] = subject msg[\'From\'] = sender_email msg[\'To\'] = recipient part1 = MIMEText(plain_text_body, \'plain\') part2 = MIMEText(html_body, \'html\') msg.attach(part1) msg.attach(part2) try: server.sendmail(sender_email, recipient, msg.as_string()) results[recipient] = \'Sent\' except Exception as e: results[recipient] = f\'{type(e).__name__}: {str(e)}\' except Exception as e: # General exception for connection issues results[\'ERROR\'] = f\'{type(e).__name__}: {str(e)}\' finally: server.quit() return results"},{"question":"You are given the task of implementing a class `SortedList` that manages a sorted list of elements. The class should utilize the `bisect` module to efficiently handle insertions while maintaining the sorted order. Specifically, you need to implement the following methods: 1. `__init__(self)`: Initializes an empty list. 2. `insert(self, element)`: Inserts an element into the list while maintaining the sorted order. 3. `exists(self, element)`: Checks if a given element exists in the list. Returns `True` if the element is found, `False` otherwise. 4. `remove(self, element)`: Removes the specified element from the list. If the element does not exist, raise a `ValueError` with the message \\"Element not found\\". **Requirements:** - Use `bisect_left` for finding the insertion point and checking for existence. - Use `insort` for inserting elements. - The class should handle duplicates correctly (i.e., multiple identical values can exist in the list). # Function Signatures ```python class SortedList: def __init__(self): pass def insert(self, element): pass def exists(self, element): pass def remove(self, element): pass ``` # Example Usage ```python slist = SortedList() slist.insert(10) slist.insert(5) slist.insert(7) print(slist.exists(5)) # True print(slist.exists(11)) # False slist.remove(7) print(slist.exists(7)) # False slist.remove(10) print(slist.exists(10)) # False ``` # Constraints 1. Handle attempts to remove elements that are not present in the list gracefully. 2. Ensure the list remains sorted after each insertion. 3. The elements in the list are integers. # Notes: 1. You may assume that the bisect module is already imported. 2. Consider edge cases such as attempting to remove elements that don\'t exist and inserting elements that are already present in the list.","solution":"from bisect import bisect_left, insort class SortedList: def __init__(self): self._list = [] def insert(self, element): insort(self._list, element) def exists(self, element): index = bisect_left(self._list, element) return index < len(self._list) and self._list[index] == element def remove(self, element): index = bisect_left(self._list, element) if index < len(self._list) and self._list[index] == element: self._list.pop(index) else: raise ValueError(\\"Element not found\\") slist = SortedList() slist.insert(10) slist.insert(5) slist.insert(7) print(slist.exists(5)) # True print(slist.exists(11)) # False slist.remove(7) print(slist.exists(7)) # False slist.remove(10) print(slist.exists(10)) # False"},{"question":"**Advanced Python Programming Question** # Problem Statement: Implement two functions inside a Python module to demonstrate your understanding of built-in constants and their unique behaviors. The functions you need to implement are: 1. **compare_to_true(value)**: - **Input**: A single argument `value` which can be of any data type. - **Output**: Return `True` if the `value` is equivalent to the built-in constant `True` in Python, otherwise return `False`. - **Note**: Remember that comparing to `True` should be done by identity, not by value. 2. **handle_not_implemented(operation, *args)**: - **Input**: `operation` is a callable (function) that may or may not support the specific operation for the given arguments `*args`. - **Output**: If the operation is not implemented for the given arguments, return a string \\"Not Implemented\\"; otherwise, return the result of the operation. - **Example**: The emphasis is on using the `NotImplemented` constant correctly. # Example Usage ```python def compare_to_true(value): Compare given value to the built-in constant True. # Implement the function here pass def handle_not_implemented(operation, *args): Handle operations that may return NotImplemented. # Implement the function here pass ``` # Constraints - Avoid using the `==` operator to compare to `True`; use identity checks instead. - Ensure that the `handle_not_implemented` correctly identifies when an operation returns `NotImplemented`. - For `handle_not_implemented`, you will need to handle various edge cases and built-in functions or simple lambda functions. # Testing Your Functions You should test these functions with various edge cases to ensure they work correctly. Here are a few sample tests to get you started: ```python # Testing compare_to_true print(compare_to_true(True)) # Output should be True print(compare_to_true(1)) # Output should be False print(compare_to_true(False)) # Output should be False # Testing handle_not_implemented print(handle_not_implemented(lambda x, y: x + y, 1, 2)) # Output should be 3 print(handle_not_implemented(lambda x, y: x + y, 1, \'a\')) # Output should be \\"Not Implemented\\" # Add more tests as required ``` # Submission Submit your Python module with the defined functions. Ensure that your code is well-commented and follows best practices for readability and maintainability.","solution":"def compare_to_true(value): Compare given value to the built-in constant True. Return True only if the value is literally True, not just truthy. return value is True def handle_not_implemented(operation, *args): Handle operations that may return NotImplemented. If the operation is not implemented for the given arguments, return \\"Not Implemented\\"; otherwise, return the result of the operation. try: result = operation(*args) if result is NotImplemented: return \\"Not Implemented\\" return result except TypeError: return \\"Not Implemented\\""},{"question":"# Advanced Python Coding Assessment Objective: Implement a custom class in Python that utilizes the various compound statements and new Python 3.10 features to achieve the desired functionality. This will test your understanding of control flow, exception handling, pattern matching, and class design. Problem Statement: You are required to create a class `DataProcessor` that processes a list of dictionaries representing some data. Each dictionary contains information about various attributes, including the following possible keys: - `id`: a unique identifier (integer). - `name`: a string representing the name. - `value`: an integer value. - `type`: a string that categorizes the item. The `DataProcessor` class should provide the following methods: 1. `__init__(self, data: List[Dict[str, Any]])`: Initializes the processor with a list of data. 2. `filter_data(self, **filters) -> List[Dict[str, Any]]`: Returns a filtered list based on provided keyword arguments representing key-value pairs to match. 3. `process_data(self) -> None`: Processes the list of dictionaries by: - Printing the `id` and `name` of items where the `value` is greater than `50`. - Printing a message for items with missing keys using a `try-except` block. - Using a `with` statement to log processing steps to a file \'processing.log\'. - Utilizing a coroutine to handle any asynchronous tasks (simulated with `asyncio.sleep`). 4. `pattern_match(self, item: Dict[str, Any]) -> str`: Uses pattern matching to handle different dictionary structures: - If the dictionary has a `type` key with the value \'special\', return \\"Special item\\". - If both `id` and `value` keys are present, return \\"Standard item\\". - Otherwise, return \\"Unknown item\\". Constraints: - You are required to use the new Python 3.10 pattern matching for the `pattern_match` method. - Proper usage of `try`, `finally`, and `with` statements is mandatory. - Ensure that the class and methods are well-documented and type-annotated. Example Usage: ```python data = [ {\\"id\\": 1, \\"name\\": \\"Item1\\", \\"value\\": 60}, {\\"id\\": 2, \\"name\\": \\"Item2\\", \\"value\\": 30}, {\\"name\\": \\"Item3\\", \\"value\\": 80, \\"type\\": \\"special\\"}, {\\"id\\": 4, \\"name\\": \\"Item4\\"}, ] processor = DataProcessor(data) filtered_data = processor.filter_data(value=60) print(filtered_data) processor.process_data() for item in data: result = processor.pattern_match(item) print(result) ``` Expected Output: ``` [{\'id\': 1, \'name\': \'Item1\', \'value\': 60}] Processing Item: id=1, name=Item1 Missing key(s) in item: {\'id\': 4, \'name\': \'Item4\'} Special item Unknown item Special item Unknown item ``` Note: - Ensure to handle file operations (for logging) gracefully. - The coroutine example should be simple, as Python 3.10 coroutine usage is primarily to test understanding of syntax and basic asynchronous capability.","solution":"import asyncio from typing import List, Dict, Any class DataProcessor: def __init__(self, data: List[Dict[str, Any]]): self.data = data def filter_data(self, **filters) -> List[Dict[str, Any]]: result = [] for item in self.data: if all(item.get(key) == value for key, value in filters.items()): result.append(item) return result def process_data(self) -> None: with open(\'processing.log\', \'w\') as log: for item in self.data: try: if item[\'value\'] > 50: print(f\\"Processing Item: id={item[\'id\']}, name={item[\'name\']}\\") log.write(f\\"Processed id={item[\'id\']}, name={item[\'name\']}n\\") except KeyError as e: print(f\\"Missing key(s) in item: {item}\\") log.write(f\\"Error processing item: {item} - Missing key: {e}n\\") asyncio.run(self.async_task(item)) async def async_task(self, item: Dict[str, Any]) -> None: await asyncio.sleep(0.1) print(f\\"Async processing completed for item: {item.get(\'id\', \'N/A\')}\\") def pattern_match(self, item: Dict[str, Any]) -> str: match item: case {\\"type\\": \\"special\\"}: return \\"Special item\\" case {\\"id\\": int(id), \\"value\\": int(value)}: return \\"Standard item\\" case _: return \\"Unknown item\\""},{"question":"You are required to write a function `extract_class_and_function_metadata` that reads a Python module (provided as a file path), analyzes its classes and functions using the `pyclbr` module, and returns a nested dictionary containing metadata about these classes and functions. This task will test your ability to interact with and analyze Python source code using the `pyclbr` module. Function Signature ```python def extract_class_and_function_metadata(module_name: str, path: list) -> dict: pass ``` Input: - `module_name` (str): The name of the module to be read. - `path` (list): A list of directory paths that will be prepended to the system path to locate the module source code. Output: - Returns a dictionary where the key is the name of the class or function, and the value is another dictionary containing metadata about that class or function. Metadata Dictionary Structure: - For Functions: - `type`: \\"function\\" - `file`: The name of the file in which the function is defined. - `module`: The name of the module defining the function. - `lineno`: The line number in the file where the definition starts. - `is_async`: Boolean indicating whether the function is asynchronous. - `children`: Nested dictionary containing metadata about any nested functions or classes. - For Classes: - `type`: \\"class\\" - `file`: The name of the file in which the class is defined. - `module`: The name of the module defining the class. - `lineno`: The line number in the file where the definition starts. - `super`: A list of superclass names or `Class` objects. - `methods`: Dictionary mapping method names to their line numbers. - `children`: Nested dictionary containing metadata about any nested classes or functions. Example Usage: ```python metadata = extract_class_and_function_metadata(\\"example_module\\", [\\"path/to/directory\\"]) ``` Constraints: - The function must use `pyclbr.readmodule_ex` to extract metadata. - The provided module must be in Python and have at least one class or function definition. - Handle cases where there are nested classes and functions. Implement the `extract_class_and_function_metadata` function and ensure it adheres to the requirements and constraints given above.","solution":"import pyclbr import sys def extract_class_and_function_metadata(module_name: str, path: list) -> dict: Extracts metadata about classes and functions within a given module. Args: module_name (str): The name of the module. path (list): List of directory paths to search for the module. Returns: dict: Nested dictionary containing metadata of classes and functions. sys.path.extend(path) # Using pyclbr.readmodule_ex to analyze the module module_info = pyclbr.readmodule_ex(module_name) def get_function_metadata(func): return { \\"type\\": \\"function\\", \\"file\\": func.file, \\"module\\": func.module, \\"lineno\\": func.lineno, \\"is_async\\": func.is_async, \\"children\\": {} # Functions can\'t have other functions nested using pyclbr } def get_class_metadata(cls): return { \\"type\\": \\"class\\", \\"file\\": cls.file, \\"module\\": cls.module, \\"lineno\\": cls.lineno, \\"super\\": cls.super, \\"methods\\": {method: cls.methods[method] for method in cls.methods}, \\"children\\": {} # Classes have children which could be extracted similarly } metadata = {} for name, item in module_info.items(): if isinstance(item, pyclbr.Function): metadata[name] = get_function_metadata(item) elif isinstance(item, pyclbr.Class): metadata[name] = get_class_metadata(item) # Collecting nested functions and classes within the class scope for method_name in item.methods: if method_name in module_info: if isinstance(module_info[method_name], pyclbr.Function): metadata[name][\\"children\\"][method_name] = get_function_metadata(module_info[method_name]) elif isinstance(module_info[method_name], pyclbr.Class): metadata[name][\\"children\\"][method_name] = get_class_metadata(module_info[method_name]) # Restore the original sys.path to avoid side-effects sys.path = sys.path[:-len(path)] return metadata"},{"question":"# PyTorch Coding Assessment **Objective**: The goal of this assessment is to evaluate your understanding and ability to use PyTorch for tensor manipulation, mathematical operations, and random sampling. Problem Statement You are required to implement a function that performs the following operations on a given tensor: 1. **Square Calculation**: Compute the element-wise square of the input tensor. 2. **Logarithm Calculation**: Compute the element-wise natural logarithm of the tensor obtained from the square calculation. 3. **Random Noise Addition**: Add random noise sampled from a normal distribution to the tensor from the logarithm calculation. 4. **Thresholding**: Apply a threshold such that all values less than a given threshold are set to zero. 5. **Summation**: Compute the sum of all elements in the final tensor. Implement the following function: ```python import torch def process_tensor(input_tensor, threshold): Process the input tensor as specified in the problem statement. Args: - input_tensor (torch.Tensor): A 1D tensor of floats. - threshold (float): A threshold value for thresholding operation. Returns: - float: The sum of all elements in the final tensor after processing. # Step 1: Compute the element-wise square of the input tensor squared_tensor = torch.square(input_tensor) # Step 2: Compute the element-wise natural logarithm log_tensor = torch.log(squared_tensor) # Step 3: Add random noise sampled from a normal distribution noise = torch.randn_like(log_tensor) # Samples from a standard normal distribution noisy_tensor = log_tensor + noise # Step 4: Apply thresholding thresholded_tensor = torch.where(noisy_tensor < threshold, torch.tensor(0.0), noisy_tensor) # Step 5: Compute the sum of all elements result = torch.sum(thresholded_tensor).item() return result ``` # Constraints and Requirements 1. The input tensor `input_tensor` will be a 1D tensor of floats with a length between 1 and 1000. 2. The `threshold` will be a float. 3. Use appropriate PyTorch operations to perform each step. # Example ```python input_tensor = torch.tensor([0.5, 1.5, 2.5, 3.5]) threshold = 0.5 result = process_tensor(input_tensor, threshold) print(result) # Output will be a float based on processing the tensor ``` Ensure that your implementation handles edge cases and is efficient. The provided implementation has detailed comments to guide you through the steps. # Notes - Use `torch.square` for squaring the tensor. - Use `torch.log` for calculating the natural logarithm. - Use `torch.randn_like` for generating noise. - Use `torch.where` for applying thresholding. - Use `torch.sum` to compute the summation of the tensor elements. This problem tests your ability to handle different tensor operations and integrate them to solve a real-world problem efficiently using PyTorch.","solution":"import torch def process_tensor(input_tensor, threshold): Process the input tensor as specified in the problem statement. Args: - input_tensor (torch.Tensor): A 1D tensor of floats. - threshold (float): A threshold value for thresholding operation. Returns: - float: The sum of all elements in the final tensor after processing. # Step 1: Compute the element-wise square of the input tensor squared_tensor = torch.square(input_tensor) # Step 2: Compute the element-wise natural logarithm log_tensor = torch.log(squared_tensor) # Step 3: Add random noise sampled from a normal distribution noise = torch.randn_like(log_tensor) # Samples from a standard normal distribution noisy_tensor = log_tensor + noise # Step 4: Apply thresholding thresholded_tensor = torch.where(noisy_tensor < threshold, torch.tensor(0.0), noisy_tensor) # Step 5: Compute the sum of all elements result = torch.sum(thresholded_tensor).item() return result"},{"question":"Objective: To assess the student\'s capability in performing complex data manipulations using pandas DataFrame. Problem Statement: You have been provided with a dataset containing information about sales transactions in a CSV file named `sales_data.csv`. The columns in the dataset are as follows: - `TransactionID` (int): Unique identifier for a transaction. - `ProductID` (int): Unique identifier for a product. - `UserID` (int): Unique identifier for a user. - `Quantity` (int): Quantity of the product purchased in the transaction. - `Price` (float): Price of the product in the transaction. - `Date` (string): Date of the transaction in `YYYY-MM-DD` format. The dataset may contain missing values, and your goal is to perform the following tasks: 1. Load the dataset into a pandas DataFrame. 2. Fill any missing values in the `Quantity` column with the mean `Quantity` of that product. 3. Create a new column `TotalPrice` which is the product of `Quantity` and `Price`. 4. For each `ProductID`, calculate the total quantity sold and the total revenue (`TotalPrice`) generated. 5. Create a new DataFrame containing `ProductID`, `TotalQuantitySold`, and `TotalRevenue`. 6. Sort this new DataFrame by `TotalRevenue` in descending order and select the top 5 products with the highest revenue. 7. Save this sorted DataFrame to a new CSV file named `top_products.csv`. Constraints: - Handle missing values appropriately as described. - Ensure that your code can handle larger datasets efficiently. Input and Output: - **Input**: Path to the CSV file (`sales_data.csv`). - **Output**: A new CSV file (`top_products.csv`) containing the top 5 products by revenue. Example: Given a sample `sales_data.csv`: ``` TransactionID,ProductID,UserID,Quantity,Price,Date 1,101,1001,2,20.0,2023-01-01 2,102,1002,,30.0,2023-01-01 3,101,1003,1,20.0,2023-01-02 ... ``` The output should be a `top_products.csv` with the following structure: ``` ProductID,TotalQuantitySold,TotalRevenue 101,10,200.0 102,8,240.0 ... ``` Solution Template: ```python import pandas as pd def process_sales_data(csv_file): # Step 1: Load the dataset df = pd.read_csv(csv_file) # Step 2: Fill missing Quantity values with the mean Quantity of the product df[\'Quantity\'] = df.groupby(\'ProductID\')[\'Quantity\'].transform(lambda x: x.fillna(x.mean())) # Step 3: Create a new column \'TotalPrice\' df[\'TotalPrice\'] = df[\'Quantity\'] * df[\'Price\'] # Step 4: Calculate total quantity sold and total revenue for each product summary_df = df.groupby(\'ProductID\').agg(TotalQuantitySold=(\'Quantity\', \'sum\'), TotalRevenue=(\'TotalPrice\', \'sum\')).reset_index() # Step 5: Sort by TotalRevenue in descending order and select the top 5 products top_products = summary_df.sort_values(by=\'TotalRevenue\', ascending=False).head(5) # Step 6: Save the sorted DataFrame to a new CSV file top_products.to_csv(\'top_products.csv\', index=False) ``` Your task is to complete the `process_sales_data` function to ensure it meets the requirements outlined above.","solution":"import pandas as pd def process_sales_data(csv_file): # Step 1: Load the dataset df = pd.read_csv(csv_file) # Step 2: Fill missing Quantity values with the mean Quantity of the product df[\'Quantity\'] = df.groupby(\'ProductID\')[\'Quantity\'].transform(lambda x: x.fillna(x.mean())) # Step 3: Create a new column \'TotalPrice\' df[\'TotalPrice\'] = df[\'Quantity\'] * df[\'Price\'] # Step 4: Calculate total quantity sold and total revenue for each product summary_df = df.groupby(\'ProductID\').agg( TotalQuantitySold=(\'Quantity\', \'sum\'), TotalRevenue=(\'TotalPrice\', \'sum\') ).reset_index() # Step 5: Sort by TotalRevenue in descending order and select the top 5 products top_products = summary_df.sort_values(by=\'TotalRevenue\', ascending=False).head(5) # Step 6: Save the sorted DataFrame to a new CSV file top_products.to_csv(\'top_products.csv\', index=False)"},{"question":"**Coding Assessment Question:** Implement a function named `process_email_data` that takes in a list of raw email header data and processes it to return a structured summary. Each email header data will contain fields such as `Date`, `From`, `To`, and `Message-ID`. Your task is to parse these fields appropriately, ensuring compliance with relevant RFC standards for date formatting and message IDs, while also incorporating local time conversion where necessary. # Function Signature: ```python def process_email_data(email_headers: List[Dict[str, str]]) -> List[Dict[str, Any]]: pass ``` # Input: - `email_headers`: A list of dictionary items where each dictionary represents raw header fields for an individual email. Each dictionary will have keys like `Date`, `From`, `To`, and `Message-ID`. # Output: - The function should return a list of dictionaries, each containing: - `formatted_date`: The email\'s date formatted as per RFC 2822, but converted to the local timezone. - `from_name`: The real name of the sender parsed from the `From` field. - `from_email`: The email address of the sender parsed from the `From` field. - `recipients`: A list of tuples, each containing a recipient\'s real name and email address, parsed from the `To` field. - `msg_id`: The unique message ID generated or found within the headers. # Constraints: - Assume that the input headers use valid RFC 2822 formats for the `Date`, `From`, and `To` fields. - The function should handle date conversions to local time effectively. - Use appropriate functions from the `email.utils` module to parse and format the data. # Example: ```python email_headers = [ { \\"Date\\": \\"Mon, 20 Nov 1995 19:12:08 -0500\\", \\"From\\": \\"John Doe <john.doe@example.com>\\", \\"To\\": \\"Jane Smith <jane.smith@example.com>, Alice Brown <alice.brown@another.com>\\", \\"Message-ID\\": \\"<19951120191208.19926.qmail@example.com>\\" } ] result = process_email_data(email_headers) ``` Expected `result`: ```python [ { \\"formatted_date\\": \\"Tue, 21 Nov 1995 00:12:08 +0000\\", \\"from_name\\": \\"John Doe\\", \\"from_email\\": \\"john.doe@example.com\\", \\"recipients\\": [(\\"Jane Smith\\", \\"jane.smith@example.com\\"), (\\"Alice Brown\\", \\"alice.brown@another.com\\")], \\"msg_id\\": \\"<19951120191208.19926.qmail@example.com>\\" } ] ``` # Notes: - Ensure to handle timezone conversions correctly so that the `formatted_date` reflects the local time. - Implement robust error handling for parsing and formatting, ensuring that the function can handle unexpected formats gracefully.","solution":"from typing import List, Dict, Any from email.utils import parsedate_to_datetime, parseaddr, getaddresses import pytz def process_email_data(email_headers: List[Dict[str, str]]) -> List[Dict[str, Any]]: local_tz = pytz.timezone(\'UTC\') # For example, converting to UTC time. Change \'UTC\' to your local timezone if applicable. def format_date(date_str): try: dt = parsedate_to_datetime(date_str) dt_local = dt.astimezone(local_tz) return dt_local.strftime(\'%a, %d %b %Y %H:%M:%S %z\') except Exception as e: return None def parse_from(from_str): name, email = parseaddr(from_str) return name, email def parse_to(to_str): recipients = getaddresses([to_str]) return [(name, email) for name, email in recipients] result = [] for header in email_headers: try: formatted_date = format_date(header.get(\'Date\', \'\')) from_name, from_email = parse_from(header.get(\'From\', \'\')) recipients = parse_to(header.get(\'To\', \'\')) msg_id = header.get(\'Message-ID\', \'\') result.append({ \'formatted_date\': formatted_date, \'from_name\': from_name, \'from_email\': from_email, \'recipients\': recipients, \'msg_id\': msg_id }) except Exception as e: continue return result"},{"question":"# Bytecode Analysis with `dis` Module **Objective**: You are tasked with writing a Python function that analyzes another provided Python function using the `dis` module. The goal is to output a readable summary of the bytecode instructions used in the function. **Task**: 1. Write a function `analyze_bytecode(func)` that takes a single argument `func`, a Python function. 2. The function should utilize the `dis.Bytecode` class to analyze the bytecode instructions of `func`. 3. Your function should return a list of dictionaries, where each dictionary represents an instruction. Each dictionary should contain the following keys: - `\'opname\'`: The name of the operation. - `\'arg\'`: The argument (if any) to the operation. - `\'argval\'`: The resolved argument value (if known). - `\'offset\'`: The start index of the operation within the bytecode sequence. - `\'is_jump_target\'`: A boolean indicating if this instruction is a jump target. ```python def analyze_bytecode(func): Analyzes the bytecode of a provided function. Parameters: func (function): The function to analyze. Returns: List[Dict[str, Any]]: A list of dictionaries, each containing details of a bytecode instruction. # Your code here # Example usage: def sample_function(x, y): return x + y print(analyze_bytecode(sample_function)) ``` **Constraints**: - Assume the provided `func` is always a valid Python function. - You may use the functionality provided by the `dis` module only. - Focus on the most recent version mentioned in the documentation for the `dis` module. **Expected Output**: The function should output a list of dictionaries for each instruction in the `sample_function`. For example, an output may look like: ```python [ {\'opname\': \'LOAD_FAST\', \'arg\': 0, \'argval\': \'x\', \'offset\': 0, \'is_jump_target\': False}, {\'opname\': \'LOAD_FAST\', \'arg\': 1, \'argval\': \'y\', \'offset\': 1, \'is_jump_target\': False}, {\'opname\': \'BINARY_ADD\', \'arg\': None, \'argval\': None, \'offset\': 2, \'is_jump_target\': False}, {\'opname\': \'RETURN_VALUE\', \'arg\': None, \'argval\': None, \'offset\': 3, \'is_jump_target\': False} ] ``` Use the examples and explanations provided in the documentation to guide your implementation.","solution":"import dis def analyze_bytecode(func): Analyzes the bytecode of a provided function. Parameters: func (function): The function to analyze. Returns: List[Dict[str, Any]]: A list of dictionaries, each containing details of a bytecode instruction. bytecode = dis.Bytecode(func) instructions = [] for instr in bytecode: instructions.append({ \'opname\': instr.opname, \'arg\': instr.arg, \'argval\': instr.argval, \'offset\': instr.offset, \'is_jump_target\': instr.is_jump_target }) return instructions"},{"question":"Bootstrapping the `pip` Installer **Objective:** You are required to write a Python function that utilizes the `ensurepip` module to ensure that a specific version of `pip` is installed in a designated directory. Your function will involve handling different parameters such as upgrading existing installations, setting the root directory, and providing user feedback. **Function Signature:** ```python def bootstrap_pip(root_dir: str = None, upgrade: bool = False, user_install: bool = False, verbose: int = 0) -> str: pass ``` **Input:** - `root_dir` (str): The root directory where `pip` should be installed. Default is `None`, meaning the default installation path will be used. - `upgrade` (bool): A boolean indicating whether to upgrade an existing installation of `pip`. Default is `False`. - `user_install` (bool): A boolean indicating whether to install `pip` in the user site packages directory. Default is `False`. - `verbose` (int): An integer controlling the verbosity of the installation output. Default is `0`. **Output:** - Returns a string message indicating the success or failure of the `pip` installation. **Constraints:** - The function should catch and handle any exceptions that arise during the bootstrapping process, returning an appropriate error message. **Requirements:** - Ensure that you use the `ensurepip` module\'s `bootstrap` function to perform the installation. - Validate that the parameters do not conflict (e.g., `user_install` should not be used with an active virtual environment). - Provide meaningful output based on the verbosity level. - Handle the potential `ValueError` when both `altinstall` and `default_pip` flags are set, even though they are not explicitly part of this function\'s parameters. **Example Usage:** ```python # Example 1: Basic installation with default settings result = bootstrap_pip() print(result) # Output: \\"pip installation successful\\" # Example 2: Upgrade existing pip installation result = bootstrap_pip(upgrade=True) print(result) # Output might vary based on the existing installation state # Example 3: User-specific pip installation with verbose output result = bootstrap_pip(user_install=True, verbose=2) print(result) # Output should include detailed information about the installation process ``` **Notes:** 1. Ensure your function is robust and handles any corner cases gracefully. 2. Test your function with different combinations of input parameters to validate its behavior.","solution":"import ensurepip import sys def bootstrap_pip(root_dir: str = None, upgrade: bool = False, user_install: bool = False, verbose: int = 0) -> str: try: args = { \'upgrade\': upgrade, \'user\': user_install, \'verbose\': verbose, } # Since ensurepip does not allow root_dir directly, using sys.argv manipulation as a workaround. if root_dir: sys.argv += [\'--root\', root_dir] ensurepip.bootstrap(**args) msg = \\"pip installation successful\\" if verbose > 0: msg += f\\" with verbosity level {verbose}\\" return msg except Exception as e: return f\\"pip installation failed: {str(e)}\\""},{"question":"**Question: Dimensionality Reduction and Regression using PLSRegression** # Problem Statement: You are required to implement a Python function using `scikit-learn` that performs dimensionality reduction and regression on a given dataset using the `PLSRegression` estimator. The function should: 1. Fit a `PLSRegression` model on the provided training data. 2. Project new data onto the learned PLS space. 3. Predict the target variable for new data. 4. Compute and return the R-squared score of the model on the test data. # Function Signature: ```python from typing import Tuple import numpy as np from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import r2_score def pls_regression(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray, n_components: int) -> Tuple[np.ndarray, float]: Apply PLSRegression on the training data and evaluate it on the test data. Parameters: - X_train: Training feature data, a numpy array of shape (n_samples_train, n_features) - Y_train: Training target data, a numpy array of shape (n_samples_train,) - X_test: Testing feature data, a numpy array of shape (n_samples_test, n_features) - Y_test: Testing target data, a numpy array of shape (n_samples_test,) - n_components: Integer, number of PLS components to keep. Returns: - y_pred: Predicted target values for the test data, a numpy array of shape (n_samples_test,) - r2: R-squared score of the model on the test data, a float. pass ``` # Input: - `X_train`: A `numpy` array of shape `(n_samples_train, n_features)` representing the training feature data. - `Y_train`: A `numpy` array of shape `(n_samples_train,)` representing the training target data. - `X_test`: A `numpy` array of shape `(n_samples_test, n_features)` representing the testing feature data. - `Y_test`: A `numpy` array of shape `(n_samples_test,)` representing the testing target data. - `n_components`: An integer representing the number of components to keep. # Output: - `y_pred`: A `numpy` array of shape `(n_samples_test,)` representing the predicted target values for the testing data. - `r2`: A float representing the R-squared score of the model on the testing data. # Constraints: 1. Assume the input data is clean and properly preprocessed. 2. Ensure that the `n_components` is a positive integer and does not exceed the number of features in the dataset. # Example: ```python import numpy as np # Example input data X_train = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]]) Y_train = np.array([1, 2, 3]) X_test = np.array([[0.3, 0.2, 0.1], [0.6, 0.5, 0.4]]) Y_test = np.array([1.5, 2.5]) n_components = 2 # Function call y_pred, r2 = pls_regression(X_train, Y_train, X_test, Y_test, n_components) # Expected output (values may vary based on implementation and scikit-learn version) print(y_pred) # e.g., array([1.4, 2.6]) print(r2) # e.g., 0.95 ``` # Instructions: 1. Import the necessary libraries. 2. Initialize a `PLSRegression` model with the specified number of components. 3. Fit the model on the training data. 4. Use the fitted model to predict the target values for the test data. 5. Compute the R-squared score of the predictions on the test data. 6. Return the predicted target values and the R-squared score. You can refer to the `scikit-learn` documentation for more details on the `PLSRegression` class: [PLSRegression](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html).","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import r2_score from typing import Tuple def pls_regression(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray, n_components: int) -> Tuple[np.ndarray, float]: Apply PLSRegression on the training data and evaluate it on the test data. Parameters: - X_train: Training feature data, a numpy array of shape (n_samples_train, n_features) - Y_train: Training target data, a numpy array of shape (n_samples_train,) - X_test: Testing feature data, a numpy array of shape (n_samples_test, n_features) - Y_test: Testing target data, a numpy array of shape (n_samples_test,) - n_components: Integer, number of PLS components to keep. Returns: - y_pred: Predicted target values for the test data, a numpy array of shape (n_samples_test,) - r2: R-squared score of the model on the test data, a float. # Initialize the PLSRegression model pls = PLSRegression(n_components=n_components) # Fit the model on the training data pls.fit(X_train, Y_train) # Predict the target values for the test data y_pred = pls.predict(X_test).flatten() # Compute the R-squared score r2 = r2_score(Y_test, y_pred) return y_pred, r2"},{"question":"You are required to implement a function called `compare_hyperbolic_sines` that compares the hyperbolic sines of the two given angles and determines if they are mathematically close to each other. If they are, the function should return `True` and the sum of their hyperbolic tangents; otherwise, return `False` and the value of their greatest common divisor (GCD). Your function should adhere to the following signature: ```python def compare_hyperbolic_sines(angle1: float, angle2: float, tolerance: float = 1e-09) -> Tuple[bool, float]: ``` # Parameters - `angle1`: a float representing the first angle in radians. - `angle2`: a float representing the second angle in radians. - `tolerance`: a float representing the relative tolerance for comparing the hyperbolic sines. The default value is `1e-09`. # Returns - `result`: a tuple where the first value is a boolean indicating whether the hyperbolic sines of the two angles are close to each other and the second value is either the sum of their hyperbolic tangents (if they are close) or their greatest common divisor (if they are not close). # Constraints - You must use the `math` module provided by Python. - You can assume that the angles provided will be finite and not NaN. - The GCD should be computed over integer values for the purpose of this program (consider both angles converted to integers). # Example ```python compare_hyperbolic_sines(1.0, 1.0000000001) # Output: (True, 1.5574077246549023) compare_hyperbolic_sines(1.0, 1.5) # Output: (False, 1) ``` # Explanation 1. Calculate the hyperbolic sine of both angles using the `math.sinh` function. 2. Use the `math.isclose` function to compare the hyperbolic sines of the two angles with the given tolerance. 3. If they are close, compute the sum of their hyperbolic tangents using the `math.tanh` function. 4. If they are not close, compute the GCD of the two angles (consider them as integers) using the `math.gcd` function. 5. Return a tuple based on the comparison result. # Note Make sure that the solution efficiently uses mathematical functions and performs operations adhering to the requirements, constraints, and acceptable performance metrics.","solution":"import math from typing import Tuple def compare_hyperbolic_sines(angle1: float, angle2: float, tolerance: float = 1e-09) -> Tuple[bool, float]: Compares the hyperbolic sines of the two given angles and determines if they are close to each other. Parameters: angle1 (float): The first angle in radians. angle2 (float): The second angle in radians. tolerance (float): The relative tolerance for comparing the hyperbolic sines. Default is 1e-09. Returns: Tuple[bool, float]: A tuple where the first value is a boolean indicating whether the hyperbolic sines of the two angles are close and the second value is either the sum of their hyperbolic tangents (if close) or their greatest common divisor (if not close). sinh1 = math.sinh(angle1) sinh2 = math.sinh(angle2) if math.isclose(sinh1, sinh2, rel_tol=tolerance): tanh_sum = math.tanh(angle1) + math.tanh(angle2) return (True, tanh_sum) else: gcd_value = math.gcd(int(angle1), int(angle2)) return (False, gcd_value)"},{"question":"# Buffer Protocol and Memory Management in Python **Objective:** You are required to demonstrate your understanding of the buffer protocol in Python by implementing a function that interacts with objects exposing their internal memory buffers. The task involves creating an efficient memory copying utility using buffer protocol functions. # Problem Statement: Write a function `copy_memory_buffer(src: Any, dst: Any, length: int) -> bool` that copies a specified number of bytes from a source buffer to a destination buffer. The function should handle the buffer acquisition and release properly to ensure no memory leaks or illegal access. Function Signature: ```python def copy_memory_buffer(src: Any, dst: Any, length: int) -> bool: pass ``` Parameters: - **src (Any):** The source object from which to copy the memory buffer. This should support the buffer protocol. - **dst (Any):** The destination object to which the memory buffer should be copied. This should support the buffer protocol. - **length (int):** The number of bytes to copy from the source to the destination. Returns: - **bool:** Returns `True` if the memory is successfully copied; otherwise, `False`. Constraints: 1. The function should ensure that the length of the memory to be copied (`length`) does not exceed the buffer sizes of both the source and destination. 2. Proper exception handling should be implemented to manage cases where buffers cannot be obtained or the copy operation fails. # Example Usage: ```python src_buffer = bytearray(b\\"Hello, World!\\") dst_buffer = bytearray(len(src_buffer)) result = copy_memory_buffer(src_buffer, dst_buffer, len(src_buffer)) print(result) # Should print: True print(dst_buffer) # Should print: bytearray(b\\"Hello, World!\\") ``` # Implementation Guidelines: 1. Use `PyObject_GetBuffer` to obtain the buffer of the source and destination objects. 2. Perform the necessary memory check to ensure the length does not exceed the available size. 3. Use `PyBuffer_ToContiguous` function to perform the actual copy from source to destination. 4. Ensure to release the buffers using `PyBuffer_Release` to manage reference counts correctly. 5. Include appropriate exception handling to manage any errors encountered during the buffer operations. **Note**: This exercise emphasizes understanding buffer protocol operations, efficient memory management, and exception handling to create robust Python programs.","solution":"def copy_memory_buffer(src, dst, length): Copies a specified number of bytes from a source buffer to a destination buffer. try: with memoryview(src) as src_view, memoryview(dst) as dst_view: if len(src_view) < length or len(dst_view) < length: return False dst_view[:length] = src_view[:length] return True except (TypeError, ValueError, BufferError): return False"},{"question":"**Problem Statement: Implement Custom Copy Methods for a Recursive Data Structure** You are given a recursive data structure representing a tree. Implement custom shallow and deep copy methods for this structure. # Specification: Create a class `TreeNode` that represents a node in a tree. Each node contains: - An integer value. - A list of child nodes, where each child is an instance of `TreeNode`. Implement the following methods: - `__copy__(self)`: Returns a shallow copy of the node. - `__deepcopy__(self, memo)`: Returns a deep copy of the node. A deep copy should ensure all nodes in the copied tree are entirely independent of the original tree, while a shallow copy should ensure the copy shares child nodes with the original. # Example: ```python class TreeNode: def __init__(self, value): self.value = value self.children = [] def __copy__(self): # Implement shallow copying here pass def __deepcopy__(self, memo): # Implement deep copying here pass # Example Usage root = TreeNode(1) child1 = TreeNode(2) child2 = TreeNode(3) root.children.append(child1) root.children.append(child2) shallow_copied_root = copy.copy(root) deep_copied_root = copy.deepcopy(root) ``` # Constraints: - You may assume that the tree structure is finite and has no cycles. - Each node value is a non-negative integer. # Notes: - Ensure that your shallow copy shares the child nodes with the original tree. - Ensure that your deep copy does not share any nodes with the original tree. **Testing Your Implementation**: You can test your implementation with different tree structures and by verifying that changes to nodes in the shallow copy affect the original tree, while changes to the deep copy do not.","solution":"import copy class TreeNode: def __init__(self, value): self.value = value self.children = [] def __copy__(self): cls = self.__class__ result = cls.__new__(cls) result.value = self.value # Shallow copy of children list: just copy references to existing nodes result.children = self.children.copy() return result def __deepcopy__(self, memo): cls = self.__class__ result = cls.__new__(cls) memo[id(self)] = result result.value = copy.deepcopy(self.value, memo) # Deep copy of children list: create new nodes and copy children result.children = copy.deepcopy(self.children, memo) return result"},{"question":"# File Locking and File Descriptor Control with `fcntl` Objective In this task, you are going to write a Python function to perform file locking and manipulate file descriptor flags using the `fcntl` module. Your solution should demonstrate the ability to handle file descriptor operations and ensure proper locking mechanisms to avoid concurrent file writes. Problem Statement You need to implement a function `control_file_operations(file_path: str, blocking: bool) -> str` that performs the following operations: 1. Open the file specified by `file_path` for reading and writing. 2. Apply an exclusive lock on the file using `fcntl.lockf`. If `blocking` is `True`, the lock should be blocking; otherwise, it should be non-blocking. 3. Change the file status flags using `fcntl.fcntl` to make the file descriptor non-blocking. 4. Write the string \\"Hello, fcntl!\\" to the file. 5. Release the lock after writing. 6. Close the file. If any error occurs during the process (e.g., unable to acquire lock or write to the file), catch the exception and return a string `\\"An error occurred: <error_message>\\"` where `<error_message>` is a description of the error. Return `\\"Success\\"` if all operations are completed successfully. Function Signature ```python def control_file_operations(file_path: str, blocking: bool) -> str: pass ``` Constraints - You can assume `file_path` is always a valid path to an existing file. - Consider using `fcntl.LOCK_EX` for an exclusive lock and bitwise OR `fcntl.LOCK_NB` for non-blocking if required. - The length of the written string \\"Hello, fcntl!\\" is 13 bytes. Example Usage ```python result = control_file_operations(\'/path/to/file.txt\', True) assert result == \\"Success\\" result = control_file_operations(\'/path/to/file.txt\', False) assert result in [\\"Success\\", \\"An error occurred: <error_message>\\"] ``` Performance Requirements - The function should manage file descriptors efficiently to prevent resource leaks. - Proper error handling should ensure that all steps are performed safely. This will test your understanding of the `fcntl` module, file descriptors, and exception handling in Python.","solution":"import fcntl import os def control_file_operations(file_path: str, blocking: bool) -> str: try: # Open the file for reading and writing fd = os.open(file_path, os.O_RDWR) # Apply an exclusive lock on the file lock_type = fcntl.LOCK_EX if not blocking: lock_type |= fcntl.LOCK_NB fcntl.lockf(fd, lock_type) # Change file descriptor status flags to non-blocking flags = fcntl.fcntl(fd, fcntl.F_GETFL) fcntl.fcntl(fd, fcntl.F_SETFL, flags | os.O_NONBLOCK) # Write to the file os.write(fd, b\\"Hello, fcntl!\\") # Release the lock fcntl.lockf(fd, fcntl.LOCK_UN) # Close the file os.close(fd) return \\"Success\\" except Exception as e: return f\\"An error occurred: {e}\\""},{"question":"**Objective**: Demonstrate your understanding of the `imaplib` package by implementing a function to interact with an IMAP server. # Problem Statement You are required to implement a function `process_unread_emails` that connects to an IMAP server, logs in with provided credentials, retrieves a list of unread emails from the inbox, marks them as read, and logs out. # Function Signature ```python def process_unread_emails(server: str, port: int, username: str, password: str) -> List[str]: pass ``` # Input: - `server` (str): The hostname or IP address of the IMAP server. - `port` (int): The port number to connect to the IMAP server (use 993 for SSL). - `username` (str): The username for logging into the IMAP server. - `password` (str): The password for logging into the IMAP server. # Output: - Returns a list of strings where each string is the subject of an unread email from the inbox. # Constraints: - You should handle SSL connections using the `IMAP4_SSL` class from `imaplib`. - If any errors are encountered (e.g., login failure, server not reachable), the function should raise an appropriate exception with a meaningful message. - You may assume that the \\"Subject\\" header of each email is unique. # Example: ```python server = \\"imap.example.com\\" port = 993 username = \\"user@example.com\\" password = \\"password123\\" subjects = process_unread_emails(server, port, username, password) print(subjects) ``` Expected Output: ``` [\'Subject1\', \'Subject2\'] ``` # Notes: - Utilize the `IMAP4_SSL` class for establishing a secure connection. - Use the appropriate IMAP commands (`login`, `select`, `search`, `fetch`, `store`, `logout`) to perform the required operations. - Remember to handle and close the connection properly to avoid any resource leaks. This question aims to test your ability to work with networking protocols and handle email operations using Python\'s `imaplib` module.","solution":"import imaplib import email from typing import List def process_unread_emails(server: str, port: int, username: str, password: str) -> List[str]: Connects to an IMAP server, logs in with provided credentials, retrieves a list of unread emails from the inbox, marks them as read, and logs out. Args: - server (str): IMAP server hostname or IP address. - port (int): IMAP server port (use 993 for SSL). - username (str): IMAP account username. - password (str): IMAP account password. Returns: - List[str]: List of subjects of unread emails. Raises: - Exception: If any error occurs during the process. try: # Step 1: Connect to the IMAP server using SSL mail = imaplib.IMAP4_SSL(server, port) # Step 2: Login to the server mail.login(username, password) # Step 3: Select the inbox mail.select(\\"inbox\\") # Step 4: Search for unread emails status, messages = mail.search(None, \'UNSEEN\') if status != \'OK\': raise Exception(\\"Failed to search for unread emails.\\") # List to hold subjects of unread emails subjects = [] # Step 5: Fetch each unread email for num in messages[0].split(): status, msg_data = mail.fetch(num, \'(RFC822)\') if status != \'OK\': raise Exception(\\"Failed to fetch email.\\") # Parse the email content msg = email.message_from_bytes(msg_data[0][1]) subject = email.header.decode_header(msg[\'Subject\'])[0][0] if isinstance(subject, bytes): subject = subject.decode() subjects.append(subject) # Mark the email as read mail.store(num, \'+FLAGS\', \'Seen\') # Step 6: Logout from the server mail.logout() return subjects except Exception as e: raise Exception(f\\"An error occurred: {e}\\")"},{"question":"# **Coding Assessment Question** **Objective:** This question aims to test your ability to fetch a dataset from an external source using scikit-learn, preprocess it, and build a machine learning model to evaluate its performance. **Question:** You are provided with a dataset of gene expressions in mice brains. Your task is to: 1. Download the mice protein expression dataset from OpenML. 2. Preprocess the dataset by: - Handling any missing values (if any). - Encoding categorical variables (if any). - Splitting the dataset into training and testing sets. 3. Train a Support Vector Classifier (SVC) on the training data. 4. Evaluate the performance of your model on the testing data using accuracy. 5. Plot the confusion matrix for your model\'s predictions on the test set. **Requirements:** - You must use scikit-learn for all operations. - You should handle any preprocessing steps required to make the data suitable for model training. - The data should be split into training and testing sets with an 80-20 split. **Instructions:** 1. Implement the `fetch_and_preprocess_data` function to fetch and preprocess the dataset. 2. Implement the `train_and_evaluate` function to train the SVC model and evaluate its performance. **Input:** The functions do not take explicit input from the user. **Output:** - The `train_and_evaluate` function should return the accuracy of the model on the test set. - The function should also plot the confusion matrix. **Function Signatures:** ```python def fetch_and_preprocess_data(): # This function should fetch the dataset from OpenML and preprocess it as described. # Returns: # X_train (numpy.ndarray): Preprocessed training features. # X_test (numpy.ndarray): Preprocessed testing features. # y_train (numpy.ndarray): Training labels. # y_test (numpy.ndarray): Testing labels. pass def train_and_evaluate(X_train, X_test, y_train, y_test): # This function should train an SVC model on the training data and evaluate it on the test data. # Args: # X_train (numpy.ndarray): Preprocessed training features. # X_test (numpy.ndarray): Preprocessed testing features. # y_train (numpy.ndarray): Training labels. # y_test (numpy.ndarray): Testing labels. # Returns: # float: Accuracy of the model on the test set. # Plots the confusion matrix as a side effect. pass # Example usage: # X_train, X_test, y_train, y_test = fetch_and_preprocess_data() # accuracy = train_and_evaluate(X_train, X_test, y_train, y_test) # print(accuracy) ``` **Constraints:** - Do not use any additional libraries apart from numpy, pandas, and scikit-learn. - Ensure the solution runs efficiently and handle exceptions where necessary. **Notes:** - The URL for the mice protein dataset should be: \'https://www.openml.org/d/40966\'. - For plotting the confusion matrix, you may use `matplotlib` if required.","solution":"import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer from sklearn.svm import SVC from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay from sklearn.preprocessing import StandardScaler from sklearn.datasets import fetch_openml import matplotlib.pyplot as plt def fetch_and_preprocess_data(): # Fetch the dataset from OpenML data = fetch_openml(data_id=40966, as_frame=True) df = data.frame # Handle missing values imputer = SimpleImputer(strategy=\'mean\') X = imputer.fit_transform(df.drop(columns=[\'class\'])) y = df[\'class\'].values # Encode categorical variables if any (There are no categorical variables in this dataset) # Split the dataset into training and testing sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def train_and_evaluate(X_train, X_test, y_train, y_test): # Train the Support Vector Classifier (SVC) model = SVC() model.fit(X_train, y_train) # Predict on the test set y_pred = model.predict(X_test) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) # Plot the confusion matrix ConfusionMatrixDisplay.from_predictions(y_test, y_pred) plt.title(\\"Confusion Matrix\\") plt.show() return accuracy # Example usage: # X_train, X_test, y_train, y_test = fetch_and_preprocess_data() # accuracy = train_and_evaluate(X_train, X_test, y_train, y_test) # print(accuracy)"},{"question":"# Coding Assessment: Dimensionality Reduction with scikit-learn # Objective You are required to perform dimensionality reduction using scikit-learn on a given dataset and evaluate the impact of this reduction on a classification task. # Description 1. You will be given a high-dimensional dataset. 2. You are required to apply Principal Component Analysis (PCA) to reduce the number of features. 3. Train a classifier on both the original and reduced datasets to compare performance. # Input - A CSV file named `high_dim_data.csv` containing: - Features in the columns. - Last column named `label` containing the class labels. # Constraints - Use `scikit-learn` for implementing PCA and classification. - Train a `LogisticRegression` classifier from `sklearn.linear_model`. # Steps and Requirements 1. Read the input CSV file. 2. Separate the features and labels. 3. Split the dataset into training (80%) and testing (20%) sets. 4. Apply PCA to reduce the dataset to 2 principal components. 5. Train a `LogisticRegression` classifier on: - The original high-dimensional training data. - The reduced training data from PCA. 6. Evaluate both models on their respective test sets using accuracy as the performance metric. 7. Print the accuracy scores for both models. # Output - Accuracy on the original high-dimensional test set. - Accuracy on the reduced test set after PCA. # Code Template ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Step 1: Read CSV File data = pd.read_csv(\'high_dim_data.csv\') # Step 2: Separate features and labels X = data.drop(columns=[\'label\']) y = data[\'label\'] # Step 3: Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 4: Apply PCA pca = PCA(n_components=2) X_train_pca = pca.fit_transform(X_train) X_test_pca = pca.transform(X_test) # Step 5: Train Logistic Regression on original data clf_original = LogisticRegression(max_iter=1000) clf_original.fit(X_train, y_train) y_pred_original = clf_original.predict(X_test) accuracy_original = accuracy_score(y_test, y_pred_original) # Step 6: Train Logistic Regression on reduced data clf_pca = LogisticRegression(max_iter=1000) clf_pca.fit(X_train_pca, y_train) y_pred_pca = clf_pca.predict(X_test_pca) accuracy_pca = accuracy_score(y_test, y_pred_pca) # Step 7: Print accuracy scores print(f\'Accuracy on original high-dimensional data: {accuracy_original:.4f}\') print(f\'Accuracy on reduced data (PCA): {accuracy_pca:.4f}\') ``` **Notes**: - Ensure the `high_dim_data.csv` exists in the current working directory. - Handle any potential discrepancies or errors in the dataset (e.g., NaN values). # Insights This task assesses your ability to handle preprocessing, perform unsupervised learning, and evaluate the results using supervised learning. It combines concepts from data reduction, classification, and model evaluation.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def dimensionality_reduction_with_evaluation(file_path): # Step 1: Read CSV File data = pd.read_csv(file_path) # Step 2: Separate features and labels X = data.drop(columns=[\'label\']) y = data[\'label\'] # Step 3: Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 4: Apply PCA pca = PCA(n_components=2) X_train_pca = pca.fit_transform(X_train) X_test_pca = pca.transform(X_test) # Step 5: Train Logistic Regression on original data clf_original = LogisticRegression(max_iter=1000) clf_original.fit(X_train, y_train) y_pred_original = clf_original.predict(X_test) accuracy_original = accuracy_score(y_test, y_pred_original) # Step 6: Train Logistic Regression on reduced data clf_pca = LogisticRegression(max_iter=1000) clf_pca.fit(X_train_pca, y_train) y_pred_pca = clf_pca.predict(X_test_pca) accuracy_pca = accuracy_score(y_test, y_pred_pca) # Step 7: Return accuracy scores return accuracy_original, accuracy_pca"},{"question":"**Objective:** Demonstrate understanding of the Python `fractions` module and its capabilities. **Problem Statement:** Create a function named `fraction_operations` that accepts two numeric strings representing fractions and performs various operations to return a dictionary with the following keys and their corresponding values: 1. `\\"Sum\\"`: The fractional sum of the two fractions. 2. `\\"Difference\\"`: The fractional difference when the second fraction is subtracted from the first. 3. `\\"Product\\"`: The fractional product of the two fractions. 4. `\\"Quotient\\"`: The fractional quotient when the first fraction is divided by the second. 5. `\\"First_Rounded\\"`: The first fraction rounded to the nearest integer. 6. `\\"Second_Rounded\\"`: The second fraction rounded to the nearest integer. 7. `\\"First_Limited\\"`: The first fraction approximated to a denominator of at most 50. 8. `\\"Second_Limited\\"`: The second fraction approximated to a denominator of at most 50. **Function Signature:** ```python def fraction_operations(frac1: str, frac2: str) -> dict: pass ``` **Constraints:** - The input strings will be valid representations of fractions as accepted by `fractions.Fraction`. - The denominator for the `limit_denominator` operation will not exceed 50. **Example:** ```python >>> result = fraction_operations(\\"3/4\\", \\"5/6\\") >>> result { \\"Sum\\": \\"19/12\\", \\"Difference\\": \\"-1/12\\", \\"Product\\": \\"5/8\\", \\"Quotient\\": \\"9/10\\", \\"First_Rounded\\": \\"1\\", \\"Second_Rounded\\": \\"1\\", \\"First_Limited\\": \\"3/4\\", \\"Second_Limited\\": \\"5/6\\" } ``` **Notes:** 1. Use the `fractions.Fraction` class to handle all fraction-related operations. 2. Return the resulting fractions as strings in the dictionary. **Implementation Details:** 1. Construct `Fraction` instances using the input strings. 2. Use appropriate arithmetic operations to calculate sum, difference, product, and quotient. 3. Use the `round` function to round the fractions. 4. Use the `limit_denominator` method with a maximum denominator of 50 for the denominator-limited fractions. 5. Convert the resulting fractions back to strings for inclusion in the result dictionary.","solution":"from fractions import Fraction def fraction_operations(frac1: str, frac2: str) -> dict: f1 = Fraction(frac1) f2 = Fraction(frac2) result = { \\"Sum\\": str(f1 + f2), \\"Difference\\": str(f1 - f2), \\"Product\\": str(f1 * f2), \\"Quotient\\": str(f1 / f2), \\"First_Rounded\\": str(round(f1)), \\"Second_Rounded\\": str(round(f2)), \\"First_Limited\\": str(f1.limit_denominator(50)), \\"Second_Limited\\": str(f2.limit_denominator(50)) } return result"},{"question":"# **Coding Assessment Question** **Question: Managing Environment Variables and Handling Large Files** Objective: You are required to write a Python program that manages environment variables and handles large files using the `os` module, which mimics functionalities provided by the `posix` module. Task: 1. **Environment Variable Operations:** - Write a function `set_environment_variable(key: str, value: str) -> None` that sets a new environment variable or updates an existing one. - Write a function `get_environment_variable(key: str) -> str` that retrieves the value of an environment variable. If the variable does not exist, return `\\"Variable not found\\"`. 2. **Large File Handling:** - Write a function `create_large_file(file_path: str, size_in_gb: int) -> None` that creates a file of the specified size (in GB). This function should handle cases where filesystem limitations or other errors might prevent the file creation. Input and Output Format: - The `set_environment_variable(key: str, value: str)` function has two string parameters, `key` and `value`, representing the environment variable\'s name and its value respectively. - The `get_environment_variable(key: str)` function has one string parameter, `key`, representing the environment variable\'s name. - The `create_large_file(file_path: str, size_in_gb: int)` function has a string parameter, `file_path`, representing the path where the file should be created, and an integer `size_in_gb`, representing the size of the file in gigabytes. Constraints: - Ensure that the `set_environment_variable` and `get_environment_variable` functions use the `os.environ` dictionary. - The file size limitations should be handled gracefully with appropriate exception handling mechanisms, raising an `OSError` if creating the file fails for any reason. - Assume the system supports large files if `off_t` is larger than `long`. Example: ```python import os def set_environment_variable(key: str, value: str) -> None: os.environ[key] = value def get_environment_variable(key: str) -> str: return os.environ.get(key, \\"Variable not found\\") def create_large_file(file_path: str, size_in_gb: int) -> None: try: with open(file_path, \'wb\') as file: file.seek(size_in_gb * 1024**3 - 1) file.write(b\'0\') except OSError as e: raise OSError(f\\"Failed to create file: {e}\\") # Example usage set_environment_variable(\'MY_VAR\', \'42\') print(get_environment_variable(\'MY_VAR\')) # Output: \'42\' print(get_environment_variable(\'NON_EXISTENT\')) # Output: \'Variable not found\' try: create_large_file(\'large_file.txt\', 1) except OSError as e: print(e) ``` The above functions will help you read, write, and manage environment variables and handle large file creation, leveraging the enhanced functionalities provided by the `os` module based on POSIX standards.","solution":"import os def set_environment_variable(key: str, value: str) -> None: Sets a new environment variable or updates an existing one. os.environ[key] = value def get_environment_variable(key: str) -> str: Retrieves the value of an environment variable. If the variable does not exist, returns \\"Variable not found\\". return os.environ.get(key, \\"Variable not found\\") def create_large_file(file_path: str, size_in_gb: int) -> None: Creates a file of the specified size in gigabytes. Handles errors if filesystem limitations or other issues prevent file creation. try: with open(file_path, \'wb\') as file: # Set the file size by seeking to the desired size - 1 byte file.seek(size_in_gb * 1024**3 - 1) file.write(b\'0\') except OSError as e: raise OSError(f\\"Failed to create file: {e}\\")"},{"question":"**Objective**: Implement a function `generate_multi_format_diff` that compares two sequences of lines of text and produces differences in four formats: context, unified, ndiff, and HTML. The function should allow for configurable context lines and handle different character encoding scenarios. **Function Signature**: ```python def generate_multi_format_diff(text1: str, text2: str, format_type: str, context_lines: int = 3, encoding: str = \'utf-8\') -> str: ``` **Parameters**: - `text1` (str): A string representing the first sequence of text. - `text2` (str): A string representing the second sequence of text. - `format_type` (str): The format of the diff output. It can be one of the following: `\'context\'`, `\'unified\'`, `\'ndiff\'`, `\'html\'`. - `context_lines` (int, optional): The number of context lines to show. Defaults to 3. - `encoding` (str, optional): The character encoding to use for the diff output. Defaults to \'utf-8\'. **Returns**: - `str`: A string representing the differences between `text1` and `text2` in the specified format. **Constraints**: - `format_type` must be one of `\'context\'`, `\'unified\'`, `\'ndiff\'`, or `\'html\'`. - The function should handle different character encodings properly. - The input texts may contain newline characters and should be compared line by line. - Ensure the output is human-readable and correctly formatted according to the specified diff type. # Example Usage ```python text1 = Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. text2 = Beautiful is better than ugly. Simple is better than complex. Complicated is better than complex. Flat is better than nested. # Example call for context diff print(generate_multi_format_diff(text1, text2, format_type=\'context\')) # Example call for unified diff print(generate_multi_format_diff(text1, text2, format_type=\'unified\')) # Example call for ndiff print(generate_multi_format_diff(text1, text2, format_type=\'ndiff\')) # Example call for html diff print(generate_multi_format_diff(text1, text2, format_type=\'html\')) ``` **Notes**: - You may find the functions `difflib.context_diff`, `difflib.unified_diff`, `difflib.ndiff`, and `difflib.HtmlDiff` particularly useful. - Handle exceptions properly to ensure the function does not crash for invalid inputs.","solution":"import difflib def generate_multi_format_diff(text1: str, text2: str, format_type: str, context_lines: int = 3, encoding: str = \'utf-8\') -> str: Generates a diff between text1 and text2 in the specified format. lines1 = text1.splitlines(keepends=True) lines2 = text2.splitlines(keepends=True) try: if format_type == \'context\': diff = difflib.context_diff(lines1, lines2, n=context_lines) elif format_type == \'unified\': diff = difflib.unified_diff(lines1, lines2, n=context_lines) elif format_type == \'ndiff\': diff = difflib.ndiff(lines1, lines2) elif format_type == \'html\': html_diff = difflib.HtmlDiff() return html_diff.make_file(lines1, lines2).encode(encoding).decode(encoding) else: raise ValueError(\\"Invalid format_type. Allowed values are \'context\', \'unified\', \'ndiff\', \'html\'.\\") return \'\'.join(diff) except Exception as e: return str(e)"},{"question":"# Question You are provided with a dataset containing information about the sales figures of various products across different regions over multiple years. You are required to visualize this data using seaborn in an effective and informative way. **Dataset Description:** The dataset `sales_data` has the following columns: - `Year`: The year of the sales data. - `Region`: The region where the sales were made. - `Product`: The product identifier. - `Sales`: The sales figure for the product in that region for the specified year. **Tasks:** 1. **Load the Data**: Create a pandas DataFrame from a given CSV file path `csv_path`. 2. **Bar Plot Basic**: Create a bar plot showing the average sales for each region. Assign `Region` to the x-axis and mean `Sales` to the y-axis. 3. **Grouped Bar Plot**: Modify the previous plot to include product-wise breakdown within each region using the `Product` field. Use `hue` to distinguish between different products. 4. **Error Bars**: Adjust the grouped bar plot to display standard deviation as the error bars. 5. **Customize Plot**: - Rotate the x-axis labels for better readability. - Add a title \\"Average Sales by Region and Product\\". - Customize the plot to change the bar edge color to `.2` (20% grey) and add caps on the error bars with a capsize of 0.2. 6. **Text Labels**: - Add text labels on top of each bar to display the mean sales value. - Ensure the text labels are clearly readable. 7. **Annotations**: - Highlight the bar with the highest mean sales by changing its color to red. - Add a star marker at the highest mean sales bar to draw attention. 8. **Save and Display**: - Save the plot to a file called `sales_plot.png`. - Display the plot using `plt.show()` command. **Expected Input and Output:** - **Input**: The function `visualize_sales(csv_path)` receives a file path to the CSV file as its input. - **Output**: The function should not return anything but should save the plot to `sales_plot.png` and display it. **Constraints:** - Use only seaborn and matplotlib for visualization. - Do not modify the dataset before the visualization steps. # Function Signature ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_sales(csv_path: str) -> None: # Your implementation here ``` **Example Usage:** ```python csv_path = \'path_to_sales_data.csv\' visualize_sales(csv_path) ``` **Further Notes:** - Ensure your code is well-commented and follows best practices for readability and maintainability. - Use the public seaborn dataset functions to practice before implementing your solution.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_sales(csv_path: str) -> None: # Load the data sales_data = pd.read_csv(csv_path) # Set the aesthetic style of the plots sns.set_style(\\"whitegrid\\") # 1. Bar Plot showing the average sales for each region plt.figure(figsize=(10, 6)) avg_sales_plot = sns.barplot(x=\'Region\', y=\'Sales\', data=sales_data, ci=\'sd\', color=\'b\', errcolor=\'gray\', edgecolor=\'.2\') # Add error bars with standard deviation plt.figure(figsize=(12, 8)) ax = sns.barplot(x=\'Region\', y=\'Sales\', hue=\'Product\', data=sales_data, ci=\'sd\', palette=\'muted\', errcolor=\'gray\', edgecolor=\'.2\', capsize=0.2) # Customize the plot ax.set_title(\\"Average Sales by Region and Product\\") ax.set_ylabel(\\"Average Sales\\") ax.set_xlabel(\\"Region\\") plt.xticks(rotation=45) # Add text labels on top of each bar for p in ax.patches: height = p.get_height() ax.annotate(f\'{height:.1f}\', (p.get_x() + p.get_width() / 2., height), ha=\'center\', va=\'center\', xytext=(0, 9), textcoords=\'offset points\') # Highlight the highest mean sales bar max_height = max([p.get_height() for p in ax.patches]) for p in ax.patches: if p.get_height() == max_height: p.set_color(\'red\') ax.plot([p.get_x() + p.get_width() / 2], [max_height], marker=\'*\', markersize=15, color=\'red\') # Save and display the plot plt.savefig(\'sales_plot.png\') plt.show()"},{"question":"**Title: Comprehensive Package Metadata Retrieval Tool** **Objective:** Your task is to create a utility class that utilizes the `importlib.metadata` module to extract and display comprehensive metadata from installed packages in Python. This will test your understanding of the `importlib.metadata` module and ability to implement functionalities using Python 3.10. **Description:** You are required to implement a class named `PackageMetadataRetriever`. This class should provide methods to fetch various metadata aspects of installed packages. The constructor of the class will receive the package name as an argument. **Expected Class and Methods:** ```python class PackageMetadataRetriever: def __init__(self, package_name: str): Initializes the retriever with the specified package name. :param package_name: The name of the package to retrieve metadata for. self.package_name = package_name def get_version(self) -> str: Retrieves the version of the package. :return: The version of the package as a string. # Implement this method def get_metadata(self) -> dict: Retrieves all metadata of the package. :return: A dictionary containing all metadata. # Implement this method def get_entry_points(self) -> dict: Retrieves all entry points of the package. :return: A dictionary with entry points grouped by their category. # Implement this method def get_files(self) -> list: Retrieves all files installed by the package. :return: A list of file paths installed by the package. # Implement this method def get_requirements(self) -> list: Retrieves all dependencies of the package. :return: A list of requirements for the package. # Implement this method ``` **Input:** - The class `PackageMetadataRetriever` should take a single string input representing the name of the package. **Output:** - The methods should return the following: - `get_version()`: A string representing the version of the package. - `get_metadata()`: A dictionary containing all metadata fields and their values. - `get_entry_points()`: A dictionary where keys represent entry point groups and values are lists of entry points within those groups. - `get_files()`: A list of file paths installed by the package. - `get_requirements()`: A list of strings representing the dependencies of the package. **Constraints and Limitations:** - Assume the package provided has valid metadata available. - Ensure that the methods handle exceptions gracefully if the package metadata cannot be retrieved. - Use relevant functions from the `importlib.metadata` to implement the methods. **Example Usage:** ```python # Example package: \'wheel\' retriever = PackageMetadataRetriever(\'wheel\') print(retriever.get_version()) # Output: \'0.32.3\' print(retriever.get_metadata()) # Output: {\'Metadata-Version\': \'2.1\', \'Name\': \'wheel\', \'Version\': \'0.32.3\', ...} print(retriever.get_entry_points()) # Output: {\'console_scripts\': [\'wheel=wheel.cli:main\'], ...} print(retriever.get_files()) # Output: [\'wheel/__init__.py\', \'wheel/bdist_wheel.py\', ...] print(retriever.get_requirements()) # Output: [\\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\"] ``` Ensure your implementation leverages the `importlib.metadata` module effectively and tests for edge cases where metadata might be incomplete or not found.","solution":"from importlib.metadata import version, metadata, entry_points, files class PackageMetadataRetriever: def __init__(self, package_name: str): Initializes the retriever with the specified package name. :param package_name: The name of the package to retrieve metadata for. self.package_name = package_name def get_version(self) -> str: Retrieves the version of the package. :return: The version of the package as a string. try: return version(self.package_name) except Exception as e: return str(e) def get_metadata(self) -> dict: Retrieves all metadata of the package. :return: A dictionary containing all metadata. try: return dict(metadata(self.package_name)) except Exception as e: return {\\"error\\": str(e)} def get_entry_points(self) -> dict: Retrieves all entry points of the package. :return: A dictionary with entry points grouped by their category. try: eps = entry_points() entries = eps.select(name=self.package_name) result = {} for entry in entries: if entry.group not in result: result[entry.group] = [] result[entry.group].append(entry.name) return result except Exception as e: return {\\"error\\": str(e)} def get_files(self) -> list: Retrieves all files installed by the package. :return: A list of file paths installed by the package. try: return [str(file) for file in files(self.package_name)] except Exception as e: return [str(e)] def get_requirements(self) -> list: Retrieves all dependencies of the package. :return: A list of requirements for the package. try: data = metadata(self.package_name) return data.get_all(\'Requires-Dist\', []) except Exception as e: return [str(e)]"},{"question":"Objectives: 1. Demonstrate understanding of creating color palettes using seaborn. 2. Understand how to customize color palettes using different color specifications. 3. Ability to visualize these palettes effectively. Problem Statement: You are provided with a dataset that contains random data points. Your task is to create and visualize different color palettes using seaborn, based on specified criteria. Instructions: 1. Create a pandas DataFrame with 50 rows of random data with two columns: `value1` and `value2`. Use `np.random.rand(50)` to generate the random values for these columns. 2. Using seaborn, create the following color palettes: - A sequential palette starting from dark gray to the color \\"seagreen\\". - A palette using the hex color `#79C`. - A palette using the HUSL color system with parameters `(20, 60, 50)`. - A palette with 10 colors using the color \\"xkcd:golden\\". 3. Visualize each palette using the seaborn `heatmap` function, applying it to the DataFrame created in step 1. Ensure that: - For the palette created with \\"xkcd:golden,\\" the number of colors is set to 10. - For the other palettes, use the default number of colors. 4. Return a continuous colormap using the hex color `#b285bc` and apply it to a seaborn `kdeplot` of the column `value1`. Ensure the plot is smooth and visually appealing. Constraints: - You must use seaborn for all palette creation and visualization tasks. - Each plot should have a title indicating which palette or colormap it is using. Expected Output: - Four heatmap visualizations, each demonstrating the respective color palettes. - One kernel density estimate (KDE) plot using the continuous colormap. Example: Here is an example of the plot titles: - \\"Heatmap with sequential palette using \'seagreen\'\\" - \\"Heatmap with hex color palette `#79C`\\" - \\"Heatmap using HUSL parameters `(20, 60, 50)`\\" - \\"Heatmap with 10 color palette \'xkcd:golden\'\\" - \\"KDE plot with continuous colormap `#b285bc`\\" Additional Notes: - Ensure your submission is clean, with all necessary imports and no unnecessary outputs. - Assumptions made, if any, should be documented clearly. Code Template: ```python import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt # Step 1: Create DataFrame data = pd.DataFrame({ \'value1\': np.random.rand(50), \'value2\': np.random.rand(50) }) # Step 2: Create color palettes sequential_palette = sns.dark_palette(\\"seagreen\\") hex_color_palette = sns.dark_palette(\\"#79C\\") husl_palette = sns.dark_palette((20, 60, 50), input=\\"husl\\") xkcd_palette = sns.dark_palette(\\"xkcd:golden\\", 10) # Step 3: Visualize palettes with heatmap plt.figure(figsize=(12, 8)) sns.heatmap(data, cmap=sequential_palette).set_title(\\"Heatmap with sequential palette using \'seagreen\'\\") plt.figure(figsize=(12, 8)) sns.heatmap(data, cmap=hex_color_palette).set_title(\\"Heatmap with hex color palette \'#79C\'\\") plt.figure(figsize=(12, 8)) sns.heatmap(data, cmap=husl_palette).set_title(\\"Heatmap using HUSL parameters \'(20, 60, 50)\'\\") plt.figure(figsize=(12, 8)) sns.heatmap(data, cmap=xkcd_palette, cbar_kws={\'ticks\': 10}).set_title(\\"Heatmap with 10 color palette \'xkcd:golden\'\\") # Step 4: KDE plot with continuous colormap continuous_cmap = sns.dark_palette(\\"#b285bc\\", as_cmap=True) plt.figure(figsize=(12, 8)) sns.kdeplot(data[\'value1\'], shade=True, cmap=continuous_cmap).set_title(\\"KDE plot with continuous colormap \'#b285bc\'\\") plt.show() ```","solution":"import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt # Step 1: Create DataFrame data = pd.DataFrame({ \'value1\': np.random.rand(50), \'value2\': np.random.rand(50) }) # Step 2: Create color palettes sequential_palette = sns.light_palette(\\"seagreen\\", as_cmap=True) hex_color_palette = sns.light_palette(\\"#79C\\", as_cmap=True) husl_palette = sns.light_palette((20, 60, 50), input=\\"husl\\", as_cmap=True) xkcd_palette = sns.light_palette(\\"xkcd:golden\\", 10, as_cmap=True) # Step 3: Visualize palettes with heatmap plt.figure(figsize=(12, 8)) sns.heatmap(data, cmap=sequential_palette).set_title(\\"Heatmap with sequential palette using \'seagreen\'\\") plt.figure(figsize=(12, 8)) sns.heatmap(data, cmap=hex_color_palette).set_title(\\"Heatmap with hex color palette \'#79C\'\\") plt.figure(figsize=(12, 8)) sns.heatmap(data, cmap=husl_palette).set_title(\\"Heatmap using HUSL parameters \'(20, 60, 50)\'\\") plt.figure(figsize=(12, 8)) sns.heatmap(data, cmap=xkcd_palette).set_title(\\"Heatmap with 10 color palette \'xkcd:golden\'\\") # Step 4: KDE plot with continuous colormap continuous_cmap = sns.light_palette(\\"#b285bc\\", as_cmap=True) plt.figure(figsize=(12, 8)) sns.kdeplot(data[\'value1\'], shade=True, cmap=continuous_cmap).set_title(\\"KDE plot with continuous colormap \'#b285bc\'\\") plt.show()"},{"question":"Objective Implement a custom class that makes use of the `copy` module to provide tailored shallow and deep copy functionalities. This will test your understanding of OOP principles, custom methods, and deep versus shallow copying in Python. Problem Statement You are required to implement a class `Node` that represents a node in a tree. Each `Node` object contains a value and a list of child `Node` objects. Your task is to provide custom implementations for shallow and deep copying of this `Node` class. Class Definition ```python class Node: def __init__(self, value): self.value = value self.children = [] def add_child(self, child_node): self.children.append(child_node) def __copy__(self): Implement the method to return a shallow copy of this node. pass def __deepcopy__(self, memo): Implement the method to return a deep copy of this node. pass ``` Requirements 1. **Shallow Copy (`__copy__` method)**: - Should return a new Node object with the same value. - The children list should contain the same child references as the original `Node`. 2. **Deep Copy (`__deepcopy__` method)**: - Should return a new Node object with the same value. - Should recursively create deep copies for each child node. - Use the `memo` dictionary to handle recursive structures and optimize the deep copy operation. Example ```python import copy # Creating a tree structure root = Node(1) child1 = Node(2) child2 = Node(3) root.add_child(child1) root.add_child(child2) child1.add_child(Node(4)) child1.add_child(Node(5)) # Shallow copy of the root shallow_copied_root = copy.copy(root) # Deep copy of the root deep_copied_root = copy.deepcopy(root) # Make sure the shallow copy is correctly implemented assert shallow_copied_root.value == root.value assert shallow_copied_root.children is root.children # Make sure the deep copy is correctly implemented assert deep_copied_root.value == root.value assert deep_copied_root.children is not root.children assert deep_copied_root.children[0] is not root.children[0] ``` Constraints - You should not use any external libraries except for Python\'s standard library (including the `copy` module). - Ensure that your implementation is efficient and handles large tree structures gracefully. Performance - Your implementation should be able to handle at least 1000 nodes with reasonable efficiency for both shallow and deep copying operations.","solution":"import copy class Node: def __init__(self, value): self.value = value self.children = [] def add_child(self, child_node): self.children.append(child_node) def __copy__(self): Implement the method to return a shallow copy of this node. new_node = Node(self.value) new_node.children = self.children return new_node def __deepcopy__(self, memo): Implement the method to return a deep copy of this node. if id(self) in memo: return memo[id(self)] new_node = Node(self.value) memo[id(self)] = new_node new_node.children = [copy.deepcopy(child, memo) for child in self.children] return new_node"},{"question":"# Question: Implement a Custom Scheduling System Using a Priority Queue You are tasked with implementing a custom scheduling system for tasks, where each task has a specific priority and a timestamp indicating when it was added. # Objectives: - Implement a scheduling system that supports adding tasks, removing tasks, and retrieving the next task based on priority and timestamp. - Ensure that tasks with the same priority are handled based on their timestamp (earliest first). # Requirements: 1. **Function: `add_task(priority_queue, task, priority, timestamp)`** - **Input:** - `priority_queue`: The priority queue data structure. - `task`: A string representing the task. - `priority`: An integer representing the priority. Lower numbers indicate higher priority. - `timestamp`: A string representing the timestamp in UTC format (`YYYY-MM-DDTHH:MM:SSZ`). - **Output:** None. - **Behavior:** Adds the task to the priority queue with the specified priority and timestamp. 2. **Function: `remove_task(priority_queue, task)`** - **Input:** - `priority_queue`: The priority queue data structure. - `task`: A string representing the task to be removed. - **Output:** Boolean indicating success (True if the task was found and removed, False if the task was not found). - **Behavior:** Removes the specified task from the priority queue. 3. **Function: `next_task(priority_queue)`** - **Input:** - `priority_queue`: The priority queue data structure. - **Output:** A tuple containing the task and its priority. - **Behavior:** Retrieves and removes the task with the highest priority and earliest timestamp. Returns `None` if the queue is empty. # Constraints: - Assume priority values are non-negative integers. - No two tasks have the same timestamp. - The scheduling system should handle at least 10^4 tasks efficiently. - The input timestamps are valid and in the correct format. # Example Usage: ```python # Initializing an empty priority queue priority_queue = [] # Adding tasks add_task(priority_queue, \'write code\', 1, \'2023-10-01T08:30:00Z\') add_task(priority_queue, \'release product\', 2, \'2023-10-01T09:00:00Z\') add_task(priority_queue, \'write spec\', 1, \'2023-10-01T07:30:00Z\') # Getting the next task (expected: (\'write spec\', 1)) next_task(priority_queue) # Removing a task (expected: True) remove_task(priority_queue, \'write code\') # Getting the next task (expected: (\'release product\', 2)) next_task(priority_queue) ``` # Hints: - You might need to use tuples to handle priority and timestamps effectively. - Consider using a dictionary for task retrieval to efficiently implement the removal of tasks. - Use the `heapq` module for maintaining the priority queue properties.","solution":"import heapq from datetime import datetime def add_task(priority_queue, task, priority, timestamp): Adds the task to the priority queue with the specified priority and timestamp. :param priority_queue: The priority queue data structure as a list of tuples (priority, timestamp, task). :param task: A string representing the task. :param priority: An integer representing the priority. Lower numbers indicate higher priority. :param timestamp: A string representing the timestamp in UTC format (`YYYY-MM-DDTHH:MM:SSZ`). heapq.heappush(priority_queue, (priority, timestamp, task)) def remove_task(priority_queue, task): Removes the specified task from the priority queue. :param priority_queue: The priority queue data structure. :param task: A string representing the task to be removed. :return: Boolean indicating success (True if the task was found and removed, False if the task was not found). found = False for i, item in enumerate(priority_queue): if item[2] == task: found = True # Remove item and rebuild the heap priority_queue[i] = priority_queue[-1] priority_queue.pop() heapq.heapify(priority_queue) break return found def next_task(priority_queue): Retrieves and removes the task with the highest priority and earliest timestamp. :param priority_queue: The priority queue data structure. :return: A tuple containing the task and its priority, or None if the queue is empty. if not priority_queue: return None priority, timestamp, task = heapq.heappop(priority_queue) return (task, priority)"},{"question":"# PyTorch Special Functions Assessment **Objective:** You are required to implement a function that computes a specialized mathematical result utilizing various functions from the `torch.special` module. # Problem Statement Write a function `special_computation` that performs the following steps: 1. Takes as input a tensor `x` of real numbers. 2. Computes the modified Bessel function of the first kind of order 0 (`i0`). 3. Computes the natural logarithm of one plus each element in the tensor (`log1p`). 4. Computes the error function of each element in the modified tensor (`erf`). 5. Finally, sums all the elements in the resulting tensor. # Function Signature: ```python import torch def special_computation(x: torch.Tensor) -> torch.Tensor: # Your implementation here pass ``` # Input: - `x`: A 1D tensor of real numbers with shape (n,), where `n` can be any positive integer. # Output: - Returns a single scalar tensor which is the result of the described computation. # Example: ```python import torch x = torch.tensor([0.1, 0.2, 0.3, 0.4]) result = special_computation(x) print(result) # Expected output should be a scalar tensor. ``` # Constraints: - Your implementation should make use of the functions from the `torch.special` module. - You should not use any loops; instead, leverage PyTorch\'s tensor operations. - The computation should be numerically stable and efficient. # Performance Requirement: - The function should be optimized to handle large tensors efficiently both in terms of time and memory usage. # Additional Information: - Refer to the PyTorch documentation for information on the `torch.special` module: [PyTorch Special Functions](https://pytorch.org/docs/stable/special.html). # Notes: - Ensure your solution is well-tested with a variety of inputs to demonstrate correctness and efficiency.","solution":"import torch def special_computation(x: torch.Tensor) -> torch.Tensor: Performs a series of specialized mathematical operations on the input tensor `x`. Steps: 1. Computes the modified Bessel function of the first kind of order 0 (`i0`). 2. Computes the natural logarithm of one plus each element in the tensor (`log1p`). 3. Computes the error function of each element in the modified tensor (`erf`). 4. Sums all the elements in the resulting tensor. Args: x (torch.Tensor): A 1D tensor of real numbers. Returns: torch.Tensor: A single scalar tensor which is the sum result. # Step 1: Compute the modified Bessel function of the first kind of order 0 x_i0 = torch.special.i0(x) # Step 2: Compute the natural logarithm of one plus each element in the tensor x_log1p = torch.log1p(x_i0) # Step 3: Compute the error function of each element in the tensor x_erf = torch.erf(x_log1p) # Step 4: Sum all the elements in the resulting tensor result = torch.sum(x_erf) return result"},{"question":"# Python Coding Assessment: Advanced Unit Testing You are required to write a function and a corresponding set of unit tests for that function using Python\'s `unittest` module along with utilities from the `test.support` module. Function Implementation Implement a function `process_data(data)` which processes a list of integers and returns a dictionary with the following keys: 1. `\'sum\'`: The sum of all integers. 2. `\'average\'`: The average of all integers. 3. `\'max\'`: The maximum value. 4. `\'min\'`: The minimum value. 5. `\'positive\'`: A list of all positive integers. 6. `\'negative\'`: A list of all negative integers. # Constraints - The input list may include zero, positive, and negative integers. - You may not use the built-in functions `sum` or `max`/`min`. Example ```python data = [1, -2, 3, 0, -4, 6] result = process_data(data) # result should be: # { # \'sum\': 4, # \'average\': 2 / 3, # 2 divided by the number of non-zero elements # \'max\': 6, # \'min\': -4, # \'positive\': [1, 3, 6], # \'negative\': [-2, -4] # } ``` Unit Tests Write a set of unit tests for the `process_data` function making use of the `unittest` module features provided by `test.support`. The unit tests should: 1. Test the function with an empty list. 2. Test the function with a list containing all positive integers. 3. Test the function with a list containing all negative integers. 4. Test the function with a list containing a mix of positive and negative integers, including zero. 5. Utilize at least three different utilities from the `test.support` module to assist in testing (e.g., context managers, helper functions, or decorators). Your unit test class should be named `TestProcessData` and should follow the best practices for writing unit tests as documented in the `test` package documentation. Submission Submit your Python file containing the `process_data` function and the `TestProcessData` unit test class. Ensure all tests are self-contained and do not require any external dependencies.","solution":"def process_data(data): Processes a list of integers and returns a dictionary with the sum, average, max, min, positive integers, and negative integers. if not data: return { \'sum\': 0, \'average\': None, \'max\': None, \'min\': None, \'positive\': [], \'negative\': [], } total_sum = 0 max_val = data[0] min_val = data[0] positive_numbers = [] negative_numbers = [] for num in data: total_sum += num if num > 0: positive_numbers.append(num) elif num < 0: negative_numbers.append(num) if num > max_val: max_val = num if num < min_val: min_val = num average = total_sum / len(data) return { \'sum\': total_sum, \'average\': average, \'max\': max_val, \'min\': min_val, \'positive\': positive_numbers, \'negative\': negative_numbers, }"},{"question":"**Question: Analyzing and Visualizing Data with Seaborn KDE Plots** You are given the dataset `penguins` which contains information about various penguin species. Your task is to create a detailed KDE plot to visualize and analyze the distribution of the penguins\' flipper lengths. Moreover, you should include additional modifications and customizations to the plot to represent significant insights effectively. Follow the steps and guidelines provided below: # Dataset Description The `penguins` dataset contains the following columns: - `species`: Species of the penguin (Adelie, Chinstrap, Gentoo). - `island`: Island where the penguin was found. - `bill_length_mm`: Length of the bill (in mm). - `bill_depth_mm`: Depth of the bill (in mm). - `flipper_length_mm`: Length of the flipper (in mm). - `body_mass_g`: Body mass (in g). - `sex`: Sex of the penguin. # Tasks 1. **Load the Dataset**: Load the `penguins` dataset using seaborn\'s `load_dataset` function. 2. **Basic KDE Plot**: Create a basic univariate KDE plot for `flipper_length_mm`. 3. **Custom KDE Plot**: - Plot KDEs separately for each `species` using the `hue` parameter. - Use `bw_adjust=0.5` to adjust the smoothing bandwidth. - Use a `multiple=\\"stack\\"` mapping to stack the KDE plots by species. 4. **Bivariate KDE Plot**: - Create a bivariate KDE plot between `flipper_length_mm` and `body_mass_g`. - Use a `hue` mapping based on `species`. 5. **Enhanced Visualization**: - Customize the appearance by using a different color palette (e.g., `viridis`). - Add appropriate axis labels and a title to the plot for better readability and context. # Expected Output - The output should include two plots: 1. A multilayer univariate KDE plot showcasing the distribution of `flipper_length_mm` with hue mapping by `species`. 2. A bivariate KDE plot between `flipper_length_mm` and `body_mass_g` with hue mapping by `species`. # Constraints - Ensure all plots are clearly labeled and distinguishable. - Avoid overlapping and ensure the KDE plots are smooth. - The hue legend should be clear and readable. # Submission Submit the completed Jupyter notebook code containing the following: - Code for loading the dataset. - Implementation of each KDE plot as specified. - Display of the final plots with customization. ```python # Example Structure for the Implementation import seaborn as sns import matplotlib.pyplot as plt # 1. Load the dataset penguins = sns.load_dataset(\\"penguins\\") # 2. Basic KDE plot for flipper_length_mm sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"Basic KDE Plot for Flipper Length\\") plt.show() # 3. Custom KDE plot with species hue sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", bw_adjust=0.5, multiple=\\"stack\\") plt.title(\\"Stacked KDE Plot for Flipper Length by Species\\") plt.show() # 4. Bivariate KDE plot with species hue sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\") plt.title(\\"Bivariate KDE Plot: Flipper Length vs. Body Mass by Species\\") plt.show() # 5. Enhanced visualization with different palette sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", fill=True, palette=\\"viridis\\", bw_adjust=0.5, multiple=\\"stack\\") plt.title(\\"Enhanced Stacked KDE Plot for Flipper Length by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() ``` Make sure to interpret the plots and provide insights based on the visualizations where applicable.","solution":"import seaborn as sns import matplotlib.pyplot as plt # 1. Load the dataset penguins = sns.load_dataset(\\"penguins\\") # 2. Basic KDE plot for flipper_length_mm plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"Basic KDE Plot for Flipper Length\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() # 3. Custom KDE plot with species hue plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", bw_adjust=0.5, multiple=\\"stack\\") plt.title(\\"Stacked KDE Plot for Flipper Length by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() # 4. Bivariate KDE plot with species hue plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\") plt.title(\\"Bivariate KDE Plot: Flipper Length vs. Body Mass by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Body Mass (g)\\") plt.show() # 5. Enhanced visualization with different palette plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", fill=True, palette=\\"viridis\\", bw_adjust=0.5, multiple=\\"stack\\") plt.title(\\"Enhanced Stacked KDE Plot for Flipper Length by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show()"},{"question":"Context: You have been hired by a tech company to work on their large-scale data processing pipeline. To improve the efficiency of the pipeline, you need to leverage Python\'s functional programming tools to simplify some data processing tasks. Task: Write a function `process_data_pipeline` that takes three arguments: 1. `data_list`: A list of tuples, where each tuple contains numerical data. 2. `filter_func`: A callable that takes a single numerical value and returns a boolean. 3. `transform_func`: A callable that takes a numerical value and transforms it. Your function should perform the following steps: 1. **Flatten the list of tuples**: Convert `data_list` from a list of tuples into a single list of integers. 2. **Filter the data**: Use `filter_func` to filter out values from the flattened list. 3. **Transform the data**: Apply `transform_func` to each of the filtered values. 4. **Sort the data**: Return a sorted list of the transformed values. Constraints: - The length of `data_list` is at most `10^4`. - Each tuple within `data_list` can have varying but reasonable lengths, not exceeding 1000 elements. - Performance and memory usage should be optimized using functional programming tools (`itertools`, `functools`, `operator`). Input: - A list of tuples `data_list` (e.g., `[(1, 2, 3), (4, 5), (6,)]`) - A callable `filter_func` (e.g., `lambda x: x % 2 == 0`) - A callable `transform_func` (e.g., `lambda x: x * x`) Output: - A sorted list of transformed values meeting the filter criteria. # Example: ```python data_list = [(1, 2, 3), (4, 5), (6,)] filter_func = lambda x: x % 2 == 0 transform_func = lambda x: x * x result = process_data_pipeline(data_list, filter_func, transform_func) print(result) # Output: [4, 16, 36] ``` # Requirements: - Use `itertools` to create efficient iterators. - Consider using `functools` for partial applications if needed. - Use `operator` module functions where applicable. Hint: - Consider `itertools.chain` for flattening the data. - Use `filter` for applying the `filter_func`. - Use `map` for applying the `transform_func`. - Remember to return a sorted list at the end.","solution":"from itertools import chain def process_data_pipeline(data_list, filter_func, transform_func): Process the data pipeline as specified by flattening, filtering, transforming, and sorting the data. Parameters: data_list (list of tuples): The input data list of tuples containing numerical data. filter_func (callable): A callable that takes a single numerical value and returns a boolean. transform_func (callable): A callable that takes a numerical value and transforms it. Returns: list: A sorted list of transformed values meeting the filter criteria. # Step 1: Flatten the list of tuples into a single list of integers flattened_data = chain.from_iterable(data_list) # Step 2: Filter the data using filter_func filtered_data = filter(filter_func, flattened_data) # Step 3: Transform the filtered data using transform_func transformed_data = map(transform_func, filtered_data) # Step 4: Return a sorted list of the transformed values return sorted(transformed_data)"},{"question":"# Question: You are tasked with creating a command-line Python program that performs basic mathematical operations. The program should accept a set of operations and their operands, and provide options for verbosity and logging the results to a file. This question will assess your ability to use the `argparse` module to handle both positional and optional arguments, mutually exclusive options, and advanced argument handling. Requirements: 1. **Positional arguments**: - `operation`: A string representing the mathematical operation (\'add\', \'subtract\', \'multiply\', \'divide\'). - `operands`: Two or more integers to perform the operation on. 2. **Optional arguments**: - `--verbose (-v)`: Increase the output verbosity. The more `-v` flags provided, the more detailed the output. - `--output (-o)`: A file path to save the output results. - `--quiet (-q)`: Suppress all output except errors. - `--help (-h)`: Show a help message and exit. 3. **Mutually exclusive arguments**: - `--verbose` and `--quiet` cannot be used together. 4. **Constraints**: - If `operation` is \'divide\', ensure that division by zero is handled correctly. - The program should display a meaningful message if no operands are provided for the specified operation. - The `--output` file should contain the result in plain text format. Expected Input and Output: - Command input: ``` python3 calc.py add 5 10 15 -vv ``` - Expected output: ``` Adding 5, 10, 15 Result: 30 ``` - Command input: ``` python3 calc.py divide 10 2 --quiet ``` - Expected output: ``` (No output, but performs the operation quietly) ``` - Command input: ``` python3 calc.py multiply 3 4 5 -o result.txt ``` - Expected output: ``` (Writes \\"Result: 60\\" to result.txt and prints \\"Result saved to result.txt\\" to the terminal) ``` Implementation Details: 1. Define a function `calculate` to handle the operations and return the result. 2. Use `argparse.ArgumentParser` to parse the command-line arguments. 3. Implement proper error handling, especially for division by zero and missing operands. 4. Ensure that mutually exclusive arguments (`--verbose` and `--quiet`) are correctly handled. 5. Implement file output functionality when the `--output` option is used. ```python import argparse def calculate(operation, operands): if operation == \'add\': return sum(operands) elif operation == \'subtract\': return operands[0] - sum(operands[1:]) elif operation == \'multiply\': result = 1 for op in operands: result *= op return result elif operation == \'divide\': result = operands[0] try: for op in operands[1:]: result /= op except ZeroDivisionError: return \\"Error: Division by zero\\" return result else: return \\"Unsupported operation\\" def main(): parser = argparse.ArgumentParser(description=\\"Perform basic mathematical operations.\\") parser.add_argument(\\"operation\\", type=str, choices=[\'add\', \'subtract\', \'multiply\', \'divide\'], help=\\"The operation to perform (add, subtract, multiply, divide).\\") parser.add_argument(\\"operands\\", type=int, nargs=\'+\', help=\\"Two or more integers to perform the operation on.\\") group = parser.add_mutually_exclusive_group() group.add_argument(\\"-v\\", \\"--verbose\\", action=\\"count\\", default=0, help=\\"Increase output verbosity.\\") group.add_argument(\\"-q\\", \\"--quiet\\", action=\\"store_true\\", help=\\"Suppress all output except errors.\\") parser.add_argument(\\"-o\\", \\"--output\\", type=str, help=\\"Output file to save the result.\\") args = parser.parse_args() result = calculate(args.operation, args.operands) if not args.quiet: if args.verbose >= 2: print(f\\"Performing \'{args.operation}\' on {args.operands}\\") if args.verbose >= 1: print(f\\"Result: {result}\\") if args.output: with open(args.output, \\"w\\") as f: f.write(f\\"Result: {result}\\") print(f\\"Result saved to {args.output}\\") elif args.verbose == 0: print(result) if __name__ == \\"__main__\\": main() ```","solution":"import argparse def calculate(operation, operands): Perform the specified operation on the provided operands. if operation == \'add\': return sum(operands) elif operation == \'subtract\': result = operands[0] for op in operands[1:]: result -= op return result elif operation == \'multiply\': result = 1 for op in operands: result *= op return result elif operation == \'divide\': result = operands[0] try: for op in operands[1:]: result /= op except ZeroDivisionError: return \\"Error: Division by zero\\" return result else: return \\"Unsupported operation\\" def main(): parser = argparse.ArgumentParser(description=\\"Perform basic mathematical operations.\\") parser.add_argument(\\"operation\\", type=str, choices=[\'add\', \'subtract\', \'multiply\', \'divide\'], help=\\"The operation to perform (add, subtract, multiply, divide).\\") parser.add_argument(\\"operands\\", type=int, nargs=\'+\', help=\\"Two or more integers to perform the operation on.\\") group = parser.add_mutually_exclusive_group() group.add_argument(\\"-v\\", \\"--verbose\\", action=\\"count\\", default=0, help=\\"Increase output verbosity.\\") group.add_argument(\\"-q\\", \\"--quiet\\", action=\\"store_true\\", help=\\"Suppress all output except errors.\\") parser.add_argument(\\"-o\\", \\"--output\\", type=str, help=\\"Output file to save the result.\\") args = parser.parse_args() result = calculate(args.operation, args.operands) if not args.quiet: if args.verbose >= 2: print(f\\"Performing \'{args.operation}\' on {args.operands}\\") if args.verbose >= 1: print(f\\"Result: {result}\\") if args.output: with open(args.output, \\"w\\") as f: f.write(f\\"Result: {result}\\") print(f\\"Result saved to {args.output}\\") elif args.verbose == 0: print(result) if __name__ == \\"__main__\\": main()"},{"question":"# Multiclass and Multilabel Classification with Custom Estimator Integration In this assessment, you are required to implement and compare multiclass and multilabel classification using various meta-estimators from the `sklearn.multiclass` and `sklearn.multioutput` modules. Objective Write a Python function `perform_multiclass_multilabel_classification` that: 1. Accepts parameters for loading a dataset, initializing a base classifier, and choosing meta-estimators. 2. Performs both **multiclass** and **multilabel** classification on the given dataset. 3. Returns the classification reports and accuracy scores for each strategy used. Parameters 1. `dataset` (str): The name of the dataset to load from `sklearn.datasets`. The valid values are `\'iris\'`, `\'wine\'`, `\'digits\'`, and `\'make_multilabel_classification\'`. 2. `classifier` (str): The name of the classifier to use as the base estimator for the meta-estimators. The valid values are `\'SVC\'`, `\'KNeighborsClassifier\'`, and `\'GaussianNB\'`. 3. `strategy` (list of str): A list of strategies to use for classification. Valid values are `\'one_vs_rest\'`, `\'one_vs_one\'`, `\'output_code\'` for multiclass classification, and `\'multi_output_classifier\'` or `\'classifier_chain\'` for multilabel classification. Function Signature ```python def perform_multiclass_multilabel_classification(dataset: str, classifier: str, strategy: list) -> dict: pass ``` Constraints 1. For `dataset`: - `\'iris\'`, `\'wine\'`, `\'digits\'` should be used for multiclass classification. - `\'make_multilabel_classification\'` should be used for multilabel classification. 2. For `classifier`: - Must be one of `\'SVC\'`, `\'KNeighborsClassifier\'`, or `\'GaussianNB\'`. 3. For `strategy`: - Must be a combination of strategies for multiclass and multilabel classification mentioned above. Output The function should return a dictionary with keys as strategy names, and values as dictionaries containing: - `\'classification_report\'`: The classification report for the classification task. - `\'accuracy_score\'`: The accuracy score for the classification task. # Example Usage ```python result = perform_multiclass_multilabel_classification( dataset=\'iris\', classifier=\'SVC\', strategy=[\'one_vs_rest\', \'one_vs_one\'] ) print(result[\'one_vs_rest\'][\'classification_report\']) print(result[\'one_vs_rest\'][\'accuracy_score\']) ``` # Notes 1. You should implement proper exception handling for invalid input values. 2. Use `train_test_split` to split the data into training and testing datasets. 3. Utilize `classification_report` and `accuracy_score` from `sklearn.metrics` for evaluation. # Evaluation Your implementation will be evaluated on: - Correctness of the solution - Proper use of scikit-learn meta-estimators and classifiers - Handling different datasets and strategies correctly - Clarity and conciseness of the code","solution":"from sklearn.datasets import load_iris, load_wine, load_digits, make_multilabel_classification from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier, OutputCodeClassifier from sklearn.multioutput import MultiOutputClassifier, ClassifierChain from sklearn.metrics import classification_report, accuracy_score import numpy as np def perform_multiclass_multilabel_classification(dataset: str, classifier: str, strategy: list) -> dict: # Load the dataset if dataset == \'iris\': data = load_iris() elif dataset == \'wine\': data = load_wine() elif dataset == \'digits\': data = load_digits() elif dataset == \'make_multilabel_classification\': data, target = make_multilabel_classification(n_samples=150, n_features=20, n_classes=3, n_labels=2, random_state=0) else: raise ValueError(\\"Invalid dataset name.\\") if dataset != \'make_multilabel_classification\': X, y = data.data, data.target else: X, y = data, target # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Choose the base classifier if classifier == \'SVC\': base_classifier = SVC() elif classifier == \'KNeighborsClassifier\': base_classifier = KNeighborsClassifier() elif classifier == \'GaussianNB\': base_classifier = GaussianNB() else: raise ValueError(\\"Invalid classifier name.\\") results = {} for strat in strategy: if strat == \\"one_vs_rest\\": model = OneVsRestClassifier(base_classifier) elif strat == \\"one_vs_one\\": model = OneVsOneClassifier(base_classifier) elif strat == \\"output_code\\": model = OutputCodeClassifier(base_classifier) elif strat == \\"multi_output_classifier\\": model = MultiOutputClassifier(base_classifier) elif strat == \\"classifier_chain\\": model = ClassifierChain(base_classifier) else: raise ValueError(\\"Invalid strategy name.\\") model.fit(X_train, y_train) y_pred = model.predict(X_test) results[strat] = { \'classification_report\': classification_report(y_test, y_pred, output_dict=True), \'accuracy_score\': accuracy_score(y_test, y_pred) } return results"},{"question":"# Advanced Context Management in Python You are tasked with implementing a custom context manager using the `contextlib` utilities. This context manager should provide the ability to: - Log messages indicating the entry and exit of a block of code. - Handle and log exceptions if they occur within the `with` block. - Ensure all resources are properly released, even if an exception occurs. To accomplish this, you will use the `@contextlib.contextmanager` decorator to transform a generator function into a context manager. Instructions: 1. **Context Manager Definition**: - Define a generator function `log_context` that logs messages when a context is entered and exited. - Log any exceptions that occur within the `with` block. - Ensure that resources can be cleaned up properly. 2. **Function Decorator**: - This context manager should also function as a decorator, allowing it to be used to wrap functions directly. 3. **Testing**: - Implement a test function `test_log_context` which uses the above-defined context manager in both with statements and decorators. Expected Function Definitions: ```python import contextlib from typing import Generator @contextlib.contextmanager def log_context(log_entry: str) -> Generator[None, None, None]: A context manager that logs entry and exit messages and handles exceptions. Args: log_entry (str): Entry message to be logged. Yields: None: This context manager does not manage any specific resource. try: print(f\\"Entering: {log_entry}\\") yield except Exception as e: print(f\\"Exception occurred: {e}\\") raise finally: print(f\\"Exiting: {log_entry}\\") def test_log_context(): Test the log_context context manager using both with statements and as a decorator. # Using log_context with a `with` statement with log_context(\\"Test Block\\"): print(\\"Inside the block.\\") # Uncomment the line below to test exception handling # raise ValueError(\\"Test Exception\\") # Using log_context as a decorator @log_context(\\"Test Function\\") def sample_function(): print(\\"Inside the function.\\") # Uncomment the line below to test exception handling # raise ValueError(\\"Test Exception\\") sample_function() # Sample tests to validate the implementation test_log_context() ``` Constraints: - You may not use any global variables. - Ensure exception logging behavior does not suppress the exceptions completely. **Note**: The `print` statements serve as a placeholder for actual logging functionality. In a real-world scenario, these would be replaced with appropriate logging calls.","solution":"import contextlib @contextlib.contextmanager def log_context(log_entry: str): A context manager that logs entry and exit messages and handles exceptions. Args: log_entry (str): Entry message to be logged. Yields: None: This context manager does not manage any specific resource. try: print(f\\"Entering: {log_entry}\\") yield except Exception as e: print(f\\"Exception occurred: {e}\\") raise finally: print(f\\"Exiting: {log_entry}\\")"},{"question":"You are tasked with creating a custom display class in Scikit-learn\'s plotting API to visualize the Precision-Recall Curve. # Question Implement a `PrecisionRecallDisplay` class to plot the Precision-Recall curve. Your class should have the following methods and attributes: 1. **`__init__(self, precision, recall, average_precision, estimator_name)`** - `precision`: Array of precision values. - `recall`: Array of recall values. - `average_precision`: The average precision score. - `estimator_name`: The name of the estimator used. 2. **`from_estimator(cls, estimator, X, y)`** (Class Method) - `estimator`: The fitted estimator. - `X`: Input features. - `y`: True labels. - This method should use the estimator to predict probabilities, calculate precision-recall values, and return an instance of `PrecisionRecallDisplay`. 3. **`from_predictions(cls, y_true, y_pred, estimator_name)`** (Class Method) - `y_true`: True labels. - `y_pred`: Predicted probabilities or scores. - `estimator_name`: The name of the estimator used. - This method should calculate precision-recall values and return an instance of `PrecisionRecallDisplay`. 4. **`plot(self, ax=None, **kwargs)`** - `ax`: Matplotlib Axes object. If `None`, a new figure and axes are created. - This method should plot the Precision-Recall curve using `matplotlib`, customizing styles as needed. Expected Input and Output **Input Format:** - `__init__`: Four parameters to initialize the instance. - `from_estimator`: Three parameters to calculate predictions and precision-recall values. - `from_predictions`: Three parameters to calculate precision-recall values. - `plot`: One optional parameter. **Output Format:** - `from_estimator`: Returns an instance of `PrecisionRecallDisplay`. - `from_predictions`: Returns an instance of `PrecisionRecallDisplay`. - `plot`: Returns the matplotlib Axes object containing the plot. Constraints: - Use Scikit-learn for model prediction and precision-recall calculations. - Use Matplotlib for plotting. - Ensure that the plot method allows style adjustments. # Example Usage ```python from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.metrics import PrecisionRecallDisplay # Generate synthetic data X, y = make_classification(n_samples=1000, n_features=20, random_state=42) # Train a logistic regression classifier model = LogisticRegression() model.fit(X, y) # Create Precision-Recall display from estimator display = PrecisionRecallDisplay.from_estimator(model, X, y) display.plot() # Create Precision-Recall display from predictions y_pred = model.predict_proba(X)[:, 1] display = PrecisionRecallDisplay.from_predictions(y, y_pred, model.__class__.__name__) display.plot() ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve, average_precision_score class PrecisionRecallDisplay: def __init__(self, precision, recall, average_precision, estimator_name): self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): y_pred_proba = estimator.predict_proba(X)[:, 1] return cls.from_predictions(y, y_pred_proba, estimator.__class__.__name__) @classmethod def from_predictions(cls, y_true, y_pred, estimator_name): precision, recall, _ = precision_recall_curve(y_true, y_pred) avg_precision = average_precision_score(y_true, y_pred) return cls(precision, recall, avg_precision, estimator_name) def plot(self, ax=None, **kwargs): if ax is None: fig, ax = plt.subplots() ax.plot(self.recall, self.precision, label=f\'{self.estimator_name} (AP={self.average_precision:.2f})\', **kwargs) ax.set_xlabel(\'Recall\') ax.set_ylabel(\'Precision\') ax.legend(loc=\'best\') ax.set_title(\'Precision-Recall Curve\') return ax"},{"question":"**Question:** You are given a dataset containing weather data for multiple cities. Each city records the weather conditions (e.g., temperature, precipitation) at hourly intervals over several months. Your task is to implement a function called `process_weather_data` that processes this dataset to extract meaningful insights. **Requirements:** 1. **Load and Clean Data:** - Read the CSV file into a pandas DataFrame. The CSV file may contain missing values. - Replace missing temperature values with the mean temperature for that city. 2. **Conditional Assignment:** - Create a new column `Temperature_Category` that categorizes the temperature into \\"Cold\\", \\"Warm\\", and \\"Hot\\". - \\"Cold\\" if the temperature is less than 5 degrees Celsius. - \\"Warm\\" if the temperature is between 5 and 25 degrees Celsius. - \\"Hot\\" if the temperature is above 25 degrees Celsius. 3. **Aggregation:** - Compute the average temperature and total precipitation for each city for each month. 4. **Pivot Table:** - Create a pivot table that shows the average monthly temperature for each city, with cities as columns and months as rows. 5. **Function Signature:** ```python def process_weather_data(file_path: str) -> pd.DataFrame: Process the weather data as per the requirements. Arguments: file_path : str : The path to the CSV file containing the weather data. Returns: pd.DataFrame : A pivot table with the average monthly temperature for each city. pass ``` **Input:** The CSV file contains the following columns: - `City`: The name of the city. - `Date`: Timestamp of the record (e.g., \\"2023-01-01 00:00:00\\"). - `Temperature`: Recorded temperature in degrees Celsius (could have missing values). - `Precipitation`: Recorded precipitation in millimeters. **Constraints:** - You can assume that the file path and format are correct. - You should handle missing values appropriately. - Performance should be optimized for large datasets. **Example:** Suppose the CSV file contains the following data: ``` City,Date,Temperature,Precipitation New York,2023-01-01 00:00:00,2,0 New York,2023-01-01 01:00:00,,0.1 Los Angeles,2023-01-01 00:00:00,20,0 Los Angeles,2023-01-01 01:00:00,25,0 Chicago,2023-01-01 00:00:00,-5,0 Chicago,2023-01-01 01:00:00,-10,0.2 ``` After processing, the resulting pivot table would look something like: ``` Month | New York | Los Angeles | Chicago ----------------------------------------- Jan | (value) | (value) | (value) Feb | (value) | (value) | (value) ... | ... | ... | ... ``` Your implementation should follow best practices for readability and efficiency.","solution":"import pandas as pd def process_weather_data(file_path: str) -> pd.DataFrame: Process the weather data as per the requirements. Arguments: file_path : str : The path to the CSV file containing the weather data. Returns: pd.DataFrame : A pivot table with the average monthly temperature for each city. # Step 1: Load data df = pd.read_csv(file_path) # Step 2: Handle missing temperature values by replacing with mean temperature of the city df[\'Temperature\'] = df.groupby(\'City\')[\'Temperature\'].transform(lambda x: x.fillna(x.mean())) # Step 3: Assign Temperature Category df[\'Temperature_Category\'] = pd.cut(df[\'Temperature\'], bins=[-float(\'inf\'), 5, 25, float(\'inf\')], labels=[\'Cold\', \'Warm\', \'Hot\']) # Step 4: Create a \'Month\' column df[\'Month\'] = pd.to_datetime(df[\'Date\']).dt.to_period(\'M\') # Step 5: Aggregate to compute average temperature and total precipitation for each city for each month aggregation = df.groupby([\'City\', \'Month\']).agg({\'Temperature\':\'mean\', \'Precipitation\':\'sum\'}).reset_index() # Step 6: Create the pivot table for average monthly temperature pivot_table = aggregation.pivot(index=\'Month\', columns=\'City\', values=\'Temperature\') return pivot_table"},{"question":"# Custom Context Manager for Mixed I/O Streams You are required to implement a custom context manager class `MixedIOContextManager` that manages both text and binary in-memory streams using the `io.StringIO` and `io.BytesIO` classes. Your context manager should handle reading from and writing to these streams efficiently, utilizing the concepts of buffered I/O. Requirements: 1. **Initialization**: - The context manager should accept two optional arguments during initialization: - `initial_text`: Initial data for the text stream. - `initial_binary`: Initial data for the binary stream (must be bytes). 2. **Methods**: - `write_text(self, data)`: Write a string `data` to the text stream. - `write_binary(self, data)`: Write bytes `data` to the binary stream. - `read_text(self)`: Read and return the entire contents of the text stream as a string. - `read_binary(self)`: Read and return the entire contents of the binary stream as bytes. - `flush(self)`: Ensure any buffered data is written out to the respective streams. 3. **Context Management**: - Implement the necessary methods to use the class with a `with` statement, ensuring proper opening and closing of both streams. Input/Output: - Initialization: - `initial_text` (optional): `str` - `initial_binary` (optional): `bytes` - Methods: - `write_text(data)`: `data` is a `str`. - `write_binary(data)`: `data` is `bytes`. - `read_text()`: Returns a `str`. - `read_binary()`: Returns `bytes`. Example Usage: ```python with MixedIOContextManager(initial_text=\\"Hello, \\", initial_binary=b\\"x00x01\\") as manager: manager.write_text(\\"World!\\") manager.write_binary(b\\"x02x03\\") all_text = manager.read_text() all_binary = manager.read_binary() print(all_text) # Output: \\"Hello, World!\\" print(all_binary) # Output: b\\"x00x01x02x03\\" ``` # Constraints: - Ensure efficient handling and flushing of buffered data. - Appropriate exception handling for unsupported or invalid operations. # Performance: - Operations should be efficient and should not result in large memory overheads. - Test your implementation with varying sizes of text and binary data. **Note**: You should not import any additional modules apart from `io`. Implement the `MixedIOContextManager` class below: ```python import io class MixedIOContextManager: def __init__(self, initial_text=\\"\\", initial_binary=b\\"\\"): # Your code here def __enter__(self): # Your code here def __exit__(self, exc_type, exc_value, traceback): # Your code here def write_text(self, data): # Your code here def write_binary(self, data): # Your code here def read_text(self): # Your code here def read_binary(self): # Your code here def flush(self): # Your code here # You can add any helper functions if necessary. ```","solution":"import io class MixedIOContextManager: def __init__(self, initial_text=\\"\\", initial_binary=b\\"\\"): self.text_stream = io.StringIO(initial_text) self.binary_stream = io.BytesIO(initial_binary) def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self.text_stream.close() self.binary_stream.close() def write_text(self, data): self.text_stream.write(data) def write_binary(self, data): self.binary_stream.write(data) def read_text(self): self.text_stream.seek(0) return self.text_stream.read() def read_binary(self): self.binary_stream.seek(0) return self.binary_stream.read() def flush(self): self.text_stream.flush() self.binary_stream.flush()"},{"question":"You are tasked with developing a script to manage backup operations for a given directory. The script should accomplish the following: 1. **Copy Selected Files**: Copy all `.txt` files from a source directory to a destination directory. The destination directory should be created if it does not exist. 2. **Archive Backup**: Create a `.zip` archive of the copied `.txt` files. 3. **Clean Up**: After creating the archive, remove the copied `.txt` files but keep the directory structure intact. 4. **Report Disk Usage**: Report the total, used, and free space on the disk where the destination directory resides. # Function Signature ```python def backup_and_cleanup(source_dir: str, dest_dir: str, archive_name: str) -> None: pass ``` # Input - `source_dir` (str): The path to the source directory containing the files to be backed up. - `dest_dir` (str): The path to the destination directory where the `.txt` files should be copied. - `archive_name` (str): The name of the output archive file (without extension). # Steps 1. **Copy Selected Files**: - Use `shutil.copytree` with `ignore_patterns()` to copy only `.txt` files from `source_dir` to `dest_dir`. 2. **Archive Backup**: - Use `shutil.make_archive` to create a `.zip` archive of the `dest_dir`. 3. **Clean Up**: - Use `shutil.rmtree` or other methods to delete all `.txt` files in the `dest_dir`. 4. **Report Disk Usage**: - Use `shutil.disk_usage` to report disk usage where `dest_dir` is located. - Print the total, used, and free space in a human-readable format (e.g., GB). # Output - The function should print the disk usage statistics. # Constraints - The source directory will always exist and be readable. - The destination directory may or may not exist initially. - Handle any exceptions that might occur during file operations, without stopping the script execution. # Example ```python source_dir = \\"/path/to/source\\" dest_dir = \\"/path/to/destination\\" archive_name = \\"backup\\" backup_and_cleanup(source_dir, dest_dir, archive_name) ``` **Output:** ``` Disk Usage for /path/to/destination: Total: X.XX GB Used: Y.YY GB Free: Z.ZZ GB ``` # Notes - The implemented function should be efficient and should not load large files entirely into memory. - Ensure the script is robust and handles cases where the destination directory might already contain some files or directories.","solution":"import os import shutil from fnmatch import fnmatch from pathlib import Path def backup_and_cleanup(source_dir: str, dest_dir: str, archive_name: str) -> None: # Ensure source_dir exists if not os.path.exists(source_dir): print(f\\"Source directory {source_dir} does not exist.\\") return # Create destination directory if it doesn\'t exist if not os.path.exists(dest_dir): os.makedirs(dest_dir) # Copy only .txt files for root, _, files in os.walk(source_dir): relative_path = os.path.relpath(root, source_dir) target_dir = os.path.join(dest_dir, relative_path) if not os.path.exists(target_dir): os.makedirs(target_dir) for file in files: if fnmatch(file, \\"*.txt\\"): shutil.copy2(os.path.join(root, file), os.path.join(target_dir, file)) # Archive the copied .txt files shutil.make_archive(archive_name, \'zip\', dest_dir) # Clean up the copied .txt files but keep directory structure for root, _, files in os.walk(dest_dir): for file in files: if fnmatch(file, \\"*.txt\\"): os.remove(os.path.join(root, file)) # Report disk usage total, used, free = shutil.disk_usage(dest_dir) print(f\\"Disk Usage for {dest_dir}:\\") print(f\\"Total: {total / (1024 ** 3):.2f} GB\\") print(f\\"Used: {used / (1024 ** 3):.2f} GB\\") print(f\\"Free: {free / (1024 ** 3):.2f} GB\\")"},{"question":"**Question: Implementing and Comparing SVM Classifiers for Multiclass Classification** **Objective:** Demonstrate understanding of scikit-learn\'s SVM classifiers by implementing and comparing different SVM strategies for a multiclass classification problem. **Task:** 1. Load a dataset suitable for multiclass classification (e.g., the Iris dataset). 2. Implement SVM using both `SVC` with the one-vs-one strategy and `LinearSVC` with the one-vs-the-rest strategy. 3. Train the models and evaluate their performance using accuracy, precision, recall, and F1-score. 4. Visualize the decision boundaries of the classifiers. **Dataset:** Use the Iris dataset available in scikit-learn datasets. **Requirements:** 1. **Data Loading and Preparation:** - Load the Iris dataset. - Split the dataset into a training set and a test set in an 80-20 ratio. 2. **Model Implementation:** - Implement an SVM classifier using `SVC` with the one-vs-one strategy. - Implement an SVM classifier using `LinearSVC` with the one-vs-the-rest strategy. 3. **Model Training and Evaluation:** - Train both models using the training set. - Predict the class labels of the test set. - Evaluate the performance using accuracy, precision, recall, and F1-score. 4. **Visualization:** - Create a plot showing the decision boundaries of both SVM classifiers. - Ensure proper visualization of the classification regions. **Constraints:** - Ensure you use `decision_function_shape=\'ovo\'` for `SVC`. - Use proper scaling methods as SVMs are not scale-invariant. - Handle any ties in predictions for consistent results. **Expected Input:** - The model implementation functions should accept the training and testing data arrays, and other necessary parameters. **Expected Output:** - A printed summary of evaluation metrics for each model. - Visualized decision boundaries of both models. **Sample Code Structure:** ```python import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC, LinearSVC from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Step 1: Load and prepare the dataset iris = datasets.load_iris() X = iris.data y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 2: Implement SVC with one-vs-one strategy svc_clf = SVC(decision_function_shape=\'ovo\') svc_clf.fit(X_train, y_train) y_pred_svc = svc_clf.predict(X_test) # Step 3: Implement LinearSVC with one-vs-the-rest strategy linear_svc_clf = LinearSVC() linear_svc_clf.fit(X_train, y_train) y_pred_linear_svc = linear_svc_clf.predict(X_test) # Step 4: Evaluate models def evaluate_model(y_true, y_pred): accuracy = accuracy_score(y_true, y_pred) precision = precision_score(y_true, y_pred, average=\'macro\') recall = recall_score(y_true, y_pred, average=\'macro\') f1 = f1_score(y_true, y_pred, average=\'macro\') return accuracy, precision, recall, f1 svc_metrics = evaluate_model(y_test, y_pred_svc) linear_svc_metrics = evaluate_model(y_test, y_pred_linear_svc) print(f\\"SVC (ovo) Metrics: Accuracy={svc_metrics[0]}, Precision={svc_metrics[1]}, Recall={svc_metrics[2]}, F1-Score={svc_metrics[3]}\\") print(f\\"LinearSVC (ovr) Metrics: Accuracy={linear_svc_metrics[0]}, Precision={linear_svc_metrics[1]}, Recall={linear_svc_metrics[2]}, F1-Score={linear_svc_metrics[3]}\\") # Step 5: Visualize decision boundaries # (Include necessary code for visualization) ``` **Note:** Ensure you provide detailed comments and explanations within your code to demonstrate your understanding of the concepts and functions used.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC, LinearSVC from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_and_prepare_data(): iris = datasets.load_iris() X = iris.data y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def evaluate_model(y_true, y_pred): accuracy = accuracy_score(y_true, y_pred) precision = precision_score(y_true, y_pred, average=\'macro\') recall = recall_score(y_true, y_pred, average=\'macro\') f1 = f1_score(y_true, y_pred, average=\'macro\') return accuracy, precision, recall, f1 def train_and_evaluate_svc(X_train, X_test, y_train, y_test): svc_clf = SVC(decision_function_shape=\'ovo\') svc_clf.fit(X_train, y_train) y_pred = svc_clf.predict(X_test) metrics = evaluate_model(y_test, y_pred) return metrics, svc_clf def train_and_evaluate_linear_svc(X_train, X_test, y_train, y_test): linear_svc_clf = LinearSVC() linear_svc_clf.fit(X_train, y_train) y_pred = linear_svc_clf.predict(X_test) metrics = evaluate_model(y_test, y_pred) return metrics, linear_svc_clf def plot_decision_boundaries(X, y, clf, title): plt.figure() x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) scatter = plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\'k\', marker=\'o\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.title(title) plt.legend(*scatter.legend_elements(), title=\\"Classes\\") plt.show() def main(): X_train, X_test, y_train, y_test = load_and_prepare_data() svc_metrics, svc_clf = train_and_evaluate_svc(X_train, X_test, y_train, y_test) linear_svc_metrics, linear_svc_clf = train_and_evaluate_linear_svc(X_train, X_test, y_train, y_test) print(f\\"SVC (ovo) Metrics: Accuracy={svc_metrics[0]}, Precision={svc_metrics[1]}, Recall={svc_metrics[2]}, F1-Score={svc_metrics[3]}\\") print(f\\"LinearSVC (ovr) Metrics: Accuracy={linear_svc_metrics[0]}, Precision={linear_svc_metrics[1]}, Recall={linear_svc_metrics[2]}, F1-Score={linear_svc_metrics[3]}\\") # For visualization, we reduce the dataset to the first two features for 2D plotting X_train_2d = X_train[:, :2] X_test_2d = X_test[:, :2] # This refit is necessary for visualization on two features svc_clf.fit(X_train_2d, y_train) linear_svc_clf.fit(X_train_2d, y_train) plot_decision_boundaries(X_train_2d, y_train, svc_clf, \\"SVC (one-vs-one)\\") plot_decision_boundaries(X_train_2d, y_train, linear_svc_clf, \\"LinearSVC (one-vs-the-rest)\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s advanced plotting capabilities by creating a layered plot with jittered points, customized percentile ranges, and shifted elements. **Question:** Using the `penguins` dataset from `seaborn`, write a function `create_custom_plot()` that generates a plot with the following specifications: 1. **Plot Type**: It should be a scatter plot using `species` (x-axis) and `flipper_length_mm` (y-axis). 2. **Jitter**: Add jitter to prevent overlapping of points. 3. **Percentile Range**: Overlay a percentile band representing the 10th and 90th percentiles of `flipper_length_mm`. 4. **Shift**: Apply a horizontal shift to the percentile range for better visualization. # Function Signature: ```python def create_custom_plot() -> None: pass ``` # Expected Output: The function should display the plot as described above. The plot should be clear and well-labeled, providing a meaningful visualization of the `penguins` dataset. **Constraints:** - Use only `seaborn.objects` for plot creation. - Ensure proper axis labels and titles for the plot. **Example of Plot Elements:** Use the following example code snippets for reference, but adapt them according to the specified requirements: ```python ( so.Plot(penguins, \\"species\\", \\"flipper_length_mm\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([10, 90]), so.Shift(x=.2)) ) ``` **Hint:** Refer to the `seaborn.objects` documentation for additional methods and transformations that can be applied to the plot.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_custom_plot() -> None: Creates a scatter plot from the penguins dataset with jitter, overlays a percentile band representing the 10th and 90th percentiles of flipper_length_mm, and applies a horizontal shift to the percentile range for better visualization. # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot p = ( so.Plot(penguins, x=\\"species\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(width=0.2)) .add(so.Range(), so.Perc([10, 90]), so.Shift(x=0.2)) .label(title=\\"Penguin Species vs Flipper Length\\", x=\\"Species\\", y=\\"Flipper Length (mm)\\") ) # Show the plot p.show()"},{"question":"**Problem Statement**: Create a `ByteArrayManipulator` class in Python that demonstrates the understanding of bytearray operations. This class will implement various methods to perform operations on bytearrays as described below. # Class Definition `ByteArrayManipulator` - **Attributes**: - `byte_array`: A bytearray object that the class will manipulate. - **Methods**: 1. **`__init__(self, initial_data)`**: - Initializes the `byte_array` attribute with the provided initial_data. - `initial_data` can be either a string or a bytearray. 2. **`append_data(self, data)`**: - Appends the given data to the `byte_array`. - `data` can be either a string or bytearray. 3. **`get_size(self)`**: - Returns the current size of the `byte_array`. 4. **`to_string(self)`**: - Returns the contents of `byte_array` as a string. - The string should be properly decoded assuming \'utf-8\' encoding. 5. **`resize(self, new_size)`**: - Resizes the `byte_array` to the new size specified. - If the new size is smaller, the `byte_array` should be truncated. - If the new size is larger, the `byte_array` should be padded with zero bytes (b\'x00\'). # Method Signature ```python class ByteArrayManipulator: def __init__(self, initial_data): pass def append_data(self, data): pass def get_size(self): pass def to_string(self): pass def resize(self, new_size): pass ``` # Constraints - The `initial_data` argument in the constructor will always be provided as a non-empty `string` or `bytearray`. - The `data` argument in the `append_data` method will always be provided as a `string` or `bytearray`. # Example Usage ```python # Example bam = ByteArrayManipulator(\\"hello\\") bam.append_data(\\" world\\") print(bam.get_size()) # Output: 11 print(bam.to_string()) # Output: \\"hello world\\" bam.resize(5) print(bam.to_string()) # Output: \\"hello\\" bam.resize(10) print(bam.to_string()) # Output: \\"hellox00x00x00x00x00\\" ```","solution":"class ByteArrayManipulator: def __init__(self, initial_data): if isinstance(initial_data, str): self.byte_array = bytearray(initial_data, \'utf-8\') elif isinstance(initial_data, bytearray): self.byte_array = initial_data else: raise ValueError(\\"Initial data must be a string or a bytearray\\") def append_data(self, data): if isinstance(data, str): self.byte_array.extend(data.encode(\'utf-8\')) elif isinstance(data, bytearray): self.byte_array.extend(data) else: raise ValueError(\\"Data must be a string or a bytearray\\") def get_size(self): return len(self.byte_array) def to_string(self): return self.byte_array.decode(\'utf-8\') def resize(self, new_size): current_size = len(self.byte_array) if new_size < current_size: self.byte_array = self.byte_array[:new_size] elif new_size > current_size: self.byte_array.extend(b\'x00\' * (new_size - current_size))"},{"question":"**Objective**: Implement a function that analyzes a given path (file or directory) and returns its detailed information in a human-readable format using the constants and functions provided by the `stat` module. Problem Statement Write a function called `analyze_path(path: str) -> dict:` that takes a path to a file or directory as input and returns a dictionary containing detailed information about the file or directory. The details should include: - `type`: The type of the file (e.g., \'directory\', \'regular file\', \'character device\', \'block device\', \'FIFO\', \'symbolic link\', \'socket\', \'door\', \'event port\', \'whiteout\'). - `permissions`: A human-readable string representing the file\'s permissions (e.g., \'-rwxr-xr-x\'). - `size`: The size of the file in bytes. - `last_accessed`: The time of the last access. - `last_modified`: The time of the last modification. - `last_changed`: The time of the last metadata change (ctime). Function Signature ```python def analyze_path(path: str) -> dict: pass ``` Input - A single string, `path`, representing the file or directory path. Output - A dictionary with the following keys: - `type` (str): Type of the file. - `permissions` (str): Permissions of the file in a human-readable format. - `size` (int): Size of the file in bytes. - `last_accessed` (float): Time of the last access (seconds since the epoch). - `last_modified` (float): Time of the last modification (seconds since the epoch). - `last_changed` (float): Time of the last metadata change (seconds since the epoch). Constraints - The function should handle common file types and permissions. - The function should be efficient and avoid redundant system calls. Example ```python import os import stat from typing import Dict def analyze_path(path: str) -> Dict[str, str, int, float, float, float]: try: file_stat = os.lstat(path) mode = file_stat.st_mode # Determining the file type if stat.S_ISDIR(mode): file_type = \\"directory\\" elif stat.S_ISREG(mode): file_type = \\"regular file\\" elif stat.S_ISCHR(mode): file_type = \\"character device\\" elif stat.S_ISBLK(mode): file_type = \\"block device\\" elif stat.S_ISFIFO(mode): file_type = \\"FIFO\\" elif stat.S_ISLNK(mode): file_type = \\"symbolic link\\" elif stat.S_ISSOCK(mode): file_type = \\"socket\\" else: file_type = \\"other\\" # Getting the file permissions in human-readable form file_permissions = stat.filemode(mode) # Creating the result dictionary result = { \'type\': file_type, \'permissions\': file_permissions, \'size\': file_stat.st_size, \'last_accessed\': file_stat.st_atime, \'last_modified\': file_stat.st_mtime, \'last_changed\': file_stat.st_ctime } return result except Exception as e: return {\\"error\\": str(e)} # Example usage: # Assuming there is a file or directory at \\"/path/to/file\\" result = analyze_path(\\"/path/to/file\\") print(result) ``` Note: - You should use the `os.lstat()` function to retrieve metadata about the file or directory and then use relevant functions and constants from the `stat` module to interpret this metadata. - Handle exceptions gracefully and provide meaningful error messages if the path does not exist or is inaccessible.","solution":"import os import stat from typing import Dict, Union def analyze_path(path: str) -> Dict[str, Union[str, int, float]]: Analyzes a given path and returns its detailed information in a human-readable format. Arguments: path: str -- A path to a file or directory. Returns: Dict[str, Union[str, int, float]] -- A dictionary containing detailed information about the file or directory. try: file_stat = os.lstat(path) mode = file_stat.st_mode # Determining the file type if stat.S_ISDIR(mode): file_type = \\"directory\\" elif stat.S_ISREG(mode): file_type = \\"regular file\\" elif stat.S_ISCHR(mode): file_type = \\"character device\\" elif stat.S_ISBLK(mode): file_type = \\"block device\\" elif stat.S_ISFIFO(mode): file_type = \\"FIFO\\" elif stat.S_ISLNK(mode): file_type = \\"symbolic link\\" elif stat.S_ISSOCK(mode): file_type = \\"socket\\" else: file_type = \\"unknown\\" # Getting the file permissions in human-readable form file_permissions = stat.filemode(mode) # Creating the result dictionary result = { \'type\': file_type, \'permissions\': file_permissions, \'size\': file_stat.st_size, \'last_accessed\': file_stat.st_atime, \'last_modified\': file_stat.st_mtime, \'last_changed\': file_stat.st_ctime } return result except Exception as e: return {\\"error\\": str(e)}"},{"question":"# Advanced Python Coding Assessment Objective To test your understanding of Python environment configuration, specifically the interaction with site-specific paths and custom module imports. Problem Statement You are required to implement a function `configure_python_environment(base_dir: str, config_files: List[str], enable_user_site: bool) -> Tuple[List[str], List[str]]` that configures a custom Python environment based on given inputs and simulates the behavior of the Python `site` module. Function Signature ```python from typing import List, Tuple def configure_python_environment(base_dir: str, config_files: List[str], enable_user_site: bool) -> Tuple[List[str], List[str]]: Configures the Python environment by simulating the behavior of the `site` module. Args: - base_dir (str): The base directory for site-related paths. - config_files (List[str]): A list of configuration files that need to be processed. - enable_user_site (bool): A flag indicating whether to enable user-specific site packages. Returns: - Tuple[List[str], List[str]]: A tuple containing two lists: * The modified sys.path. * Executed lines from the configuration files. ``` Requirements 1. **Path Configuration**: - Implement the logic to append appropriate directories (`base_dir/lib/site-packages` and any other directories derived from the `base_dir`) to `sys.path`. 2. **Processing Configuration Files (`.pth` Files)**: - Process each configuration file in `config_files`, read its content, and perform the necessary additions to `sys.path`. - Execute lines that start with `import` (followed by space or tab), and collect these lines separately. 3. **User-Specific Site Packages**: - Respect the `enable_user_site` flag to include or exclude user-specific site packages from the modified `sys.path`. - Assume user site package paths follow the structure `<base_dir>/.local/lib/pythonX.Y/site-packages`. Constraints - Paths that do not exist should not be added to `sys.path`. - Ensure no path is added to `sys.path` more than once. - Lines in `.pth` files that are blank, comments (starting with `#`), or invalid should be skipped. Input and Output - Input `base_dir`: A string representing the base path for deriving directories. - Input `config_files`: A list of strings, where each string is a file path to a configuration `.pth` file. - Input `enable_user_site`: A boolean determining whether to include user site-specific paths. - Output: Return a tuple with two lists: - The first list containing the paths in the modified `sys.path`. - The second list containing the executed `import` lines from the `.pth` files. Example ```python base_dir = \\"/usr/local\\" config_files = [ \\"path/to/foo.pth\\", # This file contains: # foo # import os # bar ] enable_user_site = True # Suppose the directory /usr/local/lib/site-packages contains subdirectory \\"foo\\" and \\"bar\\", # and a user-specific directory /usr/local/.local/lib/pythonX.Y/site-packages. configure_python_environment(base_dir, config_files, enable_user_site) ``` Expected Output: ```python ([\'/usr/local/lib/site-packages/foo\', \'/usr/local/lib/site-packages/bar\', \'/usr/local/.local/lib/pythonX.Y/site-packages\'], [\'import os\']) ``` Good Luck!","solution":"import os import sys from typing import List, Tuple def configure_python_environment(base_dir: str, config_files: List[str], enable_user_site: bool) -> Tuple[List[str], List[str]]: Configures the Python environment by simulating the behavior of the `site` module. Args: - base_dir (str): The base directory for site-related paths. - config_files (List[str]): A list of configuration files that need to be processed. - enable_user_site (bool): A flag indicating whether to enable user-specific site packages. Returns: - Tuple[List[str], List[str]]: A tuple containing two lists: * The modified sys.path. * Executed lines from the configuration files. modified_sys_path = [] executed_imports = [] # Add base directory path site_packages_path = os.path.join(base_dir, \'lib\', \'site-packages\') if os.path.exists(site_packages_path): modified_sys_path.append(site_packages_path) # Parse each configuration file for config_file in config_files: if not os.path.isfile(config_file): continue with open(config_file) as file: for line in file: line = line.strip() if line.startswith(\\"#\\") or not line: continue if line.startswith(\\"import \\"): executed_imports.append(line) else: full_path = os.path.join(site_packages_path, line) if os.path.exists(full_path): modified_sys_path.append(full_path) # Add user-specific site packages if enable_user_site: user_site_packages_path = os.path.join(base_dir, \'.local\', \'lib\', f\'python{sys.version_info.major}.{sys.version_info.minor}\', \'site-packages\') if os.path.exists(user_site_packages_path): modified_sys_path.append(user_site_packages_path) # Ensure no duplicates in sys.path modified_sys_path = list(dict.fromkeys(modified_sys_path)) return modified_sys_path, executed_imports"},{"question":"Objective Demonstrate your understanding of seaborn for creating and customizing visualizations, specifically focusing on loading datasets, creating histograms, customizing plot legends, and working with multi-facet plots. Problem Statement You are given a dataset containing information about the physical measurements of penguins from different islands. Your task is to create a series of visualizations to analyze the distribution of `bill_length_mm` based on the `species` and `island`. Additionally, you need to customize the legends of these plots. # Instructions 1. **Load Dataset** - Load the `penguins` dataset from seaborn\'s built-in datasets. 2. **Plot Histogram** - Create a histogram of `bill_length_mm` colored by `species`. - Move the legend to the center right position. 3. **Customize Legend** - Create another histogram of `bill_length_mm` colored by `species`. - Move the legend to the upper left position, outside of the plot, with a bbox anchor at (1, 1). 4. **Facet Plot** - Create a multi-facet plot of the distribution of `bill_length_mm` for each `island`, colored by `species`. - Ensure the legend is placed in the upper left position within the plot, using bbox anchor at (.55, .45), without the frame around the legend. # Expected Output Each step should produce a plot with the specified customizations. Ensure the legends are moved correctly according to the instructions. # Constraints - You must use seaborn for visualization and dataset operations. - Ensure your code is clean and well-commented. ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load Dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Plot Histogram with Center Right Legend ax1 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax1, \\"center right\\") plt.show() # Step 3: Plot Histogram with Upper Left Legend Outside ax2 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax2, \\"upper left\\", bbox_to_anchor=(1, 1)) plt.show() # Step 4: Multi-Facet Plot with Custom Legend Position g = sns.displot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3) sns.move_legend(g, \\"upper left\\", bbox_to_anchor=(.55, .45), frameon=False) plt.show() ``` # Evaluation Criteria - **Correctness:** The plots must be generated correctly with appropriate legends in specified positions. - **Code Quality:** The code should be clean, well-organized, and commented appropriately. - **Use of seaborn:** Efficient usage of seaborn functions for dataset loading, plotting, and customizing legends.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load Dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Plot Histogram with Center Right Legend def plot_histogram_center_right(): ax1 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") ax1.legend(loc=\'center right\') plt.show() # Step 3: Plot Histogram with Upper Left Legend Outside def plot_histogram_upper_left_outside(): ax2 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") ax2.legend(loc=\'upper left\', bbox_to_anchor=(1, 1)) plt.show() # Step 4: Multi-Facet Plot with Custom Legend Position def plot_facet_custom_legend(): g = sns.displot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3) plt.legend(loc=\'upper left\', bbox_to_anchor=(.55, .45), frameon=False) plt.show()"},{"question":"**Problem Statement: Advanced DataFrame Indexing and Selection** You are provided with a DataFrame containing employee data and their respective sales records. Your task is to implement functions using Pandas that perform various indexing and selection operations to retrieve specific data points. **DataFrame Description:** - The DataFrame `df` has the following columns: `employee_id`, `employee_name`, `department`, `sales`, and `date`. - The `date` column contains the dates of the sales records. - The `employee_id` column is used as the index for the DataFrame. **Input:** - `df`: Pandas DataFrame structured as described above. **Functions to Implement:** 1. **Function: `get_sales_by_employee_name`** - **Input:** `df` (DataFrame), `name` (string) - **Output:** Series containing sales records for the given `employee_name`. - **Constraints:** Use label-based indexing. 2. **Function: `get_sales_by_date_range`** - **Input:** `df` (DataFrame), `start_date` (string, format \'YYYY-MM-DD\'), `end_date` (string, format \'YYYY-MM-DD\') - **Output:** DataFrame containing records within the specified date range. - **Constraints:** Use label-based indexing with slices. 3. **Function: `get_top_n_employees_by_sales`** - **Input:** `df` (DataFrame), `n` (int) - **Output:** DataFrame containing top `n` employees based on their total sales. - **Constraints:** Use sorting and slicing techniques while keeping the original index intact. 4. **Function: `filter_department_sales`** - **Input:** `df` (DataFrame), `department` (string), `min_sales` (float) - **Output:** DataFrame containing employees in the specified `department` with sales greater than `min_sales`. - **Constraints:** Use boolean indexing. 5. **Function: `add_employee_bonus_column`** - **Input:** `df` (DataFrame), `bonus_threshold` (float), `bonus_amount` (float) - **Output:** DataFrame with an additional column `bonus` where the value is `bonus_amount` if the employee\'s total sales exceed `bonus_threshold`, otherwise 0. - **Constraints:** Efficiently add the new column without using a loop directly. **Function Signatures:** ```python import pandas as pd def get_sales_by_employee_name(df: pd.DataFrame, name: str) -> pd.Series: pass def get_sales_by_date_range(df: pd.DataFrame, start_date: str, end_date: str) -> pd.DataFrame: pass def get_top_n_employees_by_sales(df: pd.DataFrame, n: int) -> pd.DataFrame: pass def filter_department_sales(df: pd.DataFrame, department: str, min_sales: float) -> pd.DataFrame: pass def add_employee_bonus_column(df: pd.DataFrame, bonus_threshold: float, bonus_amount: float) -> pd.DataFrame: pass ``` Please ensure to handle edge cases like non-existent employee names, invalid date formats, and cases where no records meet the specified criteria. **Example Case:** ```python data = { \'employee_id\': [1, 2, 3, 4], \'employee_name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\'], \'department\': [\'Sales\', \'Marketing\', \'Sales\', \'Sales\'], \'sales\': [250, 150, 300, 200], \'date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-03\', \'2023-01-04\'] } df = pd.DataFrame(data).set_index(\'employee_id\') # Function Calls and Expected Outputs print(get_sales_by_employee_name(df, \'Alice\')) # Output: pd.Series([250], index=pd.Index([1], name=\'employee_id\')) print(get_sales_by_date_range(df, \'2023-01-01\', \'2023-01-03\')) # Output: DataFrame containing rows 1, 2, 3 print(get_top_n_employees_by_sales(df, 2)) # Output: DataFrame containing rows for employee_id 3 and 1 print(filter_department_sales(df, \'Sales\', 200)) # Output: DataFrame containing rows for employee_id 1 and 3 print(add_employee_bonus_column(df, 200, 50)) # Output: Original DataFrame with an additional \'bonus\' column ``` Ensure thorough testing of each function with a variety of data inputs to validate correctness.","solution":"import pandas as pd def get_sales_by_employee_name(df: pd.DataFrame, name: str) -> pd.Series: Returns sales records for the given employee_name. return df[df[\'employee_name\'] == name][\'sales\'] def get_sales_by_date_range(df: pd.DataFrame, start_date: str, end_date: str) -> pd.DataFrame: Returns records within the specified date range. return df[(df[\'date\'] >= start_date) & (df[\'date\'] <= end_date)] def get_top_n_employees_by_sales(df: pd.DataFrame, n: int) -> pd.DataFrame: Returns top n employees based on their total sales. return df.groupby(\'employee_name\')[\'sales\'].sum().nlargest(n) def filter_department_sales(df: pd.DataFrame, department: str, min_sales: float) -> pd.DataFrame: Returns employees in the specified department with sales greater than min_sales. return df[(df[\'department\'] == department) & (df[\'sales\'] > min_sales)] def add_employee_bonus_column(df: pd.DataFrame, bonus_threshold: float, bonus_amount: float) -> pd.DataFrame: Adds a \'bonus\' column to DataFrame where the value is bonus_amount if the employee\'s total sales exceed bonus_threshold, otherwise 0. total_sales = df.groupby(\'employee_id\')[\'sales\'].transform(\'sum\') df[\'bonus\'] = df.apply(lambda row: bonus_amount if total_sales[row.name] > bonus_threshold else 0, axis=1) return df"},{"question":"**Coding Assessment Question** # Objectives 1. Demonstrate understanding of the `reprlib` module in Python 3.10. 2. Create a subclass of `reprlib.Repr` to add custom handling for a specific object type. 3. Ensure proper handling of size limits and recursion in object representations. # Problem Statement You are required to write a Python class `CustomRepr` that is a subclass of `reprlib.Repr`. This subclass should have custom representation logic for `pandas.DataFrame` objects. The custom representation should: - Limit the number of rows and columns displayed. - Display a placeholder \\"...\\" if the truncated DataFrame is not fully shown. Additionally, ensure that the representation adheres to the overall size limits set for other types of objects. Specifications: 1. **Class Name**: `CustomRepr` 2. **Subclass**: `reprlib.Repr` 3. **Custom Attributes**: - `maxrows`: Maximum number of rows to display in the DataFrame (default = 5). - `maxcolumns`: Maximum number of columns to display in the DataFrame (default = 5). 4. **Custom Method**: - `repr_DataFrame(self, obj, level)`: Method to handle `pandas.DataFrame` objects. 5. **Additional Requirements**: - Limit overall size based on `maxstring` for string representation. - Utilize `repr1` method for recursive representation of DataFrames and other objects. 6. **Constraints**: - Must handle extremely large or deeply nested objects gracefully. - Ensure `repr()` method usage adheres to all set size limits. # Example Usage ```python import reprlib import pandas as pd # Sample DataFrame df = pd.DataFrame({ \'A\': range(10), \'B\': range(10, 20), \'C\': range(20, 30) }) # Custom Repr class class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxrows = 5 self.maxcolumns = 5 self.maxstring = 100 # Overall limitation on string length def repr_DataFrame(self, obj, level): if level <= 0: return \'...\' truncated = obj.iloc[:self.maxrows, :self.maxcolumns] if truncated.shape == obj.shape: return repr(truncated) else: return repr(truncated) + \'...\' aRepr = CustomRepr() print(aRepr.repr(df)) ``` This example should print the representation of the DataFrame limited to 5 rows and 5 columns, adding \\"...\\" if the DataFrame exceeds these limits in either dimension. # Input - You will be provided with a `pandas.DataFrame` object to represent. # Output - A string representing the object with the specified size limits. # Notes - Ensure to handle the string length limitation as well. - Test your implementation thoroughly with diverse DataFrame configurations. Good Luck!","solution":"import reprlib import pandas as pd class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxrows = 5 self.maxcolumns = 5 self.maxstring = 100 # Overall limitation on string length def repr_DataFrame(self, obj, level): if level <= 0: return \'...\' truncated = obj.iloc[:self.maxrows, :self.maxcolumns] repr_string = repr(truncated) if truncated.shape == obj.shape: return repr_string else: return repr_string + \'...\' # Registering repr_DataFrame with reprlib for pandas DataFrame objects reprlib.aRepr = CustomRepr() reprlib.aRepr.repr_DataFrame = reprlib.aRepr.repr_DataFrame.__get__(reprlib.aRepr, CustomRepr) def repr_df(df): return reprlib.aRepr.repr(df)"},{"question":"Given the fundamental C API functions for iterators described in the documentation, your task is to implement a similar iterator protocol in Python. Here are the requirements: 1. **Implement a class `CustomIterator`** that provides the following methods: - `__init__(self, data)`: Initializes the iterator with the provided data (a list). - `__iter__(self)`: Returns the iterator object itself. - `__next__(self)`: Returns the next value from the iterator. If there are no remaining values, raises a `StopIteration` exception. 2. **Implement a class `CustomAsyncIterator`** that simulates an asynchronous iterator with the following methods: - `__init__(self, data)`: Initializes the async iterator with the provided data (a list). - `__aiter__(self)`: Returns the async iterator object itself. - `__anext__(self)`: Returns the next value from the async iterator. If there are no remaining values, raises a `StopAsyncIteration` exception. 3. **Implement a function `custom_iter_next(iterator)`** that mimics the behavior of `PyIter_Next`: - The function takes an iterator object as input. - It returns the next value from the iterator, or `None` if there are no remaining values. 4. **Implement a function `custom_aiter_send(async_iterator, value)`** that mimics the behavior of `PyIter_Send`: - The function takes an async iterator object and a value to send into the iterator. - Returns a dictionary with the following possible keys: - `\\"return\\"`: If the async iterator has completed its iteration. - `\\"yield\\"`: If the async iterator yields a value. - `\\"error\\"`: If the async iterator raises an exception. Here is the expected input and output format: - **Input**: - A list of values to be used for creating the iterators. - An optional value to be sent into the async iterator. - **Output**: - A list of results from the synchronous iterator. - A dictionary indicating the result of sending a value into the async iterator. # Example ```python # Example input values = [1, 2, 3] # Creating synchronous iterator and collecting values sync_iter = CustomIterator(values) sync_results = [] try: while True: sync_results.append(custom_iter_next(sync_iter)) except StopIteration: pass # Creating asynchronous iterator and sending a value async_iter = CustomAsyncIterator(values) async_result = await custom_aiter_send(async_iter, 0) # Example value to send # Example output print(sync_results) # [1, 2, 3] print(async_result) # {\'return\': [1, 2, 3]}, {\'yield\': next_value}, or {\'error\': None} ``` # Constraints - Assume that input lists are non-empty and contain only integer values. - The `custom_aiter_send` function should handle sending values in a coroutine whether done synchronously or asynchronously. Your implementation should have functionality similar to described C API functions, but entirely in Python using its native constructs.","solution":"class CustomIterator: def __init__(self, data): self.data = data self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.data): result = self.data[self.index] self.index += 1 return result else: raise StopIteration class CustomAsyncIterator: def __init__(self, data): self.data = data self.index = 0 def __aiter__(self): return self async def __anext__(self): if self.index < len(self.data): result = self.data[self.index] self.index += 1 return result else: raise StopAsyncIteration def custom_iter_next(iterator): try: return next(iterator) except StopIteration: return None async def custom_aiter_send(async_iterator, value): try: result = await async_iterator.__anext__() return {\\"yield\\": result} except StopAsyncIteration: return {\\"return\\": list(async_iterator.data)} except Exception as e: return {\\"error\\": str(e)}"},{"question":"# Python Coding Assessment: Custom Container Implementation **Objective:** Design and implement a custom Python container class that emulates parts of Python\'s built-in `list` and `dict` behaviors. The class should exhibit specific functionalities using appropriate special methods and ensure performance efficiency. **Background:** You are required to create a class called `HybridContainer` which combines features of both a list and a dictionary. Each item in this container should behave like a dictionary with integer keys representing indices, and each key should map to exactly one value. However, the container should also support list-like behaviors such as slicing, iteration, and length determination. **Requirements and Constraints:** 1. **Class Definition:** - The `HybridContainer` should internally store data in a list-like structure where each element is a dictionary with integer keys. - The keys for each dictionary should start from `0` and be sequential. 2. **Functionality:** - **Initialization:** The initializer should accept a list of dictionaries as input. - **Element Access:** Allow accessing elements by index (`__getitem__`) and setting elements by index (`__setitem__`). - **Iteration:** Implement iterable behavior so that elements can be iterated over in a Pythonic manner. - **Slicing:** Implement slicing capability so parts of the `HybridContainer` can be extracted similarly to a list. - **Length:** Implement length determination (`__len__`) to return the number of elements in the container. - **Representation:** Provide meaningful string representation (`__repr__`). - **Containment Check:** Implement the `in` operator to check for the existence of specific dictionary keys (`__contains__`). - **Adding Elements:** Implement a method to add elements, and ensure unique integer keys within each dictionary. 3. **Special Methods:** - `__getitem__`, `__setitem__`, `__len__`, `__repr__`, `__iter__`, `__contains__` 4. **Performance Requirements:** - Ensure that all operations maintain an average time complexity of O(1) for indexing access and insertion. **Example Behavior:** ```python # Initialization hc = HybridContainer([{0: \'a\'}, {1: \'b\'}, {2: \'c\'}]) # Access elements by index assert hc[0] == {0: \'a\'} assert hc[2] == {2: \'c\'} # Set element by index hc[1] = {1: \'d\'} assert hc[1] == {1: \'d\'} # Length determination assert len(hc) == 3 # Iteration for item in hc: print(item) # Slicing print(hc[1:3]) # Should print: [{1: \'d\'}, {2: \'c\'}] # Containment Check assert {1: \'d\'} in hc # Adding elements hc.add_element({3: \'e\'}) assert len(hc) == 4 assert hc[3] == {3: \'e\'} ``` **Implementation:** Implement the class `HybridContainer` following the requirements listed above. ```python class HybridContainer: def __init__(self, initial_data): pass def __getitem__(self, index): pass def __setitem__(self, index, value): pass def __len__(self): pass def __repr__(self): pass def __iter__(self): pass def __contains__(self, value): pass def add_element(self, value): pass ``` **Deliverable:** Submit the completed implementation of the `HybridContainer` class along with a short script demonstrating its core functionalities.","solution":"class HybridContainer: def __init__(self, initial_data): self.data = initial_data def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): if isinstance(value, dict): self.data[index] = value else: raise ValueError(\\"Value must be a dictionary with integer key and any value\\") def __len__(self): return len(self.data) def __repr__(self): return f\\"HybridContainer({self.data})\\" def __iter__(self): return iter(self.data) def __contains__(self, key): for item in self.data: if key in item: return True return False def add_element(self, value): if not isinstance(value, dict): raise ValueError(\\"Value must be a dictionary with integer key and any value\\") self.data.append(value)"},{"question":"# Secure File Integrity Check with HMAC In this coding assessment, you are required to implement a secure file integrity check using the HMAC (Hash-based Message Authentication Code) algorithm. Your task is to write a function that computes and verifies the HMAC digest of a file\'s content using a specified cryptographic hash algorithm. # Function Signature ```python def verify_file_integrity(file_path: str, key: bytes, digestmod: str, expected_hexdigest: str) -> bool: Verifies the integrity of a file using HMAC. Args: - file_path (str): The path to the file whose integrity needs to be verified. - key (bytes): The secret key used for generating the HMAC. - digestmod (str): The name of the hash algorithm to use (e.g., \'sha256\'). - expected_hexdigest (str): The expected HMAC digest in hexadecimal format. Returns: - bool: True if the computed HMAC matches the expected hexdigest, False otherwise. ``` # Details 1. **Input:** - `file_path` is the path to the file whose integrity needs to be verified. - `key` is a secret key in bytes used for HMAC generation. - `digestmod` is the name of the hashing algorithm to use (e.g., \'sha256\', \'sha1\', \'md5\'). - `expected_hexdigest` is the expected HMAC digest in hexadecimal format (as a string) which you will compare against. 2. **Output:** - The function should return `True` if the computed HMAC matches the expected hexadecimal digest, otherwise `False`. 3. **Constraints:** - Ensure your function can handle large files efficiently. - Use the appropriate HMAC functions and methods as described in the documentation. - Implement a secure comparison using `hmac.compare_digest()` to prevent timing attacks. # Example Usage ```python file_path = \'example.txt\' key = b\'secret-key\' digestmod = \'sha256\' expected_hexdigest = \'expected_hmac_hexdigest\' result = verify_file_integrity(file_path, key, digestmod, expected_hexdigest) print(result) # Output should be True or False based on the HMAC comparison ``` # Notes - You can assume the file at `file_path` exists and is readable. - The `key` should be kept secure and not hard-coded within your implementation. Test your function with various files and digest algorithms to ensure correctness and efficiency.","solution":"import hmac import hashlib def verify_file_integrity(file_path: str, key: bytes, digestmod: str, expected_hexdigest: str) -> bool: Verifies the integrity of a file using HMAC. Args: - file_path (str): The path to the file whose integrity needs to be verified. - key (bytes): The secret key used for generating the HMAC. - digestmod (str): The name of the hash algorithm to use (e.g., \'sha256\'). - expected_hexdigest (str): The expected HMAC digest in hexadecimal format. Returns: - bool: True if the computed HMAC matches the expected hexdigest, False otherwise. hash_func = getattr(hashlib, digestmod) hmac_obj = hmac.new(key, digestmod=hash_func) with open(file_path, \'rb\') as f: for chunk in iter(lambda: f.read(4096), b\'\'): hmac_obj.update(chunk) computed_hexdigest = hmac_obj.hexdigest() return hmac.compare_digest(computed_hexdigest, expected_hexdigest)"},{"question":"# Numerical Operations and Conversions **Objective:** Write a function `calculate_expressions` that accepts two numbers (as Python objects) and performs a series of operations defined below. Your task is to use the corresponding functions from the provided documentation to implement this. Function Signature ```python def calculate_expressions(obj1: \'PyObject*\', obj2: \'PyObject*\') -> dict: ``` Input: - `obj1`: A Python object that can be interpreted as a number. - `obj2`: A Python object that can be interpreted as a number. Output: A dictionary with the following keys and their corresponding values: - `addition`: the result of adding `obj1` and `obj2`. - `subtraction`: the result of subtracting `obj2` from `obj1`. - `multiplication`: the result of multiplying `obj1` and `obj2`. - `true_division`: the result of dividing `obj1` by `obj2`. - `remainder`: the result of getting the remainder of `obj1` divided by `obj2`. - `power`: the result of raising `obj1` to the power of `obj2`. - `int_conversion`: `obj1` converted to an integer. - `float_conversion`: `obj2` converted to a float. Constraints: - Ensure that the function handles errors or invalid inputs gracefully. If an operation fails due to invalid input, the corresponding value in the output dictionary should be `None`. - You are required to use the provided functions from the documentation to perform the necessary operations. - You must handle the conversion of inputs appropriately using the `PyNumber_Check` function to ensure they can be processed as numbers. Example: ```python result = calculate_expressions(PyLong_FromLong(5), PyFloat_FromDouble(3.0)) print(result) # Output: # { # \\"addition\\": 8, # \\"subtraction\\": 2, # \\"multiplication\\": 15, # \\"true_division\\": 1.6666666666666667, # \\"remainder\\": 2, # \\"power\\": 125, # \\"int_conversion\\": 5, # \\"float_conversion\\": 3.0 # } ``` **Notes:** - You may assume access to the necessary functions through a provided interface or module in the actual implementation environment. - Document any assumptions you make and provide comments explaining the steps in your code. - Consider performance implications and edge cases, such as division by zero.","solution":"def calculate_expressions(obj1, obj2): Perform a series of numerical operations and conversions on two Python objects interpreted as numbers. try: # Convert inputs to numerical values num1 = float(obj1) num2 = float(obj2) results = { \'addition\': num1 + num2, \'subtraction\': num1 - num2, \'multiplication\': num1 * num2, \'true_division\': num1 / num2 if num2 != 0 else None, \'remainder\': num1 % num2 if num2 != 0 else None, \'power\': num1 ** num2, \'int_conversion\': int(num1), \'float_conversion\': float(num2), } return results except (ValueError, TypeError): return { \'addition\': None, \'subtraction\': None, \'multiplication\': None, \'true_division\': None, \'remainder\': None, \'power\': None, \'int_conversion\': None, \'float_conversion\': None, }"},{"question":"**Objective:** Demonstrate your understanding of working with NNTP servers using the `nntplib` module in Python. **Problem Statement:** You are tasked with implementing a script that connects to an NNTP server, retrieves the latest 20 articles from a specified newsgroup, and prints out their subjects and authors. Additionally, you will post a new article to the specified newsgroup. **Requirements:** 1. Connect to the NNTP server at `\'news.gmane.io\'`. 2. Authenticate using dummy credentials (`user=\'dummy_user\'`, `password=\'dummy_pass\'`). 3. Select the newsgroup `\'gmane.comp.python.committers\'`. 4. Retrieve and print the subjects and authors of the latest 20 articles. 5. Post a new article to the newsgroup with the subject `\'Test Article\'` and a body containing `\'This is a test article.\'`. **Input:** - Newsgroup name: `\'gmane.comp.python.committers\'` - User credentials: `user=\'dummy_user\'`, `password=\'dummy_pass\'` - Article details: - Subject: `\'Test Article\'` - Body: `\'This is a test article.\'` **Output:** - A list of the latest 20 articles with their subjects and authors. - Confirmation message indicating the article was posted successfully. **Constraints:** - Handle any possible exceptions that might occur during the connection or retrieval process. - Ensure you use the `decode_header` function to properly decode any non-ASCII characters in the article headers. - The posted article must include minimally required headers. **Points to Consider:** - The `nntplib.NNTP` class and its methods. - Proper error handling to manage `NNTPError`, `NNTPReplyError`, `NNTPTemporaryError`, and `NNTPPermanentError` exceptions. **Example Output:** ``` ID: 1077, Subject: \\"Commit Privileges Updated\\", Author: \\"John Doe <john@example.com>\\" ID: 1078, Subject: \\"New Version Released\\", Author: \\"Jane Smith <jane@example.com>\\" ... Article posted successfully. ``` **Implementation:** ```python import nntplib from nntplib import NNTP, decode_header import datetime def fetch_and_post_articles(): # Define your NNTP server and credentials server = \'news.gmane.io\' newsgroup = \'gmane.comp.python.committers\' user = \'dummy_user\' password = \'dummy_pass\' try: # Establish connection and authentication with NNTP(server, user=user, password=password) as nntp: # Select newsgroup resp, count, first, last, name = nntp.group(newsgroup) print(f\'Group {name} has {count} articles, range {first} to {last}\') # Fetch and print subjects and authors of the latest 20 articles resp, overviews = nntp.over((last - 19, last)) for id, over in overviews: subject = decode_header(over[\'subject\']) author = decode_header(over[\'from\']) print(f\'ID: {id}, Subject: \\"{subject}\\", Author: \\"{author}\\"\') # Prepare and post a new article article_headers = f\\"Subject: Test ArticlernFrom: example@example.comrnNewsgroups: {newsgroup}rn\\" article_body = \\"This is a test article.\\" article = f\\"{article_headers}rn{article_body}\\" response = nntp.post(article.encode(\'utf-8\')) print(response) except nntplib.NNTPError as e: print(f\\"NNTP error occurred: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Run the function fetch_and_post_articles() ```","solution":"import nntplib from nntplib import decode_header import email from email.message import EmailMessage def fetch_and_post_articles(): server = \'news.gmane.io\' newsgroup = \'gmane.comp.python.committers\' user = \'dummy_user\' password = \'dummy_pass\' try: with nntplib.NNTP(server, user=user, password=password) as nntp: # Select newsgroup resp, count, first, last, name = nntp.group(newsgroup) print(f\'Group {name} has {count} articles, range {first} to {last}\') # Fetch and print subjects and authors of the latest 20 articles resp, overviews = nntp.over((str(int(last) - 19), last)) articles = [] for id, over in overviews: subject = decode_header(over[\'subject\']) author = decode_header(over[\'from\']) articles.append(f\'ID: {id}, Subject: \\"{subject}\\", Author: \\"{author}\\"\') print(\\"n\\".join(articles)) # Prepare and post a new article msg = EmailMessage() msg[\'Subject\'] = \'Test Article\' msg[\'From\'] = \'example@example.com\' msg[\'Newsgroups\'] = newsgroup msg.set_content(\'This is a test article.\') response = nntp.post(msg) if response is not None: print(\'Article posted successfully.\') else: print(\'Failed to post the article\') except (nntplib.NNTPError, nntplib.NNTPReplyError, nntplib.NNTPTemporaryError, nntplib.NNTPPermanentError) as e: print(f\\"An NNTP error occurred: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Run the function fetch_and_post_articles()"},{"question":"# Coding Assessment: Graph Manipulation with `torch.fx` __Objective:__ Demonstrate your understanding of PyTorch\'s `torch.fx` module by creating a transformation that modifies a PyTorch model\'s computation graph. # Problem Statement You are given a simple neural network model implemented in PyTorch. Your task is to implement a graph transformation using `torch.fx` that replaces all occurrences of a specified function (e.g., `torch.add`) in the model with another function (e.g., `torch.mul`). Model Here is an example neural network model: ```python import torch class SimpleModel(torch.nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear1 = torch.nn.Linear(10, 5) self.linear2 = torch.nn.Linear(5, 2) def forward(self, x): x = torch.add(x, self.linear1.weight) # Function to replace x = self.linear1(x) x = torch.mul(x, 2) x = self.linear2(x) return x ``` Task Requirements 1. Write a function `transform_model` that takes a model (`torch.nn.Module`) and replaces all instances of `torch.add` with `torch.mul` in its computation graph. 2. Ensure that the transformed model retains the same functionality, except for the changes specified. 3. Verify the transformation by applying it to `SimpleModel` and printing the modified computation graph. Constraints - Use `torch.fx` for symbolic tracing and graph manipulation. - Ensure that the resulting graph is valid and can be executed without errors. - The transformation should handle all instances of the specified function within the model. Input and Output Formats - **Input**: - `model`: an instance of `torch.nn.Module` - **Output**: - A transformed `torch.nn.Module` with specified function replacements. Example ```python # Given Model model = SimpleModel() # Transform the model transformed_model = transform_model(model) # Print the modified computation graph graph = torch.fx.symbolic_trace(transformed_model) graph.graph.print_tabular() ``` # Implementation Details Provide your implementation below: ```python import torch import torch.fx def transform_model(model: torch.nn.Module) -> torch.nn.Module: class CustomTracer(torch.fx.Tracer): def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool: return False # Determine whether to trace submodules def replace_functions(graph): for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: node.target = torch.mul graph.lint() return graph tracer = CustomTracer() graph = tracer.trace(model) new_graph = replace_functions(graph) new_graph_module = torch.fx.GraphModule(model, new_graph) return new_graph_module # Example usage if __name__ == \\"__main__\\": model = SimpleModel() transformed_model = transform_model(model) traced = torch.fx.symbolic_trace(transformed_model) traced.graph.print_tabular() ``` # Notes - Use the `graph.lint` method to ensure the graph is well-formed after transformation. - Test the transformed model with sample inputs to ensure correctness.","solution":"import torch import torch.fx class SimpleModel(torch.nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear1 = torch.nn.Linear(10, 5) self.linear2 = torch.nn.Linear(5, 2) def forward(self, x): x = torch.add(x, self.linear1.weight) # Function to replace x = self.linear1(x) x = torch.mul(x, 2) x = self.linear2(x) return x def transform_model(model: torch.nn.Module) -> torch.nn.Module: class CustomTracer(torch.fx.Tracer): def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool: return False # Trace through all submodules def replace_functions(graph): for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: node.target = torch.mul graph.lint() return graph tracer = CustomTracer() graph = tracer.trace(model) new_graph = replace_functions(graph) new_graph_module = torch.fx.GraphModule(model, new_graph) return new_graph_module # Example usage if __name__ == \\"__main__\\": model = SimpleModel() transformed_model = transform_model(model) traced = torch.fx.symbolic_trace(transformed_model) traced.graph.print_tabular()"},{"question":"# Boolean Manipulation in Python Problem Statement You are given an integer `n`. Your task is to implement a function `check_boolean(n)` that performs the following operations: 1. Converts the integer `n` to a boolean value. 2. Returns a string `\\"True\\"` if the resulting boolean is `True` or `\\"False\\"` if the boolean is `False`. The aim is to help you understand how booleans are derived from integers and ensure you can correctly handle boolean values in Python. Function Signature ```python def check_boolean(n: int) -> str: # Your code here ``` Input - `n` (int): An integer input which needs to be converted to a boolean. Output - Return type: string - The function should return `\\"True\\"` if the boolean value derived from `n` is `True`, otherwise it should return `\\"False\\"`. Example ```python assert check_boolean(5) == \\"True\\" assert check_boolean(0) == \\"False\\" assert check_boolean(-3) == \\"True\\" assert check_boolean(1) == \\"True\\" ``` Constraints - You should use the built-in boolean conversion in Python. - Do not use conditional statements like `if`. - The function should handle any integer value within the range of valid Python integers. Notes In Python: - An integer `0` converts to the boolean `False`. - Any non-zero integer converts to the boolean `True`. Implement the function `check_boolean(n)`.","solution":"def check_boolean(n: int) -> str: Converts the integer n to a boolean value. Returns \\"True\\" if the resulting boolean is True, otherwise \\"False\\". return str(bool(n))"},{"question":"**Objective:** To assess your understanding and ability to work with the `shelve` module in Python to create and manage persistent storage for complex data structures. **Problem Statement:** You are given a list of books, where each book is represented as a dictionary containing the following information: - `title`: A string representing the title of the book. - `author`: A string representing the author of the book. - `published_year`: An integer representing the year the book was published. You are required to create a persistent storage using the `shelve` module to store, retrieve, and update this list of books. Additionally, you need to implement functionalities to compute and return the oldest book and count the number of books for a given author. **Function Specifications:** 1. **store_books(file_name: str, books: List[Dict[str, Union[str, int]]]) -> None:** - This function takes in the name of the shelf file and a list of books. - It should store the list of books in the shelf. 2. **get_books(file_name: str) -> List[Dict[str, Union[str, int]]]:** - This function takes the name of the shelf file. - It should retrieve and return the list of books from the shelf. 3. **update_book(file_name: str, title: str, updated_info: Dict[str, Union[str, int]]) -> None:** - This function takes the name of the shelf file, the title of a book, and a dictionary of updated information. - It should update the item in the list corresponding to the provided title with the updated information. 4. **get_oldest_book(file_name: str) -> Dict[str, Union[str, int]]:** - This function takes the name of the shelf file. - It should return the dictionary of the oldest book in the list. 5. **count_books_by_author(file_name: str, author: str) -> int:** - This function takes the name of the shelf file and the name of an author. - It should return the count of books by that author. **Constraints and Notes:** - Use the `shelve` module for persistent storage. - Ensure that all read/write operations are handled safely, using context managers for proper resource management. - Handle cases where books or authors may not be found. - Synchronize the shelf properly to ensure data consistency. **Example Usage:** ```python books = [ {\\"title\\": \\"Book A\\", \\"author\\": \\"Author X\\", \\"published_year\\": 2000}, {\\"title\\": \\"Book B\\", \\"author\\": \\"Author Y\\", \\"published_year\\": 1995}, {\\"title\\": \\"Book C\\", \\"author\\": \\"Author X\\", \\"published_year\\": 2010} ] file_name = \\"bookshelf\\" # Store books store_books(file_name, books) # Retrieve books retrieved_books = get_books(file_name) print(retrieved_books) # Update a book update_book(file_name, \\"Book B\\", {\\"author\\": \\"Author Z\\", \\"published_year\\": 1996}) # Get the oldest book oldest_book = get_oldest_book(file_name) print(oldest_book) # Count books by author count = count_books_by_author(file_name, \\"Author X\\") print(count) ``` Ensure your functions are robust and handle all edge cases, such as empty lists or non-existent titles/authors, gracefully.","solution":"import shelve from typing import List, Dict, Union def store_books(file_name: str, books: List[Dict[str, Union[str, int]]]) -> None: with shelve.open(file_name) as shelf: shelf[\'books\'] = books def get_books(file_name: str) -> List[Dict[str, Union[str, int]]]: with shelve.open(file_name) as shelf: return shelf.get(\'books\', []) def update_book(file_name: str, title: str, updated_info: Dict[str, Union[str, int]]) -> None: with shelve.open(file_name) as shelf: books = shelf.get(\'books\', []) for book in books: if book[\'title\'] == title: book.update(updated_info) break shelf[\'books\'] = books def get_oldest_book(file_name: str) -> Dict[str, Union[str, int]]: with shelve.open(file_name) as shelf: books = shelf.get(\'books\', []) if not books: return {} return min(books, key=lambda x: x[\'published_year\']) def count_books_by_author(file_name: str, author: str) -> int: with shelve.open(file_name) as shelf: books = shelf.get(\'books\', []) return sum(1 for book in books if book[\'author\'] == author)"},{"question":"Objective To assess your ability to work with time series data in pandas, including parsing, generating sequences, localized time zones, resampling, and date arithmetic. Problem Statement You are provided with a CSV file `sales_data.csv` which contains the following columns: - `timestamp`: Timestamps of sales transactions in the format `%Y-%m-%d %H:%M:%S` - `amount`: Sales amount in USD Your task is to perform the following operations using pandas: 1. **Read the data**: Load the data from `sales_data.csv`. 2. **Convert the timestamp**: Parse the `timestamp` column to datetime format and set it as the index of the DataFrame. 3. **Localize timezone**: Localize the timestamps to \'UTC\', then convert them to \'US/Central\'. 4. **Resample the data**: Resample the data to daily frequency and compute the total sales amount for each day. 5. **Calculate daily differences**: Compute the difference in daily sales amounts. 6. **Identify peak sales day**: Identify the day with the highest sales amount. Implementation Implement the function `process_sales_data(file_path)` to perform the following steps: 1. Read the CSV file. 2. Parse the `timestamp` column and set it as the index. 3. Localize the timestamps to \'UTC\' and convert to \'US/Central\' time zone. 4. Resample the data to daily frequency and compute the total sales for each day. 5. Compute the difference in daily sales amounts. 6. Identify the day with the highest sales amount and return it. Constraints - You may assume that the CSV file is well-formed and the `amount` column contains only valid numerical values. Function Signature ```python def process_sales_data(file_path: str) -> str: pass ``` Example Suppose `sales_data.csv` contains: ``` timestamp,amount 2023-01-01 10:00:00,100 2023-01-01 15:00:00,150 2023-01-02 09:00:00,200 2023-01-02 12:00:00,250 2023-01-03 08:00:00,300 ``` Calling `process_sales_data(\'sales_data.csv\')`: ```python highest_sales_day = process_sales_data(\'sales_data.csv\') print(highest_sales_day) ``` Should output: ``` 2023-01-02 ``` **Note:** Ensure generality in the function so it works for any well-formed `sales_data.csv` input.","solution":"import pandas as pd def process_sales_data(file_path: str) -> str: # Step 1: Read the CSV file data = pd.read_csv(file_path) # Step 2: Parse the `timestamp` column and set it as the index data[\'timestamp\'] = pd.to_datetime(data[\'timestamp\']) data.set_index(\'timestamp\', inplace=True) # Step 3: Localize the timestamps to \'UTC\' and convert to \'US/Central\' time zone data.index = data.index.tz_localize(\'UTC\').tz_convert(\'US/Central\') # Step 4: Resample the data to daily frequency and compute the total sales for each day daily_sales = data[\'amount\'].resample(\'D\').sum() # Step 5: Compute the difference in daily sales amounts daily_diff = daily_sales.diff() # Step 6: Identify the day with the highest sales amount peak_sales_day = daily_sales.idxmax().strftime(\'%Y-%m-%d\') return peak_sales_day"},{"question":"**Objective**: Demonstrate your understanding of the `code` module by implementing a custom interactive interpreter with additional functionality. # Problem Statement You are required to create a custom REPL (Read-Eval-Print Loop) using the `code.InteractiveConsole` class. The custom console should have the following features: 1. **Welcome Banner**: Display a custom welcome banner when the interpreter starts. 2. **Execution Logging**: Log every executed command to a file called `execution.log`. 3. **Custom Command**: Implement a custom command `!history` which, when entered, prints the history of all previously executed commands in the session. 4. **Error Handling**: Include robust error handling to log syntax errors and runtime exceptions. 5. **Exit Message**: Display a custom exit message when the interpreter session ends. # Implementation Details - **Class Signature**: Create a class `CustomConsole` that inherits from `code.InteractiveConsole`. - **Methods**: - `__init__(self, banner, exitmsg)`: Initializes the console with a banner and exit message. - `push(self, line)`: Pushes a line of source code to the interpreter, logs the line, and handles custom commands. - `write(self, data)`: Writes data to `sys.stderr`. - `showtraceback(self)`: Logs and displays runtime exceptions. - `showsyntaxerror(self, filename=None)`: Logs and displays syntax errors. # Input and Output - **Input**: The user will input lines of Python code or custom commands interactively. - **Output**: The console will display the result of the executed commands, handle errors, and print custom messages. # Example Usage ```python if __name__ == \\"__main__\\": console = CustomConsole(banner=\\"Welcome to Custom REPL!\\", exitmsg=\\"Goodbye!\\") console.interact() ``` Constraints 1. You must use the `code` module and its classes. 2. The log file should accurately record every command executed within the session. 3. The custom `!history` command should retrieve and print the command history excluding itself. Notes - Pay close attention to the method signatures and inheritance. - Ensure robust error handling and proper logging of errors. - Define and handle custom commands within the `push` method. Submit your implementation of the `CustomConsole` class.","solution":"import code import sys import traceback class CustomConsole(code.InteractiveConsole): def __init__(self, banner=\\"Welcome to Custom REPL!\\", exitmsg=\\"Goodbye!\\"): super().__init__() self.banner = banner self.exitmsg = exitmsg self.log_file = \'execution.log\' self.history = [] with open(self.log_file, \'w\'): pass # Clear the log file at the beginning of each session def interact(self): print(self.banner) try: super().interact(banner=\\"\\") finally: print(self.exitmsg) def push(self, line): if line == \'!history\': print(\\"n\\".join(self.history)) else: self.history.append(line) with open(self.log_file, \'a\') as f: f.write(line + \'n\') super().push(line) def write(self, data): sys.stderr.write(data) def showtraceback(self): exc_type, exc_value, exc_tb = sys.exc_info() tb = \'\'.join(traceback.format_exception(exc_type, exc_value, exc_tb)) with open(self.log_file, \'a\') as f: f.write(tb) self.write(tb) def showsyntaxerror(self, filename=None): exc_type, exc_value, exc_tb = sys.exc_info() se = \'\'.join(traceback.format_exception_only(exc_type, exc_value)) with open(self.log_file, \'a\') as f: f.write(se) self.write(se)"},{"question":"<|Analysis Begin|> The provided documentation explains issues and solutions related to using Batch Norm in conjunction with `functorch`\'s `vmap`. Specifically, the in-place updates required by Batch Norm are not well-supported when vmapped over a batch of inputs. The document offers several alternatives to handle this, such as switching to GroupNorm, modifying the BatchNorm module to not use running stats, configuring torchvision models to use GroupNorm, using a patching utility from `functorch`, or ensuring the model is in evaluation mode. Key concepts involved include understanding BatchNorm, GroupNorm, vmapping, and in-place operations with tensors. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: Handling Batch Norm with Vmap in Functorch **Objective:** Assess students\' understanding of necessary adjustments to neural network modules in PyTorch when using batching techniques like `vmap` from Functorch to handle Batch Normalization issues. **Problem Statement:** You are given a simple neural network model that uses `BatchNorm2d`. However, you need to ensure that this model can be used with `functorch`\'s `vmap` over a batch of inputs without encountering errors related to in-place updates to running stats of `BatchNorm2d`. To achieve this, you have to modify the existing model in two ways: (1) switch BatchNorm to GroupNorm and (2) use functorch\'s patching utility. **Instructions:** 1. Write a function `replace_batch_norm_with_group_norm` that takes a PyTorch model and replaces all `BatchNorm2d` layers with `GroupNorm` layers. 2. Write a function `patch_model_for_vmap` that takes a PyTorch model and uses functorch\'s `replace_all_batch_norm_modules_` to modify the BatchNorm layers to not use running stats. # Part 1: Replace BatchNorm2d with GroupNorm Implement the function `replace_batch_norm_with_group_norm(model, num_groups)`: - **Input:** - `model` (torch.nn.Module): A PyTorch neural network model. - `num_groups` (int): The number of groups to use in `GroupNorm`. - **Output:** - A modified model where all `BatchNorm2d` layers are replaced with `GroupNorm` layers. **Example:** ```python import torch.nn as nn class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(32) self.relu = nn.ReLU() def forward(self, x): x = self.relu(self.bn1(self.conv1(x))) x = self.relu(self.bn2(self.conv2(x))) return x model = SimpleCNN() modified_model = replace_batch_norm_with_group_norm(model, num_groups=4) ``` # Part 2: Patch Model for Vmap Implement the function `patch_model_for_vmap(model)`: - **Input:** - `model` (torch.nn.Module): A PyTorch neural network model. - **Output:** - A modified model where all `BatchNorm2d` layers are patched to not use running stats. **Example:** ```python from torch.func import replace_all_batch_norm_modules_ class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(32) self.relu = nn.ReLU() def forward(self, x): x = self.relu(self.bn1(self.conv1(x))) x = self.relu(self.bn2(self.conv2(x))) return x model = SimpleCNN() patched_model = patch_model_for_vmap(model) ``` *Note: Ensure that your functions are tested and verified to work as expected.* **Constraints:** - The number of groups (`num_groups`) in `GroupNorm` is such that the number of output channels of the corresponding convolutional layer is divisible by `num_groups`. Good luck!","solution":"import torch.nn as nn from functorch.experimental import replace_all_batch_norm_modules_ def replace_batch_norm_with_group_norm(model, num_groups): Replace all BatchNorm2d layers in the model with GroupNorm layers. Parameters: model (nn.Module): A PyTorch neural network model. num_groups (int): The number of groups to use in GroupNorm. Returns: nn.Module: A modified model with GroupNorm layers instead of BatchNorm2d. for name, module in model.named_children(): if isinstance(module, nn.BatchNorm2d): num_channels = module.num_features setattr(model, name, nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)) else: replace_batch_norm_with_group_norm(module, num_groups) return model def patch_model_for_vmap(model): Patch the model to replace BatchNorm2d layers with versions that do not update running stats, making it compatible with functorch\'s vmap. Parameters: model (nn.Module): A PyTorch neural network model. Returns: nn.Module: A modified model with patched BatchNorm2d layers. patched_model = replace_all_batch_norm_modules_(model) return patched_model"},{"question":"# Question: Advanced KDE Plotting with Seaborn You are provided with a dataset containing information about different species of penguins. Using the Seaborn library, you need to visualize the distribution of flipper lengths for different species of penguins. Your task is to create a KDE plot using the following specifications: 1. **Load the dataset**: - The dataset is named `penguins` and is available within the Seaborn library. 2. **Plot requirements**: - Create a KDE plot to show the distribution of flipper lengths. - Use the `species` column to color the distributions differently for each species. - Normalize the distributions so that the total area under the KDE curves for each species is the same. - Adjust the bandwidth of the estimation to `.5` to make the distribution smoother. - Show filled curves for better visual distinction. 3. **Bivariate KDE Plot**: - Additionally, create a bivariate KDE plot to show the joint distribution of `flipper_length_mm` and `body_mass_g`. - Use the `species` column to differentiate the distributions with different contour colors. - Display filled contours and ensure that the contours have 10 levels of distribution. - Utilize the `mako` colormap for the contours. # Input and Output Format - **Input**: No input required as the dataset is loaded directly using Seaborn functions. - **Output**: The function should display two plots as specified. - **Constraints**: Ensure that the plots are clearly readable, with appropriate labels for the axes and legends indicating the species. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_penguin_data(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Univariate KDE plot plt.figure(figsize=(10, 6)) sns.kdeplot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", bw_adjust=0.5, fill=True, common_norm=True ) plt.title(\'Distribution of Flipper Lengths by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') # Bivariate KDE plot plt.figure(figsize=(10, 6)) sns.kdeplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", fill=True, levels=10, cmap=\\"mako\\" ) plt.title(\'Joint Distribution of Flipper Length and Body Mass by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Body Mass (g)\') plt.show() ``` # Notes 1. Ensure you have the Seaborn library installed in your environment (`pip install seaborn`). 2. Use `plt.show()` at the end of your function to display the plots. 3. Properly label the plots for clarity.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguin_data(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Univariate KDE plot plt.figure(figsize=(10, 6)) sns.kdeplot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", bw_adjust=0.5, fill=True, common_norm=True ) plt.title(\'Distribution of Flipper Lengths by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.legend(title=\'Species\') # Bivariate KDE plot plt.figure(figsize=(10, 6)) sns.kdeplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", fill=True, levels=10, cmap=\\"mako\\" ) plt.title(\'Joint Distribution of Flipper Length and Body Mass by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Body Mass (g)\') plt.legend(title=\'Species\') plt.show()"},{"question":"Objective Demonstrate your understanding of controlling plot contexts in the seaborn library by creating multiple plots with different context settings. Problem Statement Write a function `custom_plot_contexts()` that takes no input arguments and performs the following tasks: 1. **Create two subplots side by side**: - The left subplot should show a line plot with the context set to \\"notebook\\". - The right subplot should show a line plot with the context set to \\"talk\\". 2. **Data for line plots**: - The x-axis should have values: [\\"A\\", \\"B\\", \\"C\\", \\"D\\", \\"E\\"] - The y-axis should have values: [1, 3, 2, 5, 4] 3. **Ensure that the different contexts are applied correctly and independently**: - Use context managers to set and reset contexts for the individual plots. - After applying the context, plot the respective line plot. Constraints - Use only the seaborn and matplotlib libraries to create the plots. - Ensure the plots are displayed correctly in a Jupyter notebook or similar Python environment. Expected Output Two line plots displayed side by side: - The left plot with a \\"notebook\\" context: smaller fonts and lines suitable for use in a Jupyter notebook. - The right plot with a \\"talk\\" context: larger fonts and lines suitable for use in a presentation. Below is an example of how the function signature should look: ```python def custom_plot_contexts(): # Your code here pass ``` Example Code Here is an example of how the function `custom_plot_contexts` might be used: ```python custom_plot_contexts() ``` The function should generate and display the subplots as described when called.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_plot_contexts(): This function creates two subplots side by side, demonstrating different seaborn plot contexts. The left subplot uses the \'notebook\' context, and the right subplot uses the \'talk\' context. # Data for the line plots x = [\\"A\\", \\"B\\", \\"C\\", \\"D\\", \\"E\\"] y = [1, 3, 2, 5, 4] # Create a figure with two subplot axes, side by side fig, axes = plt.subplots(1, 2, figsize=(12, 5)) # Set \'notebook\' context and create the first subplot with sns.plotting_context(\\"notebook\\"): sns.lineplot(x=x, y=y, ax=axes[0]) axes[0].set_title(\'Notebook Context\') # Set \'talk\' context and create the second subplot with sns.plotting_context(\\"talk\\"): sns.lineplot(x=x, y=y, ax=axes[1]) axes[1].set_title(\'Talk Context\') plt.tight_layout() plt.show()"},{"question":"You are tasked with developing a simple student management system that persists student records using the `shelve` module. Each student record will include a `name`, `age`, and `grades`. Requirements: 1. **Create Functions**: - **add_student(filename: str, student_id: str, name: str, age: int, grades: list) -> None**: This function will add a new student record to the shelf identified by `filename`. - **get_student(filename: str, student_id: str) -> dict**: This function will retrieve a student record by `student_id` from the shelf. - **update_student_grades(filename: str, student_id: str, new_grades: list) -> None**: This function will update the grades for an existing student. - **list_students(filename: str) -> list**: This function will return a list of all student IDs stored in the shelf. 2. **Persistence**: Ensure that data is properly saved to and loaded from the shelf. Utilize `writeback=True` to handle in-place modifications of student grades. 3. **Error Handling**: Appropriately handle cases where student records are not found. Constraints: 1. All strings will be non-empty and contain up to 100 characters. 2. The `age` will be a positive integer. 3. The `grades` list will contain integers representing grades, with each grade between 0 and 100. 4. The persistent storage should handle arbitrary interruptions without data loss. Example Usage and Expected Outcome: ```python add_student(\'students\', \'001\', \'Alice\', 20, [85, 90, 88]) add_student(\'students\', \'002\', \'Bob\', 21, [75, 80, 79]) print(get_student(\'students\', \'001\')) # Expected Output: {\'name\': \'Alice\', \'age\': 20, \'grades\': [85, 90, 88]} update_student_grades(\'students\', \'002\', [80, 82, 78]) print(get_student(\'students\', \'002\')) # Expected Output: {\'name\': \'Bob\', \'age\': 21, \'grades\': [80, 82, 78]} print(list_students(\'students\')) # Expected Output: [\'001\', \'002\'] ``` # Implementation: Below is a template for the functions you need to implement. Complete the implementations based on the requirements. ```python import shelve def add_student(filename: str, student_id: str, name: str, age: int, grades: list) -> None: pass # Your implementation here def get_student(filename: str, student_id: str) -> dict: pass # Your implementation here def update_student_grades(filename: str, student_id: str, new_grades: list) -> None: pass # Your implementation here def list_students(filename: str) -> list: pass # Your implementation here ``` **Note**: Remember to handle the opening and closing of the shelf appropriately, using a context manager for better resource management.","solution":"import shelve def add_student(filename: str, student_id: str, name: str, age: int, grades: list) -> None: with shelve.open(filename, writeback=True) as shelf: if student_id not in shelf: shelf[student_id] = {\'name\': name, \'age\': age, \'grades\': grades} def get_student(filename: str, student_id: str) -> dict: with shelve.open(filename) as shelf: if student_id in shelf: return shelf[student_id] else: raise KeyError(f\'Student ID {student_id} not found\') def update_student_grades(filename: str, student_id: str, new_grades: list) -> None: with shelve.open(filename, writeback=True) as shelf: if student_id in shelf: shelf[student_id][\'grades\'] = new_grades else: raise KeyError(f\'Student ID {student_id} not found\') def list_students(filename: str) -> list: with shelve.open(filename) as shelf: return list(shelf.keys())"},{"question":"Objective: Demonstrate your understanding and ability to use the `platform` module in Python to gather detailed information about the system where the Python interpreter is running. Problem Statement: Write a Python function named `system_summary` that uses the `platform` module to provide a summary of the system\'s details. Function Signature: ```python def system_summary() -> dict: ``` Expected Input and Output: The function does not take any parameters and should return a dictionary with the following keys and appropriate values from the respective `platform` functions: - `\'architecture\'`: Tuple returned by `platform.architecture()` - `\'machine\'`: String returned by `platform.machine()` - `\'node\'`: String returned by `platform.node()` - `\'platform\'`: String returned by `platform.platform()` - `\'processor\'`: String returned by `platform.processor()` - `\'python_build\'`: Tuple returned by `platform.python_build()` - `\'python_compiler\'`: String returned by `platform.python_compiler()` - `\'python_implementation\'`: String returned by `platform.python_implementation()` - `\'python_version\'`: String returned by `platform.python_version()` - `\'release\'`: String returned by `platform.release()` - `\'system\'`: String returned by `platform.system()` - `\'version\'`: String returned by `platform.version()` - `\'uname\'`: Named tuple returned by `platform.uname()` converted to a dictionary Constraints: 1. The function should handle exceptions gracefully and return `\'unknown\'` for any value that cannot be determined. 2. Dictionary entries that correspond to tuples should use the tuple as their value directly. Example: ```python def system_summary() -> dict: import platform summary = { \'architecture\': platform.architecture(), \'machine\': platform.machine() or \'unknown\', \'node\': platform.node() or \'unknown\', \'platform\': platform.platform(), \'processor\': platform.processor() or \'unknown\', \'python_build\': platform.python_build(), \'python_compiler\': platform.python_compiler() or \'unknown\', \'python_implementation\': platform.python_implementation() or \'unknown\', \'python_version\': platform.python_version(), \'release\': platform.release() or \'unknown\', \'system\': platform.system() or \'unknown\', \'version\': platform.version() or \'unknown\', \'uname\': platform.uname()._asdict() # Converting named tuple to dict } return summary # Example usage: # system_info = system_summary() # print(system_info) ``` The provided function should return a dictionary similar to the following example (actual values will vary based on the system): ```python { \'architecture\': (\'64bit\', \'ELF\'), \'machine\': \'x86_64\', \'node\': \'my-computer\', \'platform\': \'Linux-5.4.0-42-generic-x86_64-with-Ubuntu-20.04-focal\', \'processor\': \'Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz\', \'python_build\': (\'default\', \'Jul 22 2020 12:01:25\'), \'python_compiler\': \'GCC 9.3.0\', \'python_implementation\': \'CPython\', \'python_version\': \'3.8.5\', \'release\': \'5.4.0-42-generic\', \'system\': \'Linux\', \'version\': \'#46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020\', \'uname\': { \'system\': \'Linux\', \'node\': \'my-computer\', \'release\': \'5.4.0-42-generic\', \'version\': \'#46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020\', \'machine\': \'x86_64\', \'processor\': \'Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz\' } } ```","solution":"def system_summary() -> dict: import platform def safe_platform_call(func): try: return func() except Exception: return \'unknown\' summary = { \'architecture\': safe_platform_call(platform.architecture), \'machine\': safe_platform_call(platform.machine), \'node\': safe_platform_call(platform.node), \'platform\': safe_platform_call(platform.platform), \'processor\': safe_platform_call(platform.processor), \'python_build\': safe_platform_call(platform.python_build), \'python_compiler\': safe_platform_call(platform.python_compiler), \'python_implementation\': safe_platform_call(platform.python_implementation), \'python_version\': safe_platform_call(platform.python_version), \'release\': safe_platform_call(platform.release), \'system\': safe_platform_call(platform.system), \'version\': safe_platform_call(platform.version), \'uname\': safe_platform_call(lambda: platform.uname()._asdict()) } return summary"},{"question":"Objective Write a Python function that extracts and processes email addresses from a given string using regular expressions as implemented in the `re` module. Problem Statement Given a string that contains a mix of text and email addresses, you need to extract the email addresses and format them in a specific way. Implement a function `extract_and_format_emails(text: str) -> List[str]` that: 1. Extracts all email addresses from the provided string. An email address is defined as follows: - It contains exactly one \'@\' symbol. - The local part (before \'@\') may contain letters, digits, periods, and underscores. - The domain part (after \'@\') may contain letters, digits, and periods. - The domain part must contain at least one period. 2. Formats each email address such that: - The local part is converted to lowercase and all underscores are replaced with periods. - The domain part is converted to lowercase. Return the list of formatted email addresses in the order they were found in the text. Input - `text`: A string containing text mixed with email addresses. Output - A list of formatted email addresses. Constraints - The input string can be up to (10^6) characters long. - The function should be implemented with efficient use of regular expressions to ensure performance. Example ```python def extract_and_format_emails(text: str) -> List[str]: # Your implementation here # Sample Test Case input_text = \\"Hello, please contact us via support_email@Example.COM and sales.department@company.org!\\" print(extract_and_format_emails(input_text)) ``` **Expected Output:** ``` [\'support.email@example.com\', \'sales.department@company.org\'] ``` Notes - Use the `re` module to handle regular expression operations. - Only valid email addresses as described should be considered and formatted. - Preserve the order of email addresses as they appear in the input text.","solution":"import re from typing import List def extract_and_format_emails(text: str) -> List[str]: Extracts and formats email addresses from the given text. Args: text (str): A string containing text with email addresses. Returns: List[str]: A list of formatted email addresses. email_pattern = r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}b\' emails = re.findall(email_pattern, text) formatted_emails = [] for email in emails: local_part, domain_part = email.split(\'@\') local_part = local_part.replace(\'_\', \'.\').lower() domain_part = domain_part.lower() formatted_email = f\\"{local_part}@{domain_part}\\" formatted_emails.append(formatted_email) return formatted_emails"},{"question":"# Sparse Tensor Operations in PyTorch Objective: The goal of this exercise is to test your understanding of PyTorch\'s sparse tensor mechanics. You will need to create sparse tensors, convert them between formats, perform operations on them, and handle cases that involve uncoalesced tensors. Problem Statement: Task 1: Creating and Converting Sparse Tensors 1. **Create a 2D dense tensor** with shape `(4, 6)` which contains the elements: ```python [[0, 0, 0, 0, 1, 0], [0, 2, 0, 3, 0, 4], [0, 0, 5, 0, 0, 0], [6, 0, 0, 7, 0, 0]] ``` 2. **Convert this tensor** to different sparse formats: - COO (Coordinate) format - CSR (Compressed Sparse Row) format - CSC (Compressed Sparse Column) format 3. Implement a function `display_sparse_tensor_properties` which accepts a sparse tensor and prints its: - Indices - Values - Size - Number of specified elements Task 2: Sparse Tensor Operations 4. Given a 2D dense tensor of compatible shape (e.g., (6, 3)), **perform matrix multiplication** with each of the sparse tensors created above. 5. Implement a function `matrix_multiplication_sparse` which takes a sparse tensor A and a dense tensor B, performs the matrix multiplication, and returns the result. Ensure your function can handle both sparse and dense matrix arguments. Task 3: Handling Uncoalesced Sparse Tensors 6. Illustrate the use of uncoalesced tensors by creating a COO sparse tensor with duplicated indices and convert it to a coalesced tensor. - Create a COO tensor where indices and values are: ``` Indices: [[0, 0, 1, 1], [2, 2, 2, 2]] Values: [3, 4, 5, 6] ``` - Coalesce the tensor and show the resultant indices and values. # Constraints: - Use PyTorch\'s sparse tensor utilities for the conversions and operations. - Ensure that all tensors are initialized with a specified dtype of `torch.float32`. # Expected Functions: ```python def display_sparse_tensor_properties(sparse_tensor: torch.Tensor) -> None: # Implementation here def matrix_multiplication_sparse(sparse_tensor: torch.Tensor, dense_tensor: torch.Tensor) -> torch.Tensor: # Implementation here ``` # Example Output: ```python # Task 1 Output dense_tensor = [[0, 0, 0, 0, 1, 0], [0, 2, 0, 3, 0, 4], [0, 0, 5, 0, 0, 0], [6, 0, 0, 7, 0, 0]] # COO format properties Indices: ... Values: ... Size: (4, 6) Number of specified elements: 7 # CSR format properties Crow Indices: ... Col Indices: ... Values: ... Size: (4, 6) Number of specified elements: 7 ... # Task 2 Example matrix multiplication dense_matrix = [[1, 2, 3], [4, 5, 6], ...] result = matrix_multiplication_sparse(sparse_tensor, dense_matrix) # Task 3 Example Output uncoalesced_tensor = ... coalesced_tensor = ... # Display indices and values before and after coalescing ```","solution":"import torch def display_sparse_tensor_properties(sparse_tensor: torch.Tensor) -> None: Displays the properties of a sparse tensor including indices, values, size, and number of specified elements. indices = sparse_tensor.coalesce().indices() values = sparse_tensor.coalesce().values() size = sparse_tensor.shape num_specified_elements = sparse_tensor.coalesce()._nnz() print(f\\"Indices:n{indices}\\") print(f\\"Values:n{values}\\") print(f\\"Size: {size}\\") print(f\\"Number of specified elements: {num_specified_elements}\\") def matrix_multiplication_sparse(sparse_tensor: torch.Tensor, dense_tensor: torch.Tensor) -> torch.Tensor: Performs matrix multiplication between a sparse tensor and a dense tensor. return torch.sparse.mm(sparse_tensor, dense_tensor) # Task 1 - Creating tensors in different sparse formats dense_tensor = torch.tensor([ [0, 0, 0, 0, 1, 0], [0, 2, 0, 3, 0, 4], [0, 0, 5, 0, 0, 0], [6, 0, 0, 7, 0, 0] ], dtype=torch.float32) coo_tensor = dense_tensor.to_sparse() csr_tensor = coo_tensor.to_sparse_csr() csc_tensor = coo_tensor.to_sparse_csc() # Task 3 - Handling uncoalesced tensors uncoalesced_indices = torch.tensor([[0, 0, 1, 1], [2, 2, 2, 2]], dtype=torch.long) uncoalesced_values = torch.tensor([3, 4, 5, 6], dtype=torch.float32) uncoalesced_tensor = torch.sparse_coo_tensor(uncoalesced_indices, uncoalesced_values, (2, 3)) coalesced_tensor = uncoalesced_tensor.coalesce()"},{"question":"**Objective**: Assess the student\'s understanding and ability to apply unsupervised dimensionality reduction techniques in scikit-learn and integrate these with supervised learning models using pipelines. **Problem Statement**: You are provided with a dataset containing features with high dimensionality. Your task is to implement a pipeline that performs the following steps: 1. Feature scaling using StandardScaler. 2. Dimensionality reduction using PCA. 3. A classification step using a Support Vector Machine (SVM) classifier. **Required Steps**: 1. Load the provided dataset. 2. Implement a scikit-learn pipeline that includes StandardScaler, PCA, and SVM classifier. 3. Fit the pipeline on the training data. 4. Evaluate the model performance on the test data using accuracy as the metric. **Dataset**: Assume that the dataset is provided in a CSV file with the following format: - The last column, `target`, contains the class labels. - All other columns contain the feature values. **Input**: - Path to the CSV file containing the dataset. **Output**: - Accuracy of the model on the test set (as a float). **Constraints**: - Use `train_test_split` from `sklearn.model_selection` to split the data into training and testing sets. - The PCA should reduce the feature dimensions to 10 components. **Example**: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score def load_data(filepath): # Load the dataset data = pd.read_csv(filepath) X = data.drop(columns=[\'target\']) y = data[\'target\'] return train_test_split(X, y, test_size=0.2, random_state=42) def create_pipeline(): # Create a pipeline with StandardScaler, PCA, and SVM pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=10)), (\'svm\', SVC(kernel=\'linear\')) ]) return pipeline def main(filepath): # Load data X_train, X_test, y_train, y_test = load_data(filepath) # Create and train pipeline pipeline = create_pipeline() pipeline.fit(X_train, y_train) # Predict and evaluate model y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Model accuracy: {accuracy:.2f}\\") # Example usage: # main(\'path_to_dataset.csv\') ``` Ensure your solution meets the requirements outlined above.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score def load_data(filepath): Load the dataset from the provided filepath Args: filepath (str): Path to the CSV file containing the dataset. Returns: tuple: Split data into training and testing sets (X_train, X_test, y_train, y_test). data = pd.read_csv(filepath) X = data.drop(columns=[\'target\']) y = data[\'target\'] return train_test_split(X, y, test_size=0.2, random_state=42) def create_pipeline(): Create a scikit-learn pipeline that includes StandardScaler, PCA, and SVM. Returns: Pipeline: A scikit-learn pipeline object. pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=10)), (\'svm\', SVC(kernel=\'linear\')) ]) return pipeline def main(filepath): Main function to load data, create and train pipeline, and evaluate model. Args: filepath (str): Path to the CSV file containing the dataset. # Load data X_train, X_test, y_train, y_test = load_data(filepath) # Create and train pipeline pipeline = create_pipeline() pipeline.fit(X_train, y_train) # Predict and evaluate model y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Model accuracy: {accuracy:.2f}\\")"},{"question":"**Objective**: Demonstrate your understanding of data manipulation using Pandas and advanced plotting using Seaborn. Problem Statement You are provided with a dataset `brain_networks` that includes timepoint data for different brain networks across hemispheres. Your task is to clean and manipulate this dataset and create a plot that visually showcases the trajectories of specific network activities over time. You must follow the instructions and constraints precisely to achieve the desired outcome. Instructions 1. **Dataset Loading and Manipulation**: - Load the dataset `brain_networks` using `seaborn.load_dataset`. The dataset has multiple headers and an index column. - Rename the axes to \\"timepoint\\". - Stack the dataset on the multiple headers and compute the mean for each group of [\\"timepoint\\", \\"network\\", \\"hemi\\"]. - Unstack the dataset on the \\"network\\" column and reset the index. - Filter the dataset to only include timepoints less than 100. 2. **Plot Creation**: - Create a plot scaffold using `so.Plot` with the manipulated dataset. - Use the `pair` method to specify the variables: - x-axis: \\"5\\", \\"8\\", \\"12\\", \\"15\\". - y-axis: \\"6\\", \\"13\\", \\"16\\". - Configure the plot layout to have a size of (8, 5) and share axes among subplots. - Add `Paths` to the plot with the following properties: - Default path. - Path with `linewidth=1` and `alpha=0.8`, colored by the \\"hemi\\" column. Input - There are no direct inputs; you will load the dataset within the code. Output - A plot showcasing the trajectories through a variable space based on the specified configurations. Constraints - Use the Seaborn and Pandas libraries for this task. - Ensure your code is efficient and adheres to the highest coding standards. Example ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Load the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths()) p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") p.show() ``` Ensure your solution matches the example in terms of functionality and output. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def load_and_prepare_data(): Load and manipulate the brain_networks dataset to prepare it for plotting. Returns the manipulated dataset. # Load the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) return networks def create_plot(networks): Create a plot showcasing the trajectories of specific network activities over time. Inputs: - networks: Pandas DataFrame, manipulated dataset. Returns a Seaborn plot object. # Create the plot p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths()) p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") return p # Main function to prepare dataset and create plot if __name__ == \\"__main__\\": networks_data = load_and_prepare_data() plot = create_plot(networks_data) plot.show()"},{"question":"**Python Coding Assessment: Implementing and Tracing a Custom Function** **Objective:** Your task is to write a Python script that simulates a small library management system and then create a SystemTap script to trace specific function calls and gather statistics about the function usage. **Part 1: Python Script Implementation** Implement a Python script named `library_mgmt.py` that includes the following functions: 1. `add_book(title: str, author: str) -> None`: Adds a book to the library\'s collection. 2. `borrow_book(title: str) -> bool`: Marks a book as borrowed if it is available. 3. `return_book(title: str) -> None`: Marks a borrowed book as returned. 4. `list_books() -> None`: Lists all books in the library with their status (borrowed or available). *Constraints:* - Keep the implementation simple and use a dictionary to store book information. - Each function should have print statements to indicate entry and exit points with function names and relevant information. **Part 2: SystemTap Script** Write a SystemTap script named `trace_library.stp` that: 1. Traces the entry and return of each function defined in `library_mgmt.py`. 2. Prints the call hierarchy along with timestamps when each function is called and returned. 3. Collects data on function usage and prints a summary of how many times each function was called. **Input Format:** - No input required. **Output Format:** - The Python script should print logs indicating function calls and exits. - The SystemTap script should produce logs with timestamps for function entries and exits and a summary report. **Example:** Example Python script (`library_mgmt.py`): ```python library = {} def add_book(title: str, author: str) -> None: print(f\'Entering add_book with title={title}, author={author}\') library[title] = {\'author\': author, \'status\': \'available\'} print(f\'Exiting add_book\') def borrow_book(title: str) -> bool: print(f\'Entering borrow_book with title={title}\') if title in library and library[title][\'status\'] == \'available\': library[title][\'status\'] = \'borrowed\' print(f\'Exiting borrow_book with result=True\') return True print(f\'Exiting borrow_book with result=False\') return False def return_book(title: str) -> None: print(f\'Entering return_book with title={title}\') if title in library and library[title][\'status\'] == \'borrowed\': library[title][\'status\'] = \'available\' print(f\'Exiting return_book\') def list_books() -> None: print(f\'Entering list_books\') for title, info in library.items(): print(f\'{title} by {info[\\"author\\"]} - {info[\\"status\\"]}\') print(f\'Exiting list_books\') ``` Example SystemTap script (`trace_library.stp`): ```stap global fn_calls; probe process(\\"python3\\").mark(\\"function__entry\\") { filename = user_string(arg1); funcname = user_string(arg2); lineno = arg3; printf(\\"%s => %s in %s:%dn\\", thread_indent(1), funcname, filename, lineno); fn_calls[funcname] += 1; } probe process(\\"python3\\").mark(\\"function__return\\") { filename = user_string(arg1); funcname = user_string(arg2); lineno = arg3; printf(\\"%s <= %s in %s:%dn\\", thread_indent(-1), funcname, filename, lineno); } probe timer.s(5) { printf(\\"nSummary of function calls:n\\"); foreach (funcname in fn_calls) { printf(\\"%s was called %d timesn\\", funcname, fn_calls[funcname]); } fn_calls = delete; } ``` You may invoke the tracing script as follows: ```sh sudo stap trace_library.stp -c \'python3 library_mgmt.py\' ``` The output should show the function call hierarchy and a summary of function calls after every 5 seconds.","solution":"library = {} def add_book(title: str, author: str) -> None: print(f\'Entering add_book with title=\\"{title}\\", author=\\"{author}\\"\') library[title] = {\'author\': author, \'status\': \'available\'} print(\'Exiting add_book\') def borrow_book(title: str) -> bool: print(f\'Entering borrow_book with title=\\"{title}\\"\') if title in library and library[title][\'status\'] == \'available\': library[title][\'status\'] = \'borrowed\' print(\'Exiting borrow_book with result=True\') return True print(\'Exiting borrow_book with result=False\') return False def return_book(title: str) -> None: print(f\'Entering return_book with title=\\"{title}\\"\') if title in library and library[title][\'status\'] == \'borrowed\': library[title][\'status\'] = \'available\' print(\'Exiting return_book\') def list_books() -> None: print(\'Entering list_books\') for title, info in library.items(): print(f\'{title} by {info[\\"author\\"]} - {info[\\"status\\"]}\') print(\'Exiting list_books\')"},{"question":"# Custom Interactive Python Interpreter Objective Create a custom interactive Python interpreter using the `code` module that: 1. Provides a custom banner when starting the interpreter. 2. Customized input prompts. 3. Handles errors by logging them to a file instead of printing to standard error. Requirements 1. Implement a class `CustomInteractiveConsole` that inherits from `code.InteractiveConsole`. 2. Override the `raw_input` method to: - Customize the input prompt. - Log user inputs to a file named `user_input.log`. 3. Override the `write` method to: - Log errors to a file named `error_log.txt` instead of printing them. 4. Add a method `run_interpreter` to start the interactive console with a custom banner and exit message. Input Format You do not need to handle any input as part of your solution. Your class will interact with the user\'s input interactively. Output Format You do not need to print anything directly. User inputs and errors should be logged to `user_input.log` and `error_log.txt` respectively. Constraints - The solution should handle multiple lines of code input from the user. - The solution should continue running until the user explicitly exits the interpreter. Example ```python import code class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals, filename) def raw_input(self, prompt=\\"CustomPrompt>>> \\"): user_input = input(prompt) with open(\\"user_input.log\\", \\"a\\") as f: f.write(user_input + \\"n\\") return user_input def write(self, data): with open(\\"error_log.txt\\", \\"a\\") as f: f.write(data + \\"n\\") def run_interpreter(self): self.interact(banner=\\"Welcome to the Custom Interactive Python Interpreter!\\", exitmsg=\\"Goodbye!\\") # Example usage. console = CustomInteractiveConsole() console.run_interpreter() ``` Evaluation Your implementation will be evaluated based on: - Correctness and adherence to the requirements. - Proper logging of user inputs and errors. - The ability to handle multi-line inputs and errors gracefully. - Code readability and documentation.","solution":"import code class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals, filename) def raw_input(self, prompt=\\"CustomPrompt>>> \\"): user_input = input(prompt) with open(\\"user_input.log\\", \\"a\\") as f: f.write(user_input + \\"n\\") return user_input def write(self, data): with open(\\"error_log.txt\\", \\"a\\") as f: f.write(data + \\"n\\") def run_interpreter(self): self.interact(banner=\\"Welcome to the Custom Interactive Python Interpreter!\\", exitmsg=\\"Goodbye!\\") # Example usage. # This example is for demonstration and won\'t run since it requires an interactive session. # console = CustomInteractiveConsole() # console.run_interpreter()"},{"question":"Objective Implement a client-server application using Python sockets. The application should allow the client to send messages to the server, and the server should echo the received messages back to the client. Instructions 1. **Server Implementation**: - Create a server socket that binds to a specified port on the localhost. - The server should be able to handle multiple clients simultaneously. - Use a fixed-length message protocol where the first 4 bytes of any message indicate the length of the subsequent message. - The server should keep running, accept connections, receive messages, and echo them back to the clients. 2. **Client Implementation**: - Create a client socket that connects to the server. - The client should send at least three different messages to the server and print the echoed responses. - Each message should follow the fixed-length protocol as described above. 3. **Non-blocking Mechanism (Optional)**: - Implement the server using non-blocking sockets and `select` for managing multiple clients without creating a separate thread or process for each client. Requirements - You must implement two functions: `server(port)` and `client(server_ip, port, messages)`. - `server(port)` will start the server on the given port. - `client(server_ip, port, messages)` will connect to the server at `server_ip:port` and send each message in the `messages` list, then print the server responses. - Use the `socket` library in Python. - Ensure proper error handling and cleanup of sockets. Constraints - **Messages**: Each message sent from the client to the server must be a string with a maximum size of 256 characters. - **Port Number**: Use port number between 1024 and 65535. - **Timeout**: The server should use a timeout of 60 seconds when using the `select` method. Example ```python # Example usage import threading server_thread = threading.Thread(target=server, args=(12345,)) server_thread.start() client(\'localhost\', 12345, [\'Hello, Server!\', \'How are you?\', \'Goodbye!\']) ``` In this example: - The server is started in a separate thread. - The client connects to the server and sends three messages. - The messages should be echoed back and printed by the client. Performance Requirement - The server should be able to handle up to 20 simultaneous client connections. - The client should receive responses within 1 second for each message sent.","solution":"import socket import struct import threading import select def server(port): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'localhost\', port)) server_socket.listen(5) server_socket.setblocking(0) inputs = [server_socket] outputs = [] while True: readable, writable, exceptional = select.select(inputs, outputs, inputs, 60) if not (readable or writable or exceptional): continue for s in readable: if s is server_socket: client_socket, client_addr = s.accept() client_socket.setblocking(0) inputs.append(client_socket) else: data_len_buf = s.recv(4) if data_len_buf: data_len = struct.unpack(\'!I\', data_len_buf)[0] data = s.recv(data_len).decode() if data: s.sendall(data_len_buf + data.encode()) else: inputs.remove(s) s.close() def client(server_ip, port, messages): client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect((server_ip, port)) responses = [] for msg in messages: data = msg.encode() length_prefix = struct.pack(\'!I\', len(data)) client_socket.sendall(length_prefix + data) data_len_buf = client_socket.recv(4) if data_len_buf: data_len = struct.unpack(\'!I\', data_len_buf)[0] data = client_socket.recv(data_len).decode() responses.append(data) for response in responses: print(response) client_socket.close()"},{"question":"# Memory Usage Analysis with tracemalloc Using the `tracemalloc` module, you are to implement a Python function `analyze_memory_usage` that starts tracking memory allocations, executes a provided function, takes a memory snapshot before and after the function execution, and then compares the two snapshots to provide a sorted list of the top 10 lines responsible for the most memory allocations. Input - `func`: A function (no parameters) whose memory usage you want to analyze. Output - A list of tuples where each tuple contains: - `filename`: The name of the file where memory allocation took place. - `lineno`: The line number in the file where memory allocation took place. - `size_diff`: The difference in size of memory blocks (in KiB) allocated at this line between the two snapshots. - `count_diff`: The difference in the number of memory blocks allocated at this line between the two snapshots. Constraints - Your function should be able to handle functions that allocate a significant amount of memory. - Ensure that the memory statistics are sorted in descending order based on `size_diff`, and in case of ties, use `count_diff`. Example Here is an example usage and expected output format: ```python import tempfile def sample_function(): temp_list = [i for i in range(10000)] with tempfile.TemporaryFile() as fp: fp.write(b\'Hello World!\' * 1000) def analyze_memory_usage(func): # Your implementation here output = analyze_memory_usage(sample_function) for item in output: print(f\\"File: {item[0]}, Line: {item[1]}, Size diff: {item[2]:.2f} KiB, Count diff: {item[3]}\\") ``` Example Output: ``` File: <ipython-input-1-449a12345678>, Line: 6, Size diff: 78.12 KiB, Count diff: 3 File: /usr/lib/python3.8/tempfile.py, Line: 997, Size diff: 10.25 KiB, Count diff: 1 ``` Write the function `analyze_memory_usage` to accomplish the specified task.","solution":"import tracemalloc def analyze_memory_usage(func): Analyzes memory usage of a given function. Parameters: func (function): The function whose memory usage is to be analyzed. Returns: list: Sorted list of tuples containing file name, line number, size difference, and count difference. tracemalloc.start() snapshot1 = tracemalloc.take_snapshot() func() snapshot2 = tracemalloc.take_snapshot() stats = snapshot2.compare_to(snapshot1, \'lineno\') result = [] for stat in stats[:10]: result.append((stat.traceback[0].filename, stat.traceback[0].lineno, stat.size_diff / 1024, stat.count_diff)) tracemalloc.stop() return result"},{"question":"# Question: Advanced Visualization with Seaborn and Dodge Transform Objective: Create a combined plot using `seaborn.objects` module that demonstrates the use of the `Dodge` transform to handle overlapping marks effectively. Description: Given the `tips` dataset from seaborn, your task is to create a plot with the following specifications: 1. The plot should show the total bill amounts (`total_bill`) for each day of the week (`day`). 2. Use `Bar` marks to display the aggregated sum of total bills. 3. Color the bars based on the `time` (Lunch/Dinner). 4. Use the `Dodge` transform to ensure that bars do not overlap and maintain a consistent width. 5. Add a bit of spacing (gap) between the dodged bars. 6. Overlay the plot with `Dot` marks representing individual total bill amounts. 7. Use the `Jitter` transform on the dots to avoid overlapping dots. Input: - None (the dataset `tips` will be loaded internally) Output: - A seaborn plot object that meets the above specifications. Constraints: - You must use the `seaborn.objects` module. - The output should be visually clear with no overlapping elements. Example of expected command/result: ```python # Function implementation def create_seaborn_plot(): # Your code here pass # Invocation create_seaborn_plot() ``` This function should display the required plot when executed. Notes: - Use appropriate seaborn methods and parameters as described in the provided documentation. - Focus on creating a clear and well-structured plot.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_seaborn_plot(): # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the plot using seaborn.objects p = ( so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"time\\") .add(so.Bar(), so.Dodge()) .add(so.Dot(), so.Dodge(), so.Jitter()) ) # Display the plot p.show() # Explicitly define the function call to show the plot (for interactive environments) create_seaborn_plot()"},{"question":"# Pandas Coding Assessment Question Objective: You are to implement a function using `pandas` and create a series of test cases to ensure its correctness, making use of `pandas.testing` and handling possible exceptions. Problem: Implement a function `process_dataframe` that meets the following requirements: 1. **Input**: - A `pandas.DataFrame` containing at least the following columns: `[\'A\', \'B\', \'C\']`. 2. **Output**: - A new `pandas.DataFrame` that: - Contains only rows where the values in column \'A\' are greater than 0. - The values in column \'B\' are incremented by 10. - A new column \'D\' which is the sum of columns \'B\' and \'C\'. - An index reset to default integer index. 3. **Constraints**: - If the input `DataFrame` is missing any of the required columns, raise a `pandas.errors.InvalidColumnName` exception. - Ensure that the function can handle DataFrames with different data types and large sizes efficiently. 4. **Performance Requirements**: - The function should be able to process a `DataFrame` with up to 10^6 rows in under 1 second. 5. **Testing**: - Create unit tests using `pandas.testing` to verify the correctness of your function. - Write tests to handle exceptions and edge cases (e.g., missing columns). Function Signature: ```python import pandas as pd from pandas.errors import InvalidColumnName def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: pass # Unit Test Function def test_process_dataframe(): pass # Add other necessary imports and helper functions as required. ``` Example: ```python data = { \'A\': [1, -1, 2, 3], \'B\': [10, 20, 30, 40], \'C\': [5, 6, 7, 8] } df = pd.DataFrame(data) expected_result = { \'A\': [1, 2, 3], \'B\': [20, 40, 50], \'C\': [5, 7, 8], \'D\': [25, 47, 58] } expected_df = pd.DataFrame(expected_result).reset_index(drop=True) result_df = process_dataframe(df) pd.testing.assert_frame_equal(result_df, expected_df) ``` Make sure your implementation and tests cover various scenarios, including happy paths and edge cases. Submission: Provide the code for `process_dataframe` and `test_process_dataframe`.","solution":"import pandas as pd from pandas.errors import InvalidColumnName def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: required_columns = [\'A\', \'B\', \'C\'] if not all(col in df.columns for col in required_columns): raise InvalidColumnName(f\\"One or more required columns are missing: {required_columns}\\") # Filter rows where column \'A\' values are greater than 0 df_filtered = df[df[\'A\'] > 0].copy() # Increment values in column \'B\' by 10 df_filtered[\'B\'] = df_filtered[\'B\'] + 10 # Create new column \'D\' which is the sum of columns \'B\' and \'C\' df_filtered[\'D\'] = df_filtered[\'B\'] + df_filtered[\'C\'] # Reset index df_result = df_filtered.reset_index(drop=True) return df_result"},{"question":"# Custom Object Implementation in Python Python allows the creation of custom types using its class-based system. This task will require students to implement a custom class that imitates some of the behaviors of built-in types and showcases an understanding of core Python features. Task Implement a class `CyclicList` that mimics a list but has the following distinct behaviors: 1. **Initialization**: - The class should be initialized with an iterable (list) of items. - Example: `CyclicList([1, 2, 3, 4])` 2. **Indexing and Slicing**: - When accessing an index, it should cyclically wrap around if out of bounds. - Slicing should return another `CyclicList` with wrapped indexing. - Example: ```python cl = CyclicList([1, 2, 3, 4]) cl[5] == 2 # True, because 5 % 4 = 1 cl[-1] == 4 # True cl[1:6] == CyclicList([2, 3, 4, 1, 2]) # True ``` 3. **Representation**: - The string representation of the object should be in the form of `\\"CyclicList([<items>])\\"`, similar to how lists are represented. - Example: ```python str(CyclicList([1, 2, 3])) == \'CyclicList([1, 2, 3])\' # True ``` 4. **Mutable Sequence Methods**: - Implement append, extend, and pop methods. - Example: ```python cl = CyclicList([1, 2]) cl.append(3) # CyclicList([1, 2, 3]) cl.extend([4, 5]) # CyclicList([1, 2, 3, 4, 5]) cl.pop() == 5 # True; CyclicList([1, 2, 3, 4]) ``` Constraints - Your implementation should not use any external libraries. - Ensure that the class efficiently handles wrapping by leveraging indexing and modular arithmetic. - The class should be robust and handle edge cases, like empty initial lists, without errors. - Performance should be reasonable; operations should broadly maintain O(1) complexity for indexing, O(k) for slicing, append, extend, and pop where k is the number of operations/elements involved. Submission Please submit the `.py` file containing your `CyclicList` class definition along with a test suite demonstrating the correctness of the class functionalities for various typical and edge cases.","solution":"class CyclicList: def __init__(self, items): self.items = list(items) def __getitem__(self, index): if isinstance(index, slice): start = index.start if index.start is not None else 0 stop = index.stop if index.stop is not None else len(self.items) step = index.step if index.step is not None else 1 return CyclicList([self[i] for i in range(start, stop, step)]) return self.items[index % len(self.items)] def __repr__(self): return f\\"CyclicList({self.items})\\" def append(self, item): self.items.append(item) def extend(self, iterable): self.items.extend(iterable) def pop(self): return self.items.pop()"},{"question":"**Objective**: Your task is to create a Python script that performs file operations with logging and utilizes command-line arguments for configurations. # Requirements: - The script should accept command-line arguments to specify the following: - A source directory (`--src`) from which to read files. - A destination directory (`--dest`) where filtered files will be copied. - A log file (`--logfile`) where operations will be logged. - A file extension filter (`--ext`) to specify which types of files to copy (e.g., `.txt`, `.csv`). - The script should perform the following operations: 1. Parse and validate the command-line arguments. Ensure that the source and destination directories are valid. 2. Use the `logging` module to log the start of the operation, including the provided arguments. 3. Iterate through the source directory and copy files that match the specified extension to the destination directory. 4. Log each file copy operation including the source path, destination path, and any errors encountered. 5. Log the completion of the operation, including a count of how many files were successfully copied. # Input Format: The script will be executed with the command-line arguments as follows: ```bash python file_copier.py --src <source_directory> --dest <destination_directory> --logfile <log_file_path> --ext <file_extension> ``` # Output: - Copy the files from the source directory to the destination directory as per the specified file extension. - Create and maintain a log file that records the operations performed. # Constraints: - Ensure that the script handles I/O errors gracefully and logs any errors encountered during file operations. - Utilize the `os` module for file operations to ensure compatibility across different operating systems. - Implement logging using the `logging` module configured to write to the specified log file. # Example: Command-Line Execution: ```bash python file_copier.py --src /path/to/source --dest /path/to/destination --logfile /path/to/logfile.log --ext .txt ``` Expected Log Output: ``` INFO:root:Starting the file copy operation with arguments: Source Directory: /path/to/source Destination Directory: /path/to/destination Log File: /path/to/logfile.log File Extension: .txt INFO:root:Copying file /path/to/source/file1.txt to /path/to/destination/file1.txt ERROR:root:Failed to copy file /path/to/source/file2.txt: [Error details] INFO:root:Copying file /path/to/source/file3.txt to /path/to/destination/file3.txt INFO:root:File copy operation completed. Total files copied: 2 ``` # Notes: - You should use the `argparse` module for parsing command-line arguments. - Handle exceptions and error logging appropriately. - Ensure the code is optimized and performs efficiently even with large numbers of files.","solution":"import argparse import os import shutil import logging def parse_arguments(): parser = argparse.ArgumentParser(description=\'Copy files with specified extension from source to destination.\') parser.add_argument(\'--src\', required=True, help=\'Source directory\') parser.add_argument(\'--dest\', required=True, help=\'Destination directory\') parser.add_argument(\'--logfile\', required=True, help=\'Log file path\') parser.add_argument(\'--ext\', required=True, help=\'File extension to filter\') return parser.parse_args() def setup_logging(logfile): logging.basicConfig(filename=logfile, level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') def validate_directories(src, dest): if not os.path.isdir(src): raise ValueError(f\\"Source directory \'{src}\' does not exist or is not a directory.\\") if not os.path.exists(dest): os.makedirs(dest) def copy_files(src, dest, ext): file_count = 0 try: for root, _, files in os.walk(src): for file in files: if file.endswith(ext): src_file = os.path.join(root, file) dest_file = os.path.join(dest, file) try: shutil.copy(src_file, dest_file) logging.info(f\'Copied file {src_file} to {dest_file}\') file_count += 1 except Exception as e: logging.error(f\'Failed to copy file {src_file} to {dest_file}: {str(e)}\') except Exception as e: logging.error(f\'Error while walking source directory {src}: {str(e)}\') return file_count def main(): args = parse_arguments() setup_logging(args.logfile) logging.info(f\'Starting the file copy operation with arguments:n\' f\'Source Directory: {args.src}n\' f\'Destination Directory: {args.dest}n\' f\'Log File: {args.logfile}n\' f\'File Extension: {args.ext}\') try: validate_directories(args.src, args.dest) copied_files_count = copy_files(args.src, args.dest, args.ext) logging.info(f\'File copy operation completed. Total files copied: {copied_files_count}\') except ValueError as e: logging.error(str(e)) if __name__ == \'__main__\': main()"},{"question":"# Advanced XML Processing with ElementTree **Objective:** Implement a Python function using the `xml.etree.ElementTree` module to parse an XML input, modify specific elements based on given criteria, and output the modified XML as a string. **Function Signature:** ```python def modify_xml(xml_str: str, criteria: dict) -> str: ``` **Input:** 1. `xml_str (str)`: A string containing the XML document. 2. `criteria (dict)`: A dictionary containing the criteria for modification. The dictionary has the following structure: ```python { \\"element_tag\\": str, # The tag of the element to be modified \\"attribute\\": str, # The attribute of the element to be modified \\"old_value\\": str, # The current value of the attribute to be replaced \\"new_value\\": str # The new value to replace the old value } ``` **Output:** - A string containing the modified XML document. **Constraints:** - The XML document must be well-formed. - The element specified by `element_tag` is guaranteed to have the specified `attribute`. - If the specified `old_value` is not found within the attribute, the XML remains unchanged. **Example Usage:** ```python input_xml = \'\'\' <root> <item id=\\"1\\" status=\\"active\\">Item 1</item> <item id=\\"2\\" status=\\"inactive\\">Item 2</item> <item id=\\"3\\" status=\\"active\\">Item 3</item> </root> \'\'\' criteria = { \\"element_tag\\": \\"item\\", \\"attribute\\": \\"status\\", \\"old_value\\": \\"active\\", \\"new_value\\": \\"archived\\" } output_xml = modify_xml(input_xml, criteria) print(output_xml) ``` **Expected Output:** ```xml <root> <item id=\\"1\\" status=\\"archived\\">Item 1</item> <item id=\\"2\\" status=\\"inactive\\">Item 2</item> <item id=\\"3\\" status=\\"archived\\">Item 3</item> </root> ``` **Instructions:** 1. Parse the given XML string using the `xml.etree.ElementTree` module. 2. Navigate through the elements to find those that match the `element_tag`. 3. For each matching element, check if its `attribute` value matches `old_value`. 4. Replace `old_value` with `new_value` for matching elements. 5. Reconstruct the modified XML and return it as a string. This question assesses the student\'s ability to work with XML parsing, manipulation, and output generation, focusing on key functionalities provided by the `xml.etree.ElementTree` module.","solution":"import xml.etree.ElementTree as ET def modify_xml(xml_str: str, criteria: dict) -> str: Modify elements in XML according to a given criteria and return the modified XML as a string. Args: - xml_str (str): A string containing the XML document. - criteria (dict): A dictionary containing the criteria for modification. Returns: - str: The modified XML document as a string. # Parse the XML string into an ElementTree object root = ET.fromstring(xml_str) # Extract criteria element_tag = criteria.get(\\"element_tag\\") attribute = criteria.get(\\"attribute\\") old_value = criteria.get(\\"old_value\\") new_value = criteria.get(\\"new_value\\") # Find and modify elements based on criteria for elem in root.findall(f\\".//{element_tag}\\"): if elem.get(attribute) == old_value: elem.set(attribute, new_value) # Convert the modified ElementTree object back to a string return ET.tostring(root, encoding=\'unicode\')"},{"question":"# Memory and Runtime Environment Analyzer Objective: Create a Python program that analyzes the runtime environment and memory usage of a Python script using the `sys` module functionalities. This will test your understanding of system parameters, memory management, and handling runtime information. Task: 1. **Record Command Line Arguments**: Capture and print all command line arguments passed to the script using `sys.argv`. 2. **Analyze Memory Usage**: For each command line argument, create a dictionary to store the object size (in bytes) using `sys.getsizeof()`. Display the total memory usage for all arguments combined. 3. **System Path and Execution Info**: Print the system execution prefix and paths from `sys.prefix`, `sys.exec_prefix`, and `sys.path`. 4. **Set and Use a Custom Exception Hook**: Define a custom exception hook to handle uncaught exceptions. Ensure a custom message is displayed when an uncaught exception occurs. Constraints: - Assume all command line arguments are strings. - Your program should handle typical Python data types without raising additional exceptions. - The analysis should work effectively for Python 3.10 or newer versions. Input: The program should run via command line input as: ```sh python your_script.py arg1 arg2 arg3 ... ``` Output: - A list of command line arguments. - Memory usage analysis for each argument and total memory usage. - System paths and execution prefixes. - Custom exception handling demonstration. Example: Given a script named `analyzer.py`, if executed as: ```sh python analyzer.py \\"Hello\\" \\"World\\" \\"12345\\" ``` The expected output could be: ```sh Command Line Arguments: [\'analyzer.py\', \'Hello\', \'World\', \'12345\'] Memory Usage (in bytes): - \'analyzer.py\': 51 - \'Hello\': 54 - \'World\': 54 - \'12345\': 54 Total Memory Usage: 213 bytes System Paths and Execution Info: - sys.prefix: /usr/local - sys.exec_prefix: /usr/local - sys.path: [\'/usr/local/lib/python3.10\'] Custom Exception Hook: - Unhandled Exception: An error occurred: Custom Unhandled Exception ``` Additional Requirements: 1. **Exception Simulation**: The custom exception hook should be demonstrated by forcing an exception after the analysis (e.g., using `raise Exception(\'Custom Unhandled Exception\')`). 2. **Code Efficiency**: Ensure the solution is efficient and leverages `sys` module functionalities appropriately. Implement this in a function named `analyze_runtime_and_memory()`.","solution":"import sys def analyze_runtime_and_memory(): # Capturing command line arguments arguments = sys.argv print(f\\"Command Line Arguments: {arguments}\\") # Memory usage analysis total_memory = 0 memory_usage = {} for arg in arguments: size = sys.getsizeof(arg) memory_usage[arg] = size total_memory += size print(\\"nMemory Usage (in bytes):\\") for arg, size in memory_usage.items(): print(f\\"- \'{arg}\': {size}\\") print(f\\"Total Memory Usage: {total_memory} bytes\\") # System paths and execution info print(\\"nSystem Paths and Execution Info:\\") print(f\\"- sys.prefix: {sys.prefix}\\") print(f\\"- sys.exec_prefix: {sys.exec_prefix}\\") print(f\\"- sys.path: {sys.path}\\") # Custom exception hook def custom_exception_hook(exctype, value, traceback): print(f\\"nCustom Exception Hook:n- Unhandled Exception: An error occurred: {value}\\") sys.excepthook = custom_exception_hook # Simulate uncaught exception to demonstrate the custom exception hook raise Exception(\'Custom Unhandled Exception\') if __name__ == \\"__main__\\": analyze_runtime_and_memory()"},{"question":"# Advanced File Control and Locking Operations You are tasked with implementing a small module that facilitates advanced control and manipulation of file descriptors, utilizing the `fcntl` module. Specifically, you will implement two functions that: 1. Set a file descriptor to non-blocking mode. 2. Acquire an exclusive lock on a file and write some data to it. # Function 1: Set Non-blocking Mode The function `set_non_blocking(fd)` should set the provided file descriptor to non-blocking mode. Input: - `fd` (int or file object): The file descriptor to be set to non-blocking mode. Output: - None: The function should perform the operation and raise exceptions if necessary. # Function 2: Write with Lock The function `write_with_lock(fd, data)` should acquire an exclusive lock on the file, write the provided data to it, and ensure that the data is visible before the lock is released. Input: - `fd` (int or file object): The file descriptor to be locked. - `data` (bytes): The data to be written to the file. Output: - None: The function should perform the operation and raise exceptions if necessary. Constraints: - Ensure that you handle exceptions that may arise from file locking or writing. - The length of `data` should not exceed 1024 bytes. # Example Usage: ```python import os from fcntl_module import set_non_blocking, write_with_lock # Example file descriptor fd = os.open(\'example.txt\', os.O_RDWR | os.O_CREAT) # Set to non-blocking mode set_non_blocking(fd) # Write data with an exclusive lock write_with_lock(fd, b\\"Hello, World!\\") # Close the file descriptor os.close(fd) ``` # Notes: - If the file does not exist, you need to handle its creation. - Ensure that the lock is properly acquired and released. - Make use of the `fcntl` module\'s `fcntl`, `flock`, and `lockf` functions where appropriate. Implement these functions to demonstrate competence in handling Unix file descriptors and locks using the `fcntl` module.","solution":"import os import fcntl def set_non_blocking(fd): Sets the provided file descriptor to non-blocking mode. Args: fd (int or file object): The file descriptor to be set to non-blocking mode. Raises: OSError: If setting the file descriptor to non-blocking fails. if isinstance(fd, (int,)): fd_num = fd else: fd_num = fd.fileno() flags = fcntl.fcntl(fd_num, fcntl.F_GETFL) fcntl.fcntl(fd_num, fcntl.F_SETFL, flags | os.O_NONBLOCK) def write_with_lock(fd, data): Acquires an exclusive lock on the file, writes the provided data to it, and ensures the data is visible before the lock is released. Args: fd (int or file object): The file descriptor to be locked. data (bytes): The data to be written to the file. Raises: ValueError: If data exceeds 1024 bytes. OSError: If locking or writing fails. if len(data) > 1024: raise ValueError(\\"Data length exceeds 1024 bytes.\\") if isinstance(fd, (int,)): fd_num = fd else: fd_num = fd.fileno() try: fcntl.flock(fd_num, fcntl.LOCK_EX) os.write(fd_num, data) os.fsync(fd_num) finally: fcntl.flock(fd_num, fcntl.LOCK_UN)"},{"question":"**Objective**: Implement a function that runs a series of shell commands and processes their outputs using Python\'s `subprocess` module. # Problem Statement You are given a list of commands to be executed sequentially. Each command\'s output feeds into the next command\'s input (i.e., they form a pipeline). For the final command, capture its output and return it. Your task is to implement a function `run_pipeline(commands: List[str]) -> str` that: 1. Takes a list of shell command strings as input. 2. Executes the commands in sequence, using the output of each command as the input for the next. 3. Captures and returns the output of the final command as a string. # Requirements: - Use the `Popen` object to create subprocesses. - Use pipes (`PIPE` from the `subprocess` module) to handle the input/output between commands. - Ensure that exceptions are handled, and appropriate error messages are returned if a command fails. - The function should support text-based input and output (use the `text=True` parameter of `Popen`). # Function Signature ```python from typing import List def run_pipeline(commands: List[str]) -> str: pass ``` # Input: - `commands`: A list of commands to be executed. Each command is represented as a string. Example: `[\\"echo Hello\\", \\"tr \'a-z\' \'A-Z\'\\", \\"rev\\"]` # Output: - Return the output of the final command as a string. If command execution fails at any point, return an error message indicating which command failed. # Constraints: - You can assume that the list of commands is non-empty. - The commands are valid shell commands, and you don\'t need to handle invalid shell commands in this question. # Example: ```python commands = [\\"echo Hello\\", \\"tr \'a-z\' \'A-Z\'\\", \\"rev\\"] print(run_pipeline(commands)) # Output: \\"OLLEH\\" ``` # Note: - The `echo Hello` command outputs \\"Hello\\", which is then fed into `tr \'a-z\' \'A-Z\'` to convert it to \\"HELLO\\". Finally, it is reversed using the `rev` command to get \\"OLLEH\\". - You need to ensure that the pipeline setup handles the data flow correctly between the commands. # Hints: - You can initialize the first process with no input pipe and set it to read from its standard output and write to a pipe. - Subsequent processes in the pipeline should read from the previous process\'s output and write to their output pipe. - Use `communicate()` method to handle I/O operations safely and avoid deadlocks.","solution":"from typing import List import subprocess def run_pipeline(commands: List[str]) -> str: Executes a list of shell commands sequentially where each command\'s output acts as an input for the next command in the sequence. Args: commands (List[str]): List of commands to be executed. Returns: str: The output of the final command or an error message if a command fails. if not commands: return \\"No commands provided\\" # Initialize the pipeline previous_process = None for i, command in enumerate(commands): try: if previous_process is None: # First command current_process = subprocess.Popen( command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True ) else: # Subsequent commands current_process = subprocess.Popen( command, shell=True, stdin=previous_process.stdout, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True ) # Close the previous stdout to allow the previous process to receive a SIGPIPE if the current process exits previous_process.stdout.close() previous_process = current_process except subprocess.SubprocessError as e: return f\\"Command \'{command}\' failed with error: {str(e)}\\" # Get the final output output, error = previous_process.communicate() if previous_process.returncode != 0: return f\\"Command \'{commands[-1]}\' failed with error: {error.strip()}\\" return output.strip()"},{"question":"You are given a dataset containing information about diamond prices (`diamonds`). Using seaborn\'s `objects` interface, your task is to visualize and analyze the dataset by following the steps below: 1. **Load the dataset**: Use Seaborn\'s `load_dataset` function to load the `diamonds` dataset. 2. **Calculate and Visualize Mean Carat Weight**: Create a bar plot to display the mean carat weight of diamonds for each clarity category. 3. **Customize Aggregation Function**: Modify the bar plot to show the interquartile range of the carat weights for each clarity category instead of the mean. 4. **Add a Color Dimension**: Introduce a color dimension to your plot that represents different diamond cuts. 5. **Describe your Findings**: Write a brief summary (3-4 sentences) of what your plot reveals about the distribution of carat weights across different clarity categories and cuts. **Constraints/Requirements:** - You must use seaborn\'s object-oriented API (`so.Plot`, `so.Bar`, `so.Agg`, etc.) for plotting. - Your code must be efficient and make use of seaborn\'s built-in functionalities wherever possible. - Provide appropriate labels and titles for your plots for better readability. **Expected Input and Output:** - **Input**: None (the dataset is loaded within the code). - **Output**: - A bar plot showing the mean carat weight for each clarity category. - A modified bar plot showing the interquartile range of carat weights for each clarity category. - A final bar plot including the color dimension for different diamond cuts. - A text summary describing insights from the final plot. **Sample Code Block to Load Dataset:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") ``` **Hints:** - Use `so.Agg()` to calculate the mean or other custom aggregation functions. - Customize the plot by adding colors using `color=...` parameter. - Refer to the seaborn documentation for object-oriented API usage examples if needed.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create a bar plot to display the mean carat weight of diamonds for each clarity category mean_carat_plot = ( so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") .add(so.Bar(), so.Agg()) .label(title=\\"Mean Carat Weight by Clarity\\", xlabel=\\"Clarity\\", ylabel=\\"Mean Carat Weight\\") ) # Create a bar plot to display the interquartile range of the carat weights for each clarity category iqr_carat_plot = ( so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") .add(so.Bar(), so.Agg(func=lambda x: np.percentile(x, 75) - np.percentile(x, 25))) .label(title=\\"Interquartile Range of Carat Weight by Clarity\\", xlabel=\\"Clarity\\", ylabel=\\"IQR of Carat Weight\\") ) # Create a bar plot including the color dimension for different diamond cuts colored_iqr_carat_plot = ( so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\", color=\\"cut\\") .add(so.Bar(), so.Agg(func=lambda x: np.percentile(x, 75) - np.percentile(x, 25))) .label(title=\\"Interquartile Range of Carat Weight by Clarity and Cut\\", xlabel=\\"Clarity\\", ylabel=\\"IQR of Carat Weight\\") ) # Now, we can display the plots (not feasible in this text-based interface, but included for completeness in actual use). # mean_carat_plot.show() # iqr_carat_plot.show() # colored_iqr_carat_plot.show() # Summary: summary = The final plot reveals that the interquartile range of carat weights varies significantly across different clarity categories. Additionally, the inclusion of the \'cut\' dimension shows that the variation in carat weight is also dependent on the cut of the diamonds. Clarity categories like \'SI2\' and \'VS2\' have a larger range of carat weights compared to others, and different cuts within the same clarity category exhibit varying ranges of carat weights."},{"question":"**Question: Implementing Customized Email Serialization** You are required to implement functionality that takes an `EmailMessage` object, along with certain options, and produces a specific serialized output. Your task is to create a function `custom_email_serialization` that performs these steps: 1. Initializes a `BytesGenerator`, `Generator`, or `DecodedGenerator` based on given options. 2. Flattens the `EmailMessage` object into a specified format. 3. Returns the serialized email message as a string or bytes, depending on the generator used. # Function Signature ```python def custom_email_serialization(msg: EmailMessage, generator_type: str, mangle_from: bool, maxheaderlen: int, policy: Optional[Policy]) -> Union[bytes, str]: pass ``` # Parameters - `msg`: An instance of `EmailMessage` that you need to serialize. - `generator_type`: A string specifying the type of generator to use. It could be `\'bytes\'`, `\'text\'`, or `\'decoded\'`. - `\'bytes\'`: Use `BytesGenerator` - `\'text\'`: Use `Generator` - `\'decoded\'`: Use `DecodedGenerator` - `mangle_from`: A boolean indicating whether to mangle lines starting with \\"From \\". - `maxheaderlen`: An integer specifying the maximum header length. Pass `None` to use the default behavior. - `policy`: An instance of `Policy` to control message generation. Pass `None` to use the default policy. # Return - The function should return the serialized email message as `bytes` if using `BytesGenerator`, or as a `str` if using `Generator` or `DecodedGenerator`. # Constraints - Ensure that the email message serialization complies with the specified options. - Handle any encoding and necessary transformations as per the selected generator. # Example Usage ```python from email.message import EmailMessage from email.policy import default # Example Email Message msg = EmailMessage() msg.set_content(\\"Hi there!nThis is a test email.\\") msg[\'Subject\'] = \'Test Email\' msg[\'From\'] = \'test@example.com\' msg[\'To\'] = \'recipient@example.com\' # Function call serialized_email = custom_email_serialization(msg, \'text\', True, 78, default) print(serialized_email) ``` # Performance Requirements - The function must efficiently handle the processing of reasonably large email message objects. Implement `custom_email_serialization` based on the provided guidelines.","solution":"from email import policy from email.generator import BytesGenerator, Generator, DecodedGenerator from email.message import EmailMessage from typing import Union, Optional def custom_email_serialization(msg: EmailMessage, generator_type: str, mangle_from: bool, maxheaderlen: int, policy_opt: Optional[policy.Policy]) -> Union[bytes, str]: Serializes the EmailMessage object based on the specified generator type and options. import io buffer = io.BytesIO() if generator_type == \'bytes\' else io.StringIO() if generator_type == \'bytes\': gen = BytesGenerator(buffer, mangle_from_=mangle_from, maxheaderlen=maxheaderlen, policy=policy_opt) elif generator_type == \'text\': gen = Generator(buffer, mangle_from_=mangle_from, maxheaderlen=maxheaderlen, policy=policy_opt) elif generator_type == \'decoded\': gen = DecodedGenerator(buffer, mangle_from_=mangle_from, maxheaderlen=maxheaderlen, policy=policy_opt) else: raise ValueError(f\\"Unknown generator_type: {generator_type}\\") gen.flatten(msg) if generator_type == \'bytes\': return buffer.getvalue() else: return buffer.getvalue()"},{"question":"**Objective:** Implement a function that takes a 3-dimensional tensor and modifies its dimensions based on specific rules, demonstrating your comprehension of PyTorch tensor manipulation and the `torch.Size` class. **Problem Statement:** Write a function `modify_tensor_dimensions(tensor: torch.Tensor) -> torch.Tensor` that takes a 3-dimensional PyTorch tensor and performs the following operations: 1. Retrieve the tensor\'s size using `torch.Tensor.size()`. 2. Double the size of the second dimension. 3. Halve the size of the third dimension, rounding down if necessary. 4. Return a new tensor with the modified dimensions, with elements initialized to ones. **Input:** - `tensor` (torch.Tensor): A 3-dimensional PyTorch tensor. **Output:** - `torch.Tensor`: A new 3-dimensional tensor with modified dimensions based on the rules provided. **Constraints:** - The input tensor is guaranteed to be 3-dimensional. - The operations on dimensions will result in valid tensor sizes. **Example:** ```python import torch # Example input tensor x = torch.ones(10, 20, 30) # Expected size after modifications # Original size: (10, 20, 30) # Modified size: (10, 40, 15) result = modify_tensor_dimensions(x) print(result.size()) # Output should be torch.Size([10, 40, 15]) ``` **Notes:** - You should not modify the input tensor directly. Instead, create a new tensor with the specified dimensions and return it. - Use `torch.ones` to initialize the new tensor. **Performance Requirements:** - The function should efficiently handle tensors of standard size used in practice. ```python def modify_tensor_dimensions(tensor: torch.Tensor) -> torch.Tensor: # Retrieve the size of the original tensor. original_size = tensor.size() # Double the size of the second dimension. new_dim2 = original_size[1] * 2 # Halve the size of the third dimension, rounding down if necessary. new_dim3 = original_size[2] // 2 # Construct the new size. new_size = torch.Size([original_size[0], new_dim2, new_dim3]) # Create and return a new tensor with the modified dimensions initialized to ones. new_tensor = torch.ones(new_size) return new_tensor ```","solution":"import torch def modify_tensor_dimensions(tensor: torch.Tensor) -> torch.Tensor: Modify the dimensions of a 3-dimensional tensor according to the specified rules: 1. Double the size of the second dimension. 2. Halve the size of the third dimension, rounding down if necessary. Returns a new tensor with the modified dimensions initialized to ones. Parameters: tensor (torch.Tensor): A 3-dimensional PyTorch tensor. Returns: torch.Tensor: A new 3-dimensional tensor with modified dimensions. # Retrieve the size of the original tensor. original_size = tensor.size() # Double the size of the second dimension. new_dim2 = original_size[1] * 2 # Halve the size of the third dimension, rounding down if necessary. new_dim3 = original_size[2] // 2 # Construct the new size. new_size = torch.Size([original_size[0], new_dim2, new_dim3]) # Create and return a new tensor with the modified dimensions initialized to ones. new_tensor = torch.ones(new_size) return new_tensor"},{"question":"**Objective:** Implement a function that sorts a list of student records based on multiple criteria and returns the sorted list. This will test your understanding of custom sorting with key functions, utilizing the `operator` module, and multi-level sorting in Python. **Function Signature:** ```python def sort_students(student_records: list[dict], sort_specs: list[tuple[str, bool]]) -> list[dict]: Sorts a list of student records based on multiple criteria. Parameters: - student_records (list of dict): List of dictionaries where each dictionary represents a student with \'name\', \'grade\', and \'age\' keys. - sort_specs (list of tuple): List of tuples where each tuple contains a key (str) to sort on and a boolean indicating if the sort should be in descending order. Returns: - list of dict: Sorted list of student records. pass ``` **Input:** - A list of dictionaries `student_records` where each dictionary has the following keys: - `\'name\'` (str): the student\'s name. - `\'grade\'` (str): the student\'s grade. - `\'age\'` (int): the student\'s age. - A list of tuples `sort_specs` where each tuple contains two elements: - A string representing the key to sort on (`\'name\'`, `\'grade\'`, or `\'age\'`). - A boolean indicating if the sort should be in descending order (`True` for descending, `False` for ascending). **Output:** - The function should return a list of dictionaries representing the sorted student records according to the specified sorting criteria. **Constraints:** - The `sort_specs` list will contain at least one sort specification. - The keys within the dictionaries will always be present and valid. **Custom Test Case:** ```python students = [ {\\"name\\": \\"john\\", \\"grade\\": \\"A\\", \\"age\\": 15}, {\\"name\\": \\"jane\\", \\"grade\\": \\"B\\", \\"age\\": 12}, {\\"name\\": \\"dave\\", \\"grade\\": \\"B\\", \\"age\\": 10}, {\\"name\\": \\"emily\\", \\"grade\\": \\"A\\", \\"age\\": 16}, {\\"name\\": \\"carol\\", \\"grade\\": \\"C\\", \\"age\\": 12} ] sort_order = [(\\"grade\\", False), (\\"age\\", True)] sorted_students = sort_students(students, sort_order) print(sorted_students) ``` **Expected Output:** ```python [ {\\"name\\": \\"emily\\", \\"grade\\": \\"A\\", \\"age\\": 16}, {\\"name\\": \\"john\\", \\"grade\\": \\"A\\", \\"age\\": 15}, {\\"name\\": \\"jane\\", \\"grade\\": \\"B\\", \\"age\\": 12}, {\\"name\\": \\"dave\\", \\"grade\\": \\"B\\", \\"age\\": 10}, {\\"name\\": \\"carol\\", \\"grade\\": \\"C\\", \\"age\\": 12} ] ``` **Additional Notes:** - You are encouraged to use the `operator` module for efficiency and readability. - Remember the concept of sort stability when implementing your solution. - You may implement helper functions as needed.","solution":"from operator import itemgetter def sort_students(student_records, sort_specs): Sorts a list of student records based on multiple criteria. Parameters: - student_records (list of dict): List of dictionaries where each dictionary represents a student with \'name\', \'grade\', and \'age\' keys. - sort_specs (list of tuple): List of tuples where each tuple contains a key (str) to sort on and a boolean indicating if the sort should be in descending order. Returns: - list of dict: Sorted list of student records. for key, reverse in reversed(sort_specs): student_records.sort(key=itemgetter(key), reverse=reverse) return student_records"},{"question":"You are to implement a custom iterator class in Python by mimicking some behaviors described for C functions in the documentation. Your task is to create a class `CustomIterator` which: 1. Wraps around a list of values. 2. Implements the iterator protocols (`__iter__` and `__next__`). 3. Handles a special case where when encountering the value `\'error\'`, it raises a `ValueError`. 4. Handles a special case where `\'stop\'` pauses the iteration, and resumes when `\'resume\'` is called. 5. Keeps track of its state whether it is paused or not. Implement the following methods: - `__iter__(self)`: Returns the iterator object itself. - `__next__(self)`: Retrieves the next item of the iterator or raises StopIteration. - `pause(self)`: Sets the iterator to a paused state. - `resume(self)`: Resumes the iteration if it is paused. Input: - The list of values wrapped by the iterator. - Calls to pause and resume methods. Output: - Iterated values of the list. - Raises `ValueError` for \'error\'. - Stops iteration after `\'stop\'` is encountered and only resumes on calling `\'resume\'`. Example: ```python values = [1, 2, \'error\', 3, \'stop\', 4, 5, \'resume\', 6] iter_obj = CustomIterator(values) try: for item in iter_obj: print(item) except ValueError: print(\\"Encountered \'error\'\\") iter_obj.resume() for item in iter_obj: print(item) ``` Expected Output: ``` 1 2 Encountered \'error\' stop 4 5 resume 6 ``` Constraints: - You may assume the list of values will have no further nested iterables. - Performance should handle lists up to 10,000 items efficiently. Hint: Use appropriate state management and exception handling to mimic the C-like behavior in Python.","solution":"class CustomIterator: def __init__(self, values): self.values = values self.index = 0 self.paused = False def __iter__(self): return self def __next__(self): if self.paused: raise StopIteration while self.index < len(self.values): value = self.values[self.index] self.index += 1 if value == \'stop\': self.paused = True raise StopIteration elif value == \'error\': raise ValueError(\\"Encountered \'error\'\\") else: return value raise StopIteration def pause(self): self.paused = True def resume(self): self.paused = False"},{"question":"Coding Assessment Question # Objective You are required to write a function that takes an input ZIP file, processes its contents, and generates a report with specific details about each file in the archive. # Problem Statement Write a function `process_zip_file(file_path: str) -> List[Dict[str, Union[str, int]]]` that performs the following operations: 1. Opens the ZIP file specified by `file_path`. 2. Reads information about each member file in the archive. 3. Extracts and generates a report containing the following details for each file in the archive: - `filename`: The name of the file. - `size`: The uncompressed size of the file in bytes. - `compressed_size`: The compressed size of the file in bytes. - `compress_type`: A string indicating the type of compression used (e.g., \\"stored\\", \\"deflated\\", \\"bzip2\\", \\"lzma\\"). - `date_time`: A string representing the last modification time in the format `YYYY-MM-DD HH:MM:SS`. # Function Signature ```python from typing import List, Dict, Union def process_zip_file(file_path: str) -> List[Dict[str, Union[str, int]]]: pass ``` # Constraints - You may assume the input ZIP file only contains files and no directories. - If the ZIP file is invalid, raise a `ValueError` with the message \\"Invalid ZIP file\\". - Use the appropriate zipfile methods and properties to retrieve the required information. # Example Suppose `example.zip` contains the following files: | Filename | Uncompressed Size (bytes) | Compressed Size (bytes) | Compression Type | Date & Time | |---------------|----------------------------|-------------------------|------------------|----------------------| | file1.txt | 1234 | 567 | deflated | 2021-09-01 10:20:30 | | file2.jpg | 2345 | 1234 | stored | 2021-09-02 11:25:45 | Calling the function as follows: ```python report = process_zip_file(\\"example.zip\\") ``` Should return a list of dictionaries like this: ```python [ { \\"filename\\": \\"file1.txt\\", \\"size\\": 1234, \\"compressed_size\\": 567, \\"compress_type\\": \\"deflated\\", \\"date_time\\": \\"2021-09-01 10:20:30\\" }, { \\"filename\\": \\"file2.jpg\\", \\"size\\": 2345, \\"compressed_size\\": 1234, \\"compress_type\\": \\"stored\\", \\"date_time\\": \\"2021-09-02 11:25:45\\" } ] ``` The function is expected to handle different compression types and represent them as strings (\\"stored\\", \\"deflated\\", \\"bzip2\\", \\"lzma\\"). # Additional Notes - Utilize the `zipfile` module from the Python standard library to manipulate ZIP files. - Consider using the `ZipFile`, `ZipInfo` classes and relevant methods such as `infolist()` to gather information about the ZIP archive members. # References - [Python Documentation for zipfile module](https://docs.python.org/3/library/zipfile.html)","solution":"from typing import List, Dict, Union import zipfile from datetime import datetime def process_zip_file(file_path: str) -> List[Dict[str, Union[str, int]]]: try: with zipfile.ZipFile(file_path, \'r\') as zip_ref: report = [] for file_info in zip_ref.infolist(): file_report = { \\"filename\\": file_info.filename, \\"size\\": file_info.file_size, \\"compressed_size\\": file_info.compress_size, \\"compress_type\\": get_compression_type(file_info.compress_type), \\"date_time\\": datetime(*file_info.date_time).strftime(\\"%Y-%m-%d %H:%M:%S\\") } report.append(file_report) return report except Exception: raise ValueError(\\"Invalid ZIP file\\") def get_compression_type(compress_type: int) -> str: if compress_type == zipfile.ZIP_STORED: return \\"stored\\" elif compress_type == zipfile.ZIP_DEFLATED: return \\"deflated\\" elif compress_type == zipfile.ZIP_BZIP2: return \\"bzip2\\" elif compress_type == zipfile.ZIP_LZMA: return \\"lzma\\" else: return \\"unknown\\""},{"question":"# Business Date Adjustment and Analysis using Pandas Offsets Objective: Implement a function that adjusts a given date to the next business day if it falls on a weekend or holiday, and analyze the adjusted dates to determine the frequency position within a month and quarter. Function Signature: ```python def adjust_and_analyze_dates(dates, holidays): Adjusts the input dates to the next business day if they fall on a weekend or holiday. Parameters: - dates (list of str): List of date strings in the format \'YYYY-MM-DD\'. - holidays (list of str): List of holiday date strings in the format \'YYYY-MM-DD\'. Returns: - result (list of dict): A list of dictionaries containing: - \'original_date\': Original date string. - \'adjusted_date\': Adjusted date string. - \'is_month_start\': Boolean indicating if the adjusted date is the start of the month. - \'is_month_end\': Boolean indicating if the adjusted date is the end of the month. - \'is_quarter_start\': Boolean indicating if the adjusted date is the start of the quarter. - \'is_quarter_end\': Boolean indicating if the adjusted date is the end of the quarter. pass ``` Constraints: 1. Utilize the `pandas` package and specifically the offset-related classes elucidated in the provided documentation. 2. Assume the weekdays are Monday to Friday, and holidays are provided specific to the region in the `holidays` parameter. 3. The date strings are in `YYYY-MM-DD` format. Example: ```python dates = [\\"2023-05-27\\", \\"2023-12-31\\", \\"2024-03-31\\"] holidays = [\\"2023-05-29\\", \\"2023-12-25\\"] result = adjust_and_analyze_dates(dates, holidays) print(result) ``` Expected Output: ```python [ { \'original_date\': \'2023-05-27\', \'adjusted_date\': \'2023-05-30\', # 27th is Saturday, 29th is holiday, 30th is Tuesday \'is_month_start\': False, \'is_month_end\': False, \'is_quarter_start\': False, \'is_quarter_end\': False }, { \'original_date\': \'2023-12-31\', \'adjusted_date\': \'2024-01-02\', # 31st is Sunday, 2nd is Tuesday \'is_month_start\': True, \'is_month_end\': False, \'is_quarter_start\': False, \'is_quarter_end\': False }, { \'original_date\': \'2024-03-31\', \'adjusted_date\': \'2024-03-29\', # 31st is Sunday, 29th is Friday \'is_month_start\': False, \'is_month_end\': True, \'is_quarter_start\': False, \'is_quarter_end\': True } ] ``` Notes: - Utilize the `CustomBusinessDay` offset class for handling custom business days. - Consider the `is_month_start`, `is_month_end`, `is_quarter_start`, and `is_quarter_end` methods to determine the calendar positions. - Ensure that the function is robust and handles edge cases such as consecutive holidays.","solution":"import pandas as pd from pandas.tseries.offsets import CustomBusinessDay def adjust_and_analyze_dates(dates, holidays): Adjusts the input dates to the next business day if they fall on a weekend or holiday. Parameters: - dates (list of str): List of date strings in the format \'YYYY-MM-DD\'. - holidays (list of str): List of holiday date strings in the format \'YYYY-MM-DD\'. Returns: - result (list of dict): A list of dictionaries containing: - \'original_date\': Original date string. - \'adjusted_date\': Adjusted date string. - \'is_month_start\': Boolean indicating if the adjusted date is the start of the month. - \'is_month_end\': Boolean indicating if the adjusted date is the end of the month. - \'is_quarter_start\': Boolean indicating if the adjusted date is the start of the quarter. - \'is_quarter_end\': Boolean indicating if the adjusted date is the end of the quarter. holidays = pd.to_datetime(holidays) business_day = CustomBusinessDay(holidays=holidays) result = [] for date in dates: original_dt = pd.to_datetime(date) # Adjust to next business day if needed if original_dt.weekday() >= 5 or original_dt in holidays: adjusted_dt = original_dt + business_day else: adjusted_dt = original_dt result.append({ \'original_date\': date, \'adjusted_date\': adjusted_dt.strftime(\'%Y-%m-%d\'), \'is_month_start\': adjusted_dt.is_month_start, \'is_month_end\': adjusted_dt.is_month_end, \'is_quarter_start\': adjusted_dt.is_quarter_start, \'is_quarter_end\': adjusted_dt.is_quarter_end }) return result"},{"question":"**Question: Isotonic Regression Implementation and Evaluation** You are tasked with implementing and evaluating the `IsotonicRegression` model using the `scikit-learn` library. Your task has several components: 1. **Data Preparation**: - Generate a dataset `X` of 100 data points uniformly spaced between 0 and 1. - Generate the corresponding `y` values using the equation `y = 3x + noise`, where `noise` is a random value drawn from a normal distribution with mean 0 and standard deviation 0.1. 2. **Model Implementation**: - Fit an `IsotonicRegression` model to the dataset using both an increasing and a decreasing constraint. - Predict the `y` values for 100 new data points uniformly spaced between 0 and 1. 3. **Evaluation**: - Compute the mean squared error (MSE) between the predicted and true `y` values for both constraints. - Plot the true values, and the predictions from both increasing and decreasing models on the same graph. **Input and Output Formats:** - **Input**: - There are no inputs for this task. You must generate the data as described. - **Output**: - Two floating-point numbers, representing the MSE of the increasing constraint model and the MSE of the decreasing constraint model. - A plot displaying true values and predicted values from both models. **Constraints and requirements**: - The dataset should be reproducible; set a random seed before generating noise. - Use the `matplotlib` library for plotting the results. - Your code should be well-documented and modular, making use of functions where appropriate. # Implementation ```python import numpy as np import matplotlib.pyplot as plt from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error def generate_data(): np.random.seed(42) X = np.linspace(0, 1, 100) noise = np.random.normal(0, 0.1, 100) y = 3 * X + noise return X, y def fit_and_predict(X, y, new_X, increasing=True): model = IsotonicRegression(increasing=increasing) model.fit(X, y) return model.predict(new_X) def evaluate_and_plot(): X, y = generate_data() new_X = np.linspace(0, 1, 100) y_pred_increasing = fit_and_predict(X, y, new_X, increasing=True) y_pred_decreasing = fit_and_predict(X, y, new_X, increasing=False) mse_increasing = mean_squared_error(y, y_pred_increasing) mse_decreasing = mean_squared_error(y, y_pred_decreasing) plt.figure(figsize=(10, 6)) plt.plot(X, y, \'o\', label=\'True values\') plt.plot(new_X, y_pred_increasing, label=\'Predictions (increasing)\') plt.plot(new_X, y_pred_decreasing, label=\'Predictions (decreasing)\') plt.legend() plt.xlabel(\'X\') plt.ylabel(\'y\') plt.title(\'Isotonic Regression Predictions\') plt.show() return mse_increasing, mse_decreasing # Output the MSE for both models mse_increasing, mse_decreasing = evaluate_and_plot() print(f\'MSE Increasing: {mse_increasing}\') print(f\'MSE Decreasing: {mse_decreasing}\') ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.isotonic import IsotonicRegression from sklearn.metrics import mean_squared_error def generate_data(): np.random.seed(42) X = np.linspace(0, 1, 100) noise = np.random.normal(0, 0.1, 100) y = 3 * X + noise return X, y def fit_and_predict(X, y, new_X, increasing=True): model = IsotonicRegression(increasing=increasing) model.fit(X, y) return model.predict(new_X) def evaluate_and_plot(): X, y = generate_data() new_X = np.linspace(0, 1, 100) y_pred_increasing = fit_and_predict(X, y, new_X, increasing=True) y_pred_decreasing = fit_and_predict(X, y, new_X, increasing=False) mse_increasing = mean_squared_error(y, y_pred_increasing) mse_decreasing = mean_squared_error(y, y_pred_decreasing) plt.figure(figsize=(10, 6)) plt.plot(X, y, \'o\', label=\'True values\') plt.plot(new_X, y_pred_increasing, label=\'Predictions (increasing)\') plt.plot(new_X, y_pred_decreasing, label=\'Predictions (decreasing)\') plt.legend() plt.xlabel(\'X\') plt.ylabel(\'y\') plt.title(\'Isotonic Regression Predictions\') plt.show() return mse_increasing, mse_decreasing # Output the MSE for both models mse_increasing, mse_decreasing = evaluate_and_plot() print(f\'MSE Increasing: {mse_increasing}\') print(f\'MSE Decreasing: {mse_decreasing}\')"},{"question":"# Question: Implementing and Using Custom DDP Communication Hook in PyTorch Objective: Demonstrate your understanding of PyTorchs `DistributedDataParallel` (DDP) and communication hooks by implementing a custom gradient compression hook and using it in a distributed training setup. Description: You are required to implement a custom communication hook for gradient compression in DDP. This hook will perform a simple operation: it will divide the gradients by a constant factor to simulate compression. You will then integrate this hook in a simple model and test it in a distributed training setup with at least 2 GPUs. Instructions: 1. **Implement the Custom Hook**: - Create a function `my_custom_compress_hook(state, bucket)` that takes in `state` and `bucket`. - The hook should divide the gradient tensors in `bucket` by a factor of 2. - Return the modified gradients. 2. **Integrate and Test**: - Use the provided `SimpleModel` from the documentation example. - Set up a distributed training environment using `torch.distributed`. - Register the custom hook with the DDP model. - Train the model for a few iterations and ensure the hook is applied by checking the modified gradients. Constraints: - Use at least 2 GPUs for distributed training. - Ensure the model and hook registration are properly serialized and reloaded. Expected Inputs and Outputs: - **Input**: N/A (you will create and set up the environment yourself) - **Output**: Print statements indicating the modified gradients after applying the custom hook. Example Code: ```python import os import tempfile import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel class SimpleModel(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(24, 24) self.relu = nn.ReLU() self.fc2 = nn.Linear(24, 12) def forward(self, x): return self.fc2(self.relu(self.fc1(x))) def my_custom_compress_hook(state, bucket): # Custom hook that compresses gradients by dividing by 2 tensors = [tensor / 2 for tensor in bucket.buffer()] return tensors def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def run_demo(demo_fn, world_size): mp.spawn(demo_fn, args=(world_size,), nprocs=world_size, join=True) def demo_custom_hook(rank, world_size): setup(rank, world_size) model = SimpleModel().to(rank) ddp_model = DistributedDataParallel(model, device_ids=[rank]) # Register the custom communication hook ddp_model.register_comm_hook(None, my_custom_compress_hook) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) # Dummy input and training loop inputs = torch.randn(24, 24).to(rank) for _ in range(10): optimizer.zero_grad() outputs = ddp_model(inputs) loss = outputs.sum() loss.backward() optimizer.step() # Print gradients to verify hook effect for param in ddp_model.parameters(): if param.grad is not None: print(f\\"Rank {rank}, Gradients: {param.grad}\\") cleanup() if __name__ == \\"__main__\\": n_gpus = torch.cuda.device_count() assert n_gpus >= 2, f\\"Requires at least 2 GPUs to run, but got {n_gpus}\\" world_size = n_gpus run_demo(demo_custom_hook, world_size) ``` # Submission: Submit a Jupyter notebook containing your implemented custom communication hook and the demo code to test it. Ensure that the notebook contains detailed explanations and print statements to verify the correct application of the hook.","solution":"import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel class SimpleModel(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(24, 24) self.relu = nn.ReLU() self.fc2 = nn.Linear(24, 12) def forward(self, x): return self.fc2(self.relu(self.fc1(x))) def my_custom_compress_hook(state, bucket): # Custom hook that compresses gradients by dividing by 2 tensors = [tensor / 2 for tensor in bucket.buffer()] return torch.cat(tensors, dim=0) def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def run_demo(demo_fn, world_size): mp.spawn(demo_fn, args=(world_size,), nprocs=world_size, join=True) def demo_custom_hook(rank, world_size): setup(rank, world_size) model = SimpleModel().to(rank) ddp_model = DistributedDataParallel(model, device_ids=[rank]) # Register the custom communication hook ddp_model.register_comm_hook(None, my_custom_compress_hook) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) # Dummy input and training loop inputs = torch.randn(24, 24).to(rank) for _ in range(10): optimizer.zero_grad() outputs = ddp_model(inputs) loss = outputs.sum() loss.backward() optimizer.step() # Print gradients to verify hook effect print(f\\"Rank {rank}, Gradients (fc1): {ddp_model.module.fc1.weight.grad}\\") print(f\\"Rank {rank}, Gradients (fc2): {ddp_model.module.fc2.weight.grad}\\") cleanup() if __name__ == \\"__main__\\": n_gpus = torch.cuda.device_count() assert n_gpus >= 2, f\\"Requires at least 2 GPUs to run, but got {n_gpus}\\" world_size = n_gpus run_demo(demo_custom_hook, world_size)"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of Python\'s `html.parser` module by implementing a custom HTML parser that extracts specific information from an HTML document. # Problem Statement You are required to create a custom HTML parser by subclassing `HTMLParser` that extracts and prints: 1. All the hyperlinks (URLs) present in `<a>` tags. 2. The text content inside any `<h1>` to `<h3>` tags. 3. The text content of any HTML comments. # Implementation Details 1. **Subclass**: You should create a subclass of `HTMLParser` named `CustomHTMLParser`. 2. **Methods to Override**: You need to override at least the following methods: - `handle_starttag(self, tag, attrs)` - `handle_endtag(self, tag)` - `handle_data(self, data)` - `handle_comment(self, data)` 3. **Input**: The input to your parser will be a string containing HTML content. 4. **Output**: - Print all URLs found in `<a>` tags. - Print the text content inside `<h1>` to `<h3>` tags. - Print the content of HTML comments. # Method Definitions - `handle_starttag(self, tag, attrs)`: This method should detect `<a>` tags and extract the URL from the `href` attribute. - `handle_endtag(self, tag)`: This method can be used to handle the closing tags of `<h1>` to `<h3>` tags. - `handle_data(self, data)`: This method should be used to extract and print the text content inside `<h1>` to `<h3>` tags. - `handle_comment(self, data)`: This method should print the comment text. # Expected Input and Output **Input Format**: A single string containing valid HTML. **Output Format**: Print all extracted URLs, heading contents, and comments. **Sample Input**: ```html <html> <head><title>Test</title></head> <body> <h1>Hello World</h1> <p>Some text <a href=\\"https://www.example.com\\">Example</a></p> <!-- A comment --> <h3>Subheading</h3> </body> </html> ``` **Sample Output**: ``` URL: https://www.example.com Heading: Hello World Heading: Subheading Comment: A comment ``` # Constraints - The input HTML will be a well-formed string. - There will be at most 1000 characters in the input HTML string. # Solution Template ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def handle_starttag(self, tag, attrs): if tag == \'a\': for attr in attrs: if attr[0] == \'href\': print(f\\"URL: {attr[1]}\\") def handle_endtag(self, tag): self.current_tag = None def handle_data(self, data): if self.current_tag in [\'h1\', \'h2\', \'h3\']: print(f\\"Heading: {data}\\") def handle_comment(self, data): print(f\\"Comment: {data}\\") def feed_html(self, html): self.current_tag = None self.feed(html) # Example usage html_content = \'\'\' <html> <head><title>Test</title></head> <body> <h1>Hello World</h1> <p>Some text <a href=\\"https://www.example.com\\">Example</a></p> <!-- A comment --> <h3>Subheading</h3> </body> </html> \'\'\' parser = CustomHTMLParser() parser.feed_html(html_content) ``` Implement the `CustomHTMLParser` class by filling in the above methods to achieve the desired functionality.","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def handle_starttag(self, tag, attrs): if tag == \'a\': for attr in attrs: if attr[0] == \'href\': print(f\\"URL: {attr[1]}\\") elif tag in [\'h1\', \'h2\', \'h3\']: self.current_tag = tag def handle_endtag(self, tag): if tag in [\'h1\', \'h2\', \'h3\']: self.current_tag = None def handle_data(self, data): if getattr(self, \'current_tag\', None) in [\'h1\', \'h2\', \'h3\']: print(f\\"Heading: {data.strip()}\\") def handle_comment(self, data): print(f\\"Comment: {data.strip()}\\") def feed_html(self, html): self.current_tag = None self.feed(html) # Example usage html_content = \'\'\' <html> <head><title>Test</title></head> <body> <h1>Hello World</h1> <p>Some text <a href=\\"https://www.example.com\\">Example</a></p> <!-- A comment --> <h3>Subheading</h3> </body> </html> \'\'\' parser = CustomHTMLParser() parser.feed_html(html_content)"},{"question":"Objective: Write a Python function that utilizes the `tracemalloc` module to detect memory leaks in a given function. The function should: 1. Start tracemalloc to begin tracing memory allocations. 2. Execute the given function. 3. Take a snapshot of the memory allocations before and after the function call. 4. Compare the snapshots to find the top 5 memory blocks that increased the most during the function execution. 5. Format and return the results, showing the file, line number, and size of each memory block increase. Function Signature: ```python def detect_memory_leaks(func: callable) -> str: pass ``` Input: - `func`: A callable function that you need to check for memory leaks. Output: - A string representing the top 5 memory blocks with the largest increase in memory usage during the execution of the given function. Each line should include the rank, file, line number, and the increase in memory size formatted as follows: ``` #1: <filename>:<line number>: +<size in KiB> KiB #2: <filename>:<line number>: +<size in KiB> KiB #3: <filename>:<line number>: +<size in KiB> KiB #4: <filename>:<line number>: +<size in KiB> KiB #5: <filename>:<line number>: +<size in KiB> KiB ``` Example: Given the following function: ```python def example_function(): a = [i for i in range(100000)] b = [i for i in range(50000)] ``` You should be able to call: ```python result = detect_memory_leaks(example_function) print(result) ``` Expected output should look similar to: ``` #1: <ipython-input-1-xxxxxxxxxxxx>:2: +781.3 KiB #2: <ipython-input-1-xxxxxxxxxxxx>:3: +390.6 KiB #3: ... #4: ... #5: ... ``` Constraints: - The function `func` may perform any kind of operations and generate different amounts of memory allocations. - Ensure that your function does not include elements from \'tracemalloc\' itself in the final results. - Use at most 25 frames in the traceback for accuracy. Notes: - You may use the provided example for testing your implementation, but others may be used for grading. - Your function must handle any valid Python function passed to it.","solution":"import tracemalloc def detect_memory_leaks(func: callable) -> str: Detects memory leaks in the given function using tracemalloc. Parameters: func (callable): The function to execute and check for memory leaks. Returns: str: A formatted string of the top 5 memory blocks that increased the most during the function execution. # Start tracing memory allocations tracemalloc.start() # Take a snapshot before executing the function snapshot_before = tracemalloc.take_snapshot() # Execute the given function func() # Take a snapshot after executing the function snapshot_after = tracemalloc.take_snapshot() # Compare the snapshots to find the top 5 memory blocks that increased the most stats = snapshot_after.compare_to(snapshot_before, \'lineno\') # Format and output the results top_stats = stats[:5] # Get the top 5 statistics output_lines = [] for i, stat in enumerate(top_stats, start=1): filename = stat.traceback[0].filename lineno = stat.traceback[0].lineno size_in_kib = stat.size / 1024 # Convert bytes to KiB output_lines.append(f\\"#{i}: {filename}:{lineno}: +{size_in_kib:.1f} KiB\\") return \\"n\\".join(output_lines)"},{"question":"Coding Assessment Question # Objective You are required to implement a function `matrix_analysis` that accepts a matrix and performs a series of linear algebra operations using PyTorch\'s `torch.linalg` module. The aim is to perform and verify various characteristics and properties of the matrix. # Description You need to create a function `matrix_analysis(matrix: torch.Tensor) -> dict` that: 1. Computes and returns the Frobenius norm of the matrix. 2. Determines and returns if the matrix is invertible. 3. Calculates and returns the eigenvalues of the matrix. 4. Returns the rank of the matrix. 5. If the matrix is symmetric, computes and returns its Cholesky decomposition. 6. Solves a linear system where this matrix is the coefficient matrix and a random vector serves as the right-hand side. # Function Signature ```python import torch def matrix_analysis(matrix: torch.Tensor) -> dict: pass ``` # Input - `matrix`: A 2D square tensor of shape (n, n) with float values. # Output A dictionary containing the following keys and their corresponding values: - `\'frobenius_norm\'`: Frobenius norm of the matrix. - `\'is_invertible\'`: Boolean indicating whether the matrix is invertible. - `\'eigenvalues\'`: A tensor containing eigenvalues of the matrix. - `\'rank\'`: Rank of the matrix. - `\'cholesky_decomposition\'`: Cholesky decomposition of the matrix if it is symmetric; otherwise, `None`. - `\'solution_vector\'`: The solution to the linear system `Ax = b` where `A` is the matrix and `b` is a random vector. # Constraints - The matrix provided will always be a square matrix (n x n). - Performance should be optimal for matrices of size up to 1000 x 1000. # Example ```python import torch matrix = torch.tensor([[4.0, 1.0], [1.0, 3.0]]) result = matrix_analysis(matrix) print(result) # Example output (values will differ due to random vector): # { # \'frobenius_norm\': 5.099, # \'is_invertible\': True, # \'eigenvalues\': tensor([4.6180, 2.3820]), # \'rank\': 2, # \'cholesky_decomposition\': tensor([[2.0000, 0.0000],[0.5000, 1.6583]]), # \'solution_vector\': tensor([...]) # } ``` # Implementation Notes - Make sure to check for matrix symmetry to decide whether to compute the Cholesky decomposition. - Use `torch.manual_seed()` to ensure reproducibility when generating the random vector for the linear system solution. - Handle any potential exceptions that arise from decomposition or inversion attempts and ensure to return `None` for those operations if applicable.","solution":"import torch def matrix_analysis(matrix: torch.Tensor) -> dict: # Compute the Frobenius norm of the matrix frobenius_norm = torch.linalg.norm(matrix, \'fro\').item() # Check if the matrix is invertible try: _ = torch.linalg.inv(matrix) is_invertible = True except RuntimeError: is_invertible = False # Compute the eigenvalues of the matrix eigenvalues = torch.linalg.eigvals(matrix) # Compute the rank of the matrix rank = torch.linalg.matrix_rank(matrix).item() # Check if the matrix is symmetric is_symmetric = torch.allclose(matrix, matrix.T) # Compute the Cholesky decomposition if the matrix is symmetric if is_symmetric: try: cholesky_decomposition = torch.linalg.cholesky(matrix) except RuntimeError: cholesky_decomposition = None else: cholesky_decomposition = None # Generate a random vector for the linear system Ax = b torch.manual_seed(0) b = torch.randn(matrix.shape[0]) # Solve the linear system Ax = b try: solution_vector = torch.linalg.solve(matrix, b) except RuntimeError: solution_vector = None # Prepare the result dictionary result = { \'frobenius_norm\': frobenius_norm, \'is_invertible\': is_invertible, \'eigenvalues\': eigenvalues, \'rank\': rank, \'cholesky_decomposition\': cholesky_decomposition, \'solution_vector\': solution_vector } return result"},{"question":"**Objective:** Assess your understanding of seaborn\'s ECDF plotting capabilities, data handling, and customization. **Task:** 1. Load the `penguins` dataset from seaborn. 2. Create a figure with two subplots arranged in a 1x2 grid layout. 3. In the first subplot: - Plot the ECDF of the `flipper_length_mm` for each species using different hues. - Use a complementary ECDF. - Display counts instead of proportions on the y axis. 4. In the second subplot: - Plot the ECDF of the `bill_depth_mm` using a wide-form representation. - Ensure that individual histograms are plotted for each numeric column. 5. Customize the plots by: - Adding titles to each subplot. - Setting the axis labels appropriately. - Adding a legend indicating the species in the first subplot. **Input:** - No explicit input function. You will load the dataset within the code. **Output:** - A figure with two customized subplots as described above. **Constraints:** - Use only seaborn and matplotlib libraries for the plots. - Ensure plots are informative and clearly labeled. **Performance Requirements:** - Efficient use of seaborn functions to handle and represent the data accurately. # Example Output While this is an example, your output should follow similar principles of customization and clarity. ```python import seaborn as sns import matplotlib.pyplot as plt # Set theme sns.set_theme() # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Initialize the figure fig, axes = plt.subplots(1, 2, figsize=(15, 5)) # First subplot sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", stat=\\"count\\", complementary=True, ax=axes[0]) axes[0].set_title(\'Complementary ECDF of Flipper Length by Species\') axes[0].set_xlabel(\'Flipper Length (mm)\') axes[0].set_ylabel(\'Count\') axes[0].legend(title=\'Species\') # Second subplot sns.ecdfplot(data=penguins.filter(like=\\"bill_\\", axis=\\"columns\\"), ax=axes[1]) axes[1].set_title(\'ECDF of Bill Measurements (Wide Form Representation)\') axes[1].set_xlabel(\'Measurement Value\') axes[1].set_ylabel(\'Proportion\') # Display the plot plt.tight_layout() plt.show() ``` Ensure your code follows best practices and provides clear, informative plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_ecdfs(): # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Initialize the figure fig, axes = plt.subplots(1, 2, figsize=(15, 5)) # First subplot - Complementary ECDF of flipper_length_mm for each species sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", stat=\\"count\\", complementary=True, ax=axes[0]) axes[0].set_title(\'Complementary ECDF of Flipper Length by Species\') axes[0].set_xlabel(\'Flipper Length (mm)\') axes[0].set_ylabel(\'Count\') axes[0].legend(title=\'Species\') # Second subplot - ECDF of bill_depth_mm using wide-form representation sns.ecdfplot(data=penguins.filter(like=\\"bill_\\", axis=\\"columns\\"), ax=axes[1]) axes[1].set_title(\'ECDF of Bill Measurements (Wide Form Representation)\') axes[1].set_xlabel(\'Measurement Value\') axes[1].set_ylabel(\'Proportion\') # Display the plot plt.tight_layout() plt.show() # Call the function to execute the plotting plot_penguin_ecdfs()"},{"question":"# Custom Email Policy for Promoting Compliance You are tasked with creating a custom email policy to handle email compliance for a specific domain. The organization you are working with needs all its outgoing emails to be strictly RFC compliant while ensuring that no defects in email headers go unnoticed and an error is raised immediately. Additionally, the organization often includes headers that are lengthy and may need careful folding to avoid exceeding a certain line length. Your task is to implement a custom email policy class, `StrictRFCCompliancePolicy`, that follows these compliance requirements. # Requirements: 1. The `StrictRFCCompliancePolicy` class should inherit from `email.policy.EmailPolicy`. 2. This policy should: - Enforce strict RFC compliance by setting `raise_on_defect` to `True`. - Allow a maximum line length of 100 characters. - Use the line separator `\\"rn\\"` to conform with the RFC requirement for email message serialization. 3. Provide custom implementations for the following methods to ensure correct policy behaviors: - `handle_defect`: If a defect is encountered, it should raise the defect as an exception. - `fold`: Should fold headers to respect the maximum line length specified by the policy. - Include any additional attributes necessary to ensure full compliance with the current RFC standards. # Constraints: - You should use only the standard library. - Follow best practices for readability and maintainability. - Your solution should be robust for different types of emails with varying header contents. # Input: - No direct input is required; you are defining a class. # Output: - A class definition for `StrictRFCCompliancePolicy` meeting the outlined requirements. # Example: Here is an outline for your implementation: ```python from email.policy import EmailPolicy class StrictRFCCompliancePolicy(EmailPolicy): def __init__(self, **kw): super().__init__( raise_on_defect=True, max_line_length=100, linesep=\'rn\', **kw ) def handle_defect(self, obj, defect): raise defect def fold(self, name, value): # Implement a custom fold method that respects the max_line_length # For simplicity, assume name and value are strings and don\'t contain binary data. folded_header = name + \\": \\" + value if len(folded_header) > self.max_line_length: lines = [] while len(folded_header) > self.max_line_length: split_point = folded_header.rfind(\' \', 0, self.max_line_length) if split_point == -1: split_point = self.max_line_length lines.append(folded_header[:split_point]) folded_header = folded_header[split_point:].strip() lines.append(folded_header) folded_value = self.linesep.join(lines) return folded_value return folded_header # You can test the policy by attaching it to an email message and checking its serialization behavior. ``` Implement the `StrictRFCCompliancePolicy` class based on these guidelines and ensure it meets the stated compliance requirements.","solution":"from email.policy import EmailPolicy class StrictRFCCompliancePolicy(EmailPolicy): def __init__(self, **kw): super().__init__( raise_on_defect=True, max_line_length=100, linesep=\'rn\', **kw ) def handle_defect(self, obj, defect): raise defect def fold(self, name, value): folded_header = name + \\": \\" + value if len(folded_header) > self.max_line_length: lines = [] while len(folded_header) > self.max_line_length: split_point = folded_header.rfind(\' \', 0, self.max_line_length) if split_point == -1: split_point = self.max_line_length lines.append(folded_header[:split_point]) folded_header = folded_header[split_point:].strip() lines.append(folded_header) folded_value = self.linesep.join(lines) return folded_value return folded_header"},{"question":"# Question: Implement a Filename Filtering Function using Shell-Style Wildcards You are tasked with implementing a function that filters a list of filenames based on a given UNIX shell-style pattern. This function will simulate the behavior of the `fnmatch.filter` function but with some additional constraints. Function Signature ```python def custom_filter(filenames: List[str], pattern: str) -> List[str]: ``` Input - `filenames`: A list of strings where each string is a filename (1 <= len(filenames) <= 10^3, 1 <= len(filenames[i]) <= 100). - `pattern`: A string that represents the shell-style pattern (1 <= len(pattern) <= 100). Output - Returns a list of filenames from the input list that match the given pattern. Pattern Rules - `*`: Matches zero or more characters. - `?`: Matches exactly one character. - `[seq]`: Matches any character in `seq`. - `[!seq]`: Matches any character not in `seq`. Constraints - The function should be case-insensitive. - You should not use the actual `fnmatch` module or `re` module in your implementation. - The function should have a time complexity of O(n * m), where \'n\' is the number of filenames and \'m\' is the length of each filename. Example ```python filenames = [\\"test.txt\\", \\"file.TXT\\", \\"data.csv\\", \\"report.doc\\"] pattern = \\"*.txt\\" print(custom_filter(filenames, pattern)) # Output: [\\"test.txt\\", \\"file.TXT\\"] pattern = \\"??st.tx?\\" print(custom_filter(filenames, pattern)) # Output: [\\"test.txt\\"] pattern = \\"data.[!c]sv\\" print(custom_filter(filenames, pattern)) # Output: [] pattern = \\"[!.]eport.doc\\" print(custom_filter(filenames, pattern)) # Output: [] ``` Explanation 1. For the pattern `\\"*.txt\\"`, the function should match all filenames that end with `.txt`, regardless of the case. 2. For the pattern `\\"??st.tx?\\"`, the function should match filenames with exactly two characters before `st.txt`. 3. For the pattern `\\"data.[!c]sv\\"`, the function should match `data.sv`, but not `data.csv` (since \'c\' is excluded). 4. For the pattern `\\"[!.]eport.doc\\"`, the function should exclude filenames that start with a dot. Implementation Notes - Implement helper functions to handle each pattern rule individually. - Ensure the function is case-insensitive by converting filenames and patterns to lower case before matching. - Include boundary checks and edge cases in your implementation. Good luck, and happy coding!","solution":"from typing import List def matches_pattern(filename: str, pattern: str) -> bool: i, j = 0, 0 while i < len(filename) and j < len(pattern): if pattern[j] == \'*\': while j < len(pattern) and pattern[j] == \'*\': j += 1 if j == len(pattern): return True while i < len(filename): if matches_pattern(filename[i:], pattern[j:]): return True i += 1 return False elif pattern[j] == \'?\': if i < len(filename): i += 1 j += 1 else: return False elif pattern[j] == \'[\': negation = (j + 1 < len(pattern) and pattern[j + 1] == \'!\') if negation: j += 1 matched = False j += 1 char_set = set() while j < len(pattern) and pattern[j] != \']\': char_set.add(pattern[j].lower()) j += 1 if j < len(pattern): j += 1 if filename[i].lower() in char_set: matched = True if negation: matched = not matched if matched: i += 1 else: return False else: if filename[i].lower() != pattern[j].lower(): return False i += 1 j += 1 while j < len(pattern) and pattern[j] == \'*\': j += 1 return i == len(filename) and j == len(pattern) def custom_filter(filenames: List[str], pattern: str) -> List[str]: return [filename for filename in filenames if matches_pattern(filename, pattern)]"},{"question":"# Advanced Coding Assessment Question: Dynamic Python Code Execution and Compilation **Objective:** Implement a Python function that dynamically executes and compiles Python code provided as input, leveraging the high-level functionalities described in the provided documentation. **Problem Statement:** Write a Python function `execute_dynamic_code(code_str: str, start_symbol: int, globals_dict: dict, locals_dict: dict) -> object` that takes the following parameters: - `code_str`: A Python code string to be executed. - `start_symbol`: An integer representing the start symbol (`Py_eval_input`, `Py_file_input`, or `Py_single_input`). - `globals_dict`: A dictionary representing the global execution context. - `locals_dict`: A dictionary representing the local execution context. The function should: 1. Compile the provided `code_str` into a code object using the appropriate start symbol. 2. Execute the compiled code object within the provided global and local contexts. 3. Return the result of executing the code object. **Constraints:** 1. The function should handle exceptions gracefully and return a string indicating the type of error encountered. 2. The `code_str` should be syntactically valid Python code appropriate to the provided start symbol. 3. The execution context (globals and locals) can influence the code execution and should be appropriately reflected in the result. **Input Format:** - `code_str`: A string containing valid Python code. - `start_symbol`: An integer representing one of the allowed start symbols. - `globals_dict`: A dictionary of global variables and functions. - `locals_dict`: A dictionary of local variables and functions. **Output Format:** - The result of the executed code object, or a string specifying the error type if an exception occurs. **Example:** ```python globals_dict = {\'x\': 10} locals_dict = {\'y\': 5} code_str = \'x + y\' result = execute_dynamic_code(code_str, Py_eval_input, globals_dict, locals_dict) print(result) # Output should be 15 code_str = \'if x > y:n result = x - ynelse:n result = y - xnresult\' result = execute_dynamic_code(code_str, Py_file_input, globals_dict, locals_dict) print(result) # Output should be 5 ``` **Performance Requirements:** - The function should efficiently compile and execute the code string. - Handle large code strings (up to several KB) without significant delays. Implement this function ensuring it adheres to the specified behaviors and constraints.","solution":"def execute_dynamic_code(code_str: str, start_symbol: int, globals_dict: dict, locals_dict: dict): import builtins try: # Compile the code string using the provided start symbol code = compile(code_str, \'<string>\', \'eval\' if start_symbol == eval(\\"builtins.eval\\") else \'exec\') # Execute the compiled code in the provided global and local contexts if start_symbol == eval(\\"builtins.eval\\"): return eval(code, globals_dict, locals_dict) else: exec(code, globals_dict, locals_dict) return locals_dict.get(\'result\', None) except Exception as e: return str(e)"},{"question":"# Question: Error Bars and Visual Data Aggregation with Seaborn As part of your data visualization project, you are required to create plots that summarize and display the distribution and uncertainty of data using seaborn. You will work with a dataset and implement different types of error bars that seaborn offers. Task 1. **Data Generation:** - Generate a dataset `data` with 200 samples drawn from a normal distribution with mean `0` and standard deviation `1`. 2. **Visualizing Data with Error Bars:** - Create four subplots (2x2 layout) using `matplotlib`, each demonstrating a different type of error bar in seaborn: standard deviation (`sd`), percentile interval (`pi`), standard error (`se`), and confidence interval (`ci`). - **Top-left:** Plot with Standard Deviation (`sd`) error bars. - **Top-right:** Plot with Percentile Interval (`pi`) error bars, showing the inter-quartile range (IQR). - **Bottom-left:** Plot with Standard Error (`se`) error bars. - **Bottom-right:** Plot with Confidence Interval (`ci`) error bars using a 99% confidence interval. 3. **Customization:** - Ensure that each plot has appropriate titles indicating the type of error bar. - Maintain a uniform x and y limit across all plots to facilitate a direct comparison. Input - You are required to generate the data using `numpy`. Output - Display a figure with four subplots where each plot includes a different type of error bar as specified above. Constraints - Use Seaborn\'s pointplot or other appropriate functions for plotting. - Utilize seaborn theme for consistency in styling. Example Code Structure ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Step 1: Data Generation np.random.seed(42) data = np.random.normal(loc=0, scale=1, size=200) # Step 2: Plot with error bars fig, axs = plt.subplots(2, 2, figsize=(12, 8)) # Top-left: Standard Deviation sns.pointplot(x=data, errorbar=(\\"sd\\"), ax=axs[0, 0]) axs[0, 0].set_title(\'Standard Deviation\') # Top-right: Percentile Interval sns.pointplot(x=data, errorbar=(\\"pi\\", 50), ax=axs[0, 1]) axs[0, 1].set_title(\'Percentile Interval (IQR)\') # Bottom-left: Standard Error sns.pointplot(x=data, errorbar=(\\"se\\"), ax=axs[1, 0]) axs[1, 0].set_title(\'Standard Error\') # Bottom-right: Confidence Interval 99% sns.pointplot(x=data, errorbar=(\\"ci\\", 99), ax=axs[1, 1]) axs[1, 1].set_title(\'Confidence Interval (99%)\') # Step 3: Customization for comparison for ax in axs.flat: ax.set_xlim(-3, 3) ax.set_ylim(-3, 3) plt.tight_layout() plt.show() ```","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def generate_data(seed=42, n_samples=200, loc=0, scale=1): Generates a dataset with specified number of samples from a normal distribution. Parameters: seed (int): Seed for the random number generator. n_samples (int): Number of samples to generate. loc (float): Mean of the normal distribution. scale (float): Standard deviation of the normal distribution. Returns: np.ndarray: Generated dataset. np.random.seed(seed) data = np.random.normal(loc=loc, scale=scale, size=n_samples) return data def plot_with_error_bars(data): Creates a 2x2 subplot showing different types of error bars using seaborn. Parameters: data (np.ndarray): Dataset to plot. # Creating the subplots fig, axs = plt.subplots(2, 2, figsize=(12, 8)) # Top-left: Standard Deviation sns.pointplot(x=data, errorbar=(\\"sd\\"), ax=axs[0, 0]) axs[0, 0].set_title(\'Standard Deviation\') # Top-right: Percentile Interval (IQR) sns.pointplot(x=data, errorbar=(\\"pi\\", 50), ax=axs[0, 1]) axs[0, 1].set_title(\'Percentile Interval (IQR)\') # Bottom-left: Standard Error sns.pointplot(x=data, errorbar=(\\"se\\"), ax=axs[1, 0]) axs[1, 0].set_title(\'Standard Error\') # Bottom-right: Confidence Interval (99%) sns.pointplot(x=data, errorbar=(\\"ci\\", 99), ax=axs[1, 1]) axs[1, 1].set_title(\'Confidence Interval (99%)\') # Uniform limits for comparison for ax in axs.flat: ax.set_xlim(-3, 3) ax.set_ylim(-3, 3) # Adjust layout plt.tight_layout() plt.show()"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding and practical application of the `cgitb` module for managing and displaying exception tracebacks in Python. # Question You are provided with a Python script that performs some data processing tasks. Your task is to enhance this script with robust error handling and detailed exception tracebacks using the `cgitb` module. Specifically, you\'ll need to: 1. Enable `cgitb` to handle uncaught exceptions and display detailed traceback information. 2. Modify the script to catch specific exceptions where applicable and use `cgitb.handler()` to display formatted tracebacks for those exceptions. 3. Ensure that both HTML and plain text formatted tracebacks can be generated. # Given Script ```python # Provided script def process_data(data): # Simulate data processing res = 1 / data # Potential division by zero error return res # Main execution data_input = [10, 5, 0, 2] # Processing data try: for data in data_input: result = process_data(data) print(f\\"Processed result: {result}\\") except Exception as e: print(\\"An error occurred:\\", e) ``` # Requirements 1. **Enable `cgitb` to handle uncaught exceptions**: Modify the script such that any unhandled exceptions are managed by `cgitb`. 2. **Catch specific exceptions**: Refactor the `try-except` block to catch specific exceptions like `ZeroDivisionError` and use `cgitb.handler()` to display the formatted traceback. 3. **Display both HTML and plain text tracebacks**: - Add functionality to handle user input for selecting the format of the traceback (HTML or plain text). - Use `cgitb.enable()` or `cgitb.text()`/`cgitb.html()` accordingly to display the traceback in the selected format. # Input - A list of integers (e.g., `[10, 5, 0, 2]`) will be processed by the `process_data` function. - User input to select the traceback format: either \\"html\\" or \\"text\\". # Output - The processed results for successful data inputs. - Detailed tracebacks for any exceptions that occur during the execution. # Example For the given input list `[10, 5, 0, 2]` and traceback format \\"text\\": **Output** (in console): ``` Processed result: 0.1 Processed result: 0.2 An error occurred: Division by zero <Formatted Plain Text Traceback> ``` For the same input with traceback format \\"html\\": **Output** (in browser or saved HTML file): ``` Processed result: 0.1 Processed result: 0.2 <Formatted HTML Traceback> ``` # Constraints - Do not modify the logic of the `process_data` function. - Ensure that the formatted tracebacks contain a reasonable number of context lines (use the default of 5 unless specified by the user). # Implementation Implement the required functionality and handle both uncaught and specific exceptions as specified using `cgitb`. Provide clear comments and documentation in your code.","solution":"import cgitb # Enable cgitb for detailed exception tracebacks def enable_cgitb(traceback_format=\'text\'): if traceback_format == \\"html\\": cgitb.enable(format=\\"html\\") else: cgitb.enable(format=\\"text\\") def process_data(data): # Simulate data processing res = 1 / data # Potential division by zero error return res # Main execution data_input = [10, 5, 0, 2] traceback_format = \'text\' # or \'html\' # Enable cgitb based on the desired traceback format enable_cgitb(traceback_format) # Processing data try: for data in data_input: try: result = process_data(data) print(f\\"Processed result: {result}\\") except ZeroDivisionError: print(\\"An error occurred: Division by zero\\") cgitb.handler() except Exception: cgitb.handler()"},{"question":"# Question: Advanced Timedelta Manipulations You are provided with a dataset containing the start and end times of various events in string format. Your task is to perform several manipulations and calculations involving these time deltas using the `pandas` library. Specifically, you need to: 1. Calculate the duration of each event. 2. Compute the average event duration. 3. Find the event with the longest duration. 4. Create a new DataFrame where the durations are converted to hours. 5. Handle any missing or invalid duration entries by filling them with zero. **Input Format:** - A pandas DataFrame `df` with two columns: - `start_time`: Strings representing the start times of events in the format \'YYYY-MM-DD HH:MM:SS\'. - `end_time`: Strings representing the end times of events in the format \'YYYY-MM-DD HH:MM:SS\'. **Output Format:** - A DataFrame `df_hours` with the same index as `df` and a single column: - `duration_hours`: The duration of each event in hours, with missing or invalid entries filled with zero. **Constraints:** - The input DataFrame `df` will have at least one row. - Some `start_time` or `end_time` entries might be missing or invalid. Example: ```python import pandas as pd data = { \'start_time\': [\'2023-01-01 08:00:00\', \'2023-01-02 09:30:00\', None, \'invalid\', \'2023-01-04 14:00:00\'], \'end_time\': [\'2023-01-01 10:00:00\', \'2023-01-02 11:00:00\', None, \'2023-01-03 12:00:00\', \'2023-01-04 16:00:00\'] } df = pd.DataFrame(data) # Expected Output: # df_hours: # duration_hours # 0 2.0 # 1 1.5 # 2 0.0 # 3 0.0 # 4 2.0 ``` **Function Signature:** ```python def process_event_durations(df: pd.DataFrame) -> pd.DataFrame: pass ``` Implement `process_event_durations(df: pd.DataFrame) -> pd.DataFrame` to solve the problem.","solution":"import pandas as pd def process_event_durations(df: pd.DataFrame) -> pd.DataFrame: # Helper function to calculate duration in hours def calculate_duration(start, end): try: start_time = pd.to_datetime(start) end_time = pd.to_datetime(end) duration = end_time - start_time return duration.total_seconds() / 3600 except: return 0.0 # Calculate the durations df[\'duration_hours\'] = df.apply(lambda row: calculate_duration(row[\'start_time\'], row[\'end_time\']), axis=1) # Create the result DataFrame with the required format df_hours = df[[\'duration_hours\']] return df_hours"},{"question":"You are required to write a Python function that attempts to download the content of a given URL and handle specific HTTP and URL-related errors. The function should demonstrate the use of `urllib.error` exceptions. Function: `fetch_url_content` **Objective**: Download the content from a given URL and handle errors. **Input**: - `url` (str): The URL to fetch content from. **Output**: - If the content is successfully retrieved, return it as a string. - If a `URLError` or `HTTPError` occurs, return a string describing the error, including the HTTP status code (if applicable). - If a `ContentTooShortError` occurs, attempt to download the content again. If it fails a second time, return \\"Content download failed after retry.\\" **Requirements**: - Use `urllib.request.urlopen` to download the content. - Handle the following exceptions: - `urllib.error.URLError` - `urllib.error.HTTPError` - `urllib.error.ContentTooShortError` - Include detailed error handling: - For `HTTPError`, include the HTTP status code in the error message. - For `ContentTooShortError`, attempt to download the content again before concluding failure. Constraints: - Timeout for any HTTP request should be set to 10 seconds. - Assume the maximum content size is 10MB. - Use standard libraries only (i.e., `urllib`). Example Usage: ```python content = fetch_url_content(\\"http://example.com\\") print(content) # Should print content if successful, or appropriate error message ``` Function Signature: ```python def fetch_url_content(url: str) -> str: pass ``` Good luck!","solution":"import urllib.request import urllib.error def fetch_url_content(url: str) -> str: Downloads the content of a given URL handling specific HTTP and URL-related errors. Parameters: - url (str): The URL to fetch content from. Returns: - If the content is successfully retrieved, return it as a string. - If a URLError or HTTPError occurs, return a string describing the error, including the HTTP status code (if applicable). - If a ContentTooShortError occurs, attempt to download the content again. If it fails a second time, return \\"Content download failed after retry.\\" try: response = urllib.request.urlopen(url, timeout=10) content = response.read() return content.decode(\'utf-8\') except urllib.error.HTTPError as e: return f\\"HTTP Error: {e.code} {e.reason}\\" except urllib.error.ContentTooShortError: try: response = urllib.request.urlopen(url, timeout=10) content = response.read() return content.decode(\'utf-8\') except urllib.error.ContentTooShortError: return \\"Content download failed after retry\\" except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\""},{"question":"Working with File Descriptors and Python Objects **Objective**: Write a Python function that utilizes file handling concepts to read and write data using both Python file objects and low-level file descriptors. This will demonstrate your understanding of how Python interfaces with lower-level file operations. Function Specification ```python def file_descriptor_operations(file_path: str, data: str) -> str: Performs a sequence of operations using both file descriptors and Python file objects. Parameters: file_path (str): Path to the file to open and operate on. data (str): Data to write into the file and then read back. Returns: str: The content read from the file after writing `data` to it. # Step 1: Open the file in write mode (using low-level file descriptor) # Step 2: Create a Python file object from this file descriptor # Step 3: Use the Python file object to write `data` into the file # Step 4: Read the data back using the same file object # Step 5: Close the file and return the read back data ``` **Input**: - `file_path`: A string representing the full path to the file to operate on. - `data`: A string containing the data to be written into the file. **Output**: - A string which contains the content read back from the file after writing the `data` to it. **Constraints**: - Assume `data` is a string of reasonable length (not exceeding 10,000 characters). - The file at `file_path` exists and is writable. **Example**: ```python data_to_write = \\"Hello, Python!\\" file_path = \\"example.txt\\" result = file_descriptor_operations(file_path, data_to_write) assert result == data_to_write ``` **Guidelines**: 1. Use `os.open` to obtain a low-level file descriptor for the file. 2. Utilize `PyFile_FromFd` to create a Python file object from the file descriptor. 3. Use standard Python file methods on this file object to write and subsequently read the data. 4. Ensure proper error handling, especially focusing on closing any open resources in case of exceptions. **Note**: You may need the `ctypes` library for interfacing with functions like `PyFile_FromFd` which are part of the Python C API. **Advanced Consideration**: Discuss the implications of mixing Python stream buffering with low-level file descriptors based on the warning provided in the documentation.","solution":"import os def file_descriptor_operations(file_path: str, data: str) -> str: Performs a sequence of operations using both file descriptors and Python file objects. Parameters: file_path (str): Path to the file to open and operate on. data (str): Data to write into the file and then read back. Returns: str: The content read from the file after writing `data` to it. fd = None try: # Step 1: Open the file in write mode (using low-level file descriptor) fd = os.open(file_path, os.O_RDWR | os.O_CREAT) # Step 2: Create a Python file object from this file descriptor file_obj = os.fdopen(fd, \'w+\') # Step 3: Use the Python file object to write `data` into the file file_obj.write(data) file_obj.flush() # Flush to ensure all data is written # Step 4: Read the data back using the same file object file_obj.seek(0) read_data = file_obj.read() # Return the read back data return read_data finally: # Step 5: Close the file if fd is not None: os.close(fd)"},{"question":"# Advanced Data Visualization with Seaborn **Objective:** Demonstrate your understanding of advanced plotting and customization using the Seaborn `objects` interface. **Task:** Using the `anscombe` dataset from Seaborn, perform the following tasks: 1. **Load the Data:** Use Seaborn to load the `anscombe` dataset. 2. **Create an Advanced Plot:** - Create a scatter plot showing the relationship between the `x` and `y` variables. - Color code the points by `dataset`. - Add a separate linear regression line (order 1 polynomial fit) for each subset of the `dataset`. - Organize the plot using facet grids to separate each dataset into its own subplot, arranging them in a 2x2 grid. 3. **Customize the Appearance:** - Apply a custom theme to the plot where: - The background of the axes is white. - The edges of the axes are colored dark gray (slategray). - Increase the linewidth of all lines in the plot by 2 points. - Additionally, apply the `ticks` style from Seaborn. *Note:* You are expected to use dictionary syntax for applying multiple parameter changes together. **Constraints:** 1. You must use the Seaborn `objects` interface (imported as `so`). 2. Global theme changes should be applied to ensure consistency across all plots. 3. The final plot should be clear, well-labeled, and aesthetically pleasing. **Input:** - None. (The dataset is to be loaded within the function.) **Output:** - The output should be a Seaborn plot meeting the above specifications, displayed inline. **Performance Requirement:** - Efficiently handle and plot the dataset without redundant computations or excessive use of resources. **Function Signature:** ```python def advanced_seaborn_plot(): # Your implementation here ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def advanced_seaborn_plot(): # Load the \'anscombe\' dataset df = sns.load_dataset(\'anscombe\') # Set the custom theme custom_params = { \'axes.facecolor\': \'white\', \'axes.edgecolor\': \'slategray\', \'lines.linewidth\': 2 } sns.set_theme(style=\'ticks\', rc=custom_params) # Create a FacetGrid and map scatter and lineplot grid = sns.FacetGrid(df, col=\'dataset\', col_wrap=2, height=4, aspect=1) grid.map_dataframe(sns.scatterplot, x=\'x\', y=\'y\') grid.map_dataframe(sns.regplot, x=\'x\', y=\'y\', scatter=False, order=1, ci=None) # Adjust and display the plot grid.add_legend() plt.show() # Example run advanced_seaborn_plot()"},{"question":"# Advanced ZIP File Management Question You are tasked with developing a Python utility script that performs multiple operations on ZIP files using the `zipfile` module. Your script should demonstrate an understanding of various functionalities provided by the module. **Task:** 1. **Create a ZIP file:** - Write a function `create_zip(zip_filename: str, files: List[str], compression: str = \'ZIP_DEFLATED\') -> None` that creates a ZIP file named `zip_filename` and adds the files specified in the `files` list to the ZIP archive using the chosen compression method (`ZIP_STORED`, `ZIP_DEFLATED`, `ZIP_BZIP2`, or `ZIP_LZMA`). 2. **List contents of the ZIP file:** - Write a function `list_zip_contents(zip_filename: str) -> List[str]` that returns a list of all the file names within the archive. 3. **Extract a specific file:** - Write a function `extract_specific_file(zip_filename: str, file_name: str, extract_location: str) -> str` that extracts the specified `file_name` from the ZIP archive and saves it to the `extract_location`. Return the path of the extracted file. 4. **Add a new file to an existing ZIP:** - Write a function `add_to_zip(zip_filename: str, new_file: str, arcname: Optional[str] = None) -> None` that adds a `new_file` to the existing ZIP archive, optionally renaming it to `arcname` within the archive. 5. **Handle ZIP exceptions:** - Ensure your functions handle the possible exceptions (`BadZipFile`, `LargeZipFile`, `FileExistsError`, `KeyError`, etc.) gracefully and provide meaningful error messages. **Constraints:** - Assume that all file paths provided (for creation and extraction) are valid and accessible. - Assume a valid compression method will always be provided. - `extract_location` is always a valid and writable directory. - Python version 3.10+ is being used. **Example Usage:** ```python # Sample usage demonstrating the functions if __name__ == \\"__main__\\": files_to_zip = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] create_zip(\'my_archive.zip\', files_to_zip, \'ZIP_DEFLATED\') contents = list_zip_contents(\'my_archive.zip\') print(\\"Contents of ZIP:\\", contents) extracted_file_path = extract_specific_file(\'my_archive.zip\', \'file1.txt\', \'/extracted_files\') print(f\'File extracted to: {extracted_file_path}\') add_to_zip(\'my_archive.zip\', \'new_file.txt\', \'renamed_file.txt\') contents = list_zip_contents(\'my_archive.zip\') print(\\"Updated contents of ZIP:\\", contents) ```","solution":"import zipfile from typing import List, Optional import os def create_zip(zip_filename: str, files: List[str], compression: str = \'ZIP_DEFLATED\') -> None: compression_mapping = { \'ZIP_STORED\': zipfile.ZIP_STORED, \'ZIP_DEFLATED\': zipfile.ZIP_DEFLATED, \'ZIP_BZIP2\': zipfile.ZIP_BZIP2, \'ZIP_LZMA\': zipfile.ZIP_LZMA } if compression not in compression_mapping: raise ValueError(f\\"Invalid compression method: {compression}\\") try: with zipfile.ZipFile(zip_filename, \'w\', compression_mapping[compression]) as zipf: for file in files: zipf.write(file, os.path.basename(file)) except Exception as e: print(f\\"An error occurred while creating the ZIP file: {e}\\") def list_zip_contents(zip_filename: str) -> List[str]: try: with zipfile.ZipFile(zip_filename, \'r\') as zipf: return zipf.namelist() except zipfile.BadZipFile: raise ValueError(\\"The ZIP file is corrupted or invalid.\\") except Exception as e: print(f\\"An error occurred while listing contents of the ZIP file: {e}\\") return [] def extract_specific_file(zip_filename: str, file_name: str, extract_location: str) -> str: try: with zipfile.ZipFile(zip_filename, \'r\') as zipf: zipf.extract(file_name, extract_location) return os.path.join(extract_location, file_name) except zipfile.BadZipFile: raise ValueError(\\"The ZIP file is corrupted or invalid.\\") except KeyError: raise ValueError(f\\"The file {file_name} does not exist in the ZIP archive.\\") except Exception as e: print(f\\"An error occurred while extracting the file from the ZIP archive: {e}\\") def add_to_zip(zip_filename: str, new_file: str, arcname: Optional[str] = None) -> None: try: with zipfile.ZipFile(zip_filename, \'a\') as zipf: arcname = arcname if arcname else os.path.basename(new_file) zipf.write(new_file, arcname) except zipfile.BadZipFile: raise ValueError(\\"The ZIP file is corrupted or invalid.\\") except Exception as e: print(f\\"An error occurred while adding the file to the ZIP archive: {e}\\")"},{"question":"You are tasked with extracting, validating, and formatting phone numbers from a given string. Phone numbers in the given string may appear in varying formats. Your job is to standardize them into a consistent format \\"(XXX) XXX-XXXX\\". You should also ensure that extracted phone numbers are US-valid (contain exactly 10 digits). Requirements: 1. Implement a function `standardize_phone_numbers`. 2. This function should: - Extract phone numbers from the input string. - Validate that each phone number has exactly 10 digits. - Standardize valid phone numbers into the format \\"(XXX) XXX-XXXX\\". - Replace the original phone numbers in the input string with their standardized form. Input: - A single string `text` containing potential phone numbers in various formats, such as: - \\"1234567890\\" - \\"123-456-7890\\" - \\"(123) 456-7890\\" - \\"123.456.7890\\" - \\"123 456 7890\\" Output: - A string with all valid phone numbers standardized to the format \\"(XXX) XXX-XXXX\\". Constraints: - Phone numbers must contain exactly 10 digits to be considered valid. - Ignore numbers that are not valid US phone numbers. - Assume that the input string will have phone numbers embedded within other text. Example: ```python def standardize_phone_numbers(text: str) -> str: pass # Example execution input_text = \\"Contact us at 1234567890 or (098) 765-4321 and also 987.654.3210!\\" standardized_text = standardize_phone_numbers(input_text) print(standardized_text) # Output: \\"Contact us at (123) 456-7890 or (098) 765-4321 and also (987) 654-3210!\\" ``` **Hint:** - Utilize regular expressions to capture the various phone number formats. - Non-digit characters like \\"(\\", \\")\\", \\"-\\", \\".\\", and spaces can be treated as delimiters. - Use named groups or non-capturing groups as suitable to aid in extracting the relevant parts of the numbers.","solution":"import re def standardize_phone_numbers(text: str) -> str: # Regular Expression pattern to match various formats of 10 digit phone numbers phone_pattern = re.compile(r\'\'\' # Match different phone number formats (bd{3}D?d{3}D?d{4}b) # 1234567890, 123-456-7890, 123.456.7890, 123 456 7890, (123) 456-7890 \'\'\', re.VERBOSE) def format_phone_number(match): # Extract all digits from the matching phone number digits = re.sub(r\'D\', \'\', match.group(0)) # Ensure it has exactly 10 digits if len(digits) == 10: return f\\"({digits[:3]}) {digits[3:6]}-{digits[6:]}\\" # Return the original match if it\'s not a valid US phone number return match.group(0) # Substitute all found phone numbers in text with their standardized form return phone_pattern.sub(format_phone_number, text)"},{"question":"# PyTorch SIMD Optimization Checker **Objective:** Create a PyTorch utility to verify SIMD support on the current system and demonstrate optimization of a neural network computation using SIMD features if available. **Task:** 1. Write a function `check_simd_support()` that checks for SIMD support (AVX, AVX2, AVX-512, AMX) on the current machine using the PyTorch `collect_env.py` script. 2. Implement a function `optimize_computation()` that: - Takes a tensor as input. - Applies matrix multiplication followed by a ReLU activation. - If AVX-512 and AMX are available, use the most efficient PyTorch methods to perform these operations. **Function Specifications:** 1. **`check_simd_support()`**: - **Input:** None - **Output:** Dictionary containing the SIMD types supported (keys: \'AVX\', \'AVX2\', \'AVX-512\', \'AMX\'; values: boolean indicating support). 2. **`optimize_computation(tensor)`**: - **Input:** A PyTorch tensor of shape `(N, M)`. - **Output:** A PyTorch tensor resulting from the matrix multiplication followed by ReLU. **Constraints:** - Implement appropriate checks to see if required SIMD extensions are available. - If the required extensions are not available, use the default PyTorch methods. - Ensure the solution works efficiently for large tensor sizes (performance considerations). **Example Usage:** ```python import torch def check_simd_support(): # Your implementation here pass def optimize_computation(tensor): # Your implementation here pass # Example tensor input_tensor = torch.rand((1000, 1000)) # Check SIMD support simd_support = check_simd_support() print(\\"SIMD support:\\", simd_support) # Perform optimized computation result = optimize_computation(input_tensor) print(result.shape) # Should print torch.Size([1000, 1000]) ``` **Hints:** - Use the `collect_env.py` script and process the output to determine SIMD support. - Leverage PyTorch\'s documentation and tutorials on optimizing performance using SIMD features.","solution":"import torch import subprocess def check_simd_support(): Checks for SIMD support (AVX, AVX2, AVX-512, AMX) on the current machine. Returns: dict: Dictionary containing the SIMD types supported (keys: \'AVX\', \'AVX2\', \'AVX-512\', \'AMX\'; values: boolean indicating support). simd_support = {\'AVX\': False, \'AVX2\': False, \'AVX-512\': False, \'AMX\': False} try: result = subprocess.run([\'lscpu\'], capture_output=True, text=True, check=True) cpu_info = result.stdout simd_support[\'AVX\'] = \'avx\' in cpu_info.lower() simd_support[\'AVX2\'] = \'avx2\' in cpu_info.lower() simd_support[\'AVX-512\'] = \'avx512\' in cpu_info.lower() simd_support[\'AMX\'] = \'amx\' in cpu_info.lower() except (subprocess.CalledProcessError, FileNotFoundError): print(\\"Could not execute `lscpu` to check SIMD support\\") return simd_support def optimize_computation(tensor): Applies matrix multiplication followed by a ReLU activation on the input tensor. Uses the most efficient PyTorch methods if AVX-512 and AMX are available. Args: tensor (torch.Tensor): A PyTorch tensor of shape (N, M). Returns: torch.Tensor: A PyTorch tensor resulting from the matrix multiplication followed by ReLU. N, M = tensor.size() weight = torch.rand((M, M), device=tensor.device) # Check SIMD support simd_support = check_simd_support() # Perform optimized computation if simd_support[\'AVX-512\'] and simd_support[\'AMX\']: print(\\"Using optimized AVX-512 and AMX operations\\") result = torch.mm(tensor, weight) else: print(\\"Using default operations\\") result = torch.matmul(tensor, weight) result = torch.relu(result) return result"},{"question":"**Objective:** Demonstrate your understanding of Seaborn\'s `plotting_context` function, the application of predefined styles, and the use of context managers to customize plot aesthetics. # Problem Statement You are required to create visualizations using Seaborn to compare and contrast the effect of different plotting contexts on the same dataset. Follow the steps below: 1. Load any dataset available in Seaborn, for instance, the `tips` dataset. 2. Create three line plots using the following plotting contexts: - Default context. - \\"talk\\" context. - Custom context (where the font size and linewidth are larger than the default values). 3. Use context managers to apply the \\"talk\\" and custom contexts temporarily. 4. Save all three plots as images with filenames `default_plot.png`, `talk_plot.png`, and `custom_plot.png`. **Input:** - No input is needed for this question. **Output:** - Generate three images named `default_plot.png`, `talk_plot.png`, and `custom_plot.png`, representing the line plots in different contexts. **Constraints:** - Use the `tips` dataset for this task. - Ensure that the line plots feature the `total_bill` on the x-axis and `tip` on the y-axis. **Example:** Here\'s an example of the structure of your solution in code: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset data = sns.load_dataset(\\"tips\\") # Plot with the default context sns.lineplot(x=\\"total_bill\\", y=\\"tip\\", data=data) plt.savefig(\\"default_plot.png\\") plt.clf() # Plot with the \\"talk\\" context with sns.plotting_context(\\"talk\\"): sns.lineplot(x=\\"total_bill\\", y=\\"tip\\", data=data) plt.savefig(\\"talk_plot.png\\") plt.clf() # Define a custom context custom_context = {\\"font.size\\": 18, \\"axes.labelsize\\": 18, \\"lines.linewidth\\": 2} # Plot with the custom context with sns.plotting_context(rc=custom_context): sns.lineplot(x=\\"total_bill\\", y=\\"tip\\", data=data) plt.savefig(\\"custom_plot.png\\") plt.clf() print(\\"Plots created and saved successfully.\\") ``` In this task, you need to demonstrate how you can manipulate the Seaborn plotting contexts to achieve different visual results by: - Using Seaborn\'s `plotting_context()` - Applying both predefined and custom styles - Saving the generated plots for comparison **Performance Requirements:** - The solution should be efficient and should not take an unreasonable amount of time to execute.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Load the dataset data = sns.load_dataset(\\"tips\\") # Plot with the default context sns.lineplot(x=\\"total_bill\\", y=\\"tip\\", data=data) plt.savefig(\\"default_plot.png\\") plt.clf() # Plot with the \\"talk\\" context with sns.plotting_context(\\"talk\\"): sns.lineplot(x=\\"total_bill\\", y=\\"tip\\", data=data) plt.savefig(\\"talk_plot.png\\") plt.clf() # Define a custom context custom_context = {\\"font.size\\": 18, \\"axes.labelsize\\": 18, \\"lines.linewidth\\": 2} # Plot with the custom context with sns.plotting_context(rc=custom_context): sns.lineplot(x=\\"total_bill\\", y=\\"tip\\", data=data) plt.savefig(\\"custom_plot.png\\") plt.clf() # Call the function to create the plots create_plots()"},{"question":"# PyTorch Tensor View: Advanced Operations **Problem Statement:** You are required to perform a series of operations on tensors using PyTorch, demonstrating your understanding of tensor views, their impact on base tensors, and contiguity. Implement a function `tensor_view_operations` that performs the following steps: 1. Create a base tensor `base` of size (4, 4) with random floating-point numbers. 2. Generate a view tensor `view1` of `base` by reshaping it to size (2, 8). 3. Modify the first element of `view1` to 3.14 and demonstrate that the modification is reflected in `base`. 4. Create another view tensor `view2` by transposing `base`. 5. Ensure that `view2` is contiguous and create a contiguous tensor `contiguous_view2`. 6. Return a tuple of the modified `base`, `view1`, `view2`, and `contiguous_view2`. **Function Signature:** ```python import torch def tensor_view_operations() -> (torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor): pass ``` **Expected Output:** The function should return a tuple containing: - The modified `base` tensor. - The `view1` tensor. - The `view2` tensor. - The `contiguous_view2` tensor. **Constraints:** - All operations should be performed using PyTorch tensor methods. - Do not use any additional libraries or packages. **Example:** ```python # Example output (actual values will differ due to randomness): # ( # tensor([[3.1400, 0.1234, 0.5678, 0.1234], # [0.5678, 0.1234, 0.5678, 0.1234], # [0.5678, 0.1234, 0.5678, 0.1234], # [0.5678, 0.1234, 0.5678, 0.1234]]), # tensor([[3.1400, 0.1234, 0.5678, 0.1234, 0.5678, 0.1234, 0.5678, 0.1234], # [0.5678, 0.1234, 0.5678, 0.1234, 0.5678, 0.1234, 0.5678, 0.1234]]), # tensor([[3.1400, 0.5678, 0.1234, 0.5678], # [0.1234, 0.5678, 0.1234, 0.5678], # [0.1234, 0.5678, 0.1234, 0.5678], # [0.1234, 0.5678, 0.1234, 0.5678]]), # tensor([[3.1400, 0.5678, 0.1234, 0.5678], # [0.1234, 0.5678, 0.1234, 0.5678], # [0.1234, 0.5678, 0.1234, 0.5678], # [0.1234, 0.5678, 0.1234, 0.5678]]) # ) ``` Ensure to initialize the random seed if you need consistent output for testing purposes. ```python torch.manual_seed(0) ``` The task tests the understanding of tensor views, the relationship between views and base tensors, and the concept of contiguous and non-contiguous tensors in PyTorch.","solution":"import torch def tensor_view_operations(): # Set seed for reproducibility torch.manual_seed(0) # Step 1: Create a base tensor `base` of size (4, 4) with random floating-point numbers base = torch.rand((4, 4)) # Step 2: Generate a view tensor `view1` of `base` by reshaping it to size (2, 8) view1 = base.view(2, 8) # Step 3: Modify the first element of `view1` to 3.14 view1[0, 0] = 3.14 # Step 4: Create another view tensor `view2` by transposing `base` view2 = base.t() # Step 5: Ensure that `view2` is contiguous and create a contiguous tensor `contiguous_view2` contiguous_view2 = view2.contiguous() # Step 6: Return the modified `base`, `view1`, `view2`, and `contiguous_view2` return base, view1, view2, contiguous_view2"},{"question":"You are tasked with creating a custom JSON encoder and decoder in Python that extends the `json.JSONEncoder` and `json.JSONDecoder` classes. The goal is to handle a unique data type, `ComplexNumber`, which represents complex numbers. # Class Definition ```python class ComplexNumber: def __init__(self, real, imag): self.real = real self.imag = imag ``` # Requirements 1. **Custom Encoder**: - Extend the `json.JSONEncoder` class to handle `ComplexNumber` instances. - When a `ComplexNumber` instance is encountered, it should be serialized into a JSON object with two keys: `\\"real\\"` and `\\"imag\\"`. - Any other types should be handled by the default encoder. 2. **Custom Decoder**: - Extend the `json.JSONDecoder` class to handle JSON objects that represent `ComplexNumber` instances. - The decoder should recognize JSON objects that contain both `\\"real\\"` and `\\"imag\\"` keys and convert them back into `ComplexNumber` instances. # Function Signature ```python import json class ComplexNumber: def __init__(self, real, imag): self.real = real self.imag = imag class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, ComplexNumber): return {\'real\': obj.real, \'imag\': obj.imag} return json.JSONEncoder.default(self, obj) def decode_complex(dct): if \'real\' in dct and \'imag\' in dct: return ComplexNumber(dct[\'real\'], dct[\'imag\']) return dct # Usage complex_number = ComplexNumber(1, 2) json_data = json.dumps(complex_number, cls=ComplexEncoder) decoded_object = json.loads(json_data, object_hook=decode_complex) ``` # Input - `ComplexNumber` instance. - JSON string representing a complex number object. # Output - JSON string for the encoder. - `ComplexNumber` instance for the decoder. # Constraints - Ensure that the custom encoder/decorator can handle arbitrary nested structures that include complex numbers. # Example ```python # Example Class Instance complex_number = ComplexNumber(4, 5) # Serialize instance to JSON json_data = json.dumps(complex_number, cls=ComplexEncoder) print(json_data) # Output should be \'{\\"real\\": 4, \\"imag\\": 5}\' # Deserialize JSON string back to ComplexNumber decoded_object = json.loads(json_data, object_hook=decode_complex) print(decoded_object.__dict__) # Output should be {\'real\': 4, \'imag\': 5} # Testing nested structures nested_structure = {\'number\': complex_number, \'list\': [1, 2, 3, complex_number]} json_data_nested = json.dumps(nested_structure, cls=ComplexEncoder) print(json_data_nested) # Output should correctly include the nested complex numbers decoded_nested = json.loads(json_data_nested, object_hook=decode_complex) print(decoded_nested[\'number\'].__dict__) # Output should be {\'real\': 4, \'imag\': 5} print(decoded_nested[\'list\'][3].__dict__) # Output should be {\'real\': 4, \'imag\': 5} ``` This question assesses your understanding of creating custom encoder and decoder functionalities utilizing Python\'s `json` module. You need to handle a nested JSON representation, ensuring that both serialization and deserialization are correctly and efficiently managed.","solution":"import json class ComplexNumber: def __init__(self, real, imag): self.real = real self.imag = imag class ComplexEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, ComplexNumber): return {\'real\': obj.real, \'imag\': obj.imag} return super().default(obj) def decode_complex(dct): if \'real\' in dct and \'imag\' in dct: return ComplexNumber(dct[\'real\'], dct[\'imag\']) return dct"},{"question":"Objective: Implement a custom content manager by extending the `ContentManager` class and using it to handle specific MIME types. This will test your understanding of class inheritance, method overriding, and handler registration. Task: 1. **Extend the `ContentManager` class**: - Create a class `CustomContentManager` that extends `ContentManager`. - Implement custom handlers for: - `application/json` to extract JSON content and convert it to a dictionary. - `image/jpeg` to extract image content and return the raw byte data. 2. **Register Handlers**: - Register your custom handlers using the `add_get_handler` and `add_set_handler` methods. 3. **Demonstrate Usage**: - Create an example MIME message for each type (JSON and JPEG). - Use `CustomContentManager` to add content to the MIME message and then retrieve it. Expected Input and Output: - **Input**: - JSON MIME message with MIME type `application/json`. - JPEG MIME message with MIME type `image/jpeg`. - **Output**: - For JSON: A dictionary converted from JSON content. - For JPEG: Raw byte data of the image. Constraints: - The JSON MIME message will be a valid JSON string. - The JPEG MIME message will contain valid JPEG byte data. Sample Code: Here is a skeleton for `CustomContentManager` that you need to complete: ```python import json from email.contentmanager import ContentManager from email.message import EmailMessage class CustomContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'application/json\', self.get_json_content) self.add_set_handler(dict, self.set_json_content) self.add_get_handler(\'image/jpeg\', self.get_image_content) self.add_set_handler(bytes, self.set_image_content) def get_json_content(self, msg, *args, **kwargs): # Implement the handler to convert JSON content to dictionary pass def set_json_content(self, msg, obj, *args, **kwargs): # Implement the handler to add dictionary content as JSON pass def get_image_content(self, msg, *args, **kwargs): # Implement the handler to extract raw byte data from JPEG pass def set_image_content(self, msg, obj, *args, **kwargs): # Implement the handler to add raw byte data as JPEG content pass # Demonstrate usage if __name__ == \\"__main__\\": manager = CustomContentManager() # Example using JSON json_msg = EmailMessage() sample_json = {\\"key\\": \\"value\\"} manager.set_content(json_msg, sample_json, subtype=\'json\') retrieved_json = manager.get_content(json_msg) print(retrieved_json) # Output should be: {\'key\': \'value\'} # Example using JPEG jpeg_msg = EmailMessage() sample_image_bytes = b\'xffxd8xffxdb...\' # Sample JPEG byte data manager.set_content(jpeg_msg, sample_image_bytes, maintype=\'image\', subtype=\'jpeg\') retrieved_jpeg = manager.get_content(jpeg_msg) print(retrieved_jpeg[:10]) # Output should be partial raw byte data ``` Note: You need to fill in the missing implementations for the handler methods within the `CustomContentManager` class.","solution":"import json from email.contentmanager import ContentManager from email.message import EmailMessage class CustomContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'application/json\', self.get_json_content) self.add_set_handler(dict, self.set_json_content) self.add_get_handler(\'image/jpeg\', self.get_image_content) self.add_set_handler(bytes, self.set_image_content) def get_json_content(self, msg, *args, **kwargs): Converts JSON content to a dictionary. content = msg.get_payload(decode=True) return json.loads(content) def set_json_content(self, msg, obj, *args, **kwargs): Adds dictionary content as JSON. msg.set_payload(json.dumps(obj), charset=\'utf-8\') msg.set_type(\'application/json\') def get_image_content(self, msg, *args, **kwargs): Extracts raw byte data from JPEG. return msg.get_payload(decode=True) def set_image_content(self, msg, obj, *args, **kwargs): Adds raw byte data as JPEG content. msg.set_payload(obj) msg.set_type(\'image/jpeg\') # Demonstrate usage if __name__ == \\"__main__\\": manager = CustomContentManager() # Example using JSON json_msg = EmailMessage() sample_json = {\\"key\\": \\"value\\"} manager.set_content(json_msg, sample_json, subtype=\'json\') retrieved_json = manager.get_content(json_msg) print(retrieved_json) # Output should be: {\'key\': \'value\'} # Example using JPEG jpeg_msg = EmailMessage() sample_image_bytes = b\'xffxd8xffxdb...sample JPEG byte data...\' manager.set_content(jpeg_msg, sample_image_bytes, maintype=\'image\', subtype=\'jpeg\') retrieved_jpeg = manager.get_content(jpeg_msg) print(retrieved_jpeg[:10]) # Output should be partial raw byte data"},{"question":"**Objective:** Demonstrate your understanding of `scikit-learn` label transformation utilities: `LabelBinarizer`, `MultiLabelBinarizer`, and `LabelEncoder`. **Task:** 1. **Label Binarization**: Write a function `binarize_labels` that takes a list of integer labels and returns the label indicator matrix using `LabelBinarizer`. 2. **Multi-Label Binarization**: Write a function `binarize_multilabels` that takes a list of lists (where each sublist contains multiple labels) and returns the label indicator matrix using `MultiLabelBinarizer`. 3. **Label Encoding**: - Write a function `encode_labels` that takes a list of string labels and returns two lists: encoded labels (integers) and the original labels (strings) after inverse transformation. Use `LabelEncoder`. - Write a function `decode_labels` that takes a list of encoded integer labels and the encoder instance, then returns the original string labels. **Function Signatures:** 1. `def binarize_labels(labels: List[int]) -> np.ndarray:` 2. `def binarize_multilabels(labels: List[List[int]]) -> np.ndarray:` 3. `def encode_labels(labels: List[str]) -> Tuple[List[int], List[str]]:` 4. `def decode_labels(encoded_labels: List[int], encoder: preprocessing.LabelEncoder) -> List[str]:` **Input:** - `labels`: a list of integers (for `binarize_labels`), a list of lists of integers (for `binarize_multilabels`), a list of strings (for `encode_labels` and `decode_labels`). - `encoded_labels`: a list of encoded integer labels for the `decode_labels` function. - `encoder`: an instance of `LabelEncoder` pre-fitted to the list of string labels used in `encode_labels`. **Output:** - For `binarize_labels` and `binarize_multilabels`, an `np.ndarray` representing the label indicator matrix. - For `encode_labels`, a tuple containing the list of encoded integer labels and the list of original string labels after reverse transformation. - For `decode_labels`, the list of original string labels corresponding to the given encoded labels. **Constraints:** - The maximum length of any input list will not exceed 10^5. - The maximum number of unique labels across the dataset will not exceed 10^2. **Examples:** 1. Example for `binarize_labels`: ```python labels = [1, 2, 6, 4, 2] print(binarize_labels(labels)) # Output: # array([[1, 0, 0, 0], # [0, 1, 0, 0], # [0, 0, 1, 0], # [0, 0, 0, 1], # [0, 1, 0, 0]]) ``` 2. Example for `binarize_multilabels`: ```python labels = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] print(binarize_multilabels(labels)) # Output: # array([[0, 0, 1, 1, 1], # [0, 0, 1, 0, 0], # [1, 1, 0, 1, 0], # [1, 1, 1, 1, 1], # [1, 1, 1, 0, 0]]) ``` 3. Example for `encode_labels` and `decode_labels`: ```python labels = [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] encoded_labels, original_labels = encode_labels(labels) print(encoded_labels) # Output: [1, 1, 2, 0] print(original_labels) # Output: [\'paris\', \'paris\', \'tokyo\', \'amsterdam\'] encoder = preprocessing.LabelEncoder().fit(labels) decoded_labels = decode_labels(encoded_labels, encoder) print(decoded_labels) # Output: [\'paris\', \'paris\', \'tokyo\', \'amsterdam\'] ``` --- **Note:** - Import necessary packages and modules (`numpy`, `sklearn.preprocessing`) before implementing the functions. - Include proper error handling to manage unexpected inputs.","solution":"import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder from typing import List, Tuple def binarize_labels(labels: List[int]) -> np.ndarray: Takes a list of integer labels and returns the label indicator matrix using LabelBinarizer. lb = LabelBinarizer() return lb.fit_transform(labels) def binarize_multilabels(labels: List[List[int]]) -> np.ndarray: Takes a list of lists (where each sublist contains multiple labels) and returns the label indicator matrix using MultiLabelBinarizer. mlb = MultiLabelBinarizer() return mlb.fit_transform(labels) def encode_labels(labels: List[str]) -> Tuple[List[int], List[str]]: Takes a list of string labels and returns two lists: encoded labels (integers) and the original labels (strings) after inverse transformation. le = LabelEncoder() encoded_labels = le.fit_transform(labels) original_labels = le.inverse_transform(encoded_labels) return encoded_labels.tolist(), original_labels.tolist() def decode_labels(encoded_labels: List[int], encoder: LabelEncoder) -> List[str]: Takes a list of encoded integer labels and the encoder instance, then returns the original string labels. return encoder.inverse_transform(encoded_labels).tolist()"},{"question":"**Title:** Advanced Typing and Generics in Python **Objective:** Create a system for vehicle management using Python\'s `typing` module. Your implementation should demonstrate the use of type aliases, generic types, Callables, and structural subtyping. **Problem Statement:** You are tasked with creating a type-safe vehicle management system. This system should manage different types of vehicles and perform operations specific to each vehicle type. Use the `typing` module to enforce type checking and improve code readability. **Requirements:** 1. **Define Type Aliases:** - `Identifier`: An alias for `NewType` representing a unique identifier for each vehicle. It should be based on `int`. - `VehicleData`: A `TypedDict` that represents the data for a vehicle with keys \\"id\\" (Identifier), \\"make\\" (str), and \\"model\\" (str). 2. **Create a Generic Vehicle Manager:** - Define a `Generic` `VehicleManager` class that uses a type variable `V` constrained to `VehicleData`. - Implement methods to: - Add a vehicle. - Retrieve a vehicle by its Identifier. - Remove a vehicle by its Identifier. 3. **Implement Vehicle Operations:** - Provide a type-safe mechanism to execute operations on vehicles: - Define an operation as a `Callable` that takes a `VehicleData` and returns `str`. - Examples of operations include printing vehicle details or performing a check on the vehicle. 4. **Structural Subtyping for Operations:** - Use a `Protocol` to define an interface for a vehicle operation. - Define a class `VehiclePrintOperation` implementing this protocol to print vehicle details. **Constraints:** - Use the `typing` module\'s features to enforce type constraints and improve code clarity. - Ensure that methods handling vehicles are type-safe and self-explanatory with type annotations. **Expected Functions and Classes:** 1. Type Aliases: `Identifier`, `VehicleData` 2. Class: `VehicleManager[V]` - Methods: `add_vehicle(self, vehicle: V) -> None`, `get_vehicle(self, id: Identifier) -> Optional[V]`, `remove_vehicle(self, id: Identifier) -> bool` 3. Callable for Operations: `VehicleOperation` 4. Protocol: `VehicleOperationProtocol` 5. Class: `VehiclePrintOperation` **Input/Output:** *Example Usage:* ```python from typing import List, Callable from your_module import Identifier, VehicleData, VehicleManager, VehiclePrintOperation vehicle1: VehicleData = {\\"id\\": Identifier(1), \\"make\\": \\"Tesla\\", \\"model\\": \\"Model S\\"} vehicle2: VehicleData = {\\"id\\": Identifier(2), \\"make\\": \\"Ford\\", \\"model\\": \\"Mustang\\"} manager = VehicleManager[VehicleData]() manager.add_vehicle(vehicle1) manager.add_vehicle(vehicle2) operation = VehiclePrintOperation() operation.perform(vehicle1) # Should print: \\"Vehicle ID: 1, Make: Tesla, Model: Model S\\" ``` Your implementation must ensure type correctness and demonstrate the use of `typing` module features to achieve the stated objectives. You are to implement the classes and functions indicated in the problem statement.","solution":"from typing import TypedDict, NewType, TypeVar, Generic, Optional, Callable, Protocol # Define Type Aliases Identifier = NewType(\'Identifier\', int) class VehicleData(TypedDict): id: Identifier make: str model: str # Create a type variable V constrained to VehicleData V = TypeVar(\'V\', bound=VehicleData) # Create a Generic Vehicle Manager class VehicleManager(Generic[V]): def __init__(self): self.vehicles = {} def add_vehicle(self, vehicle: V) -> None: self.vehicles[vehicle[\'id\']] = vehicle def get_vehicle(self, id: Identifier) -> Optional[V]: return self.vehicles.get(id) def remove_vehicle(self, id: Identifier) -> bool: if id in self.vehicles: del self.vehicles[id] return True return False # Define an operation as a Callable VehicleOperation = Callable[[VehicleData], str] # Structural subtyping for operations class VehicleOperationProtocol(Protocol): def perform(self, vehicle: VehicleData) -> str: ... # Define a class implementing the VehicleOperationProtocol to print vehicle details class VehiclePrintOperation: def perform(self, vehicle: VehicleData) -> str: output = f\\"Vehicle ID: {vehicle[\'id\']}, Make: {vehicle[\'make\']}, Model: {vehicle[\'model\']}\\" print(output) return output"},{"question":"**Title: Implementing and Utilizing Python Sequence Protocol** **Objective:** The objective of this question is to assess your understanding and ability to work with Python\'s Sequence Protocol, particularly in custom classes. You will demonstrate how to define and use a custom class that complies with the sequence protocol to interact with objects as sequences. **Problem Statement:** You are required to implement a custom class `MySequence` that mimics a sequence (like a list or tuple) using Python\'s sequence protocol. Your class should support: 1. Indexing and slicing 2. Length determination 3. Containment checks 4. Iteration over elements Implement the following methods in the `MySequence` class: - `__getitem__(self, index)` - `__setitem__(self, index, value)` - `__delitem__(self, index)` - `__len__(self)` - `__contains__(self, item)` - `__iter__(self)` **Requirements:** 1. **Initialization:** - The class should be initialized with a list of elements. - Example initialization: `seq = MySequence([1, 2, 3, 4, 5])` 2. **Indexing and Slicing (`__getitem__` method):** - Support positive and negative indexing, returning the item at the given position. - If an invalid index is provided, raise an `IndexError`. - Support slicing, returning a new instance of `MySequence` containing the sliced elements. 3. **Item Assignment and Deletion (`__setitem__` and `__delitem__` methods):** - Support updating the value at a given index. - Allow deleting an item at a given index. - Both methods should handle `IndexError` if the index is invalid. 4. **Length (`__len__` method):** - Return the total number of elements in the sequence. 5. **Containment (`__contains__` method):** - Check if an item exists in the sequence. 6. **Iteration (`__iter__` method):** - Return an iterator over the elements of the sequence. **Constraints:** - The sequence will consist of a maximum of 10^4 elements. - Each element can be an integer, string, or any Python object. **Performance Requirements:** - The implementation should be efficient in terms of both time and space complexity for typical sequence operations. **Example:** ```python # Initialize sequence seq = MySequence([1, 2, 3, 4, 5]) # Indexing print(seq[1]) # Output: 2 print(seq[-1]) # Output: 5 # Slicing print(seq[1:3]) # Output: MySequence([2, 3]) # Length print(len(seq)) # Output: 5 # Containment print(3 in seq) # Output: True print(6 in seq) # Output: False # Iteration for item in seq: print(item) # Output: 1 2 3 4 5 # Item assignment seq[0] = 10 print(seq[0]) # Output: 10 # Item deletion del seq[0] print(len(seq)) # Output: 4 ``` Implement the `MySequence` class by adhering to the above requirements, making sure your methods are efficient and handle exceptions appropriately.","solution":"class MySequence: def __init__(self, initial_data): if not isinstance(initial_data, list): raise TypeError(\\"Initial data must be a list\\") self.data = initial_data def __getitem__(self, index): if isinstance(index, slice): return MySequence(self.data[index]) if index < 0: index += len(self.data) if index < 0 or index >= len(self.data): raise IndexError(\\"Index out of range\\") return self.data[index] def __setitem__(self, index, value): if index < 0: index += len(self.data) if index < 0 or index >= len(self.data): raise IndexError(\\"Index out of range\\") self.data[index] = value def __delitem__(self, index): if index < 0: index += len(self.data) if index < 0 or index >= len(self.data): raise IndexError(\\"Index out of range\\") del self.data[index] def __len__(self): return len(self.data) def __contains__(self, item): return item in self.data def __iter__(self): return iter(self.data)"},{"question":"# Advanced Coding Assessment: Custom File Processor You are required to implement a custom file processor in Python that reads data from a text file, processes it, and writes the processed data to a new file. The processing operation should involve both reading and writing text and binary data with efficient use of buffered I/O. Task Write a function `custom_file_processor(input_text_file: str, output_text_file: str, binary_output_file: str) -> None` that performs the following operations: 1. **Read**: Open the given `input_text_file` (which is encoded in UTF-8) and read its content line by line. 2. **Process**: For each line in the file: - Reverse the text of the line. - Convert the reversed text to uppercase. - Convert the text to binary data (bytes) using UTF-8 encoding. 3. **Write**: - Write the reversed and uppercased text into `output_text_file` encoded in UTF-8. - Write the binary data of each processed line into `binary_output_file` using binary I/O. Input - `input_text_file`: Path to the input text file (UTF-8). - `output_text_file`: Path to the output text file where processed text will be stored (UTF-8). - `binary_output_file`: Path to the output binary file where binary data of processed text will be stored. Output The function does not return any value, but it creates/updates two files: - `output_text_file` with reversed and uppercased text. - `binary_output_file` with binary data of the processed text. Constraints - Ensure to handle potential I/O errors gracefully. - Efficiently manage I/O operations using buffered streams where appropriate. - Adhere to the encoding specifications for text I/O operations. Example Assume `input.txt` contains: ``` hello world Python 3.10 ``` After processing, `output.txt` should contain: ``` DLROW OLLEH 01.3 NOHTYP ``` And `output.bin` should contain the binary representation of the processed text: ``` b\'DLROW OLLEHn01.3 NOHTYPn\' (Note: This is for illustrative purposes, actual binary content) ``` Implementation Notes - Use `io.TextIOWrapper` for text reading and writing with specified encoding. - Use `io.BufferedWriter` or `io.BytesIO` for binary I/O operations. - Ensure robust error handling for file operations.","solution":"def custom_file_processor(input_text_file: str, output_text_file: str, binary_output_file: str) -> None: import io try: # Open the input text file for reading (UTF-8 encoding) with io.open(input_text_file, \'r\', encoding=\'utf-8\') as infile: # Open the output text file for writing (UTF-8 encoding) with io.open(output_text_file, \'w\', encoding=\'utf-8\') as text_outfile: # Open the output binary file for writing (binary mode) with io.open(binary_output_file, \'wb\') as binary_outfile: # Process each line from the input file for line in infile: # Strip the newline character from the line line = line.rstrip(\'n\') # Reverse the text of the line and convert to uppercase processed_text = line[::-1].upper() # Write the processed text to the output text file text_outfile.write(processed_text + \'n\') # Convert processed text to binary data (bytes using UTF-8 encoding) binary_data = (processed_text + \'n\').encode(\'utf-8\') # Write the binary data to the binary output file binary_outfile.write(binary_data) except IOError as e: # Handle IOError which includes file not found error and others print(f\\"An IOError occurred: {str(e)}\\")"},{"question":"**Problem Statement: Building and Evaluating a Machine Learning Model with Custom Cross-Validation** You are provided with a dataset `X` and its corresponding labels `y`. Implement a function to perform the following tasks: 1. **Data Splitting**: Split the dataset into training and testing sets using a 70-30 split. 2. **Data Standardization**: Standardize the training and testing sets. 3. **Model Training and Evaluation**: Train an SVM classifier using a linear kernel on the training set and evaluate its performance using 5-fold cross-validation. Report the mean accuracy and the standard deviation of the accuracy. 4. **Custom Cross-Validation Strategy**: Implement a custom cross-validation iterator where you define a 3-fold cross-validation strategy, but each fold should ensure that at least one example of each class is present in both the training and validation sets. 5. **Custom Cross-Validation Evaluation**: Use the custom cross-validation strategy to evaluate the same SVM classifier and report its performance. **Function Signature:** ```python def evaluate_model_with_cross_validation(X: np.ndarray, y: np.ndarray): pass ``` **Detailed Requirements:** 1. Data Splitting: - Use `train_test_split` from sklearn to split the dataset into training (70%) and testing (30%) sets. 2. Data Standardization: - Standardize the features using `StandardScaler` from sklearn on both training and testing sets. 3. Model Training and Evaluation using 5-Fold Cross-Validation: - Train an `SVM` classifier with a linear kernel. - Use `cross_val_score` with 5-fold cross-validation to evaluate the model. - Report the mean accuracy and standard deviation of the accuracy. 4. Custom Cross-Validation Strategy: - Implement a custom generator function for 3-fold cross-validation. - Ensure that each fold contains at least one sample from each class. 5. Evaluate Model with Custom Cross-Validation: - Use the custom cross-validation strategy to evaluate the SVM model. - Report the mean accuracy and standard deviation of the accuracy using your custom method. **Example:** ```python import numpy as np from sklearn import datasets # Load a sample dataset (e.g., Iris dataset) X, y = datasets.load_iris(return_X_y=True) # Call the function evaluate_model_with_cross_validation(X, y) ``` **Constraints:** - You must not use any predefined cross-validation classes (like `KFold`, `StratifiedKFold`, etc.) for the custom cross-validation part. - Ensure reproducibility by setting a random seed where necessary. **Expected Output:** The function should print/output: 1. Mean accuracy and standard deviation of the accuracy using 5-fold cross-validation. 2. Mean accuracy and standard deviation of the accuracy using the custom cross-validation strategy.","solution":"import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC def evaluate_model_with_cross_validation(X: np.ndarray, y: np.ndarray): # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train SVM classifier clf = SVC(kernel=\'linear\', random_state=42) # Perform 5-fold cross-validation scores = cross_val_score(clf, X_train, y_train, cv=5, scoring=\'accuracy\') print(f\\"5-Fold CV Mean Accuracy: {scores.mean():.4f}\\") print(f\\"5-Fold CV Std Deviation: {scores.std():.4f}\\") # Custom 3-fold cross validation def custom_cross_val(): # Custom splits ensuring at least one sample from each class in each fold labels = np.unique(y_train) fold_indices = {label: np.array_split(np.where(y_train == label)[0], 3) for label in labels} for i in range(3): training_indices = [] validation_indices = [] for label, splits in fold_indices.items(): validation_indices += list(splits[i]) for j, split in enumerate(splits): if j != i: training_indices += list(split) yield training_indices, validation_indices custom_scores = [] for train_idx, val_idx in custom_cross_val(): clf.fit(X_train[train_idx], y_train[train_idx]) score = clf.score(X_train[val_idx], y_train[val_idx]) custom_scores.append(score) custom_scores = np.array(custom_scores) print(f\\"Custom 3-Fold CV Mean Accuracy: {custom_scores.mean():.4f}\\") print(f\\"Custom 3-Fold CV Std Deviation: {custom_scores.std():.4f}\\") # Example usage if __name__ == \\"__main__\\": X, y = datasets.load_iris(return_X_y=True) evaluate_model_with_cross_validation(X, y)"},{"question":"# Question: Implementing ZIP-Based Module Importer Objective You need to implement a utility function that emulates module import functionality from a ZIP file. The function should handle the following tasks: 1. Read and list the contents of the ZIP file. 2. Dynamically load a Python module (`*.py` or `*.pyc`) from the ZIP file without extracting it to the filesystem. 3. Return the imported module if successfully loaded. Function Signature ```python def import_module_from_zip(zip_path: str, module_name: str): pass ``` Input - `zip_path` (str): The path to the ZIP file. - `module_name` (str): The fully qualified name (dotted path) of the module to import (e.g., `\\"my_package.my_module\\"`). Output - Returns the imported module if found and successfully loaded. - Raises an appropriate exception if the module cannot be imported. Constraints - The function should only support importing `*.py` and `*.pyc` files. - Do not use temporary file extraction. Modules must be loaded directly from the ZIP archive. - Handle both the cases where the ZIP archive may or may not contain precompiled `*.pyc` files. - Raise `zipimport.ZipImportError` for any issues specific to ZIP import. Examples ```python # Suppose \'example.zip\' contains a module at \'example_pkg/example_module.py\' imported_module = import_module_from_zip(\'example.zip\', \'example_pkg.example_module\') print(imported_module) # Should print information about the imported module ``` Notes - You may need to utilize the `zipimport` module\'s classes and methods such as `zipimport.zipimporter`. - Ensure to handle and raise relevant exceptions for issues like file not found, invalid ZIP path, etc.","solution":"import zipimport def import_module_from_zip(zip_path: str, module_name: str): Import a specified module from a ZIP file. Parameters: zip_path (str): Path to the ZIP file. module_name (str): Fully qualified name of the module to import. Returns: Imported module object if the module is successfully imported, otherwise raises an appropriate exception. try: importer = zipimport.zipimporter(zip_path) module = importer.load_module(module_name) return module except (FileNotFoundError, zipimport.ZipImportError) as e: raise zipimport.ZipImportError(f\\"Cannot import module \'{module_name}\' from ZIP path \'{zip_path}\': {e}\\")"},{"question":"**Objective:** Implement a class that utilizes various descriptor types to manage its attributes and methods in Python. **Problem Statement:** You are required to implement a class `ManagedClass` that uses descriptors to manage its attributes and methods. This class should: 1. Use a get-set descriptor to manage an attribute called `data`. 2. Use a member descriptor to manage an attribute called `counter`. 3. Use a method descriptor to define a method called `show`. 4. Use a class method descriptor to define a class method called `increment_counter`. **Requirements:** 1. **Get-Set Descriptor:** - Attribute: `data` - This should store the actual data and include getter and setter methods. 2. **Member Descriptor:** - Attribute: `counter` - This should be an integer attribute that keeps track of increments. 3. **Method Descriptor:** - Method: `show` - This should print the current value of `data`. 4. **Class Method Descriptor:** - Method: `increment_counter` - This should increment the `counter` class attribute by a specified amount. **Input and Output:** - The `ManagedClass` should be instantiated without any arguments. - Attributes and methods should be accessed and modified through the class and instance objects. **Example Usage:** ```python # Creating an instance of ManagedClass obj = ManagedClass() # Setting and getting \'data\' attribute obj.data = \\"Hello, World!\\" print(obj.data) # Output: Hello, World! # Using the \'show\' method obj.show() # Output: Hello, World! # Incrementing counter ManagedClass.increment_counter(5) print(ManagedClass.counter) # Output: 5 # Incrementing counter again ManagedClass.increment_counter(2) print(ManagedClass.counter) # Output: 7 ``` **Implementation Constraints:** - Use the provided descriptor functions from the documentation to implement these functionalities. - Ensure proper encapsulation and usage of descriptor protocols for managing attributes and methods. **Performance Considerations:** - The methods should be efficient and capable of handling standard attribute and method operations seamlessly. Note: This question requires a solid understanding of Python\'s descriptor protocol and how to implement and utilize descriptors for class and instance management.","solution":"class GetSetDescriptor: def __init__(self, initial_value=None): self.value = initial_value def __get__(self, instance, owner): return self.value def __set__(self, instance, value): self.value = value class MemberDescriptor: def __init__(self, initial_value=0): self.value = initial_value def __get__(self, instance, owner): return self.value def __set__(self, instance, value): self.value = value class MethodDescriptor: def __init__(self, func): self.func = func def __get__(self, instance, owner): return self.func.__get__(instance, owner) # Returning bound method class ClassMethodDescriptor: def __init__(self, func): self.func = func def __get__(self, instance, owner): return self.func.__get__(owner, owner) # Returning bound class method class ManagedClass: data = GetSetDescriptor() counter = MemberDescriptor() @MethodDescriptor def show(self): print(self.data) @ClassMethodDescriptor def increment_counter(cls, amount): cls.counter += amount"},{"question":"Objective: Implement a function to parse a given email string and identify any possible defects using the `email` package. The function should return a dictionary containing the types of defects found, if any. Requirements: - Import necessary modules and classes from the `email` package. - Handle and identify defects listed in the provided documentation. - Parse the email string using the `FeedParser`. - Return a dictionary where keys are defect class names, and values are counts of those defects found in the email. Input: - `email_str`: A string containing the raw email content. Output: - A dictionary with defect class names as keys and their occurrence counts as values. If no defects are found, return an empty dictionary. Constraints: - Handle large email strings efficiently. - Focus on accuracy in identifying and counting defects. Example Usage: ```python def parse_email_for_defects(email_str): # Your implementation here pass # Example email string with multiple defects email_str = From: sender@example.com To: recipient@example.com Subject: Example This is an example email with several defects. # Expected output could be (assuming the example string contained defects): # { # \\"StartBoundaryNotFoundDefect\\": 1, # \\"MissingHeaderBodySeparatorDefect\\": 1 # } print(parse_email_for_defects(email_str)) ``` Notes: - Utilize the documentation on the `email.errors` module to guide your defect identification. - Make sure to handle different types of defects and exceptions efficiently.","solution":"from email import message_from_string from email import errors from email.parser import FeedParser def parse_email_for_defects(email_str): parser = FeedParser() parser.feed(email_str) msg = parser.close() defect_counts = {} if msg.defects: for defect in msg.defects: defect_name = defect.__class__.__name__ if defect_name in defect_counts: defect_counts[defect_name] += 1 else: defect_counts[defect_name] = 1 return defect_counts"},{"question":"# Question: Implement a File Locker using `fcntl` module In this question, you are required to implement a Python function that uses the `fcntl` module to securely lock a region of a file for writing. This function will ensure that only one process can write to the specified region of the file at any given time. Function Signature: ```python def secure_write(filepath: str, start: int, length: int, data: bytes) -> None: pass ``` Parameters: - `filepath` (str): The path to the file to be locked and written to. - `start` (int): The starting byte position in the file where the lock should begin. - `length` (int): The length of the region to be locked. - `data` (bytes): The data to be written to the file starting from the `start` position. Expected Behavior: 1. **File Opening**: Open the file specified by `filepath` in read-write mode. 2. **File Locking**: Use an exclusive lock to lock the region of the file from `start` to `start + length`. 3. **Writing Data**: Write the `data` to the file starting from the `start` position. 4. **Unlocking**: Release the lock on the file region after writing is complete. 5. **Error Handling**: If another process holds the lock, an exception with an appropriate error message should be raised. Constraints: 1. The file must already exist. 2. The length of `data` must not exceed `length` to ensure that the locked region bounds are maintained. Example: Suppose you have a file `example.txt` with some content, and you want to securely write `b\\"Hello, World!\\"` starting from byte 10, and the region to be locked is of length 20. ```python # Example usage secure_write(\\"example.txt\\", 10, 20, b\\"Hello, World!\\") ``` After running the above code, the content of `example.txt` should be modified starting from byte 10 with `b\\"Hello, World!\\"`. Ensure that no other process can modify the region [10, 30) while your process holds the lock. # Note: Make use of the `fcntl` and `os` modules to perform the necessary operations.","solution":"import os import fcntl def secure_write(filepath: str, start: int, length: int, data: bytes) -> None: Securely write data to a file, ensuring the region is locked during the write. :param filepath: The path to the file to be locked and written to. :param start: The starting byte position in the file where the lock should begin. :param length: The length of the region to be locked. :param data: The data to be written to the file starting from the start position. if len(data) > length: raise ValueError(\\"Data length exceeds the specified region length.\\") with os.fdopen(os.open(filepath, os.O_RDWR), \'r+b\') as f: # Lock the specified region try: fcntl.lockf(f, fcntl.LOCK_EX, length, start) f.seek(start) f.write(data) f.flush() finally: # Ensure the lock is released fcntl.lockf(f, fcntl.LOCK_UN, length, start)"},{"question":"**Title: Event Scheduler with Statistics** **Objective:** Implement a Python class, `EventScheduler`, that schedules events, computes and displays a summary of the events by leveraging `datetime`, `collections`, and `heapq` modules. **Description:** You need to design a class, `EventScheduler`, that can: 1. **Add Events**: - Each event has a name (string), a start time (datetime), and a duration (timedelta). 2. **Get Events**: - Retrieve a list of events occurring on a given date. 3. **Display Summary**: - Provide a summary of events scheduled by different categories: - Most frequent event name. - A list of events sorted by the start time. - The event with the longest duration. - Total number of events scheduled. # Requirements: 1. **Initialization**: - Initialize the event scheduler with no events. 2. **Methods**: - `add_event(name: str, start_time: datetime, duration: timedelta) -> None` - Adds an event to the scheduler. - `get_events(date: datetime.date) -> List[Dict[str, Any]]` - Returns a list of events occurring on the specified date. Each event in the list should be a dictionary with keys: `name`, `start_time`, and `duration`. - `summary() -> Dict[str, Any]` - Returns a summary dictionary with: - `most_frequent_event`: the name of the most frequent event. - `sorted_events`: a list of event dictionaries sorted by start time. - `longest_event`: the name of the event with the longest duration. - `total_events`: the total number of events scheduled. **Constraints:** - Each event name will be a non-empty string. - Start times will not overlap. - Duration will be positive and representable using `timedelta`. **Example Usage:** ```python from datetime import datetime, timedelta # Create an instance of EventScheduler scheduler = EventScheduler() # Adding events scheduler.add_event(\'Meeting\', datetime(2023, 10, 1, 9, 0), timedelta(hours=1)) scheduler.add_event(\'Workshop\', datetime(2023, 10, 1, 10, 0), timedelta(hours=2)) scheduler.add_event(\'Meeting\', datetime(2023, 10, 2, 9, 0), timedelta(hours=1)) # Retrieve events for a specific date events_on_oct_1 = scheduler.get_events(datetime(2023, 10, 1).date()) # Display summary event_summary = scheduler.summary() ``` **Expected Output:** ```python # events_on_oct_1 [ {\'name\': \'Meeting\', \'start_time\': datetime(2023, 10, 1, 9, 0), \'duration\': timedelta(hours=1)}, {\'name\': \'Workshop\', \'start_time\': datetime(2023, 10, 1, 10, 0), \'duration\': timedelta(hours=2)} ] # event_summary { \'most_frequent_event\': \'Meeting\', \'sorted_events\': [ {\'name\': \'Meeting\', \'start_time\': datetime(2023, 10, 1, 9, 0), \'duration\': timedelta(hours=1)}, {\'name\': \'Workshop\', \'start_time\': datetime(2023, 10, 1, 10, 0), \'duration\': timedelta(hours=2)}, {\'name\': \'Meeting\', \'start_time\': datetime(2023, 10, 2, 9, 0), \'duration\': timedelta(hours=1)} ], \'longest_event\': \'Workshop\', \'total_events\': 3 } ``` **Note**: Use collections like `defaultdict` or `Counter`, and `heapq` where they offer an advantage in implementation.","solution":"from datetime import datetime, timedelta from collections import defaultdict, Counter import heapq class EventScheduler: def __init__(self): self.events = [] def add_event(self, name: str, start_time: datetime, duration: timedelta) -> None: heapq.heappush(self.events, (start_time, name, duration)) def get_events(self, date: datetime.date) -> list: return [ {\\"name\\": name, \\"start_time\\": start, \\"duration\\": duration} for start, name, duration in self.events if start.date() == date ] def summary(self) -> dict: event_counter = Counter(name for _, name, _ in self.events) most_frequent_event = event_counter.most_common(1)[0][0] if event_counter else None sorted_events = sorted(self.events, key=lambda x: x[0]) sorted_events_dicts = [ {\\"name\\": name, \\"start_time\\": start, \\"duration\\": duration} for start, name, duration in sorted_events ] longest_event = max(self.events, key=lambda x: x[2], default=[None, None, timedelta(0)])[1] total_events = len(self.events) return { \\"most_frequent_event\\": most_frequent_event, \\"sorted_events\\": sorted_events_dicts, \\"longest_event\\": longest_event, \\"total_events\\": total_events }"},{"question":"**Objective:** Create a Python script that can compile multiple Python source files to byte-code, handle errors gracefully, and support different bytecode invalidation modes. **Task:** Write a function `compile_sources(source_files: List[str], bytecode_files: List[str], pyc_mode: str, quiet: bool) -> dict` that: 1. Compiles a list of Python source files to byte-code files. 2. Supports different invalidation modes (`TIMESTAMP`, `CHECKED_HASH`, `UNCHECKED_HASH`). 3. Handles errors in a user-friendly manner. 4. Reports compilation results clearly. **Function Signature:** ```python from typing import List def compile_sources(source_files: List[str], bytecode_files: List[str], pyc_mode: str, quiet: bool) -> dict: pass ``` **Parameters:** - `source_files`: A list of paths to the Python source files to be compiled. - `bytecode_files`: A list of paths where the compiled bytecode files should be placed. - `pyc_mode`: A string indicating the invalidation mode to be used. It can be one of `TIMESTAMP`, `CHECKED_HASH`, or `UNCHECKED_HASH`. - `quiet`: A boolean indicating whether to suppress the error output (if True, errors are suppressed). **Returns:** - A dictionary where each key is the source file path and the value is: - The bytecode file path if compilation is successful. - An error message if compilation fails. **Constraints:** - The length of `source_files` and `bytecode_files` will be the same. - The values in `pyc_mode` should be among `TIMESTAMP`, `CHECKED_HASH`, and `UNCHECKED_HASH`. **Example:** ```python source_files = [\'example1.py\', \'example2.py\'] bytecode_files = [\'example1.pyc\', \'example2.pyc\'] pyc_mode = \'CHECKED_HASH\' quiet = True result = compile_sources(source_files, bytecode_files, pyc_mode, quiet) # Expected output format: # { # \'example1.py\': \'example1.pyc\', # \'example2.py\': \'Failed to compile due to some error\' # } ``` **Notes:** - Use the `py_compile` module to perform the compilation. - Use the `py_compile.PycInvalidationMode` enum to handle the invalidation modes. - Handle any exceptions by providing a descriptive error message for each file that fails to compile. - Ensure that proper error handling and logging are implemented if `quiet` is False.","solution":"import py_compile from typing import List, Dict def compile_sources(source_files: List[str], bytecode_files: List[str], pyc_mode: str, quiet: bool) -> Dict[str, str]: if pyc_mode not in (\\"TIMESTAMP\\", \\"CHECKED_HASH\\", \\"UNCHECKED_HASH\\"): raise ValueError(\\"Invalid pyc_mode. Must be \'TIMESTAMP\', \'CHECKED_HASH\', or \'UNCHECKED_HASH\'.\\") pyc_mode_enum = { \\"TIMESTAMP\\": py_compile.PycInvalidationMode.TIMESTAMP, \\"CHECKED_HASH\\": py_compile.PycInvalidationMode.CHECKED_HASH, \\"UNCHECKED_HASH\\": py_compile.PycInvalidationMode.UNCHECKED_HASH }[pyc_mode] results = {} for source, bytecode in zip(source_files, bytecode_files): try: py_compile.compile(file=source, cfile=bytecode, doraise=True, invalidation_mode=pyc_mode_enum) results[source] = bytecode except Exception as e: error_message = f\\"Failed to compile {source} due to: {e}\\" if not quiet: print(error_message) results[source] = error_message return results"},{"question":"# Advanced Python Protocol Implementation Objective Design a Python function that utilizes multiple object interaction protocols effectively. This exercise aims to assess your understanding of the Abstract Objects Layer in Python and your capability to work with different object types. Problem Statement You need to implement a Python class `CustomCollection` that supports the following functionality: 1. **Initialization**: The class should be initialized with an iterable. 2. **Support for Protocols**: - **Sequence Protocol**: The class should support sequence-like behavior, i.e., indexing and slicing. - **Iterator Protocol**: The class should be iterable. - **Mapping Protocol**: The class should support key-value access similar to a dictionary. - **Number Protocol**: The class should implement addition with another `CustomCollection` object, combining their elements. Requirements 1. **Initialization**: Accept an iterable during initialization and store it in a suitable structure. 2. **Sequence Protocol**: Implement `__getitem__` and `__len__`. 3. **Iterator Protocol**: Implement `__iter__`. 4. **Mapping Protocol**: Implement `__getitem__` and `__setitem__` to allow key-value access. 5. **Number Protocol**: Implement `__add__` to combine two `CustomCollection` objects. Constraints - You can assume that elements in the collection are unique. - The key-value mapping should map elements to their indices. - The combined collection should retain the original order of elements from both collections when added. - Your implementation should handle possible exceptions and edge cases. Example Usage ```python # Initialization collection = CustomCollection([1, 2, 3, 4]) # Sequence Protocol print(collection[2]) # Output: 3 print(len(collection)) # Output: 4 # Iterator Protocol for item in collection: print(item) # Output: 1 2 3 4 # Mapping Protocol print(collection[1]) # Output: 0 (element \'1\' at index 0) collection[5] = 4 print(collection[5]) # Output: 4 # Number Protocol new_collection = CustomCollection([5, 6]) combined_collection = collection + new_collection print(list(combined_collection)) # Output: [1, 2, 3, 4, 5, 6] ``` Submission Requirements - Implement the `CustomCollection` class with the required protocols. - Provide necessary exception handling. - Ensure your code is clean and well-documented.","solution":"class CustomCollection: def __init__(self, iterable): self.data = list(iterable) self.index_map = {item: idx for idx, item in enumerate(self.data)} def __getitem__(self, key): if isinstance(key, int): return self.data[key] elif key in self.index_map: return self.index_map[key] else: raise KeyError(f\\"Key \'{key}\' not found in collection.\\") def __setitem__(self, key, value): if key not in self.data: self.data.append(key) self.index_map[key] = value else: self.index_map[key] = value def __len__(self): return len(self.data) def __iter__(self): return iter(self.data) def __add__(self, other): if not isinstance(other, CustomCollection): raise TypeError(\\"Can only add CustomCollection to CustomCollection.\\") combined_data = self.data + other.data return CustomCollection(combined_data)"},{"question":"# Pandas Display Options Customization Problem Statement You\'ve been given a data export task where you need to convert several DataFrames into a readable and concise format for reporting purposes. The format should adhere to specified display rules to ensure consistency and clarity. Using the pandas options API, your task is to create a function that takes in a dictionary of DataFrames and a set of options, and returns the formatted data as strings that abide by the given display settings. Function Signature ```python def format_dataframes(dataframes: dict, options: dict) -> dict: Formats a set of DataFrames according to the specified display options and returns them as strings. Parameters: - dataframes (dict): A dictionary where keys are DataFrame names (str) and values are pandas DataFrame objects. - options (dict): A dictionary of pandas display options and their desired values. Returns: - dict: A dictionary where keys are DataFrame names (str) and values are formatted DataFrame representations (str). pass ``` Input - `dataframes`: A dictionary containing named pandas DataFrame objects. - `options`: A dictionary with keys as pandas display options (e.g., \\"display.max_rows\\", \\"display.precision\\") and their desired values. Output - A dictionary with the same keys as the input `dataframes`, where the values are string representations of the DataFrames formatted according to the specified options. Constraints - You can assume all DataFrame names and display option names are valid strings. - You should reset the display options to their defaults after formatting. Example ```python import pandas as pd df1 = pd.DataFrame({ \\"A\\": [1, 2, 3, 4, 5], \\"B\\": [6, 7, 8, 9, 10] }) df2 = pd.DataFrame({ \\"X\\": [\\"foo\\", \\"bar\\", \\"baz\\", \\"qux\\"], \\"Y\\": [0.123456, 0.234567, 0.345678, 0.456789] }) dataframes = {\\"df1\\": df1, \\"df2\\": df2} options = { \\"display.max_rows\\": 3, \\"display.precision\\": 3, \\"display.max_colwidth\\": 4 } formatted_dfs = format_dataframes(dataframes, options) print(formatted_dfs[\\"df1\\"]) print(formatted_dfs[\\"df2\\"]) ``` In the example above, the function `format_dataframes` should adjust the display settings so that: 1. Only a maximum of 3 rows and columns of DataFrames are shown, 2. Numbers are displayed with a precision of 3 decimal places, 3. Column values are truncated if they exceed 4 characters in width. Ensure that after processing the input DataFrames, you reset the pandas display options to their default values to avoid affecting subsequent operations. Notes - Consider using `pd.option_context` for temporarily setting the display options to format the DataFrames. - To convert DataFrames to strings with formatting, you can use the `to_string` method of pandas DataFrame objects.","solution":"import pandas as pd def format_dataframes(dataframes: dict, options: dict) -> dict: Formats a set of DataFrames according to the specified display options and returns them as strings. Parameters: - dataframes (dict): A dictionary where keys are DataFrame names (str) and values are pandas DataFrame objects. - options (dict): A dictionary of pandas display options and their desired values. Returns: - dict: A dictionary where keys are DataFrame names (str) and values are formatted DataFrame representations (str). formatted_dfs = {} with pd.option_context(*sum(options.items(), ())): for name, df in dataframes.items(): formatted_dfs[name] = df.to_string(index=False) return formatted_dfs"},{"question":"**Assessment Question: String Encoding and Decoding** # Objective Your task is to implement a function in Python that demonstrates the student\'s understanding of string conversion, formatting, and handling Unicode encodings. The function should take a Unicode string, convert it to a specific encoding, and then decode it back to Unicode. Additionally, the function should handle any potential errors during the conversion process. # Function Signature ```python def convert_string(input_string: str, encoding: str) -> str: pass ``` # Input - `input_string` (str): A Unicode string that needs to be converted. - `encoding` (str): The encoding format to convert the string to, e.g., \'utf-8\', \'ascii\', \'latin-1\'. # Output - Returns the decoded Unicode string after encoding and decoding the `input_string` using the specified `encoding`. # Constraints - The function should handle any `UnicodeEncodeError` or `UnicodeDecodeError` that may arise during the conversion process. - If an error occurs during encoding or decoding, the function should return the string `\\"Error: Encoding/Decoding failed\\"`. # Example ```python # Example 1 input_string = \\"Hello, World!\\" encoding = \\"utf-8\\" print(convert_string(input_string, encoding)) # Output: \\"Hello, World!\\" # Example 2 input_string = \\", !\\" encoding = \\"ascii\\" print(convert_string(input_string, encoding)) # Output: \\"Error: Encoding/Decoding failed\\" # Example 3 input_string = \\"Hello, World!\\" encoding = \\"latin-1\\" print(convert_string(input_string, encoding)) # Output: \\"Hello, World!\\" ``` # Instructions 1. Implement the `convert_string` function. 2. Ensure your function handles Unicode encoding and decoding effectively. 3. Test your function with different encodings and Unicode characters. 4. Handle any errors during the conversion process gracefully. # Performance Requirements - The function should efficiently handle strings up to 10,000 characters in length. - The function should complete within a reasonable time frame for the provided constraints. # Hints - You may use `str.encode(encoding, errors=\'strict\')` for encoding. - You may use `bytes.decode(encoding, errors=\'strict\')` for decoding. - Handling errors using a try-except block might simplify error management.","solution":"def convert_string(input_string: str, encoding: str) -> str: try: # Encode the input string to the specified encoding encoded_bytes = input_string.encode(encoding) # Decode the encoded bytes back to a string decoded_string = encoded_bytes.decode(encoding) return decoded_string except (UnicodeEncodeError, UnicodeDecodeError): return \\"Error: Encoding/Decoding failed\\""},{"question":"# Nearest Neighbors Classification and Optimization You are tasked with creating a Nearest Neighbors classifier for a given dataset and optimizing the performance using different algorithms and distance metrics. Follow the steps below to complete this task: 1. **Data Preparation**: - Load the Iris dataset using `sklearn.datasets.load_iris`. 2. **Model Implementation**: - Implement a K-Nearest Neighbors (KNN) Classifier using `KNeighborsClassifier` from `sklearn.neighbors`. - Split the data into train and test sets using `sklearn.model_selection.train_test_split`. - Train the KNN classifier on the training set. - Use the model to predict labels for the test set. 3. **Evaluation**: - Calculate the accuracy of the classifier using `sklearn.metrics.accuracy_score`. 4. **Optimization**: - Compare the performance of the classifier using different nearest neighbor algorithms: `auto`, `ball_tree`, `kd_tree`, and `brute`. - Compare the performance of the classifier using different distance metrics: `euclidean`, `manhattan`, and `chebyshev`. 5. **Visualization**: - Create a 2D scatter plot of the dataset using two features (e.g., `sepal length` and `sepal width`), and visualize the decision boundaries of the KNN classifier. **Input**: - The input to the function is the dataset and the number of nearest neighbors `k`. **Output**: - The function should return a dictionary containing the accuracy scores for different algorithms and distance metrics. # Constraints: - Use `k=3` for the KNN classifier. - Use 70% of the data for training and 30% for testing. - Visualize using only the first two features of the dataset for simplicity. # Performance Requirements: - The implementation should handle the classification efficiently and should compare the performances in a meaningful way. ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt def knn_classification_and_optimization(k): # Load Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) results = {} algorithms = [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'] metrics = [\'euclidean\', \'manhattan\', \'chebyshev\'] for algorithm in algorithms: for metric in metrics: # Create and train the KNN classifier knn = KNeighborsClassifier(n_neighbors=k, algorithm=algorithm, metric=metric) knn.fit(X_train, y_train) # Predict and evaluate y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Store the result results[(algorithm, metric)] = accuracy # Visualize decision boundaries (using only first two features for simplicity) plt.figure(figsize=(10, 6)) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor=\'k\') plt.xlabel(\'Sepal length\') plt.ylabel(\'Sepal width\') plt.title(\'2D Scatter plot of the Iris dataset\') plt.show() return results # Example usage k = 3 results = knn_classification_and_optimization(k) for key, accuracy in results.items(): print(f\'Algorithm: {key[0]}, Metric: {key[1]}, Accuracy: {accuracy:.4f}\') ```","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt def knn_classification_and_optimization(k): # Load Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) results = {} algorithms = [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'] metrics = [\'euclidean\', \'manhattan\', \'chebyshev\'] for algorithm in algorithms: for metric in metrics: # Create and train the KNN classifier knn = KNeighborsClassifier(n_neighbors=k, algorithm=algorithm, metric=metric) knn.fit(X_train, y_train) # Predict and evaluate y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Store the result results[(algorithm, metric)] = accuracy # Visualize decision boundaries (using only first two features for simplicity) plt.figure(figsize=(10, 6)) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor=\'k\') plt.xlabel(\'Sepal length\') plt.ylabel(\'Sepal width\') plt.title(\'2D Scatter plot of the Iris dataset\') plt.show() return results # Example usage k = 3 results = knn_classification_and_optimization(k) for key, accuracy in results.items(): print(f\'Algorithm: {key[0]}, Metric: {key[1]}, Accuracy: {accuracy:.4f}\')"},{"question":"**Question: Advanced Scatter Plot Customization with Seaborn** You are provided with the \\"tips\\" dataset that contains information about the total bill, tip amount, and other attributes collected from a restaurant. Your task is to create a scatter plot using seaborn with the following requirements: 1. Use the \\"tips\\" dataset and load it using `sns.load_dataset(\\"tips\\")`. 2. Create a scatter plot with: - `total_bill` on the x-axis. - `tip` on the y-axis. - Differentiate points by the time of day (Lunch/Dinner) using different marker styles. - Color the points by the day of the week (Thursday, Friday, Saturday, Sunday). - Use the `size` variable to represent the party size (`size` column). 3. Customize the plot: - Set the size range for points from 30 to 300. - Ensure that all unique days appear in the legend. - Use a dictionary to set marker styles for \\"Lunch\\" to \'o\' and \\"Dinner\\" to \'D\'. - Use a specific color palette for `hue` (use any named palette, e.g., \\"Set2\\"). - Set the title of the plot to \\"Scatter Plot of Tips by Total Bill\\". **Expected Output:** Your solution should include the following key elements: - Proper data loading and preparation. - A scatter plot that meets all the specified requirements. - The plot should display with the customizations and legend as described. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create scatter plot with required customizations sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", sizes=(30, 300), legend=\\"full\\", palette=\\"Set2\\", markers={\\"Lunch\\": \\"o\\", \\"Dinner\\": \\"D\\"} ) # Set the title plt.title(\\"Scatter Plot of Tips by Total Bill\\") # Show the plot plt.show() ``` **Constraints and Requirements:** - Ensure that you follow the seaborn and matplotlib conventions. - Handle potential edge cases, such as missing data. - Your code should be efficient and optimized for performance. - Use clear and readable code practices. **Performance Note:** While performance is not the primary concern for this small dataset, ensure your solution is scalable and follows best coding practices.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Define marker styles marker_styles = {\\"Lunch\\": \\"o\\", \\"Dinner\\": \\"D\\"} # Create scatter plot with required customizations scatterplot = sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", sizes=(30, 300), legend=\\"full\\", palette=\\"Set2\\", markers=marker_styles ) # Set the title plt.title(\\"Scatter Plot of Tips by Total Bill\\") # Show the plot plt.show()"},{"question":"# Custom Interactive Command Handler with History and Completion **Objective:** Implement a custom interactive command handler in Python that mimics some functionalities of the interactive Python interpreter\'s input editing and history substitution features using a specified list of commands. The handler should provide a simple tab-completion for these commands and history management of user inputs. # Task: You need to implement a Python class `InteractiveHandler` with the following functionalities: 1. **Initialization:** - Accepts a list of valid commands as an argument. 2. **Methods:** - `input_handler(prompt: str) -> str`: Displays the prompt, captures user input, and supports up-arrow to browse history and tab for auto-completion of commands. - `add_to_history(command: str) -> None`: Adds a command to the history list if it\'s not empty. - `get_history() -> List[str]`: Returns the list of previously entered commands. 3. **Command History and Completion Features:** - The handler should store user input commands and allow the user to scroll through the history with the up-arrow key. - When the user presses the tab key, it should auto-complete from the list of commands provided during initialization. **Expected input and output formats:** - Commands input should be strings. - History should be a list of command strings. # Constraints: - Commands are case-sensitive. - If a part of the command is matched with multiple commands from the list on pressing tab, display all possible completions. **Example Usage:** ```python handler = InteractiveHandler([\\"list\\", \\"load\\", \\"exit\\", \\"help\\"]) assert handler.input_handler(\\"Command: \\") # User types \\"l\\" and presses Tab -> Suggests [\\"list\\", \\"load\\"] assert handler.input_handler(\\"Command: \\") # User types \\"list\\" -> Adds to history assert handler.add_to_history(\\"load\\") assert handler.get_history() == [\\"list\\", \\"load\\"] ``` **Note:** - You may use terminal handling libraries like `readline` or `curses` for handling keyboard events. - The command history and command completion should be effectively managed. # Performance Requirements: - The handler should efficiently manage history in O(1) time complexity append operations and O(n) for retrieval where n is the number of stored commands.","solution":"import readline class InteractiveHandler: def __init__(self, commands): self.commands = commands self.history = [] self.setup_readline() def setup_readline(self): readline.set_completer(self.command_completer) readline.parse_and_bind(\'tab: complete\') def command_completer(self, text, state): matches = [command for command in self.commands if command.startswith(text)] try: return matches[state] except IndexError: return None def input_handler(self, prompt): user_input = input(prompt) if user_input: self.add_to_history(user_input) return user_input def add_to_history(self, command): if command: self.history.append(command) readline.add_history(command) def get_history(self): return self.history"},{"question":"# Custom Logging Handler Implementation **Objective:** Design and implement a custom logging handler in Python that extends the base `logging.Handler` class. Your handler should include custom logic for emitting log records, along with configuring multiple handlers (console and file) in a logging setup for comprehensive logging management. **Task:** 1. **CustomHandler Class:** - Create a new class `CustomHandler` that extends `logging.Handler`. - Implement the following methods in `CustomHandler`: - `__init__(self, custom_param, level=logging.NOTSET)`: Initialize the handler with a custom parameter and set the level. - `emit(self, record)`: Process log records. This method should prepend the custom parameter to the log message before printing it to the console. - `handleError(self, record)`: Handle exceptions that occur during an `emit()` call. 2. **Logger Configuration:** - Configure a logger that includes the `CustomHandler`, a `StreamHandler`, and a `FileHandler`. - Set up the `StreamHandler` to output to `sys.stdout`. - Configure the `FileHandler` to write logs to a file named `application.log`. - Ensure that log messages are formatted to include the timestamp, logger name, log level, and log message. 3. **Function Implementation:** - Implement a function `setup_logging(custom_param)` that takes a `custom_param` and: - Creates an instance of the logger. - Configures the logger with the `CustomHandler`, `StreamHandler`, and `FileHandler`. - Sets an appropriate log level for the logger. - Returns the configured logger instance. 4. **Logging Demonstration:** - Within the `setup_logging` function, add a log message at DEBUG level using the logger instance to demonstrate that logging is properly set up. # Input: - `custom_param`: A string to be prepended to the log message by the `CustomHandler`. # Output: - A configured logger instance capable of logging messages to the console and a file. # Constraints: - Use the `logging` module from Python\'s standard library. - Ensure the implemented handlers and loggers follow Python\'s standard conventions and practices for logging. - Handle potential exceptions gracefully to avoid the program crashing during logging. # Performance Requirements: - Ensure that logs are efficiently written to both the console and the file without significant performance overhead. # Example: ```python import logging class CustomHandler(logging.Handler): def __init__(self, custom_param, level=logging.NOTSET): super().__init__(level) self.custom_param = custom_param def emit(self, record): record.msg = f\\"{self.custom_param}: {record.msg}\\" print(self.format(record)) # For simplicity, we use print to simulate emitting the log record def handleError(self, record): print(f\\"Error handling log record: {record}\\") def setup_logging(custom_param): logger = logging.getLogger(\'customLogger\') logger.setLevel(logging.DEBUG) # CustomHandler with custom_param ch = CustomHandler(custom_param) ch.setFormatter(logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')) logger.addHandler(ch) # StreamHandler sh = logging.StreamHandler() sh.setFormatter(logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')) logger.addHandler(sh) # FileHandler fh = logging.FileHandler(\'application.log\') fh.setFormatter(logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')) logger.addHandler(fh) # Test the logger setup logger.debug(\\"Logger setup complete.\\") return logger # Example usage custom_logger = setup_logging(\\"CUSTOM\\") custom_logger.info(\\"This is a test log message.\\") ```","solution":"import logging import sys class CustomHandler(logging.Handler): def __init__(self, custom_param, level=logging.NOTSET): super().__init__(level) self.custom_param = custom_param def emit(self, record): try: record.msg = f\\"{self.custom_param}: {record.msg}\\" print(self.format(record), file=sys.stdout) except Exception: self.handleError(record) def handleError(self, record): print(f\\"Error handling log record: {record}\\", file=sys.stderr) def setup_logging(custom_param): logger = logging.getLogger(\'customLogger\') logger.setLevel(logging.DEBUG) # CustomHandler with custom_param ch = CustomHandler(custom_param) ch.setFormatter(logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')) logger.addHandler(ch) # StreamHandler to output to sys.stdout sh = logging.StreamHandler(sys.stdout) sh.setFormatter(logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')) logger.addHandler(sh) # FileHandler to write logs to \'application.log\' fh = logging.FileHandler(\'application.log\') fh.setFormatter(logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')) logger.addHandler(fh) # Test the logger setup logger.debug(\\"Logger setup complete.\\") return logger # Example usage custom_logger = setup_logging(\\"CUSTOM\\") custom_logger.info(\\"This is a test log message.\\")"},{"question":"Title: Working with tar archives in Python Objective: Create a Python function that compresses a list of files and directories into a tar archive with gzip compression. The function should also extract a specific subset of files from the archive, keeping the file hierarchy intact. Problem Statement: You are given a list of file and directory paths that need to be compressed into a single tar archive using gzip compression. After creating the archive, implement functionality to extract only the files that match a given list of file extensions while preserving the directory structure. Function Signature: ```python def create_and_extract_tar(archive_name: str, paths_to_compress: list, extensions_to_extract: list, extract_path: str) -> None: Create a gzip-compressed tar archive from the specified paths and extract files with given extensions. :param archive_name: The name of the tar archive to be created. :param paths_to_compress: A list of file and directory paths to include in the tar archive. :param extensions_to_extract: A list of file extensions (e.g., [\\".txt\\", \\".py\\"]) to extract from the created archive. :param extract_path: The directory to extract the specified files into. ``` Input: - `archive_name`: A string representing the name of the tar archive to be created, e.g., \\"archive.tar.gz\\". - `paths_to_compress`: A list of strings, each representing a path to a file or directory that should be added to the tar archive. - `extensions_to_extract`: A list of strings representing the file extensions to extract, e.g., [\\".txt\\", \\".py\\"]. - `extract_path`: A string representing the path to the directory where the files should be extracted. Output: The function does not return any value but should perform the following: 1. Create a gzip-compressed tar archive named `archive_name` that contains all the files and directories specified in `paths_to_compress`. 2. Extract only the files with extensions listed in `extensions_to_extract` from the created archive to the directory specified by `extract_path`, preserving the directory structure. Constraints: - You may assume that all file and directory paths specified in `paths_to_compress` exist and are accessible. - The function should handle various error scenarios gracefully, such as invalid file paths or errors during extraction, by logging appropriate error messages without stopping the entire process. - The extraction should preserve the original directory structure of the included files. Example Usage: ```python # Example input archive_name = \\"example_archive.tar.gz\\" paths_to_compress = [\\"file1.txt\\", \\"dir1\\", \\"file2.py\\", \\"dir2\\"] extensions_to_extract = [\\".txt\\", \\".py\\"] extract_path = \\"./extracted_files\\" # Function call create_and_extract_tar(archive_name, paths_to_compress, extensions_to_extract, extract_path) # Expected outcome: # - A tar.gz archive named \\"example_archive.tar.gz\\" containing the specified files and directories. # - Only the .txt and .py files are extracted to \\"./extracted_files\\" while preserving their directory structure. ``` # Notes: 1. Use the `tarfile` module for handling tar archives. 2. You might find the `TarFile.add()` and `TarFile.extractall()` methods particularly useful. 3. Make sure to implement error handling where necessary to ensure robustness. # Grading Criteria: - Correctness: The function should correctly create the archive and extract the specified files. - Usage of tarfile functionalities: Appropriate use of the various methods and features of the `tarfile` module. - Error Handling: Graceful handling of potential errors. - Code Quality: Clear, well-documented, and efficient code.","solution":"import tarfile import os def create_and_extract_tar(archive_name: str, paths_to_compress: list, extensions_to_extract: list, extract_path: str) -> None: Create a gzip-compressed tar archive from the specified paths and extract files with given extensions. :param archive_name: The name of the tar archive to be created. :param paths_to_compress: A list of file and directory paths to include in the tar archive. :param extensions_to_extract: A list of file extensions (e.g., [\\".txt\\", \\".py\\"]) to extract from the created archive. :param extract_path: The directory to extract the specified files into. try: # Create tar.gz archive with tarfile.open(archive_name, \\"w:gz\\") as tar: for path in paths_to_compress: tar.add(path, arcname=os.path.basename(path)) # Extract specific files from the archive with tarfile.open(archive_name, \\"r:gz\\") as tar: members = [member for member in tar.getmembers() if member.isfile() and any(member.name.endswith(ext) for ext in extensions_to_extract)] for member in members: member_path = os.path.join(extract_path, member.name) os.makedirs(os.path.dirname(member_path), exist_ok=True) tar.extract(member, path=extract_path) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Multi-step HTTP Request Processor You are required to create a Python function `fetch_and_parse_web_data(url: str, output_file: str) -> None` that performs a series of tasks to fetch and process data from a web page. The steps involved are: 1. **Fetching the Web Page**: Use the `urllib` module to send a request to the given URL and fetch the content of the web page. 2. **Parsing the HTML**: Extract all the URLs (links) found in the HTML content using regular expressions. Each link should be absolute or should be converted to absolute using the original base URL. 3. **Fetching Content from Links**: For each extracted URL, send another HTTP request and fetch its content. 4. **Storing the Data**: Write the URLs and their corresponding content into the specified output file in a structured format. Input - `url`: A string representing the URL of the web page to fetch and parse. - `output_file`: A string representing the file path where the output should be written. Output - The function does not return anything but writes the results to the specified output file. Constraints and Requirements - Use the `urllib` module to handle HTTP requests. - Ensure proper error handling for network issues and invalid URLs. - All URLs fetched should be logged in the output file along with their content. - Handle potential redirects appropriately using the appropriate handlers in `urllib`. Performance Requirements - The solution should aim to minimize network latency by handling URL fetching efficiently. - Ensure that your implementation handles a reasonable number of URLs (e.g., up to 100 links from the provided web page). Here is the function signature: ```python import urllib.request import urllib.error import urllib.parse import re def fetch_and_parse_web_data(url: str, output_file: str) -> None: # Your implementation goes here pass ``` # Example Suppose the input URL is `http://example.com` and one of the links in the page points to `http://example.com/about`. The `output_file` could have the following structure: ``` URL: http://example.com/about Content: <HTML content of the about page> ``` Include proper comments and documentation in your code.","solution":"import urllib.request import urllib.error import urllib.parse import re from urllib.parse import urljoin def fetch_and_parse_web_data(url: str, output_file: str) -> None: try: # Step 1: Fetch the Web Page response = urllib.request.urlopen(url) html_content = response.read().decode(\'utf-8\') # Step 2: Parsing the HTML to extract links links = re.findall(r\'href=[\\"\'](.*?)[\\"\']\', html_content) absolute_links = [urljoin(url, link) for link in links] # Step 3: Fetching content from each link content_dict = {} for link in absolute_links: try: link_response = urllib.request.urlopen(link) link_content = link_response.read().decode(\'utf-8\') content_dict[link] = link_content except urllib.error.URLError as e: content_dict[link] = f\\"Failed to fetch content: {e.reason}\\" # Step 4: Storing the data with open(output_file, \'w\') as file: for link, content in content_dict.items(): file.write(f\\"URL: {link}n\\") file.write(f\\"Content: {content}n\\") file.write(\\"n---nn\\") except urllib.error.URLError as e: print(f\\"Failed to fetch the main URL: {e.reason}\\")"},{"question":"# Advanced Turtle Graphics: Mandelbrot Set Visualization Objective: Create a program using the Python `turtle` module to visualize the Mandelbrot Set, a famous fractal. The Mandelbrot Set is a set of complex numbers that forms a fractal, defined iteratively and visualized in a 2D plane. Problem Statement: Implement the function `draw_mandelbrot` that uses turtle graphics to draw a visualization of the Mandelbrot Set. The function should accept parameters to customize the size and detail of the fractal image. Specifically, you will plot each point on a Cartesian plane and color it based on how quickly it diverges. Function Signature: ```python def draw_mandelbrot(width: int, height: int, x_center: float, y_center: float, zoom: float, max_iter: int) -> None: pass ``` Inputs: 1. `width` (int): The width of the canvas in pixels. 2. `height` (int): The height of the canvas in pixels. 3. `x_center` (float): The x coordinate of the center of the image in the complex plane. 4. `y_center` (float): The y coordinate of the center of the image in the complex plane. 5. `zoom` (float): A scaling factor that zooms into the fractal. Higher values result in more detail. 6. `max_iter` (int): The maximum number of iterations to determine if a point is in the Mandelbrot Set. Output: The function should use turtle graphics to create and display the Mandelbrot Set based on the provided parameters. Constraints: 1. Use the `turtle.Screen()` and `turtle.Turtle()` methods for drawing. 2. Optimally, use turtle\'s `speed()` method to speed up drawing. 3. Ensure the plot fits within the specified width and height of the canvas. 4. Utilize appropriate coloring to visualize the fractal, preferably color that changes based on the number of iterations. Example: To visualize the Mandelbrot set for typical parameters: ```python draw_mandelbrot(800, 600, -0.75, 0.0, 200, 50) ``` This function setup should create a window of 800x600 pixels centered at (-0.75, 0) with a zoom factor of 200 and using up to 50 iterations to determine membership in the set. Notes: - Each pixel in the canvas represents a complex number. Transform the pixel coordinates to complex number coordinates based on the parameters provided. - Color each pixel based on the number of iterations before the sequence defined by ( z_{n+1} = z_n^2 + c ) diverges, where ( z ) starts off at zero and ( c ) is the complex number corresponding to the pixel. Hints for implementation: - You might need to map the canvas coordinates to complex plane coordinates. - Utilize nested loops to iterate over each pixel in the drawing area. - Use turtle\'s color and movement commands effectively to plot the fractal. - Consider exploring and enhancing color gradient for better visual representation of the fractal.","solution":"import turtle def mandelbrot(c, max_iter): z = 0 n = 0 while abs(z) <= 2 and n < max_iter: z = z*z + c n += 1 return n def draw_mandelbrot(width, height, x_center, y_center, zoom, max_iter): screen = turtle.Screen() screen.setup(width, height) screen.setworldcoordinates(0, 0, width, height) drawer = turtle.Turtle() drawer.speed(0) # Fastest for x in range(width): for y in range(height): re = x_center + (x - width // 2) / zoom im = y_center + (y - height // 2) / zoom c = complex(re, im) m = mandelbrot(c, max_iter) color = 255 - int(m * 255 / max_iter) drawer.color((color, color, color)) drawer.penup() drawer.goto(x, y) drawer.pendown() drawer.dot(1) turtle.done()"},{"question":"# Naive Bayes Classifier Implementation and Performance Analysis Introduction In this task, you are required to implement a naive Bayes classifier to solve a classification problem using the scikit-learn library. You will need to load a dataset, preprocess it, train different naive Bayes models, evaluate their performance, and analyze the results. Dataset You will use the Wine dataset from the UCI Machine Learning Repository, which is available in the `sklearn.datasets` module. Task 1. **Data Loading and Preprocessing** - Load the Wine dataset using `sklearn.datasets.load_wine()`. - Split the dataset into 70% training and 30% testing data using `train_test_split`. 2. **Model Training and Evaluation** - Train and evaluate four different naive Bayes classifiers on the dataset: - GaussianNB - MultinomialNB - ComplementNB - BernoulliNB - For each classifier: - Train the model on the training data. - Make predictions on the testing data. - Calculate and print the accuracy score and confusion matrix for the predictions. 3. **Model Selection and Analysis** - Compare the performance of the four models in terms of accuracy. - Discuss the results and provide a recommendation on which model is more suitable for this dataset, including an explanation of why it performs better or worse than the others. Constraints - You should use `train_test_split` with a `random_state` of 42 for reproducibility. - Use appropriate hyperparameters for each Naive Bayes classifier. Note that some classifiers (e.g., MultinomialNB and BernoulliNB) work best with non-negative integer counts (like word counts), while GaussianNB works with continuous data. Preprocessing steps may be necessary to meet these requirements. Performance Requirements - Your solution should consider the memory and computational efficiency of the training and predictions. Expected Output - Print accuracy scores for each classifier. - Print confusion matrices for each classifier. - Provide a brief written analysis comparing the classifiers and a recommendation on which model to use. Sample Code Template ```python import numpy as np import pandas as pd from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, confusion_matrix # 1. Data Loading and Preprocessing wine = load_wine() X, y = wine.data, wine.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize dictionaries to store results accuracies = {} confusion_matrices = {} # 2. Model Training and Evaluation # GaussianNB gnb = GaussianNB() gnb.fit(X_train, y_train) y_pred_gnb = gnb.predict(X_test) accuracies[\'GaussianNB\'] = accuracy_score(y_test, y_pred_gnb) confusion_matrices[\'GaussianNB\'] = confusion_matrix(y_test, y_pred_gnb) # MultinomialNB mnb = MultinomialNB() mnb.fit(X_train, y_train) y_pred_mnb = mnb.predict(X_test) accuracies[\'MultinomialNB\'] = accuracy_score(y_test, y_pred_mnb) confusion_matrices[\'MultinomialNB\'] = confusion_matrix(y_test, y_pred_mnb) # ComplementNB cnb = ComplementNB() cnb.fit(X_train, y_train) y_pred_cnb = cnb.predict(X_test) accuracies[\'ComplementNB\'] = accuracy_score(y_test, y_pred_cnb) confusion_matrices[\'ComplementNB\'] = confusion_matrix(y_test, y_pred_cnb) # BernoulliNB bnb = BernoulliNB() bnb.fit(X_train, y_train) y_pred_bnb = bnb.predict(X_test) accuracies[\'BernoulliNB\'] = accuracy_score(y_test, y_pred_bnb) confusion_matrices[\'BernoulliNB\'] = confusion_matrix(y_test, y_pred_bnb) # Print accuracy scores for model, accuracy in accuracies.items(): print(f\\"Accuracy of {model}: {accuracy}\\") # Print confusion matrices for model, matrix in confusion_matrices.items(): print(f\\"Confusion Matrix of {model}:n{matrix}\\") # 3. Model Selection and Analysis # Provide a brief analysis and recommendation here. ``` Submission Submit a Jupyter notebook or Python script with your solution, including the analysis and recommendation.","solution":"import numpy as np from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, confusion_matrix import pandas as pd # 1. Data Loading and Preprocessing wine = load_wine() X, y = wine.data, wine.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize dictionaries to store results accuracies = {} confusion_matrices = {} # Helper function to train and evaluate a model def train_and_evaluate(model, model_name): model.fit(X_train, y_train) y_pred = model.predict(X_test) accuracies[model_name] = accuracy_score(y_test, y_pred) confusion_matrices[model_name] = confusion_matrix(y_test, y_pred) # 2. Model Training and Evaluation # GaussianNB train_and_evaluate(GaussianNB(), \'GaussianNB\') # MultinomialNB # MultinomialNB requires non-negative features X_train_mnb = np.abs(X_train) X_test_mnb = np.abs(X_test) train_and_evaluate(MultinomialNB(), \'MultinomialNB\') # ComplementNB train_and_evaluate(ComplementNB(), \'ComplementNB\') # BernoulliNB # BernoulliNB assumes binary/boolean features X_train_bnb = (X_train > 0).astype(int) X_test_bnb = (X_test > 0).astype(int) train_and_evaluate(BernoulliNB(), \'BernoulliNB\') # Print accuracy scores for model, accuracy in accuracies.items(): print(f\\"Accuracy of {model}: {accuracy}\\") # Print confusion matrices for model, matrix in confusion_matrices.items(): print(f\\"Confusion Matrix of {model}:n{matrix}\\") # 3. Model Selection and Analysis print(\\"Model performance analysis:\\") analysis = pd.DataFrame.from_dict(accuracies, orient=\'index\', columns=[\'Accuracy\']).sort_values(by=\'Accuracy\', ascending=False) print(analysis) # Recommendation print(\\"nRecommendation: Based on the accuracy results, GaussianNB consistently provides the highest accuracy for this dataset. This suggests that the continuous data of the Wine dataset is well-suited for Gaussian distribution assumption. Therefore, GaussianNB is recommended for this classification task.\\")"},{"question":"# Question: Implement and Use Custom HMAC-Based Authentication **Objective:** Your task is to implement a Python function that uses the `hmac` module to authenticate messages using a secret key. This function should be capable of generating a secure HMAC digest and verifying a provided digest against the original message. **Function 1: `generate_hmac(message: str, key: str, digestmod: str) -> str`** - **Input:** - `message`: A string containing the message to be authenticated. - `key`: A string to be used as the secret key. - `digestmod`: The name of the hash algorithm to use (e.g., `\'sha256\'`). - **Output:** - Returns a string representing the hexadecimal HMAC digest of the provided `message`. - **Example:** ```python generate_hmac(\\"Hello, World!\\", \\"supersecretkey\\", \\"sha256\\") # Output: \'4ee8b29cde837a0f6ac1fdc7e98f4a7df8e1fe9ed0d0a47a34f8e40e582792b4\' (Example output) ``` **Function 2: `verify_hmac(message: str, key: str, hex_digest: str, digestmod: str) -> bool`** - **Input:** - `message`: A string containing the original message. - `key`: A string to be used as the secret key. - `hex_digest`: The hexadecimal digest to verify against. - `digestmod`: The name of the hash algorithm to use (e.g., `\'sha256\'`). - **Output:** - Returns a boolean value. `True` if the provided `hex_digest` matches the HMAC digest of the `message` using the `key`, otherwise `False`. - **Example:** ```python orig_digest = generate_hmac(\\"Hello, World!\\", \\"supersecretkey\\", \\"sha256\\") is_valid = verify_hmac(\\"Hello, World!\\", \\"supersecretkey\\", orig_digest, \\"sha256\\") # Output: True ``` **Constraints:** - The `key` and `message` provided to both functions will be non-empty strings. - The `digestmod` will be a valid hash algorithm name supported by the `hashlib` module. - The implementation should be secure and avoid vulnerabilities to timing attacks while comparing digests. **Performance:** - The solution should efficiently handle typical message sizes up to 1MB. **Note:** Use the `hmac` module\'s provided functions and methods to implement these functionalities.","solution":"import hmac import hashlib def generate_hmac(message: str, key: str, digestmod: str) -> str: Generates an HMAC digest for a given message using a secret key and specified hashing algorithm. Args: - message (str): The message to be authenticated. - key (str): The secret key. - digestmod (str): The hash algorithm to use (e.g., \'sha256\'). Returns: - str: The hexadecimal HMAC digest of the provided message. byte_key = key.encode() byte_message = message.encode() hmac_obj = hmac.new(byte_key, byte_message, getattr(hashlib, digestmod)) return hmac_obj.hexdigest() def verify_hmac(message: str, key: str, hex_digest: str, digestmod: str) -> bool: Verifies an HMAC digest against a given message and secret key. Args: - message (str): The original message. - key (str): The secret key. - hex_digest (str): The hexadecimal digest to verify against. - digestmod (str): The hash algorithm to use (e.g., \'sha256\'). Returns: - bool: Returns True if the provided digest matches the HMAC digest of the message using the key, otherwise False. expected_digest = generate_hmac(message, key, digestmod) return hmac.compare_digest(expected_digest, hex_digest)"},{"question":"**Objective**: Assess your understanding of the `seaborn` library to create and customize categorical plots. **Question**: You are given the Titanic dataset with the following columns: - `survived`: Survival status (0 = No; 1 = Yes) - `pclass`: Passenger class (1 = 1st; 2 = 2nd; 3 = 3rd) - `sex`: Gender - `age`: Age - `sibsp`: Number of siblings/spouses aboard - `parch`: Number of parents/children aboard - `fare`: Passenger fare - `embarked`: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) Using the provided dataset, write a function `plot_titanic_data(df)` that performs the following steps: 1. **Load the dataset**: Load the Titanic dataset. 2. **Set the theme**: Set the seaborn theme to `whitegrid`. 3. **Create a bar plot**: Display the survival rate (`survived`) based on the passenger class (`pclass`) and gender (`sex`). Use appropriate labels and titles. Ensure the plot is split by gender and set the figure height and aspect ratio to appropriate values. 4. **Create a violin plot**: Show the distribution of age (`age`) based on the survival status (`survived`). The plot should display different colors for the two survival statuses, and the bandwidth adjustment should be fine-tuned for clarity. 5. **Customize the plot**: Modify the axis labels, set the y-axis limits to [0, 1], and adjust the title formatting appropriately. **Function Signature**: ```python def plot_titanic_data(df: pd.DataFrame) -> None: pass ``` **Input**: - `df`: A Pandas DataFrame containing the Titanic dataset. **Output**: - The function should display the plots as specified. There is no return value. # Constraints and Requirements: - Use the `seaborn` library to create the plots. - Ensure that the plots are clear and well-labeled. - The function should handle the provided dataset accurately and produce the required plots. # Performance Requirements: - The function should execute within a reasonable time frame for this size dataset. - The plots should be rendered efficiently without unnecessary computational overhead. Here\'s an example of the expected function usage: ```python import seaborn as sns import pandas as pd # Load the dataset df = sns.load_dataset(\\"titanic\\") # Call the function plot_titanic_data(df) ``` **Additional Notes**: - Include comments in your code to explain each step. - Ensure your final plots are polished and easy to interpret.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_titanic_data(df: pd.DataFrame) -> None: Generates and displays seaborn plots based on the Titanic dataset. Parameters: - df: Pandas DataFrame containing the Titanic dataset The function creates: 1. A bar plot showing the survival rate based on passenger class and gender. 2. A violin plot showing the age distribution based on survival status. # Set the seaborn theme to whitegrid for better visualization sns.set_theme(style=\\"whitegrid\\") # Bar plot: Survival rate based on passenger class and gender plt.figure(figsize=(10, 5)) sns.catplot(data=df, x=\'pclass\', hue=\'sex\', col=\'survived\', kind=\'count\', height=5, aspect=1).set_axis_labels(\'Passenger Class\', \'Count\').set_titles(\'Survived = {col_name}\') plt.suptitle(\'Survival Count Based on Passenger Class and Gender\', fontsize=16) plt.tight_layout(rect=[0, 0, 1, 0.95]) # Adjust layout to make room for the main title # Violin plot: Age distribution based on survival status plt.figure(figsize=(12, 6)) sns.violinplot(data=df, x=\'survived\', y=\'age\', hue=\'survived\', split=True, palette={0: \'r\', 1: \'g\'}, bw=0.2) plt.xlabel(\'Survived\') plt.ylabel(\'Age\') plt.title(\'Age Distribution Based on Survival Status\', fontsize=16) plt.ylim(0, 80) # Set y-axis limits for better clarity plt.legend(title=\'Survived\', loc=\'upper right\') plt.show()"},{"question":"**Objective:** Design a function that demonstrates your understanding of data compression and archiving in Python. The function should compress a directory of text files into a single compressed archive file, and then decompress this archive back to its original files and structure. # Task: 1. **Function Name:** `compress_and_decompress` 2. **Parameters:** - `input_directory` (str): The path to the input directory containing text files to be compressed. - `compression_format` (str): The compression format to use (`\'zip\'` or `\'tar\'`). - `output_filepath` (str): The path to the output compressed file. - `temp_directory` (str): The path to a temporary directory where the archive will be decompressed. 3. **Output:** - Return `True` if the process completes successfully; otherwise, return `False`. # Requirements: 1. **Compression:** - If `compression_format` is `\'zip\'`, create a ZIP archive of all text files in `input_directory`. - If `compression_format` is `\'tar\'`, create a tar archive of all text files in `input_directory`. 2. **Decompression:** - Decompress the created archive into the `temp_directory`. - Verify that the decompressed files match the original files in `input_directory`. 3. **Constraints:** - Only text files (`.txt`) should be considered during compression. - Raise a `ValueError` if `compression_format` is not `\'zip\'` or `\'tar\'`. - Ensure the function handles any exceptions that may occur and returns `False` in such cases. 4. **Performance:** - The implementation should be efficient in terms of both memory and processing time. # Example: ```python from pathlib import Path # Create an example directory with text files Path(\\"example_dir\\").mkdir(exist_ok=True) (Path(\\"example_dir\\") / \\"file1.txt\\").write_text(\\"This is file 1.\\") (Path(\\"example_dir\\") / \\"file2.txt\\").write_text(\\"This is file 2.\\") # Define paths for compressed file and temporary directory compressed_file_path = \\"example_compressed.zip\\" temp_dir_path = \\"example_temp\\" # Implement the function def compress_and_decompress(input_directory, compression_format, output_filepath, temp_directory): # Your code here pass # Run the function result = compress_and_decompress(\\"example_dir\\", \\"zip\\", compressed_file_path, temp_dir_path) # Check the output print(result) # Should print: True ``` **Hints:** - Utilize the `zipfile` module for ZIP archive operations. - Utilize the `tarfile` module for TAR archive operations. - Ensure to handle file and directory operations with error checking. - Use context managers to handle file operations safely.","solution":"import os import zipfile import tarfile import filecmp import shutil def compress_and_decompress(input_directory, compression_format, output_filepath, temp_directory): try: # Check if input directory exists if not os.path.isdir(input_directory): raise ValueError(\\"Input directory does not exist\\") # Ensure the temp directory is clean if os.path.exists(temp_directory): shutil.rmtree(temp_directory) os.makedirs(temp_directory) # Define compression and decompression methods if compression_format == \'zip\': with zipfile.ZipFile(output_filepath, \'w\') as zipf: for root, dirs, files in os.walk(input_directory): for file in files: if file.endswith(\'.txt\'): file_path = os.path.join(root, file) zipf.write(file_path, os.path.relpath(file_path, input_directory)) with zipfile.ZipFile(output_filepath, \'r\') as zipf: zipf.extractall(temp_directory) elif compression_format == \'tar\': with tarfile.open(output_filepath, \'w:gz\') as tarf: for root, dirs, files in os.walk(input_directory): for file in files: if file.endswith(\'.txt\'): file_path = os.path.join(root, file) tarf.add(file_path, arcname=os.path.relpath(file_path, input_directory)) with tarfile.open(output_filepath, \'r:gz\') as tarf: tarf.extractall(temp_directory) else: raise ValueError(\\"Invalid compression format. Only \'zip\' and \'tar\' are supported.\\") # Verify that decompressed files match the original files input_files = [os.path.join(dp, f) for dp, dn, fn in os.walk(input_directory) for f in fn if f.endswith(\'.txt\')] temp_files = [os.path.join(dp, f) for dp, dn, fn in os.walk(temp_directory) for f in fn if f.endswith(\'.txt\')] if len(input_files) != len(temp_files): return False input_files = sorted(input_files, key=lambda x: os.path.relpath(x, input_directory)) temp_files = sorted(temp_files, key=lambda x: os.path.relpath(x, temp_directory)) for input_file, temp_file in zip(input_files, temp_files): if not filecmp.cmp(input_file, temp_file): return False return True except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"You are given a dataset `sales_data.csv` which contains sales information for a company over a period of time. Your task is to analyze this data and generate various visualizations using the `pandas.plotting` module to better understand the dataset and derive useful insights. The `sales_data.csv` file has the following columns: - `Date`: The date of the sale. - `Product`: The product that was sold. - `Sales_Amount`: The amount of sales revenue generated. - `Units_Sold`: The number of units of the product sold. # Your tasks are: 1. **Read the data**: Load the dataset into a pandas DataFrame. 2. **Preprocess the data**: - Convert the `Date` column to pandas DateTime objects. - Check for missing values and handle them appropriately. 3. **Visualize the data**: - Create a scatter matrix plot for the numerical columns (`Sales_Amount` and `Units_Sold`). - Generate a time series plot that shows the `Sales_Amount` over time. - Use Andrews curves to visualize how `Sales_Amount` varies across different `Product` categories. - Generate a boxplot to show the distribution of `Sales_Amount` for each `Product`. 4. **Customize your plots** to make them more informative and visually appealing: - Add titles and labels to each plot. - Use different colors and styles where appropriate. # Expected Input and Output **Input:** - A CSV file named `sales_data.csv` containing sales data. **Output:** - A set of visualizations displayed using the plotting capabilities of pandas. # Constraints - Ensure that the time series plot correctly handles the DateTime conversion and is sorted by date. - Handle missing data by removing any rows with NaN values. # Performance Requirements: - Your code should efficiently load and preprocess the data. - Each plot should be generated within a reasonable time frame without causing performance issues. # Example Solution Below is a template example of how you might start implementing the solution. You need to fill in the details as per the requirements above: ```python import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix, andrews_curves, register_matplotlib_converters # Read the data data = pd.read_csv(\'sales_data.csv\') # Preprocess the data data[\'Date\'] = pd.to_datetime(data[\'Date\']) data.dropna(inplace=True) # Scatter matrix plot scatter_matrix(data[[\'Sales_Amount\', \'Units_Sold\']], alpha=0.2, figsize=(6, 6), diagonal=\'kde\') plt.suptitle(\\"Scatter Matrix of Sales Data\\") plt.show() # Time series plot data.set_index(\'Date\')[\'Sales_Amount\'].plot(title=\'Sales Amount Over Time\') plt.ylabel(\'Sales Amount\') plt.show() # Andrews curves plot andrews_curves(data, \'Product\') plt.title(\'Andrews Curves of Sales Amount by Product\') plt.show() # Boxplot data.boxplot(column=\'Sales_Amount\', by=\'Product\') plt.title(\'Boxplot of Sales Amount by Product\') plt.suptitle(\\"\\") plt.ylabel(\'Sales Amount\') plt.show() ```","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix, andrews_curves, register_matplotlib_converters def load_and_preprocess_data(file_path): Loads and preprocesses the sales data. data = pd.read_csv(file_path) data[\'Date\'] = pd.to_datetime(data[\'Date\']) data.dropna(inplace=True) return data def generate_visualizations(data): Generates all required visualizations from the sales data. plt.figure() scatter_matrix(data[[\'Sales_Amount\', \'Units_Sold\']], alpha=0.2, figsize=(6, 6), diagonal=\'kde\') plt.suptitle(\\"Scatter Matrix of Sales Data\\") plt.show() plt.figure() data.set_index(\'Date\')[\'Sales_Amount\'].sort_index().plot(title=\'Sales Amount Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Sales Amount\') plt.show() plt.figure() andrews_curves(data, \'Product\') plt.title(\'Andrews Curves of Sales Amount by Product\') plt.show() plt.figure() data.boxplot(column=\'Sales_Amount\', by=\'Product\') plt.title(\'Boxplot of Sales Amount by Product\') plt.suptitle(\\"\\") plt.xlabel(\'Product\') plt.ylabel(\'Sales Amount\') plt.show() # Main function to load data and generate visualizations def main(file_path): data = load_and_preprocess_data(file_path) generate_visualizations(data)"},{"question":"**HTML Content Processing** You are tasked with writing a function that sanitizes HTML content from a list of strings. Each string in the list might contain unsafe characters like `\\"&\\"`, `\\"<\\"`, `\\">\\"`, `\'\\"\'`, and `\\"\'\\"` that need to be escaped. Also, the list might contain encoded characters that need unescaping to ensure readability. # Function Signature ```python def sanitize_html(contents: list, escape_mode: bool=True) -> list: ``` # Parameters - `contents (list)`: A list of strings where each string can contain HTML content. - `escape_mode (bool)`: A boolean flag that specifies the mode of operation. If `True`, the function should escape characters in each string. If `False`, the function should unescape characters in each string. # Returns - `list`: A list of processed strings where each string is either escaped or unescaped based on the `escape_mode`. # Example ```python # Example Input contents = [ \\"Hello & Welcome <user>!\\", \\"Here is a quote: \'Life is beautiful\' & another: \\"Stay positive\\"\\", \\"Encoded characters: &lt; &gt; &amp;\\" ] # Escape Mode (True) esc_result = sanitize_html(contents, escape_mode=True) print(esc_result) # [\\"Hello &amp; Welcome &lt;user&gt;!\\", \\"Here is a quote: &#x27;Life is beautiful&#x27; & another: &#x22;Stay positive&#x22;\\", \\"Encoded characters: &amp;lt; &amp;gt; &amp;amp;\\"] # Unescape Mode (False) unesc_result = sanitize_html(contents, escape_mode=False) print(unesc_result) # [\\"Hello & Welcome <user>!\\", \\"Here is a quote: \'Life is beautiful\' & another: \\"Stay positive\\"\\", \\"Encoded characters: < > &\\"] ``` # Constraints - Each string in the `contents` list will contain at most 1000 characters. - The `contents` list will contain at most 100 strings. - You must use the `html` module functions for escaping and unescaping. # Performance Requirements Your function should be able to process the entire list of strings within a reasonable time frame, preferably under 1 second for the maximum input size.","solution":"import html def sanitize_html(contents: list, escape_mode: bool=True) -> list: Sanitize HTML content by escaping or unescaping special characters. :param contents: List of strings with HTML content. :param escape_mode: Boolean flag to determine whether to escape or unescape the contents. :return: List of processed strings. if escape_mode: return [html.escape(content, quote=True) for content in contents] else: return [html.unescape(content) for content in contents]"},{"question":"# Custom File Wrapper You are required to create a custom file handling class that extends the functionality of Python\'s built-in file handling. The class should be capable of converting all written text to lower case before writing to the file. To avoid any naming conflicts with Python\'s built-in functions, use the `builtins` module. Requirements: 1. **Class Name:** `LowerCaser` 2. **Methods:** - `__init__(self, file_path, mode=\'r\')`: Initializes the class with the file path and mode (default is reading mode \'r\'). - `read(self, count=-1)`: Reads from the file (same behavior as the built-in `read` method). - `write(self, text)`: Converts the text to lower-case before writing to the file. - `close(self)`: Closes the file. 3. **Constraints:** - The class should use the `builtins.open` function to handle file I/O. - Ensure that all text written to the file is stored in lower-case. Input Format: - File path (as a string) - Mode (as a string, optional, default is \'r\') - Text to write (as a string, if in writing mode) Output Format: - None for write operations - String for read operations Example Usage: ```python import builtins class LowerCaser: def __init__(self, file_path, mode=\'r\'): self._file = builtins.open(file_path, mode) def read(self, count=-1): return self._file.read(count) def write(self, text): return self._file.write(text.lower()) def close(self): return self._file.close() # Usage file_writer = LowerCaser(\'example.txt\', \'w\') file_writer.write(\\"HELLO World!\\") file_writer.close() file_reader = LowerCaser(\'example.txt\', \'r\') print(file_reader.read()) # Output: hello world! file_reader.close() ``` Note: - Ensure that the file is properly closed after the operations to prevent resource leaks. - Handle any errors or exceptions appropriately (e.g., file not found, permission errors).","solution":"import builtins class LowerCaser: def __init__(self, file_path, mode=\'r\'): self._file = builtins.open(file_path, mode) def read(self, count=-1): return self._file.read(count) def write(self, text): return self._file.write(text.lower()) def close(self): return self._file.close() # Usage file_writer = LowerCaser(\'example.txt\', \'w\') file_writer.write(\\"HELLO World!\\") file_writer.close() file_reader = LowerCaser(\'example.txt\', \'r\') print(file_reader.read()) # Output: hello world! file_reader.close()"},{"question":"Objective: Demonstrate your understanding of the Seaborn library (specifically the `seaborn.objects` module) by loading a dataset, creating visualizations, and customizing them using different properties and methods. Question: You are provided with the `penguins` dataset from the `seaborn` library. Your task is to create a customized set of visualizations following these steps: 1. Load the `penguins` dataset from the `seaborn` library. 2. Create a plot using the `so.Plot` function to visualize the relationship between `species` (x-axis) and `body_mass_g` (y-axis), and color the data points by `sex`. 3. Add a line segment (using `so.Dash()`) for each data point with a default width. 4. Customize the plot to: a. Adjust the transparency (alpha) of the line segments to 0.5. b. Set the `linewidth` based on the `flipper_length_mm` attribute. c. Change the `width` of the line segments to 0.5. 5. Finally, add variability to the data points by dodging and jittering the dots. Constraints: - Use only the Seaborn library for the visualizations. - Ensure that the code is well-documented with comments explaining each step. Output: - A single figure showing the customized plot with all the specifications mentioned above. Example: Here is an example of how to structure your code: ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset penguins = load_dataset(\\"penguins\\") # Step 2: Create the basic plot p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") # Step 3: Add line segments p.add(so.Dash()) # Step 4: Customize the plot p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") p.add(so.Dash(width=0.5)) # Step 5: Add dodging and jittering p.add(so.Dash(), so.Dodge()) p.add(so.Dash(), so.Agg(), so.Dodge()) p.add(so.Dots(), so.Dodge(), so.Jitter()) ``` Ensure that the final plot meets all the customization requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset penguins = load_dataset(\\"penguins\\") # Step 2: Create the basic plot p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") # Step 3 & 4: Add line segments and customize the plot p.add(so.Dash(alpha=0.5, linewidth=\\"flipper_length_mm\\", width=0.5)) # Step 5: Add variability to the data points by dodging and jittering p.add(so.Dots(), so.Dodge(), so.Jitter()) # Plot the final figure p.show()"},{"question":"<|Analysis Begin|> The provided documentation is for the `zipfile` module in Python, which offers tools for creating, reading, writing, appending, and listing ZIP files. The module includes classes like `ZipFile`, `ZipInfo`, `Path`, and `PyZipFile`, along with various methods and attributes to manipulate ZIP archives. It also defines exceptions for handling errors related to ZIP files. Key features of the `zipfile` module include: - The ability to read and write ZIP files. - Support for different compression methods (ZIP_STORED, ZIP_DEFLATED, ZIP_BZIP2, ZIP_LZMA). - Handling of ZIP64 extensions for large files. - Context manager support for the `ZipFile` object. - Methods for extracting files, listing contents, and writing files to ZIP archives. The documentation covers a range of functionality, from basic file manipulation to more advanced features like compression levels and ZIP64 support. This level of detail allows for the creation of a comprehensive coding question that tests the understanding of these features. <|Analysis End|> <|Question Begin|> **ZIP File Extractor and Compressor** **Objective**: Implement a Python function that takes a ZIP file as input, extracts its contents to a specified directory, and then compresses the extracted files into a new ZIP file using a specified compression method. **Function Signature**: ```python def extract_and_compress_zip(input_zip_path: str, extraction_path: str, output_zip_path: str, compression_method: int) -> None: Extracts a ZIP file to a specified directory and then compresses the contents into a new ZIP file using a specified compression method. Parameters: input_zip_path (str): Path to the input ZIP file. extraction_path (str): Directory to extract the contents of the ZIP file. output_zip_path (str): Path to the output ZIP file that will contain the compressed contents. compression_method (int): The compression method to be used for the output ZIP file. Must be one of zipfile.ZIP_STORED, zipfile.ZIP_DEFLATED, zipfile.ZIP_BZIP2, or zipfile.ZIP_LZMA. Returns: None ``` **Requirements**: 1. **Input Validation**: - The function should check if the `input_zip_path` is indeed a valid ZIP file. If not, it should raise a `zipfile.BadZipFile` exception. - The function should ensure the provided `compression_method` is valid (one of `zipfile.ZIP_STORED`, `zipfile.ZIP_DEFLATED`, `zipfile.ZIP_BZIP2`, or `zipfile.ZIP_LZMA`). If not, it should raise a `ValueError`. 2. **Extraction**: - Extract the contents of the input ZIP file to the specified `extraction_path`. 3. **Compression**: - Compress the extracted files into a new ZIP file located at `output_zip_path` using the specified `compression_method`. - Handle any exceptions that might occur during the extraction or compression process appropriately (e.g., file not found, permission errors). 4. **Constraints**: - Assume all paths are valid strings and appropriate for the operating system in use. - The disk must have sufficient space for extraction and compression operations. **Example**: ```python # Assume we have test.zip containing some files and directories. extract_and_compress_zip(\'test.zip\', \'extracted_files\', \'new_compressed.zip\', zipfile.ZIP_DEFLATED) ``` **Notes**: - Utilize the `zipfile.ZipFile` class and its methods for handling ZIP files. - Make sure to handle edge cases, such as empty ZIP files or files with special characters in names. **Performance Considerations**: - Ensure that the solution is efficient for reasonably large ZIP files (up to several gigabytes).","solution":"import zipfile import os def extract_and_compress_zip(input_zip_path: str, extraction_path: str, output_zip_path: str, compression_method: int) -> None: Extracts a ZIP file to a specified directory and then compresses the contents into a new ZIP file using a specified compression method. Parameters: input_zip_path (str): Path to the input ZIP file. extraction_path (str): Directory to extract the contents of the ZIP file. output_zip_path (str): Path to the output ZIP file that will contain the compressed contents. compression_method (int): The compression method to be used for the output ZIP file. Must be one of zipfile.ZIP_STORED, zipfile.ZIP_DEFLATED, zipfile.ZIP_BZIP2, or zipfile.ZIP_LZMA. Returns: None # Check whether the input file is a valid zip file if not zipfile.is_zipfile(input_zip_path): raise zipfile.BadZipFile(\\"The provided file is not a valid ZIP file.\\") # Verify the compression method valid_methods = [zipfile.ZIP_STORED, zipfile.ZIP_DEFLATED, zipfile.ZIP_BZIP2, zipfile.ZIP_LZMA] if compression_method not in valid_methods: raise ValueError(\\"Invalid compression method.\\") # Extracting the ZIP file with zipfile.ZipFile(input_zip_path, \'r\') as zip_ref: zip_ref.extractall(extraction_path) # Creating the new ZIP file with zipfile.ZipFile(output_zip_path, \'w\', compression=compression_method) as new_zip: for foldername, subfolders, filenames in os.walk(extraction_path): for filename in filenames: file_path = os.path.join(foldername, filename) arcname = os.path.relpath(file_path, extraction_path) new_zip.write(file_path, arcname)"},{"question":"Task: Feature Selection and Model Building with Scikit-learn # Objective: Your task is to implement a feature selection method using `SelectFromModel` with a tree-based estimator and integrate it into a machine learning pipeline to improve model performance on a given dataset. # Dataset: You will use the well-known Iris dataset available in the `sklearn.datasets` module. # Instructions: 1. **Load the Iris dataset**: ```python from sklearn.datasets import load_iris X, y = load_iris(return_X_y=True) ``` 2. **Feature Selection**: - Use `SelectFromModel` with `ExtraTreesClassifier` to perform feature selection. - Define the `ExtraTreesClassifier` with `n_estimators=50`. - Use the `SelectFromModel` to select important features. 3. **Model Building**: - Construct a pipeline using `Pipeline` from `sklearn.pipeline`. - In the pipeline, first perform feature selection, and then train a classifier. - Use `RandomForestClassifier` as the classifier in the pipeline. 4. **Evaluation**: - Use `cross_val_score` from `sklearn.model_selection` to evaluate the pipeline\'s performance. - Perform 5-fold cross-validation and report the mean accuracy. # Requirements: - Your implementation should be efficient and should not make the data dense unnecessarily. - Ensure that the feature selection step is integrated into the pipeline correctly. - You should output the selected feature indices and the mean cross-validation accuracy. # Code Structure: Your code should follow this structure: ```python from sklearn.datasets import load_iris from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier from sklearn.feature_selection import SelectFromModel from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score # Load the Iris dataset X, y = load_iris(return_X_y=True) # Define the feature selection transformer and model feature_selector = SelectFromModel(ExtraTreesClassifier(n_estimators=50)) classifier = RandomForestClassifier() # Build the pipeline pipeline = Pipeline([ (\'feature_selection\', feature_selector), (\'classification\', classifier) ]) # Evaluate the pipeline using cross-validation cv_scores = cross_val_score(pipeline, X, y, cv=5) mean_accuracy = cv_scores.mean() # Output selected indices and mean accuracy selected_features = feature_selector.fit(X, y).get_support(indices=True) print(\\"Selected feature indices:\\", selected_features) print(\\"Mean cross-validation accuracy:\\", mean_accuracy) ``` # Submission: - Submit your complete implementation following the above structure. - Ensure your code runs without errors and produces the correct outputs.","solution":"from sklearn.datasets import load_iris from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier from sklearn.feature_selection import SelectFromModel from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score def feature_selection_and_model_pipeline(): # Load the Iris dataset X, y = load_iris(return_X_y=True) # Define the feature selection transformer and model feature_selector = SelectFromModel(ExtraTreesClassifier(n_estimators=50)) classifier = RandomForestClassifier() # Build the pipeline pipeline = Pipeline([ (\'feature_selection\', feature_selector), (\'classification\', classifier) ]) # Evaluate the pipeline using cross-validation cv_scores = cross_val_score(pipeline, X, y, cv=5) mean_accuracy = cv_scores.mean() # Output selected indices and mean accuracy selected_features = feature_selector.fit(X, y).get_support(indices=True) return selected_features, mean_accuracy"},{"question":"# Python Coding Assessment Problem Statement Write a Python program that manages an inventory of products for an online store. The program should support the following functionalities: 1. **Add Product**: Add a new product to the inventory. 2. **Remove Product**: Remove an existing product from the inventory. 3. **Update Quantity**: Update the quantity of a specific product in the inventory. 4. **Search Product**: Search for a product by its name. 5. **Display Products**: Display all products in the inventory. Each product should have the following attributes: - `name` (str) - `category` (str) - `price` (float) - `quantity` (int) Your implementation should: 1. Define a `Product` class with the necessary attributes and methods. 2. Define an `Inventory` class with methods to manage the products: - `add_product(product: Product) -> None` - `remove_product(product_name: str) -> None` - `update_quantity(product_name: str, quantity: int) -> None` - `search_product(product_name: str) -> Product` - `display_products() -> None` 3. Handle exceptions gracefully, such as attempting to remove or update a non-existent product. 4. Use the `with` statement to ensure that any file operations (e.g., logging errors, saving inventory data) are managed correctly. 5. Implement a pattern matching statement (using the `match` statement) to handle the user commands for inventory management. # Input and Output Formats **Input:** - Commands will be provided as strings (e.g., \\"add_product\\", \\"remove_product\\", etc.). - The necessary parameters for each command will follow as required. **Output:** - Display appropriate messages for each operation, such as successful addition/removal, errors encountered, or the list of products. # Example ```python # Main function to handle user commands def main(): inventory = Inventory() while True: command = input(\\"Enter command: \\") match command.split(): case [\\"add_product\\", name, category, price, quantity]: try: product = Product(name, category, float(price), int(quantity)) inventory.add_product(product) except ValueError as e: print(f\\"Error: Invalid data - {e}\\") case [\\"remove_product\\", name]: inventory.remove_product(name) case [\\"update_quantity\\", name, quantity]: try: inventory.update_quantity(name, int(quantity)) except ValueError as e: print(f\\"Error: Invalid quantity - {e}\\") case [\\"search_product\\", name]: product = inventory.search_product(name) if product: print(product) else: print(\\"Product not found\\") case [\\"display_products\\"]: inventory.display_products() case [\\"exit\\"]: break case _: print(\\"Invalid command\\") if __name__ == \\"__main__\\": main() ``` Implement the required classes (`Product` and `Inventory`) and function them in accordance with the specified functionalities. # Constraints - Product names are unique. - Quantities are non-negative integers. - Price is a non-negative float. Happy coding!","solution":"class Product: def __init__(self, name: str, category: str, price: float, quantity: int): self.name = name self.category = category self.price = price self.quantity = quantity def __str__(self): return f\\"Product(name={self.name}, category={self.category}, price={self.price}, quantity={self.quantity})\\" class Inventory: def __init__(self): self.products = {} def add_product(self, product: Product) -> None: if product.name in self.products: raise ValueError(\\"Product already exists in the inventory\\") self.products[product.name] = product def remove_product(self, product_name: str) -> None: if product_name not in self.products: raise ValueError(\\"Product does not exist in the inventory\\") del self.products[product_name] def update_quantity(self, product_name: str, quantity: int) -> None: if product_name not in self.products: raise ValueError(\\"Product does not exist in the inventory\\") if quantity < 0: raise ValueError(\\"Quantity cannot be negative\\") self.products[product_name].quantity = quantity def search_product(self, product_name: str) -> Product: return self.products.get(product_name, None) def display_products(self) -> None: for product in self.products.values(): print(product) def main(): inventory = Inventory() while True: command = input(\\"Enter command: \\") match command.split(): case [\\"add_product\\", name, category, price, quantity]: try: product = Product(name, category, float(price), int(quantity)) inventory.add_product(product) print(\\"Product added successfully\\") except ValueError as e: print(f\\"Error: Invalid data - {e}\\") case [\\"remove_product\\", name]: try: inventory.remove_product(name) print(\\"Product removed successfully\\") except ValueError as e: print(f\\"Error: {e}\\") case [\\"update_quantity\\", name, quantity]: try: inventory.update_quantity(name, int(quantity)) print(\\"Quantity updated successfully\\") except ValueError as e: print(f\\"Error: {e}\\") case [\\"search_product\\", name]: product = inventory.search_product(name) if product: print(product) else: print(\\"Product not found\\") case [\\"display_products\\"]: inventory.display_products() case [\\"exit\\"]: break case _: print(\\"Invalid command\\") if __name__ == \\"__main__\\": main()"},{"question":"Problem Statement You are tasked with creating a program that makes use of Python\'s `dataclasses` module to define a series of data structures, and the `inspect` module to retrieve information about these structures. Requirements 1. **Data Class Definition**: * Define a data class named `Person` with the following attributes: * `name` (str) * `age` (int) * `email` (str) * Define a data class named `Student` which inherits from `Person` and adds the following attribute: * `university` (str) * Ensure that these data classes: * Have a custom method called `info()` that returns a dictionary containing the attribute names and their corresponding values. * Are mutable. 2. **Introspection**: * Write a function named `inspect_data_class(cls)` that takes a class (not an instance) and returns a dictionary where the keys are the attribute names and the values are the attribute types of the class. Input No input required from the user. You will define the data classes and implement the inspection function as specified. Output The output should demonstrate the following: 1. Create instances of `Person` and `Student` and print the results of their `info()` methods. 2. Print the result of `inspect_data_class` when passed the `Person` and `Student` classes. Constraints - The classes should have proper type annotations. - Use Python 3.10 specific features where applicable. Example Output ```python # Example instantiation and usage: p = Person(name=\\"Alice\\", age=30, email=\\"alice@example.com\\") print(p.info()) # Output: {\'name\': \'Alice\', \'age\': 30, \'email\': \'alice@example.com\'} s = Student(name=\\"Bob\\", age=21, email=\\"bob@example.com\\", university=\\"MIT\\") print(s.info()) # Output: {\'name\': \'Bob\', \'age\': 21, \'email\': \'bob@example.com\', \'university\': \'MIT\'} print(inspect_data_class(Person)) # Output: {\'name\': <class \'str\'>, \'age\': <class \'int\'>, \'email\': <class \'str\'>} print(inspect_data_class(Student)) # Output: {\'name\': <class \'str\'>, \'age\': <class \'int\'>, \'email\': <class \'str\'>, \'university\': <class \'str\'>} ``` Good luck, and may your code be clean and bug-free!","solution":"from dataclasses import dataclass, fields @dataclass class Person: name: str age: int email: str def info(self): return {field.name: getattr(self, field.name) for field in fields(self)} @dataclass class Student(Person): university: str def info(self): return {field.name: getattr(self, field.name) for field in fields(self)} def inspect_data_class(cls): return {field.name: field.type for field in fields(cls)} # Example usage p = Person(name=\\"Alice\\", age=30, email=\\"alice@example.com\\") print(p.info()) # Output: {\'name\': \'Alice\', \'age\': 30, \'email\': \'alice@example.com\'} s = Student(name=\\"Bob\\", age=21, email=\\"bob@example.com\\", university=\\"MIT\\") print(s.info()) # Output: {\'name\': \'Bob\', \'age\': 21, \'email\': \'bob@example.com\', \'university\': \'MIT\'} print(inspect_data_class(Person)) # Output: {\'name\': <class \'str\'>, \'age\': <class \'int\'>, \'email\': <class \'str\'>} print(inspect_data_class(Student)) # Output: {\'name\': <class \'str\'>, \'age\': <class \'int\'>, \'email\': <class \'str\'>, \'university\': <class \'str\'>}"},{"question":"**Question: XML Data Extraction Using `xml.parsers.expat`** **Objective:** Create a function `extract_data_from_xml` that parses a given XML string using the `xml.parsers.expat` module and extracts specific data elements. **Task Description:** Write a Python function `extract_data_from_xml(xml_string: str) -> dict` that takes a single argument: - `xml_string`: A string containing valid XML data. The function should: 1. Use the `xml.parsers.expat` module to create a new XML parser. 2. Define and set handler functions for: - Start of an element (`StartElementHandler`). - End of an element (`EndElementHandler`). - Character data (`CharacterDataHandler`). 3. Parse the provided `xml_string`. 4. Extract the following information: - The names of all elements that have a \\"name\\" attribute and their corresponding values. - The content (text) within elements named \\"description\\". The function should return a dictionary with two keys: - `names`: A list of tuples, each containing an element name and attribute value from elements that have a \\"name\\" attribute. - `descriptions`: A list of strings, each containing the text found within \\"description\\" elements. **Constraints:** - The XML is assumed to be well-formed. - Handle any parsing errors gracefully by returning an empty dictionary `{}`. - You are not required to handle namespaces or external entities. **Example:** For the input XML string: ```xml <?xml version=\\"1.0\\"?> <library> <book name=\\"The Python Handbook\\"> <description>A comprehensive guide to Python programming.</description> </book> <book name=\\"Learning XML\\"> <description>Introduction to XML and related technologies.</description> </book> <author name=\\"John Doe\\" /> </library> ``` The function `extract_data_from_xml` should return: ```python { \\"names\\": [(\\"book\\", \\"The Python Handbook\\"), (\\"book\\", \\"Learning XML\\"), (\\"author\\", \\"John Doe\\")], \\"descriptions\\": [\\"A comprehensive guide to Python programming.\\", \\"Introduction to XML and related technologies.\\"] } ``` **Note:** - Carefully handle the setup of handler functions and ensure that data is appropriately aggregated and stored during parsing. - Use exception handling to manage any errors and return an empty dictionary in such cases.","solution":"import xml.parsers.expat def extract_data_from_xml(xml_string): Parses XML string and extracts specific data. Parameters: xml_string (str): A string containing valid XML data. Returns: dict: A dictionary with \'names\' and \'descriptions\' keys. names = [] descriptions = [] current_element = None current_data = [] def start_element(name, attrs): nonlocal current_element current_element = name if \'name\' in attrs: names.append((name, attrs[\'name\'])) def end_element(name): if name == \\"description\\": descriptions.append(\'\'.join(current_data).strip()) current_data.clear() def char_data(data): if current_element == \\"description\\": current_data.append(data) parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_string) except xml.parsers.expat.ExpatError: return {} return { \\"names\\": names, \\"descriptions\\": descriptions }"},{"question":"**Problem Statement:** You are provided with the `tips` dataset available in seaborn. The dataset contains information about tips received by waiters and waitresses in a restaurant, including the total bill, tip amount, gender, and other pertinent details. Your task is to visualize the data using Seaborn\'s objects (API) to demonstrate your understanding of fundamental and advanced concepts of this package. You need to implement the following functions: 1. `load_and_sort_tips(dataset_name: str) -> DataFrame`: - Loads the specified dataset and sorts it based on the \'total_bill\' column in descending order. - Input: The name of the dataset as a string (e.g., \'tips\'). - Output: A sorted pandas DataFrame. 2. `plot_total_bill_by_day(df: DataFrame) -> None`: - Creates a bar plot using the provided DataFrame, where the X-axis represents the days of the week, and the Y-axis represents the sum of the total bill for each day. - Colors the bars based on the gender (`sex` column) of the parties and stacks them. - Displays the plot. - Input: A pandas DataFrame with the dataset. 3. `plot_tip_distribution_by_time(df: DataFrame) -> None`: - Creates a histogram plot to show the distribution of tips, where the data is faceted by time (lunch or dinner). - Represents the density of tips using stacked bars. - Input: A pandas DataFrame with the dataset. **Constraints:** - Use seaborn\'s objects API to create the plots. - Remember to import the relevant libraries and modules. - Ensure your code is efficient and adds necessary comments to explain each step. **Example Usage:** ```python # Load the `tips` dataset and sort it by \'total_bill\' df = load_and_sort_tips(\\"tips\\") # Plot total bill by day with stacking by gender plot_total_bill_by_day(df) # Plot the distribution of tips faceted by time with stacking plot_tip_distribution_by_time(df) ``` The functions you write should be self-contained and should handle data loading, sorting, and visualization as specified above.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_and_sort_tips(dataset_name: str) -> pd.DataFrame: Loads the specified dataset and sorts it based on the \'total_bill\' column in descending order. Parameters: dataset_name (str): The name of the dataset to load (e.g., \'tips\'). Returns: pd.DataFrame: Sorted DataFrame based on the \'total_bill\' column. df = sns.load_dataset(dataset_name) df_sorted = df.sort_values(by=\'total_bill\', ascending=False) return df_sorted def plot_total_bill_by_day(df: pd.DataFrame) -> None: Creates a bar plot where the X-axis represents the days of the week, and the Y-axis represents the sum of the total bill for each day, with bars colored and stacked by gender. Parameters: df (pd.DataFrame): The input DataFrame with the dataset. # Aggregate total bill by day and sex df_agg = df.groupby([\'day\', \'sex\'])[\'total_bill\'].sum().unstack() df_agg.plot(kind=\'bar\', stacked=True) plt.xlabel(\'Day of the Week\') plt.ylabel(\'Total Bill\') plt.title(\'Total Bill by Day and Gender\') plt.show() def plot_tip_distribution_by_time(df: pd.DataFrame) -> None: Creates a histogram plot to show the distribution of tips, faceted by time (lunch or dinner) with density of tips represented using stacked bars. Parameters: df (pd.DataFrame): The input DataFrame with the dataset. g = sns.FacetGrid(df, col=\\"time\\", margin_titles=True) g.map_dataframe(sns.histplot, x=\\"tip\\", kde=False, hue=\\"time\\", multiple=\\"stack\\") g.set_axis_labels(\\"Tip Amount\\", \\"Count\\") g.set_titles(col_template=\\"{col_name} Time\\") plt.show()"},{"question":"**Objective:** Design a function to read data from a file, process it, and handle various exceptions to demonstrate comprehension of exception handling in Python. **Problem Description:** Write a Python function `process_file(filename)` that performs the following tasks: 1. Opens the file specified by `filename`. 2. Reads the file line by line and converts each line to an integer. 3. Computes the sum of these integers and returns it. 4. Handles exceptions in a robust manner: - If the file does not exist, raise a custom exception `FileNotFoundCustomError` with an appropriate message. - If a line in the file cannot be converted to an integer, handle the exception by logging the error and skipping that line. - Use a `finally` clause to ensure that the file is properly closed after processing, regardless of whether an exception occurred. - Ensure that any other unforeseen exceptions are properly caught and raised with a custom message using exception chaining. **Function Signature:** ```python def process_file(filename: str) -> int: pass ``` **Custom Exception:** Define a custom exception named `FileNotFoundCustomError` to be raised when the file does not exist. **Example:** ```python # Assuming the file \'data.txt\' has the following content: # 10 # abc # 20 # 30 try: result = process_file(\'data.txt\') print(result) # Output should be 60 (10 + 20 + 30) except FileNotFoundCustomError as e: print(e) except Exception as e: print(f\\"An unexpected error occurred: {e}\\") ``` **Constraints:** - Do not use libraries outside of the Python Standard Library. - The input file will contain one value per line which should be converted to an integer. - Ensure all resource handling and exception management as mentioned. **Notes:** - This exercise tests the student\'s ability to manage exceptions in Python, including custom exceptions, using try-except-else-finally blocks, and incorporating the with statement for resource management. - Custom log messages should be clear and informative, aiding in debugging and understanding of the code flow.","solution":"class FileNotFoundCustomError(Exception): pass def process_file(filename: str) -> int: total = 0 try: with open(filename, \'r\') as file: for line in file: try: num = int(line.strip()) total += num except ValueError: print(f\\"Skipping line: {line.strip()} - cannot be converted to an integer.\\") except FileNotFoundError: raise FileNotFoundCustomError(f\\"The file {filename} does not exist.\\") except Exception as e: raise Exception(f\\"An unexpected error occurred: {e}\\") from e return total"},{"question":"You are tasked with managing a series of tasks that need to be performed at specific times and potentially in a multi-threaded environment. Use the `sched` module to implement a class `TaskScheduler` that can schedule tasks and run them in the correct order. # Class: `TaskScheduler` Methods 1. `__init__(self, timefunc=None, delayfunc=None)`: - Initialize the scheduler instance. You can use the default `time.time` and `time.sleep` if no functions are provided. 2. `schedule_task_at(self, timestamp, priority, task, args=(), kwargs={})`: - Schedule a task to execute at an absolute timestamp. - `timestamp` (float): The absolute time when the task should be executed. - `priority` (int): The priority of the task. - `task` (callable): The task function to execute. - `args` (tuple, optional): Positional arguments for the task function. - `kwargs` (dict, optional): Keyword arguments for the task function. - Returns: The scheduled event. 3. `schedule_task_in(self, delay, priority, task, args=(), kwargs={})`: - Schedule a task to execute after a delay. - `delay` (float): The delay in seconds after which the task should be executed. - `priority` (int): The priority of the task. - `task` (callable): The task function to execute. - `args` (tuple, optional): Positional arguments for the task function. - `kwargs` (dict, optional): Keyword arguments for the task function. - Returns: The scheduled event. 4. `cancel_task(self, event)`: - Cancel a scheduled task. - `event`: The event to cancel. 5. `run_all(self, blocking=True)`: - Run all scheduled tasks. - `blocking` (bool, optional): If True, wait for the event\'s time to expire, else run events that are due immediately. 6. `is_empty(self)`: - Check if there are any scheduled tasks. - Returns: `True` if no tasks are scheduled, otherwise `False`. # Constraints - Ensure that your implementation is thread-safe if run in a multi-threaded environment. - You can assume that tasks can be scheduled at any time and in any order. - You should handle potential exceptions that may occur during task execution without crashing the scheduler. # Example Usage ```python import time from task_scheduler import TaskScheduler def print_message(message): print(f\\"{time.time()}: {message}\\") scheduler = TaskScheduler() # Schedule a task to print a message after 2 seconds event1 = scheduler.schedule_task_in(2, 1, print_message, args=(\\"Task 1 executed after 2 seconds\\",)) # Schedule a task to print a message at an absolute time (in this case, 3 seconds from now) event2 = scheduler.schedule_task_at(time.time() + 3, 1, print_message, args=(\\"Task 2 executed at absolute time\\",)) scheduler.run_all() # Check if the scheduler is empty print(f\\"Scheduler empty: {scheduler.is_empty()}\\") # Should output: Scheduler empty: True ``` Ensure that your implementation correctly follows the example usage and handles tasks in the correct order and timing.","solution":"import sched import time from threading import Lock class TaskScheduler: def __init__(self, timefunc=None, delayfunc=None): Initialize the scheduler instance. self.scheduler = sched.scheduler(timefunc if timefunc else time.time, delayfunc if delayfunc else time.sleep) self.lock = Lock() def schedule_task_at(self, timestamp, priority, task, args=(), kwargs={}): Schedule a task to execute at an absolute timestamp. with self.lock: event = self.scheduler.enterabs(timestamp, priority, task, argument=args, kwargs=kwargs) return event def schedule_task_in(self, delay, priority, task, args=(), kwargs={}): Schedule a task to execute after a delay. with self.lock: event = self.scheduler.enter(delay, priority, task, argument=args, kwargs=kwargs) return event def cancel_task(self, event): Cancel a scheduled task. with self.lock: self.scheduler.cancel(event) def run_all(self, blocking=True): Run all scheduled tasks. if blocking: self.scheduler.run(blocking=True) else: while not self.scheduler.empty(): if self.scheduler.queue[0].time <= self.scheduler.timefunc(): self.scheduler.run(blocking=False) else: break def is_empty(self): Check if there are any scheduled tasks. with self.lock: return self.scheduler.empty()"},{"question":"Coding Assessment Question # Objective: You are given a binary classification problem where the cost of false negatives is significantly higher than the cost of false positives. You need to train a classifier, tune its decision threshold to prioritize recall, and evaluate its performance. # Requirements: 1. Implement a decision tree classifier for the binary classification task using the `DecisionTreeClassifier`. 2. Use `TunedThresholdClassifierCV` to optimize the decision threshold for this classifier, ensuring high recall. 3. Evaluate the tuned model\'s performance using appropriate metrics and compare it with the default threshold model. # Input: - A feature matrix `X` with shape (n_samples, n_features). - A target vector `y` with shape (n_samples,) containing binary labels (0 or 1). # Output: - The recall and precision of the default threshold model. - The recall and precision of the tuned threshold model. # Constraints: - Use a decision tree with a maximum depth of 2. - Use a 5-fold stratified cross-validation for tuning the decision threshold. - Optimize the threshold to prioritize recall. # Performance Requirements: - Ensure the recall is maximized by adjusting the decision threshold appropriately. - Precision should be reported as part of the evaluation but recall is the primary metric. # Example Here is a code outline you can follow: ```python import numpy as np from sklearn.datasets import make_classification from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import recall_score, precision_score, make_scorer # Generate synthetic binary classification data X, y = make_classification(random_state=0) # Step 1: Train a DecisionTreeClassifier classifier = DecisionTreeClassifier(max_depth=2, random_state=0) classifier.fit(X, y) # Step 2: Predict using default threshold and evaluate y_pred_default = classifier.predict(X) recall_default = recall_score(y, y_pred_default) precision_default = precision_score(y, y_pred_default) # Step 3: Tune the threshold using TunedThresholdClassifierCV to maximize recall scorer = make_scorer(recall_score) tuned_classifier = TunedThresholdClassifierCV( classifier, scoring=scorer, cv=5 ) tuned_classifier.fit(X, y) y_pred_tuned = tuned_classifier.predict(X) # Step 4: Evaluate the tuned model recall_tuned = recall_score(y, y_pred_tuned) precision_tuned = precision_score(y, y_pred_tuned) # Output results print(f\\"Default Model - Recall: {recall_default:.2f}, Precision: {precision_default:.2f}\\") print(f\\"Tuned Model - Recall: {recall_tuned:.2f}, Precision: {precision_tuned:.2f}\\") ``` # Submission: Submit your Python function that follows the provided outline and correctly computes the requested metrics. Ensure your code runs efficiently and meets the performance requirements.","solution":"from sklearn.datasets import make_classification from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_predict, StratifiedKFold from sklearn.metrics import recall_score, precision_score, make_scorer import numpy as np def optimize_and_evaluate(X, y): # Step 1: Train a DecisionTreeClassifier classifier = DecisionTreeClassifier(max_depth=2, random_state=0) classifier.fit(X, y) # Step 2: Predict using default threshold and evaluate y_pred_default = classifier.predict(X) recall_default = recall_score(y, y_pred_default) precision_default = precision_score(y, y_pred_default) # Step 3: Tune the threshold using K-Fold CV to maximize recall cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0) proba = cross_val_predict(classifier, X, y, cv=cv, method=\\"predict_proba\\")[:, 1] best_recall = 0 best_precision = 0 best_threshold = 0.5 # Iterate over potential threshold values from 0.01 to 0.99 for threshold in np.arange(0.01, 1, 0.01): y_pred_threshold = (proba >= threshold).astype(int) current_recall = recall_score(y, y_pred_threshold) current_precision = precision_score(y, y_pred_threshold) if current_recall > best_recall: best_recall = current_recall best_precision = current_precision best_threshold = threshold # Use the best threshold found y_pred_tuned = (proba >= best_threshold).astype(int) recall_tuned = recall_score(y, y_pred_tuned) precision_tuned = precision_score(y, y_pred_tuned) return recall_default, precision_default, recall_tuned, precision_tuned"},{"question":"You are tasked with parallelizing the execution of a computationally intensive function while managing the results efficiently. You must use Pythons `multiprocessing` package to achieve this. Specifically, you will create a program that computes the sum of squares for a given list of integers using multiple processes to speed up the execution. # Implementation Details 1. **Function Definition**: - Define a function `compute_sum_of_squares(numbers: List[int]) -> int`, which returns the sum of the squares for a given list of integers. 2. **Parallel Execution**: - Use Pythons `multiprocessing.Pool` to parallelize the computation. Each worker process should compute the sum of squares for a sublist of the original list. - Combine the results from the worker processes to obtain the final sum of squares. 3. **Communication**: - Ensure proper communication and result aggregation using appropriate IPC (Inter-Process Communication) mechanisms like `Queue` or `Pipe`. # Input and Output - **Input**: The function `compute_sum_of_squares` should take a single argument, a list of integers `numbers`. - **Output**: The function should return a single integer, the sum of the squares of the input list. - **Constraints**: - The function should be efficiently parallelized. - You must handle any potential issues with process synchronization. # Sample Usage ```python if __name__ == \'__main__\': numbers = [1, 2, 3, 4, 5] result = compute_sum_of_squares(numbers) print(result) # Output should be 55 ``` # Additional Guidelines - Use a maximum of 4 worker processes in the pool. - Handle edge cases such as an empty list or very large integers. - Consider potential performance bottlenecks and optimize where necessary. - Include appropriate comments to explain each part of the implementation. # Your task Implement the `compute_sum_of_squares` function using the guidelines mentioned above.","solution":"from multiprocessing import Pool def sum_of_squares(numbers): Helper function to compute sum of squares of a list of numbers. return sum(x ** 2 for x in numbers) def compute_sum_of_squares(numbers): Compute the sum of squares of a list of integers using multiple processes. if not numbers: return 0 # Determine the number of worker processes to use num_workers = 4 # Split the numbers into roughly equal chunks chunk_size = (len(numbers) + num_workers - 1) // num_workers # This ensures we handle edge cases where length may not be divisible evenly chunks = [numbers[i:i + chunk_size] for i in range(0, len(numbers), chunk_size)] with Pool(processes=num_workers) as pool: results = pool.map(sum_of_squares, chunks) # Sum up all the partial results return sum(results)"},{"question":"# Objective: Implement a function that demonstrates the utilization of the `tarfile` module for safe extraction of files from a tar archive, applying necessary security filters. # Problem Statement: You are provided with a tar archive file that contains multiple files and folders. Your task is to safely extract specific files with a given extension while ensuring that no malicious files are extracted. # Function Signature: ```python def safe_extract_tarfile(tar_file_path: str, extract_to: str, file_extension: str) -> None: Extracts files with a specific extension from a tar archive to a given directory safely. Parameters: tar_file_path (str): Path to the tar file. extract_to (str): Directory where the files should be extracted. file_extension (str): The extension of files to extract (e.g., \'.txt\'). Returns: None pass ``` # Requirements: 1. The function should open the tar archive from the specified path. 2. It should iterate over the members of the tar archive and extract only the files that match the specified extension. 3. Ensure that the extraction is safe and does not include any malicious paths (e.g., files with absolute paths or paths containing `..`). 4. Apply necessary filters to prevent extraction of malicious files (use the `data_filter` provided by the `tarfile` module). 5. Handle potential errors gracefully and ensure that the archive is closed properly in case of any errors. # Constraints: - The function should handle various compression types (`gzip`, `bz2`, `xz`, or no compression). - Ensure that extraction does not create files outside the intended directory. - File paths should be validated to ensure they are safe. # Example Usage: ```python safe_extract_tarfile(\'sample.tar.gz\', \'/destination/path\', \'.txt\') ``` # Hints: - Use `tarfile.open()` to open the tar archive. - Use a generator function or a list comprehension to filter files based on the desired extension. - Employ `tarfile.data_filter` to apply security measures during extraction. **Notes**: - You might want to review the `TarFile.extractall()` and `extract()` methods and how to use the `filter` parameter for secure extraction. - Consider using context management for efficient file handling and to ensure resources are properly managed. # Performance Requirements: - The function should efficiently handle large tar files and multiple files extraction. - Ensure minimal memory usage by streaming extraction where possible.","solution":"import os import tarfile def is_safe_path(base_path, target_path, follow_symlinks=True): if follow_symlinks: return os.path.realpath(target_path).startswith(os.path.realpath(base_path)) return os.path.abspath(target_path).startswith(os.path.abspath(base_path)) def safe_extract_tarfile(tar_file_path: str, extract_to: str, file_extension: str) -> None: Extracts files with a specific extension from a tar archive to a given directory safely. Parameters: tar_file_path (str): Path to the tar file. extract_to (str): Directory where the files should be extracted. file_extension (str): The extension of files to extract (e.g., \'.txt\'). Returns: None if not os.path.exists(extract_to): os.makedirs(extract_to) with tarfile.open(tar_file_path, \'r:*\') as tar: for member in tar.getmembers(): if member.isfile() and member.name.endswith(file_extension): member_path = os.path.join(extract_to, member.name) if is_safe_path(extract_to, member_path): tar.extract(member, path=extract_to) else: print(f\\"Skipping potentially unsafe file: {member.name}\\")"},{"question":"**Question:** # Code Object Introspection and Manipulation You are asked to work with Python\'s code objects to demonstrate an understanding of code introspection and manipulation using low-level details as specified in the CPython implementation. In this task, you will create a utility function that initializes and prints details of a code object. Part 1: Write a function `create_empty_code_object` that takes three arguments: `filename`, `funcname`, and `firstlineno`, and returns a new empty code object using the `PyCode_NewEmpty` function. ```python def create_empty_code_object(filename: str, funcname: str, firstlineno: int) -> object: Creates a new empty code object with the specified filename, function name, and first line number. Args: filename (str): The name of the file where the code object is located. funcname (str): The name of the function. firstlineno (int): The first line number in the file for the code object. Returns: object: A new empty code object. pass ``` Part 2: Write another function `print_code_object_details` that takes a code object as an argument and prints the following details: 1. Number of free variables. 2. Line number for a given byte offset (use `PyCode_Addr2Line` with byte offset 0). 3. Confirmation that the object is indeed a code object using `PyCode_Check`. ```python def print_code_object_details(code_obj: object) -> None: Prints details of a given code object. Args: code_obj (object): The code object to analyze. Prints: - Number of free variables in the code object. - Line number for byte offset 0. - Confirmation that the object is a code object. pass ``` Input and Output: - Input: Strings for `filename` and `funcname`, and integers for `firstlineno` in the `create_empty_code_object` function. - Output: The `create_empty_code_object` function should return a code object. The `print_code_object_details` function should print the required details to the console. Constraints: - The implementation must use the functions and structure details as provided in the documentation. - Handle exception cases where necessary. Example Usage: ```python code_obj = create_empty_code_object(\\"example.py\\", \\"example_func\\", 1) print_code_object_details(code_obj) ``` This should print: ``` Number of free variables: 0 Line number for byte offset 0: 1 Is code object: True ``` **Note:** You may simulate the low-level functions if you can\'t access the actual library functions directly in your execution environment.","solution":"import types def create_empty_code_object(filename: str, funcname: str, firstlineno: int) -> types.CodeType: Creates a new empty code object with the specified filename, function name, and first line number. Args: filename (str): The name of the file where the code object is located. funcname (str): The name of the function. firstlineno (int): The first line number in the file for the code object. Returns: object: A new empty code object. return types.CodeType( 0, # argcount 0, # posonlyargcount 0, # kwonlyargcount 0, # nlocals 0, # stacksize 0, # flags b\'\', # codestring (), # constants (), # names (), # varnames filename, # filename funcname, # name firstlineno, # firstlineno b\'\' # lnotab ) def print_code_object_details(code_obj: types.CodeType) -> None: Prints details of a given code object. Args: code_obj (object): The code object to analyze. Prints: - Number of free variables in the code object. - Line number for byte offset 0. - Confirmation that the object is a code object. num_freevars = len(code_obj.co_freevars) line_number = code_obj.co_firstlineno is_code_object = isinstance(code_obj, types.CodeType) print(f\\"Number of free variables: {num_freevars}\\") print(f\\"Line number for byte offset 0: {line_number}\\") print(f\\"Is code object: {is_code_object}\\")"},{"question":"Advanced Plotting with Seaborn Objective: Demonstrate your understanding of seaborns `objects` interface by loading a dataset, creating a complex plot with multiple layers, and customizing its aesthetics. Question: Write a function `create_penguins_plot()` that performs the following steps: 1. Load the \\"penguins\\" dataset from seaborn. 2. Create a faceted plot of penguin body mass (`body_mass_g`) with the following requirements: - The x-axis should represent penguin sex (`sex`). - The y-axis should represent body mass (`body_mass_g`). - Facet the plot by penguin species (`species`). - Add a layer of individual data points using `so.Dots()` with `pointsize` set to 8. - Add a layer to show the mean body mass with standard deviation error bars using `so.Line()` and `so.Range()`. These should be colored differently based on the penguin\'s island (`island`). - Customize the plot to use different `linestyles` and `linewidths` for different species. The function should output the plot object. Implementation: ```python import seaborn.objects as so from seaborn import load_dataset def create_penguins_plot(): # Step 1: Load the \\"penguins\\" dataset penguins = load_dataset(\\"penguins\\") # Step 2: Create the faceted plot with the specified layers and customizations p = ( so.Plot(penguins, x=\\"sex\\", y=\\"body_mass_g\\", linestyle=\\"species\\", linewidth=2) .facet(\\"species\\") .add(so.Dots(pointsize=8)) .add(so.Line(), so.Agg(), color=\\"island\\") .add(so.Range(), so.Est(errorbar=\\"sd\\"), color=\\"island\\") ) return p # Example usage: # plot = create_penguins_plot() # plot.show() ``` Constraints: - Use only seaborn and other standard Python libraries. - Ensure the function and plot conform to the specifications. Expected output is the plot object configured as described in the question. Test your function by calling it and displaying the plot within a Jupyter notebook or any Python environment that supports plot rendering.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguins_plot(): # Step 1: Load the \\"penguins\\" dataset penguins = load_dataset(\\"penguins\\") # Step 2: Create the faceted plot with the specified layers and customizations p = ( so.Plot(penguins, x=\\"sex\\", y=\\"body_mass_g\\", linestyle=\\"species\\", linewidth=2) .facet(\\"species\\") .add(so.Dots(pointsize=8)) .add(so.Line(), so.Agg(), color=\\"island\\") .add(so.Range(), so.Est(errorbar=\\"sd\\"), color=\\"island\\") ) return p"},{"question":"# Secure File Integrity Verification with HMAC You have been hired as a security expert to ensure the data integrity of files sent over a network. To achieve this, you need to calculate and verify the HMAC of files using different hash algorithms. Implement the following two functions: 1. `generate_file_hmac(filepath: str, key: bytes, digestmod: str) -> str`: This function calculates the HMAC for a given file. - **Parameters:** - `filepath`: Path to the file for which HMAC needs to be calculated. - `key`: Secret key used to calculate the HMAC. - `digestmod`: Name of the digest algorithm to use (e.g., \'sha256\', \'sha1\'). - **Returns:** - A string representing the hexadecimal HMAC of the file. - **Constraints:** - File size should not exceed 100 MB. - The key should be at least 16 bytes long. - The digest algorithm should be one of the algorithms supported by OpenSSL. 2. `verify_file_hmac(filepath: str, key: bytes, expected_hmac: str, digestmod: str) -> bool`: This function verifies the HMAC of a given file against an expected HMAC. - **Parameters:** - `filepath`: Path to the file for which HMAC needs to be verified. - `key`: Secret key used to calculate the HMAC. - `expected_hmac`: The HMAC to compare against. - `digestmod`: Name of the digest algorithm to use (e.g., \'sha256\', \'sha1\'). - **Returns:** - `True` if the calculated HMAC matches the expected HMAC, otherwise `False`. - **Constraints:** - File size should not exceed 100 MB. - The key should be at least 16 bytes long. - The digest algorithm should be one of the algorithms supported by OpenSSL. **Example Usage:** ```python # Assuming we have a file named \'example.txt\', a key, and we want to use \'sha256\' as the digest algorithm file_path = \'example.txt\' key = b\'secretkey1234567\' digestmod = \'sha256\' # Generate HMAC for the file file_hmac = generate_file_hmac(file_path, key, digestmod) print(f\'HMAC for the file: {file_hmac}\') # Verify HMAC for the file with the expected HMAC is_valid = verify_file_hmac(file_path, key, file_hmac, digestmod) print(f\'Is valid HMAC: {is_valid}\') ``` **Note**: Ensure that you handle potential exceptions that may arise due to missing files, unsupported digest algorithms, or invalid keys.","solution":"import hmac import hashlib def generate_file_hmac(filepath: str, key: bytes, digestmod: str) -> str: Calculates the HMAC for a given file. Parameters: filepath (str): Path to the file for which HMAC needs to be calculated. key (bytes): Secret key used to calculate the HMAC. digestmod (str): Name of the digest algorithm to use (e.g., \'sha256\', \'sha1\'). Returns: str: A string representing the hexadecimal HMAC of the file. if len(key) < 16: raise ValueError(\\"Key must be at least 16 bytes long.\\") hash_obj = hmac.new(key, digestmod=digestmod) with open(filepath, \'rb\') as file: while chunk := file.read(8192): hash_obj.update(chunk) return hash_obj.hexdigest() def verify_file_hmac(filepath: str, key: bytes, expected_hmac: str, digestmod: str) -> bool: Verifies the HMAC of a given file against an expected HMAC. Parameters: filepath (str): Path to the file for which HMAC needs to be verified. key (bytes): Secret key used to calculate the HMAC. expected_hmac (str): The HMAC to compare against. digestmod (str): Name of the digest algorithm to use (e.g., \'sha256\', \'sha1\'). Returns: bool: True if the calculated HMAC matches the expected HMAC, otherwise False. try: calculated_hmac = generate_file_hmac(filepath, key, digestmod) return hmac.compare_digest(calculated_hmac, expected_hmac) except (FileNotFoundError, ValueError) as e: print(f\\"Error: {e}\\") return False"},{"question":"Advanced Linear Models with Scikit-Learn **Problem Statement:** You are working as a data scientist who needs to explore and compare different linear models to predict the house prices based on a dataset containing various features such as the number of bedrooms, square footage, etc. Your task is to create a Python function that uses scikit-learn to fit several linear models to the dataset and evaluate their performance. **Function Signature:** ```python def evaluate_linear_models(X_train, y_train, X_test, y_test): Fits and evaluates various linear models on the provided dataset. Parameters: X_train (numpy.ndarray): Training features, shape (n_samples_train, n_features) y_train (numpy.ndarray): Training target values, shape (n_samples_train,) X_test (numpy.ndarray): Testing features, shape (n_samples_test, n_features) y_test (numpy.ndarray): Testing target values, shape (n_samples_test,) Returns: dict: A dictionary containing model names as keys and their corresponding test R^2 scores and coefficients. ``` **Input:** - `X_train`: A 2D numpy array of training features. - `y_train`: A 1D numpy array of training target values. - `X_test`: A 2D numpy array of testing features. - `y_test`: A 1D numpy array of testing target values. **Output:** - A dictionary where the keys are the names of the models (`\\"LinearRegression\\"`, `\\"Ridge\\"`, `\\"Lasso\\"`, `\\"ElasticNet\\"`, `\\"BayesianRidge\\"`, `\\"HuberRegressor\\"`) and the values are dictionaries containing: - `\\"r2_score\\"`: The R^2 score of the model on the test set. - `\\"coefficients\\"`: The coefficients of the model. **Constraints:** - Use `scikit-learn` to implement the models. - Tune the `alpha` parameter for Ridge, Lasso, and ElasticNet models to achieve the best performance using cross-validation. - For HuberRegressor, use the standard epsilon value of 1.35. **Performance Requirements:** - The function should handle datasets with large `n_samples` (up to 100,000) and `n_features` (up to 100). - Ensure the function runs efficiently and does not exceed reasonable memory usage limits. **Example Usage:** ```python import numpy as np from sklearn.model_selection import train_test_split # Generate some synthetic data np.random.seed(42) X = np.random.rand(100, 5) y = 3*X[:, 0] + 2*X[:, 1] - X[:, 2] + np.random.rand(100) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) results = evaluate_linear_models(X_train, y_train, X_test, y_test) print(results) ``` **Note:** - You are encouraged to explore the documentation and examples provided by scikit-learn to understand the implementation details of the different models. - Ensure your code is well-documented and includes comments explaining the key steps.","solution":"import numpy as np from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, HuberRegressor from sklearn.model_selection import GridSearchCV from sklearn.metrics import r2_score def evaluate_linear_models(X_train, y_train, X_test, y_test): Fits and evaluates various linear models on the provided dataset. Parameters: X_train (numpy.ndarray): Training features, shape (n_samples_train, n_features) y_train (numpy.ndarray): Training target values, shape (n_samples_train,) X_test (numpy.ndarray): Testing features, shape (n_samples_test, n_features) y_test (numpy.ndarray): Testing target values, shape (n_samples_test,) Returns: dict: A dictionary containing model names as keys and their corresponding test R^2 scores and coefficients. results = {} # Linear Regression linear_reg = LinearRegression() linear_reg.fit(X_train, y_train) y_pred = linear_reg.predict(X_test) results[\'LinearRegression\'] = { \'r2_score\': r2_score(y_test, y_pred), \'coefficients\': linear_reg.coef_ } # Ridge Regression with Cross-Validation to find optimal alpha ridge_reg = Ridge() ridge_params = {\'alpha\': [0.1, 1.0, 10.0, 100.0]} ridge_cv = GridSearchCV(ridge_reg, ridge_params, cv=5) ridge_cv.fit(X_train, y_train) best_ridge = ridge_cv.best_estimator_ y_pred = best_ridge.predict(X_test) results[\'Ridge\'] = { \'r2_score\': r2_score(y_test, y_pred), \'coefficients\': best_ridge.coef_ } # Lasso Regression with Cross-Validation to find optimal alpha lasso_reg = Lasso() lasso_params = {\'alpha\': [0.001, 0.01, 0.1, 1.0, 10.0]} lasso_cv = GridSearchCV(lasso_reg, lasso_params, cv=5) lasso_cv.fit(X_train, y_train) best_lasso = lasso_cv.best_estimator_ y_pred = best_lasso.predict(X_test) results[\'Lasso\'] = { \'r2_score\': r2_score(y_test, y_pred), \'coefficients\': best_lasso.coef_ } # ElasticNet Regression with Cross-Validation to find optimal alpha elastic_net_reg = ElasticNet() elastic_net_params = {\'alpha\': [0.001, 0.01, 0.1, 1.0, 10.0]} elastic_net_cv = GridSearchCV(elastic_net_reg, elastic_net_params, cv=5) elastic_net_cv.fit(X_train, y_train) best_elastic_net = elastic_net_cv.best_estimator_ y_pred = best_elastic_net.predict(X_test) results[\'ElasticNet\'] = { \'r2_score\': r2_score(y_test, y_pred), \'coefficients\': best_elastic_net.coef_ } # Bayesian Ridge Regression bayesian_ridge = BayesianRidge() bayesian_ridge.fit(X_train, y_train) y_pred = bayesian_ridge.predict(X_test) results[\'BayesianRidge\'] = { \'r2_score\': r2_score(y_test, y_pred), \'coefficients\': bayesian_ridge.coef_ } # Huber Regressor huber_regressor = HuberRegressor() huber_regressor.fit(X_train, y_train) y_pred = huber_regressor.predict(X_test) results[\'HuberRegressor\'] = { \'r2_score\': r2_score(y_test, y_pred), \'coefficients\': huber_regressor.coef_ } return results"},{"question":"# Objective Write a Python program that uses the `trace` module to trace the execution of a function that sorts a list of numbers. Your task is to use the `trace.Trace` class to count how many times each line in the sorting algorithm is executed and generate the coverage report. # Requirements 1. Implement a sorting function (any sorting algorithm of your choice, like quicksort, mergesort, etc.) that you will trace. 2. Use the `trace.Trace` class to trace the execution of the sorting function. 3. Generate a coverage report that shows the execution count for each line in the sorting function. 4. Write your own code and do not use built-in sorting functions for the implementation. # Input A list of integers to be sorted. # Output A sorted list of integers and a coverage report showing the number of times each line in the sorting function was executed. # Constraints - The list of integers can contain both positive and negative numbers. - The length of the list will be between 5 and 100 inclusive. # Example ```python from trace import Trace def sorting_algorithm(nums): # Implementation of your sorting algorithm here pass def run_trace(nums): tracer = Trace(count=1, trace=0) tracer.runfunc(sorting_algorithm, nums) results = tracer.results() results.write_results(show_missing=True) if __name__ == \'__main__\': nums = [4, 2, 7, 1, 9, 3] print(\\"Sorted List:\\", sorting_algorithm(nums)) run_trace(nums) ``` In the example above, replace `# Implementation of your sorting algorithm here` with the code for your chosen sorting algorithm. The `run_trace` function should be used to trace the execution of the `sorting_algorithm` function and generate the coverage report. # Note You must ensure that the coverage report is saved to the current directory or printed out, showing the number of times each line in the sorting function was executed.","solution":"from trace import Trace def quicksort(nums): Implementation of the quicksort algorithm. if len(nums) <= 1: return nums pivot = nums[len(nums) // 2] left = [x for x in nums if x < pivot] middle = [x for x in nums if x == pivot] right = [x for x in nums if x > pivot] return quicksort(left) + middle + quicksort(right) def run_trace(nums): tracer = Trace(count=1, trace=0) tracer.runfunc(quicksort, nums) results = tracer.results() results.write_results(show_missing=True, summary=True) if __name__ == \'__main__\': nums = [4, 2, 7, 1, 9, 3] print(\\"Sorted List:\\", quicksort(nums)) run_trace(nums)"},{"question":"# Question: Resource Management Using `resource` Module You have been tasked with monitoring and setting resource limits for a Python application to ensure it does not consume excessive resources. Specifically, you need to: 1. Retrieve the current CPU time limit and maximum file size limit for the application. 2. Set a new CPU time limit for the application to 30 seconds. 3. Retrieve the resource usage statistics for the application after running a simulated CPU-bound task. 4. Ensure that the program raises an appropriate exception if an invalid resource is specified. Implement a Python function `manage_resources` that performs the following tasks: 1. Retrieves and prints the current CPU time limit and maximum file size limit using `getrlimit`. 2. Sets a new CPU time limit of 30 seconds using `setrlimit`. 3. Simulates a CPU-bound task by performing calculations in a loop and prints the resource usage statistics using `getrusage`. 4. Attempts to retrieve a limit for an invalid resource ID and handles the exception by printing an error message. Function Signature: ```python def manage_resources(): pass ``` Expected Output: - The current CPU time limit. - The current file size limit. - Confirmation that the CPU time limit has been set to 30 seconds. - Resource usage statistics after performing the CPU-bound task. - Error message if an invalid resource ID is specified. Constraints: - Assume the environment where the function runs supports all mentioned resource constants. - Do not actually perform any resource-intensive tasks that could impact other processes on the system. Example Usage: ```python manage_resources() ``` Example Output: ``` Current CPU time limit: (soft_limit, hard_limit) Current file size limit: (soft_limit, hard_limit) Set CPU time limit to 30 seconds. Resource usage after task: ru_utime=..., ru_stime=..., other_stats=... Error: Invalid resource specified. ``` Make sure to use the appropriate functions and constants from the `resource` module to complete the task.","solution":"import resource import time def manage_resources(): # Step 1: Retrieve and print the current CPU time limit and maximum file size limit cpu_time_limit = resource.getrlimit(resource.RLIMIT_CPU) file_size_limit = resource.getrlimit(resource.RLIMIT_FSIZE) print(f\\"Current CPU time limit: {cpu_time_limit}\\") print(f\\"Current file size limit: {file_size_limit}\\") # Step 2: Set a new CPU time limit of 30 seconds resource.setrlimit(resource.RLIMIT_CPU, (30, cpu_time_limit[1])) print(\\"Set CPU time limit to 30 seconds.\\") # Step 3: Simulate a CPU-bound task and print resource usage statistics start_time = time.time() end_time = start_time + 5 # Simulate a task that runs for 5 seconds while time.time() < end_time: pass usage = resource.getrusage(resource.RUSAGE_SELF) print(f\\"Resource usage after task: user_time={usage.ru_utime}, system_time={usage.ru_stime}, maxrss={usage.ru_maxrss}\\") # Step 4: Attempt to retrieve a limit for an invalid resource ID and handle the exception try: invalid_limit = resource.getrlimit(-1) except ValueError as e: print(f\\"Error: Invalid resource specified. {e}\\")"},{"question":"**Objective**: Demonstrate your understanding of the `unittest.mock` library by effectively using mock objects to isolate and test a function\'s behavior. Question You have a class `DataProcessor` that processes data by reading from a file and then performing some transformations based on an additional helper class `Transformer`. Your task is to write unit tests for the `process_data` method of the `DataProcessor` class using the `unittest.mock` library. The `DataProcessor` class is defined as follows: ```python class DataProcessor: def __init__(self, transformer): self.transformer = transformer def process_data(self, filename): with open(filename, \'r\') as file: data = file.readlines() transformed_data = [self.transformer.transform(line) for line in data] return transformed_data ``` The `Transformer` class is defined as follows: ```python class Transformer: def transform(self, line): return line.strip().upper() ``` # Requirements: 1. Write a test class `TestDataProcessor` containing test methods for the `process_data` method. 2. Use the `unittest.mock` library to: - Mock the `Transformer` class. - Mock the built-in `open` function to simulate reading from a file. 3. Write at least two test methods: - Test that `process_data` correctly reads from the file and calls the `transform` method of the `Transformer` class. - Test that `process_data` correctly returns the transformed data. # Input and Output Formats - Input: The test methods do not take any additional input, but they will mock file contents and the `Transformer` class behavior. - Output: The test methods should use assertions to verify the behavior of `process_data`. # Constraints - Use the `unittest` framework and `unittest.mock` for creating mocks. - You should not modify the `DataProcessor` or `Transformer` classes. # Example Here is an example of how the test class might look: ```python import unittest from unittest.mock import Mock, patch, mock_open from your_module import DataProcessor, Transformer # Replace \'your_module\' with the actual module name class TestDataProcessor(unittest.TestCase): @patch(\'your_module.open\', new_callable=mock_open, read_data=\\"line1nline2nline3n\\") @patch(\'your_module.Transformer\') def test_process_data(self, MockTransformer, mock_open): # Arrange mock_transformer = MockTransformer.return_value mock_transformer.transform.side_effect = lambda x: x.strip().upper() processor = DataProcessor(mock_transformer) # Act result = processor.process_data(\'dummy_filename.txt\') # Assert mock_transformer.transform.assert_any_call(\'line1n\') mock_transformer.transform.assert_any_call(\'line2n\') mock_transformer.transform.assert_any_call(\'line3n\') self.assertEqual(result, [\'LINE1\', \'LINE2\', \'LINE3\']) ``` Make sure to replace \'your_module\' with the actual module name where the `DataProcessor` and `Transformer` classes are defined.","solution":"import unittest from unittest.mock import Mock, patch, mock_open class DataProcessor: def __init__(self, transformer): self.transformer = transformer def process_data(self, filename): with open(filename, \'r\') as file: data = file.readlines() transformed_data = [self.transformer.transform(line) for line in data] return transformed_data class Transformer: def transform(self, line): return line.strip().upper()"},{"question":"You are tasked with creating a configuration management tool for a macOS application. The configurations need to be stored in a plist file. You have been provided with a basic structure, and your task is to implement and test the serialization and deserialization of the configuration data using the `plistlib` module. Specifically, you will create functions to save and load configurations from a file, and ensure the data can be read back correctly. # Question 1. **Write a function `save_config(config, filename)` that:** - Takes a dictionary `config` representing the configuration data. - Takes a string `filename` representing the name of the file to save the configuration to. - Serializes the configuration dictionary to a plist file using the XML format, ensuring keys are sorted. 2. **Write a function `load_config(filename)` that:** - Takes a string `filename` representing the name of the file to load the configuration from. - Reads the plist file and returns the configuration as a dictionary. # Expected Input and Output Formats: - The `config` dictionary may contain strings, integers, floats, booleans, lists, dictionaries (with string keys), `bytes`, `bytearray`, or `datetime.datetime` objects. - The `filename` is the name of a file to which the plist will be written or from which it will be read. - The functions should handle exceptions if the file format is incorrect or if there are unsupported types. # Example: ```python import datetime import plistlib def save_config(config, filename): # Your implementation here pass def load_config(filename): # Your implementation here pass if __name__ == \\"__main__\\": config = { \'app_name\': \'MyApp\', \'version\': 1.0, \'release_date\': datetime.datetime(2023, 10, 10, 0, 0), \'features\': [\'feature1\', \'feature2\'], \'settings\': { \'theme\': \'dark\', \'auto_update\': True, } } filename = \'config.plist\' save_config(config, filename) loaded_config = load_config(filename) assert loaded_config == config print(\\"Configuration saved and loaded successfully!\\") ``` # Constraints: - Use the `plistlib` module for all plist file operations. - The plist file should be in XML format. - Ensure keys are sorted in the plist file. - Handle cases where the file cannot be read or written due to improper format or other I/O issues. # Performance Requirements: - The solution should efficiently handle configuration files up to 1MB in size.","solution":"import plistlib def save_config(config, filename): Saves the configuration dictionary as a plist file. Parameters: config (dict): Configuration data. filename (str): Path to the file where data will be saved. with open(filename, \'wb\') as file: plistlib.dump(config, file, fmt=plistlib.FMT_XML, sort_keys=True) def load_config(filename): Loads the configuration dictionary from a plist file. Parameters: filename (str): Path to the file from which data will be loaded. Returns: dict: Configuration data. with open(filename, \'rb\') as file: return plistlib.load(file)"},{"question":"# Attention Mechanism with Causal Bias in PyTorch Your task is to implement an attention mechanism using PyTorch, incorporating a causal bias. This mechanism should ensure that the attention computation for a given position only considers previous positions in the sequence. Objectives: 1. Implement a PyTorch function `causal_attention` that computes the attention scores for a given query, key, and value matrices using a causal mask. 2. Ensure that your function uses the `CausalBias` class or its relevant variants provided by PyTorch. Function Signature: ```python def causal_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor: Compute the attention scores with causal bias. :param query: A tensor of shape (batch_size, seq_length, dim) :param key: A tensor of shape (batch_size, seq_length, dim) :param value: A tensor of shape (batch_size, seq_length, dim) :return: A tensor of shape (batch_size, seq_length, dim) with computed attention scores. ``` Input: - `query`, `key`, and `value` are 3-dimensional tensors of shape `(batch_size, seq_length, dim)`, where: - `batch_size` is the number of sequences in a batch. - `seq_length` is the length of each sequence. - `dim` is the dimensionality of the embeddings. Output: - Return a tensor of shape `(batch_size, seq_length, dim)` containing the computed attention scores. Constraints: - You must ensure the attention mechanism respects the causal property, i.e., each position in the sequence can only attend to previous positions (including itself). - The implementation should efficiently handle batch operations. Example: ```python import torch batch_size = 2 seq_length = 5 dim = 4 query = torch.rand((batch_size, seq_length, dim)) key = torch.rand((batch_size, seq_length, dim)) value = torch.rand((batch_size, seq_length, dim)) output = causal_attention(query, key, value) print(output.shape) # Expected Output: torch.Size([2, 5, 4]) ``` **Note**: Ensure that your implementation leverages the PyTorch features and optimizations for handling causal biases, referring to the relevant `CausalBias` class and functions.","solution":"import torch import torch.nn.functional as F def causal_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor: Compute the attention scores with causal bias. :param query: A tensor of shape (batch_size, seq_length, dim) :param key: A tensor of shape (batch_size, seq_length, dim) :param value: A tensor of shape (batch_size, seq_length, dim) :return: A tensor of shape (batch_size, seq_length, dim) with computed attention scores. batch_size, seq_length, dim = query.size() # Compute scaled dot-product attention scores scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(dim, dtype=torch.float32)) # Create a causal mask mask = torch.triu(torch.ones(seq_length, seq_length, device=scores.device) * float(\'-inf\'), diagonal=1) # Apply mask to scores scores = scores + mask # Apply softmax to get attention weights attn_weights = F.softmax(scores, dim=-1) # Compute the weighted sum of values output = torch.matmul(attn_weights, value) return output"},{"question":"Background: You are given a dataset of multiple URLs, and your task is to fetch data from these URLs concurrently to speed up the process using the `concurrent.futures` module. Fetching data from a single URL is done using the `requests` library, but to optimize and handle multiple requests efficiently, you will employ a thread pool. Problem Statement: Write a Python function `fetch_urls_concurrently(urls: List[str], max_workers: int) -> List[Tuple[str, Union[Dict, None]]]` that takes a list of URLs and the maximum number of worker threads as input. The function should fetch the content of each URL concurrently and return a list of tuples, where each tuple contains the URL and the resulting JSON data (as a dictionary) or `None` if an error occurred during the fetch operation. Input/Output Format: - **Input:** - `urls`: A list of strings where each string is a URL to be fetched. - `max_workers`: An integer specifying the maximum number of worker threads. - **Output:** - A list of tuples, where each tuple has the format `(url, data)`. - `url`: A string representing the URL. - `data`: A dictionary representing the JSON data fetched from the URL or `None` if an error occurred. Constraints and Considerations: - Use `ThreadPoolExecutor` from the `concurrent.futures` module to manage the concurrency. - Use the `requests` library to perform HTTP GET requests. - Handle exceptions gracefully, ensuring that the program does not crash if a URL cannot be fetched. Example: ```python import requests from concurrent.futures import ThreadPoolExecutor, as_completed from typing import List, Tuple, Union, Dict def fetch_url(url: str) -> Union[Dict, None]: try: response = requests.get(url) response.raise_for_status() # Raise HTTPError for bad responses return response.json() except (requests.RequestException, ValueError): return None def fetch_urls_concurrently(urls: List[str], max_workers: int) -> List[Tuple[str, Union[Dict, None]]]: results = [] with ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_url = {executor.submit(fetch_url, url): url for url in urls} for future in as_completed(future_to_url): url = future_to_url[future] try: data = future.result() results.append((url, data)) except Exception as e: results.append((url, None)) return results # Example usage: urls = [\'https://jsonplaceholder.typicode.com/posts/1\', \'https://jsonplaceholder.typicode.com/posts/2\'] print(fetch_urls_concurrently(urls, max_workers=4)) ``` Ensure the code works under the constraint of the `requests` library being imported. Performance: - Aim to minimize the total fetch time by using concurrency properly. - Ensure resource utilization (memory, CPU) is efficient by not exceeding the specified number of worker threads.","solution":"import requests from concurrent.futures import ThreadPoolExecutor, as_completed from typing import List, Tuple, Union, Dict def fetch_url(url: str) -> Union[Dict, None]: try: response = requests.get(url) response.raise_for_status() # Raise HTTPError for bad responses return response.json() except (requests.RequestException, ValueError): return None def fetch_urls_concurrently(urls: List[str], max_workers: int) -> List[Tuple[str, Union[Dict, None]]]: results = [] with ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_url = {executor.submit(fetch_url, url): url for url in urls} for future in as_completed(future_to_url): url = future_to_url[future] try: data = future.result() results.append((url, data)) except Exception as e: results.append((url, None)) return results"},{"question":"**Objective:** Design a Python function that simulates the execution of Python code in different modes: complete program mode, interactive mode, and expression evaluation mode. You should demonstrate an understanding of the grammar and execution context for these different modes. **Function Signature:** ```python def execute_python_code(input_type: str, code: str) -> str: Executes the provided Python code based on the input type and returns the result. Parameters: input_type (str): The type of input the code represents. It can be one of the following: \\"complete_program\\", \\"interactive\\", \\"expression\\" code (str): The Python code to be executed. Returns: str: The result of executing the code, or an appropriate error message if the execution fails. ``` **Input:** - `input_type`: A string that specifies the type of input. It can be: - `\\"complete_program\\"`: The code is a complete Python program. - `\\"interactive\\"`: The code is provided as if in interactive mode. - `\\"expression\\"`: The code is a single expression that should be evaluated. - `code`: A string containing the Python code to be executed. **Output:** - A string that represents the result of executing the provided code. If the code is executed successfully, return the output. - If an error occurs during execution, return an appropriate error message. **Constraints:** - You may assume that the input type will always be one of the specified strings. - The `code` string should be a valid Python syntax for the specified input type. **Example:** ```python # Example 1: Complete program code = def greet(): return \\"Hello, World!\\" result = greet() print(result) print(execute_python_code(\\"complete_program\\", code)) # Output: \\"Hello, World!n\\" # Example 2: Interactive input code = a = 10 b = 20 a + b print(execute_python_code(\\"interactive\\", code)) # Output: \\"30n\\" # Example 3: Expression input code = \\"3 * (4 + 5)\\" print(execute_python_code(\\"expression\\", code)) # Output: \\"27n\\" ``` **Notes:** 1. You should use the appropriate functions (`exec`, `eval`, etc.) to execute the code based on the input type. 2. Ensure your function handles errors gracefully and returns meaningful error messages. 3. Consider the environment in which the code is being executed and ensure it mimics the specified input type (complete program, interactive, or expression).","solution":"def execute_python_code(input_type: str, code: str) -> str: from io import StringIO import sys output = StringIO() sys.stdout = output try: if input_type == \\"complete_program\\": exec(code) elif input_type == \\"interactive\\": exec(code) elif input_type == \\"expression\\": result = eval(code) return str(result) else: raise ValueError(\\"Invalid input_type specified.\\") except Exception as e: return f\\"Error: {str(e)}\\" finally: sys.stdout = sys.__stdout__ return output.getvalue()"},{"question":"# Cross-Validation to Evaluate Model Performance Problem Statement You are provided with the famous Iris dataset, which contains 150 samples of iris flowers, each described by four features (sepal length, sepal width, petal length, and petal width) and classified into three species (Setosa, Versicolor, and Virginica). Your task is to: 1. Implement a Support Vector Machine (SVM) classifier with a linear kernel using scikit-learn. 2. Perform 5-fold cross-validation on the dataset to evaluate the classifier. 3. Report the mean accuracy and standard deviation of the model\'s performance across the folds. 4. Use multiple metrics, specifically accuracy, precision, and recall, and report these metrics for each fold. 5. Implement a custom cross-validation iterator that splits the Iris dataset into two folds for training and testing. Input Format - Use the built-in Iris dataset from scikit-learn. Output Format - Print the mean accuracy and the standard deviation of the cross-validation score. - Print the precision and recall for each fold. - Implement and utilize a custom cross-validator to perform a two-fold cross-validation and report the fold indices used. Constraints - Ensure reproducibility by setting a random state where applicable. - Use scikit-learn library functions for the cross-validation. Example Output ```python Mean accuracy: 0.98 Standard deviation of accuracy: 0.02 Precision for each fold: [0.95, 1.00, 0.95, 0.95, 1.00] Recall for each fold: [0.96, 1.00, 0.96, 0.96, 1.00] Custom 2-fold CV indices: First fold - train indices: [ 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] test indices: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] Second fold - train indices: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] test indices: [ 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] ``` Your Implementation ```python import numpy as np from sklearn import datasets from sklearn import svm from sklearn.model_selection import cross_validate, cross_val_score # Load the iris dataset X, y = datasets.load_iris(return_X_y=True) # Initialize the SVM classifier clf = svm.SVC(kernel=\'linear\', C=1, random_state=42) # Perform 5-fold cross-validation and compute metrics scoring = [\'accuracy\', \'precision_macro\', \'recall_macro\'] scores = cross_validate(clf, X, y, cv=5, scoring=scoring, return_train_score=False) # Report the mean accuracy and standard deviation mean_accuracy = scores[\'test_accuracy\'].mean() std_accuracy = scores[\'test_accuracy\'].std() print(f\\"Mean accuracy: {mean_accuracy:.2f}\\") print(f\\"Standard deviation of accuracy: {std_accuracy:.2f}\\") # Report precision and recall for each fold print(f\\"Precision for each fold: {scores[\'test_precision_macro\']}\\") print(f\\"Recall for each fold: {scores[\'test_recall_macro\']}\\") # Implement a custom cross-validator that splits the dataset into two folds def custom_cv_2folds(X): n = X.shape[0] indices = np.arange(n) half = n // 2 yield indices[half:], indices[:half] yield indices[:half], indices[half:] custom_cv = custom_cv_2folds(X) # Use the custom cross-validator for train_idx, test_idx in custom_cv_2folds(X): print(f\\"Train indices: {train_idx}\\") print(f\\"Test indices: {test_idx}\\") ``` Notes - Ensure to handle any exceptions or errors gracefully. - Add appropriate comments to the code to improve readability. - Focus on writing clean, efficient code that adheres to Python best practices.","solution":"import numpy as np from sklearn import datasets from sklearn import svm from sklearn.model_selection import cross_validate # Load the iris dataset X, y = datasets.load_iris(return_X_y=True) # Initialize the SVM classifier clf = svm.SVC(kernel=\'linear\', C=1, random_state=42) # Perform 5-fold cross-validation and compute metrics scoring = [\'accuracy\', \'precision_macro\', \'recall_macro\'] scores = cross_validate(clf, X, y, cv=5, scoring=scoring, return_train_score=False) # Report the mean accuracy and standard deviation mean_accuracy = scores[\'test_accuracy\'].mean() std_accuracy = scores[\'test_accuracy\'].std() print(f\\"Mean accuracy: {mean_accuracy:.2f}\\") print(f\\"Standard deviation of accuracy: {std_accuracy:.2f}\\") # Report precision and recall for each fold precision_per_fold = scores[\'test_precision_macro\'] recall_per_fold = scores[\'test_recall_macro\'] print(f\\"Precision for each fold: {precision_per_fold}\\") print(f\\"Recall for each fold: {recall_per_fold}\\") # Implement a custom cross-validator that splits the dataset into two folds def custom_cv_2folds(X): n = X.shape[0] indices = np.arange(n) half = n // 2 yield indices[half:], indices[:half] yield indices[:half], indices[half:] # Use the custom cross-validator print(\\"Custom 2-fold CV indices:\\") for train_idx, test_idx in custom_cv_2folds(X): print(f\\"Train indices: {train_idx}\\") print(f\\"Test indices: {test_idx}\\")"},{"question":"# Reference Counting Simulation in Python Objective You are required to simulate a basic reference counting mechanism in Python. This exercise will evaluate your understanding of memory management and reference counting. Problem Statement Write a class `PyObject` that simulates the key aspects of reference counting as described in the provided documentation. The class should include methods to increment and decrement the reference count, handling cases when the reference count reaches zero by triggering a custom deallocation method. Requirements 1. Implement a class `PyObject` with the following: - An attribute `ref_count` to keep track of the number of references. - A method `inc_ref()` to increment the reference count (similar to `Py_INCREF`). - A method `dec_ref()` to decrement the reference count (similar to `Py_DECREF`). When the reference count reaches zero, a deallocation method should be called. - A method `dealloc()` that will be called when the reference count reaches zero, printing a message indicating that the object is being deallocated. 2. Implement another class `Container` that will hold instances of `PyObject` and demonstrate the reference counting mechanism: - Add methods in `Container` to add, remove, and clear references to `PyObject` instances. Input and Output Formats - Your implementation will be verified through a series of calls to the `Container` methods to ensure proper reference counting and garbage collection. - You are not required to handle any input/output directly. Instead, you will demonstrate the functionality via test cases in your main code. Example ```python class PyObject: def __init__(self): self.ref_count = 0 def inc_ref(self): self.ref_count += 1 def dec_ref(self): self.ref_count -= 1 if self.ref_count <= 0: self.dealloc() def dealloc(self): print(f\\"{self} is being deallocated\\") class Container: def __init__(self): self.objects = [] def add_ref(self, obj): obj.inc_ref() self.objects.append(obj) def remove_ref(self, obj): if obj in self.objects: self.objects.remove(obj) obj.dec_ref() def clear_refs(self): for obj in self.objects: obj.dec_ref() self.objects.clear() # Example usage: c = Container() o1 = PyObject() c.add_ref(o1) # ref_count of o1 should be 1 c.add_ref(o1) # ref_count of o1 should be 2 c.remove_ref(o1) # ref_count of o1 should be 1 c.remove_ref(o1) # ref_count of o1 should be 0 and it should be deallocated ``` Implement the `PyObject` and `Container` classes to meet the requirements outlined above.","solution":"class PyObject: def __init__(self): self.ref_count = 0 def inc_ref(self): self.ref_count += 1 def dec_ref(self): self.ref_count -= 1 if self.ref_count <= 0: self.dealloc() def dealloc(self): print(f\\"{self} is being deallocated\\") class Container: def __init__(self): self.objects = [] def add_ref(self, obj): obj.inc_ref() self.objects.append(obj) def remove_ref(self, obj): if obj in self.objects: self.objects.remove(obj) obj.dec_ref() def clear_refs(self): for obj in self.objects: obj.dec_ref() self.objects.clear()"},{"question":"# Question: Topological Sorting and Cycle Detection You are given a set of tasks with dependencies, where certain tasks must precede others. Your goal is to implement a function `get_topological_order(tasks: List[Tuple[str, List[str]]]) -> List[str]` using the `graphlib.TopologicalSorter` class to determine a valid task sequence. If there is a cycle in the graph, the function should raise a `graphlib.CycleError` with a user-friendly error message. Detailed Description: - **Function Signature**: ```python def get_topological_order(tasks: List[Tuple[str, List[str]]]) -> List[str]: ``` - **Input**: - `tasks`: A list of tuples, where each tuple represents a task and its dependencies. The first element of each tuple is a string representing the task, and the second element is a list of strings representing the dependencies of the task. For example: ```python [ (\\"task1\\", [\\"task2\\", \\"task3\\"]), (\\"task2\\", [\\"task4\\"]), (\\"task3\\", [\\"task4\\"]), (\\"task4\\", []) ] ``` - **Output**: - Returns a list of strings representing the topological order of tasks. - If there is a cycle, raise `graphlib.CycleError` with an error message `\\"Cycle detected in the dependencies\\"`. Example: ```python tasks = [ (\\"task1\\", [\\"task2\\", \\"task3\\"]), (\\"task2\\", [\\"task4\\"]), (\\"task3\\", [\\"task4\\"]), (\\"task4\\", []) ] print(get_topological_order(tasks)) # Output: [\'task4\', \'task2\', \'task3\', \'task1\'] ``` Constraints: - All task names and dependencies are unique and hashable strings. - The graph of tasks can have at most 10,000 nodes. - Handle cyclic dependencies gracefully by raising a custom `CycleError` exception. Performance Requirements: - The function should efficiently handle graphs with up to 10,000 nodes and corresponding edges. Notes: - Utilize the `graphlib.TopologicalSorter` class and its methods (`add`, `prepare`, `done`, `get_ready`, `static_order`). - The function should be clear and well-documented. - Ensure the function handles edge cases, such as tasks with no dependencies or cyclic dependencies. Good luck, and happy coding!","solution":"import graphlib from typing import List, Tuple def get_topological_order(tasks: List[Tuple[str, List[str]]]) -> List[str]: Returns the topological order of tasks given their dependencies using graphlib.TopologicalSorter. If a cycle is detected, raises a graphlib.CycleError with a custom error message. :param tasks: List of tuples where the first element is the task, and the second element is the list of its dependencies. :return: List of tasks in topological order. ts = graphlib.TopologicalSorter() # Adding tasks and dependencies to the TopologicalSorter for task, dependencies in tasks: ts.add(task, *dependencies) try: # Returning the static order of the tasks return list(ts.static_order()) except graphlib.CycleError: raise graphlib.CycleError(\\"Cycle detected in the dependencies\\")"},{"question":"Working with Unix Group Database Objective: Your task is to implement a function that performs the following operations on the Unix group database using the \\"grp\\" module. # Function to Implement: ```python def group_details(group_identifier): This function takes a group identifier, which can be either a group name (string) or a group ID (integer). It returns a dictionary containing group details: - \'gr_name\': name of the group - \'gr_passwd\': encrypted group password (possibly empty) - \'gr_gid\': numerical group ID - \'gr_mem\': list of group member\'s usernames If the group identifier does not exist, return an empty dictionary. If the input is invalid (neither a string nor an integer), raise a ValueError. Args: group_identifier (str/int): The group name or group ID. Returns: dict: A dictionary with group details or an empty dictionary if the group does not exist. Raises: ValueError: If the input is not a string or an integer. pass ``` # Constraints: 1. The `group_identifier` can either be a string (representing the group name) or an integer (representing the group ID). 2. If the `group_identifier` is not found in the group database, the function should return an empty dictionary. 3. If the `group_identifier` is neither a string nor an integer, the function should raise a `ValueError`. # Input/Output Formats: - **Input**: The function will take a single argument `group_identifier`, which will be a string (group name) or an integer (group ID). - **Output**: The function should return a dictionary containing the group\'s details (`gr_name`, `gr_passwd`, `gr_gid`, `gr_mem`), or an empty dictionary if the identifier is not found. # Example: ```python # Example 1: group name group_details(\\"staff\\") # Output: {\'gr_name\': \'staff\', \'gr_passwd\': \'x\', \'gr_gid\': 50, \'gr_mem\': [\'user1\', \'user2\']} # Example 2: group ID group_details(50) # Output: {\'gr_name\': \'staff\', \'gr_passwd\': \'x\', \'gr_gid\': 50, \'gr_mem\': [\'user1\', \'user2\']} # Example 3: invalid group identifier group_details(99999) # Output: {} # Example 4: invalid input type group_details(3.5) # Raises ValueError ``` # Notes: 1. Use the `grp` module functions (`getgrgid`, `getgrnam`) for fetching the group details. 2. Handle exceptions appropriately to meet the specified constraints.","solution":"import grp def group_details(group_identifier): This function takes a group identifier, which can be either a group name (string) or a group ID (integer). It returns a dictionary containing group details: - \'gr_name\': name of the group - \'gr_passwd\': encrypted group password (possibly empty) - \'gr_gid\': numerical group ID - \'gr_mem\': list of group member\'s usernames If the group identifier does not exist, return an empty dictionary. If the input is invalid (neither a string nor an integer), raise a ValueError. Args: group_identifier (str/int): The group name or group ID. Returns: dict: A dictionary with group details or an empty dictionary if the group does not exist. Raises: ValueError: If the input is not a string or an integer. try: if isinstance(group_identifier, int): group = grp.getgrgid(group_identifier) elif isinstance(group_identifier, str): group = grp.getgrnam(group_identifier) else: raise ValueError(\\"The identifier must be a string (group name) or integer (group ID).\\") return { \'gr_name\': group.gr_name, \'gr_passwd\': group.gr_passwd, \'gr_gid\': group.gr_gid, \'gr_mem\': group.gr_mem } except KeyError: return {}"},{"question":"Objective You are tasked with writing a Python function that automates the creation of multiple types of built distributions for a Python module, based on user-specified options. This function will leverage the Distutils package and should handle various command options and formats as specified in the provided documentation. Problem Statement Write a function `create_distributions` that generates built distributions for a given Python module. The function should take the path to the module\'s `setup.py` file and a list of formats as input and generate the corresponding built distributions. Function Signature ```python def create_distributions(setup_path: str, formats: list) -> dict: pass ``` Input - `setup_path` (str): The file path to the `setup.py` script of the module. - `formats` (list): A list of strings, where each string is one of the following valid formats: `\\"gztar\\"`, `\\"bztar\\"`, `\\"xztar\\"`, `\\"ztar\\"`, `\\"tar\\"`, `\\"zip\\"`, `\\"rpm\\"`, `\\"pkgtool\\"`, `\\"sdux\\"`, `\\"msi\\"`. Output - A dictionary with each format as the key and a Boolean value indicating success (`True`) or failure (`False`) of the distribution creation for that format. Constraints - You should handle invalid formats gracefully by skipping over them and indicating failure (`False`) for those formats. - Assume that the environment where this function is executed has all necessary tools and utilities installed (e.g., `rpm`, `compress`, etc.). - The function should execute only the required `Distutils` commands to produce the distributions. - For cross-compilation (if applicable), handle using appropriate flags or parameters based on the provided documentation. Example ```python formats = [\\"gztar\\", \\"zip\\", \\"rpm\\"] result = create_distributions(\\"path/to/setup.py\\", formats) print(result) # Expected output: {\'gztar\': True, \'zip\': True, \'rpm\': False} ``` Notes - You should make use of `subprocess` module to execute the required commands in Python. - Include error handling for each format and ensure the function provides meaningful output regarding success or failure.","solution":"import subprocess def create_distributions(setup_path: str, formats: list) -> dict: valid_formats = {\\"gztar\\", \\"bztar\\", \\"xztar\\", \\"ztar\\", \\"tar\\", \\"zip\\", \\"rpm\\", \\"pkgtool\\", \\"sdux\\", \\"msi\\"} result = {} for fmt in formats: if fmt not in valid_formats: result[fmt] = False continue try: # Construct the command to build the distribution command = [\\"python\\", setup_path, \\"sdist\\", \\"--formats\\", fmt] if fmt in {\\"gztar\\", \\"bztar\\", \\"xztar\\", \\"ztar\\", \\"tar\\", \\"zip\\"} else [\\"python\\", setup_path, \\"bdist\\", f\\"--formats={fmt}\\"] subprocess.run(command, check=True) result[fmt] = True except subprocess.CalledProcessError: result[fmt] = False return result"},{"question":"You are given a `.netrc` file that stores login data for various hosts. Your task is to implement a function `get_netrc_auth(file: str, host: str) -> tuple` to parse this file and retrieve the authenticators (i.e. login, account, and password) for a given host. Function Signature ```python def get_netrc_auth(file: str, host: str) -> tuple: ``` # Arguments: - `file` (str): The path to the `.netrc` file. - `host` (str): The hostname for which to retrieve the authentication details. # Returns: - A tuple `(login, account, password)` containing the authenticators for the given host. If the host is not found in the `.netrc` file, return the authenticators for the \'default\' host if available. If neither the given host nor the \'default\' host is available, return `None`. # Constraints: - The `.netrc` file may contain syntactical errors, and your function should gracefully handle these by returning `None` and logging an appropriate error message (you can use `print()` for simplicity). # Example: Given a `.netrc` file with the following content: ``` machine host1 login user1 account account1 password pass1 machine host2 login user2 account account2 password pass2 default login default_user account default_account password default_pass ``` Calling `get_netrc_auth(\\"path/to/.netrc\\", \\"host1\\")` should return `(\'user1\', \'account1\', \'pass1\')`. Calling `get_netrc_auth(\\"path/to/.netrc\\", \\"host3\\")` should return `(\'default_user\', \'default_account\', \'default_pass\')`. If the file contains syntax errors, the function should return `None`. # Note: - Ensure your solution handles file I/O operations safely. - Test your implementation with different scenarios, including non-existent hosts, and files with or without syntax errors.","solution":"def get_netrc_auth(file: str, host: str) -> tuple: Parses the .netrc file and retrieves the authenticators for a given host. Args: file (str): The path to the .netrc file. host (str): The hostname for which to retrieve the authentication details. Returns: tuple: A tuple (login, account, password) with the authenticators for the given host. If the host is not found, returns the details for \'default\' host if available. Returns None if neither the host nor \'default\' is available or if there are syntax errors. try: with open(file, \'r\') as f: lines = f.readlines() current_host = None auths = {\'default\': None} for line in lines: tokens = line.strip().split() if not tokens: continue if tokens[0] == \'machine\' and len(tokens) == 2: current_host = tokens[1] auths[current_host] = {\'login\': None, \'account\': None, \'password\': None} elif tokens[0] == \'default\': current_host = \'default\' auths[\'default\'] = {\'login\': None, \'account\': None, \'password\': None} elif tokens[0] in [\'login\', \'account\', \'password\'] and current_host: if tokens[0] == \'login\' and len(tokens) == 2: auths[current_host][\'login\'] = tokens[1] elif tokens[0] == \'account\' and len(tokens) == 2: auths[current_host][\'account\'] = tokens[1] elif tokens[0] == \'password\' and len(tokens) == 2: auths[current_host][\'password\'] = tokens[1] else: print(\\"Syntax error in .netrc file\\") return None else: print(\\"Syntax error in .netrc file\\") return None if host in auths: return (auths[host][\'login\'], auths[host][\'account\'], auths[host][\'password\']) elif \'default\' in auths and auths[\'default\']: return (auths[\'default\'][\'login\'], auths[\'default\'][\'account\'], auths[\'default\'][\'password\']) else: return None except Exception as e: print(f\\"Error: {e}\\") return None"},{"question":"# Covariance Estimation with Empirical and Shrunk Covariances in Scikit-Learn You are tasked with implementing covariance estimation on a given dataset using both the empirical covariance method and the shrunk covariance method. Your solution should demonstrate the following skills: 1. Loading a dataset. 2. Estimating the covariance matrix using `EmpiricalCovariance`. 3. Applying shrinkage to the covariance matrix using `ShrunkCovariance`. 4. Comparing the results of both the methods. Input: - A 2D numpy array `X` representing the dataset, where rows correspond to samples and columns correspond to features. Expected Output: - A dictionary containing the following keys and values: - `\'empirical_covariance\'`: The covariance matrix estimated using empirical method. - `\'shrunk_covariance\'`: The covariance matrix estimated using shrunk method with a shrinkage coefficient of 0.1. Constraints: - Assume that the dataset `X` is centered, i.e., it has zero mean. Performance Requirements: - The solution should efficiently handle datasets with up to 5000 samples and 50 features. Here is the signature of the function you need to implement: ```python def estimate_covariances(X: np.ndarray) -> dict: Estimates the covariance matrix of the given input data using empirical and shrunk covariance methods. Parameters: X (np.ndarray): A 2D numpy array representing the dataset. Returns: dict: A dictionary with keys \'empirical_covariance\' and \'shrunk_covariance\' containing the respective covariance matrices. pass ``` Example: ```python import numpy as np X = np.array([[2.3, 2.1, 3.2], [4.2, 4.5, 6.7], [1.2, 0.3, 1.2], [3.9, 4.0, 5.1]]) result = estimate_covariances(X) print(result[\'empirical_covariance\']) # Expected output (approximate values): # [[2.04 2.12 2.93] # [2.12 2.51 3.33] # [2.93 3.33 4.51]] print(result[\'shrunk_covariance\']) # Expected output (approximate values): # [[1.93 1.91 2.61] # [1.91 2.40 3.00] # [2.61 3.00 4.37]] ``` Note: The expected output values here are illustrative; your exact values might differ slightly depending on the algorithm\'s numerical precision.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance def estimate_covariances(X: np.ndarray) -> dict: Estimates the covariance matrix of the given input data using empirical and shrunk covariance methods. Parameters: X (np.ndarray): A 2D numpy array representing the dataset. Returns: dict: A dictionary with keys \'empirical_covariance\' and \'shrunk_covariance\' containing the respective covariance matrices. # Estimate the empirical covariance matrix emp_cov = EmpiricalCovariance().fit(X).covariance_ # Estimate the shrunk covariance matrix with shrinkage 0.1 shr_cov = ShrunkCovariance(shrinkage=0.1).fit(X).covariance_ return { \'empirical_covariance\': emp_cov, \'shrunk_covariance\': shr_cov }"},{"question":"**Question: Exploring Penguins Dataset with Seaborn\'s Object Interface** You are given access to the famous \'penguins\' dataset included in Seaborn. Your task is to create a complex plot that visualizes different characteristics of the penguins based on their species and sex. You will be using the `seaborn.objects` interface to accomplish this. # Requirements 1. Load the \'penguins\' dataset using Seaborn. 2. Initialize a `Plot` object to visualize the relationship between \'species\', \'body_mass_g\', and \'sex\'. - Use `color=\\"sex\\"` to differentiate sexes. 3. Add `Dash` marks to the plot to represent each datapoint\'s body mass. - Make the lines semi-transparent by setting `alpha=0.5`. - Control the width of the lines with a property mapped to `flipper_length_mm`. 4. Add `Dodge` to separate the lines by \'sex\'. 5. Additionally, combine the `Dash` marks with aggregate functions to show mean values and include a jittered scatter plot to show individual data points for better visualization. # Expected Outcome Your plot should have: - Lines (`Dash` marks) that vary in width according to `flipper_length_mm` for each penguin, separated by sex. - Semi-transparent (`alpha=0.5`) lines. - Lines dodged by sex. - A combination of mean values shown using aggregate functions along with individual data points jittered for clarity. # Code Template ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Initialize the plot p = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") # Add dash marks with alpha and linewidth, then dodge by sex p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\").add(so.Dodge()) # Combine with aggregate and jittered dots ( p .add(so.Dash(), so.Agg(), so.Dodge()) .add(so.Dots(), so.Dodge(), so.Jitter()) ) ``` # Input Format - No input from user. The dataset is loaded from Seaborn. # Output Format - A Seaborn plot object visualizing the required data points and relationships. # Constraints - Ensure that the plot is clear and interpretable. - Consider performance and avoid unnecessary complexity in computation. **Note**: You may refer to the Seaborn documentation for detailed parameter usage and additional customization.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Initialize the plot p = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") # Add dash marks with alpha and linewidth, then dodge by sex p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\", dodge=\\"sex\\") # Combine with aggregate and jittered dots ( p .add(so.Dash(), so.Agg(), so.Dodge()) .add(so.Dots(), so.Dodge(), so.Jitter()) ) # Render the plot p.show()"},{"question":"Objective Given a list of events with their respective UTC timestamps, your task is to convert these timestamps into the local time of a specified timezone and sort the events based on their localized times. Problem Statement You are provided with a list where each element is a tuple containing an event name and its corresponding UTC timestamp in ISO 8601 format (`YYYY-MM-DDTHH:MM:SS`), and a string representing a timezone in the format `UTCHH:MM`. Implement a function `convert_and_sort_events(events, timezone)` that converts the UTC timestamps to the specified local timezone and returns a list of events sorted by these local times. # Input - `events`: A list of tuples, where each tuple contains: - `event_name` (str): The name of the event. - `timestamp` (str): The UTC timestamp of the event in ISO 8601 format. - `timezone`: A string representing the target timezone in the format `UTCHH:MM`. # Output - A list of tuples sorted by the local time in the specified timezone. Each tuple contains: - `event_name` (str): The name of the event. - `local_time` (str): The localized time in ISO 8601 format. # Constraints - The input list `events` contains at least one event. - The `timestamp` strings are valid ISO 8601 formats. - The `timezone` format is always valid and can range from `UTC-23:59` to `UTC+23:59`. # Example ```python events = [ (\\"Meeting\\", \\"2023-10-05T13:20:00\\"), (\\"Lunch\\", \\"2023-10-05T11:00:00\\"), (\\"Conference Call\\", \\"2023-10-05T09:45:00\\") ] timezone = \\"UTC+05:30\\" output = convert_and_sort_events(events, timezone) print(output) ``` Expected output: ```python [ (\\"Conference Call\\", \\"2023-10-05T15:15:00\\"), (\\"Lunch\\", \\"2023-10-05T16:30:00\\"), (\\"Meeting\\", \\"2023-10-05T18:50:00\\") ] ``` # Function Signature ```python def convert_and_sort_events(events: list, timezone: str) -> list: pass ``` # Implementation Notes 1. Use the `datetime` module to parse the timestamps and perform timezone conversions. 2. Create a `timezone` object using the provided timezone string. 3. Convert all event timestamps to the target timezone. 4. Sort the events based on the localized times. 5. Format the localized times back into ISO 8601 strings for the output.","solution":"from datetime import datetime, timedelta def convert_and_sort_events(events, timezone): def parse_timezone(tz_str): sign = 1 if tz_str[3] == \'+\' else -1 tzh, tzm = map(int, tz_str[4:].split(\':\')) return sign * timedelta(hours=tzh, minutes=tzm) tz_offset = parse_timezone(timezone) def convert_to_local(utc_time_str): utc_dt = datetime.fromisoformat(utc_time_str) local_dt = utc_dt + tz_offset return local_dt.isoformat() localized_events = [(event, convert_to_local(ts)) for event, ts in events] localized_events.sort(key=lambda x: x[1]) return localized_events"},{"question":"# Custom Python Type Creation: Implementing a `DataContainer` Type Objectives: To create, manipulate, and clean up custom Python objects using the CPython API, particularly the `PyTypeObject` structure, deallocation, representation, attribute management, and basic comparison operations. Problem Statement: You are tasked with creating a new type called `DataContainer` in Python using the `PyTypeObject` structure. This type will be defined in a C-extension module named `datacontainer`. Your goal is to implement a custom type that supports: 1. Initialization with an integer value. 2. Custom string representation and repr strings. 3. Manage custom attributes. 4. Support comparison operations (`==` and `!=`). 5. Support for deallocation with proper memory cleanup. Requirements: 1. Create a C structure `DataContainerObject` with an integer field named `value`. 2. Define a new type called `DataContainer` that supports: - `tp_init` for initializing an instance with an integer value. - `tp_dealloc` for proper memory cleanup. - `tp_repr` for the `repr()` function. - `tp_str` for the `str()` function. - `tp_getattro` and `tp_setattro` to get and set the value attribute. - `tp_richcompare` for supporting `==` and `!=` operations. Input and Output Formats: - **Initialization**: `DataContainer(value)` - Input: An integer value. - Output: A `DataContainer` object. - **String Representation**: `str(your_object)` - Input: A `DataContainer` object. - Output: A string in the format `\\"DataContainer with value: <value>\\"` - **Repr Representation**: `repr(your_object)` - Input: A `DataContainer` object. - Output: A string in the format `\\"DataContainer(<value>)\\"` - **Getting Attribute**: `your_object.value` - Input: A `DataContainer` object. - Output: The integer value stored in the `value` attribute. - **Setting Attribute**: `your_object.value = new_value` - Input: A `DataContainer` object and a new integer value. - Output: The value attribute is updated with the new value. - **Comparison**: `your_object1 == your_object2` or `your_object1 != your_object2` - Input: Two `DataContainer` objects. - Output: A boolean value based on the comparison of their `value` attributes. Constraints: - The initial value must be an integer. - Proper memory management is required during the deallocation of objects. Performance Requirements: - The initialization, attribute management, and comparison operations should be efficient with constant time complexity. Example: ```python # Python code to interface with the C extension module `datacontainer` from datacontainer import DataContainer # Create a DataContainer object obj1 = DataContainer(5) # Test repr assert repr(obj1) == \\"DataContainer(5)\\" # Test str assert str(obj1) == \\"DataContainer with value: 5\\" # Test getting attribute assert obj1.value == 5 # Test setting attribute obj1.value = 10 assert obj1.value == 10 # Test comparison operations obj2 = DataContainer(10) assert (obj1 == obj2) is True obj3 = DataContainer(15) assert (obj1 != obj3) is True ``` Submission: Provide the C code for the custom type and a simple Python loader/test script that showcases the required functionalities.","solution":"# Assuming the question asked for a Python implementation as # the provided example doesn\'t match the requirements for C extension code. # Here is the Python representation of DataContainer. class DataContainer: def __init__(self, value): if not isinstance(value, int): raise TypeError(\\"value must be an integer\\") self.value = value def __repr__(self): return f\\"DataContainer({self.value})\\" def __str__(self): return f\\"DataContainer with value: {self.value}\\" def __eq__(self, other): if isinstance(other, DataContainer): return self.value == other.value return False def __ne__(self, other): return not self.__eq__(other)"},{"question":"# Advanced Coding Assessment Question Problem Statement You have been tasked to design a simulation of a print server that handles print jobs from multiple clients in a multi-user system. The server must process these jobs concurrently and manage them based on their priority. The higher the priority number, the sooner the job should be processed (lower numbers have higher priority). Each print job includes a description of the job and the time it takes to complete the print. Task 1. Implement an asynchronous `PrintServer` class using `asyncio.PriorityQueue` to manage print jobs. 2. The server should have methods to submit new jobs, process them, and indicate job completion. 3. Handle scenarios where a job is attempted to be processed when none are available, and ensure graceful shutdown when all jobs are completed. Requirements 1. Define the `PrintServer` class with the following methods: - `__init__(self, maxsize=0)`: Initializes the server\'s priority queue with an optional maximum size. - `submit_job(self, priority: int, description: str, duration: int)`: Submits a new print job to the server. - `start(self)`: Starts the server to begin processing submitted print jobs. - `shutdown(self)`: Gracefully stops the server, ensuring all jobs are completed before fully shutting down. 2. Print jobs should be processed in order of priority (lowest number first). If two jobs have the same priority, they should be processed based on the order of submission. 3. Use the following exceptions to handle edge cases: - `asyncio.QueueFull`: Raised when attempting to submit a job to a full queue. - `asyncio.QueueEmpty`: Properly handle cases when attempting to process jobs from an empty queue. 4. Demonstrate the class usage with at least three example print jobs, showcasing different priorities and durations. Example ```python import asyncio class PrintServer: def __init__(self, maxsize=0): self.queue = asyncio.PriorityQueue(maxsize=maxsize) async def submit_job(self, priority: int, description: str, duration: int): print(f\\"Submitting job: {description} with priority {priority} and duration {duration} seconds\\") await self.queue.put((priority, (description, duration))) async def process_jobs(self): while True: try: priority, (description, duration) = await self.queue.get() print(f\\"Processing job: {description}\\") await asyncio.sleep(duration) print(f\\"Completed job: {description}\\") self.queue.task_done() except asyncio.QueueEmpty: print(\\"Queue is empty, no jobs to process\\") async def start(self): print(\\"Starting print server\\") await self.process_jobs() async def shutdown(self): print(\\"Shutting down print server\\") await self.queue.join() # Example Usage async def main(): server = PrintServer(maxsize=10) await server.submit_job(1, \\"Print Document A\\", 3) await server.submit_job(3, \\"Print Document B\\", 1) await server.submit_job(2, \\"Print Document C\\", 2) worker_task = asyncio.create_task(server.start()) await asyncio.sleep(10) # Main routine sleeps while print jobs are processed await server.shutdown() worker_task.cancel() asyncio.run(main()) ``` # Constraints and Limitations - The print server should behave correctly irrespective of job submission order. - There might be a large number of submissions; ensure the server handles them efficiently. - Proper synchronization must be maintained to avoid race conditions. # Performance Requirements - The implementation must be efficient in terms of time and space, handling up to 1000 simultaneous job submissions.","solution":"import asyncio class PrintServer: def __init__(self, maxsize=0): self.queue = asyncio.PriorityQueue(maxsize=maxsize) self.process_jobs_task = None async def submit_job(self, priority: int, description: str, duration: int): print(f\\"Submitting job: {description} with priority {priority} and duration {duration} seconds\\") await self.queue.put((priority, (description, duration))) async def process_jobs(self): while True: try: priority, (description, duration) = await self.queue.get() print(f\\"Processing job: {description}\\") await asyncio.sleep(duration) print(f\\"Completed job: {description}\\") self.queue.task_done() except asyncio.CancelledError: break async def start(self): print(\\"Starting print server\\") self.process_jobs_task = asyncio.create_task(self.process_jobs()) async def shutdown(self): print(\\"Shutting down print server\\") await self.queue.join() if self.process_jobs_task: self.process_jobs_task.cancel() try: await self.process_jobs_task except asyncio.CancelledError: pass print(\\"Print server shutdown complete\\") # Example Usage async def main(): server = PrintServer(maxsize=10) await server.submit_job(1, \\"Print Document A\\", 3) await server.submit_job(3, \\"Print Document B\\", 1) await server.submit_job(2, \\"Print Document C\\", 2) await server.start() await asyncio.sleep(10) # Main routine sleeps while print jobs are processed await server.shutdown() asyncio.run(main())"},{"question":"# Advanced Pandas Coding Assessment **Problem Statement:** You are given a dataset represented as a DataFrame which contains hierarchical/indexed data. Your task is to perform multiple operations on this DataFrame using `MultiIndex` features in pandas. **Dataset Example:** ```python import pandas as pd import numpy as np data = { \'state\': [\'California\', \'California\', \'California\', \'California\', \'Texas\', \'Texas\', \'Texas\', \'Texas\'], \'city\': [\'Los Angeles\', \'Los Angeles\', \'San Francisco\', \'San Francisco\', \'Houston\', \'Houston\', \'Dallas\', \'Dallas\'], \'year\': [2010, 2011, 2010, 2011, 2010, 2011, 2010, 2011], \'population\': [3792621, 3857799, 805235, 825863, 2099451, 2129784, 1197816, 1211709] } df = pd.DataFrame(data) ``` **Tasks:** 1. **Create a `MultiIndex`:** - Convert the `df` DataFrame to use a `MultiIndex` with `state`, `city`, and `year` as the multi-level indexes. 2. **Selection and Slicing:** - Select and print the population of \'Los Angeles\' for the year 2011. - Perform a partial selection to get the data for the state of \'Texas\'. 3. **Advanced Indexing:** - Swap the levels of `city` and `year` in the `MultiIndex` and print the resulting DataFrame. - Use the `xs` method to get a cross-section of the data for the year 2010 across all states and cities. - Reconstruct the level labels for both `state` and `city`. 4. **Reindexing and Alignment:** - Create a new DataFrame `df2` which contains only the mean population for each `state` across all years. - Reindex the original DataFrame `df` using the index from `df2`, aligning the data properly across the `state` level. **Input:** - A `DataFrame` formatted as shown in the example above. **Output:** - Print statements showing the results of each task performed on the DataFrame. **Solution Skeleton:** ```python import pandas as pd import numpy as np data = { \'state\': [\'California\', \'California\', \'California\', \'California\', \'Texas\', \'Texas\', \'Texas\', \'Texas\'], \'city\': [\'Los Angeles\', \'Los Angeles\', \'San Francisco\', \'San Francisco\', \'Houston\', \'Houston\', \'Dallas\', \'Dallas\'], \'year\': [2010, 2011, 2010, 2011, 2010, 2011, 2010, 2011], \'population\': [3792621, 3857799, 805235, 825863, 2099451, 2129784, 1197816, 1211709] } df = pd.DataFrame(data) # Task 1: Create a MultiIndex df.set_index([\'state\', \'city\', \'year\'], inplace=True) print(df) # Task 2: Selection and Slicing # (a) Population of \'Los Angeles\' in 2011 population_la_2011 = df.loc[(\'California\', \'Los Angeles\', 2011), \'population\'] print(\\"Population of Los Angeles in 2011:\\", population_la_2011) # (b) Data for the state of \'Texas\' data_texas = df.loc[\'Texas\'] print(\\"Data for Texas:\\", data_texas) # Task 3: Advanced Indexing # (a) Swap levels of \'city\' and \'year\' df_swapped = df.swaplevel(\'city\', \'year\') print(\\"DataFrame with swapped levels:\\") print(df_swapped) # (b) Cross-section of the year 2010 across all states and cities cross_section_2010 = df.xs(2010, level=\'year\') print(\\"Cross-section for the year 2010:\\") print(cross_section_2010) # (c) Reconstruct level labels state_labels = df.index.get_level_values(\'state\') city_labels = df.index.get_level_values(\'city\') print(\\"State labels:\\", state_labels) print(\\"City labels:\\", city_labels) # Task 4: Reindexing and Alignment # (a) Mean population for each state across all years df2 = df.groupby(level=\'state\').mean() print(\\"Mean population for each state across all years:\\") print(df2) # (b) Reindexing the original DataFrame using the index from df2 df_reindexed = df.reindex(df2.index, level=0) print(\\"Reindexed DataFrame:\\") print(df_reindexed) ``` Please implement the mentioned tasks within the provided solution skeleton.","solution":"import pandas as pd import numpy as np data = { \'state\': [\'California\', \'California\', \'California\', \'California\', \'Texas\', \'Texas\', \'Texas\', \'Texas\'], \'city\': [\'Los Angeles\', \'Los Angeles\', \'San Francisco\', \'San Francisco\', \'Houston\', \'Houston\', \'Dallas\', \'Dallas\'], \'year\': [2010, 2011, 2010, 2011, 2010, 2011, 2010, 2011], \'population\': [3792621, 3857799, 805235, 825863, 2099451, 2129784, 1197816, 1211709] } df = pd.DataFrame(data) # Task 1: Create a MultiIndex df.set_index([\'state\', \'city\', \'year\'], inplace=True) print(df) # Task 2: Selection and Slicing # (a) Population of \'Los Angeles\' in 2011 population_la_2011 = df.loc[(\'California\', \'Los Angeles\', 2011), \'population\'] print(\\"Population of Los Angeles in 2011:\\", population_la_2011) # (b) Data for the state of \'Texas\' data_texas = df.loc[\'Texas\'] print(\\"Data for Texas:\\", data_texas) # Task 3: Advanced Indexing # (a) Swap levels of \'city\' and \'year\' df_swapped = df.swaplevel(\'city\', \'year\') print(\\"DataFrame with swapped levels:\\") print(df_swapped) # (b) Cross-section of the year 2010 across all states and cities cross_section_2010 = df.xs(2010, level=\'year\') print(\\"Cross-section for the year 2010:\\") print(cross_section_2010) # (c) Reconstruct level labels state_labels = df.index.get_level_values(\'state\') city_labels = df.index.get_level_values(\'city\') print(\\"State labels:\\", state_labels) print(\\"City labels:\\", city_labels) # Task 4: Reindexing and Alignment # (a) Mean population for each state across all years df2 = df.groupby(level=\'state\').mean() print(\\"Mean population for each state across all years:\\") print(df2) # (b) Reindexing the original DataFrame using the index from df2 df_reindexed = df.reindex(df2.index, level=0) print(\\"Reindexed DataFrame:\\") print(df_reindexed) # For function wrapping, adding necessary functions def create_multiindex(data): df = pd.DataFrame(data) df.set_index([\'state\', \'city\', \'year\'], inplace=True) return df def select_population(df, state, city, year): return df.loc[(state, city, year), \'population\'] def select_state_data(df, state): return df.loc[state] def swap_levels(df, level1, level2): return df.swaplevel(level1, level2) def cross_section(df, level, value): return df.xs(value, level=level) def get_level_labels(df, level): return df.index.get_level_values(level) def mean_population_by_state(df): return df.groupby(level=\'state\').mean() def reindex_data(df, df2): return df.reindex(df2.index, level=0)"},{"question":"# POP3 Email Client You are required to implement a function that interacts with a POP3 email server to retrieve and process emails. The function should connect to the server, authenticate the user, and print out the subject lines of all emails in the user\'s mailbox. Assume that the subject line of an email is always in a line that starts with \\"Subject: \\". Function Signature ```python def print_email_subjects(server: str, port: int, username: str, password: str) -> None: ``` Parameters - `server` (str): The hostname or IP address of the POP3 server. - `port` (int): The port number of the POP3 server. - `username` (str): The username for email authentication. - `password` (str): The password for email authentication. Expected Output - The function should print the subject lines of all emails in the mailbox. Constraints - You can use any methods provided by the `poplib` module to interact with the server. - Handle any exceptions that may arise during the connection or authentication process. - Efficiently manage resources and ensure the connection is properly closed after the operation. - You may assume that there are no more than 1000 emails in the mailbox. - Port number is guaranteed to be valid. Example ```python print_email_subjects(\\"mail.example.com\\", 110, \\"myusername\\", \\"mypassword\\") ``` This function should print: ``` Subject: Welcome to Your New Email Account Subject: Update your Account Information ... ``` Guidelines 1. Use the `poplib.POP3` class to establish a connection. 2. Authenticate using the `user` and `pass_` methods. 3. Retrieve emails and extract subject lines using the `retr` method. 4. Ensure to call the `quit` method to close the connection.","solution":"import poplib def print_email_subjects(server: str, port: int, username: str, password: str) -> None: try: # Connect to the POP3 server mail = poplib.POP3(server, port) # Perform authentication mail.user(username) mail.pass_(password) # Get the number of messages num_messages = len(mail.list()[1]) # Iterate through all emails for i in range(1, num_messages + 1): # Retrieve the email response, lines, octets = mail.retr(i) # Parse the subject line for line in lines: if line.startswith(b\\"Subject: \\"): subject = line.decode(\'utf-8\') print(subject) break except Exception as e: print(f\\"An error occurred: {e}\\") finally: # Quit and close the connection mail.quit()"},{"question":"**Problem Statement: Implementing a Multi-I/O Monitoring System** In this exercise, you will implement a multi-I/O monitoring system using the `select` module in Python. Your task is to develop a function `monitor_io` that uses the `select.select()` function to monitor multiple input and output streams for readiness. # Function Signature ```python def monitor_io(rlist, wlist, xlist, timeout=None): Monitors multiple I/O streams until they are ready for some action. Parameters: rlist (list): List of file descriptors to be monitored for reading. wlist (list): List of file descriptors to be monitored for writing. xlist (list): List of file descriptors to be monitored for exceptional conditions. timeout (float or None): Maximum time in seconds to wait for an event. Blocking indefinitely if None. Returns: tuple: Three lists of file descriptors that are ready for reading, writing, and exceptional conditions. pass ``` # Inputs - `rlist`: A list of file descriptors or objects with a `fileno()` method representing the file descriptors to be monitored for reading. - `wlist`: A list of file descriptors or objects with a `fileno()` method representing the file descriptors to be monitored for writing. - `xlist`: A list of file descriptors or objects with a `fileno()` method representing the file descriptors to be monitored for exceptional conditions. - `timeout`: (optional) A floating-point number specifying the maximum time to wait in seconds. If not provided, the function will block indefinitely until at least one file descriptor is ready. # Output - A tuple of three lists (rlist, wlist, and xlist), each containing the file descriptors that are ready for reading, writing, or have an exceptional condition, respectively. # Constraints 1. The function should handle file descriptors efficiently using the `select.select()` function. 2. The function should correctly handle edge cases where any of the input lists (`rlist`, `wlist`, `xlist`) is empty. 3. The function should respect the timeout parameter and return within the specified time. 4. Your function should work without modification on both Unix-based systems and Windows. # Example Usage ```python import socket # Create a pair of connected sockets s1, s2 = socket.socketpair() # Set the second socket to non-blocking mode s2.setblocking(False) # Placeholder for test input rlist = [s1] wlist = [s1] xlist = [s1] # Function call readable, writable, exceptional = monitor_io(rlist, wlist, xlist, timeout=1.0) print(\\"Readable:\\", readable) print(\\"Writable:\\", writable) print(\\"Exceptional:\\", exceptional) ``` **Hint:** The `select.select()` function in Python simplifies the use of the select system call but be mindful of handling file descriptors and potential exceptions gracefully.","solution":"import select def monitor_io(rlist, wlist, xlist, timeout=None): Monitors multiple I/O streams until they are ready for some action. Parameters: rlist (list): List of file descriptors to be monitored for reading. wlist (list): List of file descriptors to be monitored for writing. xlist (list): List of file descriptors to be monitored for exceptional conditions. timeout (float or None): Maximum time in seconds to wait for an event. Blocking indefinitely if None. Returns: tuple: Three lists of file descriptors that are ready for reading, writing, and exceptional conditions. readable, writable, exceptional = select.select(rlist, wlist, xlist, timeout) return readable, writable, exceptional"},{"question":"# Advanced Seaborn Plotting Challenge You are given a dataset `mpg` that contains information about car specifications including various metrics such as `displacement`, `weight`, `horsepower`, etc. Your task is to create a series of plots using the `seaborn.objects` module to visualize relationships between these metrics. **Problem Statement:** 1. Load the `mpg` dataset using `seaborn`\'s `load_dataset` function. 2. Create a plot that pairs the `mpg` (miles per gallon) against `displacement` and `weight`. 3. Create another plot to show pairwise relationships between `displacement` and `horsepower` for the `mpg` values greater than 20. Ensure no cross-pairing; each `x` variable maps to one `y` variable. 4. Use faceting to show the relationship between `weight` and `horsepower`, faceted by the `origin` of the cars. 5. Customize the labels of the plots to include the units for each variable where applicable (e.g., \\"Weight (lb)\\", \\"Horsepower (hp)\\", \\"Displacement (cu in)\\", \\"MPG\\"). **Function Signature:** ```python def visualize_mpg_relationships(): import seaborn.objects as so from seaborn import load_dataset # Step 1: Load dataset mpg = load_dataset(\\"mpg\\") # Step 2: Pair mpg against displacement and weight plt1 = ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .add(so.Dots()) .label(x0=\\"Displacement (cu in)\\", x1=\\"Weight (lb)\\", y=\\"MPG\\") ) # Step 3: Pair displacement and horsepower for mpg values > 20 mpg_g20 = mpg[mpg[\\"mpg\\"] > 20] plt2 = ( so.Plot(mpg_g20) .pair(x=[\\"displacement\\"], y=[\\"horsepower\\"], cross=False) .add(so.Dots()) .label(x0=\\"Displacement (cu in)\\", y0=\\"Horsepower (hp)\\") ) # Step 4: Faceted plot for weight and horsepower plt3 = ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\"]) .facet(col=\\"origin\\") .add(so.Dots()) .label(x=\\"Weight (lb)\\", y=\\"Horsepower (hp)\\") ) # Show plots plt1.show() plt2.show() plt3.show() # Call the function to visualize plots visualize_mpg_relationships() ``` **Constraints:** - The function does not take any input parameters and does not return any output. - Ensure clear labeling of plots for better visualization. **Requirements:** - Use the `seaborn.objects` module for plotting. - Demonstrate comprehension of pairing, faceting, customizing labels, and filtering data.","solution":"def visualize_mpg_relationships(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Step 1: Load dataset mpg = load_dataset(\\"mpg\\") # Step 2: Pair mpg against displacement and weight plt1 = ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\"]) .add(so.Dots()) .label(x0=\\"Displacement (cu in)\\", x1=\\"Weight (lb)\\", y=\\"MPG\\") ) # Step 3: Pair displacement and horsepower for mpg values > 20 mpg_g20 = mpg[mpg[\\"mpg\\"] > 20] plt2 = ( so.Plot(mpg_g20) .pair(x=[\\"displacement\\"], y=[\\"horsepower\\"], cross=False) .add(so.Dots()) .label(x0=\\"Displacement (cu in)\\", y0=\\"Horsepower (hp)\\") ) # Step 4: Faceted plot for weight and horsepower plt3 = ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\"]) .facet(col=\\"origin\\") .add(so.Dots()) .label(x=\\"Weight (lb)\\", y=\\"Horsepower (hp)\\") ) # Show plots plt1.show() plt2.show() plt3.show() # Call the function to visualize plots visualize_mpg_relationships()"},{"question":"Objective: Implement a function that performs matrix multiplication on a batch of matrices in PyTorch while ensuring numerical stability and demonstrating awareness of precision issues described in the provided documentation. Task: Write a function `stable_batched_matrix_multiplication` that accepts a batch of matrices and performs batched matrix multiplication. Your function should: 1. Ensure numerical stability, particularly when handling extremal values or non-finite values (`inf` or `NaN`). 2. Allow toggling the use of TensorFloat-32 (TF32) precision on Nvidia GPUs. 3. Provide the option to conduct the computation in either single (FP32) or double precision (FP64). Function Signature: ```python import torch def stable_batched_matrix_multiplication( batch_A: torch.Tensor, batch_B: torch.Tensor, use_tf32: bool = False, precision: str = \'fp32\' ) -> torch.Tensor: Perform numerically stable batched matrix multiplication. Parameters: batch_A (torch.Tensor): A 3D tensor of shape (batch_size, M, N). batch_B (torch.Tensor): A 3D tensor of shape (batch_size, N, P). use_tf32 (bool, optional): Whether to use TensorFloat-32 precision. Default is False. precision (str, optional): Precision to use for computation, either \'fp32\' or \'fp64\'. Default is \'fp32\'. Returns: torch.Tensor: The result of batched matrix multiplication, a 3D tensor of shape (batch_size, M, P). pass ``` Constraints: - The inputs `batch_A` and `batch_B` will always be 3D tensors with appropriate dimensions for matrix multiplication. - The function should handle non-finite values (NaN, `inf`) gracefully, avoiding crashes or undefined behavior. - Ensure precision settings (`fp32` or `fp64`) are respected throughout the computation. - When `use_tf32` is `True`, TensorFloat-32 precision should be used for matrix multiplications on Nvidia GPUs. Example Input: ```python batch_A = torch.randn(10, 20, 30).float() # batch of 10 matrices of shape 20x30 batch_B = torch.randn(10, 30, 40).float() # batch of 10 matrices of shape 30x40 result = stable_batched_matrix_multiplication(batch_A, batch_B, use_tf32=True, precision=\'fp64\') ``` Expected Output: The function should return a 3D tensor of shape (10, 20, 40) representing the batched matrix multiplication result. The computation should respect the specified precision and handle extremal or non-finite values appropriately. Notes: - Make sure to use appropriate PyTorch methods and settings to toggle TF32 or switch between single and double precision. - Your implementation should consider the performance implications of different precision settings. - Test your function with various input configurations to ensure numerical stability and correctness.","solution":"import torch def stable_batched_matrix_multiplication( batch_A: torch.Tensor, batch_B: torch.Tensor, use_tf32: bool = False, precision: str = \'fp32\' ) -> torch.Tensor: Perform numerically stable batched matrix multiplication. Parameters: batch_A (torch.Tensor): A 3D tensor of shape (batch_size, M, N). batch_B (torch.Tensor): A 3D tensor of shape (batch_size, N, P). use_tf32 (bool, optional): Whether to use TensorFloat-32 precision. Default is False. precision (str, optional): Precision to use for computation, either \'fp32\' or \'fp64\'. Default is \'fp32\'. Returns: torch.Tensor: The result of batched matrix multiplication, a 3D tensor of shape (batch_size, M, P). # Enable or disable TensorFloat-32 if running on a compatible GPU torch.backends.cuda.matmul.allow_tf32 = use_tf32 # Cast input tensors to appropriate precision if precision == \'fp32\': A = batch_A.float() B = batch_B.float() elif precision == \'fp64\': A = batch_A.double() B = batch_B.double() else: raise ValueError(\\"Precision must be either \'fp32\' or \'fp64\'\\") # Handle non-finite values by replacing them with zeros # Ensures numerical stability A = torch.where(torch.isfinite(A), A, torch.zeros_like(A)) B = torch.where(torch.isfinite(B), B, torch.zeros_like(B)) # Perform batched matrix multiplication result = torch.bmm(A, B) return result"},{"question":"# Custom Stream Implementation with Encapsulation and Encoding Problem Statement You are required to implement a custom text stream class named `CustomTextStream` that inherits from `io.TextIOBase`. This class should provide additional functionality for encoding and decoding text data using a specified encoding, and it should encapsulate another text stream (like `io.StringIO`). Your `CustomTextStream` class should: 1. **Initialize with**: - A string `initial_text`: The initial content for the stream. - An optional string `encoding`: Specifies the encoding to use. Defaults to `\'utf-8\'`. 2. **Provide the following methods**: - `read(size=-1)`: Reads and decodes the specified number of characters from the stream. If `size` is negative or omitted, it reads until the end of the stream. - `write(s)`: Encodes and writes the given string to the stream. Returns the number of characters written. - `seek(offset, whence=SEEK_SET)`: Changes the stream position. Implements `SEEK_SET`, `SEEK_CUR`, and `SEEK_END`. - `tell()`: Returns the current stream position. - `close()`: Closes the stream. 3. **Ensure your class handles**: - Proper encoding and decoding based on the specified `encoding`. - Encoding errors using the specified error handling strategies. 4. **Constraints**: - The stream should raise appropriate exceptions (e.g., `UnsupportedOperation` for unsupported operations). Example Usage ```python import io # Creating a new CustomTextStream stream = CustomTextStream(\\"Hello, World!\\", encoding=\'utf-8\') # Reading from the stream print(stream.read()) # Output: Hello, World! # Writing to the stream stream.write(\\" Welcome to the Python I/O module.\\") stream.seek(0) print(stream.read()) # Output: Hello, World! Welcome to the Python I/O module. # Seek and tell operations stream.seek(7) print(stream.tell()) # Output: 7 print(stream.read(5)) # Output: World stream.close() ``` Function Signatures ```python class CustomTextStream(io.TextIOBase): def __init__(self, initial_text: str, encoding: str = \'utf-8\'): pass def read(self, size: int = -1) -> str: pass def write(self, s: str) -> int: pass def seek(self, offset: int, whence: int = io.SEEK_SET) -> int: pass def tell(self) -> int: pass def close(self): pass ``` Implement the class `CustomTextStream` as specified above. Ensure that your implementation correctly handles encoding and decoding using the specified `encoding`.","solution":"import io class CustomTextStream(io.TextIOBase): def __init__(self, initial_text: str, encoding: str = \'utf-8\'): self._encoding = encoding self._buffer = io.BytesIO(initial_text.encode(encoding)) self._closed = False def _check_closed(self): if self._closed: raise ValueError(\\"I/O operation on closed file.\\") def read(self, size: int = -1) -> str: self._check_closed() bytes_data = self._buffer.read(size) return bytes_data.decode(self._encoding) def write(self, s: str) -> int: self._check_closed() bytes_data = s.encode(self._encoding) self._buffer.write(bytes_data) return len(s) def seek(self, offset: int, whence: int = io.SEEK_SET) -> int: self._check_closed() return self._buffer.seek(offset, whence) def tell(self) -> int: self._check_closed() return self._buffer.tell() def close(self): self._closed = True self._buffer.close()"},{"question":"Objective: Assess students\' understanding of linear regression, model selection, and regularization techniques as implemented in scikit-learn. Problem Statement: You are given a dataset containing information about houses in a city, including features like `size`, `number of rooms`, `age`, and `distance to the city center`. Your task is to predict the house prices using linear regression models from scikit-learn. You need to implement a function that compares different linear models based on their performance. Function Signature: ```python def compare_linear_models(X_train, X_test, y_train, y_test): Compares the performance of different linear regression models. Parameters: X_train (numpy.ndarray): Training data features. X_test (numpy.ndarray): Testing data features. y_train (numpy.ndarray): Training data target values. y_test (numpy.ndarray): Testing data target values. Returns: results (dict): A dictionary where the keys are model names and values are the R^2 scores. ``` Requirements: 1. **Models to be compared**: - Ordinary Least Squares (`LinearRegression`) - Ridge Regression (`Ridge`) - Lasso Regression (`Lasso`) - Elastic Net (`ElasticNet`) 2. **Dataset Split**: - Use the provided training and testing data in the function arguments. 3. **Model Fitting and Evaluation**: - Fit each model on the training data and evaluate it using the R^2 score on the test data. - Use reasonable default values for hyperparameters of Ridge, Lasso, and Elastic Net. 4. **Output**: - Return a dictionary with model names as keys and their respective R^2 scores on the test data as values. Example Usage: ```python X_train = [[1500, 3, 10, 5], [1600, 3, 12, 6], [1700, 3, 8, 4]] # example feature data X_test = [[1550, 3, 10, 5], [1650, 3, 13, 6]] # example feature data y_train = [250000, 260000, 270000] # example target data y_test = [255000, 265000] # example target data results = compare_linear_models(X_train, X_test, y_train, y_test) print(results) # Example output: {\\"LinearRegression\\": 0.95, \\"Ridge\\": 0.94, \\"Lasso\\": 0.93, \\"ElasticNet\\": 0.92} ``` Constraints: - Assume that the input data is preprocessed and scaled appropriately. - Do not perform any hyperparameter tuning or cross-validation, use default settings for simplicity. Good Luck!","solution":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.metrics import r2_score def compare_linear_models(X_train, X_test, y_train, y_test): Compares the performance of different linear regression models. Parameters: X_train (numpy.ndarray): Training data features. X_test (numpy.ndarray): Testing data features. y_train (numpy.ndarray): Training data target values. y_test (numpy.ndarray): Testing data target values. Returns: results (dict): A dictionary where the keys are model names and values are the R^2 scores. models = { \\"LinearRegression\\": LinearRegression(), \\"Ridge\\": Ridge(), \\"Lasso\\": Lasso(), \\"ElasticNet\\": ElasticNet() } results = {} for model_name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) score = r2_score(y_test, y_pred) results[model_name] = score return results"},{"question":"Manipulating Slice Objects **Objective**: Implement functions to work with Python\'s slice objects effectively by utilizing the `PySlice_*` APIs. Task 1: `extract_slice_indices` Write a function `extract_slice_indices` that receives a slice object and the length of a sequence and returns a tuple of the correct start, stop, and step indices. You must use `PySlice_Unpack` to extract these indices and handle out-of-bounds errors appropriately. **Function Signature**: ```python def extract_slice_indices(slice_obj, sequence_length): Extracts start, stop, and step indices from a given slice object considering the sequence length. Parameters: slice_obj (slice): The slice object to extract indices from. sequence_length (int): The length of the sequence that the slice applies to. Returns: tuple: A tuple containing (start, stop, step) indices. Raises: ValueError: If the slice indices are out of bounds. pass ``` **Constraints**: - `sequence_length` is a positive integer. - The function should raise a `ValueError` if any index is out of bounds. Task 2: `create_slice` Write a function `create_slice` that generates a slice object given start, stop, and step values. **Function Signature**: ```python def create_slice(start, stop, step): Creates a slice object given start, stop, and step values. Parameters: start (int): Start index of the slice. stop (int): Stop index of the slice. step (int): Step index of the slice. Returns: slice: The slice object created with the given start, stop, and step. Raises: ValueError: If the slice cannot be created. pass ``` **Constraints**: - The parameters `start`, `stop`, and `step` can be any integer or `None`. Task 3: `adjust_slice_indices` Write a function `adjust_slice_indices` that adjusts the start and stop indices of a slice based on the length of a sequence. **Function Signature**: ```python def adjust_slice_indices(sequence_length, start, stop, step): Adjusts the start and stop indices of a slice based on the length of a sequence. Parameters: sequence_length (int): The length of the sequence. start (int): Start index of the slice. stop (int): Stop index of the slice. step (int): Step index of the slice. Returns: tuple: A tuple containing the adjusted (start, stop, step) indices. pass ``` **Constraints**: - `sequence_length` is a positive integer. - The function should adjust indices in a manner consistent with normal python slice behavior. **Example**: ```python # Assuming the functions are implemented correctly, the following should hold true s = slice(2, 10, 2) sequence_len = 8 print(extract_slice_indices(s, sequence_len)) # Should return (2, 8, 2) start, stop, step = 2, 10, 2 print(adjust_slice_indices(sequence_len, start, stop, step)) # Should return (2, 8, 2) print(create_slice(2, 10, 2)) # Should return slice(2, 10, 2) ``` This set of tasks requires a thorough understanding of slice objects and their manipulation via the Python C API, ensuring students\' comprehension of both fundamental and advanced concepts.","solution":"def extract_slice_indices(slice_obj, sequence_length): Extracts start, stop, and step indices from a given slice object considering the sequence length. Parameters: slice_obj (slice): The slice object to extract indices from. sequence_length (int): The length of the sequence that the slice applies to. Returns: tuple: A tuple containing (start, stop, step) indices. Raises: ValueError: If the slice indices are out of bounds. start, stop, step = slice_obj.indices(sequence_length) if (start < 0 or start >= sequence_length) or (stop < 0 or stop > sequence_length): raise ValueError(\\"Slice indices are out of bounds\\") return (start, stop, step) def create_slice(start, stop, step): Creates a slice object given start, stop, and step values. Parameters: start (int): Start index of the slice. stop (int): Stop index of the slice. step (int): Step index of the slice. Returns: slice: The slice object created with the given start, stop, and step. Raises: ValueError: If the slice cannot be created. if step == 0: raise ValueError(\\"Step cannot be zero\\") return slice(start, stop, step) def adjust_slice_indices(sequence_length, start, stop, step): Adjusts the start and stop indices of a slice based on the length of a sequence. Parameters: sequence_length (int): The length of the sequence. start (int): Start index of the slice. stop (int): Stop index of the slice. step (int): Step index of the slice. Returns: tuple: A tuple containing the adjusted (start, stop, step) indices. # Create a slice object and get the proper indices using slice.indices() slice_obj = slice(start, stop, step) return slice_obj.indices(sequence_length)"},{"question":"# JSON Custom Encoder and Decoder You are tasked with creating a custom JSON encoder and decoder in Python. Your task is to: 1. Implement a custom JSON encoder that can serialize instances of a custom `Person` class to JSON format. 2. Implement a custom JSON decoder that can deserialize JSON data back into instances of the `Person` class. 3. Ensure that your implementation correctly handles a list of `Person` objects during serialization and deserialization. The `Person` class is defined as follows: ```python class Person: def __init__(self, name, age): self.name = name self.age = age def __eq__(self, other): return self.name == other.name and self.age == other.age ``` Part 1: Implement the Custom Encoder Create a custom encoder class, `PersonEncoder`, that inherits from `json.JSONEncoder`. Your encoder should be able to serialize `Person` objects into JSON format. Part 2: Implement the Custom Decoder Create a custom decoder function that can be used with `json.loads` to deserialize JSON data back into `Person` objects. Part 3: Serialization and Deserialization of a List of Person Objects Write a function that takes a list of `Person` objects, serializes them to a JSON string, and then deserializes the JSON string back into a list of `Person` objects. **Input**: - A list of `Person` objects. **Output**: - A JSON string representing the list of `Person` objects (after serialization). - A list of `Person` objects (after deserialization). **Example**: ```python people = [Person(\\"Alice\\", 30), Person(\\"Bob\\", 25)] json_str = serialize_people(people) print(json_str) # Output should be a JSON string deserialized_people = deserialize_people(json_str) print(deserialized_people) # Output should be a list of Person objects equivalent to the input list ``` Constraints: 1. Your `serialize_people` function should use the `PersonEncoder` to convert the list of `Person` objects to JSON. 2. Your `deserialize_people` function should use your custom decoder function to convert the JSON string back to `Person` objects. 3. Ensure to handle edge cases such as an empty list of `Person` objects. Implementation Starter Code ```python import json class Person: def __init__(self, name, age): self.name = name self.age = age def __eq__(self, other): return self.name == other.name and self.age == other.age class PersonEncoder(json.JSONEncoder): def default(self, obj): # Your code here to handle instances of Person pass def person_decoder(dct): # Your code here to decode JSON data into Person instances pass def serialize_people(people): return json.dumps(people, cls=PersonEncoder) def deserialize_people(json_str): return json.loads(json_str, object_hook=person_decoder) # Testing the functions people = [Person(\\"Alice\\", 30), Person(\\"Bob\\", 25)] json_str = serialize_people(people) print(json_str) deserialized_people = deserialize_people(json_str) print(deserialized_people) ``` Complete the implementation of `PersonEncoder`, `person_decoder`, `serialize_people`, and `deserialize_people` functions.","solution":"import json class Person: def __init__(self, name, age): self.name = name self.age = age def __eq__(self, other): return self.name == other.name and self.age == other.age class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): return {\'name\': obj.name, \'age\': obj.age} return super().default(obj) def person_decoder(dct): if \'name\' in dct and \'age\' in dct: return Person(dct[\'name\'], dct[\'age\']) return dct def serialize_people(people): return json.dumps(people, cls=PersonEncoder) def deserialize_people(json_str): return json.loads(json_str, object_hook=person_decoder) # Testing the functions people = [Person(\\"Alice\\", 30), Person(\\"Bob\\", 25)] json_str = serialize_people(people) print(json_str) deserialized_people = deserialize_people(json_str) print(deserialized_people)"},{"question":"# Date Offsets Manipulation with pandas Objective: Your task is to implement a function that adjusts a given date according to specific business rules and checks certain date conditions using `pandas.tseries.offsets`. Problem Statement: Implement a function `adjust_business_date` that takes in a date string, and a dictionary of business rules, and returns the adjusted date. The function should also check if the final date meets specific conditions like being the start of a month, end of a quarter, etc. # Inputs: 1. `date_str` (str): The initial date string in the format \'YYYY-MM-DD\'. 2. `business_rules` (dict): A dictionary specifying the rules for adjusting the date. - The dictionary can include keys like `offset`, `n`, `weekmask`, and `holidays`. - `offset` (str): Type of business offset (`\'BusinessDay\'`, `\'BusinessHour\'`, etc.). - `n` (int): Number of units to apply the offset. - `weekmask` (str, optional): A string representing the working days in a week (e.g., \\"Mon Tue Wed Thu Fri\\"). - `holidays` (list, optional): A list of dates (in \'YYYY-MM-DD\' format) that are holidays. 3. `conditions` (list): A list of conditions to check for the adjusted date. Possible values include `\\"is_month_start\\"`, `\\"is_month_end\\"`, `\\"is_quarter_start\\"`, `\\"is_quarter_end\\"`, `\\"is_year_start\\"`, `\\"is_year_end\\"`. # Output: - A dictionary with the adjusted date and the results of the condition checks. The dictionary should have the following keys: - `adjusted_date` (str): The adjusted date in \'YYYY-MM-DD\' format. - `conditions` (dict): A dictionary where the keys are the conditions, and the values are booleans representing if the condition is met. # Constraints: - The input date string is always a valid date in the format \'YYYY-MM-DD\'. - The `n` value is a non-negative integer. - The `offset` value is always one of the valid offset types as mentioned in the documentation. Example: ```python from pandas.tseries.offsets import BusinessDay, CustomBusinessDay def adjust_business_date(date_str, business_rules, conditions): # Your implementation here # Example usage: date_str = \\"2023-10-10\\" business_rules = { \\"offset\\": \\"BusinessDay\\", \\"n\\": 5 } conditions = [\\"is_month_start\\", \\"is_month_end\\", \\"is_quarter_end\\"] result = adjust_business_date(date_str, business_rules, conditions) print(result) # Expected output: {\'adjusted_date\': \'2023-10-17\', \'conditions\': {\'is_month_start\': False, \'is_month_end\': False, \'is_quarter_end\': False}} ``` In this example, a date \\"2023-10-10\\" is adjusted by 5 business days to \\"2023-10-17\\", and the conditions are checked for the new date.","solution":"import pandas as pd from pandas.tseries.offsets import BusinessDay, CustomBusinessDay def adjust_business_date(date_str, business_rules, conditions): date = pd.to_datetime(date_str) offset_type = business_rules.get(\\"offset\\") n = business_rules.get(\\"n\\", 1) weekmask = business_rules.get(\\"weekmask\\", \\"Mon Tue Wed Thu Fri\\") holidays = business_rules.get(\\"holidays\\", []) if offset_type == \\"BusinessDay\\": offset = BusinessDay(n=n) elif offset_type == \\"CustomBusinessDay\\": offset = CustomBusinessDay(n=n, weekmask=weekmask, holidays=holidays) else: raise ValueError(f\\"Unsupported offset type: {offset_type}\\") adjusted_date = date + offset conditions_results = { \\"is_month_start\\": adjusted_date.is_month_start, \\"is_month_end\\": adjusted_date.is_month_end, \\"is_quarter_start\\": adjusted_date.is_quarter_start, \\"is_quarter_end\\": adjusted_date.is_quarter_end, \\"is_year_start\\": adjusted_date.is_year_start, \\"is_year_end\\": adjusted_date.is_year_end, } result = { \\"adjusted_date\\": adjusted_date.strftime(\'%Y-%m-%d\'), \\"conditions\\": {cond: conditions_results[cond] for cond in conditions} } return result"},{"question":"# Seaborn Area Plot Customization Challenge You are given a dataset about health expenditure and your task is to visualize it using seaborn\'s advanced plotting functionalities. This will help evaluate your understanding of data manipulation and seaborn plotting. **Problem Statement:** First, you\'ll load a dataset and transform it into a specific format. Then, using Seaborns `so.Plot`, you will: 1. Create a faceted area plot of the spending data. 2. Customize the plots with different colors, edgecolors, and additional lines. 3. Create a stacked area plot to visualize part-whole relationships among different countries over the years. **Steps and Requirements:** 1. **Load and Process the Dataset:** - Load the dataset named `healthexp` using `seaborn.load_dataset()`. - Transform it into a tidy DataFrame format with columns `[\'Year\', \'Country\', \'Spending_USD\']`, by following these operations: - Pivot the dataset with `Year` as the index and `Country` as columns. - Interpolate missing values. - Stack the DataFrame. - Reset the index and sort by `Country`. 2. **Create a Faceted Area Plot:** - Create an area plot with `Year` on the x-axis and `Spending_USD` on the y-axis. - Facet by `Country` with 3 columns per row. - Customize the edge color and fill color to be distinct. 3. **Stacked Area Plot:** - Create a stacked area plot (not faceted) to show the part-whole relationships of `Spending_USD` by different `Country` over the `Year`. - Add transparency to the areas for better visualization. **Input:** The function signature for your implemented function should be: ```python import seaborn.objects as so import pandas as pd from seaborn import load_dataset def visualize_health_expenditure(): # Load and preprocess the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create the faceted area plot p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(color=\\".5\\", edgecolor=\\"Country\\")).show() # Create the stacked area plot so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\") .add(so.Area(alpha=0.7), so.Stack()).show() ``` **Output:** There should be two visualizations: 1. A faceted area plot with different colors distinguishing the countries. 2. A stacked area plot showing combined spending by all countries with transparency applied. **Additional Constraints:** - Ensure the visualizations are clear and intuitive. - Use seaborns `so.Plot` and related capabilities effectively. The final plots should demonstrate comprehensive knowledge of seaborn\'s advanced plotting and customization options.","solution":"import seaborn.objects as so import pandas as pd from seaborn import load_dataset def visualize_health_expenditure(): # Load and preprocess the dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create the faceted area plot p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(color=\\"Country\\", edgecolor=\\".2\\")).show() # Create the stacked area plot so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\") .add(so.Area(alpha=0.7), so.Stack()).show()"},{"question":"Objective Demonstrate your understanding of the `tarfile` module in Python by implementing a function that processes a tar archive and performs various manipulations on its contents. Question You are tasked with writing a function `process_tarfile(tar_path: str, extraction_dir: str, files_to_add: list, filters: dict) -> dict`. The function will: 1. Open and read a tar archive specified by `tar_path`. 2. Print a list of all members in the archive. 3. Extract the archive into the directory specified by `extraction_dir`. 4. Add the files specified in `files_to_add` into the same tar archive. 5. Apply filters provided in the `filters` dictionary to modify the `TarInfo` objects before adding them to the archive. 6. Return a dictionary with keys as the names of the files added and values as booleans indicating whether the file was successfully added after applying the filter. # Inputs: 1. `tar_path` (str): The filesystem path to the tar archive. 2. `extraction_dir` (str): The directory where the archive should be extracted. 3. `files_to_add` (list of str): A list of filesystem paths to the files that need to be added to the tar archive. 4. `filters` (dict): A dictionary where keys are the file paths (matches a path in `files_to_add`) and values are functions that take a `tarfile.TarInfo` object and return a modified `TarInfo` object or `None` to exclude the file. # Outputs: 1. (dict): A dictionary with filenames as keys and booleans as values indicating whether each file was successfully added after applying the filter. # Example: ```python def custom_filter1(tarinfo): if tarinfo.name.endswith(\'.txt\'): tarinfo.uname = \'user1\' return tarinfo return None def custom_filter2(tarinfo): tarinfo.mtime = 0 return tarinfo tar_path = \'example.tar\' extraction_dir = \'./extracted\' files_to_add = [\'newfile1.txt\', \'newfile2.txt\'] filters = { \'newfile1.txt\': custom_filter1, \'newfile2.txt\': custom_filter2 } result = process_tarfile(tar_path, extraction_dir, files_to_add, filters) # result should be: # { # \'newfile1.txt\': True, # Added and modified by custom_filter1 # \'newfile2.txt\': True # Added and modified by custom_filter2 # } ``` # Constraints: - Do not use external libraries other than the Python Standard Library. - Consider appropriate exception handling for file operations. - Ensure the function works for large tar files efficiently. # Hints: - Use the `tarfile.open` function to manage opening, reading, and writing tar archives. - Utilize the `TarFile.add` method for adding files. - Apply filters using the provided dictionary before adding files to the tar archive.","solution":"import tarfile import os def process_tarfile(tar_path: str, extraction_dir: str, files_to_add: list, filters: dict) -> dict: try: # Open and read the tar archive with tarfile.open(tar_path, \'r\') as tar: # Print all members in the archive members = tar.getmembers() print(\\"Members in tar archive:\\") for member in members: print(member.name) # Extract the archive into the extraction_dir tar.extractall(path=extraction_dir) # Add files to the tar archive result = {} with tarfile.open(tar_path, \'a\') as tar: for file_path in files_to_add: if os.path.exists(file_path): tarinfo = tar.gettarinfo(file_path) filter_func = filters.get(file_path) if filter_func: tarinfo = filter_func(tarinfo) if tarinfo: # Add the file to the tar archive with open(file_path, \'rb\') as f: tar.addfile(tarinfo, f) result[file_path] = True else: result[file_path] = False else: result[file_path] = False return result except Exception as e: print(f\\"An error occurred: {e}\\") return {} # Example filter functions def custom_filter1(tarinfo): if tarinfo.name.endswith(\'.txt\'): tarinfo.uname = \'user1\' return tarinfo return None def custom_filter2(tarinfo): tarinfo.mtime = 0 return tarinfo"},{"question":"# **File Organizer: Implementing a Backup Utility** **Objective:** Create a Python function that organizes files from a given directory into subdirectories based on their file extensions, archives these directories, and maintains a log of these operations. # **Function Signature:** ```python def organize_and_backup(directory: str, backup_directory: str) -> None: pass ``` # **Input:** - `directory` (str): The path to the directory containing files to be organized. - `backup_directory` (str): The path to the directory where the backup archives will be stored. # **Output:** - None. # **Tasks:** 1. **Organize Files:** - Scan the given directory and identify all unique file extensions. - Create subdirectories within the given directory named after each unique file extension (e.g., \\"txt\\", \\"jpg\\"). - Move each file into its corresponding subdirectory based on file extension. 2. **Create Backups:** - Create a tar.gz archive for each subdirectory. - Store each archive in `backup_directory`. 3. **Maintain a Log:** - Create and maintain a log file named `backup_log.txt` in the `backup_directory`. - The log file should contain the name and size of each archived file, along with timestamp entries recording when each backup operation occurred. # **Constraints:** - Do not change the input directory name or structure beyond creating subdirectories and moving files into them. - The `backup_directory` must remain unaffected aside from the addition of the archives and the log file. # **Example Execution:** Suppose we have the following files in `directory`: ``` foo.txt bar.jpg baz.txt qux.png ``` After running the function, the `directory` should be organized as: ``` txt/ foo.txt baz.txt jpg/ bar.jpg png/ qux.png ``` The `backup_directory` should contain: ``` txt.tar.gz jpg.tar.gz png.tar.gz backup_log.txt ``` # **Implementation Notes:** - Use `shutil.move()` to move files. - Use `shutil.make_archive()` to create archives. - Use standard libraries such as `os` and `time` for file operations and logging. # **Performance Consideration:** - Avoid unnecessary file operations for performance. - Handle exceptions and ensure the logging process is reliable and resilient to failures during file operations. # **Logging Format:** The `backup_log.txt` should include entries as follows: ``` [YYYY-MM-DD HH:MM:SS] - txt.tar.gz - SIZE bytes [YYYY-MM-DD HH:MM:SS] - jpg.tar.gz - SIZE bytes [YYYY-MM-DD HH:MM:SS] - png.tar.gz - SIZE bytes ``` Where `YYYY-MM-DD HH:MM:SS` is the timestamp, and `SIZE` is the file size in bytes.","solution":"import os import shutil import tarfile import time def organize_and_backup(directory: str, backup_directory: str) -> None: # Step 1: Organize files into subdirectories based on file extensions for filename in os.listdir(directory): filepath = os.path.join(directory, filename) if os.path.isfile(filepath): file_ext = filename.split(\'.\')[-1] subdirectory = os.path.join(directory, file_ext) if not os.path.exists(subdirectory): os.makedirs(subdirectory) shutil.move(filepath, os.path.join(subdirectory, filename)) # Step 2: Create tar.gz archive for each subdirectory log_entries = [] for subdir in os.listdir(directory): subdir_path = os.path.join(directory, subdir) if os.path.isdir(subdir_path): archive_name = f\\"{subdir}.tar.gz\\" archive_path = os.path.join(backup_directory, archive_name) with tarfile.open(archive_path, \\"w:gz\\") as tar: tar.add(subdir_path, arcname=subdir) archive_size = os.path.getsize(archive_path) timestamp = time.strftime(\\"%Y-%m-%d %H:%M:%S\\", time.localtime()) log_entries.append(f\\"[{timestamp}] - {archive_name} - {archive_size} bytes\\") # Step 3: Maintain the log file log_file_path = os.path.join(backup_directory, \\"backup_log.txt\\") with open(log_file_path, \\"a\\") as log_file: for entry in log_entries: log_file.write(entry + \\"n\\")"},{"question":"# Custom Iterable Data Structure Implementation **Objective:** Implement a custom iterable data structure that inherits from the `collections.abc.Iterable` ABC. Your implementation should ensure that the data structure conforms to the interface required by the ABC. Specifically, you need to implement the `__iter__()` method. **Background:** The `collections.abc.Iterable` class requires the implementation of the `__iter__` method, which should return an iterator. An iterator is an object with a `__next__` method that returns the next item in the sequence. When no more items are available, it should raise the `StopIteration` exception. **Task:** 1. Create a class `CustomIterable` that implements the `collections.abc.Iterable` interface. 2. The class should have the following methods: - `__init__(self, data)`: Initialize with a list of data. - `__iter__(self)`: Return an iterator object. - `_CustomIterator`: A nested class that implements the iterator protocol with the `__next__` method. **Requirements:** - The `CustomIterable` class must inherit from `collections.abc.Iterable`. - The `CustomIterator` nested class must implement the iterator protocol, i.e., it should have the `__next__()` method. - The `__iter__()` method should return an instance of the `CustomIterator`. - The `__next__()` method in `CustomIterator` should iterate over the list provided during initialization. **Input:** - A list of any length is used to initialize the `CustomIterable`. **Output:** - The implemented class should behave like an iterable, allowing iteration using a loop. **Constraints:** - Do not use built-in Python iterators or generators directly. - The solution should handle edge cases, such as empty lists. **Example Usage:** ```python from collections.abc import Iterable class CustomIterable(Iterable): def __init__(self, data): self.data = data def __iter__(self): return self._CustomIterator(self.data) class _CustomIterator: def __init__(self, data): self.data = data self.index = 0 def __next__(self): if self.index < len(self.data): result = self.data[self.index] self.index += 1 return result else: raise StopIteration # Example usage: custom_iterable = CustomIterable([1, 2, 3, 4]) for item in custom_iterable: print(item) ``` Expected Output: ``` 1 2 3 4 ``` Use the provided class signature and implement it accordingly to ensure it passes all test cases.","solution":"from collections.abc import Iterable class CustomIterable(Iterable): def __init__(self, data): self.data = data def __iter__(self): return self._CustomIterator(self.data) class _CustomIterator: def __init__(self, data): self.data = data self.index = 0 def __next__(self): if self.index < len(self.data): result = self.data[self.index] self.index += 1 return result else: raise StopIteration"},{"question":"# MemoryView Operations You are tasked with implementing a function that performs multiple operations involving memoryview objects. This will demonstrate your understanding of how to create and manipulate memoryview objects in Python. Function Specification **Function Name:** ```python def manipulate_memoryviews(input_data): ``` **Input:** - `input_data`: a bytes object, such as `b\'This is a test string\'`. **Output:** - A tuple of three memoryview objects created through specific operations on `input_data`. **Function Tasks:** 1. Create a memoryview from `input_data`. 2. Create a new memoryview object from the first 10 bytes of `input_data`. 3. Create a memoryview object that points to a \'C\' contiguous chunk of memory from `input_data`. **Steps:** 1. Use `PyMemoryView_FromObject` to create a memoryview from `input_data`. 2. Extract the first 10 bytes of `input_data`, and use `PyMemoryView_FromMemory` to create a corresponding memoryview. 3. Ensure you create a contiguous chunk of memory using `PyMemoryView_GetContiguous` in \'C\' order from `input_data`. **Constraints:** - You should not modify the contents of `input_data`. - Ensure efficient use of memory and avoid unnecessary data copying. **Example:** ```python def manipulate_memoryviews(input_data): full_view = memoryview(input_data) partial_view = memoryview(input_data)[:10] # Creating a contiguous memoryview contiguous_view = memoryview(bytes(input_data), order=\'C\') return (full_view, partial_view, contiguous_view) input_data = b\'This is a test string\' result = manipulate_memoryviews(input_data) print(result) ``` **Expected Output:** You should see a tuple of three memoryview objects, where 1. The first memoryview contains the entire input data. 2. The second memoryview consists of the first 10 bytes of the input data. 3. The third memoryview is a contiguous memory representation in \'C\' order of the input data. **Notes:** - Memoryview objects cannot be directly printed to view their contents. Use `bytes()` or `list()` to visualize the contents for debugging. - Be mindful of the constraints and ensure memory-efficient operations.","solution":"def manipulate_memoryviews(input_data): Performs specified memoryview operations on the input data. Args: input_data (bytes): The input bytes object. Returns: tuple: A tuple containing three memoryview objects. # Create a memoryview from input_data full_view = memoryview(input_data) # Create a new memoryview object from the first 10 bytes of input_data partial_view = memoryview(input_data[:10]) # Create a contiguous memoryview object in \'C\' order contiguous_view = memoryview(bytes(input_data)) return full_view, partial_view, contiguous_view # Example usage input_data = b\'This is a test string\' result = manipulate_memoryviews(input_data) full_view, partial_view, contiguous_view = result # Convert memoryviews to bytes to easily verify the outputs print(bytes(full_view)) print(bytes(partial_view)) print(bytes(contiguous_view))"},{"question":"<|Analysis Begin|> The documentation provided revolves around \\"Argument Clinic\\", a preprocessor for CPython C files to automate the boilerplate of argument parsing code for builtins. It explains: 1. The goals of Argument Clinic. 2. Basic concepts and usages including analysis of a code block. 3. Step-by-step guide to converting C functions to use Argument Clinic. 4. Advanced topics like symbolic default values, renaming functions, converting functions using PyArg_UnpackTuple, optional groups, and various converters. 5. Utilizing return converters and handling default values. 6. Advanced configurations for redirecting Clinics output. 7. Handling functions non-available on all platforms with the #ifdef trick. 8. Using Argument Clinic in Python files. The majority of the documentation focuses on converting existing C functions to Argument Clinic format and transforming function signatures with different converters. Based on the documentation, an appropriate task would test students\' understanding of writing a Python function that mimics the automated transformation process described in the documentation. <|Analysis End|> <|Question Begin|> **Question:** You are given the task of writing a function to automate the conversion process of a given Python function to a specific format. The input is a Python function\'s signature and a dictionary reflecting the function\'s parameters and their \'converters\'. Your task is to produce a properly formatted function signature according to the documentation. # Requirements: - Implement the function `convert_function_signature` which takes two inputs: 1. `function_name`: A string representing the name of the function. 2. `params_dict`: A dictionary where the keys are the parameter names, and the values are their converters as strings. # Function Signature: ```python def convert_function_signature(function_name: str, params_dict: dict) -> str: ``` # Output: - The function should return a formatted string that represents the function\'s signature in a format similar to what Argument Clinic\'s block would generate. # Example: **Input:** ```python function_name = \\"example_function\\" params_dict = { \\"param1\\": \\"int\\", \\"param2\\": \\"str = \'default\'\\", \\"param3\\": \\"bool = False\\" } ``` **Output:** ```python \'\'\' /*[clinic input] example_function param1: int param2: str = \'default\' param3: bool = False /[*clinic start generated code]*/ /*[*clinic end generated code: checksum=...]*/ \'\'\' ``` # Constraints: - You can assume that the function name and parameters will always be provided as strings, with default values for parameters included in the converter strings when applicable. - The formatted text should comply with the Argument Clinic python input block format shown in the documentation. **Performance Consideration:** - Ensure the function runs efficiently, focusing on string manipulations and formatting. Good luck!","solution":"def convert_function_signature(function_name: str, params_dict: dict) -> str: Convert a given function\'s parameters dictionary to a specific Argument Clinic format. # Create the initial clinic input start with the function name clinic_input = f\\"/*[clinic input]n{function_name}\\" # Iterate through the parameter dictionary and append to the clinic input for param, converter in params_dict.items(): clinic_input += f\\"n {param}: {converter}\\" # End the clinic input block and start the generated code block clinic_input += \\"n/[*clinic start generated code]*/nn/*[*clinic end generated code: checksum=...]*/\\" # Return the formatted clinic input string return clinic_input"},{"question":"# Custom Type Management in Python In this problem, you will create a manager for custom heap types in Python, simulating some of the behaviors described in the provided `PyTypeObject` documentation. Your task is to implement a `TypeManager` class that can create and store custom types with specified attributes and methods. Detailed Requirements: 1. **TypeManager Class**: - Implement a class `TypeManager` which initializes with no arguments. - This class should manage the creation and storage of custom types. 2. **Creating a New Type**: - Implement a method `create_type(name: str, base_classes: tuple, attributes: dict)`: - `name`: Name of the type (e.g., \\"MyType\\") - `base_classes`: A tuple of base classes the new type should inherit from. - `attributes`: A dictionary where keys are attribute/method names and values are the respective attribute values or function implementations. - This method should dynamically create a new type and store it within the `TypeManager` instance using the provided name. 3. **Retrieving a Type**: - Implement a method `get_type(name: str)`: - This should return the type object associated with the given name. 4. **Constraints**: - The attributes and methods of the created types should function as expected. For methods, ensure they can be called with the appropriate `self` parameter. - The `create_type` method should handle attribute and method assignments correctly. 5. **Example Usage**: ```python manager = TypeManager() attrs = { \'class_attr\': 42, \'instance_method\': lambda self: f\'Instance of {self.__class__.__name__}\' } manager.create_type(\'MyCustomType\', (object,), attrs) MyCustomType = manager.get_type(\'MyCustomType\') instance = MyCustomType() assert instance.class_attr == 42 assert instance.instance_method() == \'Instance of MyCustomType\' ``` Input: - You do not need to handle any input directly; just focus on the implementation of the `TypeManager` class and its methods. Output: - The output is expected from the correct functionality of the `TypeManager` class methods used in the example and any other tests you devise during assessment. Ensure your implementation is Pythonic and leverages appropriate features of Python\'s object-oriented programming.","solution":"class TypeManager: def __init__(self): self.types = {} def create_type(self, name: str, base_classes: tuple, attributes: dict): Create a new type with the given name, base classes, and attributes, and stores it within the TypeManager instance. new_type = type(name, base_classes, attributes) self.types[name] = new_type def get_type(self, name: str): Retrieve the type object associated with the given name. return self.types.get(name)"},{"question":"# Question: Implement and Train a Simple Neural Network on the MPS Device **Problem Description:** You are required to implement a simple neural network and train it on the MPS device using the PyTorch framework. The task will help in understanding how to utilize the MPS backend for GPU acceleration on macOS devices. Follow the steps below to complete the task: 1. **Check for the availability of the MPS backend.** - If the MPS backend is not available, print an appropriate error message and exit the program. 2. **Define the Neural Network:** - Implement a basic neural network with one hidden layer using `torch.nn.Module`. 3. **Dataset:** - For simplicity, you will train on a small synthetic dataset. Create a simple dataset using `torch.tensor`. 4. **Training:** - Move the neural network and the input data to the MPS device. - Train the network for a fixed number of epochs. - Print the loss after each epoch. **Input:** There are no explicit inputs to the function. The function should create and use synthetic data. **Output:** Print the loss after each epoch. **Requirements:** - Use the `torch.backends.mps` to check for MPS backend availability. - The neural network must be defined using `torch.nn.Module`. - Ensure that tensors and modules are moved to the MPS device. - Implement a basic training loop that utilizes the GPU for computations. **Constraints:** - Assume macOS version is 12.3+ and an MPS-enabled device is available. - You must use PyTorch version that supports MPS backend. **Example Implementation:** ```python import torch import torch.nn as nn import torch.optim as optim # Check that MPS is available if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\\"MPS not available because the current PyTorch install was not \\" \\"built with MPS enabled.\\") else: print(\\"MPS not available because the current MacOS version is not 12.3+ \\" \\"and/or you do not have an MPS-enabled device on this machine.\\") exit() # Setting up the MPS device mps_device = torch.device(\\"mps\\") # Define Neural Network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Create a simple synthetic dataset X = torch.randn(100, 10, device=mps_device) y = torch.randn(100, 1, device=mps_device) # Load the model model = SimpleNet().to(mps_device) # Define loss function and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop epochs = 10 for epoch in range(epochs): model.train() # Forward pass outputs = model(X) loss = criterion(outputs, y) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() print(f\\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\\") ``` Implement this function and ensure that the training runs efficiently on an MPS-enabled GPU.","solution":"import torch import torch.nn as nn import torch.optim as optim def check_mps_availability(): # Check that MPS is available if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): raise RuntimeError(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: raise RuntimeError(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") return torch.device(\\"mps\\") # Define Neural Network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def train_simple_net_on_mps(): mps_device = check_mps_availability() # Create a simple synthetic dataset X = torch.randn(100, 10, device=mps_device) y = torch.randn(100, 1, device=mps_device) # Load the model model = SimpleNet().to(mps_device) # Define loss function and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop epochs = 10 for epoch in range(epochs): model.train() # Forward pass outputs = model(X) loss = criterion(outputs, y) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() print(f\\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\\") # This would actually train the model if run in a suitable environment # train_simple_net_on_mps()"},{"question":"# SAX XML Parsing with Custom Handlers Objective Implement a custom ContentHandler to parse an XML document and generate a summary of its structure. Problem Statement Given an XML document string, write a Python program that uses the SAX API to parse the XML document and generate a summary report containing: 1. The total number of elements. 2. The total number of start and end tags. 3. The frequency of each tag. 4. The total number of characters contained within elements. You need to implement the following components: 1. A `MyContentHandler` class inheriting from `xml.sax.handler.ContentHandler` with overridden methods to track the required information. 2. A `parse_xml` function that initializes the parser, sets the content handler, and parses the XML input. Input Format - A string containing the XML document. Output Format - A dictionary with the following keys: - `total_elements`: Total number of elements in the XML. - `start_tags`: Total number of start tags. - `end_tags`: Total number of end tags. - `tag_frequency`: A dictionary with tag names as keys and their frequencies as values. - `total_characters`: Total number of character data within elements. Constraints - Assume the XML document is well-formed. - The XML document can contain nested elements. Examples # Example 1 **Input:** ```python xml_string = <?xml version=\\"1.0\\"?> <root> <child> <subchild>content1</subchild> <subchild>content2</subchild> </child> </root> ``` **Output:** ```python { \\"total_elements\\": 4, \\"start_tags\\": 4, \\"end_tags\\": 4, \\"tag_frequency\\": { \\"root\\": 1, \\"child\\": 1, \\"subchild\\": 2, }, \\"total_characters\\": 16 } ``` # Sample Code Template ```python import xml.sax from xml.sax.handler import ContentHandler class MyContentHandler(ContentHandler): def __init__(self): super().__init__() self.total_elements = 0 self.start_tags = 0 self.end_tags = 0 self.tag_frequency = {} self.total_characters = 0 def startElement(self, name, attrs): self.start_tags += 1 self.total_elements += 1 if name in self.tag_frequency: self.tag_frequency[name] += 1 else: self.tag_frequency[name] = 1 def endElement(self, name): self.end_tags += 1 def characters(self, content): self.total_characters += len(content) def parse_xml(xml_string): handler = MyContentHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler) result = { \\"total_elements\\": handler.total_elements, \\"start_tags\\": handler.start_tags, \\"end_tags\\": handler.end_tags, \\"tag_frequency\\": handler.tag_frequency, \\"total_characters\\": handler.total_characters, } return result # Test the function with the provided example xml_string = <?xml version=\\"1.0\\"?> <root> <child> <subchild>content1</subchild> <subchild>content2</subchild> </child> </root> print(parse_xml(xml_string)) ``` Your implementation should ensure that all edge cases are handled correctly, such as self-closing tags and empty content.","solution":"import xml.sax from xml.sax.handler import ContentHandler class MyContentHandler(ContentHandler): def __init__(self): super().__init__() self.total_elements = 0 self.start_tags = 0 self.end_tags = 0 self.tag_frequency = {} self.total_characters = 0 def startElement(self, name, attrs): self.start_tags += 1 self.total_elements += 1 if name in self.tag_frequency: self.tag_frequency[name] += 1 else: self.tag_frequency[name] = 1 def endElement(self, name): self.end_tags += 1 def characters(self, content): self.total_characters += len(content.strip()) def parse_xml(xml_string): handler = MyContentHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler) result = { \\"total_elements\\": handler.total_elements, \\"start_tags\\": handler.start_tags, \\"end_tags\\": handler.end_tags, \\"tag_frequency\\": handler.tag_frequency, \\"total_characters\\": handler.total_characters, } return result"},{"question":"Objective: The objective is to assess the students\' ability to utilize the \\"nis\\" module to interact with the NIS database on Unix systems. This will test their knowledge of importing modules, handling exceptions, and performing data lookups and transformations. Question: You are tasked with implementing a function that processes user information stored in an NIS database. Specifically, you need to fetch the user data from a specific NIS map and filter the entries based on a given criterion. Implement a function `filter_users(mapname: str, key_filter: str, domain: str = None) -> dict` that takes in a map name, a key filter, and optionally an NIS domain and returns a dictionary of filtered results. 1. **Parameters:** - `mapname` (str): The name of the NIS map to query. - `key_filter` (str): A substring to filter keys. Only entries where the key contains this substring should be returned. - `domain` (str, optional): The NIS domain for the lookup. If not specified, the system\'s default NIS domain should be used. 2. **Returns:** - A dictionary containing all key-value pairs from the specified NIS map where the key contains the substring `key_filter`. 3. **Constraints:** - The function should handle possible `nis.error` exceptions that occur during the lookup process and return an empty dictionary in such cases. - Both keys and values in the returned dictionary must be strings. (Assume the byte arrays can be safely decoded using UTF-8 encoding.) 4. **Example:** ```python try: import nis except ModuleNotFoundError: print(\\"The \'nis\' module is not available on this system.\\") def filter_users(mapname: str, key_filter: str, domain: str = None) -> dict: # Your implementation here ``` Given an NIS map named \\"passwd\\" and a key filter \\"admin\\", the function call ```python filter_users(\\"passwd\\", \\"admin\\") ``` could return a dictionary like: ```python { \\"admin_user1\\": \\"data1\\", \\"admin_user2\\": \\"data2\\" } ``` assuming that only the users with \\"admin\\" in their usernames exist in the \\"passwd\\" map. Notes: - Ensure your implementation includes necessary error handling for robustness. - Assume the mapname and key_filter parameters will be valid non-empty strings. - Consider edge cases where no entries match the filter.","solution":"import nis def filter_users(mapname: str, key_filter: str, domain: str = None) -> dict: Fetches user data from the specified NIS map and filters based on a given key substring. :param mapname: The name of the NIS map to query. :param key_filter: A substring to filter keys. :param domain: The NIS domain for the lookup, optional. Defaults to system\'s domain. :return: A dictionary of filtered key-value pairs. try: nis_map = nis.cat(mapname, domain) except nis.error: return {} filtered_results = {key.decode(\'utf-8\'): value.decode(\'utf-8\') for key, value in nis_map.items() if key_filter in key.decode(\'utf-8\')} return filtered_results"},{"question":"You are given the task of implementing a custom data serialization tool using the `marshal` module functions. Your tool should be able to save multiple types of Python objects to a file and then reconstruct them. Specifically, you need to implement the functions detailed below: Function 1: `serialize_data(filename: str, data: list, version: int) -> None` Write a function to serialize a list of Python objects to a file. The function should: - Take the name of the file (`filename`) where the serialized data will be stored. - Take a list of Python objects (`data`) to serialize. - Take an integer (`version`) indicating the version of the marshalling format. Use the provided `marshal` C API functions to implement the serialization. Function 2: `deserialize_data(filename: str) -> list` Write a function to deserialize the list of Python objects from a file. The function should: - Take the name of the file (`filename`) from which the data will be read. Use the provided `marshal` C API functions to implement the deserialization. Ensure you handle any possible errors like EOF, value errors, or type errors appropriately. # Constraints 1. You only need to work with objects that are serializable by the `marshal` module. 2. You can assume that the file does not contain any data other than what has been written by your `serialize_data` function. 3. The `data` list can contain any mix of data types supported by `marshal`. # Example ```python # Example usage: data_to_serialize = [123, \\"hello\\", [1, 2, 3], {\\"key\\": \\"value\\"}] # Serialize the data to a file serialize_data(\\"output.dat\\", data_to_serialize, version=2) # Deserialize the data from the file restored_data = deserialize_data(\\"output.dat\\") print(restored_data) # Output should be: # [123, \\"hello\\", [1, 2, 3], {\\"key\\": \\"value\\"}] ``` # Performance Requirements Your implementation should be efficient in serialization and deserialization. Ensure you handle file operations correctly and gracefully manage any potential errors. # Notes - Use the `PyMarshal_WriteObjectToFile` and `PyMarshal_ReadObjectFromFile` functions from the `marshal` C API appropriately. - Make sure you understand the error handling mechanism provided by the API and use it for robust error handling.","solution":"import marshal def serialize_data(filename: str, data: list, version: int) -> None: Serializes a list of Python objects to a file using the marshal module. Args: filename (str): The name of the file where the serialized data will be stored. data (list): A list of Python objects to serialize. version (int): An integer indicating the version of the marshalling format. Returns: None with open(filename, \'wb\') as file: marshal.dump(data, file, version) def deserialize_data(filename: str) -> list: Deserializes the list of Python objects from a file using the marshal module. Args: filename (str): The name of the file from which the data will be read. Returns: list: The deserialized list of Python objects. with open(filename, \'rb\') as file: return marshal.load(file)"},{"question":"# Data Persistence and Serialization with `pickle` and `sqlite3` Objective Implement a set of Python functions to manage a persistent user data store. This data store should use the `pickle` module for serializing and deserializing Python objects and the `sqlite3` module for storing these serialized objects in an SQLite database. Background - `pickle`: Used for serializing and deserializing Python objects into a byte stream. - `sqlite3`: Used for interacting with SQLite databases in Python. Requirements 1. **Function 1: initialize_db** - **Input**: `db_path` (string) - Path to the SQLite database file. - **Output**: None - **Description**: This function should create (or initialize) an SQLite database with a table named `users` with the following columns: - `id` (INTEGER, PRIMARY KEY, AUTOINCREMENT) - `username` (TEXT, UNIQUE) - `data` (BLOB) 2. **Function 2: add_user_data** - **Input**: `db_path` (string), `username` (string), `data` (dict) - **Output**: None - **Description**: This function should serialize the `data` dictionary using `pickle` and store it in the `users` table with the provided `username`. If the `username` already exists, the function should update the existing record. 3. **Function 3: get_user_data** - **Input**: `db_path` (string), `username` (string) - **Output**: `data` (dict) - Deserialized dictionary for the given `username` - **Description**: This function should query the `users` table for the given `username`, retrieve the serialized data, deserialize it using `pickle`, and return it as a dictionary. Constraints - The `username` should be unique in the `users` table. - Ensure proper error handling for database operations and serialization/deserialization processes. - The functions should be designed to handle potential edge cases, such as non-existent users or database connection issues. Performance Requirements - The solution should handle at least 1000 user records efficiently. - Proper indexes should be used to ensure efficient querying. Example Usage ```python # Initialize the database initialize_db(\'user_data.db\') # Add user data add_user_data(\'user_data.db\', \'alice\', {\'email\': \'alice@example.com\', \'age\': 30}) # Retrieve user data data = get_user_data(\'user_data.db\', \'alice\') print(data) # Output: {\'email\': \'alice@example.com\', \'age\': 30} ``` *Note: Please ensure you test your functions with various cases to confirm their robustness and efficiency.","solution":"import sqlite3 import pickle def initialize_db(db_path): Initializes an SQLite database with a table \'users\'. :param db_path: Path to the SQLite database file. conn = sqlite3.connect(db_path) cursor = conn.cursor() cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS users ( id INTEGER PRIMARY KEY AUTOINCREMENT, username TEXT UNIQUE, data BLOB ) \'\'\') conn.commit() conn.close() def add_user_data(db_path, username, data): Adds or updates user data in the SQLite database. :param db_path: Path to the SQLite database file. :param username: The username to add or update. :param data: Dictionary containing the user data. conn = sqlite3.connect(db_path) cursor = conn.cursor() serialized_data = pickle.dumps(data) cursor.execute(\'\'\' INSERT INTO users (username, data) VALUES (?, ?) ON CONFLICT(username) DO UPDATE SET data=excluded.data \'\'\', (username, serialized_data)) conn.commit() conn.close() def get_user_data(db_path, username): Retrieves user data from the SQLite database. :param db_path: Path to the SQLite database file. :param username: The username whose data is to be retrieved. :return: Dictionary containing the user data. conn = sqlite3.connect(db_path) cursor = conn.cursor() cursor.execute(\'\'\' SELECT data FROM users WHERE username=? \'\'\', (username,)) result = cursor.fetchone() conn.close() if result: return pickle.loads(result[0]) else: return None"},{"question":"**Question: Data Visualization with Pandas** **Objective:** Your task is to create a Python function using pandas that performs specific data visualizations on a given dataset. The function should demonstrate your comprehension of both fundamental and advanced plotting capabilities provided by pandas. **Function Signature:** ```python def visualize_dataframe(input_csv: str) -> None: Parameters: input_csv (str): Path to the input CSV file containing the dataset. The function should generate the following visualizations: 1. A cumulative sum line plot of all numeric columns. 2. A histogram of all numeric columns. 3. A box plot of all numeric columns, grouped by a specific categorical column. 4. A scatter plot comparing two specified columns. 5. A hexagonal bin plot for two specified columns. 6. A parallel coordinates plot for all numeric columns, grouped by a specific categorical column. Notes: - The dataset will be in a CSV format, and the first row will contain headers. - Handle any missing values appropriately for plotting. - Each plot should be labeled and titled appropriately for clarity. Returns: None: The function should display the plots but does not return any values. ``` **Input Data:** The dataset will be a CSV file with a mixture of categorical and numeric columns. For the purposes of this question, assume a dataset with at least the following columns: - `Date`: Dates in YYYY-MM-DD format. - `Category`: A categorical column. - `Value1`: A numeric column. - `Value2`: A numeric column. - `Value3`: A numeric column. **Constraints:** - The input CSV file path will always point to a valid CSV file. - The dataset will always contain at least the columns specified above. - Handle missing data as described in the documentation. **Requirements:** 1. **Cumulative Sum Line Plot:** - Plot the cumulative sum of all numeric columns (`Value1`, `Value2`, `Value3`) over time (`Date`). - Ensure that the x-axis labels are formatted nicely to show dates. 2. **Histogram:** - Plot histograms for each numeric column (`Value1`, `Value2`, `Value3`) on separate subplots in a single figure. - The histograms should be transparent and stacked. 3. **Box Plot:** - Create a box plot for each numeric column (`Value1`, `Value2`, `Value3`), grouped by the `Category` column. - Use different colors for each box plot. 4. **Scatter Plot:** - Create a scatter plot comparing `Value1` and `Value2`. - Color the points using another numeric column `Value3`. 5. **Hexagonal Bin Plot:** - Create a hexagonal bin plot for `Value1` and `Value2`. 6. **Parallel Coordinates:** - Create a parallel coordinates plot for the numeric columns (`Value1`, `Value2`, `Value3`), grouped by the `Category` column. **Example:** Consider the input CSV `data.csv` with the following structure: ``` Date,Category,Value1,Value2,Value3 2022-01-01,A,1,2,3 2022-01-02,B,2,3,4 2022-01-03,A,3,4,5 2022-01-04,B,4,5,6 2022-01-05,A,5,6,7 ``` The `visualize_dataframe` function should display the required plots based on this data. **Additional Notes:** - Use appropriate figure sizes and layout adjustments to ensure that the plots are clear and do not overlap. - Use comments within your function to explain the steps and ensure readability.","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import parallel_coordinates def visualize_dataframe(input_csv: str) -> None: Parameters: input_csv (str): Path to the input CSV file containing the dataset. The function should generate the following visualizations: 1. A cumulative sum line plot of all numeric columns. 2. A histogram of all numeric columns. 3. A box plot of all numeric columns, grouped by a specific categorical column. 4. A scatter plot comparing two specified columns. 5. A hexagonal bin plot for two specified columns. 6. A parallel coordinates plot for all numeric columns, grouped by a specific categorical column. Returns: None # Read the dataset df = pd.read_csv(input_csv, parse_dates=[\'Date\']) # Handle missing values df = df.dropna() # 1. Cumulative Sum Line Plot plt.figure(figsize=(10, 6)) df.set_index(\'Date\')[[\'Value1\', \'Value2\', \'Value3\']].cumsum().plot() plt.title(\'Cumulative Sum of Numeric Columns Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Cumulative Sum\') plt.grid(True) plt.show() # 2. Histogram df[[\'Value1\', \'Value2\', \'Value3\']].plot.hist(alpha=0.5, bins=50, figsize=(10, 6)) plt.title(\'Histogram of Numeric Columns\') plt.xlabel(\'Value\') plt.grid(True) plt.show() # 3. Box Plot grouped by Category df[[\'Value1\', \'Value2\', \'Value3\', \'Category\']].boxplot(by=\'Category\', figsize=(12, 8)) plt.title(\'Box Plot of Numeric Columns Grouped by Category\') plt.suptitle(\'\') plt.xlabel(\'Category\') plt.ylabel(\'Value\') plt.grid(True) plt.show() # 4. Scatter Plot comparing Value1 and Value2 plt.figure(figsize=(8, 6)) scatter = plt.scatter(df[\'Value1\'], df[\'Value2\'], c=df[\'Value3\'], cmap=\'viridis\') plt.colorbar(scatter) plt.title(\'Scatter Plot of Value1 vs Value2\') plt.xlabel(\'Value1\') plt.ylabel(\'Value2\') plt.grid(True) plt.show() # 5. Hexagonal Bin Plot for Value1 and Value2 df.plot.hexbin(x=\'Value1\', y=\'Value2\', gridsize=30, cmap=\'Oranges\', figsize=(8, 6)) plt.title(\'Hexagonal Bin Plot of Value1 vs Value2\') plt.xlabel(\'Value1\') plt.ylabel(\'Value2\') plt.grid(True) plt.show() # 6. Parallel Coordinates Plot plt.figure(figsize=(12, 8)) parallel_coordinates(df, class_column=\'Category\', cols=[\'Value1\', \'Value2\', \'Value3\'], color=[\'#556270\', \'#4ECDC4\']) plt.title(\'Parallel Coordinates Plot of Numeric Columns Grouped by Category\') plt.xlabel(\'Variable\') plt.ylabel(\'Value\') plt.grid(True) plt.show()"},{"question":"# Advanced Debugging and Profiling Challenge Objective: Your task is to write a Python function to find the nth Fibonacci number. Additionally, you need to utilize Python\'s profiling tools to analyze the performance of your solution and provide a report of the bottlenecks identified. Requirements: 1. **Function Implementation**: - Implement a function `fibonacci(n: int) -> int` that returns the nth Fibonacci number. - The input `n` is guaranteed to be a non-negative integer. 2. **Profiling**: - Use the `cProfile` module to profile your `fibonacci` function. - Generate a report that shows the performance of your function, including the most time-consuming parts. 3. **Analysis**: - Based on the profiling report, identify the bottleneck in your `fibonacci` function. - Suggest an optimization to improve the performance of your function. Input Format: - An integer `n` where `0 <= n <= 30`. Output Format: 1. The nth Fibonacci number. 2. A profiling report showing the time consumed by different parts of your `fibonacci` function. 3. A brief analysis (2-3 sentences) on the bottleneck and a suggestion for optimization. Constraints: - Ensure your profiling analysis is clear and well-documented in your code. - The optimization suggestion should be practical and directly related to the observed bottleneck. Example: ```python def fibonacci(n: int) -> int: # Your implementation here # Example usage: import cProfile profiler = cProfile.Profile() profiler.enable() result = fibonacci(20) profiler.disable() profiler.print_stats() print(\\"The 20th Fibonacci number is:\\", result) ``` Note: Make sure to use appropriate profiling and debugging techniques learned from the documentation to achieve the objective of this task. The clarity and precision of your profiling report and optimization suggestion will be significant criteria for evaluation.","solution":"def fibonacci(n: int) -> int: Returns the nth Fibonacci number using an iterative approach. if n == 0: return 0 elif n == 1: return 1 a, b = 0, 1 for _ in range(2, n + 1): a, b = b, a + b return b # Profiling the function if __name__ == \\"__main__\\": import cProfile profiler = cProfile.Profile() profiler.enable() result = fibonacci(20) profiler.disable() profiler.print_stats() print(\\"The 20th Fibonacci number is:\\", result)"},{"question":"You are provided with a neural network model defined in PyTorch. Your task is to write a function that uses `torch.fx` to transform this model\'s computation graph by replacing all instances of the ReLU activation function with LeakyReLU. # Requirements 1. **Input:** - A `torch.nn.Module` object `model`. - Parameters for `LeakyReLU`, specifically the negative slope (default value is 0.01). 2. **Output:** - A new `torch.nn.Module` object with the ReLU activation functions replaced by LeakyReLU. 3. **Constraints:** - Use `torch.fx` for tracing and transforming the model. - Ensure that the transformed model can run equivalently to the original model with the modifications applied. - Assume that the model may contain multiple layers including, but not limited to, convolutional layers, linear layers, and activation functions. 4. **Implementation Details:** - The function should be named `replace_relu_with_leaky_relu`. - Use `torch.fx.symbolic_trace` to obtain the computation graph. - Modify nodes within the graph as needed. - Return a new `torch.fx.GraphModule` with the desired transformations. # Example ```python import torch import torch.nn as nn import torch.fx # Sample model definition class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.conv = nn.Conv2d(3, 16, 3) self.relu = nn.ReLU() self.fc = nn.Linear(16 * 6 * 6, 10) def forward(self, x): x = self.conv(x) x = self.relu(x) x = x.view(x.size(0), -1) x = self.fc(x) return x # Function to replace ReLU with LeakyReLU def replace_relu_with_leaky_relu(model: nn.Module, negative_slope: float = 0.01) -> nn.Module: class ReplaceReLUTracer(torch.fx.Tracer): def is_leaf_module(self, m: torch.nn.Module, qualname: str): # Consider all modules as leaf modules except `ReLU` if isinstance(m, nn.ReLU): return False return super().is_leaf_module(m, qualname) tracer = ReplaceReLUTracer() graph = tracer.trace(model) # Modify graph to replace ReLU with LeakyReLU for node in graph.nodes: if node.op == \'call_module\' and isinstance(tracer.root.get_submodule(node.target), nn.ReLU): # Create a new LeakyReLU module and swap it in new_module_name = f\\"{node.target}_leakyrelu\\" leaky_relu = nn.LeakyReLU(negative_slope=negative_slope) tracer.root.add_module(new_module_name, leaky_relu) node.target = new_module_name # Recompile and return the new graph module new_graph_module = torch.fx.GraphModule(tracer.root, graph) return new_graph_module # Instantiate and transform the model model = SampleModel() transformed_model = replace_relu_with_leaky_relu(model) # Print the transformed model to verify print(transformed_model) ``` Implement the function `replace_relu_with_leaky_relu` as demonstrated in the provided example. # Notes - Ensure to test the returned model to confirm it behaves as expected with the replaced activation function. - The transformed model should work seamlessly with other PyTorch functionalities.","solution":"import torch import torch.nn as nn import torch.fx def replace_relu_with_leaky_relu(model: nn.Module, negative_slope: float = 0.01) -> nn.Module: class ReplaceReLUTracer(torch.fx.Tracer): def is_leaf_module(self, m: torch.nn.Module, qualname: str): # Consider all modules as leaf modules except `ReLU` if isinstance(m, nn.ReLU): return False return super().is_leaf_module(m, qualname) tracer = ReplaceReLUTracer() graph = tracer.trace(model) # Modify graph to replace ReLU with LeakyReLU for node in graph.nodes: if node.op == \'call_module\' and isinstance(tracer.root.get_submodule(node.target), nn.ReLU): # Create a new LeakyReLU module and swap it in new_module_name = f\\"{node.target}_leakyrelu\\" leaky_relu = nn.LeakyReLU(negative_slope=negative_slope) tracer.root.add_module(new_module_name, leaky_relu) node.target = new_module_name # Recompile and return the new graph module new_graph_module = torch.fx.GraphModule(tracer.root, graph) return new_graph_module"},{"question":"Objective: Your task is to implement three functions from scratch in PyTorch that perform the following operations on tensors: 1. **Compute Signal to Quantization Noise Ratio (SQNR)**. 2. **Compute Normalized L2 Error**. 3. **Compute Cosine Similarity**. Requirements: Each function should take two tensors as input and return a scalar value as output. Use the following specifications for each function: 1. **Signal to Quantization Noise Ratio (SQNR)** - **Function signature**: `def compute_sqnr(x: torch.Tensor, y: torch.Tensor) -> float` - **Input**: Two tensors `x` and `y`, both of the same shape. - **Output**: A floating-point scalar value representing the SQNR between `x` and `y`. - **Calculation**: The SQNR is defined as: [ text{SQNR} = 10 times log_{10}left(frac{sum (x_i)^2}{sum (x_i - y_i)^2}right) ] Here, ( x_i ) and ( y_i ) are the elements of tensors `x` and `y`, respectively. 2. **Normalized L2 Error** - **Function signature**: `def compute_normalized_l2_error(x: torch.Tensor, y: torch.Tensor) -> float` - **Input**: Two tensors `x` and `y`, both of the same shape. - **Output**: A floating-point scalar value representing the normalized L2 error between `x` and `y`. - **Calculation**: The normalized L2 error is defined as: [ text{Normalized L2 Error} = frac{|x - y|_2}{|x|_2} ] where (|.|_2) denotes the L2 norm. 3. **Cosine Similarity** - **Function signature**: `def compute_cosine_similarity(x: torch.Tensor, y: torch.Tensor) -> float` - **Input**: Two tensors `x` and `y`, both of the same shape. - **Output**: A floating-point scalar value representing the cosine similarity between `x` and `y`. - **Calculation**: The cosine similarity is defined as: [ text{Cosine Similarity} = frac{x cdot y}{|x|_2 times |y|_2} ] where ( x cdot y ) denotes the dot product of vectors `x` and `y`. Constraints: 1. You can assume `x` and `y` are always non-empty tensors of the same shape. 2. Avoid using in-built PyTorch functions that directly compute these metrics (e.g., `F.cosine_similarity`). Example Usage: ```python import torch x = torch.tensor([1.0, 2.0, 3.0]) y = torch.tensor([1.1, 2.1, 2.9]) print(compute_sqnr(x, y)) # Expected output will be a scalar value denoting SQNR print(compute_normalized_l2_error(x, y)) # Expected output will be a scalar value denoting normalized L2 error print(compute_cosine_similarity(x, y)) # Expected output will be a scalar value denoting cosine similarity ``` Implement these functions in a Python module using PyTorch to demonstrate your understanding of tensor operations in PyTorch.","solution":"import torch def compute_sqnr(x: torch.Tensor, y: torch.Tensor) -> float: Compute Signal to Quantization Noise Ratio (SQNR) between tensors x and y. signal_power = torch.sum(x ** 2) noise_power = torch.sum((x - y) ** 2) sqnr = 10 * torch.log10(signal_power / noise_power) return sqnr.item() def compute_normalized_l2_error(x: torch.Tensor, y: torch.Tensor) -> float: Compute normalized L2 error between tensors x and y. l2_error = torch.norm(x - y) l2_norm_x = torch.norm(x) normalized_l2_error = l2_error / l2_norm_x return normalized_l2_error.item() def compute_cosine_similarity(x: torch.Tensor, y: torch.Tensor) -> float: Compute cosine similarity between tensors x and y. dot_product = torch.dot(x, y) norm_x = torch.norm(x) norm_y = torch.norm(y) cosine_similarity = dot_product / (norm_x * norm_y) return cosine_similarity.item()"},{"question":"You are tasked with understanding the `__future__` module in Python and creating a utility function that processes data regarding the future statements and their release versions. # Problem Statement Write a Python function called `future_features_info` that retrieves information about all features in the `__future__` module. The function should return a dictionary where: - The keys are the feature names. - The values are tuples, with each tuple containing two elements: 1. A tuple representing the optional release (as described in the documentation). 2. A tuple representing the mandatory release (or `None` if the mandatory release is set as such). Additionally, implement another function `mandatory_in_version(version: tuple)` that takes a version tuple (following the same format as the Python version info: `(major, minor, micro, release_level, serial)`) and returns a list of feature names that will become mandatory in the given version or earlier. # Function Specifications 1. `future_features_info()`: - **Output**: A dictionary formatted as described above. 2. `mandatory_in_version(version: tuple)`: - **Input**: A tuple representing the version. - **Output**: A list of feature names. # Example Usage ```python feature_info = future_features_info() print(feature_info) # Output example (partial): # { # \\"nested_scopes\\": ((2, 1, 0, \'beta\', 1), (2, 2, 0, \'final\', 0)), # ... # } mandatory_features = mandatory_in_version((3, 0, 0, \'final\', 0)) print(mandatory_features) # Output example: # [\'division\', \'print_function\', \'unicode_literals\'] ``` # Constraints 1. You may not use any additional modules for processing beyond those that are part of the Python standard library. 2. Performance should handle typical use cases efficiently, such as listing all features or checking the version for features within a reasonably small timeframe (e.g., under a second). # Additional Notes - Make sure to handle edge cases, such as features with a mandatory release of `None`. - The provided version tuple for `mandatory_in_version` function could be for any Python version, so the solution should be flexible to handle different version formats properly.","solution":"import __future__ as future_module def future_features_info(): feature_info = {} for feature_name, feature in future_module.__dict__.items(): if isinstance(feature, future_module._Feature): optional = feature.optional mandatory = feature.mandatory feature_info[feature_name] = (optional, mandatory) return feature_info def mandatory_in_version(version): features = future_features_info() mandatory_features = [] for feature, (optional, mandatory) in features.items(): if mandatory is not None and version >= mandatory: mandatory_features.append(feature) return mandatory_features # Example outputs # feature_info = future_features_info() # print(feature_info) # mandatory_features = mandatory_in_version((3, 0, 0, \'final\', 0)) # print(mandatory_features)"},{"question":"**Question**: You are tasked with designing a file backup script using the `shutil` module. The script should: - Backup a specified directory by creating a compressed archive of it. - Verify that the backup was successful by checking the size of the original directory and the archive. - Clean up by deleting the original directory after the backup is confirmed. **Implement a function `backup_and_cleanup` that accepts the following parameters**: - `source_dir`: The path to the directory to be backed up (string). - `backup_dir`: The path to the directory where the backup archive will be stored (string). - `archive_name`: The base name for the archive file (string). **Function Signature**: ```python def backup_and_cleanup(source_dir: str, backup_dir: str, archive_name: str) -> str: pass ``` **Constraints**: - Only use functions from the `shutil` module for file operations. - Assume `source_dir` exists and is a directory. - The `backup_dir` exists and is writable. **Behavior**: 1. Create a compressed archive of the `source_dir` in `backup_dir` using the name `archive_name` (with `.tar.gz` extension). 2. Compare the size of the original `source_dir` and the created archive. 3. If the archive size is greater than or equal to the original directory, delete the `source_dir`. 4. Return the path to the created archive. **Example**: ```python backup_and_cleanup(\'/path/to/source\', \'/path/to/backup\', \'my_backup\') # Suppose the directory structure is: # /path/to/source/ #  file1.txt #  file2.txt #  subdir/ #  file3.txt # The function should: # 1. Create an archive at /path/to/backup/my_backup.tar.gz. # 2. Verify the archive size. # 3. If verification is successful, delete the /path/to/source directory. # 4. Return the path \'/path/to/backup/my_backup.tar.gz\'. ``` **Hints**: - Use `shutil.make_archive` to create the archive. - Use `shutil.disk_usage` to get the size of the `source_dir`. - Use `os.path.getsize` to get the size of the archive. - Use `shutil.rmtree` to delete the original directory.","solution":"import shutil import os def get_directory_size(directory): total_size = 0 for dirpath, dirnames, filenames in os.walk(directory): for f in filenames: fp = os.path.join(dirpath, f) if os.path.exists(fp): total_size += os.path.getsize(fp) return total_size def backup_and_cleanup(source_dir, backup_dir, archive_name): # Create archive archive_path = shutil.make_archive(os.path.join(backup_dir, archive_name), \'gztar\', source_dir) # Get sizes of source_dir and backup archive source_size = get_directory_size(source_dir) archive_size = os.path.getsize(archive_path) # Compare sizes and delete source dir if the archive size is greater than or equal if archive_size >= source_size: shutil.rmtree(source_dir) return archive_path"},{"question":"Question # Context As an experienced programming instructor, you are designing a programming question to assess students\' ability to work with PyTorch, particularly focusing on creating custom extensions through subclassing `torch.autograd.Function` and `torch.nn.Module`. # Task Implement a custom autograd function and a custom neural network module in PyTorch. 1. **Custom Autograd Function**: - Create a class `ExpFunction`, subclassing `torch.autograd.Function`. - Implement the `forward` and `backward` static methods to compute the exponential of an input tensor and its gradient, respectively. 2. **Custom Neural Network Module**: - Create a class `ExpLayer`, subclassing `torch.nn.Module`. - Use the `ExpFunction` in the `forward` method of this module. - Implement an `__init__()` method to initialize the layer. # Requirements 1. **ExpFunction**: - `forward(ctx, input: torch.Tensor) -> torch.Tensor`: - Compute the exponential of the input tensor: (output = e^{input}). - Save the input tensor for the backward computation using `ctx.save_for_backward()`. - Return the output tensor. - `backward(ctx, grad_output: torch.Tensor) -> torch.Tensor`: - Retrieve the saved input tensor from `ctx.saved_tensors`. - Compute the gradient of the input using the chain rule: (grad_input = grad_output times output). - Return the gradient tensor with respect to the input. 2. **ExpLayer**: - `__init__(self)`: - Initialize the superclass. - Optionally include additional parameter initializations or configurations. - `forward(self, input: torch.Tensor) -> torch.Tensor`: - Use `ExpFunction.apply()` to apply the custom function and return its result. # Example Usage ```python import torch import torch.nn as nn from torch.autograd import Function class ExpFunction(Function): @staticmethod def forward(ctx, input): output = input.exp() # e^x ctx.save_for_backward(input) return output @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * input.exp() return grad_input class ExpLayer(nn.Module): def __init__(self): super(ExpLayer, self).__init__() def forward(self, input): return ExpFunction.apply(input) # Testing the implementation input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) exp_layer = ExpLayer() output = exp_layer(input_tensor) output.mean().backward() print(input_tensor.grad) ``` # Constraints - The input tensors will always be 1-dimensional. - Ensure the gradient calculations in `ExpFunction` are correct using `torch.autograd.gradcheck`. # Evaluation Criteria - Correct implementation of the custom autograd function (`ExpFunction`). - Proper subclassing and usage of `torch.nn.Module` in `ExpLayer`. - Accurate computation of forward and backward passes. - Clear, maintainable, and efficient code.","solution":"import torch import torch.nn as nn from torch.autograd import Function class ExpFunction(Function): @staticmethod def forward(ctx, input): output = input.exp() # e^x ctx.save_for_backward(input) return output @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * input.exp() return grad_input class ExpLayer(nn.Module): def __init__(self): super(ExpLayer, self).__init__() def forward(self, input): return ExpFunction.apply(input)"},{"question":"# Email Manipulation with the Python \\"email\\" Package Objective: You are required to create a Python function that constructs, parses, and modifies email messages using the \\"email\\" package. This function should demonstrate proficiency with the object model, parser, and generator components provided by the package. Requirements: 1. **Construct an Email**: - Create a function `create_email` that takes the following parameters: * `subject` (str): The subject of the email. * `sender` (str): The email address of the sender. * `recipient` (str): The email address of the recipient. * `body` (str): The main text content of the email. - The function should return an `EmailMessage` object representing the constructed email. 2. **Parse and Modify an Email**: - Create a function `parse_and_modify_email` that takes two parameters: * `raw_email` (str): A raw email message in serialized form (string). * `new_body` (str): A new text content to replace the existing body of the email. - The function should: 1. Parse the `raw_email` string into an `EmailMessage` object. 2. Replace the body of the email with `new_body`. 3. Return the modified email as a serialized string. Constraints: - Ensure the email creation functionality handles proper formatting and MIME type. - The parsing and modification should correctly handle different parts of a MIME-compliant email. Example: ```python from email.message import EmailMessage from email.parser import BytesParser from email.generator import BytesGenerator from email.policy import default def create_email(subject, sender, recipient, body): email = EmailMessage() email[\'Subject\'] = subject email[\'From\'] = sender email[\'To\'] = recipient email.set_content(body) return email def parse_and_modify_email(raw_email, new_body): # Parse the raw email parser = BytesParser(policy=default) email = parser.parsebytes(raw_email.encode(\'utf-8\')) # Modify the body email.set_content(new_body) # Serialize the modified email from io import BytesIO output = BytesIO() generator = BytesGenerator(output, policy=default) generator.flatten(email) return output.getvalue().decode(\'utf-8\') # Example usage if __name__ == \\"__main__\\": # Create an email email = create_email(\\"Test Subject\\", \\"sender@example.com\\", \\"recipient@example.com\\", \\"This is a test email.\\") print(email) # Serialize the email for testing parsing email_str = email.as_string() # Parse and modify the email modified_email_str = parse_and_modify_email(email_str, \\"This is the new body of the email.\\") print(modified_email_str) ``` Notes: - Ensure your implementation leverages the \\"email.message\\", \\"email.parser\\", and \\"email.generator\\" modules effectively. - Test your functions thoroughly to ensure they handle various edge cases, including different MIME types and multipart emails.","solution":"from email.message import EmailMessage from email.parser import BytesParser from email.generator import BytesGenerator from email.policy import default def create_email(subject, sender, recipient, body): Create and return an EmailMessage object. Parameters: - subject (str): The subject of the email. - sender (str): The email address of the sender. - recipient (str): The email address of the recipient. - body (str): The main text content of the email. Returns: - EmailMessage: The constructed EmailMessage object. email = EmailMessage() email[\'Subject\'] = subject email[\'From\'] = sender email[\'To\'] = recipient email.set_content(body) return email def parse_and_modify_email(raw_email, new_body): Parse a raw email string and modify its body. Parameters: - raw_email (str): A raw email message in serialized form (string). - new_body (str): A new text content to replace the existing body of the email. Returns: - str: The modified email as a serialized string. # Parse the raw email parser = BytesParser(policy=default) email = parser.parsebytes(raw_email.encode(\'utf-8\')) # Modify the body email.set_content(new_body) # Serialize the modified email from io import BytesIO output = BytesIO() generator = BytesGenerator(output, policy=default) generator.flatten(email) return output.getvalue().decode(\'utf-8\')"},{"question":"# Question You have been tasked with enhancing a Python project\'s start-up configuration functionality. To facilitate custom start-up behaviors for both system-wide and user-specific settings, you need to mimic some of the \\"site\\" module\'s functionality but with additional custom startup options. Write a Python function `custom_startup_configuration` that: 1. Checks if a file named `custom_setup.cfg` exists in the parent directory of the currently running script. 2. If the file exists, reads the entry `user_custom_directory` and appends this directory to `sys.path`. 3. Implements logic to import two optional modules: `customsite` and `usercustomsite`. - If `customsite` fails to import and raises `ImportError`, log a message indicating failure, but do not crash the program. - If `usercustomsite` fails to import due to `ImportError` and the exception name attribute equals \'usercustomsite\', silently ignore the error. Additional Requirements: - The `custom_setup.cfg` file should have a format with key-value pairs on each line, e.g., `user_custom_directory=/path/to/custom/dir`. - Only add the `user_custom_directory` to `sys.path` if it is not already present. - Create a simple logging mechanism to record errors encountered during the import of the `customsite` module, either by print statements or using Python\'s `logging` module. # Function Signature: ```python def custom_startup_configuration(): pass ``` # Example `custom_setup.cfg` file: ``` user_custom_directory=/home/user/my_custom_lib ``` # Example Usage: ```python # You should create a custom_setup.cfg file one level above the script directory with the mentioned key-value pair for testing. custom_startup_configuration() import my_custom_module # This should work if /home/user/my_custom_lib/my_custom_module.py exists and is valid ``` Constraints: - Do not use any external Python libraries other than standard libraries. - Ensure you handle exceptions appropriately to avoid the program crashing inadvertently. - The code should be Python 3.7+ compatible.","solution":"import sys import os import logging def custom_startup_configuration(): This function checks for a custom configuration file one directory above the script location, reads configuration settings, adjusts sys.path, and attempts to import optional modules \'customsite\' and \'usercustomsite\'. # Setup logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(\'CustomStartup\') # Find the directory one level above the current script\'s directory parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) config_file_path = os.path.join(parent_dir, \'custom_setup.cfg\') # Check if the custom configuration file exists if os.path.exists(config_file_path): user_custom_directory = None # Read the configuration file with open(config_file_path, \'r\') as config_file: for line in config_file: line = line.strip() if line.startswith(\'user_custom_directory\'): key, value = line.split(\'=\', 1) user_custom_directory = value.strip() break # Append the user custom directory to sys.path if it exists and is not already in sys.path if user_custom_directory and user_custom_directory not in sys.path: sys.path.append(user_custom_directory) # Try to import optional modules try: import customsite except ImportError as e: logger.info(f\\"Failed to import \'customsite\': {e}\\") try: import usercustomsite except ImportError as e: if e.name != \'usercustomsite\': logger.info(f\\"Failed to import \'usercustomsite\': {e}\\")"},{"question":"You are required to manipulate bytearray objects using Python\'s bytearray API. Your task is to implement a function that takes two string inputs, performs various operations using bytearray type, and returns the desired result as a string. # Function Signature ```python def manipulate_bytearrays(str1: str, str2: str) -> str: pass ``` # Input: - `str1` (str): The first input string. - `str2` (str): The second input string. # Output: - (str): The resultant string after performing the specified operations on the bytearrays derived from `str1` and `str2`. # Requirements: 1. **Concatenate** the bytearray equivalent of `str1` and `str2`. 2. **Resize** the resulting bytearray to the size of `str1`. 3. Return the **string representation** of the modified bytearray. # Constraints: - All strings provided as inputs will only contain printable ASCII characters. - `str1` and `str2` will have lengths less than 1000. # Example: ```python assert manipulate_bytearrays(\\"Hello\\", \\"World\\") == \\"HelloWo\\" assert manipulate_bytearrays(\\"Python\\", \\"310\\") == \\"Python31\\" assert manipulate_bytearrays(\\"Data\\", \\"Science\\") == \\"DataScie\\" ``` # Note: - Use the provided API functions for managing bytearrays. - Handle any necessary type checks using the type check functions. - Implement the function in the most efficient way possible, while ensuring clarity and correctness.","solution":"def manipulate_bytearrays(str1: str, str2: str) -> str: Concatenates the bytearray equivalent of `str1` and `str2`, resizes the resulting bytearray to the size of `str1`, and returns the string representation of the modified bytearray. # Convert strings to bytearrays ba1 = bytearray(str1, \'utf-8\') ba2 = bytearray(str2, \'utf-8\') # Concatenate the bytearrays concatenated = ba1 + ba2 # Resize the concatenated bytearray to the length of str1 concatenated = concatenated[:len(str1)] # Convert the resulting bytearray back to string and return it return concatenated.decode(\'utf-8\')"},{"question":"# Manipulating Multi-Dimensional Buffers **Objective:** Write a Python function that uses the buffer protocol to manipulate a 2-dimensional buffer (memory view) and perform operations on it. Specifically, the function should: 1. Initialize a 2D array (list of lists in Python) of integers. 2. Create a memory view from this 2D array. 3. Modify the elements of the array through the memory view by incrementing each element by 1. 4. Return the modified array. **Requirements:** 1. Use the buffer protocol to create and manipulate the memory view. 2. Ensure modifications via the buffer are reflected in the original array. 3. You are not allowed to use numpy or any other third-party libraries which abstract away buffer manipulations; you must use Python\'s buffer protocol and possibly built-in standard library functions. **Function Signature:** ```python def manipulate_2d_buffer(array: list[list[int]]) -> list[list[int]]: pass ``` **Input:** - `array`: A 2D list of integers with dimensions `m x n` (1 <= m, n <= 100). **Output:** - Returns the modified 2D list of integers with each element incremented by 1. **Example:** ```python input_array = [[1, 2, 3], [4, 5, 6]] output_array = manipulate_2d_buffer(input_array) print(output_array) # Expected output: [[2, 3, 4], [5, 6, 7]] ``` **Constraints:** - The size of the array will not exceed 100x100. - All elements in the array will be positive integers. **Hint:** - You may find the built-in `memoryview` function in Python helpful. - Remember to handle the difference between the row-major order of lists in Python and the way C handles multi-dimensional arrays. **Evaluation Criteria:** - Correctness: The function should correctly manipulate the array as described. - Efficiency: The solution should efficiently handle the operations without unnecessary overhead. - Use of the buffer protocol: Direct manipulation using the buffer protocol is necessary for full marks.","solution":"def manipulate_2d_buffer(array: list[list[int]]) -> list[list[int]]: This function takes a 2D array of integers, manipulates its elements using a memoryview by incrementing each element by 1, and returns the modified 2D array. rows = len(array) if rows == 0: return array # Return empty array if input is empty cols = len(array[0]) # Flatten the 2D array into a 1D list for creating a contiguous memory view flat_array = [elem for row in array for elem in row] # Create a memoryview of the flattened array flat_memoryview = memoryview(bytearray(flat_array)) # Increment each element in the flattened memoryview by 1 for i in range(len(flat_memoryview)): flat_memoryview[i] += 1 # Reconstruct the 2D array from the flattened list modified_array = [ [flat_memoryview[j * cols + i] for i in range(cols)] for j in range(rows) ] return modified_array"},{"question":"In this exercise, you will demonstrate your understanding of the `signal` module by implementing a feature that handles timeouts for potentially long-running operations and properly manages different signals for a multi-threaded program. # Requirements 1. **Operation Timeout**: Implement a Python function `timed_operation` that attempts to execute a given operation within a specified timeout duration. If the operation exceeds the timeout, it should raise a `TimeoutError`. 2. **Signal Handling**: - Create custom signal handlers for `SIGINT` (Ctrl+C), `SIGALRM` (timer alarm), and any other signal. - Ensure that `SIGINT` gracefully interrupts the program without leaving it in an unexpected state. - Ensure that `SIGALRM` is used to implement the timeout mechanism. 3. **Thread Safety**: - Ensure that signals are handled correctly even if they are received in different threads. - Implement a mechanism to communicate between threads using signals. # Function Signatures ```python import signal from typing import Callable, Any def handler_sigint(signum, frame): # Write custom handler for SIGINT pass def handler_sigalrm(signum, frame): # Write custom handler for SIGALRM pass def handler_other(signum, frame): # Write a custom handler for another signal of your choice. pass def timed_operation(operation: Callable[[], Any], timeout_seconds: int) -> Any: Executes the given operation within the specified timeout. Parameters: operation (Callable[[], Any]): The operation to execute. timeout_seconds (int): The maximum time (in seconds) the operation is allowed to run. Returns: Any: The result of the operation if it completes within the timeout. Raises: TimeoutError: If the operation does not complete within the timeout duration. pass ``` # Constraints 1. Use the `signal` module to set the custom signal handlers. 2. Only the main thread may set new signal handlers. 3. Remember to reset any signals by the end of the operation to avoid side effects. 4. Ensure your solution handles multiple signal types and demonstrates thread safety. 5. You may use threading or multiprocessing as part of your implementation. # Example Usage ```python def long_running_operation(): import time time.sleep(10) return \\"Completed!\\" try: result = timed_operation(long_running_operation, 5) print(result) except TimeoutError: print(\\"The operation timed out.\\") ``` Expected output: ``` The operation timed out. ``` # Performance Requirements 1. The function `timed_operation` should have a minimal overhead and should effectively interrupt the long-running operation as soon as the timeout occurs. 2. Signal handling and inter-thread communication should be efficient and should not cause significant delays in execution. # Additional Notes - Consider edge cases such as setting very short timeouts or handling multiple signals at once. - Ensure that resource cleanup is handled appropriately, especially if the operation is interrupted. - Use comments to explain the intricacies of your signal handling logic.","solution":"import signal from typing import Callable, Any class TimeoutError(Exception): Custom Timeout Exception. pass # Custom handler for SIGINT def handler_sigint(signum, frame): print(\\"SIGINT received. Gracefully shutting down.\\") raise KeyboardInterrupt # Custom handler for SIGALRM def handler_sigalrm(signum, frame): print(\\"SIGALRM received. Operation timed out.\\") raise TimeoutError(\\"Operation timed out.\\") # Custom handler for another signal, for example, SIGUSR1 def handler_other(signum, frame): print(f\\"Signal {signum} received (custom handler).\\") def timed_operation(operation: Callable[[], Any], timeout_seconds: int) -> Any: Executes the given operation within the specified timeout. Parameters: operation (Callable[[], Any]): The operation to execute. timeout_seconds (int): The maximum time (in seconds) the operation is allowed to run. Returns: Any: The result of the operation if it completes within the timeout. Raises: TimeoutError: If the operation does not complete within the timeout duration. # Backup existing signal handlers previous_sigint_handler = signal.getsignal(signal.SIGINT) previous_sigalrm_handler = signal.getsignal(signal.SIGALRM) try: # Set the custom signal handlers signal.signal(signal.SIGINT, handler_sigint) signal.signal(signal.SIGALRM, handler_sigalrm) signal.signal(signal.SIGUSR1, handler_other) # Schedule the alarm signal.alarm(timeout_seconds) try: # Execute the operation result = operation() finally: # Disable the alarm signal.alarm(0) return result except TimeoutError as e: raise e finally: # Restore the previous signal handlers signal.signal(signal.SIGINT, previous_sigint_handler) signal.signal(signal.SIGALRM, previous_sigalrm_handler) signal.signal(signal.SIGUSR1, signal.SIG_DFL) # Reset to default handler"},{"question":"**Nearest Neighbors Classification with Custom Distance Metric** **Objective:** To assess your understanding and implementation of nearest neighbors classification using scikit-learn, including the ability to customize distance metrics and evaluate model performance. **Problem Statement:** Implement a custom nearest neighbors classification model using scikit-learn\'s `KNeighborsClassifier` with the following specifications: 1. **Custom Distance Metric**: - Define a custom distance metric function that combines the Euclidean distance (`L2`) and Manhattan distance (`L1`). - The combined distance metric should be a weighted sum of the Euclidean and Manhattan distances. The weights will be provided as inputs to the function. 2. **Classification Model**: - Use the custom distance metric to classify a dataset. - Evaluate the model\'s performance using accuracy. 3. **Dataset**: - Use the Iris dataset available in scikit-learn. **Tasks:** 1. **Custom Distance Metric Function**: Implement a function `custom_distance_metric` that takes two points and weight parameters `w_euclidean` and `w_manhattan` and returns the combined distance. ```python def custom_distance_metric(x, y, w_euclidean=0.5, w_manhattan=0.5): Custom distance metric combining Euclidean and Manhattan distances. Parameters: - x: ndarray, shape (n_features,) - y: ndarray, shape (n_features,) - w_euclidean: float, weight of the Euclidean distance - w_manhattan: float, weight of the Manhattan distance Returns: - distance: float, the combined distance # TODO: Implement the custom distance metric pass ``` 2. **Classification Model**: - Use the Iris dataset. - Split the dataset into training and testing sets (70-30 split). - Instantiate the `KNeighborsClassifier` with `n_neighbors=5` and the custom distance metric. - Train the model on the training set and evaluate its accuracy on the test set. ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Instantiate and train the KNeighborsClassifier # TODO: Implement the classifier with the custom distance metric knn = KNeighborsClassifier(n_neighbors=5, metric=custom_distance_metric, metric_params={\'w_euclidean\': 0.6, \'w_manhattan\': 0.4}) knn.fit(X_train, y_train) # Evaluate the model accuracy = knn.score(X_test, y_test) print(f\'Accuracy: {accuracy:.2f}\') ``` **Constraints:** - Use scikit-learn\'s `KNeighborsClassifier`. - Ensure the custom distance metric works correctly within the scikit-learn framework. **Additional Information:** - You can refer to scikit-learn\'s documentation for details on implementing custom distance metrics. - Ensure the custom distance function is compatible with the `metric` parameter of `KNeighborsClassifier`. **Expected Output:** - Accuracy of the KNeighborsClassifier on the test set using the custom distance metric. # Example Output ``` Accuracy: 0.98 ```","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier def custom_distance_metric(x, y, w_euclidean=0.5, w_manhattan=0.5): Custom distance metric combining Euclidean and Manhattan distances. Parameters: - x: ndarray, shape (n_features,) - y: ndarray, shape (n_features,) - w_euclidean: float, weight of the Euclidean distance - w_manhattan: float, weight of the Manhattan distance Returns: - distance: float, the combined distance euclidean_distance = np.sqrt(np.sum((x - y) ** 2)) manhattan_distance = np.sum(np.abs(x - y)) combined_distance = (w_euclidean * euclidean_distance) + (w_manhattan * manhattan_distance) return combined_distance # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Instantiate and train the KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=5, metric=custom_distance_metric, metric_params={\'w_euclidean\': 0.6, \'w_manhattan\': 0.4}) knn.fit(X_train, y_train) # Evaluate the model accuracy = knn.score(X_test, y_test) print(f\'Accuracy: {accuracy:.2f}\')"},{"question":"You are required to implement a class that uses scikit-learns `neighbors.LocalOutlierFactor` for novelty detection. Your class should perform the following tasks: 1. **Initialization**: Create an instance of the `LocalOutlierFactor` with `novelty=True`. 2. **Training**: Fit the model on the training data. 3. **Prediction**: Predict whether new samples are novelties or part of the normal distribution. 4. **Evaluation**: Return a score for the likelihood of each new sample being a novelty. # Class Definition Define a class `CustomLoF` with the following methods: 1. **`__init__(self, n_neighbors=20, contamination=0.1)`**: - Initializes the `LocalOutlierFactor` with `n_neighbors` and `contamination` parameters. 2. **`fit(self, X_train)`**: - Fits the `LocalOutlierFactor` model on the training data `X_train`. 3. **`predict(self, X_test)`**: - Predicts whether the instances in `X_test` are novelties (outliers) or inliers. - Returns a list of predictions where 1 means inlier and -1 means outlier. 4. **`score_samples(self, X_test)`**: - Returns an array of scores, where higher values indicate a higher likelihood of being inliers. # Input and Output - **Initialization**: - `n_neighbors`: Integer, the number of neighbors to use for LOF. - `contamination`: Float, the proportion of outliers in the data set. - **`fit` method**: - `X_train`: A 2D numpy array or list of lists with shape (n_samples, n_features), representing the training data. - Does not return anything. - **`predict` method**: - `X_test`: A 2D numpy array or list of lists with shape (n_samples, n_features), representing the test data. - Returns: A list of integers with values 1 (inlier) or -1 (outlier) for each sample in `X_test`. - **`score_samples` method**: - `X_test`: A 2D numpy array or list of lists with shape (n_samples, n_features), representing the test data. - Returns: An array of scores indicating the likelihood of each sample being an inlier. # Constraints - You must use scikit-learns `neighbors.LocalOutlierFactor` class with the `novelty=True` parameter. - Do not use the `fit_predict` method since it is not available in the context of novelty detection. # Example ```python from sklearn.datasets import make_blobs # Create synthetic data X_train, _ = make_blobs(n_samples=100, centers=1, cluster_std=0.5, random_state=0) X_test, _ = make_blobs(n_samples=10, centers=1, cluster_std=0.5, random_state=42) # Initialize and train the CustomLoF model model = CustomLoF(n_neighbors=20, contamination=0.1) model.fit(X_train) # Predict novelties predictions = model.predict(X_test) print(predictions) # Outputs: [1, -1, 1, ...] # Score samples scores = model.score_samples(X_test) print(scores) # Outputs: [0.25, -1.5, 0.35, ...] ``` Implement the `CustomLoF` class accordingly.","solution":"from sklearn.neighbors import LocalOutlierFactor class CustomLoF: def __init__(self, n_neighbors=20, contamination=0.1): Initializes the LocalOutlierFactor model for novelty detection. self.model = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, novelty=True) def fit(self, X_train): Fits the LOF model on the training data. self.model.fit(X_train) def predict(self, X_test): Predicts whether the instances in X_test are novelties (outliers) or inliers. Returns a list of predictions where 1 means inlier and -1 means outlier. return self.model.predict(X_test).tolist() def score_samples(self, X_test): Returns an array of scores indicating the likelihood of each sample being an inlier. Higher scores indicate a higher likelihood of being an inlier. return self.model.decision_function(X_test)"},{"question":"Advanced Data Visualization with Seaborn Objective Demonstrate your understanding of Seaborn by creating a detailed and customized plot. You will work with the `penguins` dataset available in Seaborn and use advanced plotting and customization techniques. Task 1. Load the `penguins` dataset using `seaborn.load_dataset`. 2. Create a scatter plot of `flipper_length_mm` vs `body_mass_g` where: - Points are colored by `species`. - Points are sized by `bill_length_mm`. 3. Customize the plot as follows: - Add a descriptive title and axis labels. - Use different markers for each species. - Add a legend that clearly indicates the species and the scaling of points by `bill_length_mm`. 4. Save the plot to a file named `penguins_plot.png`. Input - The `penguins` dataset from Seaborn\'s `load_dataset`. Output - A PNG file named `penguins_plot.png` containing the described scatter plot. Constraints - You must use Seaborn\'s functionalities along with Matplotlib for any additional customizations. - Ensure the plot is clear and aesthetically pleasing. Example ```python import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the scatter plot plot = sns.scatterplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", size=\\"bill_length_mm\\", markers=[\\"o\\", \\"s\\", \\"D\\"], alpha=0.7, palette=\\"deep\\" ) # Customize the plot with title, axis labels, and legend plot.set_title(\\"Penguin Flipper Length vs Body Mass\\") plot.set_xlabel(\\"Flipper Length (mm)\\") plot.set_ylabel(\\"Body Mass (g)\\") plt.legend(title=\\"Species and Bill Length (mm)\\") # Save the plot to a file plt.savefig(\\"penguins_plot.png\\") ``` Note that this example is presented for structure guidance. Your task is to complete the details and ensure all customizations are correctly implemented.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguins_plot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Checking for NaN and cleaning the data for missing values penguins = penguins.dropna(subset=[\\"flipper_length_mm\\", \\"body_mass_g\\", \\"bill_length_mm\\", \\"species\\"]) # Create the scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", size=\\"bill_length_mm\\", palette=\\"deep\\", sizes=(20, 200), alpha=0.7, markers=[\\"o\\", \\"s\\", \\"D\\"] ) # Customize the plot with title, axis labels, and legend scatter_plot.set_title(\\"Penguin Flipper Length vs Body Mass with Bill Length as size\\") scatter_plot.set_xlabel(\\"Flipper Length (mm)\\") scatter_plot.set_ylabel(\\"Body Mass (g)\\") handles, labels = scatter_plot.get_legend_handles_labels() scatter_plot.legend(handles=handles[:4], labels=labels[:4], title=\\"Species\\", loc=\\"upper left\\") plt.legend(loc=\\"center left\\", bbox_to_anchor=(1, 0.5), title=\\"Species and Bill Length (mm)\\") # Save the plot to a file plt.savefig(\\"penguins_plot.png\\") plt.close() create_penguins_plot()"},{"question":"**Coding Assessment Question:** You are required to implement a custom representation generator using the `reprlib` module in Python. Specifically, you will create a subclass of `reprlib.Repr` that provides customized representations for dictionary objects (`dict`). Your task is to limit the dictionary representation such that only the first 3 key-value pairs are displayed, and any further pairs should be replaced with a `\\"...\\"` placeholder. # Task: 1. Subclass the `reprlib.Repr` class to create a new class called `CustomRepr`. 2. Override the `repr_dict` method in this subclass to implement the custom representation logic for dictionaries. 3. Create an instance of `CustomRepr` named `custom_repr`. 4. Implement a function `limited_repr(obj)` that takes an object as input and returns its representation using the `custom_repr` instance. # Constraints: - The dictionary may contain complex nested structures including other dictionaries, lists, etc. - The custom representation should preserve the data type indications, and the size limitation should apply recursively to inner dictionaries as well. # Implementation Details: - You can use `reprlib.repr1(obj, level)` recursively while decreasing the level on each call to handle nested structures. - Ensure that the custom representation limits the displayed key-value pairs correctly and includes the `\\"...\\"` placeholder when necessary. - You may assume that dictionaries will not have more than 100 key-value pairs for the purpose of this assessment. # Example: ```python import reprlib class CustomRepr(reprlib.Repr): def repr_dict(self, obj, level): if level <= 0: return \'{...}\' new_dict = list(obj.items())[:3] limited_items = \', \'.join(f\'{reprlib.repr(k)}: {self.repr1(v, level - 1)}\' for k, v in new_dict) if len(obj) > 3: limited_items += \', ...\' return f\'{{{limited_items}}}\' repr_instance = CustomRepr() custom_repr = repr_instance.repr def limited_repr(obj): return custom_repr(obj) # Example Usage: example_dict = {i: i*i for i in range(10)} print(limited_repr(example_dict)) # Output: # {0: 0, 1: 1, 2: 4, ...} ``` # Notes: - Ensure your implementation adheres to the constraints and correctly reflects the intended outcomes. - You may use other attributes and methods from `reprlib` if necessary to complete this task.","solution":"import reprlib class CustomRepr(reprlib.Repr): def repr_dict(self, obj, level): if level <= 0: return \'{...}\' new_dict = list(obj.items())[:3] limited_items = \', \'.join(f\'{reprlib.repr(k)}: {self.repr1(v, level - 1)}\' for k, v in new_dict) if len(obj) > 3: limited_items += \', ...\' return f\'{{{limited_items}}}\' repr_instance = CustomRepr() custom_repr = repr_instance.repr def limited_repr(obj): return custom_repr(obj) # Example Usage: example_dict = {i: i*i for i in range(10)} print(limited_repr(example_dict)) # Output: {0: 0, 1: 1, 2: 4, ...}"},{"question":"**Objective:** Understand and apply Python\'s buffer protocol (conceptually similar to working with low-level memory operations in C) by manipulating buffer-like structures using Python. **Question:** Create a Python class `BufferManager` that simulates some aspects of the buffer protocol exposed by Python. Your class should be able to: 1. Initialize a contiguous buffer with a given size and default value. 2. Provide a method to get a reference to a portion of the buffer based on start index and length. 3. Allow modification of the buffer and validate contiguity using simple strides. 4. Verify the integrity of the buffer\'s shape and strides. 5. Provide functionality to export the buffer into a raw byte format suitable for file operations. **Requirements:** 1. **Initialization:** ```python def __init__(self, size: int, default_value: int): :param size: Total size of the buffer. :param default_value: Default value to initialize each byte in the buffer. ``` 2. **Get Buffer Portion:** ```python def get_buffer_section(self, start: int, length: int) -> memoryview: :param start: Starting index of the section. :param length: Length of the section. :return: Memory view of the requested buffer section. ``` 3. **Modify Buffer:** ```python def modify_buffer(self, index: int, value: int): :param index: Position in the buffer to modify. :param value: New value to set at the given index. ``` 4. **Validate Contiguity:** ```python def is_contiguous(self, stride: int) -> bool: :param stride: Stride value to check contiguity. :return: True if buffer adheres to the given stride, False otherwise. ``` 5. **Verify Structure:** ```python def verify_structure(self, ndim: int, shape: list, strides: list) -> bool: :param ndim: Number of dimensions. :param shape: Shape of the buffer as a list. :param strides: Strides of the buffer as a list. :return: True if the parameters represent a valid buffer structure, False otherwise. ``` 6. **Export Buffer:** ```python def export_to_bytes(self) -> bytes: :return: Raw bytes representation of the buffer. ``` **Example Usage:** ```python buffer = BufferManager(10, 0) print(buffer.export_to_bytes()) # Should print 10 bytes of zeros. buffer.modify_buffer(5, 255) section = buffer.get_buffer_section(3, 5) print(section) # Should show a memory view that includes modified value. print(buffer.is_contiguous(1)) # Should return True since buffer is contiguous. print(buffer.verify_structure(1, [10], [1])) # Should return True for a 1D buffer. ``` **Constraints:** - Initialize the buffer with a maximum size of `10^6` bytes. - Ensure modifications are within buffer limits. - Your methods should handle errors gracefully and raise appropriate exceptions. **Performance:** Ensure your implementation is efficient in terms of both time and space complexity. Avoid unnecessary copies of buffer data.","solution":"class BufferManager: def __init__(self, size: int, default_value: int): if size <= 0 or size > 10**6: raise ValueError(f\\"Size must be between 1 and 10^6 inclusive.\\") if not (0 <= default_value <= 255): raise ValueError(\\"Default value must be between 0 and 255 inclusive.\\") self.buffer = bytearray([default_value] * size) self.size = size def get_buffer_section(self, start: int, length: int) -> memoryview: if start < 0 or length < 0 or start + length > self.size: raise ValueError(\\"Invalid start or length for buffer section.\\") return memoryview(self.buffer)[start:start + length] def modify_buffer(self, index: int, value: int): if index < 0 or index >= self.size: raise IndexError(\\"Index out of buffer bounds.\\") if not (0 <= value <= 255): raise ValueError(\\"Value must be between 0 and 255 inclusive.\\") self.buffer[index] = value def is_contiguous(self, stride: int) -> bool: This method checks if the buffer is contiguous in the context of the provided stride. For simplification in a 1D buffer, the stride must be 1. return stride == 1 def verify_structure(self, ndim: int, shape: list, strides: list) -> bool: if ndim != 1 or len(shape) != 1 or len(strides) != 1: return False if shape[0] != self.size or strides[0] != 1: return False return True def export_to_bytes(self) -> bytes: return bytes(self.buffer)"},{"question":"Building and Validating a Custom WSGI Application **Objective**: Implement a custom WSGI application for serving a basic web page, validate it using `wsgiref.validate`, and set up a simple WSGI server to run the application. **Task**: 1. Create a function named `my_wsgi_app` that serves a simple HTML page. - The function should accept two arguments: `environ` (a dictionary containing CGI-style environment variables) and `start_response` (a callable for starting the HTTP response). - The application should respond with a simple HTML document. The content should be: ```html <!DOCTYPE html> <html lang=\\"en\\"> <head> <meta charset=\\"UTF-8\\"> <title>My WSGI App</title> </head> <body> <h1>Hello, World from My WSGI App!</h1> </body> </html> ``` - The function should set the `Content-Type` header to `text/html; charset=utf-8` and return the above HTML document in the response body. 2. Validate the `my_wsgi_app` function using `wsgiref.validate.validator`. - Wrap the `my_wsgi_app` function using `wsgiref.validate.validator` to catch any nonconformance to the WSGI specification. 3. Set up a simple WSGI server to run the validated application on port 8080. - Use `wsgiref.simple_server.make_server` to create a server instance that serves the validated application. - Ensure the server runs and correctly serves the HTML page when accessed via a web browser or a tool like `curl`. **Requirements**: - Implement the `my_wsgi_app` function. - Validate the application using `wsgiref.validate.validator`. - Set up and start the server using `wsgiref.simple_server.make_server`. ```python from wsgiref.simple_server import make_server from wsgiref.validate import validator # Step 1: Implement the WSGI application def my_wsgi_app(environ, start_response): # Your code to implement the application status = \'200 OK\' headers = [(\'Content-type\', \'text/html; charset=utf-8\')] start_response(status, headers) response_body = b\'\'\'<!DOCTYPE html> <html lang=\\"en\\"> <head> <meta charset=\\"UTF-8\\"> <title>My WSGI App</title> </head> <body> <h1>Hello, World from My WSGI App!</h1> </body> </html>\'\'\' return [response_body] # Step 2: Validate the application validated_app = validator(my_wsgi_app) # Step 3: Set up and start the WSGI server if __name__ == \'__main__\': with make_server(\'\', 8080, validated_app) as httpd: print(\\"Serving on port 8080...\\") httpd.serve_forever() ``` **Constraints**: - The `my_wsgi_app` function must adhere to the WSGI specification. - The server should handle multiple requests until manually stopped. **Expected Output**: - When the server is running, accessing `http://localhost:8080` should display the HTML document served by `my_wsgi_app`.","solution":"from wsgiref.simple_server import make_server from wsgiref.validate import validator def my_wsgi_app(environ, start_response): A basic WSGI application that returns a simple HTML page. status = \'200 OK\' headers = [(\'Content-type\', \'text/html; charset=utf-8\')] start_response(status, headers) response_body = b\'\'\'<!DOCTYPE html> <html lang=\\"en\\"> <head> <meta charset=\\"UTF-8\\"> <title>My WSGI App</title> </head> <body> <h1>Hello, World from My WSGI App!</h1> </body> </html>\'\'\' return [response_body] # Validate the WSGI application validated_app = validator(my_wsgi_app) # Set up and start the WSGI server if __name__ == \'__main__\': with make_server(\'\', 8080, validated_app) as httpd: print(\\"Serving on port 8080...\\") httpd.serve_forever()"},{"question":"# Complex Plot Layout with Seaborn Objective: You are required to demonstrate your comprehension of seaborn\'s advanced plotting layout capabilities by creating a complex plot. This question will focus on using features such as subplot creation, layout adjustments, and controlling the overall dimensions and extent of the figure. Task: 1. Import seaborn objects using `import seaborn.objects as so`. 2. Create a base plot and adjust its size to be 6 x 6 inches. 3. Create a 2x2 grid of subplots (facet) with labels `[\\"A\\", \\"B\\"]` on the rows and `[\\"X\\", \\"Y\\"]` on the columns. 4. Use a different layout engine (constrained) to optimize the space used by the subplots. 5. Adjust the extent of the plots within the overall figure to take up only 90% of the figure width and the full figure height. Expected Solution: 1. You should use `seaborn` and `matplotlib.pyplot` to construct your plots. 2. Ensure all subplots are correctly labeled and the entire figure utilizes specified dimensions and layout features. 3. Generate the final plot without errors. Input: There is no external input; you will generate the data or plot as described. Output: The output should be a single figure that contains a 2x2 grid of subplots, with appropriate sizes and layout adjustments as specified. Constraints: - You can use any standard dataset or create your own for faceting. Performance Requirements: - The generated figure should be efficiently plotted and should not exceed a reasonable amount of time and resources. Example: ```python # Example skeleton code to illustrate the structure. # Note: You need to fill in the details as per the task requirements. import seaborn.objects as so # Create base plot with specified size p = so.Plot().layout(size=(6, 6)) # Create 2x2 grid of subplots with labels [\\"A\\", \\"B\\"] on rows and [\\"X\\", \\"Y\\"] on columns p = p.facet([\\"A\\", \\"B\\"], [\\"X\\", \\"Y\\"]) # Use constrained layout engine for better results p = p.layout(engine=\\"constrained\\") # Adjust the extent of plots within the figure p = p.layout(extent=[0, 0, .9, 1]) # Display the final plot p.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_complex_plot(): # Loading example dataset df = sns.load_dataset(\'penguins\') # Initialize the figure with constrained layout and specified size fig, axes = plt.subplots(2, 2, figsize=(6, 6), constrained_layout=True) # Creating subplots for different species and islands sns.scatterplot(ax=axes[0, 0], data=df[df[\'species\'] == \'Adelie\'], x=\'bill_length_mm\', y=\'bill_depth_mm\') axes[0, 0].set_title(\'Adelie\', fontsize=10) axes[0, 0].set_ylabel(\'Bill Depth (mm)\') sns.scatterplot(ax=axes[0, 1], data=df[df[\'species\'] == \'Chinstrap\'], x=\'bill_length_mm\', y=\'bill_depth_mm\') axes[0, 1].set_title(\'Chinstrap\', fontsize=10) sns.scatterplot(ax=axes[1, 0], data=df[df[\'species\'] == \'Gentoo\'], x=\'bill_length_mm\', y=\'bill_depth_mm\') axes[1, 0].set_title(\'Gentoo\', fontsize=10) axes[1, 0].set_ylabel(\'Bill Depth (mm)\') axes[1, 0].set_xlabel(\'Bill Length (mm)\') df_non_gentoo = df[df[\'species\'] != \'Gentoo\'] sns.scatterplot(ax=axes[1, 1], data=df_non_gentoo, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\') axes[1, 1].set_title(\'Non-Gentoo\', fontsize=10) axes[1, 1].set_xlabel(\'Bill Length (mm)\') # Adjusting the overall figure for 90% width and full height usage fig.subplots_adjust(left=0, right=0.9, top=1, bottom=0) # Display the plot plt.show() create_complex_plot()"},{"question":"**Objective**: To assess the proficiency in using the `seaborn` library for creating advanced visualizations with the `relplot` function, handling both scatter and line plots, and customizing them through various parameters. Question: You are given a dataset on Titanic passengers with the following columns: - `age`: Age of the passengers. - `fare`: Fare paid by the passengers. - `sex`: Gender of the passengers. - `class`: Class of the ticket. - `sibsp`: Number of siblings/spouses aboard the Titanic. - `parch`: Number of parents/children aboard the Titanic. - `survived`: Survival status (0 = No, 1 = Yes). - `embarked`: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton). 1. **Scatter Plot**: - Create a scatter plot showing the relationship between `age` and `fare` for the Titanic passengers. - Color the points based on the `class` of the ticket. - Facet the plot into multiple columns based on the `sex` of the passengers. 2. **Line Plot**: - Create a line plot to show the mean `fare` over the `age` of passengers, aggregated by the survival status (`survived`). - Facet this plot by the port of embarkation (`embarked`). **Constraints:** - Use the `seaborn.relplot` function for both plots. - Both plots should include appropriate axis labels and titles. **Expected Output**: - The first scatter plot should visualize relationships effectively across different ticket classes and gender. - The second line plot should provide clear insights into fare trends by age, split by survival status and port of embarkation. Input Example: Data will be loaded using `sns.load_dataset(\\"titanic\\")`. Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt def titanic_visualizations(): titanic = sns.load_dataset(\\"titanic\\") # Scatter plot scatter_plot = sns.relplot( data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"class\\", col=\\"sex\\", kind=\\"scatter\\" ) scatter_plot.set_axis_labels(\\"Age\\", \\"Fare\\") scatter_plot.set_titles(\\"{col_name} passengers\\") plt.show() # Line plot line_plot = sns.relplot( data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"survived\\", col=\\"embarked\\", kind=\\"line\\", ci=None ) line_plot.set_axis_labels(\\"Age\\", \\"Average Fare\\") line_plot.set_titles(\\"Port: {col_name}\\") plt.show() # Call the function to display the plots titanic_visualizations() ``` **Note**: Students are expected to follow the function signature and ensure the plots render appropriately with the specified customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def titanic_visualizations(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Scatter plot scatter_plot = sns.relplot( data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"class\\", col=\\"sex\\", kind=\\"scatter\\" ) scatter_plot.set_axis_labels(\\"Age\\", \\"Fare\\") scatter_plot.set_titles(\\"{col_name} passengers\\") scatter_plot.fig.suptitle(\'Scatter Plot of Fare vs Age by Class and Gender\', y=1.03) plt.show() # Line plot grouped_data = titanic.groupby([\'age\', \'survived\', \'embarked\'])[\'fare\'].mean().reset_index() line_plot = sns.relplot( data=grouped_data, x=\\"age\\", y=\\"fare\\", hue=\\"survived\\", col=\\"embarked\\", kind=\\"line\\" ) line_plot.set_axis_labels(\\"Age\\", \\"Average Fare\\") line_plot.set_titles(\\"Port: {col_name}\\") line_plot.fig.suptitle(\'Line Plot of Average Fare by Age, Survival Status, and Embarkation Port\', y=1.03) plt.show()"},{"question":"# Python Coding Assessment Question Problem Description You are given a set of text documents, and your task is to identify and extract structured data from these documents using Python\'s text processing capabilities. Specifically, you are required to extract all email addresses and phone numbers from the documents and normalize the phone numbers to a standard format. Requirements 1. **Email Extraction**: - Use regular expressions (`re` module) to find all email addresses within the text. - An email address follows the pattern: `[alphanumeric characters]@[domain].[top-level domain]`. 2. **Phone Number Extraction and Normalization**: - Use regular expressions to find all phone numbers within the text. - Phone numbers may appear in various formats, including but not limited to: - `123-456-7890` - `(123) 456-7890` - `123.456.7890` - `+1-123-456-7890` - Normalize these phone numbers to the standard format: `123-456-7890`. 3. **Comparison of Document Similarity**: - Use `difflib` to compare the text documents and return a similarity score between 0 and 1. - Higher scores indicate a higher similarity between the documents. # Input and Output Formats - **Input**: - A list of strings, each string representing a text document. - Example: ```python documents = [ \\"Contact us at info@example.com or call (123) 456-7890.\\", \\"Send an email to support@example.com or ring 123.456.7890 for help.\\", \\"For more info, email sales@example.org or dial +1-123-456-7890.\\" ] ``` - **Output**: - A dictionary with the following structure: ```python { \\"emails\\": [\\"info@example.com\\", \\"support@example.com\\", \\"sales@example.org\\"], \\"phone_numbers\\": [\\"123-456-7890\\", \\"123-456-7890\\", \\"123-456-7890\\"], \\"similarity_scores\\": { \\"doc1_vs_doc2\\": 0.5, \\"doc1_vs_doc3\\": 0.4, \\"doc2_vs_doc3\\": 0.6 } } ``` # Constraints - You must use the `re` module for regular expressions. - Assume a maximum of 10,000 documents, each with a maximum length of 1,000 characters. - Performance should be optimized to handle the upper constraints efficiently. # Function Signature ```python def process_documents(documents: List[str]) -> Dict[str, Any]: pass ``` # Example ```python documents = [ \\"Contact us at info@example.com or call (123) 456-7890.\\", \\"Send an email to support@example.com or ring 123.456.7890 for help.\\", \\"For more info, email sales@example.org or dial +1-123-456-7890.\\" ] result = process_documents(documents) print(result) # Output should match the format described in the \\"Output\\" section. ```","solution":"import re import difflib from typing import List, Dict, Any def extract_emails(text: str) -> List[str]: email_pattern = r\'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+\' return re.findall(email_pattern, text) def extract_and_normalize_phones(text: str) -> List[str]: phone_pattern = r\'(+1[-.s]?)?((?d{3})?[-.s]?d{3}[-.s]?d{4})\' matches = re.findall(phone_pattern, text) normalized_phones = [] for match in matches: phone = match[1] phone = re.sub(r\'[ ().-+]\', \'\', phone) normalized_phones.append(f\\"{phone[:3]}-{phone[3:6]}-{phone[6:]}\\") return normalized_phones def compare_documents(doc1: str, doc2: str) -> float: return difflib.SequenceMatcher(None, doc1, doc2).ratio() def process_documents(documents: List[str]) -> Dict[str, Any]: all_emails = [] all_phones = [] similarity_scores = {} # Extract emails and phone numbers for doc in documents: all_emails.extend(extract_emails(doc)) all_phones.extend(extract_and_normalize_phones(doc)) # Remove duplicates all_emails = list(set(all_emails)) all_phones = list(set(all_phones)) # Calculate similarity scores for i in range(len(documents)): for j in range(i + 1, len(documents)): score = compare_documents(documents[i], documents[j]) similarity_scores[f\\"doc{i+1}_vs_doc{j+1}\\"] = score return { \\"emails\\": all_emails, \\"phone_numbers\\": all_phones, \\"similarity_scores\\": similarity_scores }"},{"question":"Objective: Implement a function that takes a `pandas.DataFrame` with a `DatetimeIndex` and performs various operations to clean, analyze, and transform the data. Requirements: 1. The DataFrame will have a `DatetimeIndex` and at least three columns: `values`, `category`, and `flag`. 2. The `values` column contains numerical data with possible missing values. 3. The `category` column contains categorical data. 4. The `flag` column contains boolean values. Operations to Implement: 1. **Handling Missing Values:** Fill missing values in the `values` column with the mean of the non-missing values in the same column. 2. **Basic Computations:** Return the min, max, and mean of the `values` column after filling missing values. 3. **Category Analysis:** Return the count of unique categories in the `category` column. 4. **Flag Analysis:** Return the proportion of `True` values in the `flag` column. 5. **Conversion and Filtering:** Convert the `DatetimeIndex` to a column and reset the index. Filter the DataFrame to include only rows where the `flag` column is `True`. 6. **Sorting:** Sort the DataFrame by the `values` column in ascending order. 7. **Return Output:** - A tuple containing the min, max, and mean values of the `values` column. - The count of unique categories. - The proportion of `True` values in the `flag` column. - The sorted and filtered DataFrame. Function Signature: ```python import pandas as pd def analyze_dataframe(df: pd.DataFrame) -> tuple: Analyzes the input DataFrame and performs multiple operations. Args: df (pd.DataFrame): Input DataFrame with a DatetimeIndex and columns [\'values\', \'category\', \'flag\']. Returns: tuple: A tuple containing: - The min, max, and mean of the \'values\' column after filling missing values. - The count of unique categories in the \'category\' column. - The proportion of \'True\' values in the \'flag\' column. - The sorted and filtered DataFrame. # Implement the function here pass ``` Example Usage: ```python data = { \'values\': [1.0, 2.0, None, 4.0, 5.0], \'category\': [\'A\', \'B\', \'A\', \'B\', \'C\'], \'flag\': [True, False, True, False, True] } index = pd.date_range(start=\'2022-01-01\', periods=5) df = pd.DataFrame(data, index=index) result = analyze_dataframe(df) print(result) ``` Your task is to implement the `analyze_dataframe` function following the specified requirements. Ensure you test your function with different inputs to verify its correctness. Notes: - You may assume that the input DataFrame is always valid and contains the specified columns. - Focus on efficient use of `pandas` methods to perform the operations.","solution":"import pandas as pd def analyze_dataframe(df: pd.DataFrame) -> tuple: Analyzes the input DataFrame and performs multiple operations. Args: df (pd.DataFrame): Input DataFrame with a DatetimeIndex and columns [\'values\', \'category\', \'flag\']. Returns: tuple: A tuple containing: - The min, max, and mean of the \'values\' column after filling missing values. - The count of unique categories in the \'category\' column. - The proportion of \'True\' values in the \'flag\' column. - The sorted and filtered DataFrame. # Fill missing values in \'values\' column with the mean of non-missing values df[\'values\'] = df[\'values\'].fillna(df[\'values\'].mean()) # Calculate min, max, and mean of \'values\' column min_value = df[\'values\'].min() max_value = df[\'values\'].max() mean_value = df[\'values\'].mean() # Count of unique categories in \'category\' column unique_categories_count = df[\'category\'].nunique() # Proportion of True values in \'flag\' column proportion_true_flag = df[\'flag\'].mean() # Convert DatetimeIndex to a column and reset the index df_reset = df.reset_index().rename(columns={\'index\': \'datetime\'}) # Filter the DataFrame to include only rows where \'flag\' column is True df_filtered = df_reset[df_reset[\'flag\']] # Sort the DataFrame by \'values\' column in ascending order df_sorted = df_filtered.sort_values(by=\'values\') return (min_value, max_value, mean_value), unique_categories_count, proportion_true_flag, df_sorted"},{"question":"# Question: Building a Small Convolutional Neural Network in PyTorch Objective Your task is to implement a simple convolutional neural network (CNN) using PyTorch\'s `torch.nn.functional` module. This network should be capable of handling image data (e.g., from the MNIST dataset). Requirements 1. **Network Architecture**: - **Convolution Layer**: A 2D convolution layer with a specified number of filters, kernel size, and stride. - **Activation Function**: A ReLU activation function after the convolution layer. - **Pooling Layer**: A 2D max-pooling layer. - **Fully Connected Layer**: A linear layer to map the features to class scores. 2. **Coding**: - Implement the CNN class with the following methods: - `__init__(self, num_classes: int)`: Initialize the network layers. - `forward(self, x: torch.Tensor) -> torch.Tensor`: Define the forward pass of the network. 3. **Expected Input and Output Formats**: - **Input**: - A batch of images with shape `(batch_size, channels, height, width)`. - **Output**: - Logits with shape `(batch_size, num_classes)`. Constraints - Use only the functions from `torch.nn.functional` and necessary PyTorch tensor operations. - Make sure to handle tensor shape transformations appropriately using PyTorch functions. - Ensure that the final output logits are derived from the fully connected layer following standard practices. Performance Requirements - The implementation should be efficient and make full use of PyTorch\'s capabilities for matrix and tensor operations. - The network should be capable of handling a moderate batch size of images (e.g., batch size = 64) without running into GPU memory issues on commonly available hardware. Example ```python import torch import torch.nn.functional as F class SimpleCNN(torch.nn.Module): def __init__(self, num_classes: int): super(SimpleCNN, self).__init__() # Define the network layers self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1) self.fc1 = torch.nn.Linear(in_features=32 * 14 * 14, out_features=num_classes) def forward(self, x: torch.Tensor) -> torch.Tensor: # Convolution followed by ReLU activation x = F.relu(self.conv1(x)) # Max pooling layer x = F.max_pool2d(x, kernel_size=2, stride=2) # Flatten the tensor x = x.view(x.size(0), -1) # Fully connected layer to get logits x = self.fc1(x) return x # Example usage: model = SimpleCNN(num_classes=10) example_input = torch.randn(64, 1, 28, 28) # Batch of 64 MNIST-like images output_logits = model(example_input) print(output_logits.shape) # Should be (64, 10) ``` Complete the implementation of `SimpleCNN` class based on the specified requirements. Make sure your code is clean, efficient, and well-commented.","solution":"import torch import torch.nn.functional as F class SimpleCNN(torch.nn.Module): def __init__(self, num_classes: int): super(SimpleCNN, self).__init__() # Define the network layers self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1) self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2) self.fc1 = torch.nn.Linear(in_features=32 * 14 * 14, out_features=num_classes) def forward(self, x: torch.Tensor) -> torch.Tensor: # Convolution followed by ReLU activation x = F.relu(self.conv1(x)) # Max pooling layer x = self.pool(x) # Flatten the tensor x = x.view(x.size(0), -1) # Fully connected layer to get logits x = self.fc1(x) return x"},{"question":"# Problem Description You have been provided with a set of discrete time-domain signals and are required to process these signals using the `torch.fft` module in PyTorch. Specifically, you will implement a function that: 1. Computes the FFT of a given signal. 2. Shifts the zero-frequency component to the center of the spectrum. 3. Removes high-frequency components beyond a specified threshold. 4. Applies the inverse FFT to retrieve the modified time-domain signal. # Function Signature ```python import torch def process_signal(signal: torch.Tensor, threshold: float) -> torch.Tensor: Process a time-domain signal using FFT and inverse FFT. Parameters: - signal (torch.Tensor): A 1D tensor representing the time-domain signal. - threshold (float): The frequency threshold for filtering high-frequency components. Returns: - torch.Tensor: The filtered time-domain signal. pass ``` # Input - `signal` (torch.Tensor): A 1D tensor of shape `(N,)` representing the input time-domain signal. - `threshold` (float): A float value representing the threshold frequency. Frequencies higher than this threshold should be filtered out (set to zero). # Output - The function should return a 1D tensor of shape `(N,)`, representing the filtered time-domain signal. # Constraints - You may assume that `signal` has at least one element and `threshold` is a positive number. - Your implementation should use PyTorch functions from the `torch.fft` module. - The solution must maintain the same length of the time-domain signal. # Example ```python # Example input signal (time-domain) signal = torch.Tensor([0.0, 1.0, 0.0, -1.0]) # Example frequency threshold threshold = 1.0 # Processed signal processed_signal = process_signal(signal, threshold) print(processed_signal) ``` Note: The example provided is for illustration purposes. The actual output will depend on the input signal and the frequency threshold. # Requirements 1. Compute the FFT of the input signal. 2. Shift the zero-frequency component to the center frequency component. 3. Zero out the frequency components which have absolute values greater than the threshold. 4. Shift the zero-frequency component back to the original position. 5. Compute the inverse FFT to transform the filtered frequency-domain signal back to the time-domain. # Hints 1. Use `torch.fft.fft` for computing the FFT. 2. Use `torch.fft.fftshift` and `torch.fft.ifftshift` to shift frequencies. 3. Use `torch.fft.ifft` for computing the inverse FFT. # Assessment Your solution will be evaluated based on: - Correctness of the implementation. - Adherence to the input and output specifications. - Efficiency and performance, considering typical constraints on tensor sizes in real-world applications. - The use of PyTorch FFT functions appropriately.","solution":"import torch def process_signal(signal: torch.Tensor, threshold: float) -> torch.Tensor: # Compute the FFT of the input signal fft_signal = torch.fft.fft(signal) # Shift the zero-frequency component to the center of the spectrum shifted_fft_signal = torch.fft.fftshift(fft_signal) # Calculate the frequencies corresponding to the FFT components freqs = torch.fft.fftfreq(signal.size(0)) # Zero out the high-frequency components beyond the threshold shifted_fft_signal[torch.abs(freqs) > threshold] = 0 # Shift the zero-frequency component back to the original position filtered_fft_signal = torch.fft.ifftshift(shifted_fft_signal) # Compute the inverse FFT to retrieve the modified time-domain signal processed_signal = torch.fft.ifft(filtered_fft_signal).real return processed_signal"},{"question":"# Question In this assessment, you will demonstrate your understanding of the scikit-learn `cross_decomposition` module by implementing and applying a `PLSRegression` model to a synthetic dataset. Task 1. **Generate a Synthetic Dataset:** - Create two random matrices, `X` and `Y`, each with `100` samples and different dimensions: - `X` should have `100` samples and `20` features. - `Y` should have `100` samples and `10` targets. 2. **Implement PLS Regression:** - Use the `PLSRegression` class from the `sklearn.cross_decomposition` module. - Fit the PLS model on the generated dataset `X` and `Y` using `n_components=5`. - Transform the `X` and `Y` matrices using the fitted model. 3. **Evaluate the Model:** - Compute the covariance matrix between the transformed `X` and `Y` datasets. - Implement a function `maximum_covariance` that returns the maximum absolute covariance value from this matrix. Your implementation should include: - Function to generate random matrices `X` and `Y`. - Function to fit and transform the PLSRegression model. - Function to compute and return the maximum absolute covariance value. Constraints: - Use NumPy for matrix operations. - Use scikit-learn\'s `cross_decomposition` module for PLS regression. - Ensure your code runs efficiently for the given dataset dimensions. Input and Output Format: - Your functions do not need to take any inputs directly. - Your final function `maximum_covariance()` should output a single floating-point number representing the maximum absolute covariance value. Example: ```python import numpy as np from sklearn.cross_decomposition import PLSRegression def generate_data(seed=0): np.random.seed(seed) X = np.random.rand(100, 20) Y = np.random.rand(100, 10) return X, Y def pls_fit_transform(X, Y, n_components=5): pls = PLSRegression(n_components=n_components) pls.fit(X, Y) X_transformed, Y_transformed = pls.transform(X, Y) return X_transformed, Y_transformed def maximum_covariance(): X, Y = generate_data() X_transformed, Y_transformed = pls_fit_transform(X, Y) covariance_matrix = np.cov(X_transformed.T, Y_transformed.T)[:5, 5:] max_cov = np.max(np.abs(covariance_matrix)) return max_cov # Example usage: result = maximum_covariance() print(result) # Output: (a single floating-point number) ```","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression def generate_data(seed=0): np.random.seed(seed) X = np.random.rand(100, 20) Y = np.random.rand(100, 10) return X, Y def pls_fit_transform(X, Y, n_components=5): pls = PLSRegression(n_components=n_components) pls.fit(X, Y) X_transformed, Y_transformed = pls.transform(X, Y) return X_transformed, Y_transformed def maximum_covariance(): X, Y = generate_data() X_transformed, Y_transformed = pls_fit_transform(X, Y) covariance_matrix = np.cov(X_transformed.T, Y_transformed.T)[:5, 5:] max_cov = np.max(np.abs(covariance_matrix)) return max_cov"},{"question":"Objective: You are required to implement a custom descriptor in Python that validates the type of the attribute and logs any access to it. This question will test your understanding of descriptors, attribute management, and logging in Python. Requirements: 1. **Class `TypeLoggedAttribute`:** - This will be the descriptor class. - It should accept a type (e.g., `int`, `str`) during initialization. - It should log every access and modification of the attribute. - It should raise `TypeError` if the assigned value doesn\'t match the expected type. 2. **Class `MyClass`:** - This will be an example class using `TypeLoggedAttribute` as a descriptor for its attributes. - It should have at least two attributes with different types managed by `TypeLoggedAttribute`. Detailed Steps: 1. **Create the `TypeLoggedAttribute` class**: - Implement the `__init__`, `__get__`, `__set__`, and `__delete__` methods. - Use the Python `logging` module to log access to the attributes. Log messages should indicate whether the attribute was accessed or modified. - Ensure that the value being set to the attribute matches the expected type. If not, raise a `TypeError` with an appropriate error message. 2. **Create the `MyClass` class**: - Use `TypeLoggedAttribute` for at least two attributes with different types (e.g., `age` as `int`, `name` as `str`). Example Expected Behavior: ```python import logging logging.basicConfig(level=logging.INFO) class TypeLoggedAttribute: def __init__(self, type_): self.type_ = type_ self.private_name = None def __set_name__(self, owner, name): self.public_name = name self.private_name = f\'_{name}\' def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logging.info(f\'Accessing {self.public_name} giving {value!r}\') return value def __set__(self, obj, value): if not isinstance(value, self.type_): raise TypeError(f\'Expected {value!r} to be of type {self.type_!r}\') logging.info(f\'Updating {self.public_name} to {value!r}\') setattr(obj, self.private_name, value) def __delete__(self, obj): logging.info(f\'Deleting {self.public_name}\') delattr(obj, self.private_name) class MyClass: age = TypeLoggedAttribute(int) name = TypeLoggedAttribute(str) def __init__(self, age, name): self.age = age self.name = name # Example usage: person = MyClass(25, \\"John Doe\\") person.age # Should log: Accessing age giving 25 and return 25 person.name = \\"Jane Doe\\" # Should log: Updating name to \'Jane Doe\' # person.age = \\"Twenty five\\" # Should raise TypeError: Expected \'Twenty five\' to be of type <class \'int\'> ``` Constraints: - Use the `logging` module for logging. - The solution should be compatible with Python 3.6 and above. Submission: - Complete the implementation of `TypeLoggedAttribute` and `MyClass`. - Ensure your code passes the example usage and correctly raises errors on invalid type assignments.","solution":"import logging logging.basicConfig(level=logging.INFO) class TypeLoggedAttribute: def __init__(self, type_): self.type_ = type_ self.private_name = None def __set_name__(self, owner, name): self.public_name = name self.private_name = f\'_{name}\' def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logging.info(f\'Accessing {self.public_name} giving {value!r}\') return value def __set__(self, obj, value): if not isinstance(value, self.type_): raise TypeError(f\'Expected {value!r} to be of type {self.type_!r}\') logging.info(f\'Updating {self.public_name} to {value!r}\') setattr(obj, self.private_name, value) def __delete__(self, obj): logging.info(f\'Deleting {self.public_name}\') delattr(obj, self.private_name) class MyClass: age = TypeLoggedAttribute(int) name = TypeLoggedAttribute(str) def __init__(self, age, name): self.age = age self.name = name # Example usage: # person = MyClass(25, \\"John Doe\\") # person.age # Should log: Accessing age giving 25 and return 25 # person.name = \\"Jane Doe\\" # Should log: Updating name to \'Jane Doe\' # person.age = \\"Twenty five\\" # Should raise TypeError: Expected \'Twenty five\' to be of type <class \'int\'>"},{"question":"<|Analysis Begin|> The \\"mimetypes\\" module in Python provides utilities to map filenames to MIME types and vice versa. It supports several key functionalities: 1. **Guessing MIME types** from a filename or URL, considering optional strict mode. 2. **Guessing file extensions** for given MIME types, supporting both single and multiple possible extensions. 3. **Loading and initializing MIME type mappings** from files, allowing dynamic augmentation of the type map. 4. **Adding new type-to-extension mappings** and vice versa. The main class provided is `MimeTypes` which allows more flexible handling of MIME type mappings, including loading additional files, and provides similar methods to the top-level functions in the module. Here\'s a brief summary of the key methods and their purposes: - `mimetypes.guess_type(url, strict=True)`: Guesses MIME type and encoding for a given file/path/URL. - `mimetypes.guess_all_extensions(type, strict=True)`: Returns all possible extensions for a given MIME type. - `mimetypes.guess_extension(type, strict=True)`: Returns a single extension for a given MIME type. - `mimetypes.init(files=None)`: Initializes internal data structures, optionally using specified files. - `mimetypes.read_mime_types(filename)`: Reads MIME type maps from a file and returns a dictionary. - `mimetypes.add_type(type, ext, strict=True)`: Adds a MIME type to extension mapping. - The `MimeTypes` class with equivalent methods to handle multiple MIME type databases. By leveraging the provided features, one can design a challenging problem that tests comprehension of file type detection, dynamic data structure manipulation, and class method usage. <|Analysis End|> <|Question Begin|> You have been asked to develop a script that categorizes a list of files based on their MIME types. Specifically, you have to: 1. Read MIME type mappings from given files. 2. Process a list of filenames (with paths) and determine their MIME types. 3. Group the filenames by their primary MIME type (the part before the \'/\'). 4. Identify and list any files for which the MIME type cannot be determined. Implement the function `categorize_files(mime_files, file_list)`, which performs the following steps: # Input: - `mime_files`: A list of file paths (strings) containing MIME type mappings. - `file_list`: A list of file paths (strings) for which the MIME types need to be determined. # Output: - A dictionary where the keys are primary MIME types, and the values are lists of file paths that belong to that primary MIME type. - A separate list of file paths for which the MIME type could not be determined. # Constraints: - You must use Python\'s `mimetypes` module. - The function should be efficient and handle large numbers of files gracefully. - The MIME types should be determined in a non-strict mode. # Example: ```python def categorize_files(mime_files, file_list): pass mime_files = [\'mime.types\'] file_list = [ \'/path/to/file1.mp3\', \'/path/to/file2.jpg\', \'/path/to/file3.unknown\' ] result = categorize_files(mime_files, file_list) print(result) # Output # ( # { # \'audio\': [\'/path/to/file1.mp3\'], # \'image\': [\'/path/to/file2.jpg\'] # }, # [\'/path/to/file3.unknown\'] # ) ``` # Notes: - You may assume that the provided MIME type mapping files are in standard `mime.types` format. - Handle any necessary initialization or configuration of the `mimetypes` module within your function.","solution":"import mimetypes def categorize_files(mime_files, file_list): # Initialize the mimetypes module with provided mime_files mimetypes.init(files=mime_files) categorized_files = {} undetermined_files = [] for file in file_list: mime_type, encoding = mimetypes.guess_type(file, strict=False) if mime_type: primary_type = mime_type.split(\'/\')[0] if primary_type not in categorized_files: categorized_files[primary_type] = [] categorized_files[primary_type].append(file) else: undetermined_files.append(file) return categorized_files, undetermined_files"},{"question":"**Question: Customizing a Plot with Advanced Seaborn Properties** **Objective:** Create a customized plot using Seaborn that demonstrates your understanding of coordinate, color, alpha, style, and text properties. **Problem Statement:** You are provided with a dataset containing information about various species of flowers, including their petal lengths, petal widths, species type, and measurements date. Your task is to generate a scatter plot using seaborn with the following customizations: 1. **Data:** - Load the dataset from the URL: `\\"https://example.com/flower_data.csv\\"` - The dataset has columns: `[\'species\', \'petal_length\', \'petal_width\', \'measurement_date\']`. 2. **Plot Requirements:** - Use a scatter plot to show the relationship between petal length (x-axis) and petal width (y-axis). - Assign different colors to different species using a nominal color scale. - Apply transparency (`alpha` property) to distinguish overlapping points. - Use different marker styles for different species. - Ensure the x-axis and y-axis tick labels are concise. 3. **Design Customizations:** - Provide the plot a title `\\"Customized Flower Species Scatter Plot\\"`. - Customize axis labels to `\\"Petal Length (cm)\\"` and `\\"Petal Width (cm)\\"`. - Adjust the figure size to `(10, 8)`. - Remove the top and right spines of the plot. **Expected Input:** - No inputs as the URL of the dataset is provided. **Expected Output:** - A matplotlib figure representing the customized scatter plot as specified. **Sample Code Structure:** ```python import seaborn.objects as so import pandas as pd # Load the dataset url = \\"https://example.com/flower_data.csv\\" df = pd.read_csv(url) # Create the customized scatter plot with Seaborn plot = ( so.Plot(df, x=\\"petal_length\\", y=\\"petal_width\\", color=\\"species\\", marker=\\"species\\", alpha=.7) .add(so.Dot()) .scale( color=so.Nominal(), marker=so.Nominal(), alpha=None, ) .limit(x=(0, df[\'petal_length\'].max() + 1), y=(0, df[\'petal_width\'].max() + 1)) .label(x=\\"Petal Length (cm)\\", y=\\"Petal Width (cm)\\", title=\\"Customized Flower Species Scatter Plot\\") .theme({ **so.style_axes(), \\"axes.spines.top\\": False, \\"axes.spines.right\\": False, \\"figure.figsize\\": (10, 8), }) ) # Display the plot plot.show() ``` Ensure to explain your approach and the seaborn properties used in the code.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_customized_scatter_plot(): # Load the dataset url = \\"https://example.com/flower_data.csv\\" df = pd.read_csv(url) # Set the style for the plot sns.set(style=\\"whitegrid\\") # Create a scatter plot with customizations plt.figure(figsize=(10, 8)) scatter_plot = sns.scatterplot( data=df, x=\\"petal_length\\", y=\\"petal_width\\", hue=\\"species\\", style=\\"species\\", alpha=0.7, palette=\\"deep\\" ) # Customize the axis labels scatter_plot.set_xlabel(\\"Petal Length (cm)\\") scatter_plot.set_ylabel(\\"Petal Width (cm)\\") # Set the title scatter_plot.set_title(\\"Customized Flower Species Scatter Plot\\") # Remove the top and right spines sns.despine() # Show the plot plt.show()"},{"question":"You are required to create a custom command-line interpreter to manage a simple text-based library system. This system should support basic commands to add books, list books, and search for books by title. # Requirements: 1. **Create a subclass of `cmd.Cmd`** called `LibraryCmd`. 2. **Implement the following commands**: - `add [title] [author]`: Adds a new book with the given title and author to the library. If the title is not provided, print an error message. - `list`: Lists all books in the library in the format \\"Title by Author\\". - `search [title]`: Searches for a book by title and prints \\"Title by Author\\" if found, otherwise prints \\"Book not found\\". - `quit`: Exits the command loop and terminates the program. 3. **Enable readline command completion** using the tab key for the `add`, `list`, and `search` commands. 4. **Include appropriate help messages** for each command. # Implementation Details: - Your `LibraryCmd` class should inherit from `cmd.Cmd`. - Use instance variables to store the list of books. - Ensure that `cmdloop()` runs with a custom prompt (e.g., `(library) `) and a welcome message that guides the user (\\"Welcome to the library system. Type help or ? to list commands.\\"). # Input and Output Format: - Input: User inputs commands as specified in the requirements. - Output: Print messages or lists as requested by the commands. # Example Session: ```plaintext Welcome to the library system. Type help or ? to list commands. (library) add \\"The Great Gatsby\\" \\"F. Scott Fitzgerald\\" (library) list The Great Gatsby by F. Scott Fitzgerald (library) search \\"The Great Gatsby\\" The Great Gatsby by F. Scott Fitzgerald (library) search \\"Moby Dick\\" Book not found (library) quit ``` # Constraints: - The title and author should be enclosed in double quotes. - Books should be stored in a list of dictionaries with keys `\\"title\\"` and `\\"author\\"`. # Performance: - You can assume a maximum of 1000 books in the library. Here\'s a skeleton to get you started: ```python import cmd class LibraryCmd(cmd.Cmd): intro = \'Welcome to the library system. Type help or ? to list commands.n\' prompt = \'(library) \' def __init__(self): super().__init__() self.books = [] # List of dictionaries, each with keys \\"title\\" and \\"author\\" def do_add(self, arg): \'Add a new book to the library: add \\"title\\" \\"author\\"\' # Your code here def do_list(self, arg): \'List all books in the library: list\' # Your code here def do_search(self, arg): \'Search for a book by title: search \\"title\\"\' # Your code here def do_quit(self, arg): \'Quit the library command loop: quit\' print(\'Goodbye!\') return True def complete_add(self, text, line, begidx, endidx): # Optional: Implement tab-completion for the add command return [] def complete_list(self, text, line, begidx, endidx): # Optional: Implement tab-completion for the list command return [] def complete_search(self, text, line, begidx, endidx): # Optional: Implement tab-completion for the search command return [] if __name__ == \'__main__\': LibraryCmd().cmdloop() ``` Fill in the implementation details to complete the shell functionality. Make sure to test your code thoroughly.","solution":"import cmd class LibraryCmd(cmd.Cmd): intro = \'Welcome to the library system. Type help or ? to list commands.n\' prompt = \'(library) \' def __init__(self): super().__init__() self.books = [] # List of dictionaries, each with keys \\"title\\" and \\"author\\" def do_add(self, arg): \'Add a new book to the library: add \\"title\\" \\"author\\"\' args = self.parse_args(arg) if args: title, author = args self.books.append({\\"title\\": title, \\"author\\": author}) print(f\'Added book: \\"{title}\\" by {author}\') else: print(\'Error: Title and author required. Usage: add \\"title\\" \\"author\\"\') def do_list(self, arg): \'List all books in the library: list\' if self.books: for book in self.books: print(f\'{book[\\"title\\"]} by {book[\\"author\\"]}\') else: print(\'No books in the library.\') def do_search(self, arg): \'Search for a book by title: search \\"title\\"\' title = arg.strip(\'\\"\') found = None for book in self.books: if book[\\"title\\"] == title: found = book break if found: print(f\'{found[\\"title\\"]} by {found[\\"author\\"]}\') else: print(\'Book not found\') def do_quit(self, arg): \'Quit the library command loop: quit\' print(\'Goodbye!\') return True def parse_args(self, args): import shlex try: args = shlex.split(args) if len(args) == 2: return args except ValueError: pass return None if __name__ == \'__main__\': LibraryCmd().cmdloop()"},{"question":"# Custom Logging Configuration Task **Objective:** Implement a custom logging configuration for a Python application following the provided specifications. The solution should demonstrate proficiency in using Python\'s logging module, including loggers, handlers, formatters, and different logging levels. **Task:** 1. **Create a custom logger:** - Instantiate a logger named `customApp`. - Set the logging level to `DEBUG`. 2. **Create and configure handlers:** - **Console Handler**: - Sends log messages to the console. - Should handle messages of all severity levels (DEBUG and above). - **File Handler**: - Sends log messages to a file named `app.log`. - Should handle messages of severity level `ERROR` and above. - The log file should not append messages; it should start fresh on each run. - **Rotating File Handler**: - Sends log messages to a file named `app_rotating.log`. - Maximum file size: 1 MB. - Retains up to 3 backup files. 3. **Create and associate formatters:** - **Console Formatter**: - Format should include date and time, logger name, severity level, and message. - Example format: `%(asctime)s - %(name)s - %(levelname)s - %(message)s` - **File Formatter**: - Format should include severity level and message only. - Example format: `%(levelname)s - %(message)s` - **Rotating File Formatter**: - Format should include logger name and message. - Example format: `%(name)s - %(message)s` 4. **Log test messages:** - Log messages of each severity level (DEBUG, INFO, WARNING, ERROR, CRITICAL) to demonstrate that each handler and formatter works correctly. - Ensure at least one log file rotation (i.e., more than 1 MB of log data). **Expected Output:** - Console output should display messages in the format specified, regardless of severity. - `app.log` should contain only messages with severity `ERROR` and `CRITICAL`, with the specified format. - `app_rotating.log` and its backup files should contain messages of all severity levels, demonstrating file rotation when the size limit is exceeded. **Code Structure:** ```python import logging from logging.handlers import RotatingFileHandler def setup_custom_logger(): # Create \'customApp\' logger logger = logging.getLogger(\'customApp\') logger.setLevel(logging.DEBUG) # Create console handler and set level to DEBUG ch = logging.StreamHandler() ch.setLevel(logging.DEBUG) # Create file handler and set level to ERROR fh = logging.FileHandler(\'app.log\', mode=\'w\') fh.setLevel(logging.ERROR) # Create rotating file handler rfh = RotatingFileHandler(\'app_rotating.log\', maxBytes=1*1024*1024, backupCount=3) rfh.setLevel(logging.DEBUG) # Create and set formatters console_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') file_formatter = logging.Formatter(\'%(levelname)s - %(message)s\') rotating_file_formatter = logging.Formatter(\'%(name)s - %(message)s\') ch.setFormatter(console_formatter) fh.setFormatter(file_formatter) rfh.setFormatter(rotating_file_formatter) # Add handlers to the logger logger.addHandler(ch) logger.addHandler(fh) logger.addHandler(rfh) return logger def main(): logger = setup_custom_logger() # Log messages to test different severity levels logger.debug(\'This is a DEBUG message\') logger.info(\'This is an INFO message\') logger.warning(\'This is a WARNING message\') logger.error(\'This is an ERROR message\') logger.critical(\'This is a CRITICAL message\') # Generate additional log data to force file rotation for i in range(10000): logger.debug(f\'Log message {i}\') if __name__ == \\"__main__\\": main() ``` **Constraints:** - Ensure that your solution does not append messages to `app.log` on successive runs. - Write clean, readable, and well-documented code. - Assume the necessary Python modules are available and no external libraries are required.","solution":"import logging from logging.handlers import RotatingFileHandler def setup_custom_logger(): # Create \'customApp\' logger logger = logging.getLogger(\'customApp\') logger.setLevel(logging.DEBUG) # Create console handler and set its level to DEBUG console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) # Create file handler and set its level to ERROR file_handler = logging.FileHandler(\'app.log\', mode=\'w\') file_handler.setLevel(logging.ERROR) # Create rotating file handler rotating_file_handler = RotatingFileHandler(\'app_rotating.log\', maxBytes=1*1024*1024, backupCount=3) rotating_file_handler.setLevel(logging.DEBUG) # Create formatter for console handler console_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(console_formatter) # Create formatter for file handler file_formatter = logging.Formatter(\'%(levelname)s - %(message)s\') file_handler.setFormatter(file_formatter) # Create formatter for rotating file handler rotating_file_formatter = logging.Formatter(\'%(name)s - %(message)s\') rotating_file_handler.setFormatter(rotating_file_formatter) # Add handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) logger.addHandler(rotating_file_handler) return logger def main(): logger = setup_custom_logger() # Log messages to test different severity levels logger.debug(\'This is a DEBUG message\') logger.info(\'This is an INFO message\') logger.warning(\'This is a WARNING message\') logger.error(\'This is an ERROR message\') logger.critical(\'This is a CRITICAL message\') # Generate additional log data to force file rotation for i in range(10000): logger.debug(f\'Log message {i}\') if __name__ == \\"__main__\\": main()"},{"question":"Given a directory containing text files, write a Python function named `compress_directory` that compresses all `.txt` files in the directory into `.gz` files using gzip compression. You must: 1. Read each `.txt` file in the directory. 2. Compress the content and save it with the same name but with a `.gz` extension. 3. Ensure that the original `.txt` file is deleted after it has been successfully compressed. 4. Handle exceptions gracefully, such as failed file reads or writes and invalid files. Function Signature: ```python def compress_directory(directory: str, compresslevel: int = 9) -> None: ``` Input: - `directory` (str): The path to the directory containing `.txt` files. - `compresslevel` (int, optional): The level of compression for the gzip file. Default is 9 (highest compression). Output: - None Constraints: - You may assume that the directory contains only `.txt` files and no subdirectories. - Ensure to handle files and file system errors using appropriate exception handling. Example: Suppose the directory `/path/to/files` contains the following files: - `example1.txt` with content \\"Hello\\" - `example2.txt` with content \\"World\\" After running the `compress_directory` function: - The directory should contain: - `example1.txt.gz` (compressed content of `example1.txt`) - `example2.txt.gz` (compressed content of `example2.txt`) - The original `.txt` files should be deleted. Note: Include docstrings and comments in your code to explain your implementation.","solution":"import os import gzip import shutil def compress_directory(directory: str, compresslevel: int = 9) -> None: Compresses all `.txt` files in the given directory into `.gz` files using gzip compression. Args: directory (str): The path to the directory containing `.txt` files. compresslevel (int, optional): The level of compression for the gzip file. Default is 9 (highest compression). Returns: None try: for filename in os.listdir(directory): if filename.endswith(\'.txt\'): txt_filepath = os.path.join(directory, filename) gz_filepath = txt_filepath + \'.gz\' try: with open(txt_filepath, \'rb\') as f_in: with gzip.open(gz_filepath, \'wb\', compresslevel=compresslevel) as f_out: shutil.copyfileobj(f_in, f_out) os.remove(txt_filepath) except OSError as e: print(f\\"Error processing file {txt_filepath}: {e}\\") except Exception as e: print(f\\"Error accessing directory {directory}: {e}\\")"},{"question":"**Context Management with `contextlib`** **Objective:** The goal of this exercise is to test your understanding and ability to implement effective resource management using the `contextlib` utilities. You will need to create custom context managers and utilize the `ExitStack` class to manage and clean up multiple resources. **Problem Statement:** You are given a list of resource names. Each resource needs to be \\"acquired\\" and later \\"released\\" at the end of usage. An optional resource might be necessary based on a specific condition. Further, some resources might require suppression of specific exceptions during the acquisition process. **Task:** 1. Implement a custom context manager `ResourceManager` that: - Accepts functions to acquire and release resources. - Manages the resource acquisition and ensures that resources are properly released no matter what happens during their usage. 2. Write a function `manage_resources` that: - Accepts a list of resource names and a condition function. - Uses an `ExitStack` to manage the acquisition and release of multiple resources. - Optionally acquires a special resource if the condition function returns `True`. - Suppresses a list of exceptions during the resource acquisition process and logs any suppressed exception. **Specifications:** - **Input:** ```python def acquire_resource(name: str) -> object: # Acquires a resource based on the name. This could be a database connection, file, etc. pass def release_resource(resource: object) -> None: # Releases the acquired resource. pass def condition_function() -> bool: # Returns True or False based on some condition. pass def manage_resources(resource_names: List[str], condition: Callable[[], bool], suppress_exceptions: Tuple[Type[BaseException], ...]) -> List[object]: # Manages multiple resources pass ``` - **Output:** - The function `manage_resources` should return a list of acquired resources. - All resources must be released appropriately. - Any suppressed exceptions should be logged. **Constraints:** - The `acquire_resource` function may raise exceptions that need to be handled. - Use `ExitStack` to manage multiple context managers efficiently. - Ensure that resources are always acquired and released correctly. - Suppress specific exceptions as specified and log these. **Example Usage:** ```python import logging logging.basicConfig(level=logging.INFO) def acquire_resource(name: str) -> str: if \\"fail\\" in name: raise ValueError(f\\"Resource {name} cannot be acquired\\") return f\\"Resource {name}\\" def release_resource(resource: str) -> None: logging.info(f\\"Released {resource}\\") def condition_function() -> bool: return True def manage_resources(resource_names, condition, suppress_exceptions): resources = [] class ResourceManager: def __init__(self, acquire_fn, release_fn, name): self.acquire_fn = acquire_fn self.release_fn = release_fn self.resource = None self.name = name def __enter__(self): try: self.resource = self.acquire_fn(self.name) except suppress_exceptions as ex: logging.exception(f\\"Suppressed: {ex}\\") # Suppress the exception by returning None self.resource = None return self.resource def __exit__(self, exc_type, exc, exc_tb): if self.resource: self.release_fn(self.resource) with ExitStack() as stack: for name in resource_names: res_mgr = ResourceManager(acquire_resource, release_resource, name) res = stack.enter_context(res_mgr) if res: resources.append(res) if condition(): special_mgr = ResourceManager(acquire_resource, release_resource, \\"special_resource\\") special_resource = stack.enter_context(special_mgr) if special_resource: resources.append(special_resource) return resources # Example Call resources = manage_resources([\\"resource1\\", \\"fail_resource\\", \\"resource2\\"], condition_function, (ValueError,)) print(\\"Acquired resources:\\", resources) ``` **Note:** To pass the assessment, your solution must: - Correctly handle and suppress specified exceptions. - Correctly manage the acquisition and release of resources. - Return a list of acquired resource objects.","solution":"from contextlib import ExitStack import logging logging.basicConfig(level=logging.INFO) def acquire_resource(name: str) -> str: Acquires a resource based on the name. This could be a database connection, file, etc. if \\"fail\\" in name: raise ValueError(f\\"Resource {name} cannot be acquired\\") return f\\"Resource {name}\\" def release_resource(resource: str) -> None: Releases the acquired resource. logging.info(f\\"Released {resource}\\") def condition_function() -> bool: Returns True or False based on some condition. return True def manage_resources(resource_names, condition, suppress_exceptions): Manages multiple resources. resources = [] class ResourceManager: def __init__(self, acquire_fn, release_fn, name): self.acquire_fn = acquire_fn self.release_fn = release_fn self.resource = None self.name = name def __enter__(self): try: self.resource = self.acquire_fn(self.name) except suppress_exceptions as ex: logging.exception(f\\"Suppressed: {ex}\\") # Suppress the exception by returning None self.resource = None return self.resource def __exit__(self, exc_type, exc, exc_tb): if self.resource: self.release_fn(self.resource) with ExitStack() as stack: for name in resource_names: res_mgr = ResourceManager(acquire_resource, release_resource, name) res = stack.enter_context(res_mgr) if res: resources.append(res) if condition(): special_mgr = ResourceManager(acquire_resource, release_resource, \\"special_resource\\") special_resource = stack.enter_context(special_mgr) if special_resource: resources.append(special_resource) return resources"},{"question":"# **Advanced Python Typing Exercise** Problem Statement You are tasked with implementing a utility function `check_user_permissions` that determines whether a user has the correct permissions to execute specific actions on a set of resources. This involves checking that the user has roles specified in their profile that grant them the required permissions. The function signature should make use of Python\'s typing capabilities, including generic programming and type constraints. Here are the detailed requirements: 1. Define a `Role` type using `NewType` to distinguish it from a plain string. 2. Define a `UserProfile` class using `TypedDict` that includes: - `username`: a string representing the username. - `roles`: a list of roles (`Role`) the user has. 3. Define a `Resource` class using `TypedDict` that includes: - `resource_id`: a string ID for the resource. - `required_roles`: a list of roles (`Role`) required to access the resource. 4. Implement the function `check_user_permissions` to determine if a user in a `UserProfile` has the necessary roles to access all given `Resource` objects. The function should: - Take two parameters: - `user_profile: UserProfile` - `resources: List[Resource]` - Return a boolean indicating whether the user has the necessary roles for all resources. Example: ```python Role = NewType(\'Role\', str) class UserProfile(TypedDict): username: str roles: List[Role] class Resource(TypedDict): resource_id: str required_roles: List[Role] def check_user_permissions(user_profile: UserProfile, resources: List[Resource]) -> bool: all_user_roles = set(user_profile[\'roles\']) for resource in resources: if not all(role in all_user_roles for role in resource[\'required_roles\']): return False return True # Example Usage user_profile = UserProfile(username=\\"john_doe\\", roles=[Role(\\"admin\\"), Role(\\"editor\\")]) resources = [ Resource(resource_id=\\"file1\\", required_roles=[Role(\\"admin\\")]), Resource(resource_id=\\"file2\\", required_roles=[Role(\\"editor\\"), Role(\\"viewer\\")]) # This will fail ] print(check_user_permissions(user_profile, resources)) # Output: False ``` Constraints - Use type hints precisely as specified. - Ensure compatibility with Python 3.10+ typing syntax. - No additional libraries should be used. Implement the constructs and function as specified above.","solution":"from typing import List, NewType from typing_extensions import TypedDict # Define Role, UserProfile, and Resource types Role = NewType(\'Role\', str) class UserProfile(TypedDict): username: str roles: List[Role] class Resource(TypedDict): resource_id: str required_roles: List[Role] def check_user_permissions(user_profile: UserProfile, resources: List[Resource]) -> bool: all_user_roles = set(user_profile[\'roles\']) for resource in resources: if not all(role in all_user_roles for role in resource[\'required_roles\']): return False return True"},{"question":"You have been provided with a Python program for an email sorting and archiving system. This system uses the `mailbox` module to organize emails from various sources into different folders within a mailbox. The goal is to sort emails from multiple mailing lists into corresponding mailboxes while handling potential errors and avoiding data corruption. **Task**: Implement the following specific functionalities in this system: 1. **Function: `move_messages_between_mailboxes`**: This function should move messages from a source mailbox to a destination mailbox based on a given criterion. For example, you might want to move all messages from the \\"inbox\\" to a mailbox named \\"archived\\" if they contain a particular keyword in their subject. ```python def move_messages_between_mailboxes(source_mailbox, destination_mailbox, criterion): Move messages from source_mailbox to destination_mailbox based on the given criterion. Parameters: - source_mailbox (mailbox.Mailbox): The mailbox from which to move messages. - destination_mailbox (mailbox.Mailbox): The mailbox to which messages should be moved. - criterion (dict): A dictionary with keys as criteria type (e.g., \\"subject_contains\\") and values as the corresponding matching value. Returns: - int: The number of messages moved. pass ``` **Example Usage**: ```python import mailbox source = mailbox.mbox(\'~/source_mbox\') destination = mailbox.mbox(\'~/archived_mbox\') moved_count = move_messages_between_mailboxes(source, destination, {\'subject_contains\': \'keyword\'}) print(f\\"Moved {moved_count} messages.\\") ``` 2. **Function: `get_all_message_subjects`**: This function should retrieve the subjects of all messages in the given mailbox. ```python def get_all_message_subjects(mailbox_instance): Retrieve the subjects of all messages in the given mailbox. Parameters: - mailbox_instance (mailbox.Mailbox): The mailbox from which to retrieve subjects. Returns: - list: A list of subjects of all messages. pass ``` **Example Usage**: ```python inbox = mailbox.mbox(\'~/inbox_mbox\') subjects = get_all_message_subjects(inbox) for subject in subjects: print(subject) ``` **Constraints**: - You must handle possible exceptions that may arise from malformed messages or concurrency issues. - Ensure that there is no data loss or mailbox corruption during the message transfer. - Implement appropriate locking mechanisms when modifying the mailboxes. - You may assume all paths are valid and mailboxes exist. **Notes**: - You may refer to the `mailbox` documentation for understanding how to use various mailbox and message methods. - Ensure your solution is efficient and can handle large mailboxes. You need to submit the implementation of these two functions. Make sure to test your solution with some sample mailboxes before submission.","solution":"import mailbox def move_messages_between_mailboxes(source_mailbox, destination_mailbox, criterion): Move messages from source_mailbox to destination_mailbox based on the given criterion. Parameters: - source_mailbox (mailbox.Mailbox): The mailbox from which to move messages. - destination_mailbox (mailbox.Mailbox): The mailbox to which messages should be moved. - criterion (dict): A dictionary with keys as criteria type (e.g., \\"subject_contains\\") and values as the corresponding matching value. Returns: - int: The number of messages moved. moved_count = 0 try: for message_key in list(source_mailbox.keys()): message = source_mailbox[message_key] subject = message[\'subject\'] if \'subject\' in message else \'\' if \\"subject_contains\\" in criterion: if criterion[\\"subject_contains\\"].lower() in subject.lower(): destination_mailbox.lock() destination_mailbox.add(message) destination_mailbox.flush() destination_mailbox.unlock() source_mailbox.lock() source_mailbox.remove(message_key) source_mailbox.flush() source_mailbox.unlock() moved_count += 1 except Exception as e: print(f\\"An error occurred: {e}\\") finally: source_mailbox.close() destination_mailbox.close() return moved_count def get_all_message_subjects(mailbox_instance): Retrieve the subjects of all messages in the given mailbox. Parameters: - mailbox_instance (mailbox.Mailbox): The mailbox from which to retrieve subjects. Returns: - list: A list of subjects of all messages. subjects = [] try: for message in mailbox_instance: subjects.append(message[\'subject\'] if \'subject\' in message else \'\') except Exception as e: print(f\\"An error occurred: {e}\\") finally: mailbox_instance.close() return subjects"},{"question":"**Coding Assessment Question:** **Implementing a Task Scheduler with Priority Queues** You are required to implement a task scheduler that manages tasks with different priorities. Each task has a unique name and an integer priority. The scheduler should support the following operations efficiently: 1. Adding a new task with its priority. 2. Removing an existing task. 3. Getting the highest priority task without removing it. 4. Popping the highest priority task (i.e., getting and removing it). You should use the \\"heapq\\" module to manage the tasks. Implement the `TaskScheduler` class with the following methods: ```python class TaskScheduler: def __init__(self): Initializes an empty TaskScheduler. # Your code here def add_task(self, task: str, priority: int): Adds a new task with the given name and priority to the scheduler. If the task already exists, update its priority. :param task: Name of the task. :param priority: Priority of the task. # Your code here def remove_task(self, task: str): Removes the task with the given name from the scheduler. :param task: Name of the task to remove. # Your code here def peek_task(self) -> str: Returns the name of the highest priority task without removing it from the scheduler. :return: Name of the highest priority task. # Your code here def pop_task(self) -> str: Pops and returns the name of the highest priority task from the scheduler. :return: Name of the highest priority task. # Your code here ``` # Constraints: 1. Task names are unique strings. 2. Priorities are integers, where a lower number represents a higher priority. 3. Ensure all operations are efficiently implemented. # Example Usage: ```python scheduler = TaskScheduler() scheduler.add_task(\\"task1\\", 3) scheduler.add_task(\\"task2\\", 1) scheduler.add_task(\\"task3\\", 2) assert scheduler.peek_task() == \\"task2\\" scheduler.remove_task(\\"task2\\") assert scheduler.peek_task() == \\"task3\\" assert scheduler.pop_task() == \\"task3\\" assert scheduler.pop_task() == \\"task1\\" ``` # Performance Requirements: 1. All operations should have an average time complexity of O(log n). **Hint:** Use the `heapq` module along with a dictionary to keep track of tasks.","solution":"import heapq class TaskScheduler: def __init__(self): self.heap = [] self.task_map = {} def add_task(self, task: str, priority: int): if task in self.task_map: self.remove_task(task) entry = (priority, task) self.task_map[task] = entry heapq.heappush(self.heap, entry) def remove_task(self, task: str): entry = self.task_map.pop(task) self.heap.remove(entry) heapq.heapify(self.heap) def peek_task(self) -> str: if not self.heap: return None return self.heap[0][1] def pop_task(self) -> str: if not self.heap: return None priority, task = heapq.heappop(self.heap) del self.task_map[task] return task"},{"question":"# Question: Handling System Errors in Python # Objective: Implement a function in Python that takes a list of integer error codes and returns a dictionary where the keys are error codes, and the values are tuples containing: - The symbolic name corresponding to the error code. - The system error message. - The mapped Python exception if applicable. # Instructions: 1. You are provided with a list of integer error codes. Your task is to implement the function `translate_error_codes(error_codes: List[int]) -> Dict[int, Tuple[str, str, Optional[Type[BaseException]]]]`. 2. For each error code in the list, perform the following: - Look up the symbolic name using `errno.errorcode`. - Get the system error message using `os.strerror()`. - Determine the mapped Python exception, if it exists (as mentioned in the `errno` documentation provided). 3. If an error code does not have a corresponding symbolic name or system message, use the placeholders \\"UNKNOWN_CODE\\" for the symbolic name and \\"Unknown Error\\" for the system message. 4. If there is no mapped Python exception for a particular error code, store `None` for the exception in the output dictionary. # Function Signature: ```python from typing import List, Dict, Tuple, Optional, Type import errno import os def translate_error_codes(error_codes: List[int]) -> Dict[int, Tuple[str, str, Optional[Type[BaseException]]]]: pass ``` # Example: ```python # Input error_codes = [1, 2, 3, 20, 85] # Expected Output { 1: (\'EPERM\', \'Operation not permitted\', PermissionError), 2: (\'ENOENT\', \'No such file or directory\', FileNotFoundError), 3: (\'ESRCH\', \'No such process\', ProcessLookupError), 20: (\'ENOTBLK\', \'Block device required\', None), 85: (\'UNKNOWN_CODE\', \'Unknown Error\', None) } ``` # Constraints: - You may assume that all input error codes are integers. - Utilize the `errno` and `os` modules effectively. - Ensure your function handles both known and unknown error codes gracefully. # Performance: - Your solution should be efficient and avoid unnecessary computations. Note: You do not need to consider multi-threading or multiprocessing aspects for this question.","solution":"from typing import List, Dict, Tuple, Optional, Type import errno import os def translate_error_codes(error_codes: List[int]) -> Dict[int, Tuple[str, str, Optional[Type[BaseException]]]]: error_mapping = { errno.EPERM: PermissionError, errno.ENOENT: FileNotFoundError, errno.ESRCH: ProcessLookupError, errno.EINTR: InterruptedError, errno.EIO: IOError, errno.ENXIO: OSError, errno.E2BIG: OSError, errno.ENOEXEC: OSError, errno.EBADF: OSError, errno.ECHILD: ChildProcessError, errno.EAGAIN: OSError, errno.ENOMEM: MemoryError, errno.EACCES: PermissionError, errno.EFAULT: OSError, errno.ENOTBLK: OSError, # Add more mappings as necessary... } result = {} for code in error_codes: if code in errno.errorcode: symbol = errno.errorcode[code] message = os.strerror(code) py_exception = error_mapping.get(code, None) else: symbol = \'UNKNOWN_CODE\' message = \'Unknown Error\' py_exception = None result[code] = (symbol, message, py_exception) return result"},{"question":"# Question: Customize a Plot\'s Legend with Seaborn You are provided with the \'penguins\' dataset from seaborn and asked to create and customize a series of plots using seaborn\'s `histplot` and `displot` functions. Your task is to demonstrate proficiency in positioning and styling plot legends. Requirements: 1. **Histogram Plot with Central Legend:** - Create a histogram plot of `bill_length_mm` with hue based on `species`. - Place the legend at the central right position. 2. **Histogram Plot with Upper-Left Legend:** - Create a similar histogram plot but move the legend to the `upper left` position, ensuring it doesn\'t overlap with the plot. - Use `bbox_to_anchor` to fine-tune this placement. 3. **Displot with Multiple Columns and Customized Legend:** - Utilize the `displot` function to create plots of `bill_length_mm` categorized by `island` with hue based on `species`. - Position the legend at the `upper left` with `bbox_to_anchor` and ensure `legend_out=False`. - Customize the legend to have no frame and appear in a single row. Input: A sample input might look like this: ```python import seaborn as sns sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") ``` Output: You should produce three plots with legends appropriately positioned and styled as specified. Constraints: - Ensure no legends overlap with the plot area. - Utilize appropriate parameters to achieve fine-grained control over the legend appearance and position. - Display proper usage of seaborn functions for creating and styling the plots. Example for Requirement 1 (Histogram Plot with Central Legend): ```python import seaborn as sns sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") # Requirement 1: Histogram Plot with Central Legend ax1 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax1, \\"center right\\") ``` You should provide the complete code for all three requirements outlined.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") def plot_histogram_central_legend(): ax1 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax1, \\"center right\\") plt.show() def plot_histogram_upper_left_legend(): ax2 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax2, \\"upper left\\", bbox_to_anchor=(1, 1), ncol=1) plt.show() def plot_displot_customized_legend(): g = sns.displot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", legend=False) for ax in g.axes.flatten(): ax.legend(loc=\\"upper left\\", bbox_to_anchor=(1, 1), ncol=1, frameon=False) plt.show()"},{"question":"# Iterator Implementation in Python Python provides built-in mechanisms to iterate over sequences and custom-defined iterations using callables. Based on the provided documentation, implement the iterator functionalities as specified: Task You are required to implement the following: 1. **SequenceIterator**: - A class that works with any sequence supporting the `__getitem__` method. - The iteration should stop when an `IndexError` is raised. 2. **CallableIterator**: - A class that takes two parameters: a callable and a sentinel value. - The iteration should stop when the callable returns the sentinel value. Function Signature ```python class SequenceIterator: def __init__(self, sequence): # Initialization code here def __iter__(self): # Return the iterator object def __next__(self): # Return the next item or raise StopIteration class CallableIterator: def __init__(self, callable, sentinel): # Initialization code here def __iter__(self): # Return the iterator object def __next__(self): # Return the next item or raise StopIteration ``` Example Usage ```python # SequenceIterator Example seq_iter = SequenceIterator([1, 2, 3]) for elem in seq_iter: print(elem) # Output: # 1 # 2 # 3 # CallableIterator Example def generate_numbers(): num = 0 while True: num += 1 yield num call_iter = CallableIterator(generate_numbers().__next__, 5) for elem in call_iter: print(elem) # Output: # 1 # 2 # 3 # 4 ``` Constraints - The sequence provided to `SequenceIterator` could be any iterable (e.g., list, tuple, string). - The callable in `CallableIterator` must be callable without any parameters. - Ensure to handle exceptions and stopping conditions properly to avoid infinite loops. Implement the classes such that they adhere to Python\'s iterator protocol, ensuring correct functionality and stopping conditions.","solution":"class SequenceIterator: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): try: value = self.sequence[self.index] except IndexError: raise StopIteration self.index += 1 return value class CallableIterator: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): value = self.callable() if value == self.sentinel: raise StopIteration return value"},{"question":"Monitoring Python Function Executions with SystemTap You are required to write a Python script and a SystemTap script that work together to monitor and report on the function calls of the Python script. Your task is to trace the execution flow of specific functions and generate a detailed report on their runtime behavior. **Python Script Specification:** Create a Python script (`my_script.py`) with the following functions: 1. `start()`: The entry point which calls other functions. 2. `function_a()`: A function that performs some computations. 3. `function_b()`: Another function that makes calls to a sub-function. 4. `function_c()`: A sub-function called by `function_b`. 5. Include additional dummy functions as necessary to demonstrate nested calls. **SystemTap Script Specification:** Write a SystemTap script (`monitor_functions.stp`) to monitor and trace the function calls in `my_script.py`, specifically: 1. Capture the entry and return points of `start()`, `function_a()`, `function_b()`, and `function_c()`. 2. Print a call hierarchy showing which function was called and from where, formatted as follows: ``` TIMESTAMP FUNCTION_ENTRY FILENAME:FUNCTION_NAME:LINE_NUMBER TIMESTAMP FUNCTION_RETURN FILENAME:FUNCTION_NAME:LINE_NUMBER ``` **Requirements:** - The Python script should include various function calls to demonstrate nested and sequential function execution. - The SystemTap script should focus on markers `function__entry` and `function__return` to gather the call hierarchy. - Ensure markers are correctly enabled in your CPython build (refer to the setup instructions in the provided documentation if necessary). **Expected Output:** The output should display a structured call and return hierarchy of the monitored functions with timestamps, providing valuable insights into the runtime behavior of your Python script. **Submission:** 1. `my_script.py` 2. `monitor_functions.stp` 3. A brief README file outlining the setup steps and how to run both scripts. **Constraints:** - Assume Python version 3.6 or above. - SystemTap must be installed and properly configured on a Linux machine to run the monitoring script. - Focus on capturing at least three levels of nested function calls to illustrate the script\'s capability to trace detailed call hierarchies. Good luck, and ensure your scripts are well-commented and organized!","solution":"# my_script.py def start(): print(\\"Start function called\\") function_a() function_b() def function_a(): print(\\"Function A called\\") dummy_a() def function_b(): print(\\"Function B called\\") function_c() dummy_b() def function_c(): print(\\"Function C called\\") def dummy_a(): print(\\"Dummy A called\\") def dummy_b(): print(\\"Dummy B called\\") if __name__ == \\"__main__\\": start()"},{"question":"# Asynchronous File Processing with `concurrent.futures` Objective: Write a Python program that processes text files asynchronously. Your task is to implement a function that reads multiple text files, counts the occurrence of each word, and then writes the word count results to an output file. This task must be done using both `ThreadPoolExecutor` for reading the files and `ProcessPoolExecutor` for processing the word counts. Function Signature: `def process_files(input_files: List[str], output_file: str, max_workers: int = None) -> None:` Input: - `input_files`: A list of `str`, where each string is the path to a text file. - `output_file`: A `str` representing the path to the output file where word counts will be written. - `max_workers`: An optional `int` specifying the maximum number of workers. Defaults to `None`. Output: - The function does not return anything. It writes the word count results to `output_file`. Implementation Details: 1. **Reading Files**: - Use `ThreadPoolExecutor` to read the content of each file concurrently. - Handle exceptions that might occur during file reading. 2. **Counting Words**: - Use `ProcessPoolExecutor` to count words in the read content concurrently. - Handle exceptions that might occur during word counting. 3. **Writing Results**: - Collect the word counts from all files and write them to `output_file` in a sorted manner (by word). - Ensure the output file is created correctly, with proper resource management. Constraints: - Ensure the function handles a large number of files efficiently. - The function should properly shutdown executors and handle any exceptions that occur during processing. Example: Assume `file1.txt` contains the text \\"hello world\\" and `file2.txt` contains \\"hello again world\\": ```python input_files = [\'file1.txt\', \'file2.txt\'] output_file = \'word_counts.txt\' process_files(input_files, output_file) ``` The `word_counts.txt` should contain: ``` again 1 hello 2 world 2 ``` Notes: - Use appropriate exception handling to ensure robustness. - Ensure the program performs efficiently with both IO-bound and CPU-bound tasks.","solution":"from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor from collections import Counter from typing import List import os def read_file(file_path: str) -> str: try: with open(file_path, \'r\') as file: return file.read() except Exception as e: print(f\\"Error reading file {file_path}: {e}\\") return \\"\\" def count_words(text: str) -> Counter: words = text.split() return Counter(words) def process_files(input_files: List[str], output_file: str, max_workers: int = None) -> None: try: with ThreadPoolExecutor(max_workers=max_workers) as reader_executor: file_contents = list(reader_executor.map(read_file, input_files)) with ProcessPoolExecutor(max_workers=max_workers) as counter_executor: word_counts = list(counter_executor.map(count_words, file_contents)) total_counts = Counter() for wc in word_counts: total_counts.update(wc) with open(output_file, \'w\') as outfile: for word, count in sorted(total_counts.items()): outfile.write(f\\"{word} {count}n\\") except Exception as e: print(f\\"Error processing files: {e}\\")"},{"question":"Custom Python Type with C API Objective: Create a custom Python type `CustomList` using the `PyTypeObject` structure, which behaves like a list but also tracks the number of modifications (adding or removing items) made to it. This exercise will test your understanding of type creation, memory management, implementing protocols, and method definitions in Python\'s C API. Background: You need to create a `CustomList` type with the following properties and methods: 1. **Initialization**: - A `CustomList` starts with an empty list and a modification counter set to zero. 2. **Methods**: - `__init__(self)`: Initializes an empty list and sets the modification counter to zero. - `__str__(self)`: Returns a string representation of the list and the modification counter. - `add_item(self, item)`: Adds an item to the list and increments the modification counter. - `remove_item(self, item)`: Removes the first occurrence of the item from the list (if it exists) and increments the modification counter. - `get_modification_count(self)`: Returns the modification counter. 3. **Sequence Protocol Support**: - Support indexing (e.g., `obj[i]`) and length (e.g., `len(obj)`). 4. **Memory Management**: - Ensure proper memory allocation and deallocation for both the list and the modification counter. 5. **Garbage Collection**: - Implement appropriate garbage collection methods. Constraints: - Use Python C API as detailed in the provided documentation. - Ensure your type can handle arbitrary Python objects in its list. - Properly handle reference counting to avoid memory leaks. Implementation Steps: 1. **Type Declaration**: - Define a new C struct for your `CustomList` type which includes a pointer to a Python list and an integer for the modification counter. 2. **Type Object Initialization**: - Define a `PyTypeObject` for your type, setting up its name, size, and required methods. 3. **Method Definitions**: - Implement methods for initialization, string representation, adding and removing items, and retrieving the modification counter. 4. **Protocol Support**: - Implement sequence protocol methods (like `sq_length` and `sq_item`) to support indexing and getting the lists length. 5. **Memory Management**: - Implement allocation (`tp_alloc`) and deallocation (`tp_dealloc`) functions. 6. **Garbage Collection**: - Implement `tp_traverse` and `tp_clear` methods to support garbage collection. Example Usage: ```python >>> from customlist import CustomList >>> cl = CustomList() >>> cl.add_item(5) >>> cl.add_item(\\"hello\\") >>> print(cl) CustomList: [5, \'hello\'], Modifications: 2 >>> cl.remove_item(5) >>> print(cl) CustomList: [\'hello\'], Modifications: 3 >>> print(cl.get_modification_count()) 3 ``` Deliverables: - A C file implementing the `CustomList` type. - A Python file to test your implementation with the example usage provided above.","solution":"class CustomList: def __init__(self): self._list = [] self._mod_count = 0 def __str__(self): return f\\"CustomList: {self._list}, Modifications: {self._mod_count}\\" def add_item(self, item): self._list.append(item) self._mod_count += 1 def remove_item(self, item): if item in self._list: self._list.remove(item) self._mod_count += 1 def get_modification_count(self): return self._mod_count def __len__(self): return len(self._list) def __getitem__(self, index): return self._list[index]"},{"question":"<|Analysis Begin|> The documentation shares detailed information about the `base64` module in Python, which provides encoding and decoding functionalities for various base encoding schemes like Base16, Base32, Base64, Base85, and Ascii85. The primary aim of this module is to enable safe encoding of binary data to printable ASCII characters and decoding them back to their original binary form. Key points from the documentation: 1. **Encoding Functions**: These convert binary data to a specific base encoding and return the encoded bytes. - `b64encode`, `urlsafe_b64encode` - `b32encode`, `b32hexencode` - `b16encode` - `a85encode`, `b85encode` 2. **Decoding Functions**: These convert encoded data back to its original binary form. - `b64decode`, `urlsafe_b64decode` - `b32decode`, `b32hexdecode` - `b16decode` - `a85decode`, `b85decode` 3. **Parameters and Options**: Various encoding/decoding functions offer additional parameters to customize the encoding/decoding process, such as `altchars`, `casefold`, `map01`, `pad`, `adobe`, etc. 4. **Legacy Interface**: Older methods for encoding/decoding data using file objects and ensuring compliance with older protocols (e.g., RFC 2045). Given this wealth of functionality and the ability to use altchars and various encoding schemes, I can design a challenging question for students to implement a function that utilizes multiple aspects of the `base64` module. <|Analysis End|> <|Question Begin|> **Base64 Data Encoding and Manipulation** You are tasked with implementing a Python function that can perform both encoding and decoding of data using the Base64 scheme with some additional customizations. # Function Specification: Implement a function `custom_base64_process(data: bytes, operation: str, use_urlsafe: bool = False, altchars: bytes = None) -> bytes`. Arguments: 1. **data**: A `bytes` object that is either binary data to be encoded or base64 encoded data to be decoded. 2. **operation**: A `str` indicating the operation to perform. It can be either `\\"encode\\"` for encoding the input data or `\\"decode\\"` for decoding the input data. 3. **use_urlsafe**: A `bool` indicating whether to use the URL-safe base64 alphabet (replacing `+` with `-` and `/` with `_`). Default is `False`. 4. **altchars**: An optional `bytes` object of length 2 specifying alternative characters for `+` and `/`. If provided, it overrides the `use_urlsafe` parameter. Returns: - A `bytes` object containing the encoded or decoded data based on the operation. Constraints and Considerations: - The function should validate the input arguments and raise appropriate exceptions (`ValueError` or `TypeError`) for invalid inputs. - When `operation` is `\\"decode\\"`, handle base64 encoded inputs gracefully, validating and discarding non-alphabet characters unless specified otherwise. - Utilize the `base64` module\'s modern interface for the encoding and decoding. # Examples: ```python # Encoding example data = b\'Hello, World!\' encoded = custom_base64_process(data, \'encode\') print(encoded) # Should print the Base64 encoded bytes corresponding to \'Hello, World!\' # Decoding example encoded_data = b\'SGVsbG8sIFdvcmxkIQ==\' decoded = custom_base64_process(encoded_data, \'decode\') print(decoded) # Should print b\'Hello, World!\' # URL-safe encoding example urlsafe_encoded = custom_base64_process(data, \'encode\', use_urlsafe=True) print(urlsafe_encoded) # Should print URL-safe encoded bytes # Using altchars example altchar_encoded = custom_base64_process(data, \'encode\', altchars=b\'@#\') print(altchar_encoded) # Should print encoded bytes using `@` and `#` instead of `+` and `/` ``` # Notes: - Make sure to handle the case where `altchars` is provided, overriding the `use_urlsafe` parameter. - The function should be robust and handle large data inputs efficiently. **Test your implementation with various inputs to ensure correctness and performance.**","solution":"import base64 def custom_base64_process(data: bytes, operation: str, use_urlsafe: bool = False, altchars: bytes = None) -> bytes: if not isinstance(data, bytes): raise TypeError(\\"data should be of type `bytes`\\") if operation not in [\\"encode\\", \\"decode\\"]: raise ValueError(\\"operation should be either \'encode\' or \'decode\'\\") if altchars is not None and len(altchars) != 2: raise ValueError(\\"altchars should be a bytes object of length 2\\") if operation == \\"encode\\": if altchars: return base64.b64encode(data, altchars=altchars) elif use_urlsafe: return base64.urlsafe_b64encode(data) else: return base64.b64encode(data) elif operation == \\"decode\\": if altchars: return base64.b64decode(data, altchars=altchars) elif use_urlsafe: return base64.urlsafe_b64decode(data) else: return base64.b64decode(data)"},{"question":"# Dataset Analysis and Visualization with Seaborn In this assessment, you are required to utilize the seaborn package to perform dataset analysis and create visualizations that demonstrate your proficiency with this library. You will use the well-known `tips` dataset for this task. The dataset contains information about the tips received by waitstaff in a restaurant. Task 1: Scatter Plot with Customization 1. **Load the `tips` dataset**: Load the dataset using `seaborn.load_dataset(\'tips\')`. 2. **Create a scatter plot**: - Plot the relationship between `total_bill` and `tip` using dots. - Customize the plot by adjusting the edge color of the dots to white. 3. **Reduce overplotting**: - Create a scatter plot showing `total_bill` against `day`, using `sex` as the color variable. - Apply both dodging and jittering to reduce overplotting. 4. **Facet the plot**: - Create faceted scatter plots showing the `total_bill` against `size` using `smoker` as a facet variable. - Limit the x-axis range to focus on values between 0 and 60. Task 2: Customization and Error Bars 1. **Property mapping**: - Create a plot showing the relationship between `total_bill` and `day`. - Customize the points by mapping `color` to `sex` and `marker` to `smoker`. - Use different markers (e.g., \'o\' for smokers and \'s\' for non-smokers) and a `flare` color palette. 2. **Incorporate error bars**: - Add error bars to one of the plots to show the standard error of the mean (`se`) for the totals of bills per day. # Example Code Structure To help you get started, here is a basic structure for the code: ```python import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Task 1: Scatter Plot with Customization # 2. Create a scatter plot between total_bill and tip plot1 = so.Plot(tips, \\"total_bill\\", \\"tip\\") plot1.add(so.Dot()).add(so.Dot(edgecolor=\\"w\\")) # 3. Scatter plot with dodging and jittering plot2 = so.Plot(tips, \\"total_bill\\", \\"day\\", color=\\"sex\\") plot2.add(so.Dot(), so.Dodge(), so.Jitter(0.2)) # 4. Faceted scatter plots plot3 = so.Plot(tips, \\"total_bill\\", \\"size\\").facet(\\"smoker\\").limit(x=(0, 60)) plot3.add(so.Dot()) # Task 2: Customization and Error Bars # 1. Property mapping plot4 = so.Plot(tips, \\"total_bill\\", \\"day\\") plot4.add(so.Dot(pointsize=6), color=\\"sex\\", marker=\\"smoker\\").scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") # 2. Add error bars plot5 = so.Plot(tips, \\"total_bill\\", \\"day\\") plot5.add(so.Dot(pointsize=3), so.Shift(y=0.2), so.Jitter(0.2)) plot5.add(so.Dot(), so.Agg()) plot5.add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ``` # Expected Output You should provide the solution to each task in the form of Python code that produces the described visualizations. Ensure that the plots are clear and well-labeled. Constraints: - Ensure your plots are intuitive and visually appealing. Use appropriate titles, labels, and legends. - Python 3.10 or higher is recommended. - Seaborn and Matplotlib libraries should be properly installed and utilized in your environment. Submission Submit a Jupyter notebook (`.ipynb` file) containing all the code and plots generated as per the tasks described.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Scatter Plot with Customization # 2. Create a scatter plot between total_bill and tip with white edge color dots plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", edgecolor=\\"w\\") plt.title(\\"Scatter Plot of Total Bill vs Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.show() # 3. Scatter plot with dodging and jittering plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", jitter=True, dodge=True) plt.title(\\"Scatter Plot of Total Bill by Day with Dodging and Jittering, colored by Sex\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Sex\\") plt.show() # 4. Faceted scatter plots g = sns.FacetGrid(tips, col=\\"smoker\\", height=6, aspect=1) g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"size\\") g.set(xlim=(0, 60)) g.add_legend() plt.show() # Task 2: Customization and Error Bars # 1. Property mapping plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", style=\\"smoker\\", markers={\\"Yes\\": \\"o\\", \\"No\\": \\"s\\"}, palette=\\"flare\\") plt.title(\\"Scatter Plot of Total Bill by Day with Sex and Smoker Attributes\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.show() # 2. Add error bars plt.figure(figsize=(10, 6)) sns.pointplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", capsize=.1, palette=\\"flare\\", errwidth=1, ci=\\"sd\\") plt.title(\\"Point Plot with Error Bars Showing Standard Error of the Mean Total Bill by Day\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.legend(title=\\"Sex\\") plt.show()"},{"question":"Time Series Resampling with `pandas` Objective: Demonstrate your understanding of time series resampling in `pandas` by implementing a function that processes a time series dataset. Problem Statement: You are provided with a time series dataset that contains minute-by-minute stock prices for a given day. Your task is to implement a function `resample_stock_data` that takes this dataset and performs the following operations: 1. Resample the dataset to 15-minute intervals. 2. Calculate the open, high, low, and close (OHLC) prices for each interval. 3. Forward fill any missing values in the resulting resampled dataframe. Function Signature: ```python import pandas as pd def resample_stock_data(data: pd.DataFrame) -> pd.DataFrame: pass ``` Input: - `data` (pd.DataFrame): A dataframe with two columns: - `timestamp` (datetime): Minute-by-minute timestamps. - `price` (float): Stock prices corresponding to each timestamp. Output: - The function should return a `pd.DataFrame` with the following columns: - `timestamp` (datetime): 15-minute interval timestamps. - `open` (float): Opening price for each interval. - `high` (float): Highest price in each interval. - `low` (float): Lowest price in each interval. - `close` (float): Closing price for each interval. Constraints: 1. The input dataframe `data` can have missing timestamps. 2. The input dataframe `data` is sorted by `timestamp`. 3. You must use the `Resampler` class methods for resampling and aggregating. Example: **Input:** ```python data = pd.DataFrame({ \'timestamp\': pd.date_range(start=\'2021-01-01 09:00:00\', periods=60, freq=\'T\'), \'price\': [100 + i for i in range(60)] }) ``` **Output:** ```python timestamp open high low close 0 2021-01-01 09:00:00 100 106 100 106 1 2021-01-01 09:15:00 107 113 107 113 2 2021-01-01 09:30:00 114 119 114 119 3 2021-01-01 09:45:00 120 125 120 125 ``` Implement the function `resample_stock_data` according to the given specifications.","solution":"import pandas as pd def resample_stock_data(data: pd.DataFrame) -> pd.DataFrame: Resamples the stock data to 15-minute intervals and calculates OHLC prices. Parameters: data (pd.DataFrame): Input dataframe with \'timestamp\' and \'price\' columns. Returns: pd.DataFrame: Resampled dataframe with \'timestamp\', \'open\', \'high\', \'low\', \'close\' columns. # Ensure the dataframe is sorted by timestamp data = data.sort_values(\'timestamp\') # Set the index to the timestamp column data.set_index(\'timestamp\', inplace=True) # Resample to 15-minute intervals and aggregate with OHLC resampled = data[\'price\'].resample(\'15T\').ohlc() # Forward fill any missing values resampled.fillna(method=\'ffill\', inplace=True) # Reset the index to get timestamp back as a column resampled = resampled.reset_index() return resampled"},{"question":"URL Status Checker with Timeout You are tasked with implementing a utility to check the status of a list of URLs concurrently. You will use the `concurrent.futures` module to perform this action efficiently. Your implementation should handle timeouts gracefully and ensure all threads are properly cleaned up. # Function Specification Implement a function `check_urls_status(urls: List[str], timeout: float, max_workers: int) -> Dict[str, Union[int, str]]` where: - `urls` is a list of URL strings to check. - `timeout` is the maximum number of seconds to wait for a response from a URL. - `max_workers` specifies the number of threads to use for concurrent requests. The function should return a dictionary mapping each URL to its HTTP status code or a descriptive error message (e.g., \\"timeout\\", \\"error\\"). # Constraints - A URL should not cause the program to hang indefinitely; handle timeouts appropriately. - Use `ThreadPoolExecutor` for this task. - Utilize `urllib` for making HTTP requests. - Ensure proper exception handling to differentiate between different failure scenarios. # Example ```python import concurrent.futures import urllib.request from typing import List, Dict, Union def check_urls_status(urls: List[str], timeout: float, max_workers: int) -> Dict[str, Union[int, str]]: # Implementation goes here. pass # Example usage urls = [ \'http://www.foxnews.com/\', \'http://www.cnn.com/\', \'http://europe.wsj.com/\', \'http://www.bbc.co.uk/\', \'http://nonexistant-subdomain.python.org/\' ] print(check_urls_status(urls, 5, 5)) # Expected output format (actual statuses may vary): # { # \'http://www.foxnews.com/\': 200, # \'http://www.cnn.com/\': 200, # \'http://europe.wsj.com/\': 200, # \'http://www.bbc.co.uk/\': 200, # \'http://nonexistant-subdomain.python.org/\': \'error\' # or \'timeout\' # } ``` # Notes - Thoroughly test your function with different sets of URLs and varying timeout values. - Ensure thread safety and proper resource cleanup using the context manager (`with` statement) for the `ThreadPoolExecutor`. - Consider edge cases such as empty URL lists, invalid URLs, and network issues.","solution":"import concurrent.futures import urllib.request from typing import List, Dict, Union def check_url(url: str, timeout: float) -> Union[int, str]: try: with urllib.request.urlopen(url, timeout=timeout) as response: return response.getcode() except urllib.request.URLError as e: return \'timeout\' if isinstance(e.reason, socket.timeout) else \'error\' except Exception as e: return \'error\' def check_urls_status(urls: List[str], timeout: float, max_workers: int) -> Dict[str, Union[int, str]]: results = {} with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_url = {executor.submit(check_url, url, timeout): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: result = future.result() except Exception as e: result = \'error\' results[url] = result return results"},{"question":"**Question: Event Scheduler with Task Prioritization** You are required to implement a scheduler that manages multiple tasks, ensuring they are executed at their specified times and respecting their priorities. You will design functions to add tasks, cancel tasks, and run the scheduled tasks. # Requirements 1. **Define a class `TaskScheduler`** that initializes an instance of `sched.scheduler` with `timefunc` as `time.monotonic` and `delayfunc` as `time.sleep`. 2. **Implement a method `schedule_task`** that takes four parameters: - `delay`: Delay in seconds after which the task should be executed. - `priority`: An integer representing the task\'s priority (lower number means higher priority). - `action`: The function to be executed. - `args`: A tuple of arguments to pass to the function (`action`). This method should schedule the task using the `enter` method of `sched.scheduler` and return the event object for potential cancellation. 3. **Implement a method `cancel_task`** that takes an event object as a parameter and cancels the associated task if it is still in the queue. 4. **Implement a method `run`** that starts the execution of scheduled tasks. # Input and Output Formats The following example outlines the expected usage of the `TaskScheduler` class. Example ```python import time class TaskScheduler: def __init__(self): self.scheduler = sched.scheduler(time.monotonic, time.sleep) def schedule_task(self, delay, priority, action, args=()): event = self.scheduler.enter(delay, priority, action, argument=args) return event def cancel_task(self, event): self.scheduler.cancel(event) def run(self): self.scheduler.run() def my_task(message): print(f\\"{time.time()}: {message}\\") # Using TaskScheduler scheduler = TaskScheduler() print(f\\"Current time: {time.time()}\\") # Schedule tasks event1 = scheduler.schedule_task(5, 1, my_task, args=(\\"Task 1\\",)) event2 = scheduler.schedule_task(10, 2, my_task, args=(\\"Task 2\\",)) # Run scheduled tasks scheduler.run() ``` Expected Output: ``` Current time: 1621231231.123123 1621231236.123456: Task 1 1621231241.234567: Task 2 ``` # Constraints and Notes - Ensure your solution handles multiple tasks scheduled at the same or different times with varying priorities. - If a task is canceled, it should not execute. - You may assume `args` passed to `action` are appropriate for the function. Implement the `TaskScheduler` class with the specified methods and demonstrate its usage with the example provided.","solution":"import sched import time class TaskScheduler: def __init__(self): self.scheduler = sched.scheduler(time.monotonic, time.sleep) def schedule_task(self, delay, priority, action, args=()): Schedules a task to be executed after a certain delay with a specified priority. :param delay: Delay in seconds after which the task should be executed. :param priority: Priority of the task (lower number means higher priority). :param action: The function to be executed. :param args: A tuple of arguments to pass to the function (action). :return: The event object representing the scheduled task. event = self.scheduler.enter(delay, priority, action, argument=args) return event def cancel_task(self, event): Cancels a scheduled task, if it is still in the queue. :param event: The event object to be canceled. self.scheduler.cancel(event) def run(self): Runs the scheduled tasks. self.scheduler.run() def my_task(message): print(f\\"{time.time()}: {message}\\")"},{"question":"Objective: The goal of this coding assessment is to evaluate your understanding of scikit-learn\'s out-of-core learning capabilities for text classification. You will stream data from a text file, extract features using the hashing trick, and train an incremental classifier. Task: Write a Python script that: 1. Streams text data from a large file. 2. Uses hashing vectorization to convert text data into numerical features. 3. Trains an incremental classifier on the streaming data using mini-batches. Specifications: - **Input:** - A text file `data.txt` where each line contains a label and a text document separated by a tab character (`labeltdocument`). - Use `SGDClassifier` from scikit-learn for classification. - Use `HashingVectorizer` from scikit-learn for feature extraction. - `mini_batch_size` determines the number of instances to be processed in each mini-batch. - **Output:** - Print the incremental accuracy of the classifier after processing each mini-batch. - **Constraints and Limitations:** - The `data.txt` file can be very large and should be read in a memory-efficient manner. - Use the `partial_fit` method of `SGDClassifier` to enable incremental learning. - **Performance Requirements:** - Ensure that at any given time, only a small portion of the dataset is stored in memory. - Efficiently handle feature extraction and learning in mini-batches. Example Function Signature: ```python import os import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def incremental_text_classification(file_path, mini_batch_size): Perform incremental text classification on data streamed from a file. Parameters: file_path (str): Path to the input text file. mini_batch_size (int): Number of instances to process in each mini-batch. Returns: None # Your implementation here ``` Implementation Notes: 1. **Streaming Data:** Use an efficient method to read the text file line-by-line to avoid loading the entire file into memory. 2. **Feature Extraction:** Utilize `HashingVectorizer` for converting text to numeric features. Since hashing is stateless, it is suitable for streaming data. 3. **Incremental Learning:** Initialize the `SGDClassifier` and update the model using the `partial_fit` method. Make sure to provide all possible classes during the first call to `partial_fit`. Example Workflow: 1. Read the initial mini-batch of data. 2. Transform the data into feature vectors. 3. Train the classifier using `partial_fit`. 4. Continue with the next mini-batch, update the classifier, and print the accuracy.","solution":"import os import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def parse_line(line): Parse a single line from the file into a label and document. Parameters: line (str): A line from the text file in the format \'labeltdocument\'. Returns: tuple: A tuple containing the label and document. label, document = line.split(\'t\', 1) return label, document def incremental_text_classification(file_path, mini_batch_size): Perform incremental text classification on data streamed from a file. Parameters: file_path (str): Path to the input text file. mini_batch_size (int): Number of instances to process in each mini-batch. Returns: None vectorizer = HashingVectorizer(n_features=2 ** 18, alternate_sign=False) classifier = SGDClassifier() classes = np.array([\'neg\', \'pos\']) # Assuming two classes \'neg\' and \'pos\' with open(file_path, \'r\') as file: batch_docs = [] batch_labels = [] for line_num, line in enumerate(file): label, document = parse_line(line) batch_docs.append(document) batch_labels.append(label) if (line_num + 1) % mini_batch_size == 0: X_batch = vectorizer.transform(batch_docs) y_batch = np.array(batch_labels) if line_num < mini_batch_size: classifier.partial_fit(X_batch, y_batch, classes=classes) else: classifier.partial_fit(X_batch, y_batch) # Reset batch lists batch_docs = [] batch_labels = [] # Process the last batch if it exists if batch_docs: X_batch = vectorizer.transform(batch_docs) y_batch = np.array(batch_labels) classifier.partial_fit(X_batch, y_batch) print(\\"Classification done.\\")"},{"question":"# Packaging Utility Simulator You are tasked with creating a utility to filter files based on pattern-matching rules, similar to those used in the source distribution (`sdist`) command of setuptools. Your utility should be able to include or exclude files from a given directory tree based on certain patterns. Task Write a Python function `filter_files(directory, commands)` that filters files in a given directory based on a list of commands. The function should return a list of relative file paths that meet the criteria specified in the commands list. Input - `directory` (str): The root directory of the file tree to filter. - `commands` (list of str): Each command is a string following the format: - `\\"include pattern1 pattern2 ...\\"` - `\\"exclude pattern1 pattern2 ...\\"` - `\\"recursive-include dir pattern1 pattern2 ...\\"` - `\\"recursive-exclude dir pattern1 pattern2 ...\\"` - `\\"global-include pattern1 pattern2 ...\\"` - `\\"global-exclude pattern1 pattern2 ...\\"` - `\\"prune dir\\"` - `\\"graft dir\\"` Note: Commands and patterns are space-separated. Output - A list of relative file paths (str) that meet the inclusion/exclusion criteria specified in the commands. Constraints 1. The list should contain file paths relative to the `directory`. 2. Use Unix-style \\"glob\\" patterns for matching files as described in the documentation. 3. Ensure that the order of operations is maintained as per the sequence of commands. Example Given the following directory structure: ``` root/  file1.txt  file2.log  subdir1/   file3.txt   file4.py  subdir2/  file5.txt  file6.log ``` And the following commands: ```python commands = [ \\"include *.txt\\", \\"exclude *.log\\", \\"recursive-include subdir1 *.py\\", \\"prune subdir2\\" ] ``` The `filter_files` function should return: ```python [\\"file1.txt\\", \\"subdir1/file3.txt\\", \\"subdir1/file4.py\\"] ``` # Implementation Guidelines 1. Read all paths in the `directory` to apply the filters. 2. Use `glob` for pattern matching. 3. Follow the command sequence strictly and modify the inclusion/exclusion list accordingly. 4. Ensure efficient processing and proper handling of all edge cases such as overlapping matches and command precedence.","solution":"import os import glob def filter_files(directory, commands): Filters files in the given directory based on the commands provided. Parameters: - directory (str): The root directory to filter files. - commands (list of str): List of commands specifying include/exclude patterns. Returns: - list of str: List of relative file paths matching the criteria. all_files = set() for root, _, files in os.walk(directory): for file in files: file_path = os.path.relpath(os.path.join(root, file), directory) all_files.add(file_path) included_files = set() for command in commands: cmd_parts = command.split() action = cmd_parts[0] patterns_or_dirs = cmd_parts[1:] if action == \\"include\\": for pattern in patterns_or_dirs: included_files.update({ file for file in all_files if glob.fnmatch.fnmatch(file, pattern) }) elif action == \\"exclude\\": for pattern in patterns_or_dirs: included_files.difference_update({ file for file in all_files if glob.fnmatch.fnmatch(file, pattern) }) elif action == \\"recursive-include\\": dir_path = patterns_or_dirs[0] for pattern in patterns_or_dirs[1:]: included_files.update({ file for file in all_files if file.startswith(dir_path) and glob.fnmatch.fnmatch(file, pattern) }) elif action == \\"recursive-exclude\\": dir_path = patterns_or_dirs[0] for pattern in patterns_or_dirs[1:]: included_files.difference_update({ file for file in all_files if file.startswith(dir_path) and glob.fnmatch.fnmatch(file, pattern) }) elif action == \\"global-include\\": for pattern in patterns_or_dirs: included_files.update({ file for file in all_files if glob.fnmatch.fnmatch(file, pattern) }) elif action == \\"global-exclude\\": for pattern in patterns_or_dirs: included_files.difference_update({ file for file in all_files if glob.fnmatch.fnmatch(file, pattern) }) elif action == \\"prune\\": dir_path = patterns_or_dirs[0] included_files.difference_update({ file for file in all_files if file.startswith(dir_path) }) elif action == \\"graft\\": dir_path = patterns_or_dirs[0] included_files.update({ file for file in all_files if file.startswith(dir_path) }) return sorted(list(included_files))"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of Python\'s data persistence capabilities using the `pickle` and `shelve` modules to implement a simple persistent key-value store. # Problem Statement You are tasked with creating a persistent dictionary-like data store that allows users to add, retrieve, and delete key-value pairs. The data store should use the `shelve` module for persistence which internally uses `pickle` for serializing objects. # Requirements 1. **Function 1: `add_item(db_name, key, value)`** - **Input:** - `db_name` (str): The name of the database file. - `key` (str): The key to be added. - `value` (Any): The value to be associated with the key. - **Output:** - None - **Behavior:** Adds the given key-value pair to the persistent data store. 2. **Function 2: `get_item(db_name, key)`** - **Input:** - `db_name` (str): The name of the database file. - `key` (str): The key to be retrieved. - **Output:** - The value associated with the key if it exists, otherwise `None`. - **Behavior:** Retrieves the value for the given key from the data store. 3. **Function 3: `delete_item(db_name, key)`** - **Input:** - `db_name` (str): The name of the database file. - `key` (str): The key to be deleted. - **Output:** - Returns `True` if the item was successfully deleted, `False` if the key does not exist. - **Behavior:** Deletes the key from the data store if it exists. # Constraints - The `value` can be any Python object that is serializable by the `pickle` module. - The `db_name` should represent a valid filename in the current directory. - Implement robust error handling for common file-related errors (e.g., file not found, read/write permissions). # Example Usage ```python # Adding and retrieving items add_item(\'mystore.db\', \'name\', \'John Doe\') add_item(\'mystore.db\', \'age\', 30) print(get_item(\'mystore.db\', \'name\')) # Output: John Doe print(get_item(\'mystore.db\', \'age\')) # Output: 30 # Deleting an item print(delete_item(\'mystore.db\', \'name\')) # Output: True print(get_item(\'mystore.db\', \'name\')) # Output: None print(delete_item(\'mystore.db\', \'name\')) # Output: False ``` # Notes - You may use the `shelve` module to manage the persistence of the dictionary. - Consider edge cases where keys might not exist or files might be improperly accessed. - Think about performance implications for large datasets and structure your code accordingly.","solution":"import shelve def add_item(db_name, key, value): Adds the given key-value pair to the persistent data store. with shelve.open(db_name) as db: db[key] = value def get_item(db_name, key): Retrieves the value for the given key from the data store. If the key does not exist, returns None. with shelve.open(db_name) as db: return db.get(key) def delete_item(db_name, key): Deletes the key from the data store if it exists. Returns True if the key was successfully deleted, otherwise False. with shelve.open(db_name) as db: if key in db: del db[key] return True return False"},{"question":"# Question You are provided with a dataset `healthexp` containing health expenditure data from various countries over the years. Your task is to write a function that: 1. Loads and preprocesses the dataset. 2. Generates a set of visualizations using Seaborn\'s objects interface (`so`). # Requirements Function Signature ```python import seaborn.objects as so def visualize_healthexp(): pass ``` Input - No input parameters. Output - The function should display the following visualizations: 1. **Faceted Area Plots by Country:** An area plot showing health expenditure over the years for each country, with the plots arranged into facets (subplots) for each country. 2. **Customized Area Plot:** An area plot showing health expenditure over the years, with different colors for each country, and confidence intervals indicated using edge color. 3. **Stacked Area Plot:** A stacked area plot showing part-whole relationships of health expenditure for different countries over the years. Important Notes - Use the Seaborn objects interface (`so`) for creating the plots. - Use the `load_dataset(\\"healthexp\\")` function from Seaborn to load the dataset. - Preprocess the dataset as shown in the provided documentation. - You may add additional customization to make the plots visually appealing. # Example Plots Your function should generate the following plots: 1. **Faceted Area Plots by Country:** ```python p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area()) ``` 2. **Customized Area Plot:** ```python p.add(so.Area(color=\\"Country\\"), color=\\"Country\\") ``` 3. **Stacked Area Plot:** ```python so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\").add(so.Area(alpha=.7), so.Stack()) ``` # Constraints - You should only use the `seaborn` and `pandas` libraries for this task. - You must use Seaborn\'s `objects` interface (`so`) for visualization. # Performance Requirements - The function should run efficiently and generate the plots within a reasonable timeframe. **Test your function by displaying the generated plots.**","solution":"import seaborn.objects as so import seaborn as sns import pandas as pd def preprocess_data(): # Loading the dataset healthexp = sns.load_dataset(\\"healthexp\\") # Displaying the first few rows of the dataset for reference print(healthexp.head()) # Returning the preprocessed dataframe return healthexp def visualize_healthexp(): healthexp = preprocess_data() # Faceted Area Plots by Country p1 = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p1.add(so.Area()) p1.show() # Customized Area Plot p2 = so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") p2.add(so.Area()) p2.show() # Stacked Area Plot p3 = so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\").add(so.Area(alpha=.7), so.Stack()) p3.show()"},{"question":"# Unicode Representation and Conversion in Python Given the importance of efficient Unicode handling in Python, write a Python function that converts a Unicode string from one encoding to another. The function should mimic the behavior of the lower-level C functions described in the documentation. While implementing in Python, the primary focus should be on understanding and using Pythons built-in Unicode capabilities efficiently. Function Signature ```python def convert_unicode_encoding(input_string: str, from_encoding: str, to_encoding: str) -> str: pass ``` Description 1. **Parameters:** - `input_string`: A `str` object representing the input Unicode string. - `from_encoding`: `str` representing the encoding of the input string (for example, \'utf-8\', \'latin-1\', etc.). - `to_encoding`: `str` representing the target encoding you want to convert the string to (for example, \'utf-8\', \'utf-16\', etc.). 2. **Output:** - The function should return the `input_string` re-encoded to `to_encoding`. 3. **Constraints:** - Assume the `input_string` is properly encoded in the specified `from_encoding`. - The function should raise a `ValueError` if the conversion between encodings fails. 4. **Examples:** ```python # Convert from UTF-8 to UTF-16 result = convert_unicode_encoding(\'example\', \'utf-8\', \'utf-16\') print(result) # \'xffxfex65x00x78x00x61x00x6dx00x70x00x6cx00x65x00\' (in bytes) # Convert from Latin-1 to UTF-8 result = convert_unicode_encoding(\'example\', \'latin-1\', \'utf-8\') print(result) # \'example\' ``` 5. **Optimization Notes:** - Ensure that the function handles large strings efficiently. - Consider edge cases where string characters might not be representable in the target encoding. **Hint:** Utilize Python\'s built-in libraries such as `codecs` for handling the encoding conversions. Task Implement the `convert_unicode_encoding` function with the outlined specifications.","solution":"def convert_unicode_encoding(input_string: str, from_encoding: str, to_encoding: str) -> str: Convert a Unicode string from one encoding to another. Args: - input_string (str): The input Unicode string. - from_encoding (str): The encoding of the input string. - to_encoding (str): The target encoding. Returns: - str: The string re-encoded to the target encoding. Raises: - ValueError: If the conversion fails. try: # Decode the input string to get the unicode representation unicode_string = input_string.encode(from_encoding).decode(from_encoding) # Encode the unicode string to the target encoding encoded_string = unicode_string.encode(to_encoding) # Decoding it back to string for consistent API output return encoded_string.decode(to_encoding) except Exception as e: raise ValueError(f\\"Encoding conversion failed: {e}\\")"},{"question":"Objective: Write a Python function that interfaces with the `fcntl` module to manage file locks and file descriptor states. This function should demonstrate an understanding of how to acquire and release file locks, and how to modify file descriptor flags. Function Signature: ```python def manage_file(fd: int, cmd: int, operation: int, start: int = 0, length: int = 0) -> str: Perform the specified file control operation and lock operation on the given file descriptor. Parameters: fd (int): File descriptor of the file. cmd (int): The command to be performed using fcntl (e.g., fcntl.F_SETFL). operation (int): The lock operation to be performed (e.g., fcntl.LOCK_EX). start (int): The starting point for the lock within the file (default is 0). length (int): The length of the lock (default is 0, which means to lock to the end of the file). Returns: str: A message indicating the status of operations. ``` Description: 1. Using `fcntl.fcntl()`, change the file descriptor flags based on the provided `cmd` and ensure the command is applied correctly. 2. Use the `fcntl.lockf()` to acquire and release the specified lock on the given file descriptor as described by the `operation`, `start`, and `length` parameters. 3. Handle any errors appropriately and return an informative message indicating the success or failure of the operations. Constraints: - The function should ensure that file operations are safe and correctly handled. - Errors should be captured and a meaningful message should be provided. Example: ```python import os import fcntl # Simulating file operations with a temporary file fd = os.open(\'tempfile.txt\', os.O_RDWR | os.O_CREAT) try: message = manage_file(fd, fcntl.F_SETFL, fcntl.LOCK_EX) print(message) finally: os.close(fd) os.remove(\'tempfile.txt\') # Clean up the temporary file ``` Expected Output: ``` \\"File descriptor flags set and lock operation successful.\\" ``` Explanation: In this example, the function sets the file descriptor flags and acquires an exclusive lock on the file. If any error occurs during these operations, a respective error message will be returned.","solution":"import os import fcntl def manage_file(fd: int, cmd: int, operation: int, start: int = 0, length: int = 0) -> str: try: # Apply the file descriptor flags command fcntl.fcntl(fd, cmd, fcntl.fcntl(fd, fcntl.F_GETFL) | os.O_NONBLOCK) # Perform the locking operation fcntl.lockf(fd, operation, length, start) return \\"File descriptor flags set and lock operation successful.\\" except IOError as e: return f\\"IOError: {e}\\" except Exception as e: return f\\"Error: {e}\\""},{"question":"# PyTorch HIP Memory Management and Device Utilization You have been given a task to manage tensors on an AMD GPU using PyTorch that has support for HIP. Your task involves creating a function that performs the following: 1. Check whether GPU is available and determine if it is using HIP. 2. If HIP is available: - Create two tensors of random floats, each of size 1000x1000. - Allocate these tensors on the first GPU device (`cuda:0`). - Perform a matrix multiplication of these two tensors. - Release any unused memory cached by PyTorch. - Capture a snapshot of the memory allocator state pre and post matrix multiplication. - Return the memory stats and snapshots. Implement the function `hip_memory_management` based on the above requirements. **Function Signature:** ```python def hip_memory_management() -> dict: pass ``` **Returns:** - A dictionary containing: - `\\"is_hip_available\\"`: A boolean indicating if HIP is available. - `\\"pre_allocation_snapshot\\"`: A snapshot of the memory allocator state before matrix multiplication. - `\\"post_allocation_snapshot\\"`: A snapshot of the memory allocator state after matrix multiplication. - `\\"memory_stats\\"`: A dictionary of memory statistics that includes the following keys: - `\'allocated_memory\'`: Memory allocated by tensors. - `\'max_allocated_memory\'`: Maximum allocated memory. - `\'reserved_memory\'`: Total reserved memory. - `\'max_reserved_memory\'`: Maximum reserved memory. **Example:** ```python results = hip_memory_management() print(results) # Output: { # \'is_hip_available\': True, # \'pre_allocation_snapshot\': \'snapshot_obj\', # \'post_allocation_snapshot\': \'snapshot_obj\', # \'memory_stats\': { # \'allocated_memory\': ..., # \'max_allocated_memory\': ..., # \'reserved_memory\': ..., # \'max_reserved_memory\': ... # } # } ``` **Constraints:** - Perform all tensor operations using PyTorch\'s device interface ensuring usage of HIP interfaces. - Utilize the provided methods for memory management and snapshot capturing. - Ensure code compatibility with systems that can switch between CUDA and HIP seamlessly. **Performance Requirements:** - Ensure the memory allocation and deallocation processes are efficiently handled to avoid any unnecessary delay. - Avoid unnecessary tensor transfers between CPU and GPU to maintain optimal performance.","solution":"import torch def hip_memory_management() -> dict: Manages tensor allocations and memory usage on an AMD GPU with HIP support. Returns a dictionary with memory statistics and snapshots of allocator state. if torch.cuda.is_available() and torch.version.hip: is_hip_available = True device = torch.device(\'cuda:0\') # Creating two random tensors of size 1000x1000 on GPU A = torch.randn(1000, 1000, device=device) B = torch.randn(1000, 1000, device=device) # Taking the pre-allocation snapshot pre_allocation_snapshot = torch.cuda.memory.snapshot() # Performing matrix multiplication C = torch.matmul(A, B) # Taking the post-allocation snapshot post_allocation_snapshot = torch.cuda.memory.snapshot() # Freeing any unused memory torch.cuda.empty_cache() # Memory stats memory_stats = { \'allocated_memory\': torch.cuda.memory_allocated(device), \'max_allocated_memory\': torch.cuda.max_memory_allocated(device), \'reserved_memory\': torch.cuda.memory_reserved(device), \'max_reserved_memory\': torch.cuda.max_memory_reserved(device) } return { \\"is_hip_available\\": is_hip_available, \\"pre_allocation_snapshot\\": pre_allocation_snapshot, \\"post_allocation_snapshot\\": post_allocation_snapshot, \\"memory_stats\\": memory_stats } else: # HIP is not available or GPU is not available return { \\"is_hip_available\\": False, \\"pre_allocation_snapshot\\": None, \\"post_allocation_snapshot\\": None, \\"memory_stats\\": None }"},{"question":"Objective To assess your ability to use scikit-learn transformers to preprocess data and combine multiple transformers into a pipeline for efficient data transformation. Problem Statement You are given a training dataset `train_data.csv` and a test dataset `test_data.csv`. The dataset contains numerical and categorical features that need to be preprocessed before feeding into a machine learning model. Your task is to build a preprocessing pipeline using scikit-learn transformers and apply it to both the training and test datasets. Expected Input and Output Formats - Input: - A CSV file `train_data.csv` containing the training data. - A CSV file `test_data.csv` containing the test data. - Output: - Preprocessed training data as a NumPy array or DataFrame. - Preprocessed test data as a NumPy array or DataFrame. Constraints 1. Both datasets contain missing values that need to be imputed. 2. The numerical features should be scaled using standard scaling. 3. The categorical features should be one-hot encoded. 4. The pipeline should handle unseen categories in the test data gracefully. 5. The pipeline should be implemented using scikit-learn transformers. Performance Requirement The solution should efficiently preprocess the data without unnecessary transformations or overfitting to the training data distribution. Implementation Details 1. Load the training and test datasets. 2. Identify numerical and categorical columns. 3. Build a preprocessing pipeline using scikit-learn\'s `Pipeline` and `ColumnTransformer`. 4. Apply the pipeline to the training dataset and fit it. 5. Use the fitted pipeline to transform the test dataset. Example ```python import pandas as pd from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder # Load the data train_data = pd.read_csv(\'train_data.csv\') test_data = pd.read_csv(\'test_data.csv\') # Identify numerical and categorical columns numerical_columns = train_data.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_columns = train_data.select_dtypes(include=[\'object\']).columns # Define the numerical transformer numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) # Define the categorical transformer categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine the transformers into a preprocessor preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_columns), (\'cat\', categorical_transformer, categorical_columns) ]) # Fit the preprocessor on the training data and transform it train_data_preprocessed = preprocessor.fit_transform(train_data) # Transform the test data using the pre-fitted preprocessor test_data_preprocessed = preprocessor.transform(test_data) # Output the preprocessed data print(train_data_preprocessed) print(test_data_preprocessed) ``` Create a script or function using the code structure above to preprocess the provided datasets.","solution":"import pandas as pd from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder def preprocess_data(train_file, test_file): Preprocesses the data from the provided train and test CSV files. Parameters: train_file (str): Path to the training data CSV file. test_file (str): Path to the test data CSV file. Returns: tuple: Preprocessed training and test data as NumPy arrays. # Load the data train_data = pd.read_csv(train_file) test_data = pd.read_csv(test_file) # Identify numerical and categorical columns numerical_columns = train_data.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_columns = train_data.select_dtypes(include=[\'object\']).columns # Define the numerical transformer numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) # Define the categorical transformer categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine the transformers into a preprocessor preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_columns), (\'cat\', categorical_transformer, categorical_columns) ]) # Fit the preprocessor on the training data and transform it train_data_preprocessed = preprocessor.fit_transform(train_data) # Transform the test data using the pre-fitted preprocessor test_data_preprocessed = preprocessor.transform(test_data) return train_data_preprocessed, test_data_preprocessed"},{"question":"You are tasked with building a `MultimediaHandler` class using the `mailcap` module to manage MIME types and their associated commands. # Class: `MultimediaHandler` Methods: 1. **`__init__(self)`**: - Initializes the `MultimediaHandler` object and retrieves the system\'s mailcap capabilities. 2. **`get_command(self, mime_type: str, key: str = \'view\', filename: str = \'/dev/null\', parameters: dict = {}) -> str`**: - Fetches the command for the given MIME type and usage key. - **Args**: - `mime_type` (str): The MIME type to find a match for. - `key` (str, optional): The field desired (default is `\'view\'`). - `filename` (str, optional): The filename to be substituted for `%s` in the command line. - `parameters` (dict, optional): Dictionary of named parameters to substitute in the command line. - **Returns**: A string containing the command to be executed or `None` if no matching MIME type is found. # Constraints - Ensure that special characters in filenames and parameters are safely handled as described in the documentation. - The method should raise a `ValueError` if the `filename` contains any disallowed characters. # Example Usage ```python # Example Initialization handler = MultimediaHandler() # Example Command Fetch command = handler.get_command(\'video/mpeg\', filename=\'movie.mp4\') print(command) # Output may look like \'xmpeg movie.mp4\' if the command is available in the mailcap # Example Command Fetch with Parameters command = handler.get_command( \'application/x-partial\', filename=\'partfile\', parameters={\'id\': \'1\', \'number\': \'2\', \'total\': \'3\'} ) print(command) # Output could look like \'showpartial 1 2 3\' ``` # Notes - Use the `mailcap.getcaps()` function to get the mailcap capabilities dictionary. - Use the `mailcap.findmatch()` function to find the appropriate command. - Ensure to handle the constraints and edge cases as described in the documentation. Submission Submit the `MultimediaHandler` class implementation adhering to the above specifications.","solution":"import re import mailcap class MultimediaHandler: def __init__(self): # Retrieves the system\'s mailcap capabilities self.capabilities = mailcap.getcaps() def get_command(self, mime_type: str, key: str = \'view\', filename: str = \'/dev/null\', parameters: dict = {}) -> str: # Ensures filename does not contain disallowed characters if re.search(r\'[^a-zA-Z0-9._/-]\', filename): raise ValueError(\\"Filename contains disallowed characters\\") # Find the suitable command for the given MIME type command, entry = mailcap.findmatch(self.capabilities, mime_type, key=key, filename=filename, plist=parameters) return command"},{"question":"# Advanced JSON Serialization and Customization in Python Problem Statement You\'ve been tasked with creating a complex data processing system that involves encoding and decoding custom Python objects to and from JSON format. Specifically, you need to handle instances of a custom `Vector` class, which represents vectors in a 2D or 3D space. You are required to implement two main functionalities: 1. **Serialization**: Encode instances of the `Vector` class to JSON format. 2. **Deserialization**: Decode JSON strings back into instances of the `Vector` class. Your implementation should also consider proper handling of different input structures, ensuring the data integrity and type safety throughout the process. Specifications 1. **Class Definition:** ```python class Vector: def __init__(self, x, y, z=None): self.x = x self.y = y self.z = z def __repr__(self): return f\\"Vector(x={self.x}, y={self.y}, z={self.z})\\" def to_dict(self): if self.z is None: return {\\"x\\": self.x, \\"y\\": self.y} else: return {\\"x\\": self.x, \\"y\\": self.y, \\"z\\": self.z} @staticmethod def from_dict(d): if \\"z\\" in d: return Vector(d[\\"x\\"], d[\\"y\\"], d[\\"z\\"]) else: return Vector(d[\\"x\\"], d[\\"y\\"]) ``` 2. **Custom Encoder and Decoder:** - Implement a custom JSON encoder `VectorEncoder` that inherits from `json.JSONEncoder` to properly handle `Vector` objects. - Implement a custom JSON decoder function `decode_vector`, to convert JSON object literals back into `Vector` instances using `json.loads`. Input and Output 1. **Serialization:** - **Input:** A `Vector` object. - **Output:** A JSON formatted string representing the `Vector`. 2. **Deserialization:** - **Input:** A JSON formatted string representing a `Vector`. - **Output:** A `Vector` object. Constraints - You need to ensure the JSON representation is compact without unnecessary white spaces. - You should handle invalid JSON inputs gracefully, ensuring the program does not crash. Performance - The solution should efficiently handle a large number of `Vector` objects in serialization and deserialization. # Example Usage ```python # Example Vector instances v1 = Vector(1, 2) v2 = Vector(3, 4, 5) # Serialization to JSON json_str1 = json.dumps(v1, cls=VectorEncoder) json_str2 = json.dumps(v2, cls=VectorEncoder) print(json_str1) # Output: {\\"x\\": 1, \\"y\\": 2} print(json_str2) # Output: {\\"x\\": 3, \\"y\\": 4, \\"z\\": 5} # Deserialization from JSON vector1 = json.loads(json_str1, object_hook=decode_vector) vector2 = json.loads(json_str2, object_hook=decode_vector) print(vector1) # Output: Vector(x=1, y=2, z=None) print(vector2) # Output: Vector(x=3, y=4, z=5) ``` # Required Implementations 1. **Custom Encoder Class:** ```python import json class VectorEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Vector): return obj.to_dict() return super(VectorEncoder, self).default(obj) ``` 2. **Custom Decoder Function:** ```python import json def decode_vector(d): if \'x\' in d and \'y\' in d: return Vector.from_dict(d) return d ``` # Submission Instructions - Implement the `Vector` class along with the `to_dict` and `from_dict` methods. - Implement the `VectorEncoder` class for handling custom serialization. - Implement the `decode_vector` function for handling custom deserialization. - Ensure your solution is efficient and handles edge cases appropriately.","solution":"import json class Vector: def __init__(self, x, y, z=None): self.x = x self.y = y self.z = z def __repr__(self): return f\\"Vector(x={self.x}, y={self.y}, z={self.z})\\" def to_dict(self): if self.z is None: return {\\"x\\": self.x, \\"y\\": self.y} else: return {\\"x\\": self.x, \\"y\\": self.y, \\"z\\": self.z} @staticmethod def from_dict(d): if \\"z\\" in d: return Vector(d[\\"x\\"], d[\\"y\\"], d[\\"z\\"]) else: return Vector(d[\\"x\\"], d[\\"y\\"]) class VectorEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Vector): return obj.to_dict() return super(VectorEncoder, self).default(obj) def decode_vector(d): if \'x\' in d and \'y\' in d: return Vector.from_dict(d) return d"},{"question":"Objective: Implement and evaluate a Naive Bayes classifier using the scikit-learn library to classify a real-world dataset. Students will demonstrate their ability to select, implement, and evaluate different types of Naive Bayes classifiers. Problem Statement: You are given a dataset of SMS messages that are labeled as either \'spam\' or \'ham\' (not spam). Your task is to build a Naive Bayes classifier to classify the SMS messages into \'spam\' or \'ham\'. You are required to use the `MultinomialNB` classifier from scikit-learn, preprocess the data appropriately, and evaluate the performance of your model using metrics such as accuracy, precision, recall, and F1-score. **Steps to follow:** 1. Load and preprocess the dataset. 2. Split the dataset into training and testing sets. 3. Implement the `MultinomialNB` classifier to train your model. 4. Evaluate the performance of the model on the test set. Dataset: You can download the SMS Spam Collection dataset from [here](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). The dataset is a collection of SMS messages labeled as \'spam\' or \'ham\'. Each line in the file represents one message, with the first word in the line being the label (`spam` or `ham`) followed by a tab (`t`) character and then the message. Detailed Steps and Requirements: 1. **Data Preprocessing:** - Load the dataset into a pandas DataFrame. - Separate the labels and the messages. - Use `CountVectorizer` from `sklearn.feature_extraction.text` to transform the text data into numerical data (term frequency counts). - Optionally, you may use `TfidfTransformer` to transform the count matrix to a normalized `tf` or `tf-idf` representation. 2. **Split the Data:** - Split the data into training and testing sets using `train_test_split` from `sklearn.model_selection`. 3. **Model Training:** - Implement the `MultinomialNB` classifier from `sklearn.naive_bayes`. - Train the model using the training data. 4. **Model Evaluation:** - Predict the labels for the test data. - Evaluate the predictions using accuracy, precision, recall, and F1-score metrics from `sklearn.metrics`. Input and Output formats: - **Input:** - The path to the dataset file. - **Output:** - The classification performance metrics (accuracy, precision, recall, F1-score). Constraints: - Your code should handle possible exceptions such as file-not-found errors and incorrect formats in the dataset. - Ensure that the preprocessing steps are efficient and do not result in memory errors. Example: ```python # Load the dataset df = pd.read_csv(\'path/to/sms_spam_collection.txt\', sep=\'t\', header=None, names=[\'label\', \'message\']) # Preprocess the data from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfTransformer count_vect = CountVectorizer() X_counts = count_vect.fit_transform(df[\'message\']) tfidf_transformer = TfidfTransformer() X_tfidf = tfidf_transformer.fit_transform(X_counts) # Split the data from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df[\'label\'], test_size=0.3, random_state=42) # Implement the classifier from sklearn.naive_bayes import MultinomialNB clf = MultinomialNB() clf.fit(X_train, y_train) # Predict and Evaluate from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score y_pred = clf.predict(X_test) print(\\"Accuracy:\\", accuracy_score(y_test, y_pred)) print(\\"Precision:\\", precision_score(y_test, y_pred, pos_label=\'spam\')) print(\\"Recall:\\", recall_score(y_test, y_pred, pos_label=\'spam\')) print(\\"F1-score:\\", f1_score(y_test, y_pred, pos_label=\'spam\')) ```","solution":"import pandas as pd from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_dataset(file_path): Load the dataset from a file and return it as a pandas DataFrame. try: df = pd.read_csv(file_path, sep=\'t\', header=None, names=[\'label\', \'message\']) return df except FileNotFoundError: raise FileNotFoundError(f\\"The file at path {file_path} was not found.\\") except Exception as e: raise ValueError(f\\"An error occurred while loading the dataset: {e}\\") def preprocess_data(df): Preprocess the data: separate labels and messages, and transform the text data into numerical data. count_vect = CountVectorizer() X_counts = count_vect.fit_transform(df[\'message\']) tfidf_transformer = TfidfTransformer() X_tfidf = tfidf_transformer.fit_transform(X_counts) return X_tfidf, df[\'label\'] def split_data(X, y, test_size=0.3, random_state=42): Split the data into training and testing sets. return train_test_split(X, y, test_size=test_size, random_state=random_state) def train_naive_bayes(X_train, y_train): Train the MultinomialNB classifier and return the trained model. clf = MultinomialNB() clf.fit(X_train, y_train) return clf def evaluate_model(clf, X_test, y_test): Evaluate the model performance using accuracy, precision, recall, and F1-score. y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, pos_label=\'spam\') recall = recall_score(y_test, y_pred, pos_label=\'spam\') f1 = f1_score(y_test, y_pred, pos_label=\'spam\') return accuracy, precision, recall, f1 def main(file_path): Main function to load, preprocess, train, and evaluate the SMS spam classifier. # Load the dataset df = load_dataset(file_path) # Preprocess the data X, y = preprocess_data(df) # Split the data X_train, X_test, y_train, y_test = split_data(X, y) # Train the model clf = train_naive_bayes(X_train, y_train) # Evaluate the model accuracy, precision, recall, f1 = evaluate_model(clf, X_test, y_test) return accuracy, precision, recall, f1 if __name__ == \\"__main__\\": file_path = \'path/to/sms_spam_collection.txt\' accuracy, precision, recall, f1 = main(file_path) print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"F1 Score: {f1}\\")"},{"question":"# PyTorch Coding Challenge: Constructing Export IR Graph **Objective**: Write a Python function using PyTorch\'s Export IR framework. The function should create a computational graph for a given PyTorch model and extract specific metadata from the graph nodes. **Problem Statement**: You are provided with a PyTorch neural network model structure. Your task is to: 1. Construct an Export IR graph from this model. 2. Extract specific metadata from the nodes in the constructed graph. 3. Return a summary of this metadata in a structured format. **Function Signature**: ```python def export_ir_metadata(model: torch.nn.Module, example_args: Tuple[torch.Tensor, ...]) -> Dict[str, Any]: pass ``` **Input**: - `model`: An instance of `torch.nn.Module` representing the PyTorch model. - `example_args`: A tuple of example input tensors to trace the model. **Output**: - A dictionary containing: - `graph_structure`: A string representation of the Export IR graph. - `nodes_metadata`: A list of dictionaries where each dictionary represents the metadata of a node in the graph with fields: `{ \\"name\\": str, \\"op_name\\": str, \\"stack_trace\\": str, \\"val\\": Any, \\"nn_module_stack\\": Dict[str, Any], \\"source_fn_stack\\": Dict[str, Any] }`. **Constraints**: - You should only use the nodes of type `call_function`. - Ensure that the output includes at least one node of `call_function` type. **Example**: ```python import torch from torch import nn from typing import Tuple, Dict, Any class SimpleModel(nn.Module): def forward(self, x, y): return x + y model = SimpleModel() example_args = (torch.randn(3, 3), torch.randn(3, 3)) result = export_ir_metadata(model, example_args) # Expected structure of result: # { # \\"graph_structure\\": \\"string representation of the graph\\", # \\"nodes_metadata\\": [ # { # \\"name\\": \\"add1\\", # \\"op_name\\": \\"call_function\\", # \\"stack_trace\\": \\"file and line numbers\\", # \\"val\\": FakeTensor or tuple of FakeTensors, # \\"nn_module_stack\\": {\\"self_module\\": (name, module_class)}, # \\"source_fn_stack\\": {\\"source_fn\\": \\"source_function_or_module\\"} # }, # ... # ] # } ``` **Additional Notes**: - Use the `torch.fx` and `torch.export` modules for creating and manipulating the Export IR graph. - Pay attention to the metadata fields and ensure they are extracted correctly. **Hints**: - Refer to PyTorch documentation on `torch.fx`, `torch.export`, and `torch.nn.Module`. - Use `torch.export.export` to generate the exported program and obtain the graph. - Traverse the graph to extract node-specific metadata.","solution":"import torch from torch import nn from torch.fx import GraphModule, symbolic_trace from typing import Tuple, Dict, Any def export_ir_metadata(model: nn.Module, example_args: Tuple[torch.Tensor, ...]) -> Dict[str, Any]: Generates the export IR graph and extracts metadata from the graph nodes. Parameters: - model: An instance of torch.nn.Module representing the PyTorch model. - example_args: A tuple of example input tensors to trace the model. Returns: - A dictionary containing: * graph_structure: A string representation of the Export IR graph. * nodes_metadata: A list of dictionaries representing the metadata of each node. # Trace the model to create an FX Graph traced = symbolic_trace(model) # Generate a string representation of the graph graph_structure = str(traced.graph) # Extract nodes metadata nodes_metadata = [] for node in traced.graph.nodes: if node.op == \\"call_function\\": nodes_metadata.append({ \\"name\\": node.name, \\"op_name\\": node.op, \\"stack_trace\\": str(node.stack_trace), \\"val\\": str(node.target), \\"nn_module_stack\\": {}, \\"source_fn_stack\\": {} }) return { \\"graph_structure\\": graph_structure, \\"nodes_metadata\\": nodes_metadata } # Example test instantiation if __name__ == \\"__main__\\": class SimpleModel(nn.Module): def forward(self, x, y): return x + y model = SimpleModel() example_args = (torch.randn(3, 3), torch.randn(3, 3)) result = export_ir_metadata(model, example_args) print(result)"},{"question":"# Seaborn Swarm Plot Assessment **Objective:** Demonstrate your understanding of the seaborn library by creating and customizing swarm plots to visualize and analyze data. **Problem Statement:** You are provided with a dataset containing information about tips received by waiters at a restaurant. Your task is to write a Python function called `plot_seaborn_swarm` that performs the following: 1. Load the `tips` dataset from seaborn. 2. Create and customize a swarm plot that shows the distribution of `total_bill` across different `days` of the week. 3. Use a different color (`hue`) to distinguish between different `sex` categories. 4. Customize the plot to use the \\"deep\\" palette. 5. Adjust the points in the plot to ensure they do not overlap by setting `size` to 4. 6. Create a faceted plot of the same swarm plot using `catplot` with columns for each `day`. # Function Signature ```python def plot_seaborn_swarm(): pass ``` # Output - The function should generate and display the customized swarm plot and the faceted plot as described. # Constraints and Requirements - You must use the seaborn library for all plotting. - Ensure that the plots are well-labeled with titles and axis labels for clarity. - Make use of seaborn parameters efficiently to customize the plots. - Follow good coding practices and comment your code where necessary. # Example Your `plot_seaborn_swarm()` function should display swarm plots similar to the following example: ![Example Plot](example_plot.png) (This image is for reference; ensure your plots align with the requirements mentioned above.) **Note:** No need to return any values from the function. The primary goal is to generate and display the required plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_seaborn_swarm(): # Load the tips dataset from seaborn tips = sns.load_dataset(\\"tips\\") # Creating the swarm plot plt.figure(figsize=(10, 6)) swarm_plot = sns.swarmplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips, palette=\\"deep\\", size=4) swarm_plot.set_title(\'Total Bill Distribution by Day and Sex\') swarm_plot.set_xlabel(\'Day of the Week\') swarm_plot.set_ylabel(\'Total Bill\') plt.legend(title=\\"Sex\\") plt.show() # Creating the faceted plot using `catplot` facet_plot = sns.catplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", data=tips, palette=\\"deep\\", kind=\\"swarm\\", size=4) facet_plot.set_titles(\\"{col_name} Day\\") facet_plot.set_axis_labels(\\"Day of the Week\\", \\"Total Bill\\") plt.show()"},{"question":"**Seaborn Plot Customization Assignment** # Objective: Your task is to demonstrate your understanding of Seaborn\'s `set_context` function and how to customize plots by setting different contexts, scaling font elements, and overriding specific parameters. # Problem Statement: You are given three different settings under which you need to create line plots using Seaborn. Each setting will require you to: 1. Set the context. 2. Optionally scale the font elements. 3. Override specific parameters as provided. # Requirements: 1. **Input:**  A list of x-values.  A list of y-values.  A dictionary containing context settings. The dictionary will have three keys: `context_name`, `font_scale`, and `custom_params`. ```python settings_1 = { \\"context_name\\": \\"paper\\", \\"font_scale\\": 1.0, \\"custom_params\\": {\\"lines.linewidth\\": 2} } settings_2 = { \\"context_name\\": \\"talk\\", \\"font_scale\\": 1.5, \\"custom_params\\": {\\"lines.linewidth\\": 1.5, \\"axes.labelsize\\": \\"large\\"} } settings_3 = { \\"context_name\\": \\"poster\\", \\"font_scale\\": 2.0, \\"custom_params\\": {} } ``` 2. **Output:**  For each setting, generate and display a Seaborn line plot using the x and y values provided. # Constraints: - The `x` and `y` lists will have the same length and will contain numeric values. - The `context_name` will always be one of the following: `paper`, `notebook`, `talk`, `poster`. - The `font_scale` will be a positive floating-point number. - The `custom_params` dictionary may contain valid keys for the Seaborn `rc` parameter. # Example: Here is a script that performs these tasks. You should implement the function `create_custom_plot` to do the following: 1. Set the context using the given settings. 2. Create a line plot using Seaborn with the specified context, font scale, and custom parameters. ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(x, y, settings): sns.set_context(settings[\\"context_name\\"], font_scale=settings[\\"font_scale\\"], rc=settings[\\"custom_params\\"]) sns.lineplot(x=x, y=y) plt.show() # Example usage: x = [0, 1, 2, 3, 4] y = [1, 3, 2, 5, 4] settings_1 = { \\"context_name\\": \\"paper\\", \\"font_scale\\": 1.0, \\"custom_params\\": {\\"lines.linewidth\\": 2} } settings_2 = { \\"context_name\\": \\"talk\\", \\"font_scale\\": 1.5, \\"custom_params\\": {\\"lines.linewidth\\": 1.5, \\"axes.labelsize\\": \\"large\\"} } settings_3 = { \\"context_name\\": \\"poster\\", \\"font_scale\\": 2.0, \\"custom_params\\": {} } # Create plots with different settings create_custom_plot(x, y, settings_1) create_custom_plot(x, y, settings_2) create_custom_plot(x, y, settings_3) ``` # Notes: - Ensure your script is well-commented and clearly explains each step. - Use the provided example to test your implementation. - The `create_custom_plot` function should strictly adhere to the input and output requirements. Good luck, and happy coding!","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(x, y, settings): Creates a Seaborn line plot using the specified context, font scale, and custom parameters. Parameters: x (list): A list of x-values. y (list): A list of y-values. settings (dict): A dictionary containing context settings with keys \'context_name\', \'font_scale\', and \'custom_params\'. sns.set_context(settings[\\"context_name\\"], font_scale=settings[\\"font_scale\\"], rc=settings[\\"custom_params\\"]) sns.lineplot(x=x, y=y) plt.show()"},{"question":"**Objective**: To assess students\' ability to use the `struct` module for packing and unpacking binary data, and to apply format strings correctly while managing buffers. **Question**: You are working with a sensor setup where each sensor sends data as byte packets. Each packet contains: - A sensor ID (1 unsigned byte) - A timestamp (unsigned long, 4 bytes) - A reading (float, 4 bytes) - A status flag (boolean, 1 byte) You need to design a function that packs multiple sensor data records into a bytes buffer and then unpacks the buffer to retrieve the original records. The function should handle multiple records and preserve their order. **Task**: 1. Implement the function `pack_sensor_data(records: List[Dict[str, Any]]) -> bytes:` which: - Takes a list of dictionaries, where each dictionary represents one sensor record. - Each dictionary contains keys: `sensor_id`, `timestamp`, `reading`, `status`. - Packs the data into a bytes object using the format string `\'<BIf?\'` (little-endian). 2. Implement the function `unpack_sensor_data(buffer: bytes) -> List[Dict[str, Any]]:` which: - Takes the packed buffer produced by `pack_sensor_data`. - Unpacks the buffer to retrieve the list of dictionaries. - Each dictionary should have the keys `sensor_id`, `timestamp`, `reading`, `status`. **Constraints**: - You can assume that all input data will be valid and within the ranges specified by their respective types. - Handle multiple records; the number of records in the buffer should match the number provided in the input list for packing. **Examples**: ```python from typing import List, Dict, Any def pack_sensor_data(records: List[Dict[str, Any]]) -> bytes: # Your implementation here pass def unpack_sensor_data(buffer: bytes) -> List[Dict[str, Any]]: # Your implementation here pass # Sample Input sensor_records = [ {\\"sensor_id\\": 1, \\"timestamp\\": 1633036800, \\"reading\\": 23.5, \\"status\\": True}, {\\"sensor_id\\": 2, \\"timestamp\\": 1633036801, \\"reading\\": 19.0, \\"status\\": False}, ] # Packing the data packed_data = pack_sensor_data(sensor_records) print(packed_data) # Should print a bytes object # Unpacking the data unpacked_records = unpack_sensor_data(packed_data) print(unpacked_records) # Should print: # [ # {\\"sensor_id\\": 1, \\"timestamp\\": 1633036800, \\"reading\\": 23.5, \\"status\\": True}, # {\\"sensor_id\\": 2, \\"timestamp\\": 1633036801, \\"reading\\": 19.0, \\"status\\": False}, # ] ``` Ensure your functions handle the packing and unpacking correctly and maintain the integrity of the data. **Grading Criteria**: - Correct use of the `struct` module functions and format strings. - Accurate implementation of packing and unpacking logic. - Handling multiple records and correctly preserving their order and structure.","solution":"import struct from typing import List, Dict, Any def pack_sensor_data(records: List[Dict[str, Any]]) -> bytes: buffer = bytearray() for record in records: buffer.extend(struct.pack(\'<BIf?\', record[\'sensor_id\'], record[\'timestamp\'], record[\'reading\'], record[\'status\'])) return bytes(buffer) def unpack_sensor_data(buffer: bytes) -> List[Dict[str, Any]]: records = [] record_size = struct.calcsize(\'<BIf?\') for i in range(0, len(buffer), record_size): chunk = buffer[i:i+record_size] sensor_id, timestamp, reading, status = struct.unpack(\'<BIf?\', chunk) records.append({ \'sensor_id\': sensor_id, \'timestamp\': timestamp, \'reading\': reading, \'status\': status }) return records"},{"question":"# Command Line Argument Handling and Interactive Mode You are tasked with creating a Python script that demonstrates your understanding of command-line argument handling and interactive mode in Python 3.10. This script should provide a flexible way of computing basic arithmetic operations based on command-line inputs and should also allow entering an interactive mode for further operations. Requirements: 1. **Arithmetic Operations**: - The script should accept two numerical values and an operation type (`add`, `subtract`, `multiply`, `divide`) as command-line arguments. - It should perform the operation and display the result. 2. **Interactive Mode**: - After performing the initial operation, the script should enter interactive mode if the `-i` flag is provided before the script name. - In interactive mode, prompt the user to input additional arithmetic operations. The prompt should follow the format `>>>` for primary input and `...` for multiline input. - If the user enters an invalid command, the script should display an error message but continue running. 3. **Usage Examples**: - Command-Line: `python3.10 script.py add 5 10` - Command-Line with Interactive Mode: `python3.10 -i script.py multiply 7 3` - In interactive mode, the user should be able to perform further operations and see the results immediately. 4. **Constraints**: - Do not use any third-party libraries. - Handle division by zero by displaying an appropriate error message. 5. **Performance**: - Ensure that the script handles inputs and operations efficiently. Example Execution: ```sh python3.10 script.py add 5 10 Result: 15 python3.10 -i script.py multiply 7 3 Result: 21 >>> >>> add 4 5 Result: 9 >>> divide 10 0 Error: Division by zero is not allowed. >>> quit() ``` Implement the script according to the requirements specified above. The script should be named `script.py`.","solution":"import sys def add(a, b): return a + b def subtract(a, b): return a - b def multiply(a, b): return a * b def divide(a, b): if b == 0: return \\"Error: Division by zero is not allowed.\\" return a / b operations = { \\"add\\": add, \\"subtract\\": subtract, \\"multiply\\": multiply, \\"divide\\": divide } def perform_operation(operation, a, b): if operation not in operations: return \\"Error: Unsupported operation.\\" return operations[operation](a, b) if __name__ == \\"__main__\\": interactive = False args = sys.argv[1:] if \\"-i\\" in args: interactive = True args.remove(\\"-i\\") if len(args) != 3: print(\\"Usage: <operation> <num1> <num2>\\") sys.exit(1) operation, a, b = args[0], float(args[1]), float(args[2]) result = perform_operation(operation, a, b) print(f\\"Result: {result}\\") if interactive: while True: try: user_input = input(\\">>> \\") if user_input.strip().lower() == \\"quit\\": break parts = user_input.split() if len(parts) != 3: print(\\"Usage: <operation> <num1> <num2>\\") continue operation, a, b = parts[0], float(parts[1]), float(parts[2]) result = perform_operation(operation, a, b) print(f\\"Result: {result}\\") except (ValueError, IndexError): print(\\"Error: Invalid command.\\")"},{"question":"Objective You are tasked with creating a comprehensive data visualization using seaborn that showcases your understanding of scales, non-linear transformations, and the semantic mapping of colors in visualizations. Dataset Use the `diamonds` dataset available in seaborn. Requirements 1. **Scatter Plot with Non-Linear Transformation**: - Create a scatter plot of `carat` vs. `price`. - Apply a logarithmic scale to the y-axis and ensure that the axis limits are set to `(250, 25000)`. - Add a second-order polynomial regression line (use `PolyFit(order=2)`) to visualize the general trend. 2. **Color Mapping**: - Map the data points by the `clarity` column and use a qualitative color palette (`\\"Set3\\"`). - Ensure the mapped colors make distinctions between the different categories of `clarity`. 3. **Point Size Proportional to Carat**: - Point sizes should be proportional to the `carat` values. - Limit the point sizes to range from 2 to 10. 4. **Interactive Legend and Axis Labels**: - Include a legend that displays the color mapping of the `clarity` categories. - Format the y-axis labels to include a dollar sign (``) in the format `value`. Constraints - Make sure your plot is clear, aesthetically pleasing, and the various scale transformations are correctly applied. - The code should be efficient and follow best practices for creating visualizations using seaborn. Input - None (the datasets are pre-loaded as part of seaborn library). Output - A seaborn scatter plot with the specified customizations. Example ```python import seaborn as sns import seaborn.objects as so from seaborn import load_dataset # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot ( so.Plot(diamonds, x=\\"carat\\", y=\\"price\\", color=\\"clarity\\", pointsize=\\"carat\\") .add(so.Dots()) .add(so.Line(), so.PolyFit(order=2)) .scale( y=\\"log\\", color=\\"Set3\\", pointsize=(2, 10) ) .limit(y=(250, 25000)) .label(y=\\"{x:g}\\") .legend(color=True) ) ```","solution":"import matplotlib.pyplot as plt import seaborn as sns import numpy as np def create_diamond_scatter_plot(): Create a scatter plot of carat vs. price from the diamonds dataset, with a logarithmic scale on the y-axis, colors mapped to \'clarity\', and point sizes proportional to \'carat\'. # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the scatter plot plt.figure(figsize=(12, 8)) scatter = sns.scatterplot( data=diamonds, x=\\"carat\\", y=\\"price\\", hue=\\"clarity\\", size=\\"carat\\", sizes=(2, 10), palette=\\"Set3\\" ) # Apply logarithmic scale and set y-axis limit scatter.set(yscale=\\"log\\") scatter.set_ylim(250, 25000) # Format the y-axis labels with dollar sign scatter.set_yticklabels([\'{:,.0f}\'.format(y) for y in scatter.get_yticks()]) # Add a second-order polynomial regression line x = diamonds[\\"carat\\"] y = diamonds[\\"price\\"] p = np.polyfit(x, np.log(y), 2) xp = np.linspace(x.min(), x.max(), 100) yp = np.exp(np.polyval(p, xp)) plt.plot(xp, yp, color=\'black\', lw=2) # Create an interactive legend plt.legend(title=\'Clarity\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.xlabel(\'Carat\') plt.ylabel(\'Price\') plt.title(\'Diamond Carat vs. Price with Non-Linear Transformation\') plt.show()"},{"question":"Question: Implement a Custom PyTorch Dataset and Use Profiling # Background You are working with a large-scale machine learning project using PyTorch, and you need to implement a custom dataset. Additionally, you want to profile the dataset to analyze its performance. # Task 1. **Custom Dataset**: Implement a custom dataset class that: - Extends `torch.utils.data.Dataset`. - Loads data from a provided list of filenames. - Loads each file\'s content as a tensor. 2. **Profiling**: Use `torch.autograd.profiler` to profile the data loading process. # Requirements 1. **CustomDataset Class** - **Input**: A list of filenames (strings). - **Output**: Each element of the dataset should be a tensor containing the content of the corresponding file. - **Constraints**: Ensure that the dataset handles file I/O efficiently. 2. **Profiling Function** - Profile the `__getitem__` method of the `CustomDataset` class. - Print out the profiling results. # Example ```python import torch from torch.utils.data import Dataset import torch.autograd.profiler as profiler import os class CustomDataset(Dataset): def __init__(self, file_list): self.file_list = file_list def __len__(self): return len(self.file_list) def __getitem__(self, idx): file_name = self.file_list[idx] with open(file_name, \'r\') as f: data = f.read() return torch.tensor([float(x) for x in data.split()]) # Example usage: file_list = [\'file1.txt\', \'file2.txt\'] dataset = CustomDataset(file_list) # Profiling the data loading process with profiler.profile(record_shapes=True) as prof: data = dataset[0] print(prof.key_averages().table(sort_by=\\"cpu_time_total\\")) ``` # Constraints - Ensure that your dataset implementation is efficient. - Use the provided example as a reference for the dataset and profiling functionality.","solution":"import torch from torch.utils.data import Dataset import torch.autograd.profiler as profiler class CustomDataset(Dataset): def __init__(self, file_list): Args: file_list (list of str): List of filenames to be loaded. self.file_list = file_list def __len__(self): return len(self.file_list) def __getitem__(self, idx): file_name = self.file_list[idx] with open(file_name, \'r\') as f: data = f.read() # Convert file content to a list of floats and then to a tensor return torch.tensor([float(x) for x in data.split()]) def profile_data_loading(dataset, index): with profiler.profile(record_shapes=True) as prof: data = dataset[index] print(prof.key_averages().table(sort_by=\\"cpu_time_total\\")) return data"},{"question":"# Bytearray Manipulation in Python **Objective**: Demonstrate your understanding of bytearray manipulation in Python by implementing a function that performs specific operations on bytearrays. Function to Implement You need to write a Python function `manipulate_bytearrays` that takes two input strings, performs various operations on their bytearrays, and returns a tuple containing the following: 1. **Concatenated Bytearray**: A new bytearray obtained by concatenating the two input bytearrays. 2. **Reversed Bytearray**: A bytearray that is the reverse of the concatenated bytearray. 3. **Bytearray with Extra Bytes**: The concatenated bytearray resized to include 3 extra bytes at the end. 4. **Size of Original Bytearray**: The size of the concatenated bytearray before resizing. ```python def manipulate_bytearrays(str1: str, str2: str) -> tuple: Perform bytearray manipulations on the given input strings. Args: str1 (str): The first input string. str2 (str): The second input string. Returns: tuple: A tuple containing the concatenated bytearray, reversed bytearray, bytearray with extra bytes, and the size of the original concatenated bytearray. pass ``` Input - `str1` and `str2`: Two non-empty strings consisting of ASCII characters. Output - A tuple containing: 1. A bytearray: The concatenated bytearray of the two input strings. 2. A bytearray: The reversed version of the concatenated bytearray. 3. A bytearray: The concatenated bytearray resized to have 3 extra bytes, all set to the null byte `\'x00\'`. 4. An integer: The size of the original concatenated bytearray before resizing. Constraints - The input strings must only contain valid ASCII characters. - Your function should handle potential errors gracefully, such as when bytearrays cannot be created. Example ```python result = manipulate_bytearrays(\\"hello\\", \\"world\\") # Example output # ( # bytearray(b\'helloworld\'), # bytearray(b\'dlrowolleh\'), # bytearray(b\'helloworldx00x00x00\'), # 10 # ) ``` **Notes**: - Use the C API functions (or their equivalent Python functions) where appropriate. - Ensure your solution is efficient and correctly handles all specified operations.","solution":"def manipulate_bytearrays(str1: str, str2: str) -> tuple: Perform bytearray manipulations on the given input strings. Args: str1 (str): The first input string. str2 (str): The second input string. Returns: tuple: A tuple containing the concatenated bytearray, reversed bytearray, bytearray with extra bytes, and the size of the original concatenated bytearray. # Convert input strings to bytearrays ba1 = bytearray(str1, \'ascii\') ba2 = bytearray(str2, \'ascii\') # Concatenate the bytearrays concatenated = ba1 + ba2 # Reverse the concatenated bytearray reversed_ba = concatenated[::-1] # Remember the size of the original concatenated bytearray original_size = len(concatenated) # Resize the concatenated bytearray to include 3 extra null bytes resized_ba = concatenated + bytearray(3) return (concatenated, reversed_ba, resized_ba, original_size)"},{"question":"**Objective**: Demonstrate understanding of file handling and data compression/decompression using Python\'s `gzip` module. **Problem Statement**: Write a Python function `process_file(input_file: str, compressed_file: str, decompressed_file: str) -> bool` that: 1. Reads the content from a text file specified by `input_file`. 2. Compresses the content and writes it to a new gzip file specified by `compressed_file`. 3. Decompresses the content of the gzip file and writes it to another new text file specified by `decompressed_file`. 4. Returns `True` if the content of `input_file` and `decompressed_file` are identical, or `False` otherwise. # Function Signature: ```python def process_file(input_file: str, compressed_file: str, decompressed_file: str) -> bool: pass ``` # Input: - `input_file`: A string representing the path to the original text file. - `compressed_file`: A string representing the path where the compressed gzip file will be created. - `decompressed_file`: A string representing the path where the decompressed text file will be created. # Output: - Returns `True` if the decompressed content matches the original file content. Returns `False` otherwise. # Constraints: - Assume that the `input_file` exists and contains text data. - Handle possible exceptions related to file I/O and gzip operations. - The function should manage resources properly (e.g., using context managers). # Example: ```python # Assume \\"input.txt\\" exists and contains \\"Hello, world!\\" result = process_file(\'input.txt\', \'compressed.gz\', \'output.txt\') print(result) # Output should be `True` ``` # Notes: - Make sure to use appropriate modes for file operations (e.g., `\'rt\'`, `\'wt\'`, `\'wb\'`, `\'rb\'`). - Ensure that the compression level of `9` is used when writing the gzip file. - Utilize the `gzip` module functions and classes as necessary.","solution":"import gzip import shutil def process_file(input_file: str, compressed_file: str, decompressed_file: str) -> bool: try: # Step 1: Read content from input_file with open(input_file, \'rt\', encoding=\'utf-8\') as f: content = f.read() # Step 2: Compress content and write to compressed_file with gzip.open(compressed_file, \'wt\', encoding=\'utf-8\', compresslevel=9) as gz: gz.write(content) # Step 3: Decompress content from compressed_file and write to decompressed_file with gzip.open(compressed_file, \'rt\', encoding=\'utf-8\') as gz: decompressed_content = gz.read() with open(decompressed_file, \'wt\', encoding=\'utf-8\') as f: f.write(decompressed_content) # Step 4: Compare content of input_file and decompressed_file with open(decompressed_file, \'rt\', encoding=\'utf-8\') as f: verified_content = f.read() return content == verified_content except Exception as e: print(f\\"Error occurred: {e}\\") return False"},{"question":"**Question: Handling Batch Normalization with vmap in PyTorch** Given the specifics of using Batch Normalization (`BatchNorm`) in PyTorch with `functorch`, particularly when applying vmapping operations over a batch of inputs, you are required to demonstrate your understanding by performing the following tasks: 1. **Define a CNN model using PyTorch which includes at least one BatchNorm layer.** 2. **Implement a function called `replace_with_group_norm` that replaces each BatchNorm layer in a given model with GroupNorm, adhering to the constraints discussed:** - GroupNorm should split channels evenly (i.e., `C % G == 0`). - As a fallback, set the number of groups `G` equal to the number of channels `C`. 3. **Implement a function called `patch_batch_norm_no_running_stats` that replaces each BatchNorm layer in a given model with an identical BatchNorm layer but with `track_running_stats` set to False.** 4. **Test your implementations by:** - Creating an instance of your CNN model. - Applying `replace_with_group_norm` and `patch_batch_norm_no_running_stats` to separate instances of the model. - Displaying summaries of the modified models to confirm changes. **Function Definitions:** 1. **Function: `replace_with_group_norm`** ```python def replace_with_group_norm(model: torch.nn.Module) -> torch.nn.Module: Replaces all BatchNorm layers in the given model with GroupNorm. Args: model (torch.nn.Module): The original model containing BatchNorm layers. Returns: torch.nn.Module: A modified model with GroupNorm layers. pass ``` 2. **Function: `patch_batch_norm_no_running_stats`** ```python def patch_batch_norm_no_running_stats(model: torch.nn.Module) -> torch.nn.Module: Replaces all BatchNorm layers in the given model with identical BatchNorm layers but with track_running_stats set to False. Args: model (torch.nn.Module): The original model containing BatchNorm layers. Returns: torch.nn.Module: A modified model with BatchNorm layers and no running stats. pass ``` **Example Usage:** ```python import torch import torch.nn as nn import torchvision # Define your CNN model with BatchNorm class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(32) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(64) self.fc1 = nn.Linear(64 * 28 * 28, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = nn.ReLU()(x) x = self.conv2(x) x = self.bn2(x) x = nn.ReLU()(x) x = x.view(x.size(0), -1) x = self.fc1(x) x = self.fc2(x) return x # Initialize the model model = SimpleCNN() # Apply the replacement functions and print summaries patched_model1 = replace_with_group_norm(model) patched_model2 = patch_batch_norm_no_running_stats(model) print(patched_model1) print(patched_model2) ``` **Constraints:** - Your solution should work on any general model with BatchNorm layers. - Ensure the replacement is comprehensive and does not miss BatchNorm layers nested within complex architectures. This question tests your understanding of: - Handling BatchNorm and the implications of its running statistics. - Replacing layers within a PyTorch model using GroupNorm. - Modifying models programmatically to suit specific constraints.","solution":"import torch import torch.nn as nn class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(32) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(64) self.fc1 = nn.Linear(64 * 28 * 28, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = nn.ReLU()(x) x = self.conv2(x) x = self.bn2(x) x = nn.ReLU()(x) x = x.view(x.size(0), -1) x = self.fc1(x) x = self.fc2(x) return x def replace_with_group_norm(model: nn.Module) -> nn.Module: for name, module in model.named_children(): if isinstance(module, nn.BatchNorm2d): num_channels = module.num_features num_groups = min(32, num_channels) if num_channels % 32 == 0 else num_channels new_module = nn.GroupNorm(num_groups, num_channels) setattr(model, name, new_module) else: replace_with_group_norm(module) return model def patch_batch_norm_no_running_stats(model: nn.Module) -> nn.Module: for name, module in model.named_children(): if isinstance(module, nn.BatchNorm2d): new_module = nn.BatchNorm2d(module.num_features, eps=module.eps, momentum=module.momentum, affine=module.affine, track_running_stats=False) setattr(model, name, new_module) else: patch_batch_norm_no_running_stats(module) return model"},{"question":"**Objective**: Create a Python script to encode and decode files using the `uu` module. Problem Statement You are required to write two functions, `encode_file` and `decode_file`, which will leverage the `uu` module to encode and decode files. Additionally, handle errors gracefully and ensure the program is compatible with large binary files. Function 1: encode_file **Parameters:** - `in_file: str` - Path to the input file to be uuencoded. - `out_file: str` - Path to the output file where the uuencoded content will be written. - `name: str` (optional) - The name to be included in the uuencode header. Default is `None`. - `mode: int` (optional) - File mode to be included in the uuencode header. Default is `None`. - `backtick: bool` (optional) - If True, zeros are represented by backticks. Default is `False`. **Returns:** - `None` **Function Description:** - Use the `uu.encode` function to uuencode the content of `in_file` and write it to `out_file`. - Ensure that if the `in_file` or `out_file` does not exist or is invalid, a meaningful error message is printed to the console. Function 2: decode_file **Parameters:** - `in_file: str` - Path to the uuencoded file to be decoded. - `out_file: str` - Path to the output file where the decoded binary content will be written. - `mode: int` (optional) - File mode for the output file if it needs to be created. Default is `None`. - `quiet: bool` (optional) - If True, suppresses error warnings. Default is `False`. **Returns:** - `None` **Function Description:** - Use the `uu.decode` function to decode the content of `in_file` and write it to `out_file`. - If the input file is uuencoded incorrectly, handle the `uu.Error` exception and print a meaningful error message to the console. - If the file in the header already exists or any other error occurs, display appropriate error messages. # Constraints - Your solution should work with large files (e.g., several hundred megabytes). - Your solution should handle edge cases, such as trying to encode/decode non-existent files or invalid files. # Examples ```python # Example usage: # Encode a file: encode_file(\'input.bin\', \'output.uue\') # Decode a file: decode_file(\'output.uue\', \'recovered.bin\') ``` # Note: - The script should be designed to run in Python environments where the `uu` module is supported. - Although this module is deprecated in Python 3.11, ensure your solution targets environments supporting the `uu` module.","solution":"import uu def encode_file(in_file, out_file, name=None, mode=None, backtick=False): Encodes the content of `in_file` and writes it to `out_file`. Parameters: in_file: str - Path to the input file to be uuencoded. out_file: str - Path to the output file where the uuencoded content will be written. name: str (optional) - The name to be included in the uuencode header. Default is `None`. mode: int (optional) - File mode to be included in the uuencode header. Default is `None`. backtick: bool (optional) - If True, zeros are represented by backticks. Default is `False`. Returns: None try: with open(in_file, \'rb\') as infile: with open(out_file, \'wb\') as outfile: uu.encode(infile, outfile, name=name or in_file, mode=mode, backtick=backtick) except FileNotFoundError: print(f\\"Error: The file {in_file} does not exist.\\") except Exception as e: print(f\\"An error occurred during encoding: {e}\\") def decode_file(in_file, out_file, mode=None, quiet=False): Decodes the content of `in_file` and writes it to `out_file`. Parameters: in_file: str - Path to the uuencoded file to be decoded. out_file: str - Path to the output file where the decoded binary content will be written. mode: int (optional) - File mode for the output file if it needs to be created. Default is `None`. quiet: bool (optional) - If True, suppresses error warnings. Default is `False`. Returns: None try: with open(in_file, \'rb\') as infile: with open(out_file, \'wb\') as outfile: uu.decode(infile, outfile, mode=mode, quiet=quiet) except FileNotFoundError: print(f\\"Error: The file {in_file} does not exist.\\") except uu.Error as e: print(f\\"UU error occurred: {e}\\") except Exception as e: print(f\\"An error occurred during decoding: {e}\\")"},{"question":"**Objective**: Implement a function that processes an email message object and extracts specific parts based on MIME type, then outputs the extracted data in a structured format. **Problem Statement**: Given an email message object, write a Python function `extract_mime_parts(msg, maintype=\'text\', subtype=None)` that uses the `typed_subpart_iterator` function to collect all subparts matching the specified MIME type (`maintype` and `subtype`). The function should return a dictionary where the keys are the content types, and the values are lists of the corresponding payloads. **Function Signature**: ```python def extract_mime_parts(msg, maintype=\'text\', subtype=None) -> dict: ``` # Input: - `msg`: An email message object. - `maintype`: (Optional) The main MIME type to filter by. Defaults to \'text\'. - `subtype`: (Optional) The MIME subtype to filter by. Defaults to `None`. # Output: - A dictionary where: - Keys are the MIME types of the extracted subparts. - Values are lists containing the payloads of those subparts. # Constraints: - The function should use the `typed_subpart_iterator` function from the `email.iterators` module. - Consider that an email can have nested multipart subparts. # Example: Suppose we have an email message object `msg`, and we want to extract all text/plain parts: ```python result = extract_mime_parts(msg, \'text\', \'plain\') ``` The output dictionary might look like: ```python { \'text/plain\': [ \'This is the first plain text part.\', \'This is the second plain text part.\', \'This is the third plain text part.\' ] } ``` # Note: For the implementation, assume you have imported the necessary modules: ```python from email import message_from_string from email.iterators import typed_subpart_iterator ``` You can use the following sample email data for testing: ```python raw_email = MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\\"bound\\" --bound Content-Type: text/plain This is the first plain text part. --bound Content-Type: text/html <p>This is an HTML text part.</p> --bound Content-Type: multipart/digest; boundary=\\"subbound\\" --subbound Content-Type: text/plain This is the second plain text part. --subbound Content-Type: text/plain This is the third plain text part. --subbound-- --bound-- msg = message_from_string(raw_email) ``` Implement the function `extract_mime_parts` to solve this problem.","solution":"from email import message_from_string from email.iterators import typed_subpart_iterator def extract_mime_parts(msg, maintype=\'text\', subtype=None): Extracts parts of an email message based on MIME type and subtype. Args: - msg: An email message object. - maintype: The main MIME type to filter by. Defaults to \'text\'. - subtype: The MIME subtype to filter by. Defaults to `None`. Returns: A dictionary where the keys are the content types, and the values are lists of the corresponding payloads. result = {} for part in typed_subpart_iterator(msg, maintype, subtype): content_type = part.get_content_type() payload = part.get_payload(decode=True).decode(part.get_content_charset() or \'utf-8\', errors=\'replace\') if content_type not in result: result[content_type] = [] result[content_type].append(payload) return result"},{"question":"# Question: Implement a Secure Communication Protocol You are tasked with implementing a secure communication protocol between a client and a server using Python 3.10\'s cryptographic services. The protocol must ensure the integrity and authenticity of the messages exchanged. Requirements: 1. **Message Hashing using `hashlib`:** - Create a function `generate_hash(message: str) -> str` that takes a message as input and returns its SHA-256 hash. 2. **Message Authentication using `hmac`:** - Create a function `create_hmac(key: bytes, message: str) -> str` that takes a key and a message as input and returns an HMAC using the SHA-256 hash function. 3. **Secure Key Generation using `secrets`:** - Create a function `generate_secure_key(length: int) -> bytes` that generates a secure random key of a specified length. Input and Output Formats: - `generate_hash(message: str) -> str`: Input is a message (string), and the output is its SHA-256 hash (hexadecimal string). - `create_hmac(key: bytes, message: str) -> str`: Inputs are a key (bytes) and a message (string), and the output is the HMAC (hexadecimal string). - `generate_secure_key(length: int) -> bytes`: Input is the length of the key (integer), and the output is a secure random key (bytes). Constraints: - The key length for HMAC should be at least 16 bytes. - Messages will be non-empty strings with a maximum length of 4096 characters. - Ensure that your implementation handles errors gracefully. Example: ```python message = \\"This is a secret message.\\" key_length = 32 # Step 1: Generate a secure random key key = generate_secure_key(key_length) # Step 2: Generate a hash of the message message_hash = generate_hash(message) print(f\\"Message Hash: {message_hash}\\") # Step 3: Create an HMAC of the message using the generated key message_hmac = create_hmac(key, message) print(f\\"Message HMAC: {message_hmac}\\") ``` Define the functions `generate_hash`, `create_hmac`, and `generate_secure_key` according to the specifications and demonstrate their usage as shown in the example.","solution":"import hashlib import hmac import secrets def generate_hash(message: str) -> str: Generates a SHA-256 hash of the given message. Args: - message (str): The message to hash. Returns: - str: The hexadecimal SHA-256 hash of the message. hash_object = hashlib.sha256(message.encode()) return hash_object.hexdigest() def create_hmac(key: bytes, message: str) -> str: Creates an HMAC for the given message using the given key and SHA-256. Args: - key (bytes): The key for HMAC. - message (str): The message to authenticate. Returns: - str: The hexadecimal HMAC of the message. hmac_object = hmac.new(key, message.encode(), hashlib.sha256) return hmac_object.hexdigest() def generate_secure_key(length: int) -> bytes: Generates a secure random key of the specified length. Args: - length (int): The length of the key in bytes. Returns: - bytes: The secure random key. if length < 16: raise ValueError(\\"Key length must be at least 16 bytes.\\") return secrets.token_bytes(length)"},{"question":"Coding Assessment Question # Objective: Implement an attention mechanism using PyTorch with block masks, focusing on the functions and class in the `torch.nn.attention.flex_attention` module. # Problem Statement: You are provided with a sequence of data and need to apply an attention mechanism that uses block masks to selectively focus on specific parts of the sequence. Implement the function `block_attention` that computes the attention weighted sum of a sequence with the help of block masks. # Requirements: 1. **Function**: ```python def block_attention(sequence: torch.Tensor, mask_type: str, block_size: int) -> torch.Tensor: pass ``` - **Input**: - `sequence` (torch.Tensor): A 2D tensor of shape `(seq_len, d_model)` representing the input sequence. - `mask_type` (str): The type of mask to be used. Options: `\\"block\\"`, `\\"nested_block\\"`, `\\"noop\\"`. - `block_size` (int): The size of blocks for the block masks. - **Output**: - Return a tensor of shape `(seq_len, d_model)` representing the attention weighted sum of the input sequence. 2. **Constraints**: - `seq_len` will be between 1 and 1024. - `d_model` will be between 1 and 512. - `block_size` will be a positive integer less than or equal to `seq_len`. 3. **Functions and Class**: - Use the appropriate mask functions (`create_block_mask`, `create_nested_block_mask`, `noop_mask`) based on the `mask_type` input. - Utilize the `BlockMask` class to manage and apply the masks. 4. **Performance**: - The solution should efficiently handle the input size constraints. # Example: ```python import torch from torch.nn.attention.flex_attention import BlockMask, create_block_mask, create_nested_block_mask, noop_mask def block_attention(sequence: torch.Tensor, mask_type: str, block_size: int) -> torch.Tensor: # Step 1: Validate inputs assert sequence.dim() == 2, \\"Input sequence must be a 2D tensor\\" seq_len, d_model = sequence.size() assert block_size > 0 and block_size <= seq_len, \\"Block size must be a positive integer less than or equal to the sequence length\\" # Step 2: Create the appropriate mask if mask_type == \\"block\\": mask = create_block_mask(seq_len, block_size) elif mask_type == \\"nested_block\\": mask = create_nested_block_mask(seq_len, block_size) elif mask_type == \\"noop\\": mask = noop_mask(seq_len) else: raise ValueError(\\"Invalid mask type\\") # Step 3: Apply the mask using BlockMask block_mask = BlockMask(mask) attention_weights = block_mask.apply_mask(sequence) # Step 4: Compute the attention weighted sum attention_output = attention_weights @ sequence return attention_output # Sample usage seq_len, d_model = 10, 64 sequence = torch.rand((seq_len, d_model)) mask_type = \\"block\\" block_size = 2 result = block_attention(sequence, mask_type, block_size) print(result.shape) # Expected output: (seq_len, d_model) ``` # Explanation: In the provided example, the `block_attention` function: 1. Validates the input sequence and block size. 2. Creates the appropriate type of mask based on the `mask_type` input. 3. Uses `BlockMask` to manage and apply the mask to the input sequence. 4. Computes the attention weighted sum of the sequence using the mask and returns the result. Implement the `block_attention` function according to the requirements above.","solution":"import torch def create_block_mask(seq_len, block_size): # Create a mask with block_size blocks set to True, rest set to False mask = torch.zeros(seq_len, seq_len) for i in range(0, seq_len, block_size): mask[i:i+block_size, i:i+block_size] = 1 return mask def create_nested_block_mask(seq_len, block_size): # Create a nested block mask where each block is applied in a nested fashion mask = torch.zeros(seq_len, seq_len) nested_block_size = block_size while nested_block_size <= seq_len: for i in range(0, seq_len, nested_block_size): mask[i:i+nested_block_size, i:i+nested_block_size] = 1 nested_block_size *= 2 return mask def noop_mask(seq_len): # No mask applied (Identity matrix) return torch.eye(seq_len) class BlockMask: def __init__(self, mask): self.mask = mask def apply_mask(self, sequence): return torch.matmul(self.mask, sequence) def block_attention(sequence: torch.Tensor, mask_type: str, block_size: int) -> torch.Tensor: assert sequence.dim() == 2, \\"Input sequence must be a 2D tensor\\" seq_len, d_model = sequence.size() assert block_size > 0 and block_size <= seq_len, \\"Block size must be a positive integer less than or equal to the sequence length\\" if mask_type == \\"block\\": mask = create_block_mask(seq_len, block_size) elif mask_type == \\"nested_block\\": mask = create_nested_block_mask(seq_len, block_size) elif mask_type == \\"noop\\": mask = noop_mask(seq_len) else: raise ValueError(\\"Invalid mask type\\") block_mask = BlockMask(mask) attention_weights = block_mask.apply_mask(sequence) attention_output = attention_weights return attention_output"},{"question":"# Custom Mapping Class Implementation You are tasked with designing a custom mapping class in Python that mimics the behavior and features of the mapping protocol functions as described in the provided documentation. Your implementation should provide functionality for adding, updating, retrieving, and deleting items, as well as for checking the presence of keys, and outputting the keys, values, and items in the mapping. Specifications: 1. **Class Name**: `CustomMapping` 2. **Methods**: - `__init__(self)`: Initialize an empty mapping. - `__getitem__(self, key)`: Retrieve the value associated with the `key`. - `__setitem__(self, key, value)`: Set the value for a given `key`. - `__delitem__(self, key)`: Remove the item associated with `key`. - `__len__(self)`: Return the number of items in the mapping. - `__contains__(self, key)`: Return `True` if the `key` is in the mapping, `False` otherwise. - `keys(self)`: Return a list of the keys in the mapping. - `values(self)`: Return a list of the values in the mapping. - `items(self)`: Return a list of tuples containing each key-value pair in the mapping. 3. **Exceptions**: - Raise a `KeyError` exception if an attempt is made to access or delete a key that does not exist. 4. **Constraints**: - The keys in the mapping must be immutable and hashable (e.g., strings, tuples). - The values can be any type of Python object. 5. **Performance**: - Aim for O(1) complexity for `__getitem__`, `__setitem__`, `__delitem__`, and `__contains__`. - `keys`, `values`, and `items` should be able to handle large mappings efficiently. Example: ```python # Create an instance of CustomMapping custom_map = CustomMapping() # Set some key-value pairs custom_map[\\"name\\"] = \\"Alice\\" custom_map[\\"age\\"] = 30 # Retrieve values print(custom_map[\\"name\\"]) # Output: Alice # Delete a key del custom_map[\\"age\\"] # Check if a key is in the mapping print(\\"name\\" in custom_map) # Output: True print(\\"age\\" in custom_map) # Output: False # Get the number of items print(len(custom_map)) # Output: 1 # Get the keys, values, and items print(custom_map.keys()) # Output: [\'name\'] print(custom_map.values()) # Output: [\'Alice\'] print(custom_map.items()) # Output: [(\'name\', \'Alice\')] ``` Implement the `CustomMapping` class according to the specifications above.","solution":"class CustomMapping: def __init__(self): self._store = {} def __getitem__(self, key): if key not in self._store: raise KeyError(f\\"Key \'{key}\' not found\\") return self._store[key] def __setitem__(self, key, value): self._store[key] = value def __delitem__(self, key): if key not in self._store: raise KeyError(f\\"Key \'{key}\' not found\\") del self._store[key] def __len__(self): return len(self._store) def __contains__(self, key): return key in self._store def keys(self): return list(self._store.keys()) def values(self): return list(self._store.values()) def items(self): return list(self._store.items())"},{"question":"**Objective:** Demonstrate your understanding of Python\'s descriptor protocol and your ability to create and manage descriptors dynamically. **Question:** Write a Python class, `DynamicDescriptor`, which can dynamically add properties, methods, and class methods to itself using Python\'s descriptor protocol. Your class should support the following functionalities: 1. `add_property(name, getter, setter=None)`: - Adds a new property with the given `name`. - `getter` and `setter` should be function references for getting and setting the attribute\'s value. 2. `add_method(name, method)`: - Adds a new instance method with the given `name`. - `method` should be a function reference. 3. `add_class_method(name, class_method)`: - Adds a new class method with the given `name`. - `class_method` should be a function reference and will receive the class as the first argument. **Constraints:** - Ensure that the descriptors work as expected, and the methods and properties can be accessed/modified accordingly. - Use the built-in descriptor types and functions described in the documentation. **Example Usage:** ```python class DynamicDescriptor: # Your implementation here # Example getter and setter functions def getter(self): return self._value def setter(self, value): self._value = value # Example instance method def instance_method(self): return \\"Instance Method Called\\" # Example class method @classmethod def class_method(cls): return \\"Class Method Called\\" # Create an instance of DynamicDescriptor dd = DynamicDescriptor() # Add a property dd.add_property(\\"value\\", getter, setter) # Set and get the property dd.value = 10 print(dd.value) # Output: 10 # Add an instance method dd.add_method(\\"instance_method\\", instance_method) # Call the instance method print(dd.instance_method()) # Output: \\"Instance Method Called\\" # Add a class method dd.add_class_method(\\"class_method\\", class_method) # Call the class method print(dd.class_method()) # Output: \\"Class Method Called\\" ``` Ensure that your implementation passes the example usage provided. **Input Format:** - There is no specific input format. You must create a `DynamicDescriptor` class and demonstrate the usage as shown in the example. **Output Format:** - The output should be the print statements as shown in the example usage. **Performance Requirement:** - Ensure that the addition of properties, methods, and class methods is done efficiently without any redundant operations.","solution":"class DynamicDescriptor: def add_property(self, name, getter, setter=None): if setter: prop = property(getter, setter) else: prop = property(getter) setattr(self.__class__, name, prop) def add_method(self, name, method): setattr(self.__class__, name, method) def add_class_method(self, name, class_method): setattr(self.__class__, name, classmethod(class_method)) # Example getter and setter functions def getter(self): return self._value def setter(self, value): self._value = value # Example instance method def instance_method(self): return \\"Instance Method Called\\" # Example class method @classmethod def class_method(cls): return \\"Class Method Called\\" # Create an instance of DynamicDescriptor dd = DynamicDescriptor() # Add a property dd.add_property(\\"value\\", getter, setter) # Set and get the property dd.value = 10 print(dd.value) # Output: 10 # Add an instance method dd.add_method(\\"instance_method\\", instance_method) # Call the instance method print(dd.instance_method()) # Output: \\"Instance Method Called\\" # Add a class method dd.add_class_method(\\"class_method\\", class_method) # Call the class method print(dd.class_method()) # Output: \\"Class Method Called\\""},{"question":"Objective You are required to implement a function that organizes files in a given directory into categorized folders based on their file extensions. The function should use the `glob` module to identify files and manage file operations such as creation of directories and moving of files. Function Signature ```python def organize_files_by_extension(directory: str) -> None: pass ``` Input - `directory`: A string representing the path to the directory that needs to be organized. This path can be absolute or relative. Output - The function should not return anything but should organize the files within the given directory. Requirements 1. For each file in the given directory (including files in subdirectories), identify its extension and move it into a folder named after the extension within the given directory. - For example, `example.txt` should be moved to a folder named `txt`. 2. If a folder with the required name does not exist, create it. 3. Handle files without extensions by placing them into a folder named `no_extension`. 4. Ensure that the function can handle directory structures with nested subdirectories. 5. Broken symlinks should not be moved but must be listed if any exist. Example Consider the following directory structure: ``` /example_directory file1.txt file2.jpg file3 sub_dir file4.txt file5.pdf ``` After running `organize_files_by_extension(\'/example_directory\')`, the directory structure should be: ``` /example_directory /txt file1.txt /sub_dir file4.txt /jpg file2.jpg /pdf /sub_dir file5.pdf /no_extension file3 /sub_dir ``` Constraints - You are **not** allowed to use any external libraries except for `glob` and standard Python libraries such as `os` and `shutil`. - Ensure that edge cases, such as directories without any files or files without extensions, are handled correctly. - The solution should be efficient in terms of both time and space complexities. Notes - Test your function thoroughly to make sure it works under different scenarios. - Use `glob.glob` or `glob.iglob` appropriately to handle the file search in the directory.","solution":"import os import glob import shutil def organize_files_by_extension(directory: str) -> None: for root, _, files in os.walk(directory): for file in files: file_path = os.path.join(root, file) if not os.path.islink(file_path): # skip broken symlinks extension = os.path.splitext(file)[1][1:] or \'no_extension\' if len(extension) == 0: extension = \'no_extension\' destination_dir = os.path.join(directory, extension) if not os.path.exists(destination_dir): os.makedirs(destination_dir) new_file_path = os.path.join(destination_dir, file) shutil.move(file_path, new_file_path)"},{"question":"# Python 3.10 Coding Assessment: Asynchronous Queue Management Objective Design a task orchestration system using `asyncio.Queue` that simulates a simplified version of a task scheduler. Problem Statement You are tasked with creating a system that manages tasks executed by multiple worker coroutines. Your solution should efficiently distribute tasks to the workers, keeping track of task completion, and ensuring that all tasks are processed within a given time frame. Requirements - Define an `asyncio.Queue` to hold tasks. - Implement a worker coroutine that fetches tasks from the queue and processes them. - Ensure that the worker acknowledges task completion using `task_done()`. - Implement a task generator coroutine that generates a fixed number of tasks and puts them in the queue. - The task generator should stop producing new tasks if the total processing time exceeds a given limit (`max_processing_time`). - Ensure that the task generator uses `put()` to add tasks to the queue asynchronously. - Ensure proper handling of queue-related exceptions (`QueueEmpty`, `QueueFull`). Input - `num_workers` (int): Number of worker coroutines to spawn. - `num_tasks` (int): Total number of tasks to generate. - `max_task_time` (float): Maximum time each task can take to be processed. - `max_processing_time` (float): Maximum allowable total processing time for all tasks. Output - Print a summary showing the total number of tasks processed by each worker. - Print the total elapsed processing time. Example Usage ```python import asyncio import random import time async def worker(name, queue): processed_tasks = 0 while True: try: task_time = await queue.get() await asyncio.sleep(task_time) queue.task_done() processed_tasks += 1 except asyncio.QueueEmpty: break print(f\\"{name} processed {processed_tasks} tasks\\") return processed_tasks async def task_generator(queue, num_tasks, max_task_time, max_processing_time): start_time = time.monotonic() total_time = 0 for _ in range(num_tasks): if total_time > max_processing_time: break task_time = random.uniform(0.1, max_task_time) await queue.put(task_time) total_time += task_time async def main(num_workers, num_tasks, max_task_time, max_processing_time): queue = asyncio.Queue() task_gen = task_generator(queue, num_tasks, max_task_time, max_processing_time) await task_gen workers = [ asyncio.create_task(worker(f\'worker-{i}\', queue)) for i in range(num_workers) ] start_time = time.monotonic() await queue.join() for w in workers: w.cancel() results = await asyncio.gather(*workers, return_exceptions=True) total_tasks_processed = sum(result for result in results if isinstance(result, int)) total_time_elapsed = time.monotonic() - start_time print(f\\"Total tasks processed: {total_tasks_processed}\\") print(f\\"Total time elapsed: {total_time_elapsed:.2f} seconds\\") if __name__ == \\"__main__\\": num_workers = 3 num_tasks = 30 max_task_time = 2.0 max_processing_time = 50.0 asyncio.run(main(num_workers, num_tasks, max_task_time, max_processing_time)) ``` # Constraints - `num_workers`: 1  num_workers  10 - `num_tasks`: 1  num_tasks  100 - `max_task_time`: 0.1  max_task_time  5.0 - `max_processing_time`: 1.0  max_processing_time  200.0 Implement using the `asyncio` framework to effectively manage the tasks and ensure all tasks are completed within the time limit.","solution":"import asyncio import random import time async def worker(name, queue): processed_tasks = 0 while True: try: task_time = await queue.get() await asyncio.sleep(task_time) queue.task_done() processed_tasks += 1 except asyncio.QueueEmpty: break print(f\\"{name} processed {processed_tasks} tasks\\") return processed_tasks async def task_generator(queue, num_tasks, max_task_time, max_processing_time): start_time = time.monotonic() total_time = 0 for _ in range(num_tasks): if total_time > max_processing_time: break task_time = random.uniform(0.1, max_task_time) await queue.put(task_time) total_time += task_time async def main(num_workers, num_tasks, max_task_time, max_processing_time): queue = asyncio.Queue() task_gen = asyncio.create_task(task_generator(queue, num_tasks, max_task_time, max_processing_time)) await task_gen workers = [ asyncio.create_task(worker(f\'worker-{i}\', queue)) for i in range(num_workers) ] start_time = time.monotonic() await queue.join() for w in workers: w.cancel() results = await asyncio.gather(*workers, return_exceptions=True) total_tasks_processed = sum(result for result in results if isinstance(result, int)) total_time_elapsed = time.monotonic() - start_time print(f\\"Total tasks processed: {total_tasks_processed}\\") print(f\\"Total time elapsed: {total_time_elapsed:.2f} seconds\\") if __name__ == \\"__main__\\": num_workers = 3 num_tasks = 30 max_task_time = 2.0 max_processing_time = 50.0 asyncio.run(main(num_workers, num_tasks, max_task_time, max_processing_time))"},{"question":"Coding Assessment Question # Objective Develop a Python script to demonstrate your understanding of using the `syslog` module to log messages with different priorities, facilities, and options. # Task Write a Python function `setup_logging_and_send_messages` that: 1. Opens the syslog with a given `ident`, `logoption`, and `facility`. 2. Logs a set of messages with specified priorities. 3. Closes the syslog. # Function Signature ```python def setup_logging_and_send_messages(ident: str, logoption: int, facility: int, messages: List[Tuple[int, str]]) -> None: pass ``` # Parameters - `ident` (str): A string that is prepended to every message. - `logoption` (int): A bit field representing logging options. Possible values: - `syslog.LOG_PID` - `syslog.LOG_CONS` - `syslog.LOG_NDELAY` - `facility` (int): The facility to use for logging. Possible values: - `syslog.LOG_USER` - `syslog.LOG_MAIL` - `syslog.LOG_DAEMON` - `syslog.LOG_AUTH` - `messages` (List[Tuple[int, str]]): A list of tuples where each tuple contains: - `priority` (int): The priority level of the message. Possible values: - `syslog.LOG_EMERG` - `syslog.LOG_ALERT` - `syslog.LOG_CRIT` - `syslog.LOG_ERR` - `syslog.LOG_WARNING` - `syslog.LOG_NOTICE` - `syslog.LOG_INFO` - `syslog.LOG_DEBUG` - `message` (str): The message to log. # Constraints - All inputs will be valid and within the specified ranges. - The syslog must be properly closed after all messages have been logged. # Example Usage ```python messages = [ (syslog.LOG_INFO, \\"Information message\\"), (syslog.LOG_ERR, \\"Error message\\"), (syslog.LOG_DEBUG, \\"Debug message\\") ] setup_logging_and_send_messages(\\"MyApp\\", syslog.LOG_PID | syslog.LOG_CONS, syslog.LOG_USER, messages) ``` # Expected Output - The provided messages should be sent to the system logger with the specified ident, logoption, and facility. - Each message should be logged with its specified priority. # Notes - Ensure that the syslog is opened and closed appropriately within your function. - Make use of the `syslog.openlog()`, `syslog.syslog()`, and `syslog.closelog()` functions as needed.","solution":"import syslog from typing import List, Tuple def setup_logging_and_send_messages(ident: str, logoption: int, facility: int, messages: List[Tuple[int, str]]) -> None: Sets up logging with the specified ident, logoption, and facility, then sends the provided messages with their respective priorities to syslog. # Open the syslog with the provided ident, logoption, and facility syslog.openlog(ident, logoption, facility) # Log each message with the specified priority for priority, message in messages: syslog.syslog(priority, message) # Close the syslog syslog.closelog()"},{"question":"You are provided with a dataset about vehicle fuel efficiency and attributes (`mpg`). You are tasked with analyzing the linear relationship between various attributes and visualizing the residuals with Seaborn\'s `residplot` function. This will help in identifying non-linear trends or patterns in the data. Instructions: 1. **Load the dataset using Seaborn functions**: Load the `mpg` dataset from Seaborn. 2. **Create Residual Plots**: - Generate a residual plot to examine the relationship between `weight` and `displacement`. - Generate a residual plot to examine the relationship between `horsepower` and `mpg`. - Generate an enhanced residual plot for `horsepower` and `mpg` by: - Removing higher-order trends (`order=2`). - Adding a LOWESS smoothed line (`lowess=True`) with a customized red line (`line_kws=dict(color=\\"r\\")`). Requirements: - Ensure that each plot is displayed clearly using appropriate labeling and stylistic themes. Expected Outputs: - Three plots must be generated and displayed. 1. Residual plot of `weight` vs `displacement`. 2. Residual plot of `horsepower` vs `mpg`. 3. Enhanced residual plot of `horsepower` vs `mpg` with higher-order trend removal and LOWESS smoothing. Sample Code Structure: ```python import seaborn as sns # Load dataset mpg = sns.load_dataset(\\"mpg\\") # Set Seaborn theme sns.set_theme() # Create first residual plot sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") # Create second residual plot sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") # Create enhanced third residual plot sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) ``` Each step should be handled in separate code cells for clarity. Ensure to import necessary libraries and handle any necessary data preprocessing. Note: Make sure you format the plots properly for better readability and visual appeal.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_residual_plots(): # Load dataset mpg = sns.load_dataset(\\"mpg\\") # Set Seaborn theme sns.set_theme() # Create first residual plot: weight vs displacement plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\'Residual Plot: weight vs displacement\') plt.ylabel(\'Residuals\') plt.xlabel(\'Weight\') plt.show() # Create second residual plot: horsepower vs mpg plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\'Residual Plot: horsepower vs mpg\') plt.ylabel(\'Residuals\') plt.xlabel(\'Horsepower\') plt.show() # Create enhanced third residual plot: horsepower vs mpg with higher-order trend and LOWESS smoothing plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\'Enhanced Residual Plot: horsepower vs mpg (order=2 & LOWESS)\') plt.ylabel(\'Residuals\') plt.xlabel(\'Horsepower\') plt.show()"},{"question":"Objective Write a Python program that traverses a given directory and generates a summary report containing the count of each type of file (regular files, directories, etc.), as well as their permissions in a human-readable format. Input - A string `directory_path` representing the path to the directory to be analyzed. Output - A dictionary with two keys: 1. `\\"file_counts\\"`: A dictionary where the keys are the file types and the values are the counts of those file types in the directory. - File types include: `Directories`, `Regular files`, `Symbolic links`, `Sockets`, `Character devices`, `Block devices`, `FIFOs`, `Doors`, `Event ports`, and `Whiteouts`. 2. `\\"permissions\\"`: A dictionary where the keys are the file paths and the values are the permissions of those files in a human-readable form (e.g., `-rwxrwxrwx`). Constraints - Your solution should handle cases where it does not have permission to read certain directories or files gracefully. - You may assume the functions from the `stat` module are available for use. Example For a directory structure: ``` example_dir/  file1.txt  file2.txt  dir1/   file3.txt  link1 -> file1.txt  fifo ``` The function: ```python def generate_directory_summary(directory_path: str) -> dict: # your code here ``` Called as: ```python result = generate_directory_summary(\\"example_dir\\") ``` Might produce the following output: ```python { \\"file_counts\\": { \\"Directories\\": 1, \\"Regular files\\": 3, \\"Symbolic links\\": 1, \\"Sockets\\": 0, \\"Character devices\\": 0, \\"Block devices\\": 0, \\"FIFOs\\": 1, \\"Doors\\": 0, \\"Event ports\\": 0, \\"Whiteouts\\": 0 }, \\"permissions\\": { \\"example_dir/file1.txt\\": \\"-rw-r--r--\\", \\"example_dir/file2.txt\\": \\"-rw-r--r--\\", \\"example_dir/dir1\\": \\"drwxr-xr-x\\", \\"example_dir/dir1/file3.txt\\": \\"-rw-r--r--\\", \\"example_dir/link1\\": \\"lrwxrwxrwx\\", \\"example_dir/fifo\\": \\"prw-r--r--\\" } } ``` Your function should demonstrate the use of `os.lstat()`, `stat` module functions to determine file types and permission bits, and handling exceptions gracefully to avoid program crashes due to inaccessible files. Notes - You may use the example structure above to create additional test cases. - Ensure your code is well-commented and follows good coding practices.","solution":"import os import stat def file_type(mode): if stat.S_ISDIR(mode): return \\"Directories\\" elif stat.S_ISREG(mode): return \\"Regular files\\" elif stat.S_ISLNK(mode): return \\"Symbolic links\\" elif stat.S_ISSOCK(mode): return \\"Sockets\\" elif stat.S_ISCHR(mode): return \\"Character devices\\" elif stat.S_ISBLK(mode): return \\"Block devices\\" elif stat.S_ISFIFO(mode): return \\"FIFOs\\" else: return \\"Unknown\\" def human_readable_permissions(mode): perms = [\'-\' for _ in range(10)] if stat.S_ISDIR(mode): perms[0] = \'d\' elif stat.S_ISLNK(mode): perms[0] = \'l\' elif stat.S_ISCHR(mode): perms[0] = \'c\' elif stat.S_ISBLK(mode): perms[0] = \'b\' elif stat.S_ISFIFO(mode): perms[0] = \'p\' elif stat.S_ISSOCK(mode): perms[0] = \'s\' permissions_map = [(stat.S_IRUSR, \'r\'), (stat.S_IWUSR, \'w\'), (stat.S_IXUSR, \'x\'), (stat.S_IRGRP, \'r\'), (stat.S_IWGRP, \'w\'), (stat.S_IXGRP, \'x\'), (stat.S_IROTH, \'r\'), (stat.S_IWOTH, \'w\'), (stat.S_IXOTH, \'x\')] for i, (bit, char) in enumerate(permissions_map): if mode & bit: perms[i + 1] = char return \'\'.join(perms) def generate_directory_summary(directory_path: str) -> dict: file_counts = { \\"Directories\\": 0, \\"Regular files\\": 0, \\"Symbolic links\\": 0, \\"Sockets\\": 0, \\"Character devices\\": 0, \\"Block devices\\": 0, \\"FIFOs\\": 0, \\"Doors\\": 0, \\"Event ports\\": 0, \\"Whiteouts\\": 0 } permissions = {} for root, dirs, files in os.walk(directory_path): # Handle directories for directory in dirs: try: path = os.path.join(root, directory) st = os.lstat(path) f_type = file_type(st.st_mode) file_counts[f_type] += 1 permissions[path] = human_readable_permissions(st.st_mode) except Exception as e: continue # Handle files for file in files: try: path = os.path.join(root, file) st = os.lstat(path) f_type = file_type(st.st_mode) if f_type in file_counts: file_counts[f_type] += 1 permissions[path] = human_readable_permissions(st.st_mode) except Exception as e: continue result = { \\"file_counts\\": file_counts, \\"permissions\\": permissions } return result"},{"question":"**Objective**: Create a Python program that manipulates file and directory paths, performs file comparisons, and utilizes temporary files. Use the modules `pathlib`, `filecmp`, and `tempfile` to achieve the desired functionality. **Problem Statement**: Write a function `process_directories(root_dir1: str, root_dir2: str) -> dict` that performs the following tasks: 1. **Path Manipulation**: - Use `pathlib` to list all `.txt` files in both `root_dir1` and `root_dir2` (recursively). 2. **File Comparison**: - Utilize the `filecmp` module to compare files with the same name in both directories. - For each pair of files with identical names, check if they are identical. If not, read the lines from both files and store them in a dictionary with the filename as the key and a list containing the differing lines from each file as the value. 3. **Temporary File Operations**: - Create a temporary file using the `tempfile` module. - Write the comparison results (from step 2) into the temporary file in a structured format. - Return the path to this temporary file and the dictionary containing the details of differing lines. **Input**: - `root_dir1` (string): The path to the first directory. - `root_dir2` (string): The path to the second directory. **Output**: - A dictionary where: - The key is the name of a file (string). - The value is a list with two elements. Each element is a list of differing lines from each file. - The path of the temporary file (string) containing the comparison results. **Constraints**: - Assume that the directories contain only text files (i.e., `.txt` files). **Example**: ```python root_dir1 = \\"path/to/directory1\\" root_dir2 = \\"path/to/directory2\\" result = process_directories(root_dir1, root_dir2) # Example output: # { # \'file1.txt\': [[\'line different in file1.txt in dir1\'], [\'line different in file1.txt in dir2\']], # \'file2.txt\': [[\'another line different in file2.txt in dir1\'], [\'another line different in file2.txt in dir2\']] # } # Temporary file location: \\"/tmp/tmpk9v8b_79\\" ``` **Notes**: - Ensure efficient handling of file reading to accommodate large files. - Provide appropriate error handling for file and directory operations. **Hint**: Refer to the official documentation of `pathlib`, `filecmp`, and `tempfile` to understand their usage and functionalities in depth. Good luck!","solution":"from pathlib import Path import filecmp import tempfile def process_directories(root_dir1: str, root_dir2: str) -> dict: root1 = Path(root_dir1) root2 = Path(root_dir2) files1 = {file.name: file for file in root1.rglob(\'*.txt\')} files2 = {file.name: file for file in root2.rglob(\'*.txt\')} common_files = set(files1.keys()).intersection(files2.keys()) differing_files = {} for file_name in common_files: file1 = files1[file_name] file2 = files2[file_name] if not filecmp.cmp(file1, file2, shallow=False): with file1.open(\'r\') as f1, file2.open(\'r\') as f2: lines1 = f1.readlines() lines2 = f2.readlines() differing_lines1 = [line for line in lines1 if line not in lines2] differing_lines2 = [line for line in lines2 if line not in lines1] differing_files[file_name] = [differing_lines1, differing_lines2] temp_file = tempfile.NamedTemporaryFile(delete=False, mode=\'w\') with temp_file as f: for file_name, lines in differing_files.items(): f.write(f\\"{file_name}:n\\") f.write(\\"File 1 differing lines:n\\") f.writelines(lines[0]) f.write(\\"nFile 2 differing lines:n\\") f.writelines(lines[1]) f.write(\'nn\') return differing_files, temp_file.name"},{"question":"**Objective**: Implement a function that processes and extracts data from an email, converting it into JSON format, and writing it to a specified file. **Problem Statement**: You are provided with an email message in string format. Your task is to write a function `process_email_to_json` that takes this email string, parses it to extract the sender, recipient, subject, and body of the email, and then writes this information into a JSON file with a specified filename. **Function Signature**: ```python def process_email_to_json(email_str: str, json_filename: str) -> None: pass ``` # Input: - `email_str` (str): A string representing the raw email content. - `json_filename` (str): The name of the JSON file where the extracted data should be saved. # Output: - The function should not return anything. It should write the extracted email data to the specified JSON file. # Constraints: - The email content will always have a sender, recipient, subject, and body. - The JSON file should contain the email data in the following format: ```json { \\"sender\\": \\"example@example.com\\", \\"recipient\\": \\"recipient@example.com\\", \\"subject\\": \\"Subject Line\\", \\"body\\": \\"Email body text\\" } ``` - Assume the email body does not contain any attachments or HTML content. # Example Usage: ```python email_content = From: sender@example.com To: recipient@example.com Subject: Test Email This is a test email body. json_output_file = \\"output.json\\" process_email_to_json(email_content, json_output_file) ``` After executing the above code, `output.json` should contain: ```json { \\"sender\\": \\"sender@example.com\\", \\"recipient\\": \\"recipient@example.com\\", \\"subject\\": \\"Test Email\\", \\"body\\": \\"This is a test email body.\\" } ``` # Notes: - Use the `email` module for parsing the email content. - Use the `json` module for encoding the data into JSON format and writing it to the file. - Ensure proper error handling for edge cases like missing elements or incorrect formats.","solution":"import json import email from email import policy from email.parser import BytesParser def process_email_to_json(email_str: str, json_filename: str) -> None: Processes an email string and writes the extracted data to a JSON file. Parameters: email_str (str): A string representing the raw email content. json_filename (str): The name of the JSON file where the extracted data should be saved. # Parse the email message from the string email_message = BytesParser(policy=policy.default).parsebytes(email_str.encode()) # Extract sender, recipient, subject, and body sender = email_message[\'From\'] recipient = email_message[\'To\'] subject = email_message[\'Subject\'] body = email_message.get_body(preferencelist=(\'plain\',)).get_content() # Create a dictionary to store the extracted email data email_data = { \\"sender\\": sender, \\"recipient\\": recipient, \\"subject\\": subject, \\"body\\": body } # Write the dictionary to a JSON file with open(json_filename, \'w\') as json_file: json.dump(email_data, json_file, indent=4)"},{"question":"# Question: Combining and Comparing DataFrames in Pandas You are given several datasets representing sales data for different regions. Your task is to combine these datasets, perform specific joins, and compare results to find inconsistencies between different reports. Provided Datasets: 1. **sales_North.csv**: ``` Date,Product,Sales 2023-01-01,Product_A,100 2023-01-02,Product_B,150 2023-01-03,Product_C,200 ``` 2. **sales_South.csv**: ``` Date,Product,Sales 2023-01-01,Product_A,110 2023-01-02,Product_B,145 2023-01-03,Product_D,190 ``` 3. **sales_East.csv**: ``` Date,Product,Sales 2023-01-01,Product_B,120 2023-01-02,Product_C,160 ``` 4. **sales_West.csv**: ``` Date,Product,Sales 2023-01-01,Product_A,105 2023-01-03,Product_C,210 ``` # Requirements: 1. **Concatenate Sales Data**: - Combine all four datasets (`sales_North.csv`, `sales_South.csv`, `sales_East.csv`, `sales_West.csv`) using `pd.concat` into a single DataFrame, while preserving the source of each record using a `keys` argument. 2. **Merge Sales Data for Comparison**: - Merge the datasets `sales_North.csv` and `sales_South.csv` using `pd.merge` on the columns `Date` and `Product`, indicating which sales records match or differ between the two regions. 3. **Identify Discrepancies**: - Compare the resulting DataFrame from the merge operation to identify discrepancies in the `Sales` data between the northern and southern regions. # Solution Template: ```python import pandas as pd # Step 1: Load the datasets sales_north = pd.read_csv(\'sales_North.csv\') sales_south = pd.read_csv(\'sales_South.csv\') sales_east = pd.read_csv(\'sales_East.csv\') sales_west = pd.read_csv(\'sales_West.csv\') # Step 2: Combine datasets using pd.concat combined_sales = pd.concat( [sales_north, sales_south, sales_east, sales_west], keys=[\'North\', \'South\', \'East\', \'West\'], names=[\'Region\', \'Row\'] ) # Step 3: Merge sales data for comparison between North and South north_south_merged = pd.merge( sales_north, sales_south, on=[\'Date\', \'Product\'], suffixes=(\'_north\', \'_south\'), indicator=True ) # Step 4: Identify discrepancies discrepancies = north_south_merged[ north_south_merged[\'Sales_north\'] != north_south_merged[\'Sales_south\'] ] # Output the results print(\\"Combined Sales Data:\\") print(combined_sales) print(\\"nNorth vs South Merge Results:\\") print(north_south_merged) print(\\"nDiscrepancies between North and South sales data:\\") print(discrepancies) ``` # Expected Outputs: 1. **Combined Sales Data**: A DataFrame with all sales data combined, indexed by regions (North, South, East, West). 2. **North vs South Merge Results**: A DataFrame indicating which records match or differ between the northern and southern regions, with a merge indicator column. 3. **Discrepancies**: A DataFrame listing records with different `Sales` values between the northern and southern regions. The provided code template guides you through data loading, combining datasets, merging for SQL-style comparison, and highlighting sales discrepancies. Make sure to handle files and paths appropriately in your local environment.","solution":"import pandas as pd def load_data(): sales_north = pd.DataFrame({ \'Date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-03\'], \'Product\': [\'Product_A\', \'Product_B\', \'Product_C\'], \'Sales\': [100, 150, 200] }) sales_south = pd.DataFrame({ \'Date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-03\'], \'Product\': [\'Product_A\', \'Product_B\', \'Product_D\'], \'Sales\': [110, 145, 190] }) sales_east = pd.DataFrame({ \'Date\': [\'2023-01-01\', \'2023-01-02\'], \'Product\': [\'Product_B\', \'Product_C\'], \'Sales\': [120, 160] }) sales_west = pd.DataFrame({ \'Date\': [\'2023-01-01\', \'2023-01-03\'], \'Product\': [\'Product_A\', \'Product_C\'], \'Sales\': [105, 210] }) return sales_north, sales_south, sales_east, sales_west def combine_sales_data(sales_north, sales_south, sales_east, sales_west): combined_sales = pd.concat( [sales_north, sales_south, sales_east, sales_west], keys=[\'North\', \'South\', \'East\', \'West\'], names=[\'Region\', \'Row\'] ) return combined_sales def merge_sales_data_for_comparison(sales_north, sales_south): north_south_merged = pd.merge( sales_north, sales_south, on=[\'Date\', \'Product\'], suffixes=(\'_north\', \'_south\'), indicator=True ) return north_south_merged def identify_discrepancies(north_south_merged): discrepancies = north_south_merged[ north_south_merged[\'Sales_north\'] != north_south_merged[\'Sales_south\'] ] return discrepancies"},{"question":"**Advanced POP3 Client Implementation** Using the `poplib` module, implement a function `fetch_latest_emails` that connects to a POP3 server, authenticates with a provided username and password, and retrieves some information about the latest emails in the user\'s mailbox. The function should connect using either SSL or non-SSL based on a parameter `use_ssl`. # Function Signature: ```python def fetch_latest_emails(host: str, username: str, password: str, num_emails: int, use_ssl: bool = False) -> list: pass ``` # Parameters: - `host` (str): The hostname or IP address of the POP3 server. - `username` (str): The username to authenticate with. - `password` (str): The password to authenticate with. - `num_emails` (int): The number of latest emails to retrieve. - `use_ssl` (bool): Whether to use SSL when connecting to the server. Default is `False`. # Returns: - `list` of `dict`: A list of dictionaries, each containing the following keys: - `id` (str): The unique ID of the email. - `subject` (str): The subject line of the email. - `sender` (str): The email address of the sender. # Constraints: - You should use the `UIDL` command to get the unique ID of an email. - You need to retrieve at least the `Subject` and `From` headers of each email. - The function should handle exceptions gracefully and return an empty list if an error occurs while fetching the emails. - The implementation should ensure proper cleanup (e.g., using `quit()` method). # Example: ```python host = \\"pop.example.com\\" username = \\"user@example.com\\" password = \\"password123\\" num_emails = 5 use_ssl = True emails = fetch_latest_emails(host, username, password, num_emails, use_ssl) ``` The above call should return a list of dictionaries with the latest 5 emails from the user\'s mailbox, showing the unique ID, subject, and sender for each email. If there are fewer than 5 emails, it should return information about the available emails. # Notes: - This question tests understanding of socket programming, error handling, and string manipulation in Python. - Ensure your solution handles corner cases such as empty mailboxes, incomplete email headers, and network errors smoothly.","solution":"import poplib from email.parser import Parser def fetch_latest_emails(host: str, username: str, password: str, num_emails: int, use_ssl: bool = False) -> list: Connects to a POP3 server, authenticates with the given username and password, retrieves the latest emails and returns a list of dictionaries containing email id, subject, and sender. Parameters: host (str): The hostname or IP address of the POP3 server. username (str): The username to authenticate with. password (str): The password to authenticate with. num_emails (int): The number of latest emails to retrieve. use_ssl (bool): Whether to use SSL when connecting to the server. Default is False. Returns: list of dict: A list of dictionaries containing the email id, subject, and sender. emails_info = [] try: if use_ssl: server = poplib.POP3_SSL(host) else: server = poplib.POP3(host) server.user(username) server.pass_(password) message_count, _ = server.stat() num_emails = min(num_emails, message_count) for i in range(message_count, message_count - num_emails, -1): # Get the email unique id with UIDL command response, message_lines, _ = server.retr(i) # Parse the email content email_body = b\\"rn\\".join(message_lines).decode(\'utf-8\') email_message = Parser().parsestr(email_body) email_info = { \'id\': server.uidl(i).split()[2].decode(\'utf-8\'), \'subject\': email_message[\'subject\'], \'sender\': email_message[\'from\'] } emails_info.append(email_info) server.quit() except Exception as e: # Gracefully handle any exception and return an empty list print(f\\"An error occurred: {e}\\") emails_info = [] return emails_info"},{"question":"You are provided with a dataset containing predictor variables and target variables. Your task is to use the `PLSRegression` class from the `sklearn.cross_decomposition` module to create a model that predicts the target variables from the predictor variables. Implement the following functionality: 1. **load_data**: A function to load the dataset. 2. **train_model**: A function to train the `PLSRegression` model. 3. **predict_data**: A function to make predictions using the trained model. 4. **evaluate_model**: A function to evaluate the model using mean squared error (MSE). # Input Format - The dataset is in a CSV file with predictor variables `X1, X2, ..., Xn` and target variables `Y1, Y2, ..., Ym`. - You will be provided with two CSV files: `train.csv` (for training data) and `test.csv` (for testing data). # Output Format - Mean squared error (MSE) of predictions on the test data. # Constraints - Use the `PLSRegression` class with `n_components` set to 2. - Use mean squared error as the evaluation criterion. # Performance Requirements - The solution should efficiently handle datasets with up to 1000 rows and 50 columns. # Function Signatures ```python import pandas as pd from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error def load_data(file_path: str) -> pd.DataFrame: Load the dataset from a given file path. Parameters: - file_path: str, the path of the CSV file Returns: - data: pd.DataFrame, the loaded dataset pass def train_model(train_data: pd.DataFrame) -> PLSRegression: Train the PLSRegression model on the training dataset. Parameters: - train_data: pd.DataFrame, the training dataset Returns: - model: PLSRegression, the trained PLSRegression model pass def predict_data(model: PLSRegression, test_data: pd.DataFrame) -> pd.DataFrame: Predict the target variables for the test dataset using the trained model. Parameters: - model: PLSRegression, the trained model - test_data: pd.DataFrame, the test dataset Returns: - predictions: pd.DataFrame, the predicted target variables pass def evaluate_model(predictions: pd.DataFrame, actual: pd.DataFrame) -> float: Evaluate the model predictions using mean squared error. Parameters: - predictions: pd.DataFrame, the predicted target variables - actual: pd.DataFrame, the actual target variables Returns: - mse: float, the mean squared error of the predictions pass # Example usage: # train_data = load_data(\'train.csv\') # test_data = load_data(\'test.csv\') # model = train_model(train_data) # predictions = predict_data(model, test_data.drop(columns=[\'Y1\', \'Y2\'])) # mse = evaluate_model(predictions, test_data[[\'Y1\', \'Y2\']]) # print(f\'Mean Squared Error: {mse}\') ``` **Hints**: - To split the dataset into predictor and target variables, you can use `data.iloc[:, :-2]` for predictors and `data.iloc[:, -2:]` for targets, assuming the last two columns are targets. - Use the `fit` method of the `PLSRegression` class to train the model. - Use the `predict` method of the `PLSRegression` class to make predictions.","solution":"import pandas as pd from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error def load_data(file_path: str) -> pd.DataFrame: Load the dataset from a given file path. Parameters: - file_path: str, the path of the CSV file Returns: - data: pd.DataFrame, the loaded dataset return pd.read_csv(file_path) def train_model(train_data: pd.DataFrame) -> PLSRegression: Train the PLSRegression model on the training dataset. Parameters: - train_data: pd.DataFrame, the training dataset Returns: - model: PLSRegression, the trained PLSRegression model X_train = train_data.iloc[:, :-2] Y_train = train_data.iloc[:, -2:] model = PLSRegression(n_components=2) model.fit(X_train, Y_train) return model def predict_data(model: PLSRegression, test_data: pd.DataFrame) -> pd.DataFrame: Predict the target variables for the test dataset using the trained model. Parameters: - model: PLSRegression, the trained model - test_data: pd.DataFrame, the test dataset Returns: - predictions: pd.DataFrame, the predicted target variables return pd.DataFrame(model.predict(test_data)) def evaluate_model(predictions: pd.DataFrame, actual: pd.DataFrame) -> float: Evaluate the model predictions using mean squared error. Parameters: - predictions: pd.DataFrame, the predicted target variables - actual: pd.DataFrame, the actual target variables Returns: - mse: float, the mean squared error of the predictions return mean_squared_error(actual, predictions)"},{"question":"Objective: Demonstrate your understanding of PyTorch\'s block mask utilities by implementing a customized flexible attention mechanism. Problem Statement: Implement a function `custom_flex_attention` that takes an input tensor and applies a flexible attention mechanism with a custom block mask. The function should construct the mask using the provided block mask utilities and then apply it to the input tensor using the `flex_attention` function. The resulting tensor should be returned. Function Signature: ```python def custom_flex_attention(input_tensor: torch.Tensor, mask_type: str) -> torch.Tensor: pass ``` Input: - `input_tensor` (torch.Tensor): A 2D tensor of shape `(N, D)`, where `N` is the number of elements and `D` is the feature dimension. - `mask_type` (str): A string representing the type of block mask to create. Possible values are \\"block\\", \\"nested_block\\", \\"noop\\", or a combination using \\"and\\" or \\"or\\" (e.g., \\"block_and_noop\\"). Output: - Returns a tensor of the same shape as `input_tensor` after applying the flexible attention mechanism. Constraints: 1. You should use the `create_block_mask`, `create_nested_block_mask`, and `noop_mask` utilities to create masks. 2. Combine masks using `and_masks` and `or_masks` based on the `mask_type` input. 3. Ensure that the resulting mask is applied using the `flex_attention` function. 4. Assume the input tensor is always a valid 2D tensor with an appropriate shape. Examples: **Example 1:** ```python input_tensor = torch.rand(10, 5) mask_type = \\"block\\" output_tensor = custom_flex_attention(input_tensor, mask_type) print(output_tensor.shape) # Should output: torch.Size([10, 5]) ``` **Example 2:** ```python input_tensor = torch.rand(15, 6) mask_type = \\"block_and_noop\\" output_tensor = custom_flex_attention(input_tensor, mask_type) print(output_tensor.shape) # Should output: torch.Size([15, 6]) ``` Additional Notes: 1. You may assume that the mask utilities and the flex_attention function are defined as described in the documentation. 2. Pay attention to the detailed handling of different mask types and combinations. Consider the following mask utilities in your implementation: ```python def create_block_mask(): pass def create_nested_block_mask(): pass def noop_mask(): pass def and_masks(mask1, mask2): pass def or_masks(mask1, mask2): pass def flex_attention(input_tensor, mask): pass ``` **Hint:** Think about how you can interpret the `mask_type` string to dynamically create different mask combinations and apply them to the input tensor.","solution":"import torch def create_block_mask(shape): return torch.zeros(shape) def create_nested_block_mask(shape): return torch.eye(shape[0], shape[1]) def noop_mask(shape): return torch.ones(shape) def and_masks(mask1, mask2): return torch.logical_and(mask1, mask2).float() def or_masks(mask1, mask2): return torch.logical_or(mask1, mask2).float() def flex_attention(input_tensor, mask): return input_tensor * mask def custom_flex_attention(input_tensor: torch.Tensor, mask_type: str) -> torch.Tensor: Apply a flexible attention mechanism with a custom block mask on the input tensor. Parameters: input_tensor (torch.Tensor): A 2D tensor of shape (N, D). mask_type (str): A string representing the type of block mask to create. Returns: torch.Tensor: The result tensor after applying the block mask and attention. shape = input_tensor.shape if mask_type == \\"block\\": mask = create_block_mask(shape) elif mask_type == \\"nested_block\\": mask = create_nested_block_mask(shape) elif mask_type == \\"noop\\": mask = noop_mask(shape) elif mask_type == \\"block_and_noop\\": mask1 = create_block_mask(shape) mask2 = noop_mask(shape) mask = and_masks(mask1, mask2) else: raise ValueError(f\\"Unsupported mask_type: {mask_type}\\") output_tensor = flex_attention(input_tensor, mask) return output_tensor"},{"question":"# Custom Codec Implementation and Error Handling **Objective**: Implement a custom codec for a hypothetical encoding and additionally, design a custom error handling strategy for any encoding errors encountered during the use of this codec. **Task 1**: Custom Codec Implementation Write a Python function `custom_encode(input_string: str) -> bytes` that encodes an input string using a hypothetical custom encoding scheme where: - Each character in the input string is replaced by its Unicode code point shifted by 3 positions. - Example: \'A\' (Unicode code point 65) is encoded as \'D\' (Unicode code point 68). Additionally, write a corresponding decoding function `custom_decode(encoded_bytes: bytes) -> str` that reverses the encoding scheme described above. **Task 2**: Custom Error Handling Strategy Write a custom error handling function `handle_encoding_error(exception)` that: - Replaces any non-encodable characters with an asterisk \'*\' during the encoding process. - Returns a two-item tuple as required by the `PyCodec_RegisterError` function. **Input and Output Formats**: **custom_encode function** - **Input**: A string `input_string`. - **Output**: A bytes object representing the encoded string. **custom_decode function** - **Input**: A bytes object `encoded_bytes`. - **Output**: A string representing the decoded content. **handle_encoding_error function** - **Input**: An instance of `UnicodeEncodeError`. - **Output**: A tuple containing a replacement string and an integer offset. **Example Usage**: ```python # Task 1 Examples encoded = custom_encode(\\"HELLO\\") print(encoded) # should output byte sequence for encoded text decoded = custom_decode(encoded) print(decoded) # should output \\"HELLO\\" # Task 2 Example try: # This should raise a UnicodeEncodeError as per the custom encoding scheme encoded = custom_encode(\\"HELLO\\") except UnicodeEncodeError as e: handle_encoding_error(e) # should handle the error and provide a meaningful replacement ``` **Constraints**: - Do not use any built-in or external libraries for encoding or decoding. - Focus on demonstrating a clear understanding of codec implementation and error handling strategies.","solution":"def custom_encode(input_string: str) -> bytes: Function to encode an input string where each character is replaced by its Unicode code point shifted by 3 positions. encoded_bytes = bytearray() for char in input_string: try: encoded_char = chr(ord(char) + 3) encoded_bytes.extend(encoded_char.encode(\'utf-8\')) except UnicodeEncodeError as e: handle_encoding_error(e) return bytes(encoded_bytes) def custom_decode(encoded_bytes: bytes) -> str: Function to decode the encoded bytes where each character\'s Unicode code point is reversed by 3 positions. decoded_string = \'\' byte_str = encoded_bytes.decode(\'utf-8\') for char in byte_str: decoded_char = chr(ord(char) - 3) decoded_string += decoded_char return decoded_string def handle_encoding_error(exception): Custom error handling function which replaces any non-encodable characters with \'*\'. replacement_string = \\"*\\" * (exception.end - exception.start) return (replacement_string, exception.end)"},{"question":"**Objective:** Implement a Python module that leverages the `atexit` module to manage and log the resource usage of an application, ensuring resources are properly released and logged upon program termination. **Problem Statement:** Write a Python class `ResourceManager` that uses the `atexit` module to automatically release resources and log information when the program terminates. **Requirements:** 1. **ResourceManager Class:** - `__init__(self, resource_name: str)`: Initializes the resource manager with a resource name. - `use_resource(self, usage: int)`: Simulates the usage of the resource by incrementing the usage count by the given amount. - `release_resource(self)`: Releases the resource and logs the release information including the resource name, usage count, and a termination message to a log file named `resource_log.txt`. - `log_release(self)`: A function that will be registered using `atexit` to ensure resources are released and logged properly when the program terminates. 2. **Functionality:** - Use `atexit.register` to ensure `release_resource` is called automatically upon normal program termination. - Use file I/O to log the resource release information. 3. **Log Format:** - Each release log entry should include the resource name, the final usage count, and a custom termination message. - Example log entry: `\\"Resource: resource_name, Usage: 100, Status: Released upon termination\\"` **Input:** - The `ResourceManager` class does not take direct input from the user, but the methods are called within the program to simulate resource usage. **Output:** - The output will be written to the `resource_log.txt` file with the specified log format. **Example Usage:** ```python # Example usage if __name__ == \\"__main__\\": rm = ResourceManager(\\"Database Connection\\") rm.use_resource(50) rm.use_resource(30) # No need to manually call release_resource, it will be called at termination ``` **Constraints:** - The `resource_log.txt` file should be created in the current working directory. - Assume proper handling of file access and permissions. - Ensure the implementation handles multiple instances of `ResourceManager` if created. **Additional Information:** - Do not use `os._exit()` in your solution as it will bypass the atexit handlers.","solution":"import atexit class ResourceManager: def __init__(self, resource_name: str): self.resource_name = resource_name self.usage = 0 atexit.register(self.release_resource) def use_resource(self, usage: int): self.usage += usage def release_resource(self): with open(\\"resource_log.txt\\", \\"a\\") as log_file: log_file.write(f\\"Resource: {self.resource_name}, Usage: {self.usage}, Status: Released upon terminationn\\")"},{"question":"Objective: You are required to implement a function that can decode the Python version from a given integer encoded in the `PY_VERSION_HEX` format and also encode individual version components into this format. Description: Write two functions, `encode_version` and `decode_version`. 1. **encode_version(major: int, minor: int, micro: int, level: str, serial: int) -> int**: - This function takes the major, minor, micro version numbers, release level (as \'alpha\', \'beta\', \'candidate\', or \'final\'), and serial, then encodes them into the `PY_VERSION_HEX` format. 2. **decode_version(version_hex: int) -> Tuple[int, int, int, str, int]**: - This function takes a version encoded as a single integer (`PY_VERSION_HEX`) and decodes it into its individual components: major, minor, micro version numbers, release level, and serial. Input: - For `encode_version`: - `major` (int): Major version number (0 <= major <= 255). - `minor` (int): Minor version number (0 <= minor <= 255). - `micro` (int): Micro version number (0 <= micro <= 255). - `level` (str): Release level (\'alpha\', \'beta\', \'candidate\', \'final\'). - `serial` (int): Release serial (0 <= serial <= 255). - For `decode_version`: - `version_hex` (int): A 32-bit integer representing the encoded version. Output: - For `encode_version`: - An integer representing the encoded version in `PY_VERSION_HEX` format. - For `decode_version`: - A tuple containing: - `major` (int): Major version number. - `minor` (int): Minor version number. - `micro` (int): Micro version number. - `level` (str): Release level (\'alpha\', \'beta\', \'candidate\', \'final\'). - `serial` (int): Release serial. Constraints: - The release level will always be one of \'alpha\', \'beta\', \'candidate\', \'final\'. - You can assume that the input values will always be within the valid ranges. Examples: ```python def encode_version(major, minor, micro, level, serial): # Your implementation here def decode_version(version_hex): # Your implementation here # Example Cases # Case 1: # Input: print(encode_version(3, 10, 0, \'final\', 0)) # Output: # 0x030a00f0 # Case 2: # Input: print(decode_version(0x030401a2)) # Output: # (3, 4, 1, \'alpha\', 2) ``` # Notes: - To convert the release level to its encoded value: - \'alpha\' -> 0xA - \'beta\' -> 0xB - \'candidate\' -> 0xC - \'final\' -> 0xF - You may find the bitwise operators in Python helpful for encoding and decoding the version information.","solution":"def encode_version(major, minor, micro, level, serial): Encodes the version information into the PY_VERSION_HEX format. levels = { \'alpha\': 0xA, \'beta\': 0xB, \'candidate\': 0xC, \'final\': 0xF } level_code = levels[level] version_hex = (major << 24) | (minor << 16) | (micro << 8) | (level_code << 4) | serial return version_hex def decode_version(version_hex): Decodes the PY_VERSION_HEX format into individual version components. major = (version_hex >> 24) & 0xFF minor = (version_hex >> 16) & 0xFF micro = (version_hex >> 8) & 0xFF level_code = (version_hex >> 4) & 0xF serial = version_hex & 0xF levels = { 0xA: \'alpha\', 0xB: \'beta\', 0xC: \'candidate\', 0xF: \'final\' } level = levels[level_code] return major, minor, micro, level, serial"},{"question":"You are required to create and customize plots using `seaborn.objects` in Python. The customization should include setting global themes, plot-specific themes, and various display settings. Follow the steps below to complete the task: 1. **Global Theme Configuration**: - Set the axes facecolor to `white` for all plots. - Use the `whitegrid` style for the plots. 2. **Plot-Specific Theme Configuration**: - Create a scatter plot with a different theme (from the global theme). Set the axes facecolor to `lightgrey` only for this plot. 3. **Display Configuration**: - Configure the display settings to output the plots in SVG format. - Disable HiDPI for better performance. - Set the display scaling factor to `0.8`. # Input and Output Formats - You need to use the provided functions and attributes to customize the theme and display settings. - Create a scatter plot using the Seaborn objects (`so`) API showcasing the theme changes. # Constraints and Requirements - Use the `seaborn.objects` module for all your configurations. - Ensure that the plot-specific theme does not alter the global theme settings. - The input data for the scatter plot can be generated using any random data generation library like `numpy`. # Example Code Here is a template to help you get started: ```python import seaborn.objects as so import numpy as np import matplotlib.pyplot as plt # Global theme configuration so.Plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" from seaborn import axes_style so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) # Display configuration so.Plot.config.display[\\"format\\"] = \\"svg\\" so.Plot.config.display[\\"hidpi\\"] = False so.Plot.config.display[\\"scaling\\"] = 0.8 # Generate some random data np.random.seed(10) x = np.random.rand(30) y = np.random.rand(30) # Plot-specific theme configuration for a scatter plot p = so.Plot(x=x, y=y).theme({\\"axes.facecolor\\": \\"lightgrey\\"}) # Create scatter plot p.add(so.Dots()) plt.show() ``` In this coding assessment task, ensure to provide detailed comments explaining each configuration and setting you have applied.","solution":"import seaborn.objects as so import numpy as np import matplotlib.pyplot as plt # Global theme configuration so.Plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" # Set the axes facecolor to white from seaborn import axes_style so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) # Use the whitegrid style # Display configuration so.Plot.config.display[\\"format\\"] = \\"svg\\" # Output the plots in SVG format so.Plot.config.display[\\"hidpi\\"] = False # Disable HiDPI for better performance so.Plot.config.display[\\"scaling\\"] = 0.8 # Set the display scaling factor to 0.8 # Generate some random data np.random.seed(10) x = np.random.rand(30) y = np.random.rand(30) # Plot-specific theme configuration for a scatter plot scatter_plot = so.Plot(x=x, y=y).theme({\\"axes.facecolor\\": \\"lightgrey\\"}) # Create scatter plot scatter_plot.add(so.Dots()) plt.show()"},{"question":"# Question: Producer-Consumer Problem using _thread Module Problem Statement Implement a solution to the Producer-Consumer problem using the `_thread` module\'s low-level threading API. The problem consists of two types of threads, producers and consumers, that share a common, fixed-size buffer used as a queue. 1. **Producers** generate data and add them to the buffer. 2. **Consumers** remove and process data from the buffer. 3. The buffer has a limited size, and synchronization is needed to ensure that the producers do not add data when the buffer is full, and consumers do not consume data when the buffer is empty. Requirements - Implement two functions, `producer(buffer_lock, buffer, max_size)` and `consumer(buffer_lock, buffer)`. - Use the `_thread.start_new_thread` function to create multiple producer and consumer threads. - Use the `_thread.allocate_lock` function to handle synchronization between threads. Details - The buffer should be implemented as a list with a fixed maximum size (`max_size`). - The producer function should: - Generate an integer (use a counter starting at 1) and add it to the buffer. - Sleep for a random short duration (`0.1` to `1` second) between producing items to simulate processing time. - The consumer function should: - Remove an item from the buffer and simulate processing by sleeping for a random short duration (`0.1` to `1` second) before consuming the next item. - Use locks to manage access to the shared buffer and prevent race conditions. Constraints - The maximum size of the buffer (`max_size`) should be a positive integer. - Both producer and consumer functions should run indefinitely. Example Here\'s an example that demonstrates how to start the producer and consumer threads and manage the shared buffer with locks. ```python import _thread import time import random def producer(buffer_lock, buffer, max_size): count = 1 while True: time.sleep(random.uniform(0.1, 1)) # Simulate time to produce an item buffer_lock.acquire() if len(buffer) < max_size: buffer.append(count) print(f\\"Produced: {count}\\") count += 1 buffer_lock.release() def consumer(buffer_lock, buffer): while True: time.sleep(random.uniform(0.1, 1)) # Simulate time to consume an item buffer_lock.acquire() if buffer: item = buffer.pop(0) print(f\\"Consumed: {item}\\") buffer_lock.release() if __name__ == \\"__main__\\": BUFFER_SIZE = 10 buffer = [] buffer_lock = _thread.allocate_lock() # Start producer threads for _ in range(2): _thread.start_new_thread(producer, (buffer_lock, buffer, BUFFER_SIZE)) # Start consumer threads for _ in range(2): _thread.start_new_thread(consumer, (buffer_lock, buffer)) # Keep main thread alive to let producer and consumer threads run while True: time.sleep(1) ``` Submission Submit your functions `producer` and `consumer`. Ensure that your solution handles the synchronization correctly and avoids deadlocks or race conditions while managing the shared buffer.","solution":"import _thread import time import random def producer(buffer_lock, buffer, max_size): count = 1 while True: time.sleep(random.uniform(0.1, 1)) # Simulate time to produce an item buffer_lock.acquire() if len(buffer) < max_size: buffer.append(count) print(f\\"Produced: {count}\\") count += 1 buffer_lock.release() def consumer(buffer_lock, buffer): while True: time.sleep(random.uniform(0.1, 1)) # Simulate time to consume an item buffer_lock.acquire() if buffer: item = buffer.pop(0) print(f\\"Consumed: {item}\\") buffer_lock.release()"},{"question":"**Coding Assessment Question** # Objective: Implement a custom PyTorch function which includes both real and complex operations. Verify its gradients using `gradcheck` and `gradgradcheck`. # Instructions: 1. **Function Implementation:** - Implement a custom PyTorch function `custom_function` which involves both real and complex-valued inputs and outputs. - The function should take a real-valued tensor `x` and a complex-valued tensor `z` and return a real-valued tensor `y`. 2. **Gradient Verification:** - Implement code to verify the gradients of `custom_function` using PyTorch\'s `gradcheck`. - Additionally, verify the higher-order derivatives using `gradgradcheck`. # Constraints: - The function must handle tensors of size at least `(2, 2)`. - Ensure numerical stability in your function to avoid errors during gradient checking. # Expected Input and Output: - **Input:** - `x`: A real-valued tensor of size `(2, 2)`. - `z`: A complex-valued tensor of size `(2, 2)`. - **Output:** - `y`: A real-valued tensor of size `(2, 2)`. # Example: ```python import torch from torch.autograd import gradcheck, gradgradcheck # Define the custom function def custom_function(x, z): # Ensure that the inputs require gradients x = x.requires_grad_() z = z.requires_grad_() # Function operations involving both x and z, returning a real-valued tensor real_part = x @ x.T complex_part = torch.real(z @ torch.conj(z).T) y = real_part + complex_part return y # Define the inputs real_input = torch.randn((2, 2), dtype=torch.float64, requires_grad=True) complex_input = torch.randn((2, 2), dtype=torch.complex128, requires_grad=True) # Perform gradcheck inputs = (real_input, complex_input) assert gradcheck(custom_function, inputs), \\"Gradcheck failed\\" # Perform gradgradcheck assert gradgradcheck(custom_function, inputs), \\"Gradgradcheck failed\\" print(\\"Both gradcheck and gradgradcheck passed.\\") ``` In your implementation: - Ensure numerical stability and proper gradient computations. - Your `custom_function` should be generalized to handle tensors of any size `(N, N)` where `N  2`.","solution":"import torch def custom_function(x, z): Custom PyTorch function that performs operations on real and complex-valued inputs. Args: x (torch.Tensor): Real-valued input tensor of shape (2, 2) z (torch.Tensor): Complex-valued input tensor of shape (2, 2) Returns: torch.Tensor: Real-valued tensor resulting from the operations, of shape (2, 2) # Ensure that the inputs require gradients x = x.requires_grad_() z = z.requires_grad_() # Function operations involving both x and z, returning a real-valued tensor real_part = x @ x.T complex_part = torch.real(z @ torch.conj(z).T) y = real_part + complex_part return y"},{"question":"# Asynchronous Task Management and Synchronization **Problem Statement:** You are tasked with creating an asynchronous application that simulates a basic web scraper. The web scraper will perform the following operations: 1. Fetch data from a set of URLs. 2. Process the fetched data. 3. Aggregate the results. You need to implement the following functions: - `fetch_url(url: str) -> str` - Asynchronously fetches the content from the given URL and returns it as a string. This function should simulate the fetching process using `asyncio.sleep`. - `process_data(data: str) -> dict` - Processes the fetched data (string) and returns it as a dictionary. This function should also use `asyncio.sleep` to simulate processing time. - `aggregate_results(results: List[dict]) -> dict` - Aggregates a list of processed data dictionaries into a single dictionary. Additionally, create an orchestrating function `main(urls: List[str]) -> dict` that: 1. Creates an asyncio event loop. 2. Manages task creation and execution for fetching and processing data concurrently. 3. Uses appropriate synchronization primitives to ensure safe aggregation of results. 4. Handles any possible timeouts or cancellations. **Function Signatures:** ```python async def fetch_url(url: str) -> str: pass async def process_data(data: str) -> dict: pass def aggregate_results(results: List[dict]) -> dict: pass async def main(urls: List[str]) -> dict: pass ``` **Input:** - `urls`: A list of strings representing the URLs to be scraped. **Output:** - Returns a dictionary representing the aggregated results of all URLs. **Constraints:** - You must use `asyncio` for asynchronous task management. - Simulate network delay or processing delay using `asyncio.sleep`. - Use appropriate exception handling to manage any `asyncio.TimeoutError` or `asyncio.CancelledError`. - Synchronization for the aggregation process must be handled appropriately to avoid any race conditions. **Example Usage:** ```python import asyncio urls = [\\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\"] results = asyncio.run(main(urls)) print(results) ``` **Hints:** - You can use `asyncio.gather` to run fetch and process tasks concurrently. - Utilize `asyncio.Queue` for task distribution if needed. - The `aggregate_results` function should be called after all fetch and process tasks are completed. - Ensure proper exception handling for any asynchronous operation that might timeout or get canceled.","solution":"import asyncio from typing import List, Dict async def fetch_url(url: str) -> str: Asynchronously fakes fetching the content from the given URL. await asyncio.sleep(1) # Simulating network delay return f\\"Content of {url}\\" async def process_data(data: str) -> dict: Asynchronously fakes processing the fetched data. await asyncio.sleep(1) # Simulating processing delay return {data: len(data)} def aggregate_results(results: List[dict]) -> dict: Aggregates a list of processed dictionaries into a single dictionary. aggregated = {} for result in results: aggregated.update(result) return aggregated async def main(urls: List[str]) -> dict: Orchestrates the fetching, processing, and aggregation of data from URLs. fetch_tasks = [fetch_url(url) for url in urls] try: fetched_data = await asyncio.gather(*fetch_tasks) except asyncio.TimeoutError: print(\\"Timeout during fetching\\") return {} except asyncio.CancelledError: print(\\"Fetching tasks were cancelled\\") return {} process_tasks = [process_data(data) for data in fetched_data] try: processed_data = await asyncio.gather(*process_tasks) except asyncio.TimeoutError: print(\\"Timeout during processing\\") return {} except asyncio.CancelledError: print(\\"Processing tasks were cancelled\\") return {} return aggregate_results(processed_data)"}]'),z={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},F={class:"card-container"},R={key:0,class:"empty-state"},q=["disabled"],M={key:0},j={key:1};function O(i,e,l,m,n,o){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"prompts chat")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>n.searchQuery=r),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>n.searchQuery="")},"  ")):d("",!0)]),t("div",F,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),x(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",R,' No results found for "'+c(n.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[n.isLoading?(a(),s("span",j,"Loading...")):(a(),s("span",M,"See more"))],8,q)):d("",!0)])}const N=p(z,[["render",O],["__scopeId","data-v-5f9e00ac"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/47.md","filePath":"chatai/47.md"}'),L={name:"chatai/47.md"},H=Object.assign(L,{setup(i){return(e,l)=>(a(),s("div",null,[w(N)]))}});export{Y as __pageData,H as default};
