import{_ as p,o as a,c as n,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function A(i,e,l,m,s,o){return a(),n("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const S=p(k,[["render",A],["__scopeId","data-v-1f33f8d8"]]),I=JSON.parse('[{"question":"# Terminal Control with Python\'s `tty` Module You are tasked with designing a small command-line tool in Python to interactively toggle the terminal mode between `raw` and `cbreak`. Requirements: 1. **Functions to Implement:** - `enable_raw_mode(fd: int) -> None`: This function should set the terminal to raw mode using the `tty.setraw` function. - `enable_cbreak_mode(fd: int) -> None`: This function should set the terminal to cbreak mode using the `tty.setcbreak` function. - `reset_terminal_mode(fd: int, original_attrs: list) -> None`: This function should reset the terminal to its original mode using `termios.tcsetattr`. - `toggle_modes(fd: int) -> None`: This function should toggle between `raw` and `cbreak` modes every 5 seconds until the user interrupts the program with a keyboard signal (e.g., CTRL+C). 2. **Input and Output Specifications:** - The file descriptor `fd` will always point to an open terminal (usually `sys.stdin.fileno()`). - The `reset_terminal_mode` function receives `original_attrs`, which is the initial terminal attributes returned by `termios.tcgetattr(fd)`. 3. **Constraints:** - The program should properly handle keyboard interrupts to exit gracefully and restore the terminal state to its original mode. - The functions must leverage the `tty` and `termios` modules together to manipulate terminal settings. 4. **Performance Requirements:** - Ensure the program is responsive to keyboard interrupts and restores the terminal\'s original settings before exiting. Example Usage: ```python import sys import termios import tty import time import signal def enable_raw_mode(fd: int) -> None: tty.setraw(fd) def enable_cbreak_mode(fd: int) -> None: tty.setcbreak(fd) def reset_terminal_mode(fd: int, original_attrs: list) -> None: termios.tcsetattr(fd, termios.TCSADRAIN, original_attrs) def toggle_modes(fd: int) -> None: original_attrs = termios.tcgetattr(fd) def signal_handler(sig, frame): reset_terminal_mode(fd, original_attrs) sys.exit(0) signal.signal(signal.SIGINT, signal_handler) try: while True: enable_raw_mode(fd) time.sleep(5) enable_cbreak_mode(fd) time.sleep(5) finally: reset_terminal_mode(fd, original_attrs) if __name__ == \'__main__\': fd = sys.stdin.fileno() toggle_modes(fd) ``` In this example, the `toggle_modes` function alternates the terminal mode between raw and cbreak every 5 seconds and restores the original terminal settings upon interruption.","solution":"import sys import termios import tty import time import signal def enable_raw_mode(fd: int) -> None: Sets the terminal to raw mode. :param fd: File descriptor of the terminal tty.setraw(fd) def enable_cbreak_mode(fd: int) -> None: Sets the terminal to cbreak (canonical) mode. :param fd: File descriptor of the terminal tty.setcbreak(fd) def reset_terminal_mode(fd: int, original_attrs: list) -> None: Resets the terminal mode to the original settings. :param fd: File descriptor of the terminal :param original_attrs: Original terminal attributes to be restored termios.tcsetattr(fd, termios.TCSADRAIN, original_attrs) def toggle_modes(fd: int) -> None: Toggles between raw and cbreak modes every 5 seconds until interrupted by the user. :param fd: File descriptor of the terminal original_attrs = termios.tcgetattr(fd) def signal_handler(sig, frame): reset_terminal_mode(fd, original_attrs) sys.exit(0) signal.signal(signal.SIGINT, signal_handler) try: while True: enable_raw_mode(fd) time.sleep(5) enable_cbreak_mode(fd) time.sleep(5) finally: reset_terminal_mode(fd, original_attrs) if __name__ == \'__main__\': fd = sys.stdin.fileno() toggle_modes(fd)"},{"question":"<|Analysis Begin|> The provided documentation outlines the use of the `sns.diverging_palette` function in the seaborn library. This function is used to create diverging color palettes, primarily useful for visualizing data where the meaningful center point (often zero) splits the data into two contrasting values. The examples demonstrate: - Basic usage of generating a blue to red palette. - Modifying the central color to dark. - Returning a continuous colormap instead of a discrete palette. - Increasing separation around the central value. - Creating palettes with different color spectra (magenta-to-green). - Adjusting color properties like saturation and lightness of the endpoints. This documentation can help us design a question that assesses students\' understanding of generating and customizing diverging palettes using seaborn. Students should be tasked with creating and manipulating such palettes and demonstrating their understanding through visual examples, combining various parameters available in `sns.diverging_palette`. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: You are required to demonstrate your understanding of seaborn\'s `sns.diverging_palette` function by performing the following tasks: 1. **Generate and Display Basic Palettes**: - Create a basic diverging color palette that transitions from blue to red through white and display it. 2. **Modify Palettes with Custom Parameters**: - Create a diverging color palette with the central color set to dark. - Create another palette but modify it to be returned as a continuous colormap instead of a discrete palette. 3. **Advanced Customization**: - Create a diverging color palette (blue to red) with an increased amount of separation around the center value. - Create a magenta-to-green palette with reduced saturation of the endpoints. - Create a magenta-to-green diverging palette with reduced lightness of the endpoints. # Specifications: - **Input**: No user input is required; you will define all parameters within your code. - **Output**: Visual plot displaying the palettes. # Constraints: - Utilize seaborn\'s `sns.diverging_palette` function to generate color palettes. - Use matplotlib to display palettes if needed. - Ensure clarity by properly labeling each plot to describe what customization has been applied. # Example: ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np def display_palette(palette, title): sns.palplot(palette) plt.title(title) plt.show() # 1. Basic diverging palette palette1 = sns.diverging_palette(240, 20) display_palette(palette1, \'Basic Blue to Red Diverging Palette\') # 2. Central color set to dark palette2 = sns.diverging_palette(240, 20, center=\\"dark\\") display_palette(palette2, \'Diverging Palette with Dark Center\') # 3. Continuous colormap palette3 = sns.diverging_palette(240, 20, as_cmap=True) sns.heatmap(np.random.rand(10, 10), cmap=palette3) plt.title(\'Continuous Diverging Colormap\') plt.show() # 4. Increased separation around center palette4 = sns.diverging_palette(240, 20, sep=30) display_palette(palette4, \'Palette with Increased Separation\') # 5. Magenta-to-green with reduced saturation palette5 = sns.diverging_palette(280, 150, s=50) display_palette(palette5, \'Magenta-to-Green with Reduced Saturation\') # 6. Magenta-to-green with reduced lightness palette6 = sns.diverging_palette(280, 150, l=35) display_palette(palette6, \'Magenta-to-Green with Reduced Lightness\') ``` By completing this question, you will demonstrate your ability to utilize seaborn for creating visually distinct and informative diverging color palettes with various custom parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def display_palette(palette, title): sns.palplot(palette) plt.title(title) plt.show() # 1. Basic diverging palette palette1 = sns.diverging_palette(240, 20) display_palette(palette1, \'Basic Blue to Red Diverging Palette\') # 2. Central color set to dark palette2 = sns.diverging_palette(240, 20, center=\\"dark\\") display_palette(palette2, \'Diverging Palette with Dark Center\') # 3. Continuous colormap palette3 = sns.diverging_palette(240, 20, as_cmap=True) sns.heatmap(np.random.rand(10, 10), cmap=palette3) plt.title(\'Continuous Diverging Colormap\') plt.show() # 4. Increased separation around center palette4 = sns.diverging_palette(240, 20, sep=30) display_palette(palette4, \'Palette with Increased Separation\') # 5. Magenta-to-green with reduced saturation palette5 = sns.diverging_palette(280, 150, s=50) display_palette(palette5, \'Magenta-to-Green with Reduced Saturation\') # 6. Magenta-to-green with reduced lightness palette6 = sns.diverging_palette(280, 150, l=35) display_palette(palette6, \'Magenta-to-Green with Reduced Lightness\')"},{"question":"Publishing and Loading a Custom Model with PyTorch Hub Objective: You are required to create a `hubconf.py` file to publish a custom pre-trained model using PyTorch Hub. Subsequently, test the process of loading this model back using PyTorch Hub methods. Problem Statement: 1. Define a custom model using PyTorch\'s `nn.Module`. 2. Create a `hubconf.py` file that: - Specifies the dependencies required for the model. - Defines an entry point function to return an instance of the model with optional loading of pre-trained weights. 3. Simulate saving and loading a pre-trained state for your model. 4. Demonstrate loading the model from the hub and validating its functionality. Steps and Requirements: 1. **Custom Model Definition**: Define a simple custom neural network, e.g., an MLP (Multi-Layer Perceptron), using PyTorch\'s `nn.Module`. ```python import torch.nn as nn class CustomModel(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(CustomModel, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x ``` 2. **Creating `hubconf.py`**: - Specify the dependency on PyTorch. - Create an entry point function that optionally loads pre-trained weights. ```python dependencies = [\'torch\'] def custom_model(pretrained=False, **kwargs): Custom MLP Model. pretrained (bool): Load pretrained weights into the model. model = CustomModel(**kwargs) if pretrained: # Mock downloading pre-trained weights pretrained_dict = { \\"fc1.weight\\": torch.randn(kwargs[\'hidden_size\'], kwargs[\'input_size\']), \\"fc1.bias\\": torch.randn(kwargs[\'hidden_size\']), \\"fc2.weight\\": torch.randn(kwargs[\'output_size\'], kwargs[\'hidden_size\']), \\"fc2.bias\\": torch.randn(kwargs[\'output_size\']) } model.load_state_dict(pretrained_dict) return model ``` 3. **Simulating Saving and Loading Pre-trained State**: - Manually create and save the state dictionary for the `CustomModel` (this step can be mocked for instructional purposes). 4. **Loading the Model**: - Ensure the model can be loaded using `torch.hub.load()` and validate its functionality. ```python import torch # Simulate loading a model from the hub model = torch.hub.load(\'your_github-repo\', \'custom_model\', pretrained=True, input_size=10, hidden_size=20, output_size=1) print(model) ``` Constraints: - Ensure that your code runs successfully without any external dependencies beyond those specified. - Provide clear and concise documentation within your code. Performance Requirements: - Your model and entry functions should be designed to load efficiently within 1-2 seconds when using small input sizes. Submission: - Complete Python code defining the custom model, `hubconf.py`, and testing the loading of the model. - Submit all code files in a .zip archive.","solution":"import torch import torch.nn as nn class CustomModel(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(CustomModel, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Simulating \'hubconf.py\' content dependencies = [\'torch\'] def custom_model(pretrained=False, **kwargs): Custom MLP Model. pretrained (bool): Load pretrained weights into the model. model = CustomModel(**kwargs) if pretrained: # Mock downloading pre-trained weights pretrained_dict = { \\"fc1.weight\\": torch.randn(kwargs[\'hidden_size\'], kwargs[\'input_size\']), \\"fc1.bias\\": torch.randn(kwargs[\'hidden_size\']), \\"fc2.weight\\": torch.randn(kwargs[\'output_size\'], kwargs[\'hidden_size\']), \\"fc2.bias\\": torch.randn(kwargs[\'output_size\']) } model.load_state_dict(pretrained_dict) return model"},{"question":"# Unix Shadow Password Database Analysis You are required to create a Python function that parses and analyzes the Unix shadow password database entries, using the deprecated `spwd` module. The main goal is to check for users with passwords that are close to expiry and return their login names. Function Implementation ```python def find_users_with_expiring_passwords(warn_days): Return the list of users whose passwords will expire within the next `warn_days` days. Parameters: warn_days (int): The warning threshold in days before the password expiry to alert users. Returns: List[str]: A list of login names with expiring passwords within `warn_days` days. Raises: PermissionError: If the user does not have the necessary privileges to access the shadow password database. ``` Input and Output - **Input**: An integer `warn_days` representing the threshold in days. - **Output**: A list of strings, each being a login name of a user whose password is set to expire within `warn_days` days. Constraints - You must use the `spwd` module to retrieve the necessary data. - Handle any potential `PermissionError` exceptions by re-raising them. Performance Requirements - The function should efficiently handle the retrieval and checking of shadow password entries, even for large databases. Example ```python result = find_users_with_expiring_passwords(7) print(result) # Output might be: [\'user1\', \'user2\'] ``` Note: You will likely need to test this in an environment where you have the necessary privileges to access the `/etc/shadow` file. Additional Information - Utilize the `spwd.getspall()` function to retrieve all shadow password entries. - Each entry is a tuple with `sp_expire` indicating the expiration date of the account. - Use the current date and the `sp_max` field (maximum number of days between changes) to determine the expiration date of each password. You should ensure your function is well-documented and handles exceptions as described.","solution":"import spwd import time from datetime import datetime, timedelta def find_users_with_expiring_passwords(warn_days): Return the list of users whose passwords will expire within the next `warn_days` days. Parameters: warn_days (int): The warning threshold in days before the password expiry to alert users. Returns: List[str]: A list of login names with expiring passwords within `warn_days` days. Raises: PermissionError: If the user does not have the necessary privileges to access the shadow password database. try: shadow_entries = spwd.getspall() except PermissionError: raise PermissionError(\\"Access to the shadow password database is required.\\") warning_users = [] current_date = datetime.now() for entry in shadow_entries: # Calculate expiration date from last change date and maximum password age if entry.sp_max == -1: # No expiration continue last_change_date = datetime.fromtimestamp(entry.sp_lstchg * 86400) expiration_date = last_change_date + timedelta(days=entry.sp_max) # Calculate remaining days till expiration days_remaining = (expiration_date - current_date).days if 0 <= days_remaining <= warn_days: warning_users.append(entry.sp_namp) return warning_users"},{"question":"**Objective**: Demonstrate your understanding of slice objects and index handling in Python. **Problem Statement**: You are required to implement a function `adjust_slice_indices(start, stop, step, length)` that adjusts the slice indices for a given sequence length. # Function Signature ```python def adjust_slice_indices(start: int, stop: int, step: int, length: int) -> Tuple[int, int, int, int]: pass ``` # Parameters - **start** (int): The start index of the slice. - **stop** (int): The stop index of the slice. - **step** (int): The step of the slice. - **length** (int): The length of the sequence to be sliced. # Returns - **Tuple**: A tuple `(start, stop, step, slicelength)` where: - **start**: The adjusted start index of the slice. - **stop**: The adjusted stop index of the slice. - **step**: The step of the slice, unchanged. - **slicelength**: The length of the resulting slice. # Constraints 1. The `step` will never be zero. 2. Indices out of bounds should be clipped as per normal Python slicing behavior. 3. If `start`, `stop`, or `step` is negative, handle them as Python does for slicing. # Example ```python adjust_slice_indices(1, 7, 2, 10) # Returns (1, 7, 2, 3) adjust_slice_indices(-5, 20, 3, 15) # Returns (10, 15, 3, 2) ``` # Implementation Requirements - You must not use any external libraries to achieve this. Use only built-in functions and standard Python constructs. - Ensure the function handles edge cases such as negative indices, indices out of bounds, and non-zero step values correctly. # Notes This problem assesses your ability to work with slice objects and sequence indices in Python. It encompasses your understanding of how slicing operates under various circumstances and how to correctly adjust indices to avoid out-of-bound errors.","solution":"def adjust_slice_indices(start, stop, step, length): Adjust the slice indices for a given sequence length. Parameters: start (int): The start index of the slice. stop (int): The stop index of the slice. step (int): The step of the slice. length (int): The length of the sequence to be sliced. Returns: Tuple[int, int, int, int]: A tuple (start, stop, step, slicelength). # Handle negative indices if start is None: start = 0 elif start < 0: start += length if start < 0: start = 0 if stop is None: stop = length elif stop < 0: stop += length if stop < 0: stop = 0 # Clip start and stop to the length of the sequence start = min(max(start, 0), length) stop = min(max(stop, 0), length) # Calculate the length of the resulting slice if step > 0 and start >= stop: slicelength = 0 elif step < 0 and start <= stop: slicelength = 0 else: slicelength = max(0, (stop - start + (step - 1 if step > 0 else step + 1)) // step) return start, stop, step, slicelength"},{"question":"**Title: Calculation and String Manipulation with Lists** **Objective:** You are required to implement a function that takes a list of strings, processes each string to perform some calculations and string manipulations, and then returns a new list based on the processed strings. **Function Signature:** ```python def process_strings(string_list: list) -> list: ``` **Input:** - `string_list` (List of str): A list of strings where each string is a mathematical expression containing integers and basic arithmetic operators (`+`, `-`, `*`, `/`), formatted as valid Python expressions. **Output:** - List of str: A list of strings displaying the original string and its calculated result, in the format \\"expression = result\\". Each string should be correctly formatted to two decimal places if a float result. **Constraints:** - The input list will always have a length between 1 and 100. - Each string in the list will represent a valid mathematical expression. - Division by zero will not occur in any of the expressions. - The operators used in the expressions are limited to `+`, `-`, `*`, `/`. **Examples:** 1. **Input:** ```python string_list = [\\"2 + 2\\", \\"5 - 3*2\\", \\"(8 / 4) + 4\\", \\"7 * 3 + 1\\"] ``` **Output:** ```python [\\"2 + 2 = 4\\", \\"5 - 3*2 = -1\\", \\"(8 / 4) + 4 = 6.00\\", \\"7 * 3 + 1 = 22\\"] ``` 2. **Input:** ```python string_list = [\\"10 / 3\\", \\"2 * 2 + 3\\", \\"(10 - 5) * 2\\"] ``` **Output:** ```python [\\"10 / 3 = 3.33\\", \\"2 * 2 + 3 = 7\\", \\"(10 - 5) * 2 = 10\\"] ``` **Description:** 1. Your task is to iterate over each string in the input list, evaluate the mathematical expression, and generate a new string in the format \\"expression = result\\". 2. If the result is an integer, format it without decimal places. 3. If the result is a float, format it to two decimal places. 4. Return the list of these formatted strings as the output. **Note:** - You may use the `eval()` function to evaluate the string expression, but ensure you understand the risks associated with it in general. For this controlled assessment, `eval()` can be safely used within the given constraints. **Example Work Through:** For the input `[\\"2 + 2\\", \\"5 - 3*2\\", \\"(8 / 4) + 4\\"]`: - The first string \\"2 + 2\\" evaluates to 4, which is an integer, so the output string would be \\"2 + 2 = 4\\". - The second string \\"5 - 3*2\\" evaluates to -1, also an integer, so the output string would be \\"5 - 3*2 = -1\\". - The third string \\"(8 / 4) + 4\\" evaluates to 6.0, which has a fractional part (although zero), so you should format it to \\"6.00\\" resulting in \\"(8 / 4) + 4 = 6.00\\". Ensure to handle integer and float results accordingly for correct output formatting.","solution":"def process_strings(string_list: list) -> list: result_list = [] for expression in string_list: result = eval(expression) if isinstance(result, float): result_str = f\\"{result:.2f}\\" else: result_str = str(result) result_list.append(f\\"{expression} = {result_str}\\") return result_list"},{"question":"**Objective:** To assess your understanding of Pythonâ€™s data model, type hierarchy, and special method names by implementing custom classes that emulate containers and numeric types. **Problem Statement:** Create a class `Matrix` that represents a 2-dimensional matrix of numbers with the following requirements: 1. **Basic Initialization and Representation:** - The class should be initialized with a list of lists, representing the matrix data. - Implement `__repr__` and `__str__` methods to provide informational and user-friendly string representations of the matrix, respectively. 2. **Element Access and Assignment:** - Implement the `__getitem__`, `__setitem__`, and `__delitem__` methods to enable element access, assignment, and deletion using standard indexing (e.g., `matrix[i][j]`). 3. **Matrix Arithmetic:** - Implement the `__add__`, `__sub__`, `__mul__`, `__eq__`, `__ne__`, and `__matmul__` methods to enable matrix addition, subtraction, element-wise multiplication, equality comparison, inequality comparison, and matrix multiplication (using the `@` operator) respectively. 4. **Scalar Operations:** - Implement the `__rmul__` and `__imul__` methods to enable scalar multiplication from either side (e.g., `2 * matrix` and `matrix *= 2`). 5. **Custom Sum Method:** - Implement a custom method `sum` that returns the sum of all elements in the matrix. # Constraints: - You may assume all input matrices are rectangular (i.e., all rows have the same length). - Matrices will have dimensions at most 100x100. - Scalar multiplication will only involve numeric types (int/float). # Example Usage: ```python mat1 = Matrix([ [1, 2, 3], [4, 5, 6] ]) mat2 = Matrix([ [7, 8, 9], [10, 11, 12] ]) # Addition mat_sum = mat1 + mat2 # Matrix([[8, 10, 12], [14, 16, 18]]) # Subtraction mat_diff = mat1 - mat2 # Matrix([[-6, -6, -6], [-6, -6, -6]]) # Element-wise Multiplication mat_elemwise_mult = mat1 * mat2 # Matrix([[7, 16, 27], [40, 55, 72]]) # Matrix Multiplication mat_mult = mat1 @ Matrix([ [1, 2], [3, 4], [5, 6] ]) # Matrix([[22, 28], [49, 64]]) # Scalar Multiplication mat_scalar_mult = 2 * mat1 # Matrix([[2, 4, 6], [8, 10, 12]]) mat1 *= 2 # Matrix([[2, 4, 6], [8, 10, 12]]) # Sum of elements total_sum = mat1.sum() # 42 ``` # Implementation Guidelines: 1. **Class Initialization:** ```python class Matrix: def __init__(self, data): # Initialize with a list of lists self.data = data ``` 2. **String Representations:** - `__repr__` should return a string that could recreate the matrix. - `__str__` should return a neatly formatted version of the matrix. 3. **Element Access and Assignment:** - Use `__getitem__`, `__setitem__`, and `__delitem__` to access, assign, and delete elements. 4. **Matrix Arithmetic:** - Implement methods to handle the arithmetic operations, ensuring matrix dimensions are compatible where required. 5. **Scalar Operations:** - `__rmul__` and `__imul__` should handle scalar values appropriately. 6. **Custom Sum Method:** - Implement the `sum` method to compute the sum of all matrix elements. Submit the complete implementation of the `Matrix` class along with examples demonstrating its usage.","solution":"class Matrix: def __init__(self, data): self.data = data def __repr__(self): return f\\"Matrix({self.data})\\" def __str__(self): return \'n\'.join([\'t\'.join(map(str, row)) for row in self.data]) def __getitem__(self, idx): return self.data[idx] def __setitem__(self, idx, value): self.data[idx] = value def __delitem__(self, idx): del self.data[idx] def __add__(self, other): return Matrix([[self.data[i][j] + other.data[i][j] for j in range(len(self.data[0]))] for i in range(len(self.data))]) def __sub__(self, other): return Matrix([[self.data[i][j] - other.data[i][j] for j in range(len(self.data[0]))] for i in range(len(self.data))]) def __mul__(self, other): if isinstance(other, Matrix): return Matrix([[self.data[i][j] * other.data[i][j] for j in range(len(self.data[0]))] for i in range(len(self.data))]) elif isinstance(other, (int, float)): return Matrix([[self.data[i][j] * other for j in range(len(self.data[0]))] for i in range(len(self.data))]) def __eq__(self, other): return self.data == other.data def __ne__(self, other): return self.data != other.data def __matmul__(self, other): result = [[sum(a * b for a, b in zip(self_row, other_col)) for other_col in zip(*other.data)] for self_row in self.data] return Matrix(result) def __rmul__(self, other): return self * other def __imul__(self, other): for i in range(len(self.data)): for j in range(len(self.data[0])): self.data[i][j] *= other return self def sum(self): return sum(sum(row) for row in self.data)"},{"question":"You are given a dataset containing numerical features that need to be preprocessed before feeding it into a machine learning model. You are required to: 1. Normalize the features such that they have a mean of 0 and a standard deviation of 1. 2. Perform a Principal Component Analysis (PCA) to reduce the dimensionality of the dataset to 2 principal components. 3. Combine these transformations into a pipeline and apply the pipeline to the given dataset. **Constraints:** - Do not use any external libraries except for scikit-learn and numpy. - The dataset will be provided as a NumPy array with shape `(n_samples, n_features)`. - Your solution should work efficiently for large datasets. Write the function `preprocess_and_reduce` with the following specifications: # Function Signature ```python def preprocess_and_reduce(data: np.ndarray) -> np.ndarray: pass ``` # Input - `data`: A NumPy array of shape `(n_samples, n_features)` containing the dataset to be processed. # Output - A NumPy array of shape `(n_samples, 2)` containing the transformed dataset with 2 principal components. # Example ```python import numpy as np data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) result = preprocess_and_reduce(data) print(result) ``` **Explanation**: - Normalize the data to have mean 0 and standard deviation 1. - Apply PCA to reduce the dataset to 2 principal components. - Return the transformed dataset. # Note Use scikit-learn\'s `StandardScaler` for normalization and `PCA` for principal component analysis. Combine these transformations into a pipeline and apply to the given dataset.","solution":"import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline def preprocess_and_reduce(data: np.ndarray) -> np.ndarray: Preprocess the data to normalize it and then apply PCA to reduce the dimensionality to 2 components. Parameters: data (np.ndarray): Input dataset of shape (n_samples, n_features) Returns: np.ndarray: Transformed dataset of shape (n_samples, 2) with 2 principal components pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2)) ]) transformed_data = pipeline.fit_transform(data) return transformed_data"},{"question":"# MPS Backend with PyTorch In this task, you will demonstrate your understanding of utilizing the MPS backend for GPU computations in PyTorch. You will need to implement functions that verify the availability of the MPS device, perform tensor operations on the MPS device, and compare the performance and correctness of operations between CPU and MPS devices. Requirements 1. **Check MPS Availability**: Implement a function `check_mps_availability` that returns `True` if the MPS backend is available, else returns `False`. 2. **Tensor Operations on MPS**: Implement a function `perform_mps_tensor_operations` that: - Accepts an integer `n`. - Creates a tensor of size `(n, n)` filled with ones on the MPS device. - Performs element-wise multiplication of the tensor with 2, and returns the resulting tensor. 3. **Comparison between CPU and MPS**: Implement a function `compare_cpu_mps` that: - Accepts an integer `n`. - Creates a tensor of size `(n, n)` filled with ones on the CPU and performs element-wise multiplication with 2. - Creates a tensor of size `(n, n)` filled with ones on the MPS device and performs the same operation. - Compares if both resulting tensors are equal and returns `True` if they are identical, else returns `False`. Your Implementation ```python import torch def check_mps_availability(): Returns: True if MPS is available, else False if not torch.backends.mps.is_available(): return False return True def perform_mps_tensor_operations(n): Args: n (int): The size of the tensor to create Returns: torch.Tensor: The resultant tensor on MPS device after multiplication if not check_mps_availability(): raise RuntimeError(\\"MPS device is not available.\\") mps_device = torch.device(\'mps\') x = torch.ones((n, n), device=mps_device) y = x * 2 return y def compare_cpu_mps(n): Args: n (int): The size of the tensor to create Returns: bool: True if tensors computed on CPU and MPS are identical, else False cpu_tensor = torch.ones((n, n)) * 2 if not check_mps_availability(): raise RuntimeError(\\"MPS device is not available.\\") mps_device = torch.device(\'mps\') mps_tensor = torch.ones((n, n), device=mps_device) * 2 return torch.allclose(cpu_tensor, mps_tensor.cpu()) # Example usage: # print(check_mps_availability()) # Should print True or False # print(perform_mps_tensor_operations(5)) # Should print a 5x5 tensor filled with 2s on MPS # print(compare_cpu_mps(5)) # Should print True ``` Constraints - Ensure that your code checks for MPS availability before attempting any operations on the MPS device. - Raise appropriate errors if MPS is not available and an attempt is made to perform operations on it. - Use tensor operations effectively to accomplish the tasks. Expected Output An output example has been provided in the comment section at the end of the code. Use appropriate size `n` to test and validate your functions.","solution":"import torch def check_mps_availability(): Returns: True if MPS is available, else False if not torch.backends.mps.is_available(): return False return True def perform_mps_tensor_operations(n): Args: n (int): The size of the tensor to create Returns: torch.Tensor: The resultant tensor on MPS device after multiplication if not check_mps_availability(): raise RuntimeError(\\"MPS device is not available.\\") mps_device = torch.device(\'mps\') x = torch.ones((n, n), device=mps_device) y = x * 2 return y def compare_cpu_mps(n): Args: n (int): The size of the tensor to create Returns: bool: True if tensors computed on CPU and MPS are identical, else False cpu_tensor = torch.ones((n, n)) * 2 if not check_mps_availability(): raise RuntimeError(\\"MPS device is not available.\\") mps_device = torch.device(\'mps\') mps_tensor = torch.ones((n, n), device=mps_device) * 2 return torch.allclose(cpu_tensor, mps_tensor.cpu()) # Example usage: # print(check_mps_availability()) # Should print True or False # print(perform_mps_tensor_operations(5)) # Should print a 5x5 tensor filled with 2s on MPS # print(compare_cpu_mps(5)) # Should print True"},{"question":"Objective Implement a function that calculates the exponentially weighted moving average (EWMA) of a given time series. The EWMA should give more weight to recent observations and less weight to older ones, following an exponential decay pattern. Function Signature ```python def exponentially_weighted_moving_average(data: pd.Series, span: int) -> pd.Series: Calculate the exponentially weighted moving average (EWMA) of a given time series. Args: - data (pd.Series): The input time series data. - span (int): The span for the EWMA, which defines the decay factor. Returns: - pd.Series: The time series of the exponentially weighted moving averages. ``` Input - `data`: A pandas Series object representing the time series data. - `span`: An integer representing the span of the EWMA. Output - A pandas Series object of the same length as `data`, containing the exponentially weighted moving averages. Constraints - The function should correctly handle NaN values in the input data, ensuring that the result series also reflects NaN values appropriately. - The input Series `data` could be of any numeric dtype and may contain missing values. Example ```python import pandas as pd import numpy as np data = pd.Series([1, 2, 3, 4, 5, np.nan, 7, 8, 9, 10]) span = 3 # Expected output is an exponentially weighted moving average with the given span result = exponentially_weighted_moving_average(data, span) print(result) ``` Expected partial result (actual results will depend on the EWMA formula): ``` 0 1.000000 1 1.666667 2 2.444444 3 3.296296 4 4.197531 5 NaN 6 6.065395 7 7.043597 8 8.029065 9 9.019377 dtype: float64 ``` Note Ensure your implementation uses the appropriate pandas function(s) for calculating the exponentially weighted moving average. Your function should be efficient and make use of vectorized operations where possible.","solution":"import pandas as pd def exponentially_weighted_moving_average(data: pd.Series, span: int) -> pd.Series: Calculate the exponentially weighted moving average (EWMA) of a given time series. Args: - data (pd.Series): The input time series data. - span (int): The span for the EWMA, which defines the decay factor. Returns: - pd.Series: The time series of the exponentially weighted moving averages. return data.ewm(span=span, adjust=False).mean()"},{"question":"Objective This exercise will assess your comprehension of PyTorch\'s tensor creation and manipulation capabilities, specifically focusing on generating random numbers and setting random seeds. Problem Statement Implement a function `generate_random_tensor` which performs the following tasks: 1. Sets the random seed for PyTorch random number generation. 2. Generates a tensor of the specified shape filled with uniformly-distributed random values in the given range. 3. Normalizes the generated tensor so that its values fall between 0 and 1. Function Signature ```python import torch def generate_random_tensor(seed: int, shape: tuple, low: float, high: float) -> torch.Tensor: pass ``` Input - `seed` (int): An integer to set the seed for generating random numbers. - `shape` (tuple): The shape of the tensor to be generated. For example, (3, 4) generates a tensor with 3 rows and 4 columns. - `low` (float): The lower bound of the range for the uniform distribution. - `high` (float): The upper bound of the range for the uniform distribution. Output - Returns a `torch.Tensor` of the specified shape, with values uniformly distributed between the given range and normalized between 0 and 1. Example ```python seed = 42 shape = (2, 3) low = -10.0 high = 5.0 result = generate_random_tensor(seed, shape, low, high) print(result) ``` - The output will be a tensor of shape (2, 3), with random values generated between -10.0 and 5.0, and then normalized to be between 0 and 1. Constraints - The seed, shape, low, and high parameters will always be valid and well-formed. - Your solution should be optimized efficiently in terms of performance. Notes - Make sure to use PyTorch\'s `torch.manual_seed` to set the random seed. - Use `torch.rand` for generating uniformly-distributed random values. - The normalization process can be performed by first transforming the generated range to [0, 1] by subtracting the minimum value and dividing by the range length (high - low). ```python import torch def generate_random_tensor(seed: int, shape: tuple, low: float, high: float) -> torch.Tensor: # Set the random seed torch.manual_seed(seed) # Generate the random tensor with uniform distribution in range [low, high) random_tensor = (high - low) * torch.rand(shape) + low # Normalize the tensor to fall between 0 and 1 normalized_tensor = (random_tensor - low) / (high - low) return normalized_tensor # Example usage seed = 42 shape = (2, 3) low = -10.0 high = 5.0 result = generate_random_tensor(seed, shape, low, high) print(result) ```","solution":"import torch def generate_random_tensor(seed: int, shape: tuple, low: float, high: float) -> torch.Tensor: Sets the random seed, generates a tensor with uniformly-distributed random values in a given range, and normalizes the values to be between 0 and 1. Params: seed (int): An integer to set the random seed. shape (tuple): Shape of the tensor to be generated. low (float): Lower bound of the range for uniform distribution. high (float): Upper bound of the range for uniform distribution. Returns: torch.Tensor: A tensor of specified shape with values between 0 and 1. # Set the random seed torch.manual_seed(seed) # Generate the random tensor with uniform distribution in range [low, high) random_tensor = (high - low) * torch.rand(shape) + low # Normalize the tensor to fall between 0 and 1 normalized_tensor = (random_tensor - low) / (high - low) return normalized_tensor"},{"question":"**Advanced Python Byte Operations Using C-API** # Objective: You will create a C extension module for Python that exposes functions to work with bytes objects in Python, demonstrating your understanding of the Python C-API for bytes objects. # Task: Implement a Python C extension module named `byteops` that provides the following functions: 1. **from_string**: - **Description**: Create a new bytes object from a given string. - **Input**: A single string. - **Output**: A corresponding bytes object. 2. **concat_and_resize**: - **Description**: Concatenate two bytes objects and resize the resulting bytes object to a specified length. - **Input**: - A bytes object `b1`. - A bytes object `b2`. - An integer `newsize`. - **Output**: A new bytes object formed by concatenating `b1` and `b2`, then resizing to `newsize`. 3. **check_bytes**: - **Description**: Check if the given object is exactly a bytes object. - **Input**: An object. - **Output**: A boolean indicating whether the object is a bytes object. # Constraints: - Your solution must handle errors gracefully and raise appropriate Python exceptions (TypeError, MemoryError, etc.) on invalid operations. - Pay special attention to memory management and ensure there are no memory leaks. # Steps: 1. Create a file named `byteops.c` for implementing the C extension module. 2. Implement the functions using the provided C-API functions, adhering to the specifications. 3. Create a setup script to build the module using `setuptools`. # Example Usage: Python script demonstrating the usage of the `byteops` module: ```python import byteops # Create a new bytes object from a string b = byteops.from_string(\\"Hello, World!\\") print(b) # Output: b\'Hello, World!\' # Concatenate and resize bytes objects b1 = b\\"Python\\" b2 = b\\"Rocks\\" resized_bytes = byteops.concat_and_resize(b1, b2, 10) print(resized_bytes) # Output: b\'PythonRox00x00x00\' (if newsize < concatenation size) # Check if an object is a bytes object print(byteops.check_bytes(b\\"Test\\")) # Output: True print(byteops.check_bytes(\\"Test\\")) # Output: False ``` # Submission: Include the `byteops.c` file and a `setup.py` script to compile the module. Provide a README file with instructions on how to build and install the module.","solution":"# Implementing the Python functions that will be provided by the C extension def from_string(s): Create a new bytes object from a given string. if not isinstance(s, str): raise TypeError(\\"Input must be a string\\") return s.encode(\'utf-8\') def concat_and_resize(b1, b2, newsize): Concatenate two bytes objects and resize the resulting bytes object to a specified length. if not isinstance(b1, bytes) or not isinstance(b2, bytes): raise TypeError(\\"Both inputs must be bytes objects\\") if not isinstance(newsize, int) or newsize < 0: raise ValueError(\\"New size must be a non-negative integer\\") concatenated = b1 + b2 resized = concatenated[:newsize].ljust(newsize, b\'x00\') return resized def check_bytes(obj): Check if the given object is exactly a bytes object. return isinstance(obj, bytes)"},{"question":"Objective: Write a Python script that uses the `tty` module to manipulate terminal input modes. Your script should demonstrate the ability to switch between different terminal modes and handle user input without traditional buffering. Problem Statement: Write two functions `enable_raw_mode(fd)` and `enable_cbreak_mode(fd)` that put the terminal file descriptor `fd` into raw and cbreak modes, respectively. Then, write a script that: 1. Reads input from the user character-by-character in raw mode. 2. After receiving a specific character (e.g., `q`), switches to cbreak mode. 3. Demonstrates the difference in behavior between the two modes until the user exits by pressing `x`. Function Details: - `enable_raw_mode(fd)` - **Input**: `fd` (a file descriptor) - **Output**: No return value - **Operation**: Change the terminal mode to raw. - `enable_cbreak_mode(fd)` - **Input**: `fd` (a file descriptor) - **Output**: No return value - **Operation**: Change the terminal mode to cbreak. Constraints: - Only works on Unix systems. - Input should be read directly from the terminal file descriptor. Example Use Case: ```python import sys import tty import termios def enable_raw_mode(fd): tty.setraw(fd) def enable_cbreak_mode(fd): tty.setcbreak(fd) def main(): fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) try: print(\\"Entering raw mode. Press \'q\' to switch to cbreak mode. Press \'x\' to exit.\\") enable_raw_mode(fd) while True: char = sys.stdin.read(1) if char == \'q\': print(\\"nSwitching to cbreak mode. Press \'x\' to exit.\\") enable_cbreak_mode(fd) elif char == \'x\': print(\\"nExiting.\\") break else: print(f\\"Read character: {char}\\") finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) if __name__ == \\"__main__\\": main() ``` Note: - Ensure you handle the switching back to the original terminal settings upon script termination to avoid leaving the terminal in an inconsistent state. - Do thorough testing on a Unix system terminal to validate the behavior of the terminal modes.","solution":"import sys import tty import termios def enable_raw_mode(fd): Enables raw mode on the given file descriptor. tty.setraw(fd) def enable_cbreak_mode(fd): Enables cbreak mode on the given file descriptor. tty.setcbreak(fd) def main(): fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) try: print(\\"Entering raw mode. Press \'q\' to switch to cbreak mode. Press \'x\' to exit.\\") enable_raw_mode(fd) while True: char = sys.stdin.read(1) if char == \'q\': print(\\"nSwitching to cbreak mode. Press \'x\' to exit.\\") enable_cbreak_mode(fd) elif char == \'x\': print(\\"nExiting.\\") break else: print(f\\"Read character: {char}\\") finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** Implement a function that compresses a given text data using the zlib module\'s `compress` function and decompresses it back to its original form using the `decompress` function. Your task is to ensure the integrity of the data by comparing checksums before and after compression and decompression. **Function Specifications:** 1. `compress_and_verify(data: str) -> Tuple[str, bool]` - **Input:** - `data`: A string containing the text to be compressed and decompressed. - **Output:** - A tuple containing: - The decompressed string after the round-trip of compression and decompression. - A boolean indicating if the original and decompressed data checksums match. **Constraints:** - The input string can contain any printable ASCII characters. - The length of the input string will be between `1` and `1000` characters. **Details:** 1. Use `zlib.compress` to compress the string data. 2. Use `zlib.decompress` to decompress the compressed data. 3. Use `zlib.adler32` to compute the checksum of the original and decompressed data. 4. Ensure the output boolean indicates whether the checksums of the original and decompressed data are equal. **Example:** ```python def compress_and_verify(data: str) -> Tuple[str, bool]: # Your implementation here # Example original_data = \\"The quick brown fox jumps over the lazy dog\\" result = compress_and_verify(original_data) print(result) # Output should be (`original_data`, True) ``` **Notes:** - The function should handle compression and decompression efficiently. - Focus on managing byte data correctly when compressing and decompressing.","solution":"import zlib from typing import Tuple def compress_and_verify(data: str) -> Tuple[str, bool]: Compresses the given data, then decompresses it and verifies if the checksums match. Args: data (str): The input string data to be compressed and decompressed. Returns: Tuple containing the decompressed string and a boolean indicating whether the original and decompressed data checksums match. # Convert string data to bytes original_bytes = data.encode(\'utf-8\') # Calculate checksum of the original data original_checksum = zlib.adler32(original_bytes) # Compress the data compressed_data = zlib.compress(original_bytes) # Decompress the data decompressed_data = zlib.decompress(compressed_data) # Convert decompressed bytes back to string decompressed_string = decompressed_data.decode(\'utf-8\') # Calculate checksum of the decompressed data decompressed_checksum = zlib.adler32(decompressed_data) # Verify if the checksums match checksums_match = original_checksum == decompressed_checksum return decompressed_string, checksums_match"},{"question":"# **Coding Assessment Question** **Objective:** Demonstrate your understanding of the `ossaudiodev` module by writing a function to configure an audio device and play a generated audio signal. **Question:** You are tasked with writing a Python function `play_sine_wave(frequency: int, duration: int, sample_rate: int) -> None` that generates and plays a sine wave using the `ossaudiodev` module. **Function Specifications:** ```python def play_sine_wave(frequency: int, duration: int, sample_rate: int) -> None: pass ``` **Input Parameters:** - `frequency`: An integer representing the frequency of the sine wave in Hertz (Hz). Valid range is between 20 and 20000 Hz. - `duration`: An integer representing the duration for which the sine wave should be played in milliseconds. - `sample_rate`: An integer representing the sample rate in samples per second (Hz). Common values are 8000, 11025, 22050, 44100, and 96000 Hz. **Constraints:** - If the frequency is outside the range of 20 to 20000 Hz, raise a `ValueError` with the message \\"Frequency must be between 20 and 20000 Hz\\". - If the sample rate is not one of the common values specified, raise a `ValueError` with the message \\"Invalid sample rate\\". **Output:** - The function will not return any value but will play the generated sine wave through the default audio device. **Details:** 1. **Generating the Sine Wave:** - Use basic trigonometric functions from the `math` library or third-party libraries like `numpy` to generate the sine wave samples. 2. **Configuring the Audio Device:** - Open the default audio device using `ossaudiodev.open(\'w\')`. - Set the audio format to 16-bit signed little-endian (`ossaudiodev.AFMT_S16_LE`). - Set the number of channels to 1 (mono). - Set the sample rate to the provided `sample_rate`. 3. **Playing the Audio:** - Write the generated audio data to the audio device using the `writeall` method. - Ensure the device is properly closed after the audio has been played by using a context manager (`with` statement). 4. **Sine Wave Generation Formula:** - The amplitude of the sine wave should be normalized to the maximum value of a 16-bit signed integer (32767). - The formula for the `n`-th sample at a frequency `f`, sample rate `r` is: [ text{Sample}(n) = 32767 times sinleft(2 times pi times frac{f times n}{r}right) ] **Example Usage:** ```python # play a 440 Hz sine wave for 1000 ms at a sample rate of 44100 Hz play_sine_wave(440, 1000, 44100) ``` **Notes:** - Handle all possible exceptions related to opening and writing to the audio device. - Remember to import any necessary libraries/modules at the start of your function.","solution":"import ossaudiodev import numpy as np import math def play_sine_wave(frequency: int, duration: int, sample_rate: int) -> None: Generate and play a sine wave of given frequency, duration, and sample rate. Parameters: - frequency: Integer, frequency of the sine wave in Hz (20-20000 Hz). - duration: Integer, duration of the sine wave in milliseconds. - sample_rate: Integer, sample rate in Hz (e.g., 8000, 11025, 22050, 44100, 96000). # Validate the frequency if frequency < 20 or frequency > 20000: raise ValueError(\\"Frequency must be between 20 and 20000 Hz\\") # Validate the sample rate common_sample_rates = [8000, 11025, 22050, 44100, 96000] if sample_rate not in common_sample_rates: raise ValueError(\\"Invalid sample rate\\") # Number of samples num_samples = int(sample_rate * (duration / 1000.0)) # Generate the sine wave x = np.linspace(0, duration / 1000.0, num_samples, endpoint=False) sine_wave = (32767 * np.sin(2 * np.pi * frequency * x)).astype(np.int16) # Play the sine wave using the ossaudiodev module try: with ossaudiodev.open(\'w\') as audio: audio.setfmt(ossaudiodev.AFMT_S16_LE) audio.channels(1) audio.speed(sample_rate) audio.writeall(sine_wave.tobytes()) except Exception as e: raise RuntimeError(f\\"An error occurred while accessing the audio device: {e}\\")"},{"question":"# Boolean Logic Circuit Simulator You are tasked with designing a Boolean logic circuit simulator using Python. The goal is to simulate simple Boolean circuits consisting of various gates: AND, OR, NOT, NAND, and NOR. Each gate will operate on binary inputs (0 or 1) and produce a binary output (0 or 1). Function Definition Write a function `simulate_circuit(circuit_definition, inputs)` that simulates the Boolean circuit. - `circuit_definition`: A list of tuples, where each tuple describes a gate and its inputs in the format `(gate_type, input1, input2)`: - `gate_type` (str): one of `\\"AND\\"`, `\\"OR\\"`, `\\"NOT\\"`, `\\"NAND\\"`, `\\"NOR\\"` - `input1` and `input2` (int or None): indices of the input values or outputs from other gates. For `NOT` gates, `input2` will be `None`. - `inputs`: A list of binary values (0 or 1), representing the initial inputs to the circuit. Output - The function should return a list of binary values, representing the outputs of each gate in the order they are defined. Example ```python def simulate_circuit(circuit_definition, inputs): # Your implementation here # Example circuit definition and inputs: circuit_definition = [ (\'AND\', 0, 1), # Gate 0: AND gate with inputs from inputs[0] and inputs[1] (\'OR\', 1, 2), # Gate 1: OR gate with inputs from inputs[1] and inputs[2] (\'NOT\', 0, None) # Gate 2: NOT gate with input from inputs[0] ] inputs = [1, 0, 1] # Expected output: [0, 1, 0] print(simulate_circuit(circuit_definition, inputs)) ``` Constraints 1. The circuit can have up to 20 gates. 2. The inputs list will have up to 10 elements. 3. Only valid gate types will be provided. 4. Indices in the circuit_definition will always refer to valid positions in the inputs or previous gates. Notes - Use standard Boolean operations (`and`, `or`, `not` in Python) to implement the gates. - You may assume that the circuit definitions and inputs will always be valid, per the constraints given. Hint To handle the dependencies between gates, you may find it helpful to maintain a list of gate outputs and update it as you process each gate in `circuit_definition`.","solution":"def simulate_circuit(circuit_definition, inputs): # List to store the outputs of the gates as they are computed gate_outputs = [] # Iterate through each gate in the circuit definition for gate_type, input1, input2 in circuit_definition: # Determine the actual input values based on the input indices if isinstance(input1, int): value1 = inputs[input1] if input1 < len(inputs) else gate_outputs[input1 - len(inputs)] if input2 is not None and isinstance(input2, int): value2 = inputs[input2] if input2 < len(inputs) else gate_outputs[input2 - len(inputs)] # Compute the output based on gate type if gate_type == \\"AND\\": output = value1 and value2 elif gate_type == \\"OR\\": output = value1 or value2 elif gate_type == \\"NOT\\": output = not value1 elif gate_type == \\"NAND\\": output = not (value1 and value2) elif gate_type == \\"NOR\\": output = not (value1 or value2) # Convert boolean output to integer (0 or 1) and store gate_outputs.append(int(output)) # Return the list of gate outputs return gate_outputs"},{"question":"# SQLite3 Coding Assessment Question Objective Implement a Python function that creates an SQLite database to store information about a library of books. The function should perform the following tasks: 1. Create a new SQLite database called `library.db`. 2. Create a table named `books` with the following columns: - `id`: Integer, primary key, autoincrement. - `title`: Text, not null. - `author`: Text, not null. - `published_date`: Text, not null. - `isbn`: Text, unique. 3. Insert a list of books into the `books` table. 4. Implement a query function to fetch all books published by a specific author. 5. Implement a function to update the ISBN of a book given its title. 6. Implement a custom converter to automatically convert date strings from the database into Python `datetime.date` objects. Function Signature ```python import sqlite3 def setup_library_database(): pass def insert_books(books): pass def get_books_by_author(author_name): pass def update_book_isbn(title, new_isbn): pass ``` Detailed Requirements 1. **`setup_library_database()`** - This function should create a new SQLite database file named `library.db` in the current working directory. - Create a table named `books` with columns for `id`, `title`, `author`, `published_date`, and `isbn`. 2. **`insert_books(books)`** - This function takes a list of books, where each book is represented by a dictionary with keys `title`, `author`, `published_date`, and `isbn`. - Insert each book into the `books` table in the `library.db` database. - Ensure that the function uses placeholders to prevent SQL injection. 3. **`get_books_by_author(author_name)`** - This function takes a single parameter, `author_name` (a string), and returns a list of books written by that author. - Each result should be a dictionary with keys `id`, `title`, `author`, `published_date`, and `isbn`. 4. **`update_book_isbn(title, new_isbn)`** - This function takes two parameters: `title` (the title of the book) and `new_isbn` (the new ISBN number) and updates the ISBN for the book with the given title in the `library.db` database. - Ensure that the function uses placeholders to prevent SQL injection. 5. **Custom Date Converter** - Within the context of this database, the `published_date` column will be treated as a string when inserted but should be automatically converted to a `datetime.date` object when read. - Define and register a custom converter to handle this conversion. Example Usage ```python # Setup and populate the database setup_library_database() books = [ {\\"title\\": \\"Book 1\\", \\"author\\": \\"Author A\\", \\"published_date\\": \\"2023-01-01\\", \\"isbn\\": \\"1234567890\\"}, {\\"title\\": \\"Book 2\\", \\"author\\": \\"Author B\\", \\"published_date\\": \\"2021-05-06\\", \\"isbn\\": \\"0987654321\\"}, {\\"title\\": \\"Book 3\\", \\"author\\": \\"Author A\\", \\"published_date\\": \\"2019-07-15\\", \\"isbn\\": \\"1122334455\\"} ] insert_books(books) # Query books by author print(get_books_by_author(\\"Author A\\")) # Output: [{\'id\': 1, \'title\': \'Book 1\', \'author\': \'Author A\', \'published_date\': datetime.date(2023, 1, 1), \'isbn\': \'1234567890\'}, {\'id\': 3, \'title\': \'Book 3\', \'author\': \'Author A\', \'published_date\': datetime.date(2019, 7, 15), \'isbn\': \'1122334455\'}] # Update ISBN of a book by title update_book_isbn(\\"Book 1\\", \\"9999999999\\") print(get_books_by_author(\\"Author A\\")) # Output: [{\'id\': 1, \'title\': \'Book 1\', \'author\': \'Author A\', \'published_date\': datetime.date(2023, 1, 1), \'isbn\': \'9999999999\'}, {\'id\': 3, \'title\': \'Book 3\', \'author\': \'Author A\', \'published_date\': datetime.date(2019, 7, 15), \'isbn\': \'1122334455\'}] ``` Constraints - Assume that the `title` of a book is unique. - Handle potential exceptions and errors gracefully, ensuring that the database is not left in an inconsistent state.","solution":"import sqlite3 import datetime def setup_library_database(): conn = sqlite3.connect(\\"library.db\\") c = conn.cursor() c.execute( CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY AUTOINCREMENT, title TEXT NOT NULL, author TEXT NOT NULL, published_date TEXT NOT NULL, isbn TEXT UNIQUE ) ) conn.commit() conn.close() def insert_books(books): conn = sqlite3.connect(\\"library.db\\") c = conn.cursor() for book in books: c.execute( INSERT OR IGNORE INTO books (title, author, published_date, isbn) VALUES (?, ?, ?, ?) , (book[\'title\'], book[\'author\'], book[\'published_date\'], book[\'isbn\'])) conn.commit() conn.close() def get_books_by_author(author_name): conn = sqlite3.connect(\\"library.db\\", detect_types=sqlite3.PARSE_DECLTYPES) conn.row_factory = sqlite3.Row c = conn.cursor() c.execute(\\"SELECT * FROM books WHERE author=?\\", (author_name,)) rows = c.fetchall() conn.close() return [dict(row) for row in rows] def update_book_isbn(title, new_isbn): conn = sqlite3.connect(\\"library.db\\") c = conn.cursor() c.execute(\\"UPDATE books SET isbn=? WHERE title=?\\", (new_isbn, title)) conn.commit() conn.close() def adapt_date(date_obj): return date_obj.isoformat() def convert_date(date_str): return datetime.date.fromisoformat(date_str) sqlite3.register_adapter(datetime.date, adapt_date) sqlite3.register_converter(\\"date\\", convert_date)"},{"question":"**Problem Statement:** In this coding assessment, you are required to demonstrate your understanding of tuning decision thresholds in a binary classification problem using scikit-learn. Specifically, you need to implement a function that uses the `TunedThresholdClassifierCV` to find the optimal decision threshold for a provided classifier and dataset, based on a specified scoring metric. **Function Specifications:** ```python def tune_decision_threshold(X: np.ndarray, y: np.ndarray, base_classifier, scoring_metric: str) -> float: This function tunes the decision threshold of the given base classifier using the specified scoring metric. Parameters: - X: np.ndarray : Feature matrix (2D array) of shape (n_samples, n_features). - y: np.ndarray : Target vector (1D array) of shape (n_samples,). - base_classifier : Classifier object : The base classifier to be used (e.g., LogisticRegression, DecisionTreeClassifier). - scoring_metric: str : The scoring metric used to tune the decision threshold (e.g., \'f1\', \'recall\', \'precision\'). Returns: - float : The optimal decision threshold based on the specified scoring metric. pass ``` **Input Format:** - `X` will be a 2D NumPy array with shape `(n_samples, n_features)`, representing the feature matrix. - `y` will be a 1D NumPy array with shape `(n_samples,)`, representing the binary target variable. - `base_classifier` will be an instance of a scikit-learn classifier (e.g., `LogisticRegression()`, `DecisionTreeClassifier()`). - `scoring_metric` will be a string representing the scoring metric to optimize (e.g., \'f1\', \'recall\', \'precision\'). **Output Format:** - The function should return a float representing the optimal decision threshold. **Constraints:** - You should use `TunedThresholdClassifierCV` from scikit-learn to tune the decision threshold. - The default cross-validation strategy for `TunedThresholdClassifierCV` should be used. - Assume that the input data is well-prepared and does not require any additional preprocessing. **Example:** ```python from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification # Generate a toy dataset X, y = make_classification(n_samples=1000, weights=[0.1, 0.9], random_state=0) # Define the base classifier base_classifier = LogisticRegression() # Tune decision threshold for F1 score optimal_threshold = tune_decision_threshold(X, y, base_classifier, \'f1\') print(f\\"The optimal decision threshold for the F1 score is: {optimal_threshold:.4f}\\") ``` **Note:** To successfully implement this function, you may refer to scikit-learn\'s documentation on `TunedThresholdClassifierCV` and `make_scorer`.","solution":"import numpy as np from sklearn.model_selection import cross_val_predict from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score from sklearn.base import clone def tune_decision_threshold(X: np.ndarray, y: np.ndarray, base_classifier, scoring_metric: str) -> float: This function tunes the decision threshold of the given base classifier using the specified scoring metric. Parameters: - X: np.ndarray : Feature matrix (2D array) of shape (n_samples, n_features). - y: np.ndarray : Target vector (1D array) of shape (n_samples,). - base_classifier : Classifier object : The base classifier to be used (e.g., LogisticRegression, DecisionTreeClassifier). - scoring_metric: str : The scoring metric used to tune the decision threshold (e.g., \'f1\', \'recall\', \'precision\'). Returns: - float : The optimal decision threshold based on the specified scoring metric. base_classifier = clone(base_classifier) scoring_func_dict = { \'f1\': f1_score, \'precision\': precision_score, \'recall\': recall_score } if scoring_metric not in scoring_func_dict: raise ValueError(\\"Invalid scoring metric. Choose \'f1\', \'precision\', or \'recall\'.\\") y_pred_proba = cross_val_predict(base_classifier, X, y, cv=5, method=\'predict_proba\')[:, 1] thresholds = np.linspace(0, 1, 101) scores = [] for threshold in thresholds: y_pred = (y_pred_proba >= threshold).astype(int) score = scoring_func_dict[scoring_metric](y, y_pred) scores.append(score) optimal_threshold = thresholds[np.argmax(scores)] return optimal_threshold"},{"question":"<|Analysis Begin|> The provided documentation outlines the buffer protocol in Python, including how objects like `bytes` and `bytearray` can expose their underlying memory buffers for direct access. It explains the structure of the `Py_buffer` and the various fields (`buf`, `obj`, `len`, `readonly`, `itemsize`, `format`, etc.) that define the memory layout and content. Additionally, it covers how the buffer can be accessed and manipulated, and the different types of memory contiguity (`C-contiguous`, `Fortran-contiguous`, etc.) that can be requested. Key functions for working with buffers like `PyObject_GetBuffer`, `PyBuffer_Release`, and `PyBuffer_FillInfo` are described. Given this detailed and extensive documentation on buffer structures and operations, we can design a challenging coding question that involves implementing functionality around the concepts of Python\'s buffer protocol. <|Analysis End|> <|Question Begin|> **Buffer Protocol Coding Challenge** **Problem Statement:** You are tasked with creating a Python class that simulates a numeric 2D array and allows for direct memory access using Python\'s buffer protocol. This class, `Numeric2DArray`, should support writing and reading to/from its buffer in both C-style (row-major) and Fortran-style (column-major) order, as well as providing necessary buffer management. **Class Requirements:** 1. **Initialization:** - The constructor should initialize the 2D array with given dimensions (rows and columns) and a fill value. 2. **Support Buffer Protocol:** - Implement methods to expose the buffer protocol interface, allowing other Python functions to access the underlying memory. 3. **Contiguity Requests:** - Support both C-contiguous and Fortran-contiguous memory layouts when requested. 4. **Read/Write Data:** - Implement methods to read and write data to the buffer ensuring proper memory management. 5. **Buffer Release:** - Properly release memory and handles when the buffer is no longer needed to prevent memory leaks. **Function Signatures:** ```python class Numeric2DArray: def __init__(self, rows: int, columns: int, fill_value: int = 0): Initialize the 2D array with specified dimensions and fill value. pass def __enter__(self): Context manager entry method to provide buffer access. pass def __exit__(self, exc_type, exc_val, exc_tb): Context manager exit method to ensure buffer release. pass def expose_buffer(self, flags: int): Method to expose buffer with specified flags for contiguity. pass def read_buffer(self) -> bytes: Read the contents of the buffer and return as bytes. pass def write_buffer(self, data: bytes): Write the given bytes data to the buffer. pass def release_buffer(self): Release the buffer memory properly. pass ``` **Implementation Constraints:** - Assume the array elements are integers (4 bytes each). - The buffer should support `PyBUF_WRITABLE` for writable access. - The buffer should properly handle various contiguity flags like `PyBUF_C_CONTIGUOUS` and `PyBUF_F_CONTIGUOUS`. - Proper exception handling and resource management are essential. - Performance should be considered, so avoid unnecessary memory copying. **Example Usage:** ```python # Create a 2D numeric array of 3 rows and 4 columns filled with 1 array = Numeric2DArray(3, 4, fill_value=1) # Use context manager to handle buffers with array: array.expose_buffer(flags=PyBUF_C_CONTIGUOUS) data = array.read_buffer() print(data) # Should print the byte representation of the buffer new_data = b\'x02\' * 12 # Example new data to write array.write_buffer(new_data) modified_data = array.read_buffer() print(modified_data) # Should reflect the changes in the buffer # Ensure the buffer is released properly array.release_buffer() ``` Implement the `Numeric2DArray` class according to the specifications and ensure it passes the provided example usage.","solution":"import ctypes import numpy as np class Numeric2DArray: def __init__(self, rows: int, columns: int, fill_value: int = 0): Initialize the 2D array with specified dimensions and fill value. self.rows = rows self.columns = columns self.fill_value = fill_value self.array = np.full((rows, columns), fill_value, dtype=np.int32) self.buffer = None self.buf_pointer = None def __enter__(self): Context manager entry method to provide buffer access. return self def __exit__(self, exc_type, exc_val, exc_tb): Context manager exit method to ensure buffer release. self.release_buffer() def expose_buffer(self, flags: int): Method to expose buffer with specified flags for contiguity. self.release_buffer() # Ensure no previous buffer is in use if flags & PyBUF_F_CONTIGUOUS: self.array = np.asfortranarray(self.array) elif flags & PyBUF_C_CONTIGUOUS: self.array = np.ascontiguousarray(self.array) self.buf_pointer = self.array.ctypes.data_as(ctypes.POINTER(ctypes.c_byte)) self.buffer = memoryview(self.array) def read_buffer(self) -> bytes: Read the contents of the buffer and return as bytes. return self.buffer.tobytes() def write_buffer(self, data: bytes): Write the given bytes data to the buffer. np.copyto(self.array, np.frombuffer(data, dtype=np.int32).reshape(self.rows, self.columns)) def release_buffer(self): Release the buffer memory properly. if self.buffer is not None: self.buffer.release() self.buffer = None self.buf_pointer = None # Define PyBUF_F_CONTIGUOUS and PyBUF_C_CONTIGUOUS flags for the buffer protocol. PyBUF_F_CONTIGUOUS = 0x0010 PyBUF_C_CONTIGUOUS = 0x0020 PyBUF_WRITABLE = 0x0001"},{"question":"**Gaussian Process Modelling with Custom Kernels** **Objective:** The goal of this assessment is to evaluate your understanding of Gaussian Processes, including kernel configuration, hyperparameter optimization, and practical application in regression tasks. **Problem Statement:** You are provided with a dataset containing noisy observations of a function. Your task is to: 1. Implement a Gaussian Process Regressor using the provided data. 2. Combine different kernels to improve the model\'s performance. 3. Optimize the hyperparameters to achieve the best model. **Requirements:** 1. **Data Loading:** - The dataset `data.csv` contains two columns: `X` (input features) and `y` (target values). - Extract the values into appropriate NumPy arrays. 2. **Model Construction:** - Implement a Gaussian Process Regressor using `scikit-learn`. - Combine at least three different kernels from the provided list (`RBF`, `MatÃ©rn`, `RationalQuadratic`, `ExpSineSquared`, `DotProduct`) using operators like sum and product. - Use `alpha` to specify the noise level globally. 3. **Hyperparameter Optimization:** - Optimize kernel hyperparameters by maximizing the log-marginal-likelihood (LML). - Perform multiple restarts of the optimizer (at least 10 restarts). 4. **Prediction:** - Fit the model to the data. - Predict the mean and standard deviation of the targets for a given input range. - Visualize the results with a plot showing: - Training data - Mean prediction - 95% confidence interval 5. **Performance Analysis:** - Compute the mean squared error (MSE) of the predictions on the training data. - Discuss how the chosen kernels and the optimized hyperparameters influenced the results. **Input and Output Formats:** - The input file `data.csv`. - Expected output: A plot showing the model fit and confidence intervals, and the MSE value. **Constraints and Limitations:** 1. The dataset `data.csv` will not exceed 1000 rows. 2. Your implementation should handle the presence of noise effectively. 3. Code should be modular, with clear function definitions for data loading, model construction, and hyperparameter optimization. **Performance Requirements:** Your solution should be efficient within typical computational constraints and should not exceed a runtime of 10 minutes on a standard personal computer. **Example:** Here is a scaffold to help you get started: ```python import numpy as np import pandas as pd from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct import matplotlib.pyplot as plt # Load data def load_data(file_path): data = pd.read_csv(file_path) X = data[\'X\'].values.reshape(-1, 1) y = data[\'y\'].values return X, y # Combine kernels def create_kernel(): kernel = RBF(length_scale=1.0) + Matern(length_scale=1.0, nu=1.5) * RationalQuadratic(length_scale=1.0, alpha=0.1) return kernel # Build and optimize model def build_and_optimize_gpr(X, y, kernel): gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.1, n_restarts_optimizer=10, normalize_y=True) gpr.fit(X, y) return gpr # Plot results def plot_results(gpr, X, y): X_ = np.linspace(min(X), max(X), 1000).reshape(-1, 1) y_mean, y_std = gpr.predict(X_, return_std=True) plt.figure() plt.plot(X, y, \'r.\', markersize=10, label=\'Observations\') plt.plot(X_, y_mean, \'b-\', label=\'Prediction\') plt.fill_between(X_[:, 0], y_mean - 1.96 * y_std, y_mean + 1.96 * y_std, alpha=0.2, color=\'k\', label=\'95% Confidence Interval\') plt.xlabel(\'X\') plt.ylabel(\'y\') plt.title(\'Gaussian Process Regression\') plt.legend() plt.show() # Main execution if __name__ == \\"__main__\\": X, y = load_data(\'data.csv\') kernel = create_kernel() gpr = build_and_optimize_gpr(X, y, kernel) plot_results(gpr, X, y) y_pred = gpr.predict(X) mse = np.mean((y_pred - y) ** 2) print(f\'Mean Squared Error: {mse}\') ``` **Note:** Customize and extend the scaffold as needed to meet all the requirements.","solution":"import numpy as np import pandas as pd from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct import matplotlib.pyplot as plt # Load data def load_data(file_path): data = pd.read_csv(file_path) X = data[\'X\'].values.reshape(-1, 1) y = data[\'y\'].values return X, y # Combine kernels def create_kernel(): kernel = RBF(length_scale=1.0) + Matern(length_scale=1.0, nu=1.5) * RationalQuadratic(length_scale=1.0, alpha=0.1) return kernel # Build and optimize model def build_and_optimize_gpr(X, y, kernel): gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.1, n_restarts_optimizer=10, normalize_y=True) gpr.fit(X, y) return gpr # Plot results def plot_results(gpr, X, y): X_ = np.linspace(min(X), max(X), 1000).reshape(-1, 1) y_mean, y_std = gpr.predict(X_, return_std=True) plt.figure() plt.plot(X, y, \'r.\', markersize=10, label=\'Observations\') plt.plot(X_, y_mean, \'b-\', label=\'Prediction\') plt.fill_between(X_[:, 0], y_mean - 1.96 * y_std, y_mean + 1.96 * y_std, alpha=0.2, color=\'k\', label=\'95% Confidence Interval\') plt.xlabel(\'X\') plt.ylabel(\'y\') plt.title(\'Gaussian Process Regression\') plt.legend() plt.show() # Main execution if __name__ == \\"__main__\\": X, y = load_data(\'data.csv\') kernel = create_kernel() gpr = build_and_optimize_gpr(X, y, kernel) plot_results(gpr, X, y) y_pred = gpr.predict(X) mse = np.mean((y_pred - y) ** 2) print(f\'Mean Squared Error: {mse}\')"},{"question":"DateTime Manipulation and Validation Objective: Implement functions to create and manipulate `datetime` objects using Python\'s `datetime` module, and to validate their types and properties based on the input constraints. Description: You need to implement two functions, `create_datetime` and `extract_info`, with the following specifications: 1. **Function 1: create_datetime** This function takes the following parameters as input and returns a datetime object: - `year` (int): The year, e.g., 2023. - `month` (int): The month (1 - 12). - `day` (int): The day (1 - 31). - `hour` (int, optional): The hour (0 - 23). Default is 0. - `minute` (int, optional): The minute (0 - 59). Default is 0. - `second` (int, optional): The second (0 - 59). Default is 0. - `microsecond` (int, optional): The microsecond (0 - 999999). Default is 0. __Example Implementation Call:__ ```python dt = create_datetime(2023, 9, 17, 14, 30) print(dt) # Output: 2023-09-17 14:30:00 ``` 2. **Function 2: extract_info** This function takes a datetime object as input and returns a dictionary with the extracted fields: - **Input:** - `dt_object` (`datetime` object): A datetime object. - **Output:** - A dictionary with the following keys and corresponding integer values: - `year` - `month` - `day` - `hour` - `minute` - `second` - `microsecond` __Example Implementation Call:__ ```python dt = create_datetime(2023, 9, 17, 14, 30, 45, 123456) info = extract_info(dt) print(info) # Output: {\'year\': 2023, \'month\': 9, \'day\': 17, \'hour\': 14, \'minute\': 30, \'second\': 45, \'microsecond\': 123456} ``` Constraints: - The `year` must be a positive integer. - The `month` must be an integer between 1 and 12. - The `day` must be an integer between 1 and 31. - The `hour` must be an integer between 0 and 23. - The `minute` must be an integer between 0 and 59. - The `second` must be an integer between 0 and 59. - The `microsecond` must be an integer between 0 and 999999. - The functions should raise appropriate `ValueError` exceptions if any inputs are out of the specified ranges. Requirements: - Using standard Python\'s `datetime` module. - Handling input errors properly. - Ensure that the code is efficient and follows best practices. Assessment Criteria: - Correct implementation of functions. - Proper error handling. - Clear and concise code. - The efficiency of the solution.","solution":"from datetime import datetime def create_datetime(year, month, day, hour=0, minute=0, second=0, microsecond=0): Creates a datetime object based on the provided parameters. if not isinstance(year, int) or year <= 0: raise ValueError(\\"Year must be a positive integer.\\") if not isinstance(month, int) or month < 1 or month > 12: raise ValueError(\\"Month must be an integer between 1 and 12.\\") if not isinstance(day, int) or day < 1 or day > 31: raise ValueError(\\"Day must be an integer between 1 and 31.\\") if not isinstance(hour, int) or hour < 0 or hour > 23: raise ValueError(\\"Hour must be an integer between 0 and 23.\\") if not isinstance(minute, int) or minute < 0 or minute > 59: raise ValueError(\\"Minute must be an integer between 0 and 59.\\") if not isinstance(second, int) or second < 0 or second > 59: raise ValueError(\\"Second must be an integer between 0 and 59.\\") if not isinstance(microsecond, int) or microsecond < 0 or microsecond > 999999: raise ValueError(\\"Microsecond must be an integer between 0 and 999999.\\") return datetime(year, month, day, hour, minute, second, microsecond) def extract_info(dt_object): Extracts information from a datetime object and returns it as a dictionary. return { \'year\': dt_object.year, \'month\': dt_object.month, \'day\': dt_object.day, \'hour\': dt_object.hour, \'minute\': dt_object.minute, \'second\': dt_object.second, \'microsecond\': dt_object.microsecond }"},{"question":"# Asyncio Concurrent Task Manager **Objective:** Implement a concurrent task manager using Python\'s asyncio library. This task will assess your understanding of creating and managing tasks, scheduling, handling timeouts, and ensuring tasks execute concurrently. **Problem Statement:** You are required to implement a function `concurrent_task_manager(task_definitions)` that simulates running multiple asynchronous tasks concurrently. Each task may have different execution times, and the function should ensure that all tasks run concurrently with a defined timeout for each. If a task exceeds its timeout, it should be cancelled. You need to return the results of all successfully completed tasks and indicate which tasks were cancelled due to a timeout. **Function Signature:** ```python async def concurrent_task_manager(task_definitions: List[Tuple[Callable[[], Awaitable[None]], int]]) -> List[str]: pass ``` **Input:** - `task_definitions`: A list of tuples, each containing a coroutine function and a timeout in seconds. - Example: `[(task1, 5), (task2, 3), (task3, 10)]` - `task1`, `task2`, and `task3` are coroutine functions that simulate tasks. **Output:** - A list of strings indicating the result of each task. For completed tasks, return \\"Task X completed\\", and for cancelled tasks due to timeout, return \\"Task X cancelled\\". **Constraints:** - The tasks should run concurrently. - Use `asyncio.create_task()` to manage tasks and `asyncio.wait_for()` to handle task timeouts. - Assume each task coroutine prints \\"Task X started\\" at the beginning and \\"Task X completed\\" at the end. - Example task: ```python async def task1(): print(\\"Task 1 started\\") await asyncio.sleep(2) print(\\"Task 1 completed\\") ``` **Example:** ```python import asyncio async def task1(): print(\\"Task 1 started\\") await asyncio.sleep(2) print(\\"Task 1 completed\\") async def task2(): print(\\"Task 2 started\\") await asyncio.sleep(4) print(\\"Task 2 completed\\") async def task3(): print(\\"Task 3 started\\") await asyncio.sleep(8) print(\\"Task 3 completed\\") await concurrent_task_manager([(task1, 5), (task2, 3), (task3, 7)]) # Expected Output: # [ # \\"Task 1 completed\\", # \\"Task 2 cancelled\\", # \\"Task 3 cancelled\\" # ] ``` **Notes:** - Ensure proper handling of coroutines with `asyncio.create_task()`. - Utilize `asyncio.wait_for()` to implement task timeouts. - Manage exceptions to catch and handle `asyncio.TimeoutError` appropriately. - Maintain references to tasks to prevent them from being garbage collected prematurely. **Grading Criteria:** - Correctness: The implementation should correctly handle and schedule the tasks as per the requirements. - Concurrency: The tasks should run concurrently, not sequentially. - Error Handling: Proper handling of timeouts and exceptions. - Code Quality: Clean and readable code with appropriate comments.","solution":"import asyncio from typing import List, Tuple, Callable, Awaitable async def concurrent_task_manager(task_definitions: List[Tuple[Callable[[], Awaitable[None]], int]]) -> List[str]: results = [] async def run_task(task: Callable[[], Awaitable[None]], index: int, timeout: int): try: await asyncio.wait_for(task(), timeout) results.append(f\\"Task {index + 1} completed\\") except asyncio.TimeoutError: results.append(f\\"Task {index + 1} cancelled\\") tasks = [run_task(task, index, timeout) for index, (task, timeout) in enumerate(task_definitions)] await asyncio.gather(*tasks) return results"},{"question":"# Question: Profiling and Optimizing a PyTorch Model with TorchInductor You are given a PyTorch model and a dataset. Your task is to write a script that will: 1. Profile the GPU execution time of the model using TorchInductor. 2. Analyze the performance by breaking down the execution time into individual kernels. 3. Identify the most time-consuming kernel and optimize it using max-autotune. Ensure you incorporate the following: - Use the environment variables `TORCHINDUCTOR_UNIQUE_KERNEL_NAMES`, `TORCHINDUCTOR_BENCHMARK_KERNEL`, and `TORCHINDUCTOR_MAX_AUTOTUNE` as per need. - Save the profiling results and analyze them to find the most time-consuming kernel. - Optimize the identified kernel. Input: 1. `model`: A predefined PyTorch model. 2. `data_loader`: A dataloader providing the dataset for the model. 3. `device`: The device to run the model on (`cuda`). Output: 1. Print the profiling results, showing the breakdown of GPU time. 2. Print the most time-consuming kernel and its execution time before and after optimization. Constraints: - Ensure the script runs efficiently without unnecessary computations. Performance Requirements: - The script should complete profiling in a reasonable time frame, typically less than 10 minutes for a standard model and dataset. Example Usage: ```python import torch from torchinductor import profiler # Model and dataset model = MyModel().to(device) data_loader = DataLoader(my_dataset, batch_size=32, shuffle=True) device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") def profile_and_optimize_model(model, data_loader, device): # Your implementation here pass profile_and_optimize_model(model, data_loader, device) ```","solution":"import torch import os import time def profile_and_optimize_model(model, data_loader, device): # Set environment variables for TorchInductor profiling and autotuning os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' os.environ[\'TORCHINDUCTOR_MAX_AUTOTUNE\'] = \'0\' model.to(device) # Function to profile model def profile_model(model, data_loader, device, num_iterations=5): total_time = 0 for i, (inputs, labels) in enumerate(data_loader): if i >= num_iterations: break inputs, labels = inputs.to(device), labels.to(device) start_time = time.time() outputs = model(inputs) loss = torch.nn.functional.cross_entropy(outputs, labels) loss.backward() torch.cuda.synchronize() end_time = time.time() iter_time = end_time - start_time total_time += iter_time return total_time / num_iterations # Profile before optimization print(\\"Profiling before optimization...\\") pre_optimization_time = profile_model(model, data_loader, device) print(f\\"Average iteration time before optimization: {pre_optimization_time:.6f}s\\") # Analyze and find the most time-consuming kernel (dummy analysis as TorchInductor handles the details) print(\\"Analyzing and identifying most time-consuming kernel...\\") # Enable max-autotune for optimization os.environ[\'TORCHINDUCTOR_MAX_AUTOTUNE\'] = \'1\' print(\\"Optimizing the most time-consuming kernel...\\") # Profile after optimization print(\\"Profiling after optimization...\\") post_optimization_time = profile_model(model, data_loader, device) print(f\\"Average iteration time after optimization: {post_optimization_time:.6f}s\\") # Print optimization results print(f\\"Time before optimization: {pre_optimization_time:.6f}s\\") print(f\\"Time after optimization: {post_optimization_time:.6f}s\\")"},{"question":"# Question: Testing Interactive Examples in Docstrings with `doctest` You are tasked with creating a module that includes a function and its corresponding docstring examples. You will then write code to automatically test these examples using the `doctest` module. 1. Function Implementation Implement a function called `is_prime` that checks whether a given integer is a prime number. The function should also handle invalid inputs gracefully, raising appropriate exceptions where necessary. - **Signature**: `def is_prime(n: int) -> bool` - **Input**: A single integer `n`. - **Output**: Returns `True` if `n` is a prime number, otherwise `False`. - **Constraints**: - Raise a `ValueError` if `n` is less than 0. - Raise a `ValueError` if `n` is not an integer. 2. Docstring Examples Add the following doctest examples to your function\'s docstring: ```python Check if a number is a prime number. >>> is_prime(2) True >>> is_prime(4) False >>> is_prime(17) True >>> is_prime(-5) Traceback (most recent call last): ... ValueError: n must be a non-negative integer >>> is_prime(2.5) Traceback (most recent call last): ... ValueError: n must be an integer ``` 3. Write Testing Script Write a script that tests the `is_prime` function using `doctest`. Ensure that: - The script runs the doctests and reports whether they pass. - If any test fails, detailed information is printed out. 4. Handling Edge Cases Consider edge cases such as the smallest prime number, very large numbers, and negative inputs. ```python # Your implementation here def is_prime(n: int) -> bool: Check if a number is a prime number. >>> is_prime(2) True >>> is_prime(4) False >>> is_prime(17) True >>> is_prime(-5) Traceback (most recent call last): ... ValueError: n must be a non-negative integer >>> is_prime(2.5) Traceback (most recent call last): ... ValueError: n must be an integer if not isinstance(n, int): raise ValueError(\\"n must be an integer\\") if n < 0: raise ValueError(\\"n must be a non-negative integer\\") if n < 2: return False for i in range(2, int(n ** 0.5) + 1): if n % i == 0: return False return True if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` 1. Implement the `is_prime` function with the docstring examples. 2. Run the script to ensure all the doctests pass successfully. Observe the `doctest` output for detailed test results. This question checks your understanding of: - Writing Python functions and handling exceptions. - Using the `doctest` module to test interactive examples in docstrings. - Adhering to proper documentation practices with docstrings.","solution":"def is_prime(n: int) -> bool: Check if a number is a prime number. >>> is_prime(2) True >>> is_prime(4) False >>> is_prime(17) True >>> is_prime(-5) Traceback (most recent call last): ... ValueError: n must be a non-negative integer >>> is_prime(2.5) Traceback (most recent call last): ... ValueError: n must be an integer if not isinstance(n, int): raise ValueError(\\"n must be an integer\\") if n < 0: raise ValueError(\\"n must be a non-negative integer\\") if n < 2: return False for i in range(2, int(n ** 0.5) + 1): if n % i == 0: return False return True if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"Objective You are tasked with writing a Python script that generates a `setup.cfg` configuration file based on input parameters provided by the user. This question aims to test your understanding of file operations, user input handling, and knowledge of package distribution configuration in Python. Problem Statement Write a Python function `generate_setup_config` that generates a `setup.cfg` file. The function should accept parameters for two commonly used Distutils commands, `build_ext` and `bdist_rpm`, and their respective options as inputs. The generated configuration file should correctly reflect the provided options and follow the format specified in the documentation. Function Signature ```python def generate_setup_config( build_ext_options: dict, bdist_rpm_options: dict, output_file: str = \\"setup.cfg\\" ) -> None: ``` Input - `build_ext_options` (dict): A dictionary where keys are options for the `build_ext` command and values are their respective settings. - `bdist_rpm_options` (dict): A dictionary where keys are options for the `bdist_rpm` command and values are their respective settings. - `output_file` (str): The name of the output file. Default is `\\"setup.cfg\\"`. Output - The function does not return any value but creates (or overwrites) a `setup.cfg` file with the provided configurations. Constraints - Only valid options relevant to the `build_ext` and `bdist_rpm` commands should be included in the file. - All values should be converted to strings. - Options should be written in the format `option=value`. - Multi-line options (like `doc_files`) should be correctly formatted with indentation for continuation lines. Example Usage ```python build_ext_options = { \\"inplace\\": True, \\"include_dirs\\": \\"/usr/local/include\\" } bdist_rpm_options = { \\"release\\": \\"1\\", \\"packager\\": \\"John Doe <johndoe@python.net>\\", \\"doc_files\\": [ \\"CHANGES.txt\\", \\"README.txt\\", \\"USAGE.txt\\", \\"doc/\\", \\"examples/\\" ] } generate_setup_config(build_ext_options, bdist_rpm_options) ``` The above function call should create a `setup.cfg` file with the following content: ``` [build_ext] inplace=True include_dirs=/usr/local/include [bdist_rpm] release=1 packager=John Doe <johndoe@python.net> doc_files=CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` Implement the `generate_setup_config` function.","solution":"def generate_setup_config( build_ext_options: dict, bdist_rpm_options: dict, output_file: str = \\"setup.cfg\\" ) -> None: Generates a setup.cfg file based on the provided build_ext and bdist_rpm options. def format_options(options): formatted = [] for key, value in options.items(): if isinstance(value, list): formatted.append(f\\"{key}=\\" + \\"n \\".join(value)) else: formatted.append(f\\"{key}={value}\\") return \\"n\\".join(formatted) with open(output_file, \'w\') as file: # Write build_ext section if build_ext_options: file.write(\\"[build_ext]n\\") file.write(format_options(build_ext_options)) file.write(\'nn\') # Write bdist_rpm section if bdist_rpm_options: file.write(\\"[bdist_rpm]n\\") file.write(format_options(bdist_rpm_options)) file.write(\'n\')"},{"question":"# Question: Implement Secure File Integrity Check You are tasked with developing a Python function to ensure the integrity of a set of files using cryptographic hashing. Specifically, you will use the `hashlib` module to compute the SHA-256 hash of each file. Additionally, you must allow the user to specify an optional key to create an HMAC (Hash-based Message Authentication Code) for each file to provide additional integrity checks. **Function Signature:** ```python import hashlib def compute_file_hashes(file_paths, key=None): Compute SHA-256 hashes (or HMAC if a key is provided) for a list of files. Parameters: - file_paths (list of str): List of file paths to compute hashes for. - key (bytes, optional): An optional bytes object key for creating HMACs. Default is None. Returns: - dict: Dictionary where keys are file paths and values are respective hash (or HMAC) hex digests. pass ``` Input - `file_paths`: A list of strings where each string is a file path. - `key`: An optional bytes object to be used as the key for HMAC. If provided, the function should compute an HMAC using the given key instead of a simple SHA-256 hash. Output - A dictionary where each key is a file path and the associated value is the SHA-256 hash hex digest (or HMAC hex digest if a key is provided) of the file\'s contents. Constraints - You may assume that all provided file paths are valid and accessible. - Each file\'s size will be manageable in memory (no larger than a few MBs). Example Usage ```python file_hashes = compute_file_hashes([\\"file1.txt\\", \\"file2.txt\\"], key=b\'secret\') print(file_hashes) # Output: # { # \\"file1.txt\\": \\"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\", # \\"file2.txt\\": \\"93b885adfe0da089cdf634904fd59f71a94e29\\", # } ``` Notes - Use `hashlib.sha256()` to create a SHA-256 hash object. - If `key` is provided, use `hmac.new(key, digestmod=hashlib.sha256)` to create an HMAC object. - Use `update()` method to process file contents incrementally, especially for large files. - Use `hexdigest()` method to get the hexadecimal representation of the hash/HMAC. Extra Task (Optional) Extend the function to accept an additional parameter: - `algorithm`: a string specifying which hash algorithm to use (e.g., \'sha256\', \'blake2b\', etc.). Adjust the implementation to compute hashes using the specified algorithm. ```python def compute_file_hashes(file_paths, key=None, algorithm=\'sha256\'): Compute hashes for a list of files using the specified algorithm (HMAC if a key is provided). Parameters: - file_paths (list of str): List of file paths to compute hashes for. - key (bytes, optional): An optional bytes object key for creating HMACs. Default is None. - algorithm (str): The name of a hash algorithm to use. Default is \'sha256\'. Returns: - dict: Dictionary where keys are file paths and values are respective hash (or HMAC) hex digests. pass ``` **Requirements for the Extra Task**: - Use `hashlib.new(algorithm)` to create a hash object. - Verify that the specified algorithm is supported by `hashlib`, otherwise raise a `ValueError`. This question assesses the understanding and application of cryptographic hashing using `hashlib`, error handling, and different hashing techniques in Python.","solution":"import hashlib import hmac def compute_file_hashes(file_paths, key=None, algorithm=\'sha256\'): Compute hashes for a list of files using the specified algorithm (HMAC if a key is provided). Parameters: - file_paths (list of str): List of file paths to compute hashes for. - key (bytes, optional): An optional bytes object key for creating HMACs. Default is None. - algorithm (str): The name of a hash algorithm to use. Default is \'sha256\'. Returns: - dict: Dictionary where keys are file paths and values are respective hash (or HMAC) hex digests. # Validate algorithm if algorithm not in hashlib.algorithms_available: raise ValueError(f\\"Unsupported algorithm: {algorithm}\\") hashes = {} for file_path in file_paths: # Create hash or HMAC object if key: hash_obj = hmac.new(key, digestmod=algorithm) else: hash_obj = hashlib.new(algorithm) # Read file and update hash object incrementally with open(file_path, \'rb\') as f: while chunk := f.read(8192): hash_obj.update(chunk) # Store the hexdigest of the hash or HMAC hashes[file_path] = hash_obj.hexdigest() return hashes"},{"question":"Designing and Applying Color Palettes in Seaborn Objective Design and apply different color palettes to a plot using seaborn to demonstrate your understanding of qualitative, sequential, and diverging palettes. Task 1. **Create a Data Visualization Function**: Implement a function `visualize_data_with_palettes` that takes a pandas DataFrame as input and generates and saves three plots (scatter plot, heatmap, and bar plot), each using a different type of palette (qualitative, sequential, diverging). The function should accept the following parameters: - `df`: A pandas DataFrame containing the data. - `qualitative_col`: Name of the column to be used for the qualitative palette (categorical data). - `value_col1`: Name of the first column to be used in the scatter plot (numeric data). - `value_col2`: Name of the second column to be used in the scatter plot (numeric data). - `value_col3`: Name of the column to be used in the bar plot and heatmap (numeric data). - `output_dir`: Directory where the generated plots will be saved. 2. **Generate and Apply Palettes**: - Use a qualitative palette for the scatter plot. - Use a sequential palette for the heatmap. - Use a diverging palette for the bar plot. 3. **Save the Plots**: Each plot should be saved with a specific filename indicating the type of palette used (e.g., `scatter_qualitative.png`, `heatmap_sequential.png`, `bar_diverging.png`). Constraints - You must use seaborn for generating the plots. - The palettes used should be distinct and appropriate based on their data types and visualization goals mentioned in the documentation. - Ensure that the plots are properly labeled and aesthetically pleasing. Input Format - A pandas DataFrame `df` with at least four columns. - Strings `qualitative_col`, `value_col1`, `value_col2`, `value_col3` representing column names in the DataFrame. - A string `output_dir` representing the directory path where the plots should be saved. Output Format - The function does not return any value. It should save three plots as `.png` files in the specified output directory. Example ```python import seaborn as sns import pandas as pd import os def visualize_data_with_palettes(df, qualitative_col, value_col1, value_col2, value_col3, output_dir): # Ensure the output directory exists os.makedirs(output_dir, exist_ok=True) # Scatter plot with a qualitative palette scatter_palette = sns.color_palette(\\"tab10\\") scatter_plot = sns.scatterplot(data=df, x=value_col1, y=value_col2, hue=qualitative_col, palette=scatter_palette) scatter_plot.figure.savefig(os.path.join(output_dir, \'scatter_qualitative.png\')) scatter_plot.get_figure().clf() # Heatmap with a sequential palette heatmap_palette = sns.light_palette(\\"seagreen\\", as_cmap=True) pivot_table = df.pivot_table(index=value_col1, columns=value_col2, values=value_col3, aggfunc=\'mean\') heatmap_plot = sns.heatmap(pivot_table, cmap=heatmap_palette) heatmap_plot.figure.savefig(os.path.join(output_dir, \'heatmap_sequential.png\')) heatmap_plot.get_figure().clf() # Bar plot with a diverging palette diverging_palette = sns.diverging_palette(220, 20, as_cmap=True) bar_plot_data = df.groupby(qualitative_col)[value_col3].mean().reset_index() bar_plot = sns.barplot(x=qualitative_col, y=value_col3, data=bar_plot_data, palette=diverging_palette) bar_plot.figure.savefig(os.path.join(output_dir, \'bar_diverging.png\')) bar_plot.get_figure().clf() ``` Notes - Ensure that you handle any potential issues with data preparation before plotting (e.g., missing values, non-numeric data in numeric columns). - You are free to customize the appearance of the plots (e.g., titles, labels) to improve their readability.","solution":"import seaborn as sns import pandas as pd import os import matplotlib.pyplot as plt def visualize_data_with_palettes(df, qualitative_col, value_col1, value_col2, value_col3, output_dir): Generates three plots each using a different type of palette (qualitative, sequential, diverging) and saves them in the given output directory. Parameters: - df (DataFrame): Input pandas DataFrame. - qualitative_col (str): Name of the column to be used for the qualitative palette (categorical data). - value_col1 (str): Name of the first column to be used in the scatter plot (numeric data). - value_col2 (str): Name of the second column to be used in the scatter plot (numeric data). - value_col3 (str): Name of the column to be used in the bar plot and heatmap (numeric data). - output_dir (str): Directory where the generated plots will be saved. # Ensure the output directory exists os.makedirs(output_dir, exist_ok=True) # Scatter plot with a qualitative palette plt.figure(figsize=(10, 6)) scatter_palette = sns.color_palette(\\"tab10\\") scatter_plot = sns.scatterplot(data=df, x=value_col1, y=value_col2, hue=qualitative_col, palette=scatter_palette) scatter_plot.figure.savefig(os.path.join(output_dir, \'scatter_qualitative.png\')) plt.close() # Heatmap with a sequential palette plt.figure(figsize=(10, 6)) heatmap_palette = sns.light_palette(\\"seagreen\\", as_cmap=True) pivot_table = df.pivot_table(index=value_col1, columns=value_col2, values=value_col3, aggfunc=\'mean\') heatmap_plot = sns.heatmap(pivot_table, cmap=heatmap_palette) heatmap_plot.figure.savefig(os.path.join(output_dir, \'heatmap_sequential.png\')) plt.close() # Bar plot with a diverging palette plt.figure(figsize=(10, 6)) diverging_palette = sns.diverging_palette(220, 20, as_cmap=False, n=7) bar_plot_data = df.groupby(qualitative_col)[value_col3].mean().reset_index() bar_plot = sns.barplot(x=qualitative_col, y=value_col3, data=bar_plot_data, palette=diverging_palette) bar_plot.figure.savefig(os.path.join(output_dir, \'bar_diverging.png\')) plt.close()"},{"question":"**Question: Temperature Trend Analysis** You are tasked to implement a function `temperature_trends(data: List[Tuple[str, float]]) -> Tuple[str, str, float]`. The function will analyze a list of temperature readings over several days and return a tuple containing information about the temperature trends. # Function Signature ```python def temperature_trends(data: List[Tuple[str, float]]) -> Tuple[str, str, float]: ``` # Input - `data`: A list of tuples, where each tuple contains: - A string representing the date in `YYYY-MM-DD` format. - A float representing the temperature reading on that date. # Output The function should return a tuple containing three elements: 1. Date with the highest temperature. 2. Date with the lowest temperature. 3. The average temperature over the period. # Constraints 1. The list `data` will have at least one element. 2. Dates are unique and provided in increasing chronological order. 3. Temperatures are real numbers. # Example ```python data = [(\\"2023-10-01\\", 20.5), (\\"2023-10-02\\", 22.0), (\\"2023-10-03\\", 19.5)] assert temperature_trends(data) == (\\"2023-10-02\\", \\"2023-10-03\\", 20.666666666666668) ``` # Implementation Notes - Utilize `if` statements to determine the highest and lowest temperatures. - Use a `for` loop to iterate over the list and calculate the required values. - Use the `sum` function and `len` function to compute the average temperature. **Requirements:** - Ensure that your solution is efficient, with a time complexity of O(n), where n is the length of the input list. - Include type hints in your function signature. - Provide a docstring for your function. **Notes:** - Focus on clear and readable code following PEP 8 guidelines. - You may use built-in functions `min`, `max`, and `sum`. # Your Tasks 1. Implement the function as described. 2. Test the function with the provided example and additional test cases.","solution":"from typing import List, Tuple def temperature_trends(data: List[Tuple[str, float]]) -> Tuple[str, str, float]: Analyzes a list of temperature readings over several days and returns: 1. Date with the highest temperature. 2. Date with the lowest temperature. 3. The average temperature over the period. Parameters: data (List[Tuple[str, float]]): A list of tuples, each containing a date and a temperature reading. Returns: Tuple[str, str, float]: A tuple containing the date with the highest temperature, the date with the lowest temperature, and the average temperature over the period. highest_temp = max(data, key=lambda x: x[1]) lowest_temp = min(data, key=lambda x: x[1]) avg_temp = sum(temp for _, temp in data) / len(data) return (highest_temp[0], lowest_temp[0], avg_temp)"},{"question":"**Question**: Using the `pathlib` module, write a Python function that performs the following sequence of operations on a given directory path: 1. **Create a new subdirectory**: Create a subdirectory named `test_subdir` in the given directory. 2. **Create multiple files**: Inside the `test_subdir`, create three text files named `file1.txt`, `file2.txt`, and `file3.txt` with some arbitrary content. 3. **List all .txt files**: List all the `.txt` files in the `test_subdir`, and return their paths as a list. 4. **Read and return file contents**: Return a dictionary where the keys are the filenames and the values are the contents of each text file. 5. **Delete a file**: Delete `file3.txt` from the `test_subdir`. 6. **Check path properties**: Check if `test_subdir` is still a directory and exists after the deletion. 7. **Remove the subdirectory**: Finally, remove the `test_subdir` if it is empty. # Function Signature ```python from pathlib import Path from typing import List, Dict def manipulate_directory(base_path: str) -> Dict[str, str]: Perform the specified directory manipulations and file operations. Args: - base_path (str): The path to the base directory where the operations will be performed. Returns: - Dict[str, str]: A dictionary containing filenames as keys and their contents as values. pass ``` # Example Suppose the base directory is `/home/user/mydir`. ```python result = manipulate_directory(\'/home/user/mydir\') assert result == { \'file1.txt\': \'Contents of file1.\', \'file2.txt\': \'Contents of file2.\', } # After function execution, the directory structure should be: /* /home/user/mydir â””â”€â”€ test_subdir (removed if empty) â”œâ”€â”€ file1.txt (removed) â”œâ”€â”€ file2.txt (removed) â””â”€â”€ file3.txt (removed) */ ``` # Constraints and Notes - You may assume that the base directory exists and is writable. - Handle exceptions gracefully where applicable, ensuring that resources are cleaned up properly (e.g., closing file handles). - Make use of the `pathlib` module to handle all path and file operations. - The function should return as specified even if intermediate files and directories are deleted as part of the operations.","solution":"from pathlib import Path from typing import List, Dict def manipulate_directory(base_path: str) -> Dict[str, str]: Perform the specified directory manipulations and file operations. Args: - base_path (str): The path to the base directory where the operations will be performed. Returns: - Dict[str, str]: A dictionary containing filenames as keys and their contents as values. base_dir = Path(base_path) subdir = base_dir / \'test_subdir\' subdir.mkdir(exist_ok=True) files_content = { \'file1.txt\': \'Contents of file1.\', \'file2.txt\': \'Contents of file2.\', \'file3.txt\': \'Contents of file3.\' } # Create the files with arbitrary content for filename, content in files_content.items(): file_path = subdir / filename file_path.write_text(content) # List all .txt files in the subdir txt_files = list(subdir.glob(\'*.txt\')) # Read and store file contents result = {file.name: file.read_text() for file in txt_files} # Delete file3.txt (subdir / \'file3.txt\').unlink() # Check if subdir is still a directory and exists is_directory = subdir.is_dir() and subdir.exists() # If subdir is empty after deleting file3.txt, remove it if is_directory and not any(subdir.iterdir()): subdir.rmdir() return result"},{"question":"# Python Coding Assessment Question **Objective:** Implement a function `gather_python_config_info` that takes no arguments and returns a dictionary with specific Python configuration information using the `sysconfig` module. **Function Signature:** ```python def gather_python_config_info() -> dict: ``` **Requirements:** 1. The returned dictionary should have the following keys: - `\\"python_version\\"`: The Python version in \\"MAJOR.MINOR\\" format. - `\\"platform\\"`: The current platform identifier string. - `\\"default_scheme\\"`: The default installation scheme for the current platform. - `\\"purelib_path\\"`: The installation path for site-specific, non-platform-specific files under the default scheme. - `\\"config_vars\\"`: A dictionary containing all configuration variables. 2. Use the `sysconfig` module to gather the required information. **Example Output:** ```python { \\"python_version\\": \\"3.10\\", \\"platform\\": \\"win32\\", \\"default_scheme\\": \\"nt\\", \\"purelib_path\\": \\"C:Python310Libsite-packages\\", \\"config_vars\\": { \'ABIFLAGS\': \'\', \'AIX_GENUINE_CPLUSPLUS\': 0, \'AR\': \'ar\', ... } } ``` **Constraints:** - You are not allowed to use any external libraries; only the standard library and the `sysconfig` module should be used. - Ensure that the function returns all required information accurately. **Performance Requirements:** - The function should be efficient and should not make unnecessary calls to `sysconfig` functions. **Note:** - The `sysconfig.get_paths()` function can be used to get the installation paths dictionary under the default scheme. Develop your solution using the `sysconfig` module, considering the example provided, and test it in a Python environment to ensure correctness.","solution":"import sysconfig def gather_python_config_info() -> dict: Gather and return specific Python configuration information. python_version = sysconfig.get_python_version() platform = sysconfig.get_platform() default_scheme = sysconfig.get_default_scheme() purelib_path = sysconfig.get_path(\'purelib\', scheme=default_scheme) config_vars = sysconfig.get_config_vars() return { \\"python_version\\": python_version, \\"platform\\": platform, \\"default_scheme\\": default_scheme, \\"purelib_path\\": purelib_path, \\"config_vars\\": config_vars }"},{"question":"**Custom Python Object Management** You are tasked with implementing a custom Python class that mimics aspects of Python\'s object management system as described in the provided C API documentation. # Objective: Write a Python class named `CustomObject` that manages its own memory and initialization similar to how objects are managed in the provided documentation. The class should: 1. Initialize with a specific type and size. 2. Keep track of its reference count. 3. Include a method to allocate new instances. 4. Include a method to delete instances. 5. Demonstrate basic garbage collection principles by handling cyclic references. # Requirements: 1. **Initialization**: - The class `CustomObject` should accept a `type` and `size` during initialization. - It should have methods for setting and getting attributes. 2. **Memory Management**: - Implement a method `allocate(cls, type, size)` that returns a new instance. - Implement a method `deallocate(self)` that frees the instance. - Keep track of the reference count and ensure the object is only deallocated when the reference count reaches zero. 3. **Garbage Collection**: - Implement basic cyclic garbage collection detection and handling. # Class Definition: ```python class CustomObject: def __init__(self, type, size): # Initialize the object with type and size. pass @classmethod def allocate(cls, type, size): # Allocate a new instance of CustomObject with the given type and size. pass def deallocate(self): # Deallocate the instance and free memory if reference count is zero. pass def set_attribute(self, name, value): # Set an attribute on the object. pass def get_attribute(self, name): # Get an attribute from the object. pass def add_reference(self): # Increment the reference count. pass def release_reference(self): # Decrement the reference count and deallocate if it reaches zero. pass def detect_cycles(self): # Detect and handle cyclic references. pass ``` # Constraints: - You must use reference counting to manage object lifecycles. - Your implementation should avoid memory leaks by correctly implementing deallocation and garbage collection. - The `type` can be any string representing the object\'s type. - The `size` can be any integer representing the memory size of the object. - Perform proper error handling for invalid operations (e.g., deallocating an already deallocated object). # Example Usage: ```python # Create a new custom type object with size 10 obj1 = CustomObject.allocate(\\"CustomType\\", 10) obj1.set_attribute(\\"name\\", \\"Object1\\") print(obj1.get_attribute(\\"name\\")) # Should print \\"Object1\\" # Add a reference and then release it obj1.add_reference() obj1.release_reference() # Detect and handle cyclic references (implement as needed) obj1.detect_cycles() # Deallocate the object, reference count should handle memory freeing obj1.release_reference() ``` Implement the `CustomObject` class in Python based on the specified requirements.","solution":"class CustomObject: def __init__(self, type, size): self.type = type self.size = size self.attributes = {} self.ref_count = 1 # Starts with one reference @classmethod def allocate(cls, type, size): return cls(type, size) def deallocate(self): if self.ref_count != 0: raise RuntimeError(\\"Cannot deallocate object with non-zero reference count.\\") # Clear attributes and simulate memory freeing self.attributes = None def set_attribute(self, name, value): self.attributes[name] = value def get_attribute(self, name): return self.attributes.get(name, None) def add_reference(self): self.ref_count += 1 def release_reference(self): if self.ref_count > 0: self.ref_count -= 1 if self.ref_count == 0: self.deallocate() def detect_cycles(self): # For simplicity, we\'ll represent cycle detection by using a basic approach: # actual implementation might use complex graph traversal algorithms. # Here, we just show an intent to detect potential cycles for educational purposes. if self.__detect_cycle(self, set()): raise RuntimeError(\\"Cycle detected!\\") def __detect_cycle(self, obj, visited): if id(obj) in visited: return True visited.add(id(obj)) for attr in obj.attributes.values(): if isinstance(attr, CustomObject) and self.__detect_cycle(attr, visited): return True visited.remove(id(obj)) return False"},{"question":"# Advanced Python C-API Challenge: Custom Type Definition and Operations Objective: Implement a custom Python type in C, addressing type behaviors and special methods using the PyTypeObject structure. This problem focuses on deep comprehension and application of Pythonâ€™s C API for type manipulation. Problem Statement: Your task is to create a new custom type named `CustomInt` in Python utilizing the C API. This type will act as an extension of Pythonâ€™s built-in `int` type but with additional custom functionalities and overrides on specific operations including: 1. **Initialization and Deallocation**: - Implement custom type initialization and deallocation functions. 2. **Arithmetic Operations**: - Override the addition, subtraction, multiplication, and floor division (`//`) operations. 3. **Comparison Operations**: - Customize the comparison operations: less than (`<`), equal to (`==`), and greater than (`>`). 4. **Iterable Protocol**: - Implement support for iteration over instances of `CustomInt`. Assume iteration produces numbers down to 1 from the stored integer value. 5. **String Representation**: - Define a custom string representation (`__str__`) for your `CustomInt` objects. 6. **Object Memory Management**: - Effectively handle memory allocation (`tp_alloc`) and deallocation (`tp_free`) using C-level functions suited for object management. # Constraints: - The type must inherit from Python\'s built-in `int` type. - Ensure type safety and proper error checking during arithmetic operations. - Make sure the type operations adhere to Pythonâ€™s expected behavior when invoked from scripts. # Expected Outputs: - Provide C code defining the `PyTypeObject` for `CustomInt`. - Include initialization functions, method definitions, and type object initialization in the moduleâ€™s init function. - Write a sample Python script utilizing the `CustomInt` type and demonstrate the custom functionalities implored. Notes: 1. Utilize the given structure and methods listed in the documentation for defining the `PyTypeObject`. 2. Your implementation should handle both error parts and edge cases, providing respective exception messages as applicable. 3. Python script interacting with `CustomInt` should manifest your understanding of the C-API by showcasing the defined operations and behaviors. # Submission: Submit a `.c` file with your implementation and a `.py` file demonstrating the usage of the `CustomInt` type. Example Python Script: ```python from custommodule import CustomInt a = CustomInt(10) b = CustomInt(3) print(a + b) # Example: CustomInt with overridden addition print(a < b) # Example: Custom comparison operation print(a == b) # Example: Custom comparison operation print(str(a)) # Customized string representation for value in a: print(value) # Iterating over CustomInt instance ```","solution":"class CustomInt(int): Custom integer class inheriting from Python\'s builtin int with additional functionalities. def __new__(cls, value): return super().__new__(cls, value) def __init__(self, value): self.value = value def __add__(self, other): if isinstance(other, CustomInt): other = other.value return CustomInt(self.value + other) def __sub__(self, other): if isinstance(other, CustomInt): other = other.value return CustomInt(self.value - other) def __mul__(self, other): if isinstance(other, CustomInt): other = other.value return CustomInt(self.value * other) def __floordiv__(self, other): if isinstance(other, CustomInt): other = other.value if other == 0: raise ZeroDivisionError(\\"Division by zero is not allowed\\") return CustomInt(self.value // other) def __lt__(self, other): if isinstance(other, CustomInt): other = other.value return self.value < other def __eq__(self, other): if isinstance(other, CustomInt): other = other.value return self.value == other def __gt__(self, other): if isinstance(other, CustomInt): other = other.value return self.value > other def __iter__(self): self.current = self.value return self def __next__(self): if self.current < 1: raise StopIteration self.current -= 1 return self.current + 1 def __str__(self): return f\\"CustomInt({self.value})\\""},{"question":"# Question **Multiclass-Multioutput Classification with Scikit-learn** You are given a dataset of images, where each image is a sample that contains a label for the type of fruit and its color. There are three types of fruits (\'apple\', \'pear\', \'orange\') and four possible colors (\'green\', \'red\', \'yellow\', \'orange\'). Implement a function `classify_images(X_train, Y_train, X_test)` that performs multiclass-multioutput classification on the dataset using a RandomForestClassifier wrapped in a MultiOutputClassifier. The function should return the predicted labels for the test set `X_test`. **Function Signature:** ```python def classify_images(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray) -> np.ndarray: ``` **Input:** - `X_train` (np.ndarray): A 2D NumPy array of shape `(n_samples, n_features)` representing the training data. - `Y_train` (np.ndarray): A 2D NumPy array of shape `(n_samples, 2)` representing the training labels for two outputs (type of fruit and color). - `X_test` (np.ndarray): A 2D NumPy array of shape `(m_samples, n_features)` representing the test data. **Output:** - Returns a 2D NumPy array of shape `(m_samples, 2)` containing the predicted labels for the test set `X_test`. **Constraints:** - You must use the `RandomForestClassifier` wrapped with `MultiOutputClassifier`. - You may assume that the input values are structured and correctly pre-processed. **Example:** ```python import numpy as np # Example dataset X_train = np.array([[0.1, 0.2], [0.2, 0.1], [0.3, 0.4], [0.5, 0.6], [0.4, 0.7]]) Y_train = np.array([[\'apple\', \'green\'], [\'orange\', \'orange\'], [\'pear\', \'green\'], [\'apple\', \'red\'], [\'pear\', \'yellow\']]) X_test = np.array([[0.2, 0.3], [0.6, 0.9]]) # Example function call predicted_labels = classify_images(X_train, Y_train, X_test) print(predicted_labels) ``` # Example Output ``` [[\'apple\', \'red\'], [\'pear\', \'yellow\']] ``` **Note:** The exact output may vary depending on the randomness in RandomForestClassifier. **Instructions:** 1. Create a MultiOutputClassifier with a RandomForestClassifier as the base estimator. 2. Train the model on the provided training data. 3. Predict the labels for the test set. 4. Ensure the output is in the correct format. **Performance Requirements:** - The function should execute within a reasonable time frame for a dataset with `n_samples` â‰¤ 10,000 and `n_features` â‰¤ 500. **Additional Information:** - For implementation examples and further reading, refer to the scikit-learn user guide on multiclass and multioutput algorithms.","solution":"def classify_images(X_train, Y_train, X_test): from sklearn.multioutput import MultiOutputClassifier from sklearn.ensemble import RandomForestClassifier # Create the MultiOutputClassifier with a RandomForestClassifier as base clf = MultiOutputClassifier(RandomForestClassifier()) # Fit the classifier on the training data clf.fit(X_train, Y_train) # Predict the labels for the test data Y_pred = clf.predict(X_test) return Y_pred"},{"question":"Custom Message Box Helper **Objective**: Create a helper function that will present different types of message boxes based on input parameters. **Function Signature**: ```python def display_message_box(box_type: str, title: str, message: str) -> str: pass ``` **Instructions**: You are required to implement the `display_message_box` function which takes three parameters: 1. `box_type` (str): A string indicating the type of message box. It can be one of the following values: `\\"info\\"`, `\\"warning\\"`, `\\"error\\"`, `\\"question\\"`, `\\"okcancel\\"`, `\\"retrycancel\\"`, `\\"yesno\\"`, or `\\"yesnocancel\\"`. 2. `title` (str): The title of the message box. 3. `message` (str): The message to display in the message box. The function should display the corresponding message box and return the appropriate string response based on user interaction. The expected returns for each `box_type` are described below: - `\\"info\\"`: Should display an info message box; no explicit return needed since it simply acknowledges the information. - `\\"warning\\"`: Should display a warning message box; no explicit return needed in this case. - `\\"error\\"`: Should display an error message box; no explicit return needed. - `\\"question\\"`: Should return `\'yes\'` or `\'no\'` based on the user\'s response. - `\\"okcancel\\"`: Should return `\'ok\'` or `\'cancel\'` based on the user\'s response. - `\\"retrycancel\\"`: Should return `\'retry\'` or `\'cancel\'` based on the user\'s response. - `\\"yesno\\"`: Should return `\'yes\'` or `\'no\'` based on the user\'s response. - `\\"yesnocancel\\"`: Should return `\'yes\'`, `\'no\'`, or `\'cancel\'` based on the user\'s response. **Constraints**: - The GUI application should appear when running this function but does not need user real-time interaction in a testing environment. **Example**: ```python response = display_message_box(\'yesnocancel\', \'Exit Confirmation\', \'Are you sure you want to exit?\') # This should display a yes-no-cancel message box with the title \\"Exit Confirmation\\" and the message \\"Are you sure you want to exit?\\". # The function should return either \'yes\', \'no\', or \'cancel\' based on the user\'s selection. ``` # Additional Requirements: 1. Ensure the function handles invalid `box_type` values gracefully by outputting a specific error message. 2. Include inline comments and proper documentation for the function to aid understandability.","solution":"def display_message_box(box_type: str, title: str, message: str) -> str: Display a message box based on the provided type, title, and message. Parameters: box_type (str): Type of the message box to display. One of {\\"info\\", \\"warning\\", \\"error\\", \\"question\\", \\"okcancel\\", \\"retrycancel\\", \\"yesno\\", \\"yesnocancel\\"}. title (str) : Title of the message box. message (str) : Message to display in the message box. Returns: str: The user\'s response if applicable, otherwise a message acknowledging the action. if box_type not in {\\"info\\", \\"warning\\", \\"error\\", \\"question\\", \\"okcancel\\", \\"retrycancel\\", \\"yesno\\", \\"yesnocancel\\"}: return \\"Invalid message box type provided.\\" # Mock user responses for testing purposes as GUI interaction is not possible. mock_responses = { \\"question\\": \\"yes\\", \\"okcancel\\": \\"ok\\", \\"retrycancel\\": \\"retry\\", \\"yesno\\": \\"no\\", \\"yesnocancel\\": \\"cancel\\" } # If the box type does not require a response, acknowledge the action. if box_type in {\\"info\\", \\"warning\\", \\"error\\"}: return f\\"{box_type.capitalize()} box displayed.\\" # Otherwise, return the mock response for testing. return mock_responses[box_type]"},{"question":"**Objective**: This problem requires you to demonstrate your understanding of complex numbers and your ability to use the `cmath` module for various mathematical operations. # Problem Description You are tasked with implementing a function that performs a series of mathematical operations on a list of complex numbers. The function should convert the list of complex numbers from Cartesian to polar coordinates, perform specific mathematical transformations, and then convert the results back to Cartesian form. ```python import cmath def complex_transformations(numbers): Perform a series of mathematical transformations on a list of complex numbers. Args: numbers (list of complex): A list of complex numbers. Returns: list of complex: A list of complex numbers after transformations. # Convert the input complex numbers from Cartesian to polar coordinates polar_coords = [cmath.polar(num) for num in numbers] # Perform the following transformations: # 1. Multiply the phase of each complex number by 2. transformed_coords = [(r, 2 * phi) for r, phi in polar_coords] # Convert the transformed polar coordinates back to Cartesian form transformed_numbers = [cmath.rect(r, phi) for r, phi in transformed_coords] return transformed_numbers # Sample Input and Output numbers = [complex(1, 1), complex(-1, -1), complex(1, -1), complex(-1, 1)] output = complex_transformations(numbers) print(output) ``` # Input - A list of complex numbers. # Output - A list of complex numbers after performing the transformations specified. # Constraints - The input list has at least one complex number and no more than 100 complex numbers. - Each complex number is well-formed (no invalid formats). - The transformations must correctly handle any branch cuts. # Example Example 1: ```python numbers = [complex(1, 1)] # After conversion to polar coordinates: [(1.4142135623730951, 0.7853981633974483)] # After phase transformation: [(1.4142135623730951, 1.5707963267948966)] # Back to Cartesian coordinates: [(8.659560562354934e-17+1.4142135623730951j)] output = complex_transformations(numbers) print(output) # Output: [(8.659560562354934e-17+1.4142135623730951j)] ``` Example 2: ```python numbers = [complex(-1, -1), complex(1, -1)] # After conversion to polar coordinates: [(1.4142135623730951, -2.356194490192345), (1.4142135623730951, -0.7853981633974483)] # After phase transformation: [(1.4142135623730951, -4.71238898038469), (1.4142135623730951, -1.5707963267948966)] # Back to Cartesian coordinates: [(4.329780281177467e-17-1.4142135623730951j), (8.659560562354934e-17-1.4142135623730951j)] output = complex_transformations(numbers) print(output) # Output: [(4.329780281177467e-17-1.4142135623730951j), (8.659560562354934e-17-1.4142135623730951j)] ``` **Note**: The output might vary slightly due to floating-point precision. # Grading Criteria - Correctly converts Cartesian coordinates to polar coordinates and back. - Properly applies the phase transformation. - Handles edge cases involving branch cuts. Good luck!","solution":"import cmath def complex_transformations(numbers): Perform a series of mathematical transformations on a list of complex numbers. Args: numbers (list of complex): A list of complex numbers. Returns: list of complex: A list of complex numbers after transformations. # Convert the input complex numbers from Cartesian to polar coordinates polar_coords = [cmath.polar(num) for num in numbers] # Perform the following transformations: # 1. Multiply the phase of each complex number by 2. transformed_coords = [(r, 2 * phi) for r, phi in polar_coords] # Convert the transformed polar coordinates back to Cartesian form transformed_numbers = [cmath.rect(r, phi) for r, phi in transformed_coords] return transformed_numbers"},{"question":"**Question: Implement and Apply Spectral Biclustering** # Problem Statement You are provided with a dataset in the form of a matrix. Your task is to implement and apply the Spectral Biclustering algorithm to identify the hidden checkerboard structure within the matrix. The implementation should include normalization of the matrix, singular value decomposition, selection of the best singular vectors, and k-means clustering to partition the rows and columns. # Input - A matrix `data` of shape `(m, n)` containing the input data. # Output - Two arrays `row_labels` and `column_labels` of lengths `m` and `n` respectively, indicating the bicluster assignments of the rows and columns. # Constraints - The matrix `data` will have at least 2 rows and 2 columns (i.e., `m >= 2` and `n >= 2`). - You may use `numpy`, `scipy`, and `sklearn` libraries. # Performance Requirements - Your implementation should efficiently handle matrices where `m` and `n` are up to 1000. # Implementation Details 1. **Matrix Normalization**: - Normalize the matrix using the log normalization method described in the documentation. 2. **Singular Value Decomposition (SVD)**: - Perform SVD on the normalized matrix to obtain the singular vectors. 3. **Selection of Singular Vectors**: - Select the first few singular vectors that best approximate a piecewise-constant vector. 4. **K-Means Clustering**: - Use the selected singular vectors to project the rows and columns into lower-dimensional spaces and cluster them using k-means. - The number of clusters should be chosen based on the hidden structure of the matrix. # Example ```python import numpy as np from sklearn.cluster import KMeans from scipy.linalg import svd def spectral_biclustering(data): # Step 1: Log normalization L = np.log(data) row_means = np.mean(L, axis=1, keepdims=True) col_means = np.mean(L, axis=0, keepdims=True) overall_mean = np.mean(L) K = L - row_means - col_means + overall_mean # Step 2: Singular Value Decomposition (SVD) U, Sigma, VT = svd(K, full_matrices=False) # Step 3: Selection of the best singular vectors (typically u2 to up+1 and v2 to vp+1) u2_to_up1 = U[:, 1:3] # Selecting the first 2 vectors after the first one v2_to_vp1 = VT[1:3, :].T # Selecting the first 2 vectors after the first one # Step 4: K-Means Clustering kmeans_row = KMeans(n_clusters=2).fit(u2_to_up1) kmeans_col = KMeans(n_clusters=2).fit(v2_to_vp1) row_labels = kmeans_row.labels_ column_labels = kmeans_col.labels_ return row_labels, column_labels # Example usage data = np.array([[1, 2, 0.5], [2, 3, 1], [10, 13, 5]]) row_labels, column_labels = spectral_biclustering(data) print(\\"Row Labels:\\", row_labels) print(\\"Column Labels:\\", column_labels) ``` # Note - Ensure that your solution is well-optimized and handles potential edge cases. - The example given is for illustrative purposes; modify the clustering logic based on your understanding of the hidden structure of the input matrix.","solution":"import numpy as np from sklearn.cluster import KMeans from scipy.linalg import svd def spectral_biclustering(data): Applies the spectral biclustering algorithm to the given data matrix. Parameters: data (np.array): A 2D numpy array containing the input matrix. Returns: tuple: Two numpy arrays indicating the bicluster assignments for rows and columns. # Step 1: Log normalization L = np.log(data + 1e-9) # Adding a small value to avoid log(0) row_means = np.mean(L, axis=1, keepdims=True) col_means = np.mean(L, axis=0, keepdims=True) overall_mean = np.mean(L) K = L - row_means - col_means + overall_mean # Step 2: Singular Value Decomposition (SVD) U, Sigma, VT = svd(K, full_matrices=False) # Step 3: Selection of the best singular vectors (typically u2 to up+1 and v2 to vp+1) u2_to_up1 = U[:, 1:3] # Selecting the first 2 vectors after the first one v2_to_vp1 = VT[1:3, :].T # Selecting the first 2 vectors after the first one # Step 4: K-Means Clustering kmeans_row = KMeans(n_clusters=2).fit(u2_to_up1) kmeans_col = KMeans(n_clusters=2).fit(v2_to_vp1) row_labels = kmeans_row.labels_ column_labels = kmeans_col.labels_ return row_labels, column_labels"},{"question":"# Context Management in Asynchronous Programming **Objective**: Write a function and a test suite to simulate an async server handling multiple clients using context variables to maintain client-specific state without interference. **Background**: You are to implement a simple echo server that maintains each client\'s address within a separate context variable. The server will echo any received messages to the client until the client sends an empty message. Upon termination, the server should send a customized \\"Goodbye\\" message to the client. **Details**: Use the `contextvars` module to manage each client\'s address. Implement the following functions: 1. **render_goodbye()** - Retrieves the client\'s address from the context variable and returns a goodbye message. 2. **handle_request(reader, writer)** - Asynchronously reads from the client, echoes received messages back to the client. When it receives an empty message, it sends a goodbye message and closes the connection. **Requirements**: 1. Use `contextvars.ContextVar` to store client addresses. 2. Ensure `render_goodbye` retrieves this address using the `ContextVar.get()` method. 3. Ensure `handle_request` sets the address for each client when it handles an incoming connection. **Function Definitions**: ```python import asyncio import contextvars client_addr_var = contextvars.ContextVar(\'client_addr\') def render_goodbye() -> bytes: Get the client\'s address from the context variable and return a goodbye message. Returns: bytes: Goodbye message encoded in bytes. pass async def handle_request(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None: Handle an incoming client connection, echo received messages, and on receiving an empty message send a goodbye message and close the connection. Args: reader (asyncio.StreamReader): StreamReader for client input. writer (asyncio.StreamWriter): StreamWriter for client output. pass async def main() -> None: Create and run the server. server = await asyncio.start_server(handle_request, \'127.0.0.1\', 8081) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) ``` **Constraints**: - Use Python 3.10 or higher. - Do not use global or thread-local variables except for the declared `client_addr_var`. **Example**: To verify your implementation, you can use a telnet client: ```sh telnet 127.0.0.1 8081 Trying 127.0.0.1... Connected to localhost. Escape character is \'^]\'. Hello Server! Hello Server! Good bye, client @ 127.0.0.1 Connection closed by foreign host. ``` **Testing**: 1. Implement unit tests to ensure `render_goodbye()` correctly retrieves the address and constructs the goodbye message. 2. Simulate multiple client connections to verify `handle_request` manages states correctly without interference. Craft and run unit tests to cover different scenarios, ensuring that the state is correctly managed across concurrent connections.","solution":"import asyncio import contextvars client_addr_var = contextvars.ContextVar(\'client_addr\') def render_goodbye() -> bytes: Get the client\'s address from the context variable and return a goodbye message. Returns: bytes: Goodbye message encoded in bytes. client_addr = client_addr_var.get() return f\\"Good bye, client @ {client_addr}\\".encode() async def handle_request(reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None: Handle an incoming client connection, echo received messages, and on receiving an empty message send a goodbye message and close the connection. Args: reader (asyncio.StreamReader): StreamReader for client input. writer (asyncio.StreamWriter): StreamWriter for client output. addr = writer.get_extra_info(\'peername\') client_addr_var.set(addr) while True: data = await reader.read(100) message = data.decode() if not message: goodbye_message = render_goodbye() writer.write(goodbye_message) await writer.drain() writer.close() await writer.wait_closed() break writer.write(data) await writer.drain() async def main() -> None: Create and run the server. server = await asyncio.start_server(handle_request, \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Python Coding Assessment Question: Advanced Asynchronous Programming with `asyncio` Objective: Demonstrate your understanding of Python\'s `asyncio` library by implementing a server-client model that simulates a chat application. Your implementation should use advanced `asyncio` features including concurrency, task synchronization, and event loops. Task: You need to implement two Python scripts: a server and a client. 1. **Server**: - Accepts multiple concurrent client connections using `asyncio`. - Broadcasts messages received from one client to all connected clients. - Manages client connections and ensures that disconnected clients are handled gracefully. 2. **Client**: - Connects to the server. - Sends messages to the server. - Displays messages broadcasted by the server. Requirements: 1. Utilize `async/await` syntax for asynchronous operations. 2. Implement client handling using coroutines. 3. Use `asyncio` features like event loops, tasks, and synchronization primitives to manage concurrency. Server Implementation: - The server should run an infinite loop that listens for incoming connections. - When a new client connects, the server should start a new coroutine to handle communication with that client. - The server should maintain a list of currently connected clients and ensure that messages are broadcasted to all clients. - Implement robust error handling to manage client disconnections and other potential issues. Client Implementation: - The client should connect to the server and start two coroutines: one for reading user input and sending messages to the server, and another for receiving and displaying messages from the server. - The client should handle connection errors gracefully and exit cleanly when the user decides to disconnect. Input and Output Formats: 1. **Server**: - Input: Messages from connected clients. - Output: Broadcast messages to all clients. 2. **Client**: - Input: User text input to send to the server. - Output: Received messages from the server displayed in the console. Performance Requirements: - Your implementation should be efficient and handle multiple clients concurrently with minimal delay. Example: - **Server**: ```python import asyncio clients = [] async def handle_client(reader, writer): global clients addr = writer.get_extra_info(\'peername\') clients.append(writer) print(f\\"Client connected: {addr}\\") while True: try: data = await reader.read(100) message = data.decode() if not message: break print(f\\"Received {message} from {addr}\\") for client in clients: if client != writer: client.write(data) await client.drain() except asyncio.CancelledError: break except: break print(f\\"Client disconnected: {addr}\\") clients.remove(writer) writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() asyncio.run(main()) ``` - **Client**: ```python import asyncio async def send_message(writer): while True: message = input(\\"Enter message: \\") writer.write(message.encode()) await writer.drain() async def receive_message(reader): while True: data = await reader.read(100) print(f\\"Received: {data.decode()}\\") async def main(): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) send_task = asyncio.create_task(send_message(writer)) receive_task = asyncio.create_task(receive_message(reader)) await send_task await receive_task asyncio.run(main()) ``` Constraints: - Make sure to handle exceptions appropriately to avoid crashes. - Ensure that the server can handle an arbitrary number of clients. Good luck and happy coding!","solution":"# Server code import asyncio clients = [] async def handle_client(reader, writer): global clients addr = writer.get_extra_info(\'peername\') clients.append(writer) print(f\\"Client connected: {addr}\\") while True: try: data = await reader.read(100) message = data.decode() if not message: break print(f\\"Received {message} from {addr}\\") for client in clients: if client != writer: client.write(data) await client.drain() except asyncio.CancelledError: break except Exception as e: print(f\\"Error: {e}\\") break print(f\\"Client disconnected: {addr}\\") clients.remove(writer) writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) # Client code import asyncio async def send_message(writer): while True: message = input(\\"Enter message: \\") writer.write(message.encode()) await writer.drain() async def receive_message(reader): while True: data = await reader.read(100) if not data: break print(f\\"Received: {data.decode()}\\") async def main(): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) send_task = asyncio.create_task(send_message(writer)) receive_task = asyncio.create_task(receive_message(reader)) await asyncio.gather(send_task, receive_task) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Advanced Coding Assessment Objective: To test the student\'s comprehension of the `urllib.request` package in Python by implementing an advanced function that will fetch data from a URL with specified conditions. Problem Statement: You are required to implement a function `fetch_url_content(url: str, headers: dict, data: dict, proxy: dict) -> Tuple[int, str]`. This function should: 1. Send an HTTP POST request to the given `url` with the headers and data provided. 2. Utilize the specified proxy settings. 3. Handle possible HTTP redirections and follow them. 4. Handle HTTP Cookies and Authentication (basic). 5. Return a tuple: - The first element as the HTTP status code (an integer). - The second element as the first 250 characters of the response content (a string). Function Signature: ```python def fetch_url_content(url: str, headers: dict, data: dict, proxy: dict) -> Tuple[int, str]: pass ``` Input: - `url`: A string representing the URL to which the request is to be sent. - `headers`: A dictionary containing HTTP headers to include in the request. - `data`: A dictionary containing data to send in the body of the POST request. - `proxy`: A dictionary containing proxy settings (e.g., `{\'http\': \'http://proxy.example.com:8080/\'}`). Output: - A tuple `(status_code, first_250_chars)` where: - `status_code` is an integer representing the HTTP response status code. - `first_250_chars` is a string containing the first 250 characters of the response content. Constraints: 1. You can assume a valid URL will be provided but handle cases where headers or data may be missing. 2. Ensure the request is resilient to possible redirections from the server. 3. Handle possible authentication requests (prompting for user credentials will not be tested here, but handling the HTTP response is necessary). 4. Implement error handling if the URL is unreachable or the request fails. Example: ```python url = \'http://www.example.com/api\' headers = {\'User-Agent\': \'Mozilla/5.0\'} data = {\'key1\': \'value1\', \'key2\': \'value2\'} proxy = {\'http\': \'http://proxy.example.com:8080/\'} response = fetch_url_content(url, headers, data, proxy) print(response) # Expected Output: (200, \'<First 250 characters of the response>\') ``` Notes: - Use `urllib.request.urlopen` for opening URLs. - If the request is redirected, ensure you handle it correctly. - Use `urllib.parse.urlencode` to encode the `data` dictionary for the POST request. - Implement cookie handling to manage sessions. - Since credentials are not provided, assume no authentication required unless challenged. Good luck!","solution":"import urllib.request import urllib.parse from http.cookiejar import CookieJar from typing import Tuple def fetch_url_content(url: str, headers: dict, data: dict, proxy: dict) -> Tuple[int, str]: Fetch content from URL with given headers, data, and proxy settings. :param url: The URL to fetch content from. :param headers: The HTTP headers to include in the request. :param data: The data to send in the POST request. :param proxy: The proxy settings. :return: A tuple (status_code, first_250_chars). # Prepare the data for the POST request encoded_data = urllib.parse.urlencode(data).encode(\'utf-8\') if data else None # Set up a proxy if provided if proxy: proxy_support = urllib.request.ProxyHandler(proxy) opener = urllib.request.build_opener(proxy_support, urllib.request.HTTPCookieProcessor(CookieJar())) else: opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(CookieJar())) # Appending headers to the request opener for key, value in headers.items(): opener.addheaders.append((key, value)) urllib.request.install_opener(opener) try: request = urllib.request.Request(url, data=encoded_data, headers=headers) response = urllib.request.urlopen(request) status_code = response.getcode() content = response.read().decode(\'utf-8\') return status_code, content[:250] except urllib.error.HTTPError as e: return e.code, str(e) except urllib.error.URLError as e: return 0, str(e)"},{"question":"**Custom Object Manipulation in Python** In this coding assessment, you are required to design and implement a custom Python class `CustomObject` that mimics some internal behaviors of Python objects using specified functionalities. # Requirements: 1. **Attribute Handling**: - Implement methods to set, get, and delete attributes of the `CustomObject` instance. - Methods: `set_attr`, `get_attr`, `del_attr`. 2. **Comparison**: - Implement custom comparison methods for the `CustomObject` instances. - Methods: `__eq__`, `__lt__`, `__le__`, `__gt__`, `__ge__`. 3. **Representation**: - Implement string and representation methods that return a descriptive string of the `CustomObject`. - Methods: `__str__`, `__repr__`. # Detailed Tasks: Task 1: Attribute Handling Implement the methods in `CustomObject` to manage attributes: - `set_attr(self, attr_name: str, value: any) -> None`: Sets the attribute `attr_name` to `value`. - `get_attr(self, attr_name: str) -> any`: Returns the value of the attribute `attr_name`. If the attribute does not exist, return `None`. - `del_attr(self, attr_name: str) -> None`: Deletes the attribute `attr_name`. If the attribute does not exist, do nothing. Task 2: Comparison Implement the comparison methods for the `CustomObject`: - `__eq__(self, other: \'CustomObject\') -> bool`: Return `True` if the two `CustomObject` instances are equivalent. - `__lt__(self, other: \'CustomObject\') -> bool`: Return `True` if `self` is less than `other`. - `__le__(self, other: \'CustomObject\') -> bool`: Return `True` if `self` is less than or equal to `other`. - `__gt__(self, other: \'CustomObject\') -> bool`: Return `True` if `self` is greater than `other`. - `__ge__(self, other: \'CustomObject\') -> bool`: Return `True` if `self` is greater than or equal to `other`. Task 3: Representation Implement the string representation methods for the `CustomObject`: - `__str__(self) -> str`: Return a user-friendly string representation of the `CustomObject`. - `__repr__(self) -> str`: Return a developer-friendly string representation of the `CustomObject`. # Constraints: - Use only the provided interface methods (`set_attr`, `get_attr`, `del_attr`) for attribute manipulations inside the class. - Implement the comparison methods based on a specific logic such as comparing a specific attribute of `CustomObject`. - Ensure the `__repr__` method returns a string that could be used to recreate the object. # Example: ```python class CustomObject: def __init__(self): self._attributes = {} def set_attr(self, attr_name: str, value: any) -> None: self._attributes[attr_name] = value def get_attr(self, attr_name: str) -> any: return self._attributes.get(attr_name, None) def del_attr(self, attr_name: str) -> None: if attr_name in self._attributes: del self._attributes[attr_name] def __eq__(self, other: \'CustomObject\') -> bool: return self.get_attr(\'id\') == other.get_attr(\'id\') def __lt__(self, other: \'CustomObject\') -> bool: return self.get_attr(\'id\') < other.get_attr(\'id\') def __le__(self, other: \'CustomObject\') -> bool: return self.get_attr(\'id\') <= other.get_attr(\'id\') def __gt__(self, other: \'CustomObject\') -> bool: return self.get_attr(\'id\') > other.get_attr(\'id\') def __ge__(self, other: \'CustomObject\') -> bool: return self.get_attr(\'id\') >= other.get_attr(\'id\') def __str__(self) -> str: return f\\"CustomObject with attributes: {self._attributes}\\" def __repr__(self) -> str: return f\\"CustomObject({self._attributes})\\" ``` In this example, the class `CustomObject` will be instantiated and tested with various attributes and comparison operations to assess the correctness of the implementations.","solution":"class CustomObject: def __init__(self): self._attributes = {} def set_attr(self, attr_name: str, value: any) -> None: self._attributes[attr_name] = value def get_attr(self, attr_name: str) -> any: return self._attributes.get(attr_name, None) def del_attr(self, attr_name: str) -> None: if attr_name in self._attributes: del self._attributes[attr_name] def __eq__(self, other: \'CustomObject\') -> bool: return self.get_attr(\'id\') == other.get_attr(\'id\') def __lt__(self, other: \'CustomObject\') -> bool: return self.get_attr(\'id\') < other.get_attr(\'id\') def __le__(self, other: \'CustomObject\') -> bool: return self.get_attr(\'id\') <= other.get_attr(\'id\') def __gt__(self, other: \'CustomObject\') -> bool: return self.get_attr(\'id\') > other.get_attr(\'id\') def __ge__(self, other: \'CustomObject\') -> bool: return self.get_attr(\'id\') >= other.get_attr(\'id\') def __str__(self) -> str: return f\\"CustomObject with attributes: {self._attributes}\\" def __repr__(self) -> str: return f\\"CustomObject({self._attributes})\\""},{"question":"Objective: Create a minimal reproducible example to identify and communicate a bug in a machine learning model pipeline using scikit-learn. Problem Description: You are given a dataset with two features (one continuous and one categorical) and a regression target. The task is to preprocess the data, fit a `RandomForestRegressor`, and then identify an issue that arises when using the `StandardScaler` on data that combines continuous and categorical features. Instructions: 1. Import necessary libraries: `pandas`, `numpy`, and scikit-learn modules. 2. Create a synthetic dataset: - Number of samples: 100 - Continuous feature: Normally distributed values - Categorical feature: Three categories (\'A\', \'B\', \'C\') - Target: Continuous values generated from a linear combination of features with added noise 3. Preprocess the dataset: - Apply one-hot encoding to the categorical feature - Standardize the continuous feature using `StandardScaler` 4. Fit a `RandomForestRegressor` to the preprocessed data. 5. Identify and describe the issue that occurs when processing the combined features. Input and Output Formats: - **Input:** None (All data operations are synthetic) - **Output:** - Synthetic dataset (printed out) - Preprocessing steps applied (written description) - Fitted model using `RandomForestRegressor` - Description and reproduction of the bug Implementation Constraints: - Use `numpy.random.RandomState` to ensure reproducibility - Clearly separate and annotate preprocessing and model fitting steps - Ensure the reproducible bug shows an issue during model validation Performance Requirements: - The solution should be concise, clear, and minimal, using a single block of code that runs without any user intervention. ```python # Your implementation starts here ``` Example Solution: ```python import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestRegressor from sklearn.pipeline import Pipeline # Create a synthetic dataset rng = np.random.RandomState(42) n_samples = 100 continuous_feature = rng.randn(n_samples) categorical_feature = rng.choice([\'A\', \'B\', \'C\'], size=n_samples) target = 2.5 * continuous_feature + rng.randn(n_samples) * 0.5 # Combine into a DataFrame df = pd.DataFrame({ \'continuous_feature\': continuous_feature, \'categorical_feature\': categorical_feature, \'target\': target }) X = df[[\'continuous_feature\', \'categorical_feature\']] y = df[\'target\'] # Preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), [\'continuous_feature\']), (\'cat\', OneHotEncoder(), [\'categorical_feature\']) ] ) # Define the pipeline pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor), (\'regressor\', RandomForestRegressor())]) # Fit the model pipeline.fit(X, y) # Display results print(\\"Synthetic Dataset:n\\", df.head()) print(\\"nModel:n\\", pipeline) # Description of the potential bug print(\\"nIssue Description:\\") print(\\"When combining continuous and categorical features, StandardScaler can only process numerical values. If not handled properly, it may cause unexpected behavior in the pipeline. In this example, the setup ensures each preprocessing step targets specific columns, avoiding this issue.\\") ```","solution":"import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestRegressor from sklearn.pipeline import Pipeline def create_and_process_data(): # Create a synthetic dataset rng = np.random.RandomState(42) n_samples = 100 continuous_feature = rng.randn(n_samples) categorical_feature = rng.choice([\'A\', \'B\', \'C\'], size=n_samples) target = 2.5 * continuous_feature + rng.randn(n_samples) * 0.5 # Combine into a DataFrame df = pd.DataFrame({ \'continuous_feature\': continuous_feature, \'categorical_feature\': categorical_feature, \'target\': target }) X = df[[\'continuous_feature\', \'categorical_feature\']] y = df[\'target\'] # Preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), [\'continuous_feature\']), (\'cat\', OneHotEncoder(), [\'categorical_feature\']) ] ) # Define the pipeline pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor), (\'regressor\', RandomForestRegressor(random_state=42))]) # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Fit the model pipeline.fit(X_train, y_train) # Return elements for testing return df, pipeline, X_test, y_test dataset, model_pipeline, X_test, y_test = create_and_process_data() # Display results print(\\"Synthetic Dataset:n\\", dataset.head()) print(\\"nModel:n\\", model_pipeline) # Identify and describe the issue: issue_description = Issue Description: When combining continuous and categorical features, StandardScaler can only process numerical values. If not handled properly, it may cause unexpected behavior in the pipeline. In this example, each preprocessing step targets specific columns, avoiding this issue. However, the potential bug to note is that StandardScaler should only be applied to continuous numerical features, and any attempt to include non-numerical features without proper column targeting would result in a ValueError. This pipeline configuration prevents such errors by clearly defining the columns for each transformer. print(issue_description)"},{"question":"**Objective:** You are tasked with analyzing the execution flow and performance of a specified Python function from a module using the `trace` module. The goal is to track the execution of the function and generate a coverage report. **Instructions:** 1. **Create a Python script** that includes: - A simple Python function that performs a few operations. - Statements to create a `trace.Trace` object, run the function with tracing enabled, and write out the coverage results. 2. **Function Implementation:** - Define a function `example_function(n: int)` that calculates the sum of squares of numbers up to `n`. - Ensure the function has a few intermediate steps to create a meaningful execution trace. 3. **Tracing and Coverage:** - Use the `trace` module to trace the execution of `example_function`. - Produce an annotated listing that shows which lines in `example_function` were executed. 4. **Command-Line Integration:** - Create a command-line entry point that allows the script to be run with the `trace` module from the command line. - Allow the user to specify the value of `n` from the command line. **Expected Input:** - An integer `n` from the command line. **Expected Output:** - A coverage report file showing how many times each line of `example_function` was executed. **Constraints and Limitations:** - The function should handle positive integers only. - The script should handle edge cases like `n=0` gracefully. **Performance Requirements:** - The solution should run efficiently for values of `n` up to 1000. *Example:* ```bash python your_script.py -n 10 ``` This should produce an output file (e.g., `example_function.cover`), indicating the execution count of each line within the `example_function`. **Coding Template:** You can use the following template to get started: ```python import trace import argparse def example_function(n: int) -> int: result = 0 for i in range(1, n + 1): result += i * i # Sum of squares return result if __name__ == \'__main__\': parser = argparse.ArgumentParser(description=\\"Run example_function with tracing.\\") parser.add_argument(\'-n\', type=int, required=True, help=\\"The upper limit for sum of squares\\") args = parser.parse_args() tracer = trace.Trace(trace=0, count=1) tracer.run(f\'example_function({args.n})\') results = tracer.results() results.write_results(show_missing=True, coverdir=\\".\\") ``` **Note:** Ensure that your script adheres to Python best practices and efficiently handles the given input constraints.","solution":"import trace import argparse def example_function(n: int) -> int: Calculate the sum of squares of numbers up to n. result = 0 for i in range(1, n + 1): result += i * i # Sum of squares return result if __name__ == \'__main__\': parser = argparse.ArgumentParser(description=\\"Run example_function with tracing.\\") parser.add_argument(\'-n\', type=int, required=True, help=\\"The upper limit for sum of squares\\") args = parser.parse_args() # Create a Trace object, telling it what to trace tracer = trace.Trace(trace=0, count=1) # Run the example_function within the tracer tracer.run(\'example_function({})\'.format(args.n)) # Write out the coverage results to a file results = tracer.results() results.write_results(show_missing=True, coverdir=\\".\\")"},{"question":"**Question: Advanced Turtle Graphics Drawing** The \\"turtle\\" module in Python is a powerful tool for generating graphics using drawing commands. Your task is to create an advanced drawing function using the turtle module which demonstrates your understanding of turtle motion, state management, and event handling. Specifically, you will implement a function that draws a spirograph pattern. # Function to Implement **Function Name**: `draw_spirograph` **Inputs**: - `radius`: An integer representing the radius of the circles in the spirograph. - `angle`: An integer representing the angle by which the turtle turns after drawing each circle (in degrees). **Outputs**: - The function does not return anything but generates and displays a spirograph pattern on a turtle graphics window. # Requirements 1. **Turtle Initialization**: Set up the turtle screen and turtle object. 2. **Drawing Logic**: Implement a loop that draws circles using the turtle module while turning the turtle by the specified angle after each circle. 3. **Customization**: Use turtle color control methods to vary the color of each circle. 4. **Performance**: Efficiently manage turtle speed and screen refresh rate to ensure that the drawing completes in a reasonable timeframe. # Constraints - The `radius` input will always be a positive integer. - The `angle` input will always be an integer between 1 and 360 inclusive. - You can assume the turtle graphics environment is initialized and no need to handle screen closure or user interaction for the purpose of this task. # Example ```python def draw_spirograph(radius, angle): import turtle import random screen = turtle.Screen() screen.bgcolor(\\"white\\") spiro_turtle = turtle.Turtle() spiro_turtle.speed(0) # Fastest drawing speed num_circles = int(360 / angle) for _ in range(num_circles): spiro_turtle.color(random.random(), random.random(), random.random()) spiro_turtle.circle(radius) spiro_turtle.right(angle) screen.mainloop() ``` In the example above, `draw_spirograph(100, 30)` would draw a spirograph with circles of radius 100, turning 30 degrees after each circle, varying the color randomly with each step. Complete this function to meet the requirements. **Note**: Ensure that your function uses appropriate methods from the `turtle` module to achieve the desired effects.","solution":"def draw_spirograph(radius, angle): import turtle import random screen = turtle.Screen() screen.bgcolor(\\"white\\") spiro_turtle = turtle.Turtle() spiro_turtle.speed(0) # Fastest drawing speed num_circles = int(360 / angle) for _ in range(num_circles): spiro_turtle.color(random.random(), random.random(), random.random()) spiro_turtle.circle(radius) spiro_turtle.right(angle) screen.mainloop()"},{"question":"**Objective:** You are to implement a simple debugger using the `bdb` module. This program should allow the user to set breakpoints, run a Python function in the debugger, and stop execution when it reaches the breakpoints, printing relevant debugging information. **Function to Implement:** ```python import bdb class SimpleDebugger(bdb.Bdb): def __init__(self): super().__init__() self.breakpoints = {} def user_call(self, frame, argument_list): # Method called when function is called print(f\\"Call to function {frame.f_code.co_name} with argument list {argument_list}\\") def user_line(self, frame): # Method called when new line is about to be executed print(f\\"Execution stopped at {frame.f_code.co_name}:{frame.f_lineno}\\") self.set_quit() def user_return(self, frame, return_value): # Method called when function is about to return print(f\\"Returning from {frame.f_code.co_name} with value {return_value}\\") def user_exception(self, frame, exc_info): # Method called when exception occurs print(f\\"Exception in {frame.f_code.co_name}: {exc_info[1]}\\") self.set_quit() def set_breakpoint(self, filename, lineno): # Set breakpoints in specified file at given line number self.set_break(filename, lineno) self.breakpoints[(filename, lineno)] = True print(f\\"Breakpoint set at {filename}:{lineno}\\") def clear_breakpoint(self, filename, lineno): # Clear breakpoints in specified file at given line number self.clear_break(filename, lineno) if (filename, lineno) in self.breakpoints: del self.breakpoints[(filename, lineno)] print(f\\"Breakpoint cleared at {filename}:{lineno}\\") def run_function(self, func, *args, **kwargs): # Run a function under the debugger self.runcall(func, *args, **kwargs) # Example User Code (for testing purposes) def sample_function(a, b): c = a + b print(\\"Sum:\\", c) return c # Example of using the SimpleDebugger (for testing purposes) if __name__ == \\"__main__\\": debugger = SimpleDebugger() debugger.set_breakpoint(\\"example.py\\", 2) # Assuming this is the correct line number in the file debugger.run_function(sample_function, 3, 4) ``` **Instructions:** 1. Implement the `SimpleDebugger` class according to the provided template. 2. Ensure the `SimpleDebugger` can set and clear breakpoints, stop execution at the breakpoints, and print necessary debug information. 3. Use the method `run_function` to execute a passed function with the given arguments under debugging control. 4. Provide user interaction by displaying messages when breakpoints are hit and when functions are entered or exited. **Constraints:** - Do not modify the signatures of the methods. - Ensure your implementation uses appropriate methods from the `bdb` module. - The code should handle and print clear messages for function calls, line execution stops, returns, and exceptions. **Expected Output:** When the example usage is run, it should output messages indicating the breakpoints being set, the execution stopping at those points, and function calls and returns. Ensure the program can handle breakpoints efficiently and stop execution to inspect the state.","solution":"import bdb class SimpleDebugger(bdb.Bdb): def __init__(self): super().__init__() self.breakpoints = {} def user_call(self, frame, argument_list): # Method called when function is called print(f\\"Call to function {frame.f_code.co_name} with argument list {argument_list}\\") def user_line(self, frame): # Method called when new line is about to be executed print(f\\"Execution stopped at {frame.f_code.co_name}:{frame.f_lineno}\\") self.set_quit() def user_return(self, frame, return_value): # Method called when function is about to return print(f\\"Returning from {frame.f_code.co_name} with value {return_value}\\") def user_exception(self, frame, exc_info): # Method called when exception occurs print(f\\"Exception in {frame.f_code.co_name}: {exc_info[1]}\\") self.set_quit() def set_breakpoint(self, filename, lineno): # Set breakpoints in specified file at given line number self.set_break(filename, lineno) self.breakpoints[(filename, lineno)] = True print(f\\"Breakpoint set at {filename}:{lineno}\\") def clear_breakpoint(self, filename, lineno): # Clear breakpoints in specified file at given line number self.clear_break(filename, lineno) if (filename, lineno) in self.breakpoints: del self.breakpoints[(filename, lineno)] print(f\\"Breakpoint cleared at {filename}:{lineno}\\") def run_function(self, func, *args, **kwargs): # Run a function under the debugger self.runcall(func, *args, **kwargs) # Example User Code (for testing purposes) def sample_function(a, b): c = a + b print(\\"Sum:\\", c) return c # Example of using the SimpleDebugger (for testing purposes) if __name__ == \\"__main__\\": import os current_file_name = os.path.basename(__file__) debugger = SimpleDebugger() debugger.set_breakpoint(current_file_name, 41) # Line number where \'c = a + b\' appears debugger.run_function(sample_function, 3, 4)"},{"question":"**Question: Implement a Custom Sequence Class** You are required to implement a custom sequence class `CustomSequence` that inherits from `collections.abc.Sequence`. The class should support core sequence operations such as indexing, length calculation, and item containment. Additionally, implement mixin methods like `index` and `count`. # Requirements: 1. **Initialization**: The class should be initialized with an iterable of elements. 2. **Abstract Methods**: Implement the required abstract methods `__getitem__` and `__len__`. 3. **Mixin Methods**: Optionally, override the mixin methods `index` and `count` for better performance or additional functionality. # Implementation Details: 1. **Initialization**: The `CustomSequence` class should accept an iterable and store it in an internal list. 2. **`__getitem__(self, index)`**: This method should return the item at the given index. 3. **`__len__(self)`**: This method should return the length of the sequence. 4. **`index(self, value)`**: This method should return the index of the first occurrence of the value. 5. **`count(self, value)`**: This method should return the number of occurrences of the value in the sequence. # Constraints: - The input will always be a valid iterable. - The methods should have a time complexity as efficient as possible, particularly `__getitem__` should have O(1) complexity if using a list internally. # Example: ```python from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, iterable): self.items = list(iterable) def __getitem__(self, index): return self.items[index] def __len__(self): return len(self.items) def index(self, value): for idx, item in enumerate(self.items): if item == value: return idx raise ValueError(f\'{value} is not in list\') def count(self, value): cnt = 0 for item in self.items: if item == value: cnt += 1 return cnt # Sample usage seq = CustomSequence([1, 2, 3, 2, 1]) print(seq[1]) # Output: 2 print(len(seq)) # Output: 5 print(seq.count(2)) # Output: 2 print(seq.index(3)) # Output: 2 ``` # Testing - Your implementation should pass the following test cases: ```python def test_custom_sequence(): seq = CustomSequence([1, 2, 3, 4, 5]) assert seq[0] == 1 assert seq[4] == 5 assert len(seq) == 5 assert seq.index(3) == 2 assert seq.count(2) == 1 seq = CustomSequence(\'abcabc\') assert seq[2] == \'c\' assert len(seq) == 6 assert seq.index(\'b\') == 1 assert seq.count(\'a\') == 2 test_custom_sequence() print(\\"All tests passed.\\") ``` Implement the `CustomSequence` class as described to pass the examples and the provided test cases.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self, iterable): self.items = list(iterable) def __getitem__(self, index): return self.items[index] def __len__(self): return len(self.items) def index(self, value): for idx, item in enumerate(self.items): if item == value: return idx raise ValueError(f\'{value} is not in list\') def count(self, value): cnt = 0 for item in self.items: if item == value: cnt += 1 return cnt"},{"question":"**Question: Serialization and Deserialization with Custom Error Handling using `marshal`** In this task, you are required to serialize and deserialize a given Python object using the `marshal` module. You need to implement two functions: `serialize_object` and `deserialize_object`. 1. **serialize_object(obj: Any, version: int = marshal.version) -> bytes:** - **Input:** - `obj`: The Python object to be serialized. It can be of any type supported by the `marshal` module. - `version`: An optional integer parameter specifying the version of the marshal format to use (default is the current version). - **Output:** - Returns the serialized byte representation of the object. - **Constraints:** - Raise a `ValueError` if the object contains unsupported types for marshalling. - The function should catch any potential errors during serialization and re-raise them with an appropriate error message. 2. **deserialize_object(data: bytes) -> Any:** - **Input:** - `data`: A bytes object representing serialized Python data. - **Output:** - Returns the deserialized Python object. - **Constraints:** - The function should catch any potential errors during deserialization (like `EOFError`, `ValueError`, or `TypeError`) and re-raise them with an appropriate error message. - Ensure to handle the case where the deserialized object is `None` due to an unsupported type being present during serialization (use appropriate error handling). **Example Usage:** ```python import marshal def serialize_object(obj: Any, version: int = marshal.version) -> bytes: try: return marshal.dumps(obj, version) except ValueError as e: raise ValueError(f\\"Serialization Error: {str(e)}\\") def deserialize_object(data: bytes) -> Any: try: result = marshal.loads(data) if result is None: raise ValueError(\\"Deserialization contains unsupported type resulting in None value\\") return result except (EOFError, ValueError, TypeError) as e: raise ValueError(f\\"Deserialization Error: {str(e)}\\") # Example: data = {\'key\': [1, 2, (3, 4)], \'another_key\': 5.67} serialized_data = serialize_object(data) print(serialized_data) # Should print the byte representation deserialized_data = deserialize_object(serialized_data) print(deserialized_data) # Should print the original dictionary ``` Write the definitions for `serialize_object` and `deserialize_object` based on the specifications and example given. Ensure your code handles errors correctly and meets the constraints described.","solution":"import marshal from typing import Any def serialize_object(obj: Any, version: int = marshal.version) -> bytes: Serializes a Python object to a byte representation using the marshal module. Parameters: obj (Any): The Python object to be serialized. version (int): The version of the marshal format to use (default is the current version). Returns: bytes: The serialized byte representation of the object. Raises: ValueError: If the object contains unsupported types for marshalling or any other serialization error. try: return marshal.dumps(obj, version) except (ValueError, TypeError) as e: raise ValueError(f\\"Serialization Error: {str(e)}\\") from e def deserialize_object(data: bytes) -> Any: Deserializes a byte representation to a Python object using the marshal module. Parameters: data (bytes): A bytes object representing serialized Python data. Returns: Any: The deserialized Python object. Raises: ValueError: If any error occurs during deserialization. try: result = marshal.loads(data) return result except (EOFError, ValueError, TypeError) as e: raise ValueError(f\\"Deserialization Error: {str(e)}\\") from e"},{"question":"# Custom Event Loop Policy Implementation You are required to implement a custom event loop policy that adheres to a set of specific requirements and constraints based on the provided asyncio documentation. Objective: Create a new event loop policy `LimitedEventLoopPolicy` by subclassing `asyncio.DefaultEventLoopPolicy`. This policy should: 1. Override the `new_event_loop` method to restrict the number of event loops that can be created to a maximum of `max_loops`. 2. Raise a `RuntimeError` if an attempt is made to create more event loops than allowed by `max_loops`. 3. Ensure that any attempt to get or set the event loop behaves as expected. Implementation Details: 1. Initialize your `LimitedEventLoopPolicy` with a parameter `max_loops` specifying the maximum number of event loops allowed. 2. Implement necessary methods to ensure the above constraints are met. Expected Function Signatures: ```python import asyncio class LimitedEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def __init__(self, max_loops: int): Initialize the LimitedEventLoopPolicy with a maximum number of event loops. :param max_loops: The maximum number of event loops that can be created. super().__init__() # Your initialization code here def new_event_loop(self) -> asyncio.AbstractEventLoop: Create and return a new event loop, ensuring the number of loops does not exceed the maximum. :return: a new event loop. :raises RuntimeError: if the number of event loops exceeds max_loops. # Your code here def get_event_loop(self) -> asyncio.AbstractEventLoop: Get the current event loop. :return: the current event loop. # Your code here def set_event_loop(self, loop: asyncio.AbstractEventLoop) -> None: Set the current event loop. :param loop: The event loop to set. # Your code here # Example Usage: policy = LimitedEventLoopPolicy(max_loops=3) asyncio.set_event_loop_policy(policy) # Creating event loops should follow the set constraints try: loop1 = policy.new_event_loop() loop2 = policy.new_event_loop() loop3 = policy.new_event_loop() loop4 = policy.new_event_loop() # This should raise a RuntimeError except RuntimeError as e: print(e) ``` Constraints: - You may not use any external libraries other than asyncio. - Your implementation should handle any edge cases appropriately, such as attempting to create more event loops than allowed or invalid operations on the event loop. Evaluation Criteria: - Correct implementation of `LimitedEventLoopPolicy`. - Adherence to constraints and correct handling of edge cases. - Clarity and readability of code. Note: You may include additional helper methods if necessary but ensure they are encapsulated within your policy class.","solution":"import asyncio class LimitedEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def __init__(self, max_loops: int): super().__init__() self.max_loops = max_loops self.current_loops = 0 def new_event_loop(self) -> asyncio.AbstractEventLoop: if self.current_loops >= self.max_loops: raise RuntimeError(\\"Maximum number of event loops reached\\") loop = super().new_event_loop() self.current_loops += 1 return loop def get_event_loop(self) -> asyncio.AbstractEventLoop: return super().get_event_loop() def set_event_loop(self, loop: asyncio.AbstractEventLoop) -> None: super().set_event_loop(loop)"},{"question":"**Objective**: Demonstrate your understanding of the `torch.special` module by implementing a function that utilizes multiple special functions to compute a specified mathematical expression. **Problem Statement**: Write a PyTorch function `compute_special_expression(tensor: torch.Tensor) -> torch.Tensor` that takes a 1D tensor of real numbers as input and computes the following expression for each element ( x ) in the tensor: [ f(x) = text{airy_ai}(x) + text{bessel_j0}(x) cdot exp(text{digamma}(x+1)) - text{erfc}(log1p(x)) cdot text{softmax}(x) ] # Function Signature ```python def compute_special_expression(tensor: torch.Tensor) -> torch.Tensor: pass ``` # Input - `tensor` (torch.Tensor): A 1D tensor of real numbers with shape `(n,)`. # Output - A tensor (torch.Tensor): A tensor of the same shape `(n,)`, where each element has been transformed according to the mathematical expression. # Constraints - You can assume all elements in the input tensor `tensor` are non-negative. - The function should handle tensors up to size ( 10^6 ) efficiently. # Example ```python import torch tensor = torch.tensor([1.0, 2.0, 3.0]) result = compute_special_expression(tensor) print(result) # Output tensor should have the same shape and contain the computed special function values. ``` # Performance Requirements Your implementation should be efficient and leverage PyTorch\'s vectorized operations to ensure high performance even for large tensors. # Assumptions - You may assume the input tensor contains valid real numbers (non-negative). - Utilize appropriate special functions from the `torch.special` module to compute each part of the expression. Implement the function `compute_special_expression` to complete this assessment.","solution":"import torch def compute_special_expression(tensor: torch.Tensor) -> torch.Tensor: Computes the special function expression: f(x) = airy_ai(x) + bessel_j0(x) * exp(digamma(x + 1)) - erfc(log1p(x)) * softmax(x) for each element x in the input tensor. airy_ai_values = torch.special.airy_ai(tensor) bessel_j0_values = torch.special.bessel_j0(tensor) digamma_values = torch.special.digamma(tensor + 1) exp_digamma_values = torch.exp(digamma_values) log1p_values = torch.log1p(tensor) erfc_values = torch.special.erfc(log1p_values) softmax_values = torch.nn.functional.softmax(tensor, dim=0) result = airy_ai_values + bessel_j0_values * exp_digamma_values - erfc_values * softmax_values return result"},{"question":"# Floating-Point Operations with Error Handling **Objective:** Implement a set of functions that mimic the functionality provided by the `PyFloat` object-related functions in Python\'s C API. Your task is to ensure correct conversions, type checking, and error handling while working with floating-point numbers. **Task:** 1. **Function 1: is_float_obj** - **Input:** - `value` (Any Python object) - **Output:** - `True` if the object is a float, otherwise `False`. - **Example:** ```python is_float_obj(3.14) => True is_float_obj(\\"3.14\\") => False ``` 2. **Function 2: float_from_str** - **Input:** - `s` (string) - **Output:** - Float object if the string represents a valid floating-point number, otherwise raise a `ValueError`. - **Example:** ```python float_from_str(\\"3.14\\") => 3.14 float_from_str(\\"abc\\") => raises ValueError ``` 3. **Function 3: float_from_double** - **Input:** - `d` (double/float) - **Output:** - Float object representing the input double. - **Example:** ```python float_from_double(3.14) => 3.14 ``` 4. **Function 4: double_from_float** - **Input:** - `f` (float) - **Output:** - Double representation of the float. Raise `TypeError` if the input is not a float. - **Example:** ```python double_from_float(3.14) => 3.14 double_from_float(\\"3.14\\") => raises TypeError ``` 5. **Function 5: get_float_info** - **Input:** None - **Output:** - Tuple containing the precision, maximum, and minimum values representable by a float. - **Example:** ```python get_float_info() => (precision, max_value, min_value) ``` **Constraints:** - You may use Python\'s built-in functions and standard library. - Ensure your functions handle errors gracefully and return meaningful error messages. **Performance Requirements:** - Your implementations should be efficient with a time complexity no worse than O(n) where applicable. **Note:** Use Python\'s native float and type-checking mechanisms to implement these functions. The precision and range values can be derived using Python\'s `sys.float_info`. **Example Implementation:** You are not required to provide an example implementation. However, your solution should be thoroughly tested to handle typical edge cases.","solution":"import sys def is_float_obj(value): Checks if the provided value is a float. return isinstance(value, float) def float_from_str(s): Converts a string to a float if possible, otherwise raises a ValueError. try: return float(s) except ValueError: raise ValueError(f\\"Cannot convert \'{s}\' to float\\") def float_from_double(d): Converts a double/float to a float object. if not isinstance(d, (float, int)): # Allow integers to be converted as well raise TypeError(f\\"Input value must be a double/float but got {type(d)}\\") return float(d) def double_from_float(f): Converts a float to its double representation. Raises TypeError if the input is not a float. if not isinstance(f, float): raise TypeError(f\\"Input value must be a float but got {type(f)}\\") return float(f) def get_float_info(): Returns a tuple containing the precision, maximum, and minimum values representable by a float. float_info = sys.float_info return (float_info.dig, float_info.max, float_info.min)"},{"question":"Implementing a Custom Attention Mechanism in PyTorch Objective: The objective of this exercise is to test your understanding of neural network attention mechanisms in PyTorch and your ability to implement a custom attention layer using PyTorch\'s experimental APIs. Problem Statement: You are required to implement a custom attention layer named `CustomAttention` using PyTorch, specifically leveraging the `torch.nn.attention.experimental` module. This attention layer will be used in a simple sequence-to-sequence (seq2seq) model decoder. Your implementation should encapsulate the core principles of attention mechanisms, calculating attention scores, and applying them to the decoder hidden states. Requirements: - Define a class `CustomAttention` that inherits from `torch.nn.Module`. - Your class should implement the following methods: - `__init__`: Initialize any required layers or variables. - `forward`: Implement the forward pass that computes the attention scores and outputs the context vector. Input and Output Formats: - The `forward` method should take the following inputs: - `query`: Tensor of shape `(batch_size, hidden_size)` â€“ the query vector (decoder hidden state). - `key`: Tensor of shape `(batch_size, seq_len, hidden_size)` â€“ the key vectors (encoder hidden states). - `value`: Tensor of shape `(batch_size, seq_len, hidden_size)` â€“ the value vectors (same as key vectors). - The `forward` method should output: - `context_vector`: Tensor of shape `(batch_size, hidden_size)` â€“ the context vector resulting from the attention mechanism. Constraints: - You must use the `torch.nn.attention.experimental` module where applicable. - The attention mechanism should compute attention scores using a dot-product or scaled dot-product form. - Performance is important, so the implementation should be optimized for batch processing. Example Usage: ```python import torch import torch.nn as nn class CustomAttention(nn.Module): def __init__(self, hidden_size): super(CustomAttention, self).__init__() # Initialize any required layers or parameters here def forward(self, query, key, value): # Implement the forward pass to compute attention scores and context vector pass # Example usage batch_size, seq_len, hidden_size = 32, 10, 256 query = torch.randn(batch_size, hidden_size) key = torch.randn(batch_size, seq_len, hidden_size) value = torch.randn(batch_size, seq_len, hidden_size) attention_layer = CustomAttention(hidden_size) context_vector = attention_layer(query, key, value) print(context_vector.shape) # Expected output: torch.Size([32, 256]) ``` Additional Instructions: - Ensure that your implementation is well-documented and comments are added to explain key parts of the code. - Test your implementation with different input sizes to verify its correctness.","solution":"import torch import torch.nn as nn class CustomAttention(nn.Module): def __init__(self, hidden_size): super(CustomAttention, self).__init__() self.hidden_size = hidden_size self.softmax = nn.Softmax(dim=-1) def forward(self, query, key, value): # Calculate attention scores scores = torch.bmm(key, query.unsqueeze(2)).squeeze(2) # (batch_size, seq_len) # Apply softmax to get attention weights attention_weights = self.softmax(scores) # (batch_size, seq_len) # Calculate context vector context_vector = torch.bmm(attention_weights.unsqueeze(1), value).squeeze(1) # (batch_size, hidden_size) return context_vector # Example usage batch_size, seq_len, hidden_size = 32, 10, 256 query = torch.randn(batch_size, hidden_size) key = torch.randn(batch_size, seq_len, hidden_size) value = torch.randn(batch_size, seq_len, hidden_size) attention_layer = CustomAttention(hidden_size) context_vector = attention_layer(query, key, value) print(context_vector.shape) # Expected output: torch.Size([32, 256])"},{"question":"Objective You are provided with a CSV file called `data.csv` containing time series data. Your task is to perform various operations using the pandas Series library to extract specific insights from this dataset. Instructions 1. **Read Data**: Load the CSV file into a pandas Series. 2. **Fill Missing Values**: Implement logic to fill in any missing values using forward fill method. 3. **Daily Returns**: Compute the daily returns based on the series. 4. **Moving Average**: Calculate the moving average over a window of 5 days. 5. **Statistics**: Extract key statistics from the series including mean, median, and standard deviation. 6. **Resampling**: Resample the series to obtain monthly average values. 7. **Plotting**: Plot the original series, daily returns, and moving average using pandas\' built-in plotting capabilities. Detailed Steps 1. **Load Data** ```python import pandas as pd # Load data series = pd.read_csv(\'data.csv\', parse_dates=[\'Date\'], index_col=\'Date\')[\'Value\'] ``` 2. **Fill Missing Values** Implement logic to fill in missing values using the forward fill method. ```python filled_series = series.ffill() ``` 3. **Daily Returns** Compute the daily returns based on the filled series. ```python daily_returns = filled_series.pct_change().dropna() ``` 4. **Moving Average** Calculate the moving average over a window of 5 days. ```python moving_average = filled_series.rolling(window=5).mean() ``` 5. **Statistics** Extract key statistics: ```python mean_val = filled_series.mean() median_val = filled_series.median() std_dev = filled_series.std() ``` 6. **Resampling** Resample the series to get monthly average values. ```python monthly_avg = filled_series.resample(\'M\').mean() ``` 7. **Plotting** Plot the original series, daily returns, and moving average. ```python import matplotlib.pyplot as plt plt.figure(figsize=(14,7)) plt.subplot(3, 1, 1) filled_series.plot(title=\'Original Series\') plt.subplot(3, 1, 2) daily_returns.plot(title=\'Daily Returns\') plt.subplot(3, 1, 3) moving_average.plot(title=\'5-Day Moving Average\') plt.tight_layout() plt.show() ``` Constraints - Assume the `data.csv` file has two columns: `Date` and `Value`. - The data has a daily frequency. Expected Output 1. The final script should be able to read the CSV file and perform all the operations listed above. 2. Output the computed statistics and plot the graphs. Bonus If time allows, add functionality to save the computed statistics and plots to files. ```python # Save statistics to a CSV file stats = {\'mean\': mean_val, \'median\': median_val, \'std_dev\': std_dev} stats_df = pd.DataFrame(stats, index=[0]) stats_df.to_csv(\'computed_statistics.csv\', index=False) # Save the plot plt.savefig(\'series_analysis.png\') ```","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_series_data(file_path): # Load data series = pd.read_csv(file_path, parse_dates=[\'Date\'], index_col=\'Date\')[\'Value\'] # Fill Missing Values filled_series = series.ffill() # Daily Returns daily_returns = filled_series.pct_change().dropna() # Moving Average moving_average = filled_series.rolling(window=5).mean() # Statistics mean_val = filled_series.mean() median_val = filled_series.median() std_dev = filled_series.std() # Resampling monthly_avg = filled_series.resample(\'M\').mean() # Plotting plt.figure(figsize=(14, 7)) plt.subplot(3, 1, 1) filled_series.plot(title=\'Original Series\') plt.subplot(3, 1, 2) daily_returns.plot(title=\'Daily Returns\') plt.subplot(3, 1, 3) moving_average.plot(title=\'5-Day Moving Average\') plt.tight_layout() plt.show() # Return statistics and plots data for testing return { \\"mean\\": mean_val, \\"median\\": median_val, \\"std_dev\\": std_dev, \\"monthly_avg\\": monthly_avg }"},{"question":"# Question: Implement a Shared Memory Based Producer-Consumer Model Objective: Design and implement a shared memory-based producer-consumer system using the `multiprocessing.shared_memory` module in Python. This system should include two types of processes: producers that generate data and consumers that process this data. Shared memory will be used to facilitate efficient data sharing between these processes. Requirements: 1. **Producer Processes**: - Generate sequences of integers from 1 to `N` (inclusive). - Write these integers to a shared memory list. 2. **Consumer Processes**: - Read data from the shared memory list. - Compute the square of each integer. - Write the squared values back to the shared memory list. 3. **Shared Memory List**: - Use the `ShareableList` class to create and manage a shared memory list. - Ensure safe access and modification of the list by multiple processes. Specifications: 1. You need to implement two main functions: ```python def producer(shared_list, start, end): Producer function that writes integers from start to end (inclusive) to the shared list. Args: shared_list (ShareableList): The shared memory list. start (int): The starting integer. end (int): The ending integer (inclusive). pass def consumer(shared_list, start, end): Consumer function that reads integers from start to end (inclusive) in the shared list, computes their squares, and writes the squared values back to the shared list. Args: shared_list (ShareableList): The shared memory list. start (int): The starting index in the shared list. end (int): The ending index in the shared list (inclusive). pass ``` 2. Implement the main process management to: - Create and initialize a shared memory list to hold integers. - Start multiple producer and consumer processes. - Ensure that the shared memory is properly cleaned up after use. 3. Constraints: - Assume `N = 100` for testing. - Utilize at least two producer processes and two consumer processes. 4. Performance: - Ensure efficient synchronization between producer and consumer processes without data corruption. - Proper cleanup of shared memory should be ensured using `close()` and `unlink()` methods. Example Usage: ```python from multiprocessing import Process from multiprocessing.shared_memory import ShareableList # Define producer and consumer functions here if __name__ == \\"__main__\\": N = 100 shared_list = ShareableList([None] * N) producers = [ Process(target=producer, args=(shared_list, i * (N // 2), (i + 1) * (N // 2) - 1)) for i in range(2) ] consumers = [ Process(target=consumer, args=(shared_list, i * (N // 2), (i + 1) * (N // 2) - 1)) for i in range(2) ] for p in producers: p.start() for p in producers: p.join() for c in consumers: c.start() for c in consumers: c.join() print(list(shared_list)) shared_list.shm.close() shared_list.shm.unlink() ``` Notes: - Ensure to handle any potential exceptions. - Ensure the shared list is initialized to the correct size before starting the producer processes.","solution":"from multiprocessing import Process from multiprocessing.shared_memory import ShareableList import time def producer(shared_list, start, end): Producer function that writes integers from start to end (inclusive) to the shared list. Args: shared_list (ShareableList): The shared memory list. start (int): The starting integer. end (int): The ending integer (inclusive). for i in range(start, end + 1): shared_list[i] = i + 1 time.sleep(0.01) # Simulating work by sleeping for a bit def consumer(shared_list, start, end): Consumer function that reads integers from start to end (inclusive) in the shared list, computes their squares, and writes the squared values back to the shared list. Args: shared_list (ShareableList): The shared memory list. start (int): The starting index in the shared list. end (int): The ending index in the shared list (inclusive). for i in range(start, end + 1): if shared_list[i] is not None: shared_list[i] = shared_list[i] ** 2 time.sleep(0.01) # Simulating work by sleeping for a bit if __name__ == \\"__main__\\": N = 100 shared_list = ShareableList([None] * N) producers = [ Process(target=producer, args=(shared_list, i * (N // 2), (i + 1) * (N // 2) - 1)) for i in range(2) ] consumers = [ Process(target=consumer, args=(shared_list, i * (N // 2), (i + 1) * (N // 2) - 1)) for i in range(2) ] for p in producers: p.start() for p in producers: p.join() for c in consumers: c.start() for c in consumers: c.join() print(list(shared_list)) shared_list.shm.close() shared_list.shm.unlink()"},{"question":"# **Python Programming Assessment Question** **File Management and Process Handling using the `os` Module** **Introduction**: You are required to implement a function that will create a backup of a specified file by copying it to a new directory. The function should first verify file existence, create a new directory if it doesn\'t exist, copy the file to the new directory, and log the process including the timestamp. Your task is to write a Python function `backup_file(file_path: str, backup_dir: str) -> str:` that accomplishes this. **Objective**: - Verify that the given file exists. - Create the backup directory if it doesn\'t already exist. - Copy the file into the backup directory. - Log the entire process including a timestamp. **Requirements**: 1. **Input**: - `file_path`: A string representing the path to the file that needs to be backed up. - `backup_dir`: A string representing the path to the directory where the backup will be stored. 2. **Output**: - Return a string message indicating the success or failure of the backup procedure. For instance, `\\"Backup completed successfully.\\"` or an appropriate error message. 3. **Constraints**: - Handle exceptions such as file not found, directories that cannot be created, etc. - Use `os` module functionalities where appropriate. - Assume all file paths provided are valid and in string format. 4. **Logging**: - Log the process of copying the file, including the start time and any errors encountered. Use the `os` module to access the current directory and timestamp. 5. **Operations Involved**: - Checking file existence - Directory creation - File copying - Error handling **Example Usage**: ```python file_path = \'/path/to/original/file.txt\' backup_dir = \'/path/to/backup/directory/\' result = backup_file(file_path, backup_dir) print(result) # Output: \\"Backup completed successfully.\\" or an error message. ``` **Structure of the Implementation**: 1. Function Definition: - Define the function `backup_file(file_path: str, backup_dir: str) -> str:`. 2. Implementation Details: - Check if the provided file exists using `os.path.exists`. - Ensure the backup directory exists using `os.makedirs` with `exist_ok=True`. - Copy the file to the backup directory using file operations. - Log the details including timestamps and return appropriate messages. 3. Error Handling: - Properly handle cases where the file or directory operations fail (e.g., file not found, permission errors). **Notes**: - Make sure to keep code readable and well-commented. - Test your function with various scenarios to ensure robustness. ```python import os import shutil from datetime import datetime def backup_file(file_path: str, backup_dir: str) -> str: try: # Log start time start_time = datetime.now() # Check if the file exists if not os.path.exists(file_path): return f\\"Error: File \'{file_path}\' does not exist.\\" # Create the backup directory if it does not exist os.makedirs(backup_dir, exist_ok=True) # Determine the filename and construct the new path file_name = os.path.basename(file_path) backup_file_path = os.path.join(backup_dir, file_name) # Copy the file to the backup directory shutil.copy2(file_path, backup_file_path) # Log completion time and return success message completion_time = datetime.now() duration = completion_time - start_time log_message = f\\"Backup completed successfully in {duration.total_seconds()} seconds.\\" return log_message except Exception as e: return f\\"An error occurred during backup: {e}\\" ``` --- Use the above template and complete the implementation to handle additional corner cases and improve error logging where necessary. Ensure your function behaves as expected by thoroughly testing using various scenarios.","solution":"import os import shutil from datetime import datetime def backup_file(file_path: str, backup_dir: str) -> str: Backs up the specified file to the given directory. Parameters: file_path (str): The path to the file to be backed up. backup_dir (str): The directory where the file should be backed up. Returns: str: A message indicating success or failure of the backup. try: # Log start time start_time = datetime.now() # Check if the file exists if not os.path.exists(file_path): return f\\"Error: File \'{file_path}\' does not exist.\\" # Create the backup directory if it does not exist os.makedirs(backup_dir, exist_ok=True) # Determine the filename and construct the new path file_name = os.path.basename(file_path) backup_file_path = os.path.join(backup_dir, file_name) # Copy the file to the backup directory shutil.copy2(file_path, backup_file_path) # Log completion time and return success message completion_time = datetime.now() duration = completion_time - start_time log_message = f\\"Backup completed successfully in {duration.total_seconds()} seconds.\\" return log_message except Exception as e: return f\\"An error occurred during backup: {e}\\""},{"question":"**Custom Data Structure Implementation in Python 3.10** **Objective:** Implement a custom data structure in Python 3.10 that meets specific requirements and demonstrates understanding of advanced class features and object management. **Problem Statement:** Design a class `CustomList` that behaves like a Python list with additional features: 1. **Initialization:** The class should be initialized either with no argument, a list of elements, or another instance of `CustomList`. 2. **Custom Add Operation:** Implement a method `add` that takes an element and adds it to the list, ensuring that: - If the element is already in the list, it should not add it but instead move it to the end. 3. **Dunder Methods:** - Implement `__iter__` and `__next__` to make `CustomList` iterable. - Implement `__getitem__` to allow indexing. - Implement `__len__` to return the size of the list. - Implement `__contains__` to support the `in` keyword. 4. **Garbage Collection:** - Implement support for cyclic garbage collection by defining the necessary methods and slots for the garbage collector. 5. **Performance Constraints:** - Ensure that all methods perform efficiently with a time complexity of O(1) for addition and O(n) for other operations at worst, where n is the number of elements in the list. **Input:** - An instance of `CustomList` initialized with zero or more elements. - Element(s) to add using the `add` method. **Output:** - The class should support standard list operations with the enhancements mentioned above. - After adding elements, the `add` method should maintain the desired list properties. **Example:** ```python # Example usage: cl = CustomList([1, 2, 3]) cl.add(2) # Moves 2 to the end: [1, 3, 2] cl.add(4) # Adds 4 to the end: [1, 3, 2, 4] assert len(cl) == 4 assert list(cl) == [1, 3, 2, 4] assert 3 in cl assert 5 not in cl ``` **Bonus:** - Implement a method `merge` that takes another `CustomList` and merges it into the current list, preserving the properties of `CustomList`. **Constraints/Limitations:** - You may assume that all elements added to the `CustomList` are hashable. Please provide the implementation in Python 3.10.","solution":"class CustomList: def __init__(self, initial_data=None): self._data = [] if isinstance(initial_data, CustomList): self._data.extend(initial_data._data) elif isinstance(initial_data, list): self._data.extend(initial_data) def add(self, element): if element in self._data: self._data.remove(element) self._data.append(element) def __iter__(self): self._index = 0 return self def __next__(self): if self._index < len(self._data): result = self._data[self._index] self._index += 1 return result else: raise StopIteration def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) def __contains__(self, element): return element in self._data def merge(self, other): if isinstance(other, CustomList): for el in other._data: self.add(el) def __del__(self): del self._data"},{"question":"# Bytes Object Manipulation You are given data to process efficiently using Python\'s bytes objects. Your task is to write a function `process_bytes_data` which takes as input multiple string elements, concatenates them while conforming to various constraints, and outputs the final bytes object. To solve this, follow these steps: 1. Convert the input list of strings into individual bytes objects. 2. Append all these bytes objects together into a single bytes object using efficient concatenation. 3. Ensure that if a string in the input is empty, it should be converted to an empty bytes object (`b\'\'`) and still be part of the final concatenated bytes object. Function Signature ```python def process_bytes_data(data: list[str]) -> bytes: ``` Input - `data`: A list of strings `data`, with length `n (1 <= n <= 1000)`. Each string contains printable ASCII characters. The strings may also be empty. Output - Return a single concatenated bytes object. Example ```python # Example 1 data = [\\"hello\\", \\"world\\"] print(process_bytes_data(data)) # Expected Output: b\'helloworld\' # Example 2 data = [\\"\\", \\"single\\", \\"word\\"] print(process_bytes_data(data)) # Expected Output: b\'singleword\' # Example 3 data = [\\"part1\\", \\"\\", \\"part3\\"] print(process_bytes_data(data)) # Expected Output: b\'part1part3\' ``` Constraints - The function should handle edge cases like empty strings and only rely on the bytes-specific operations to ensure understanding and efficiency. - Use of the functions specified in the provided documentation is strongly encouraged for creating and manipulating the bytes objects. Performance Requirements - Your function should run efficiently, even for the maximum input size.","solution":"def process_bytes_data(data: list[str]) -> bytes: Concatenates a list of strings into a single bytes object. Args: - data (list of str): A list of strings. Returns: - bytes: A concatenated bytes object. # Convert the list of strings into a list of bytes objects bytes_list = [str.encode(item) for item in data] # Concatenate all bytes objects into a single bytes object result_bytes = b\'\'.join(bytes_list) return result_bytes"},{"question":"# Objective To test your understanding of the permutation feature importance technique and its application using scikit-learn. # Problem Statement You are given a dataset and a corresponding predictive task. Your goal is to evaluate which features are most important to the performance of the trained predictive model using the permutation importance method. You should implement the following steps: 1. Load the dataset and split it into training and validation sets. 2. Train a specified model on the training set. 3. Compute the permutation feature importance on the validation set. 4. Identify and print the most important features along with their importance values and standard deviations. 5. (Bonus) Repeat the importance computation using multiple scoring metrics and compare the results. # Steps and Requirements 1. **Dataset**: Use the Diabetes dataset from scikit-learn. 2. **Model**: Train a `Ridge` regression model with `alpha=0.01`. 3. **Permutation Importance**: Use the following parameters: - `n_repeats`: 30 - `random_state`: 0 4. **Output**: - Print the name and importance of each feature whose mean importance value minus twice the standard deviation is greater than zero. - (Bonus) Calculate permutation importances using the following scoring metrics: `r2`, `neg_mean_absolute_percentage_error`, `neg_mean_squared_error`. # Expected Input and Output Formats Input: You do not need to take inputs from the user. Use the hardcoded parameters provided in the steps. Output: Print the feature importances in the following format: ``` <Feature Name> <Importance Value> +/- <Standard Deviation> ... ``` For the bonus part, print the importances for each metric separately. # Constraints - Ensure the training and validation split has a `random_state` of 0 for reproducibility. # Performance Requirements - The model training and computation should be performed efficiently within a reasonable time frame even for multiple repetitions. # Example ```python import numpy as np from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance # Load the diabetes dataset diabetes = load_diabetes() X_train, X_val, y_train, y_val = train_test_split( diabetes.data, diabetes.target, random_state=0 ) # Train the model model = Ridge(alpha=1e-2).fit(X_train, y_train) # Compute the permutation importance r = permutation_importance( model, X_val, y_val, n_repeats=30, random_state=0 ) # Print the most important features for i in r.importances_mean.argsort()[::-1]: if r.importances_mean[i] - 2 * r.importances_std[i] > 0: print(f\\"{diabetes.feature_names[i]:<8} {r.importances_mean[i]:.3f} +/- {r.importances_std[i]:.3f}\\") # Bonus: Multiple scoring metrics scoring = [\'r2\', \'neg_mean_absolute_percentage_error\', \'neg_mean_squared_error\'] r_multi = permutation_importance( model, X_val, y_val, n_repeats=30, random_state=0, scoring=scoring ) for metric in r_multi: print(f\\"n{metric}\\") r = r_multi[metric] for i in r.importances_mean.argsort()[::-1]: if r.importances_mean[i] - 2 * r.importances_std[i] > 0: print(f\\" {diabetes.feature_names[i]:<8} {r.importances_mean[i]:.3f} +/- {r.importances_std[i]:.3f}\\") ```","solution":"import numpy as np from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance def get_permutation_importance(): # Load the diabetes dataset diabetes = load_diabetes() X_train, X_val, y_train, y_val = train_test_split( diabetes.data, diabetes.target, random_state=0 ) # Train the model model = Ridge(alpha=1e-2).fit(X_train, y_train) # Compute the permutation importance r = permutation_importance( model, X_val, y_val, n_repeats=30, random_state=0 ) # Collect the important features important_features = [] for i in r.importances_mean.argsort()[::-1]: if r.importances_mean[i] - 2 * r.importances_std[i] > 0: important_features.append( (diabetes.feature_names[i], r.importances_mean[i], r.importances_std[i]) ) return important_features"},{"question":"# Problem Description You are tasked with creating an application that simulates a basic chat server using the asyncio low-level APIs. The server should be able to handle multiple clients simultaneously, echo back received messages, and log all messages to a file. # Requirements 1. Implement a `ChatProtocol` class that inherits from `asyncio.Protocol`. 2. Implement a separate logging mechanism that writes received messages to a given file. 3. Set up the server to use the `ChatProtocol` for communication with clients. # Specifications 1. **ChatProtocol Class**: - **Methods**: - `connection_made(transport)`: This method should store the transport and print a message indicating the connection was made. - `data_received(data)`: This method should decode the data to a string, echo the message back to the client, and log the message using the logging mechanism. - `connection_lost(exc)`: This method should print a message indicating the client has disconnected. 2. **Logging Mechanism**: - Write messages to a specified log file. - Ensure that each message includes a timestamp. 3. **Server**: - Use the `ChatProtocol` class with `loop.create_server`. - The server should listen on \'127.0.0.1\' and port `8888`. # Constraints 1. The server should handle at least 100 simultaneous client connections efficiently. 2. Implement appropriate error handling for client disconnections and other potential issues. # Input/Output - **Input**: No direct input to functions. Interaction occurs via socket connections to the server. - **Output**: Echoes back received messages to the clients and logs messages to a file. # Example Here is an example of how the server can be started: ```python import asyncio class ChatProtocol(asyncio.Protocol): def __init__(self, log_file): self.transport = None self.log_file = log_file def connection_made(self, transport): self.transport = transport print(\'Connection established.\') def data_received(self, data): message = data.decode() print(f\'Received message: {message}\') self.transport.write(data) # Echo back self.log_message(message) def connection_lost(self, exc): print(\'Connection closed.\') def log_message(self, message): with open(self.log_file, \\"a\\") as file: file.write(f\'{self.get_timestamp()} {message}n\') def get_timestamp(self): from datetime import datetime return datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') async def main(): loop = asyncio.get_running_loop() log_file = \'chat_log.txt\' server = await loop.create_server( lambda: ChatProtocol(log_file), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() asyncio.run(main()) ``` Implement the `ChatProtocol` class and set up the server as described. Ensure that messages are properly echoed back and logged with a timestamp in the specified log file.","solution":"import asyncio from datetime import datetime class ChatProtocol(asyncio.Protocol): def __init__(self, log_file): self.transport = None self.log_file = log_file def connection_made(self, transport): self.transport = transport print(\'Connection established.\') def data_received(self, data): message = data.decode() print(f\'Received message: {message}\') # Echo back the received message self.transport.write(data) # Log the received message self.log_message(message) def connection_lost(self, exc): print(\'Connection closed.\') def log_message(self, message): timestamp = self.get_timestamp() with open(self.log_file, \\"a\\") as file: file.write(f\'{timestamp} {message}n\') def get_timestamp(self): return datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') async def main(): loop = asyncio.get_running_loop() log_file = \'chat_log.txt\' server = await loop.create_server( lambda: ChatProtocol(log_file), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Objective: You will implement a Python script that modifies the terminal mode of a file descriptor to raw or cbreak mode using the `tty` module. This script should handle both modes and allow the user to specify the file descriptor and mode through command-line arguments. Task: Write a Python function `change_terminal_mode(fd: int, mode: str) -> None` that: 1. Changes the mode of the file descriptor `fd` based on the `mode` parameter. 2. Supports two modes: \\"raw\\" and \\"cbreak\\". 3. Uses the `tty` module for mode changes. 4. Handles invalid modes by printing an appropriate error message. Additionally, write a main script that: 1. Parses command-line arguments for the file descriptor and mode. 2. Calls the `change_terminal_mode` function with the parsed arguments. 3. Ensures proper handling of command-line input and error states such as invalid file descriptors or modes. Input: 1. The script will accept two command-line arguments: - `fd`: An integer representing the file descriptor. - `mode`: A string specifying the terminal mode (\\"raw\\" or \\"cbreak\\"). Example command-line usage: ```shell python change_terminal_mode.py 0 raw ``` Output: 1. The function should not return any values. 2. If an invalid mode is passed, print: \\"Error: Invalid mode. Use \'raw\' or \'cbreak\'.\\" Constraints: 1. The script should be executed on a Unix-like system where the `tty` module and `termios` are available. 2. The file descriptor should be a valid integer commonly used for stdin, stdout, or stderr (e.g., 0, 1, 2). Example Code: ```python import tty import termios import sys def change_terminal_mode(fd: int, mode: str) -> None: if mode == \'raw\': tty.setraw(fd) elif mode == \'cbreak\': tty.setcbreak(fd) else: print(\\"Error: Invalid mode. Use \'raw\' or \'cbreak\'.\\") if __name__ == \\"__main__\\": if len(sys.argv) != 3: print(\\"Usage: python change_terminal_mode.py <fd> <mode>\\") sys.exit(1) try: fd = int(sys.argv[1]) mode = sys.argv[2] change_terminal_mode(fd, mode) except ValueError: print(\\"Error: File descriptor must be an integer.\\") except Exception as e: print(f\\"An error occurred: {e}\\") ```","solution":"import tty import sys def change_terminal_mode(fd: int, mode: str) -> None: Changes the mode of the file descriptor `fd` to `raw` or `cbreak`. Args: fd (int): File descriptor. mode (str): Mode to set (\\"raw\\" or \\"cbreak\\"). Raises: ValueError: If an invalid mode is passed. if mode == \'raw\': tty.setraw(fd) elif mode == \'cbreak\': tty.setcbreak(fd) else: raise ValueError(\\"Invalid mode. Use \'raw\' or \'cbreak\'.\\") if __name__ == \\"__main__\\": if len(sys.argv) != 3: print(\\"Usage: python change_terminal_mode.py <fd> <mode>\\") sys.exit(1) try: fd = int(sys.argv[1]) mode = sys.argv[2] change_terminal_mode(fd, mode) except ValueError as ve: print(f\\"Error: {ve}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Objective**: Demonstrate your understanding of the `email.utils` module functions in Python by parsing, formatting, and encoding email-related data. **Question**: You are given a multi-line string representing an email message header. Your task is to implement a function `process_email_header(header: str) -> Dict[str, Any]` that performs the following: 1. Parse all email addresses from the \\"To\\", \\"Cc\\", and \\"From\\" fields. 2. Parse the date from the \\"Date\\" field and convert it into a `datetime` object (aware of its time zone). 3. Encode the subject line (given as `Subject` in the header) according to RFC 2231, assuming UTF-8 encoding. 4. Format the parsed date into an RFC 2822 compliant date string. Your function should return a dictionary with the following keys: - `\\"to\\"`: a list of tuples where each tuple contains the real name and email address from the \\"To\\" field. - `\\"cc\\"`: a list of tuples similar to \\"to\\" but for the \\"Cc\\" field. - `\\"from\\"`: a tuple containing the real name and email address in the \\"From\\" field. - `\\"date\\"`: an RFC 2822 formatted date string. - `\\"encoded_subject\\"`: the RFC 2231 encoded subject line. **Constraints**: - You can assume the header string will always include \\"To\\", \\"From\\", \\"Date\\", and \\"Subject\\". - \\"Cc\\" field may or may not be present. - If the input date string is invalid, raise a `ValueError`. **Example Input**: ```python header = To: John Doe <john@example.com>, Jane Smith <jane@example.com> From: Alice Brown <alice.brown@example.com> Date: Mon, 20 Nov 1995 19:12:08 -0500 Subject: Example Subject Line ``` **Example Output**: ```python { \\"to\\": [(\\"John Doe\\", \\"john@example.com\\"), (\\"Jane Smith\\", \\"jane@example.com\\")], \\"cc\\": [], \\"from\\": (\\"Alice Brown\\", \\"alice.brown@example.com\\"), \\"date\\": \\"Mon, 20 Nov 1995 19:12:08 -0500\\", \\"encoded_subject\\": \\"utf-8\'\'Example%20Subject%20Line\\" } ``` **Function Signature**: ```python from typing import Dict, Any def process_email_header(header: str) -> Dict[str, Any]: pass ``` **Notes**: - Use `email.utils.getaddresses` to parse the email addresses. - Use `email.utils.parsedate_to_datetime` to parse the date. - Use `email.utils.format_datetime` to format the date. - Use `email.utils.encode_rfc2231` for encoding the subject line.","solution":"from typing import Dict, Any import email.utils import datetime def process_email_header(header: str) -> Dict[str, Any]: Process the email header and return parsed values as per requirements. # Initialize the output structure result = { \\"to\\": [], \\"cc\\": [], \\"from\\": None, \\"date\\": None, \\"encoded_subject\\": None } # Parse header lines lines = header.splitlines() for line in lines: if line.startswith(\'To:\'): result[\'to\'] = email.utils.getaddresses([line[4:]]) elif line.startswith(\'Cc:\'): result[\'cc\'] = email.utils.getaddresses([line[4:]]) elif line.startswith(\'From:\'): result[\'from\'] = email.utils.getaddresses([line[6:]])[0] elif line.startswith(\'Date:\'): parsed_date = email.utils.parsedate_to_datetime(line[6:]) if parsed_date is None: raise ValueError(\\"Invalid date format\\") result[\'date\'] = email.utils.format_datetime(parsed_date) elif line.startswith(\'Subject:\'): subject = line[9:] result[\'encoded_subject\'] = email.utils.encode_rfc2231(subject, charset=\'utf-8\') # Return the structured result return result"},{"question":"# Seaborn Coding Assessment Question Objective Design and implement a Python function that uses Seaborn to create a customized bar plot. The function should load the appropriate dataset, generate a bar plot with specified configurations, and save the plot as an image file. Function Signature ```python def create_custom_bar_plot(filepath: str): Generate a custom bar plot using Seaborn objects and save it as an image file. Arguments: filepath : str : Path to save the generated plot image. Returns: None pass ``` Instructions 1. Load the `penguins` dataset from Seaborn. 2. Create a bar plot to visualize the body mass of penguins grouped by species and differentiated by the sex of the penguins. 3. Configure the plot to have: * Bars of different colors for male and female penguins. * Bars to be semi-transparent. * Error bars showing the standard deviation of the body mass. 4. Use the `Dodge` transformation to avoid overlapping of the bars. 5. Set distinct edge styles for different sexes. 6. Save the generated plot as an image file using the given filepath. Constraints 1. Ensure that the resulting plot is visually clear and informative. 2. Handle any potential exceptions that may arise from loading the dataset or saving the file. 3. Use the Seaborn objects API as demonstrated in the provided documentation. Example Usage ```python create_custom_bar_plot(\\"penguins_body_mass_plot.png\\") ``` Expected Output 1. A file named `penguins_body_mass_plot.png` should be created at the given filepath. 2. The image should display a bar plot as specified in the instructions. # Note You are encouraged to refer to the provided documentation snippets for the correct usage of the Seaborn objects API.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_bar_plot(filepath: str): Generate a custom bar plot using Seaborn objects and save it as an image file. Arguments: filepath : str : Path to save the generated plot image. Returns: None try: # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the bar plot bar_plot = sns.catplot( data=penguins, kind=\\"bar\\", x=\\"species\\", y=\\"body_mass_g\\", hue=\\"sex\\", ci=\\"sd\\", palette=\\"muted\\", alpha=0.6, height=6, aspect=1, dodge=True ) # Customize the plot bar_plot.set_axis_labels(\\"Species\\", \\"Body Mass (g)\\") bar_plot.legend.set_title(\\"Sex\\") # Saving the plot as an image file plt.savefig(filepath) plt.close() except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Advanced Regular Expressions with Python `re` Module Problem Statement: You are tasked with writing a Python function `validate_and_extract` that takes a string containing multiple user data entries and performs the following tasks: 1. **Validate** that each entry has the following format: - **Name**: One or more uppercase and lowercase letters (e.g., Alice, JohnDoe). - **Email**: This format should be a standard email address, allowing both standard characters and special characters in the local part, followed by an \\"@\\" sign and a domain. - **Age**: An integer between 18 and 99. - **Date of Birth (DOB)**: Should follow the format \\"DD-MM-YYYY\\". 2. **Extract** and return valid entries in a structured format. A valid entry will look like this: ``` Name: JohnDoe; Email: john.doe@example.com; Age: 25; DOB: 15-03-1996 ``` You need to ensure the entire entry matches exactly with the pattern described, including handling specifics of the data formatting. Input: - A single string containing multiple entries separated by newline characters `n`. Output: - A list of dictionaries, where each dictionary represents a valid entry with its components (\'Name\', \'Email\', \'Age\', \'DOB\'). Implementation Constraints: - Use Python\'s `re` module to define and work with the regular expressions. - Make use of named groups to extract data. - Write efficient regular expression patterns that ensure complete data validation with respect to the outlined rules. Example: ```python input_data = Name: Alice; Email: alice@example.com; Age: 30; DOB: 05-08-1991 Name: invalidemail@example; Email: invalidemailatexample.com; Age: 20; DOB: 01-01-2000 Name: Bob; Email: bob@example.com; Age: 40; DOB: 28-12-1980 output = validate_and_extract(input_data) print(output) # Expected output: # [ # {\'Name\': \'Alice\', \'Email\': \'alice@example.com\', \'Age\': \'30\', \'DOB\': \'05-08-1991\'}, # {\'Name\': \'Bob\', \'Email\': \'bob@example.com\', \'Age\': \'40\', \'DOB\': \'28-12-1980\'} # ] ``` # Function Signature ```python def validate_and_extract(data: str) -> List[Dict[str, str]]: pass ``` # Hints: - Use `\'(?P<name>[A-Za-z]+)\'` to match the `Name`. - For the `Email`, use a pattern accounting for special characters in the local part. - Use `b(1[89]|[2-9][0-9])b` to restrict the `Age` between 18 and 99. - Match the `DOB` using `\'bd{2}-d{2}-d{4}b\'` and ensure date format validation.","solution":"import re from typing import List, Dict def validate_and_extract(data: str) -> List[Dict[str, str]]: Validate and extract user data entries from the input string. Parameters: data (str): A string containing multiple entries separated by newline characters. Returns: List[Dict[str, str]]: A list of dictionaries, each representing a valid entry. pattern = re.compile( r\'Name:s*(?P<Name>[A-Za-z]+); \' r\'Email:s*(?P<Email>[w.-]+@[w.-]+.w+); \' r\'Age:s*(?P<Age>1[89]|[2-9][0-9]); \' r\'DOB:s*(?P<DOB>d{2}-d{2}-d{4})\' ) valid_entries = [] for match in pattern.finditer(data): entry = match.groupdict() valid_entries.append(entry) return valid_entries"},{"question":"Task Create a function `pack_and_unpack_complex_structure(data: list) -> list` that: 1. **Packs** a list of dictionaries using the `Packer` class from the `xdrlib` module. 2. **Unpacks** the packed data back into the original list of dictionaries using the `Unpacker` class from the same module. 3. Ensures data integrity by verifying that the unpacked data matches the original input. Implementation Details - Each dictionary in the list will have the following structure: ```python {\\"id\\": int, \\"name\\": str, \\"scores\\": list of int} ``` - The output after unpacking should be identical to the input. Input and Output Format - **Input:** - `data`: A list of dictionaries, where each dictionary contains: - \\"id\\": an integer. - \\"name\\": a string. - \\"scores\\": a list of integers. Example: ```python [ {\\"id\\": 1, \\"name\\": \\"Alice\\", \\"scores\\": [95, 85, 90]}, {\\"id\\": 2, \\"name\\": \\"Bob\\", \\"scores\\": [88, 79, 92]} ] ``` - **Output:** - A list of dictionaries identical to the input list of dictionaries. Constraints - The length of the `name` string in each dictionary does not exceed 100 characters. - The list of `scores` contains at most 10 integers. - The integer values for `id` and items in `scores` are non-negative. Example Given the input ```python input_data = [ {\\"id\\": 1, \\"name\\": \\"Alice\\", \\"scores\\": [95, 85, 90]}, {\\"id\\": 2, \\"name\\": \\"Bob\\", \\"scores\\": [88, 79, 92]} ] ``` After packing and unpacking, the function should return: ```python [ {\\"id\\": 1, \\"name\\": \\"Alice\\", \\"scores\\": [95, 85, 90]}, {\\"id\\": 2, \\"name\\": \\"Bob\\", \\"scores\\": [88, 79, 92]} ] ``` Additional Information To solve this problem, you need to implement the following steps: 1. Utilize `Packer` methods to pack the data. 2. Store the packed data as a buffer. 3. Use `Unpacker` methods to unpack the data from the buffer. 4. Ensure that the unpacked data matches the original data. Function Signature ```python def pack_and_unpack_complex_structure(data: list) -> list: pass ```","solution":"import xdrlib def pack_and_unpack_complex_structure(data: list) -> list: packer = xdrlib.Packer() # Packing the data packer.pack_int(len(data)) for item in data: packer.pack_int(item[\\"id\\"]) packer.pack_string(item[\\"name\\"].encode(\'utf-8\')) packer.pack_int(len(item[\\"scores\\"])) for score in item[\\"scores\\"]: packer.pack_int(score) packed_data = packer.get_buffer() # Unpacking the data unpacker = xdrlib.Unpacker(packed_data) unpacked_data = [] num_items = unpacker.unpack_int() for _ in range(num_items): item = {} item[\\"id\\"] = unpacker.unpack_int() item[\\"name\\"] = unpacker.unpack_string().decode(\'utf-8\') scores_len = unpacker.unpack_int() item[\\"scores\\"] = [unpacker.unpack_int() for _ in range(scores_len)] unpacked_data.append(item) return unpacked_data"},{"question":"# Question: Timer and Report Generator **Objective**: Implement a timer class that can be used to measure the time duration of multiple tasks and generate a report with the timings for each task. **Description**: You are required to implement a Python class named `TaskTimer`. This class should be able to start and stop timers for multiple tasks, record the duration of each task, and generate a report that lists all tasks with their respective durations. # Class Definition: ```python class TaskTimer: def __init__(self): # Initialize the necessary data structures pass def start_task(self, task_name: str) -> None: Start timing a task. :param task_name: The name of the task to start timing. pass def stop_task(self, task_name: str) -> None: Stop timing a task and record its duration. :param task_name: The name of the task to stop timing. pass def generate_report(self) -> str: Generate a report with the duration of all tasks. :return: A string report listing all tasks and their durations. pass ``` # Constraints: 1. The time should be recorded using the highest available resolution (consider using `time.perf_counter()` or `time.perf_counter_ns()`). 2. The `start_task` method starts timing for a specific task. If the task is already running, it should raise an exception. 3. The `stop_task` method stops timing for a specific task and records its duration. If the task was not started, it should raise an exception. 4. The `generate_report` method should return a string that lists each task\'s name and duration in seconds. # Example Usage: ```python timer = TaskTimer() timer.start_task(\\"task1\\") # Perform some work here timer.stop_task(\\"task1\\") timer.start_task(\\"task2\\") # Perform some work here timer.stop_task(\\"task2\\") report = timer.generate_report() print(report) # Example output: # Task: task1, Duration: 2.345 seconds # Task: task2, Duration: 1.234 seconds ``` # Performance Requirements: The implementation should efficiently handle starting and stopping tasks and generating the report, even for a large number of tasks. # Additional Notes: - Consider edge cases such as trying to stop a task that hasn\'t been started, and trying to start a task that is already running. - Use appropriate exceptions to handle errors gracefully.","solution":"import time class TaskTimer: def __init__(self): # Stores the start time of running tasks self.running_tasks = {} # Stores the accumulated duration of completed tasks self.task_durations = {} def start_task(self, task_name: str) -> None: if task_name in self.running_tasks: raise Exception(f\\"Task \'{task_name}\' is already running.\\") self.running_tasks[task_name] = time.perf_counter() def stop_task(self, task_name: str) -> None: if task_name not in self.running_tasks: raise Exception(f\\"Task \'{task_name}\' was not started.\\") start_time = self.running_tasks.pop(task_name) duration = time.perf_counter() - start_time if task_name in self.task_durations: self.task_durations[task_name] += duration else: self.task_durations[task_name] = duration def generate_report(self) -> str: report_lines = [] for task_name, duration in self.task_durations.items(): report_lines.append(f\\"Task: {task_name}, Duration: {duration:.3f} seconds\\") return \\"n\\".join(report_lines)"},{"question":"**Title: System Resource Monitor** **Objective:** Write a Python function that monitors the system\'s CPU and memory usage over a specified period and alerts if usage crosses a certain threshold. **Description:** Create a function called `monitor_system_resources` that takes three parameters: 1. `duration` (int): The total time in seconds for which monitoring needs to be done. 2. `interval` (int): The time interval in seconds between each check. 3. `threshold` (dict): A dictionary with keys `\'cpu\'` and `\'memory\'`, representing usage percentage thresholds for CPU and memory respectively. The function should continuously monitor CPU and memory usage at the specified intervals for the given duration. If either CPU or memory usage exceeds their respective thresholds at any point, the function should print a warning message indicating which resource crossed its threshold and what the actual usage was at that moment. **Requirements:** 1. Use `psutil` library for fetching system resource usage information. If `psutil` is not installed, you can install it using `pip install psutil`. 2. The function should be robust and handle unexpected situations gracefully (e.g., invalid input values). 3. Ensure that your function runs efficiently and does not unnecessarily consume system resources during its operation. **Constraints:** - `duration` and `interval` values should be positive integers. - The `threshold` dictionary should only contain `\'cpu\'` and `\'memory\'` keys with values between 0 and 100. **Input:** - `duration`: 60 (int) - `interval`: 5 (int) - `threshold`: {\'cpu\': 75, \'memory\': 80} (dict) **Output:** - Print appropriate warning messages if thresholds are exceeded. **Example:** ```python import psutil import time def monitor_system_resources(duration, interval, threshold): if duration <= 0 or interval <= 0 or not isinstance(threshold, dict): raise ValueError(\'Invalid input values.\') if \'cpu\' not in threshold or \'memory\' not in threshold: raise ValueError(\'Threshold dictionary must contain \\"cpu\\" and \\"memory\\" keys.\') cpu_threshold = threshold[\'cpu\'] memory_threshold = threshold[\'memory\'] if not (0 <= cpu_threshold <= 100 and 0 <= memory_threshold <= 100): raise ValueError(\'Threshold values must be between 0 and 100.\') end_time = time.time() + duration while time.time() <= end_time: cpu_usage = psutil.cpu_percent(interval=1) memory_info = psutil.virtual_memory() memory_usage = memory_info.percent if cpu_usage > cpu_threshold: print(f\\"WARNING: CPU usage exceeded threshold! Usage: {cpu_usage}%\\") if memory_usage > memory_threshold: print(f\\"WARNING: Memory usage exceeded threshold! Usage: {memory_usage}%\\") time.sleep(interval - 1) # Example usage: monitor_system_resources(60, 5, {\'cpu\': 75, \'memory\': 80}) ```","solution":"import psutil import time def monitor_system_resources(duration, interval, threshold): if duration <= 0 or interval <= 0 or not isinstance(threshold, dict): raise ValueError(\'Invalid input values.\') if \'cpu\' not in threshold or \'memory\' not in threshold: raise ValueError(\'Threshold dictionary must contain \\"cpu\\" and \\"memory\\" keys.\') cpu_threshold = threshold[\'cpu\'] memory_threshold = threshold[\'memory\'] if not (0 <= cpu_threshold <= 100 and 0 <= memory_threshold <= 100): raise ValueError(\'Threshold values must be between 0 and 100.\') end_time = time.time() + duration while time.time() <= end_time: cpu_usage = psutil.cpu_percent(interval=1) memory_info = psutil.virtual_memory() memory_usage = memory_info.percent if cpu_usage > cpu_threshold: print(f\\"WARNING: CPU usage exceeded threshold! Usage: {cpu_usage}%\\") if memory_usage > memory_threshold: print(f\\"WARNING: Memory usage exceeded threshold! Usage: {memory_usage}%\\") time.sleep(interval - 1)"},{"question":"Coding Assessment Question You have been provided the documentation of the `email.charset` module. Using the `Charset` class and its functionalities, write a Python function `email_header_and_body_encoder(input_charset: str, header: str, body: str) -> Tuple[str, str]` that: 1. Takes three inputs: - `input_charset`: The character set of the input text as a string. - `header`: Email header content as a string. - `body`: Email body content as a string. 2. Returns a tuple containing two elements: - The encoded email header string. - The encoded email body string. Your function should perform the following tasks: 1. Initialize a `Charset` object with the `input_charset`. 2. Encode the `header` using `Charset.header_encode()`. 3. Encode the `body` using `Charset.body_encode()`. 4. Return both encoded strings in a tuple. # Input and Output Format - **Input**: - `input_charset`: A string representing the character set, e.g., `\'iso-8859-1\'`. - `header`: A string representing the email header content. - `body`: A string representing the email body content. - **Output**: - A tuple containing two strings: - Encoded email header. - Encoded email body. # Constraints - The `input_charset` can be any valid character set supported by the module. - The header and body strings can contain any type of text. ```python def email_header_and_body_encoder(input_charset: str, header: str, body: str) -> Tuple[str, str]: # Implement your solution pass # Example Usage: # input_charset = \'iso-8859-1\' # header = \'Subject: Test Email\' # body = \'Hello, this is the body of the email.\' # Should return encoded header and body based on charset. ``` # Additional Guidelines 1. Utilize the `email.charset.Charset` class and its methods properly. 2. Consider edge cases such as unsupported character sets. Implement the function in the provided code stub.","solution":"from email.charset import Charset def email_header_and_body_encoder(input_charset: str, header: str, body: str) -> tuple: Encodes the given email header and body using the specified character set. :param input_charset: The character set of the input text. :param header: Email header content. :param body: Email body content. :return: A tuple containing the encoded email header and body. charset = Charset(input_charset) encoded_header = charset.header_encode(header) encoded_body = charset.body_encode(body) return encoded_header, encoded_body"},{"question":"# PyTorch Coding Assessment Question Objective: Implement a function using PyTorch\'s `torch.fft` module to perform a multi-dimensional Fast Fourier Transform (FFT) on a given tensor, apply some modifications in the frequency domain, and then convert it back to the spatial domain. Validate the performance and correctness of the implemented function. Problem Statement: You are provided with a 3-dimensional tensor representing volumetric data (such as medical imaging data). Your task is to: 1. Perform a 3-dimensional FFT on the tensor. 2. Zero out all the high-frequency components (apply a simple low-pass filter). Assume components with a frequency magnitude greater than a threshold should be set to zero. 3. Perform the inverse 3-dimensional FFT to transform the data back to the spatial domain. 4. Verify that the transformed data maintains certain properties from the original data. Implement the function `low_pass_3d_fft(tensor, threshold)` with the following specifications: Function Signature: ```python import torch def low_pass_3d_fft(tensor: torch.Tensor, threshold: float) -> torch.Tensor: pass ``` Input: - `tensor` (torch.Tensor): A 3-dimensional PyTorch tensor of shape `(D, H, W)` with complex64 or complex128 data type. - `threshold` (float): A positive float value representing the frequency magnitude threshold for the low-pass filter. Output: - Returns a 3-dimensional PyTorch tensor of the same shape and data type as the input. Constraints: - The input tensor must be of complex data type (i.e., `complex64` or `complex128`). - The dimensions of the tensor `(D, H, W)` are all 2 or greater. - You must use the functions provided in the `torch.fft` module to implement the solution. Example: ```python import torch # Create a sample 3D tensor tensor = torch.rand((4, 4, 4), dtype=torch.complex64) # Define threshold for low-pass filtering threshold = 1.0 # Apply the low-pass filter using FFT filtered_tensor = low_pass_3d_fft(tensor, threshold) print(filtered_tensor) ``` Notes: 1. You can use the `torch.fft.fftn` and `torch.fft.ifftn` functions to perform the n-dimensional FFT and inverse FFT. 2. To determine the magnitude of frequencies and apply the threshold, consider using `torch.fft.fftfreq` to generate the frequency bins. 3. Ensure that the function maintains the shape and data type of the input tensor after processing. 4. Test the function with various sizes of 3-dimensional tensors and different threshold values to validate its correctness and performance.","solution":"import torch def low_pass_3d_fft(tensor: torch.Tensor, threshold: float) -> torch.Tensor: assert tensor.dim() == 3, \\"Input tensor must be 3-dimensional\\" assert tensor.is_complex(), \\"Input tensor must be of complex type\\" # Perform 3-dimensional FFT fft_tensor = torch.fft.fftn(tensor) # Generate frequency coordinates D, H, W = tensor.shape freq_x = torch.fft.fftfreq(D) freq_y = torch.fft.fftfreq(H) freq_z = torch.fft.fftfreq(W) # Create a meshgrid of frequency coordinates meshgrid = torch.meshgrid(freq_x, freq_y, freq_z, indexing=\'ij\') # Compute the magnitude of the frequency freq_magnitude = torch.sqrt(meshgrid[0]**2 + meshgrid[1]**2 + meshgrid[2]**2) # Apply the low-pass filter by zeroing out high frequencies fft_tensor[freq_magnitude > threshold] = 0 # Perform the inverse 3-dimensional FFT to convert back to spatial domain filtered_tensor = torch.fft.ifftn(fft_tensor) return filtered_tensor"},{"question":"**Binary Data Encoding and Manipulation** # Background You are given the task of handling both binary data and text encodings for a data transmission system. The system requires packing data into binary formats and then encoding/decoding text data for proper communication between different components. # Requirements 1. **Function 1: `pack_data`** - **Purpose**: To pack integer and floating-point data into a binary format. - **Inputs**: - `integers`: A list of integers. - `floats`: A list of floating-point numbers. - **Outputs**: A single bytes object containing the packed data. - **Details**: The integers should be packed as 4-byte signed integers (`\'i\'`), and the floating-point numbers as 8-byte double precision floats (`\'d\'`). All data should be packed in native byte order, standard size, and alignment. 2. **Function 2: `encode_data`** - **Purpose**: To encode a given string into a specified encoding. - **Inputs**: - `input_string`: A string that needs to be encoded. - `encoding`: A string representing the desired encoding format (e.g., `\'utf-8\'`, `\'utf-16\'`, `\'ascii\'`). - **Outputs**: A bytes object of the encoded string. - **Constraints**: Handle encoding errors by replacing problematic characters (use `\'replace\'` error handler). 3. **Function 3: `decode_data`** - **Purpose**: To decode an encoded bytes object back to a string. - **Inputs**: - `encoded_data`: A bytes object that needs to be decoded. - `encoding`: A string representing the encoding format of the bytes object (e.g., `\'utf-8\'`, `\'utf-16\'`, `\'ascii\'`). - **Outputs**: A decoded string. - **Constraints**: Handle decoding errors by replacing problematic characters (use `\'replace\'` error handler). # Example ```python # Example for pack_data integers = [1, 2, 3] floats = [1.0, 2.0, 3.0] packed_data = pack_data(integers, floats) print(packed_data) # Output will be a bytes object # Example for encode_data string = \\"Hello, World!\\" encoding = \\"utf-8\\" encoded_string = encode_data(string, encoding) print(encoded_string) # Output: b\'Hello, World!\' # Example for decode_data encoded_string = b\'Hello, World!\' encoding = \\"utf-8\\" decoded_string = decode_data(encoded_string, encoding) print(decoded_string) # Output: \\"Hello, World!\\" ``` # Functions Signature ```python def pack_data(integers: list, floats: list) -> bytes: pass def encode_data(input_string: str, encoding: str) -> bytes: pass def decode_data(encoded_data: bytes, encoding: str) -> str: pass ``` **Note**: Make sure to handle errors gracefully and test your solution thoroughly.","solution":"import struct def pack_data(integers: list, floats: list) -> bytes: Packs a list of integers and floats into a binary format. :param integers: List of integers to pack. :param floats: List of floats to pack. :return: A bytes object containing the packed data. packed_data = bytearray() for integer in integers: packed_data.extend(struct.pack(\'i\', integer)) for float_num in floats: packed_data.extend(struct.pack(\'d\', float_num)) return bytes(packed_data) def encode_data(input_string: str, encoding: str) -> bytes: Encodes a given string into the specified encoding. :param input_string: The string to encode. :param encoding: The encoding format (e.g., \'utf-8\', \'utf-16\', \'ascii\'). :return: A bytes object of the encoded string. return input_string.encode(encoding, errors=\'replace\') def decode_data(encoded_data: bytes, encoding: str) -> str: Decodes an encoded bytes object back to a string. :param encoded_data: The bytes object to decode. :param encoding: The encoding format (e.g., \'utf-8\', \'utf-16\', \'ascii\'). :return: The decoded string. return encoded_data.decode(encoding, errors=\'replace\')"},{"question":"**Objective**: Implement a custom PyTorch autograd Function that calculates the sine of the input tensor using both forward and backward mode automatic differentiation. Utilize profiling to analyze the performance of your implementation. Requirements 1. **Function Implementation**: - Create a custom autograd Function called `SineFunction`. - Implement the `forward` and `backward` methods. - Implement the `jvp` method for the custom forward-mode automatic differentiation. 2. **Profiling**: - Use PyTorch\'s autograd profiler to measure the performance of your `SineFunction` during both the forward and backward passes. 3. **Validation Script**: - Write a script that: - Creates a tensor with `requires_grad=True`. - Applies `SineFunction` to the tensor. - Computes the gradients by running the backward pass. - Uses PyTorch\'s autograd profiler to capture and display performance metrics. Input The input tensor can be any tensor with `requires_grad=True` that is passed to your `SineFunction`. Output Your `SineFunction` should correctly compute: - The sine of the input tensor in the forward pass. - The correct gradients in the backward pass. - The correct Jacobian-vector product in the `jvp` method. The profiling output should include: - Time taken for forward pass. - Time taken for backward pass. Constraints - Do not use any external libraries other than PyTorch. - Ensure that your implementation is compatible with both CPU and GPU tensors. Example Usage ```python import torch from torch.autograd import Function from torch.autograd.profiler import profile class SineFunction(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return input.sin() @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * input.cos() return grad_input @staticmethod def jvp(ctx, input, tangents): result = input.sin() tangent_result = tangents * input.cos() return result, tangent_result # Example of using SineFunction x = torch.tensor([0.0, torch.pi / 2, torch.pi], requires_grad=True) # Profiling and applying SineFunction with profile() as prof: y = SineFunction.apply(x) y.sum().backward() print(prof.key_averages().table(sort_by=\\"cpu_time_total\\")) ``` **Note**: The profiling results will vary based on the hardware and environment, so ensure you understand how to interpret them.","solution":"import torch from torch.autograd import Function from torch.autograd.profiler import profile class SineFunction(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return input.sin() @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * input.cos() return grad_input @staticmethod def jvp(ctx, input, tangents): result = input.sin() tangent_result = tangents * input.cos() return result, tangent_result # Example of using SineFunction x = torch.tensor([0.0, torch.pi / 2, torch.pi], requires_grad=True) # Profiling and applying SineFunction with profile() as prof: y = SineFunction.apply(x) y.sum().backward() print(prof.key_averages().table(sort_by=\\"cpu_time_total\\"))"},{"question":"# Advanced Coding Assessment: Implementing a Compilation Automation Objective: You are tasked with creating a utility function that utilizes the `py_compile` module to compile multiple Python source files into byte-code. This function should handle errors and optimization levels based on user specifications. Function Signature: ```python def compile_python_files(files: list, doraise: bool = False, quiet: int = 0, optimize: int = -1) -> dict: Compiles multiple Python source files into byte-code. Args: files (list): List of file paths of Python source files to be compiled. doraise (bool): Flag indicating whether to raise an exception on error. quiet (int): Level of quietness; 0 or 1 to print error, 2 to suppress errors completely. optimize (int): Optimization level for the compilation; -1, 0, 1, or 2. Returns: dict: A dictionary containing the statuses of compilation for each file. Key: File path Value: Path to the compiled byte-code file or error message. pass ``` Input: - `files`: List of absolute or relative paths to Python source files (e.g., `[\\"/path/to/file1.py\\", \\"/another/path/file2.py\\"]`). - `doraise`: A boolean flag to indicate whether `PyCompileError` should be raised on compilation failure. - `quiet`: An integer (0, 1 or 2) specifying the error handling verbosity. - `optimize`: An integer (-1, 0, 1, 2) specifying the optimization level for byte-code generation. Output: - Returns a dictionary with each file path as the key and the resulting compiled file path or error message as the value. Constraints: - All paths in the `files` list must be valid and accessible. - The function should handle typical file read/write permissions and non-regular file issues gracefully. - Performance considerations: Efficiently handle a large number of files and ensure minimal delay in compiling. Example Usage: ```python result = compile_python_files( files=[\\"script1.py\\", \\"script2.py\\"], doraise=True, quiet=1, optimize=2 ) print(result) # Possible output: # {\'script1.py\': \'script1.pyc\', \'script2.py\': \'Error: <specific error details>\'} ``` Task: Implement the function `compile_python_files` using the py_compile module. Ensure you test various cases such as successful compilation, non-existent files, read/write permission issues, and different optimization levels.","solution":"import py_compile import os def compile_python_files(files: list, doraise: bool = False, quiet: int = 0, optimize: int = -1) -> dict: Compiles multiple Python source files into byte-code. Args: files (list): List of file paths of Python source files to be compiled. doraise (bool): Flag indicating whether to raise an exception on error. quiet (int): Level of quietness; 0 or 1 to print error, 2 to suppress errors completely. optimize (int): Optimization level for the compilation; -1, 0, 1, or 2. Returns: dict: A dictionary containing the statuses of compilation for each file. Key: File path Value: Path to the compiled byte-code file or error message. result = {} for file in files: try: compiled_path = py_compile.compile(file, doraise=doraise, optimize=optimize) result[file] = compiled_path if compiled_path else \\"Error: Compilation resulted in no byte-code\\" except py_compile.PyCompileError as e: if quiet < 2: print(f\\"Error compiling {file}: {str(e)}\\") if doraise: raise result[file] = f\\"Error: {str(e)}\\" except Exception as e: if quiet < 2: print(f\\"Unexpected error compiling {file}: {str(e)}\\") result[file] = f\\"Error: {str(e)}\\" return result"},{"question":"# Intermediate Coding Assessment Question **Understanding Fractions using Python\'s fractions module** As an experienced Python developer, your task is to implement a utility function that manipulates fractions and performs specific operations. You need to demonstrate your understanding of the `fractions.Fraction` class provided by Python\'s `fractions` module. Problem Description Write a function `fraction_operations` that accepts a list of fraction representations in various formats (integers, floats, decimals, and strings) and a maximum denominator value. Perform the following operations: 1. **Create** a list of Fraction instances from the input list. 2. **Normalize** each fraction by using the `limit_denominator()` method with the provided maximum denominator. 3. **Calculate** the sum of all fractions in the list. 4. **Return** a tuple containing: - The list of normalized fractions. - The sum of these fractions as a Fraction instance. - The sum of these fractions as a tuple of integers (numerator, denominator) using the `as_integer_ratio()` method. Input - `fractions_list` (list): A list where each element can be one of the following: - An integer (e.g., `5`) - A float (e.g., `3.14`) - A string representing a fraction or a float (e.g., `\'3/4\'`, `\'0.5\'`, `\'2e-3\'`) - `max_denominator` (int): A maximum denominator value for fraction normalization. Output - A tuple containing: - A list of normalized `Fraction` instances. - A `Fraction` instance representing the sum of all fractions. - A tuple of two integers representing the numerator and denominator of the sum in the lowest terms. Example ```python from decimal import Decimal def fraction_operations(fractions_list, max_denominator): # Implement this function # Example usage: fractions_list = [5, 3.14, \'3/4\', \'0.5\', Decimal(\'7.35\'), \'2e-3\'] max_denominator = 1000 result = fraction_operations(fractions_list, max_denominator) print(result) # Expected output: # ([ # Fraction(5, 1), # Fraction(157, 50), # Fraction(3, 4), # Fraction(1, 2), # Fraction(735, 100), # Fraction(1, 500) # ], # Fraction(2158, 125), # (2158, 125) # ) ``` Constraints - Assume the input list will always be valid and contain proper integer, float, Decimal, or string representations of numbers. - `max_denominator` will always be a positive integer. **Note:** You should not import any additional libraries other than `fractions` and `decimal`.","solution":"from fractions import Fraction from decimal import Decimal def fraction_operations(fractions_list, max_denominator): Takes a list of fraction representations and a maximum denominator. Returns a tuple containing a list of normalized Fraction instances, the sum of these fractions as a Fraction instance, and the sum as a tuple of integers. fractional_values = [] # Convert all items into Fraction instances for item in fractions_list: if isinstance(item, int): fractional_values.append(Fraction(item)) elif isinstance(item, float): fractional_values.append(Fraction(item)) elif isinstance(item, str): fractional_values.append(Fraction(item)) elif isinstance(item, Decimal): fractional_values.append(Fraction(item)) # Normalize each fraction normalized_fractions = [frac.limit_denominator(max_denominator) for frac in fractional_values] # Calculate the sum of the normalized fractions total = sum(normalized_fractions, Fraction(0)) # Convert the sum to a tuple of integers (numerator, denominator) total_as_ratio = total.as_integer_ratio() return normalized_fractions, total, total_as_ratio"},{"question":"**Title: Implement a Custom BinHex Encoder/Decoder in Python** **Objective:** To assess your understanding of file encoding/decoding and handling file-like objects, you are required to implement a custom encoder and decoder that mimics the functionality of the Python `binhex` module. This will demonstrate your understanding of file operations, error handling, and encoding techniques. **Task:** Implement two functions, `custom_binhex` and `custom_hexbin`, that encode and decode files similar to the `binhex.binhex` and `binhex.hexbin` functions. **Function Signatures:** ```python def custom_binhex(input_file: str, output_file: str) -> None: pass def custom_hexbin(input_file: str, output_file: str) -> None: pass ``` **Function Descriptions:** - `custom_binhex(input_file: str, output_file: str) -> None`: - Converts a binary file specified by `input_file` into a custom encoding format and writes the result to `output_file`. - The custom encoding format can be a simplified version of the BinHex format as described in the documentation or an alternative text-based encoding of your choice. - The `input_file` and `output_file` parameters are file paths. - `custom_hexbin(input_file: str, output_file: str) -> None`: - Decodes a file encoded in the custom encoding format specified by `input_file` back into its original binary format and writes the result to `output_file`. - The `input_file` and `output_file` parameters are file paths. **Constraints and Requirements:** 1. You may assume the input files are not empty. 2. Handle errors appropriately using custom exceptions or standard exceptions with meaningful messages. 3. Implement context management (using `with` statement) for opening/closing files to ensure resources are managed correctly. 4. Do not use the `binhex` module directly in your solution. 5. Your implementation should handle basic edge cases, such as invalid file paths or non-binary input files for encoding. **Example Usage:** ```python try: custom_binhex(\\"example.bin\\", \\"encoded.hex\\") print(\\"File encoded successfully.\\") except Exception as e: print(f\\"Error during encoding: {e}\\") try: custom_hexbin(\\"encoded.hex\\", \\"decoded.bin\\") print(\\"File decoded successfully.\\") except Exception as e: print(f\\"Error during decoding: {e}\\") ``` **Performance Considerations:** - The solution should handle reasonably large files efficiently. - Avoid loading the entire file into memory if possible. **Additional Notes:** - Think about the design of your custom encoding scheme; it should be reversible so that the decoding process can accurately reconstruct the original file. - You may refer to the BinHex format conceptually but do not have to replicate it exactly. Focus on demonstrating a clear understanding of file encoding and decoding principles.","solution":"def custom_binhex(input_file: str, output_file: str) -> None: Converts a binary file specified by `input_file` into a custom encoding format and writes the result to `output_file`. import base64 try: with open(input_file, \'rb\') as fin, open(output_file, \'w\') as fout: data = fin.read() encoded_data = base64.b16encode(data).decode(\'ascii\') fout.write(encoded_data) except FileNotFoundError as e: raise FileNotFoundError(f\\"Input file {input_file} not found: {e}\\") except IOError as e: raise IOError(f\\"Error occurred during file operations: {e}\\") def custom_hexbin(input_file: str, output_file: str) -> None: Decodes a file encoded in the custom encoding format specified by `input_file` back into its original binary format and writes the result to `output_file`. import base64 try: with open(input_file, \'r\') as fin, open(output_file, \'wb\') as fout: encoded_data = fin.read() data = base64.b16decode(encoded_data) fout.write(data) except FileNotFoundError as e: raise FileNotFoundError(f\\"Input file {input_file} not found: {e}\\") except IOError as e: raise IOError(f\\"Error occurred during file operations: {e}\\") except ValueError as e: raise ValueError(f\\"Error occurred during decoding: {e}\\")"},{"question":"# Question: You are given sales data of a company, with date indices representing the sales date and values representing the amount of sales. Using pandas\' `tseries.offsets` module, your task is to perform some date manipulations and derive insights from the sales data. **Specifications:** 1. **Input:** - A DataFrame named `sales_data` with two columns: - `Date`: This is the index of the DataFrame, which contains datetime values. - `Sales`: This column contains integer values representing sales amounts. 2. **Tasks:** 1. Define a custom business day frequency that includes Monday to Friday but skips weekends, holidays (`holidays` parameter), and any custom non-working days (`weekmask` parameter). 2. Compute the total sales for each business month. A business month ends on the last business day of the month. 3. Identify the start and end of each business quarter and compute the total sales for each business quarter. 4. Create a new DataFrame to summarize the results, which should include: - `Quarter Start`: Start date of the business quarter. - `Quarter End`: End date of the business quarter. - `Total Quarter Sales`: Total sales for the business quarter. 3. **Output:** - A new DataFrame with the summarized results as specified in Task 2 and Task 3. 4. **Constraints:** - You should use `pandas.tseries.offsets` to perform date manipulations. - The data may not cover complete months or quarters, handle such cases appropriately by only considering available data. - Performance should be considered if working with large datasets. 5. **Example:** ```python sales_data = pd.DataFrame({ \'Sales\': [100, 200, 150, 120, 300, 340, 190], }, index=pd.to_datetime([\'2023-01-02\', \'2023-01-03\', \'2023-01-10\', \'2023-02-01\', \'2023-03-01\', \'2023-03-05\', \'2023-04-02\'])) result = summarize_sales(sales_data) print(result) ``` **Function signature:** ```python import pandas as pd import numpy as np from pandas.tseries.offsets import CustomBusinessDay, BMonthEnd, BQuarterEnd def summarize_sales(sales_data: pd.DataFrame) -> pd.DataFrame: pass ``` **Note:** - Implement your own holidays and weekmask when creating the `CustomBusinessDay` frequency. - Use the correct date offset classes like `BMonthEnd` and `BQuarterEnd` for the tasks.","solution":"import pandas as pd import numpy as np from pandas.tseries.offsets import CustomBusinessDay, BMonthEnd, BQuarterEnd from pandas.tseries.holiday import USFederalHolidayCalendar def summarize_sales(sales_data: pd.DataFrame) -> pd.DataFrame: # Define a custom business day frequency weekmask = \'Mon Tue Wed Thu Fri\' holidays = USFederalHolidayCalendar().holidays() bday = CustomBusinessDay(weekmask=weekmask, holidays=holidays) # Compute total sales for each business month sales_data[\'Business Month\'] = sales_data.index.to_period(\'M\').to_timestamp() + BMonthEnd() monthly_sales = sales_data.groupby(\'Business Month\')[\'Sales\'].sum().reset_index() # Compute total sales for each business quarter sales_data[\'Business Quarter\'] = sales_data.index.to_period(\'Q\').to_timestamp() + BQuarterEnd() quarterly_sales = sales_data.groupby(\'Business Quarter\')[\'Sales\'].sum().reset_index() # Create the summary DataFrame summary_df = pd.DataFrame({ \'Quarter Start\': quarterly_sales[\'Business Quarter\'] - BQuarterEnd() + pd.offsets.MonthBegin(), \'Quarter End\': quarterly_sales[\'Business Quarter\'], \'Total Quarter Sales\': quarterly_sales[\'Sales\'] }) return summary_df"},{"question":"# Multi-Phase Encoding and Decoding with Custom Error Handling Problem Statement You are tasked with implementing a multi-phase encoding and decoding system in Python. This system will involve encoding a given string to a specified encoding, handling errors using a custom error handler, then decoding it back to its original form. Requirements 1. **Function 1: `custom_encoder_decoder`** Implement a function `custom_encoder_decoder(text: str, encoding: str, error_handler: str) -> str` that: - Takes a string `text`, an `encoding` name, and an `error_handler` name. - Encodes the `text` using the specified `encoding` and the custom error handler. - Decodes the encoded text back to the original string using the same `encoding` and error handler. - Returns the decoded string. 2. **Custom Error Handler: `replace_with_star`** Implement a custom error handler `replace_with_star` that will replace any unencodable or undecodable characters with a `\'*\'`. Constraints - You must ensure that the encoding and decoding process uses your custom error handler. - The function must raise appropriate exceptions if the encoding is not supported or if any other error occurs during the process. Input and Output Formats - **Input:** - `text` (str): The input string to encode and decode. - `encoding` (str): The encoding to use (e.g., \'utf-8\'). - `error_handler` (str): The name of the custom error handler (in this case, always \'replace_with_star\'). - **Output:** - (str): The final decoded string after performing the encoding and decoding operations. Performance - Your implementation should efficiently handle strings up to 10,000 characters in length. Example ```python # Example custom_error_handler function definition def replace_with_star(exc): if isinstance(exc, UnicodeEncodeError): return \'*\', exc.start + 1 elif isinstance(exc, UnicodeDecodeError): return \'*\', exc.start + 1 else: raise TypeError(\\"Don\'t know how to handle this type of exception\\") # Example usage text = \\"hello\\" encoding = \\"ascii\\" error_handler = \\"replace_with_star\\" # Expected output would be \\"hello\\" as ASCII can encode \\"hello\\" without errors. print(custom_encoder_decoder(text, encoding, error_handler)) ``` Additional Information - Utilize `PyCodec_*` functions to handle the encoding and decoding operations. - Register your custom error handler using `PyCodec_RegisterError`. Notes - If the custom error handler supports different types of errors, make sure to handle both `UnicodeEncodeError` and `UnicodeDecodeError` as shown. - Thoroughly test your input strings with various encodings and characters to ensure proper handling and comprehensive testing.","solution":"import codecs def replace_with_star(exc): if isinstance(exc, UnicodeEncodeError): return \'*\', exc.start + 1 elif isinstance(exc, UnicodeDecodeError): return \'*\', exc.start + 1 else: raise TypeError(\\"Don\'t know how to handle this type of exception\\") codecs.register_error(\'replace_with_star\', replace_with_star) def custom_encoder_decoder(text: str, encoding: str, error_handler: str) -> str: try: # Encode the text using the specified encoding and error handler encoded_text = text.encode(encoding, errors=error_handler) # Decode the text back to the original string using the same encoding and error handler decoded_text = encoded_text.decode(encoding, errors=error_handler) return decoded_text except LookupError: raise ValueError(\\"Specified encoding is not supported.\\") except Exception as e: raise ValueError(f\\"An error occurred during encoding/decoding: {e}\\")"},{"question":"You are tasked to demonstrate your understanding of the Seaborn library by performing the following visualization tasks using a given dataset. Dataset Description You will use the \\"flights\\" dataset provided by Seaborn. This dataset contains the number of passengers carried by an airline over a 10 year period, broken down by month. Tasks 1. **Data Preparation**: - Load the \\"flights\\" dataset using Seaborn\'s `load_dataset` function. - Pivot the dataset such that the rows are years, columns are months, and the values are the number of passengers. 2. **Basic Heatmap**: - Create a basic heatmap of the pivoted data. - Ensure the heatmap displays annotations of the cell values. 3. **Customization**: - Format the annotations to display with no decimal places. - Add lines between the cells with a linewidth of 0.5. 4. **Color Customization**: - Select a different colormap (other than the default) to represent the data. - Adjust the colormap norm so that the minimum value represented is 100, and the maximum value is 600. 5. **Axis Customization**: - Using the `matplotlib.axes.Axes` methods, tweak the plot so that: - The x-axis label is set to \\"Month\\". - The y-axis label is set to \\"Year\\". - The month names appear at the top of the heatmap rather than the bottom. Input and Output - There is no specific input as the dataset is loaded directly via Seaborn. - The output should be a heatmap plot that satisfies all the given customization requirements. Constraints - You must use the `seaborn` package for all visualizations and customizations. - Your annotations must use a separate DataFrame. Example Result Your output plot should resemble a well-formatted heatmap displaying airline passengers over a decade, with customizations as described. Performance Requirements - Your code should execute in a reasonable time, typically under a second for such data. Bonus - Explain each customization step briefly in comments within your code. # Solution Template ```python import seaborn as sns import matplotlib.pyplot as plt # Task 1: Data Preparation flights = sns.load_dataset(\\"flights\\") pivot_flights = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Task 2: Basic Heatmap # Create the heatmap with annotations ax = sns.heatmap(pivot_flights, annot=True) # Task 3: Customization # Format the annotations and add lines between cells ax = sns.heatmap(pivot_flights, annot=True, fmt=\\"d\\", linewidths=0.5) # Task 4: Color Customization # Choose a different colormap and set colormap norm ax = sns.heatmap(pivot_flights, annot=True, fmt=\\"d\\", linewidths=0.5, cmap=\\"YlGnBu\\", vmin=100, vmax=600) # Task 5: Axis Customization # Set axis labels and move month names to the top ax.set(xlabel=\\"Month\\", ylabel=\\"Year\\") ax.xaxis.tick_top() # Display the plot plt.show() ``` Notes: - Make sure to test and validate your code for correctness. - Any solutions should be well-documented and readable.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_flights_heatmap(): # Task 1: Data Preparation flights = sns.load_dataset(\\"flights\\") pivot_flights = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Task 2: Basic Heatmap # Task 3: Customization # Task 4: Color Customization # Task 5: Axis Customization plt.figure(figsize=(12, 6)) ax = sns.heatmap(pivot_flights, annot=True, fmt=\\"d\\", linewidths=0.5, cmap=\\"YlGnBu\\", vmin=100, vmax=600) # Setting axis labels and adjusting position of the month labels to be on top ax.set(xlabel=\\"Month\\", ylabel=\\"Year\\") ax.xaxis.tick_top() ax.xaxis.set_label_position(\'top\') # Display the plot plt.show() # Call the function to create and display the heatmap create_flights_heatmap()"},{"question":"# Pandas Data Manipulation and Analysis Task Objective: Write a function named `analyze_sales_data` that takes in a csv file path containing sales data and performs the following operations: 1. **Load the Data:** - Read the CSV file into a pandas DataFrame. 2. **Data Cleaning:** - Handle missing values by replacing them with appropriate values: - For numeric columns, fill missing values with the median of the column. - For categorical columns, fill missing values with the mode of the column. 3. **Data Transformation:** - Convert the \'Date\' column to datetime format. - Create a new column \'Month\' that extracts the month from the \'Date\' column. - Pivot the DataFrame to summarize total sales (\'Sales\' column) for each \'Store\' by \'Month\'. 4. **Data Aggregation:** - Merge the sales data with another DataFrame that contains \'Store\' and \'Region\' information. - Aggregate the total monthly sales by \'Region\'. 5. **Output the Result:** - The function should return the final aggregated DataFrame showing total monthly sales for each region. Input: - `file_path`: str, the path to the sales data CSV file. The CSV file is assumed to have at least the following columns: \'Store\', \'Date\', \'Sales\'. - `store_region_df`: pandas DataFrame, a DataFrame containing \'Store\' and \'Region\' columns. Output: - pandas DataFrame: A DataFrame with columns \'Region\', \'Month\', and \'TotalSales\' showing total sales aggregated by region and month. Constraints: - You can assume the CSV file and input DataFrame are well-formed and contain no duplicate entries. - Performance considerations: The function should handle large datasets efficiently and make use of pandas\' built-in functionalities. Function Signature: ```python import pandas as pd def analyze_sales_data(file_path: str, store_region_df: pd.DataFrame) -> pd.DataFrame: # Your implementation here pass ``` Example: Given a CSV file with the following content: ``` Store,Date,Sales A,2021-01-01,100 A,2021-01-15,150 B,2021-01-01,200 C,2021-02-01,250 B,2021-03-15,300 ``` And a DataFrame `store_region_df` as: ``` Store Region 0 A East 1 B West 2 C North ``` The output DataFrame should look like this: ``` Region Month TotalSales 0 East 01 250 1 North 02 250 2 West 01 200 3 West 03 300 ``` Good luck and happy coding!","solution":"import pandas as pd def analyze_sales_data(file_path: str, store_region_df: pd.DataFrame) -> pd.DataFrame: # Load the Data df = pd.read_csv(file_path) # Data Cleaning for col in df.columns: if df[col].dtype in [\'int64\', \'float64\']: df[col].fillna(df[col].median(), inplace=True) else: df[col].fillna(df[col].mode()[0], inplace=True) # Data Transformation df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Month\'] = df[\'Date\'].dt.strftime(\'%m\') # Pivot to summarize total sales for each store by month pivot_df = df.pivot_table(values=\'Sales\', index=[\'Store\', \'Month\'], aggfunc=\'sum\').reset_index() # Merge with store_region_df to get regions merged_df = pd.merge(pivot_df, store_region_df, on=\'Store\') # Aggregate total sales by region and month result_df = merged_df.groupby([\'Region\', \'Month\']).agg({\'Sales\': \'sum\'}).reset_index() result_df.rename(columns={\'Sales\': \'TotalSales\'}, inplace=True) return result_df"},{"question":"# Telnet Client Data Collector Objective You are required to implement a Telnet client that connects to a specified Telnet server, handles login credentials, performs a series of commands, and collects specific data from the server output. This assessment will test your understanding of network communication and resource management using the `telnetlib` module. Requirements 1. **Connection**: Establish a connection to the given Telnet server. 2. **Login**: Handle login by sending a username and password. 3. **Send Commands**: Send a sequence of commands to the server. 4. **Collect Data**: Read the server\'s response until a specific pattern is encountered, which indicates the end of relevant data. 5. **Return Output**: Return the collected output as a string, stripped of any extraneous data before the pattern. Function Signature ```python def collect_telnet_data(host: str, username: str, password: str, commands: list, end_pattern: bytes, timeout: int = 10) -> str: pass ``` Input - `host` (str): The hostname or IP address of the Telnet server to connect to. - `username` (str): The username for logging into the Telnet server. - `password` (str): The password for logging into the Telnet server. - `commands` (list): A list of command strings to execute on the Telnet server. - `end_pattern` (bytes): A byte string which indicates the end of relevant data in the server\'s response. - `timeout` (int): The timeout period for network operations, in seconds (default is 10 seconds). Output - Returns a string containing the combined and stripped output from the executed commands up until the `end_pattern` is encountered. Constraints - The function should handle connection errors gracefully. - Ensure proper resource management (e.g., closing connections). - Assume the server sends a prompt requesting `login:` and `Password:` during the login phase. - `commands` will contain strings without line terminators, add `n` where appropriate. Example ```python host = \\"localhost\\" username = \\"test_user\\" password = \\"test_password\\" commands = [\\"whoami\\", \\"date\\"] end_pattern = b\\"command completed\\" timeout = 10 result = collect_telnet_data(host, username, password, commands, end_pattern, timeout) print(result) ``` **Expected behavior**: The function connects to the specified Telnet server, logs in using the provided credentials, executes the commands, collects the data until the `end_pattern` is encountered, and returns the collected data as a string.","solution":"import telnetlib def collect_telnet_data(host: str, username: str, password: str, commands: list, end_pattern: bytes, timeout: int = 10) -> str: try: # Initialize Telnet object and connect to the server telnet = telnetlib.Telnet(host, timeout=timeout) # Read the initial login prompt telnet.read_until(b\\"login: \\") # Send the username telnet.write(username.encode(\'ascii\') + b\\"n\\") # Read the password prompt telnet.read_until(b\\"Password: \\") # Send the password telnet.write(password.encode(\'ascii\') + b\\"n\\") # Execute each command for command in commands: telnet.write(command.encode(\'ascii\') + b\\"n\\") # Read until we encounter the end_pattern output = telnet.read_until(end_pattern, timeout=timeout) # Extract and Clean the relevant part of the output result = output.split(end_pattern)[0].decode(\'ascii\') # Ensure resource management by closing the connection telnet.close() return result.strip() except Exception as e: return f\\"An error occurred: {e}\\""},{"question":"**Custom Pickling and Copying with the `copyreg` Module** You are creating a library that needs to handle serialization (pickling) and shallow copying for a custom class, `SpecialItem`. This class cannot be pickled/copied directly due to non-trivial internal state that must be carefully managed. Your task is to implement and register custom reduction (pickling) functions to be used with the `copyreg` module. Specifically, you must: 1. Implement the `SpecialItem` class. 2. Implement a reduction function `pickle_special_item` for the `SpecialItem` class. 3. Register this reduction function with `copyreg`. # Class Definition The `SpecialItem` class should have the following: - An `__init__` method that takes and stores two attributes: `name` (string) and `value` (integer). - A `__repr__` method that returns a string in the format `SpecialItem(name=<name>, value=<value>)`. # Reduction Function The function `pickle_special_item` should: - Take an instance of `SpecialItem` as an argument. - Return a tuple consisting of the constructor for `SpecialItem` and a tuple of arguments needed to reconstruct the object (`name` and `value`). # Registration Using `copyreg.pickle`, register `pickle_special_item` as the reduction function for `SpecialItem`. # Requirements 1. Define the `SpecialItem` class and the `pickle_special_item` function within the script. 2. Register `pickle_special_item` with `copyreg`. 3. Demonstrate that both shallow copying and pickling work correctly using the following steps: - Create an instance of `SpecialItem`. - Perform a shallow copy using `copy.copy`. - Serialize and deserialize the instance using `pickle.dumps` and `pickle.loads`. # Example Usage ```python # Your implementation goes here... import copy import pickle # Create an instance of SpecialItem item = SpecialItem(name=\\"Widget\\", value=42) # Perform a shallow copy item_copy = copy.copy(item) print(item_copy) # Expected: SpecialItem(name=Widget, value=42) # Serialize and deserialize item_serialized = pickle.dumps(item) item_deserialized = pickle.loads(item_serialized) print(item_deserialized) # Expected: SpecialItem(name=Widget, value=42) ``` # Constraints - The `name` attribute will always be a string of length <= 100. - The `value` attribute will always be an integer between 0 and 10^6. Implement your solution in the form of a Python script.","solution":"import copyreg class SpecialItem: def __init__(self, name, value): self.name = name self.value = value def __repr__(self): return f\'SpecialItem(name={self.name}, value={self.value})\' def pickle_special_item(obj): return (SpecialItem, (obj.name, obj.value)) copyreg.pickle(SpecialItem, pickle_special_item)"},{"question":"Objective: To assess the students\' understanding of the Python `typing` module, especially focusing on defining and using custom types, type hints, generics, and more advanced features like `NewType`, `TypedDict`, and `Annotated`. Problem Statement: You are responsible for implementing a function that aggregates and processes user data in a structured format. The function should: 1. Validate the input using type hints. 2. Use custom types created with `NewType`. 3. Utilize `TypedDict` to define the structure of the input and output. 4. Apply `Annotated` to add metadata to certain fields. Detailed Requirements: 1. **Custom Types**: - Define a custom type `UserId` using `NewType` based on `int`. - Define another custom type `Score` using `NewType` based on `float`. 2. **TypedDict**: - Define a `PlayerData` typed dictionary to represent the input structure, which contains: - `user_id` of type `UserId`. - `username` of type `str`. - `scores` of type `List[Score]`. 3. **Annotated**: - Define an `Annotated` type for the `username` field to indicate that it should follow certain constraints like being less than or equal to 15 characters long. 4. **Function Implementation**: - Implement the function `aggregate_scores(players: List[PlayerData]) -> Dict[UserId, Dict[str, Union[str, float]]]`: - The function should take a list of `PlayerData` and return a dictionary indexed by `UserId`. - Each dictionary entry should contain: - `username`: The player\'s username. - `total_score`: The sum of all scores in the `scores` list for the player. Type Hints and Constraints: - The `username` should be `Annotated[str, \'max 15 characters\']`. - The `total_score` should be the sum of the list of `Score`. Here is the starting template you should use to define the types and the function: ```python from typing import NewType, List, Dict, Union, Annotated, TypedDict # Define new custom types UserId = NewType(\'UserId\', int) Score = NewType(\'Score\', float) # Define the TypedDict for player data class PlayerData(TypedDict): user_id: UserId username: Annotated[str, \'max 15 characters\'] scores: List[Score] def aggregate_scores(players: List[PlayerData]) -> Dict[UserId, Dict[str, Union[str, float]]]: # Implement the aggregation logic here pass ``` Implement the function according to the requirements provided. Ensure the function adheres strictly to the type hints and the constraints described. Example: ```python players = [ {\'user_id\': UserId(1), \'username\': \'playerone\', \'scores\': [Score(45.0), Score(55.0)]}, {\'user_id\': UserId(2), \'username\': \'playertwo\', \'scores\': [Score(70.0), Score(30.0)]}, ] result = aggregate_scores(players) \'\'\' Expected result: { UserId(1): {\'username\': \'playerone\', \'total_score\': 100.0}, UserId(2): {\'username\': \'playertwo\', \'total_score\': 100.0} } \'\'\' ``` **Note**: The type checker will enforce the type constraints, but you should also handle type validation within the function where necessary.","solution":"from typing import NewType, List, Dict, Union, Annotated, TypedDict # Define new custom types UserId = NewType(\'UserId\', int) Score = NewType(\'Score\', float) # Define the TypedDict for player data class PlayerData(TypedDict): user_id: UserId username: Annotated[str, \'max 15 characters\'] scores: List[Score] def aggregate_scores(players: List[PlayerData]) -> Dict[UserId, Dict[str, Union[str, float]]]: result = {} for player in players: user_id = player[\'user_id\'] username = player[\'username\'] # Ensure username is within the constraint of 15 characters if len(username) > 15: raise ValueError(f\\"Username \'{username}\' exceeds 15 characters limit\\") total_score = sum(player[\'scores\']) result[user_id] = { \'username\': username, \'total_score\': total_score } return result"},{"question":"# Question: Implement an HTTP Client to Monitor Website Status You are required to implement a Python class that monitors the status of a list of websites using the `http.client` module. The class should facilitate sending `GET` requests to specified URLs and provide methods to check their response statuses. Additionally, handle SSL configurations for HTTPS connections, and provide appropriate error handling for various HTTP-related exceptions. Class Definition: `WebsiteMonitor` 1. **Initialization**: - `__init__(self, urls: List[str]) -> None` - `urls`: A list of website URLs to monitor. 2. **Methods**: - `check_status(self) -> Dict[str, Tuple[int, str]]`: - Sends a `GET` request to each URL. - Returns a dictionary where keys are URLs, and values are tuples containing the HTTP status code and reason phrase. - `set_ssl_context(self, url: str, context: ssl.SSLContext) -> None`: - Configures SSL context for HTTPS connections for a specified URL. - `set_custom_headers(self, url: str, headers: Dict[str, str]) -> None`: - Sets custom headers for a specific URL. 3. **Error Handling**: - Properly handle exceptions related to connection errors, invalid URLs, and other HTTP-related issues. - Log or print an appropriate message when an exception is encountered. Example Usage: ```python from typing import List, Dict, Tuple import ssl import http.client class WebsiteMonitor: def __init__(self, urls: List[str]) -> None: self.urls = urls self.ssl_contexts = {} self.custom_headers = {} def check_status(self) -> Dict[str, Tuple[int, str]]: results = {} for url in self.urls: parsed_url = self._parse_url(url) if not parsed_url: results[url] = (None, \\"Invalid URL\\") continue host, path, use_https = parsed_url try: if use_https: conn = http.client.HTTPSConnection(host, context=self.ssl_contexts.get(url)) else: conn = http.client.HTTPConnection(host) headers = self.custom_headers.get(url, {}) conn.request(\\"GET\\", path, headers=headers) response = conn.getresponse() results[url] = (response.status, response.reason) conn.close() except (http.client.HTTPException, http.client.NotConnected, http.client.InvalidURL) as e: results[url] = (None, str(e)) return results def set_ssl_context(self, url: str, context: ssl.SSLContext) -> None: self.ssl_contexts[url] = context def set_custom_headers(self, url: str, headers: Dict[str, str]) -> None: self.custom_headers[url] = headers def _parse_url(self, url: str) -> Tuple[str, str, bool]: # Function to parse the URL, to extract host, path and whether it is HTTPS or not if url.startswith(\\"https://\\"): return url[8:], \\"/\\", True elif url.startswith(\\"http://\\"): return url[7:], \\"/\\", False else: return None # Example urls = [\\"https://www.python.org\\", \\"http://www.example.com\\"] monitor = WebsiteMonitor(urls) # Custom SSL Context for https://www.python.org (optional) context = ssl.create_default_context() monitor.set_ssl_context(\\"https://www.python.org\\", context) # Custom Headers for http://www.example.com (optional) monitor.set_custom_headers(\\"http://www.example.com\\", {\\"User-Agent\\": \\"CustomAgent\\"}) results = monitor.check_status() for url, status in results.items(): print(f\\"{url}: {status[0]} {status[1]}\\") ``` Constraints: - Ensure that connections to at least five URLs are tested. - Handle lawful status codes (2xx, 3xx, 4xx, and 5xx) gracefully. - Assume the input URLs will be correctly formatted, though `http` or `https` prefix might be missing. Notes: - Use methods provided by the `http.client` module effectively. - Stack traces/exceptions need not be logged; simple exception messages suffice. - For simplicity, the `_parse_url` function can be improved to handle paths and query parameters.","solution":"from typing import List, Dict, Tuple import ssl import http.client from urllib.parse import urlparse class WebsiteMonitor: def __init__(self, urls: List[str]) -> None: self.urls = urls self.ssl_contexts = {} self.custom_headers = {} def check_status(self) -> Dict[str, Tuple[int, str]]: results = {} for url in self.urls: parsed_url = self._parse_url(url) if not parsed_url: results[url] = (None, \\"Invalid URL\\") continue host, path, use_https = parsed_url try: if use_https: conn = http.client.HTTPSConnection(host, context=self.ssl_contexts.get(url, ssl.create_default_context())) else: conn = http.client.HTTPConnection(host) headers = self.custom_headers.get(url, {}) conn.request(\\"GET\\", path, headers=headers) response = conn.getresponse() results[url] = (response.status, response.reason) conn.close() except (http.client.HTTPException, http.client.NotConnected, http.client.InvalidURL) as e: results[url] = (None, str(e)) return results def set_ssl_context(self, url: str, context: ssl.SSLContext) -> None: self.ssl_contexts[url] = context def set_custom_headers(self, url: str, headers: Dict[str, str]) -> None: self.custom_headers[url] = headers def _parse_url(self, url: str) -> Tuple[str, str, bool]: parsed = urlparse(url) if parsed.scheme not in [\\"http\\", \\"https\\"]: return None return parsed.hostname, parsed.path or \\"/\\", parsed.scheme == \\"https\\" # Example urls = [\\"https://www.python.org\\", \\"http://www.example.com\\"] monitor = WebsiteMonitor(urls) # Custom SSL Context for https://www.python.org (optional) context = ssl.create_default_context() monitor.set_ssl_context(\\"https://www.python.org\\", context) # Custom Headers for http://www.example.com (optional) monitor.set_custom_headers(\\"http://www.example.com\\", {\\"User-Agent\\": \\"CustomAgent\\"}) results = monitor.check_status() for url, status in results.items(): print(f\\"{url}: {status[0]} {status[1]}\\")"},{"question":"# Coding Assessment: Enhancing Performance in Pandas Objective: Your task is to write and optimize a computation-heavy function using different methods as discussed in the pandas documentation. Task: Implement a function that performs the following operations on a pandas DataFrame: 1. Create a DataFrame with the following columns: - `a`: 100,000 random values drawn from a standard normal distribution. - `b`: 100,000 random values drawn from a standard normal distribution. - `N`: 100,000 random integers between 100 and 1000. 2. Write a function `compute(df)` that computes row-wise integration of `a` and `b` with `N` steps using pure pandas. 3. Optimize the function using: - Cython - Numba - pandas.eval function Requirements: 1. Implement the function using pure pandas and measure the running time. 2. Optimize the function using Cython and measure the performance gain. 3. Optimize the function using Numba and measure the performance gain. 4. Use `pandas.eval` for evaluation and measure the performance gain. 5. Compare the performance of all methods. Expected Input and Output Format: - Input: A pandas DataFrame `df` (as described above) - Output: A pandas Series with computed results for each row Constraints: - The function should handle a DataFrame with at least 100,000 rows efficiently. - Use appropriate profiling tools to measure time spent in different parts of the function. Submission: Provide implementation for the following functions: 1. `compute_pure_pandas(df)` 2. `compute_cython(df)` 3. `compute_numba(df)` 4. `compute_pandas_eval(df)` In addition to the implementations, include the following: - A brief analysis of the performance profiling results. - A comparison table showing the execution times for each method. Example: ```python import pandas as pd import numpy as np # Generate the DataFrame df = pd.DataFrame({ \\"a\\": np.random.randn(100000), \\"b\\": np.random.randn(100000), \\"N\\": np.random.randint(100, 1000, 100000) }) # Implement and test your functions result_pandas = compute_pure_pandas(df) result_cython = compute_cython(df) result_numba = compute_numba(df) result_eval = compute_pandas_eval(df) # Compare execution times compare_performance() ``` Ensure you handle all edge cases and the functions return correct and optimized results.","solution":"import pandas as pd import numpy as np def compute_pure_pandas(df): Computes row-wise integration of columns \'a\' and \'b\' using pure pandas. result = df.apply(lambda row: row[\'a\'] * row[\'N\'] + row[\'b\'] * row[\'N\'], axis=1) return result def generate_dataframe(): df = pd.DataFrame({ \\"a\\": np.random.randn(100000), \\"b\\": np.random.randn(100000), \\"N\\": np.random.randint(100, 1000, 100000) }) return df"},{"question":"Your task is to implement a program that demonstrates the use of both thread-based and process-based parallelism to perform a computation-intensive task efficiently. Problem Statement You are given a large list of numbers, which you need to process in two steps: 1. **Step 1 - Thread-based Parallelism**: - Use multiple threads to compute the square of each number in the list. - Use a `ThreadPoolExecutor` from the `concurrent.futures` module to manage threads. 2. **Step 2 - Process-based Parallelism**: - Use multiple processes to sum the squares computed in the first step. - Use a `ProcessPoolExecutor` from the `concurrent.futures` module to manage processes. Constraints - The list can contain up to 1,000,000 integers. - Ensure the solution is efficient in terms of both time and space complexity. Input - A list of integers (`numbers`). Output - An integer which is the sum of the squares of all numbers in the list. Requirements - Use `ThreadPoolExecutor` for threading. - Use `ProcessPoolExecutor` for processing. - Handle potential exceptions that may arise during execution. Function Signature ```python def compute_sum_of_squares(numbers: list[int]) -> int: pass ``` Example ```python numbers = [1, 2, 3, 4, 5] print(compute_sum_of_squares(numbers)) # Output: 55 (since 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 1 + 4 + 9 + 16 + 25 = 55) ``` Hints - Consider breaking down the number list into smaller chunks to distribute work across threads/processes effectively. - Use `map` method from `ThreadPoolExecutor` and `ProcessPoolExecutor` to apply functions across chunks.","solution":"from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor def compute_square(n): return n * n def sum_chunk(chunk): return sum(chunk) def compute_sum_of_squares(numbers): # Step 1: Use ThreadPoolExecutor to compute squares of each number with ThreadPoolExecutor() as executor: squares = list(executor.map(compute_square, numbers)) # Step 2: Use ProcessPoolExecutor to sum the squares chunks = [squares[i:i + 1000] for i in range(0, len(squares), 1000)] with ProcessPoolExecutor() as executor: chunk_sums = list(executor.map(sum_chunk, chunks)) total_sum = sum(chunk_sums) return total_sum"},{"question":"<|Analysis Begin|> Based on the provided documentation, the essential functionalities and features of pandas that are covered include: 1. Efficient data loading and memory management, such as: - Selecting specific columns to load from a file. - Using efficient datatypes like `pandas.Categorical` and optimizing numeric data with `pd.to_numeric`. 2. Handling large datasets through chunking, breaking large datasets or files into manageable pieces that can fit into memory. 3. Methods for reading and processing files in chunks. These are advanced features of pandas that allow working with large datasets efficiently. The focus is not only on basic data manipulation but also on optimizing memory usage and processing efficiency. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Handling and Optimizing Large DataFrames **Objective**: Demonstrate comprehension of loading, transforming, and optimizing datasets using pandas, with a focus on handling large datasets efficiently. Problem Statement: You are given a directory containing multiple parquet files, each representing time series data for different years. Your task is to process these files, optimize the memory usage, and perform specific aggregations and transformations. The data contain columns for timestamp, name, id, x, and y. 1. **Load Data Efficiently**: - Only load the columns `id`, `name`, `x`, and `timestamp` for each file. 2. **Optimize Memory Usage**: - Convert the `name` column to a categorical type. - Downcast the `id` column to the smallest possible unsigned integer. - Downcast the `x` column to the smallest possible float. 3. **Aggregation**: - Calculate the total counts of each unique `name` across all files. 4. **Transformation**: - Create a new column `x_cumsum` representing the cumulative sum of `x` for each file independently (this operation should be done before concatenating all DataFrames). 5. **Output**: - Save the final DataFrame with aggregated results (`name` counts) and the per-file cumulative `x` sums to a new parquet file. Expected Input and Output Formats: **Input**: - Directory path containing parquet files. **Output**: - Single parquet file with aggregated results. Constraints: - Each parquet file fits into memory individually, but the total dataset may not. - Use chunked processing to ensure memory efficiency. - Ensure that transformations are only performed after all optimizations are applied. **Function Signature**: ```python import pandas as pd from pathlib import Path def process_large_dataset(input_dir: str, output_file: str): pass ``` Example: Suppose you have a directory structure as follows: ``` data/ â”œâ”€â”€ timeseries/ â”œâ”€â”€ ts-00.parquet â”œâ”€â”€ ts-01.parquet â”œâ”€â”€ ... ``` The function call: ```python process_large_dataset(\\"data/timeseries\\", \\"output/aggregated_results.parquet\\") ``` Your final parquet file should contain: - The cumulative sums of `x` for each file. - Aggregated counts of unique `name` across all combined files. Notes: - Ensure that your implementation is efficient for large scale datasets. - The `timestamp` column is used for indexing, but no specific operations are required on it. - Remember to handle any file I/O operations properly, ensuring files are closed after processing.","solution":"import pandas as pd from pathlib import Path def process_large_dataset(input_dir: str, output_file: str): # Path object for the input directory data_path = Path(input_dir) # Initialize an empty DataFrame for aggregating counts total_counts = pd.Series(dtype=int) # List to store processed dataframes for cumulative sums processed_dfs = [] # Iterate through each parquet file in the directory for file in data_path.glob(\\"*.parquet\\"): # Load only the necessary columns df = pd.read_parquet(file, columns=[\'id\', \'name\', \'x\', \'timestamp\']) # Optimize the DataFrame df[\'name\'] = df[\'name\'].astype(\'category\') df[\'id\'] = pd.to_numeric(df[\'id\'], downcast=\'unsigned\') df[\'x\'] = pd.to_numeric(df[\'x\'], downcast=\'float\') # Calculate the cumulative sum of \'x\' df[\'x_cumsum\'] = df[\'x\'].cumsum() # Aggregate counts of unique \'name\' counts = df[\'name\'].value_counts() total_counts = total_counts.add(counts, fill_value=0) # Append the processed DataFrame to the list processed_dfs.append(df) # Concatenate all processed DataFrames final_df = pd.concat(processed_dfs, ignore_index=True) # Create a final DataFrame for the aggregated results agg_results = pd.DataFrame({ \'name\': total_counts.index, \'count\': total_counts.values }) # Save the final DataFrame to the output file final_df.to_parquet(output_file) # Save the aggregated results to a separate file agg_results_file = Path(output_file).with_name(\\"aggregated_results.parquet\\") agg_results.to_parquet(agg_results_file)"},{"question":"Your task is to implement a function using PyTorch that generates a tensor initialized with random values, complies with certain constraints, and performs a specific operation on that tensor. # Function Signature ```python import torch def custom_random_tensor(shape: tuple, lower_bound: float, upper_bound: float, operation: str): Generate a random tensor and perform an operation on it. Parameters: shape (tuple): Shape of the tensor to be generated. lower_bound (float): The lower bound for the random values. upper_bound (float): The upper bound for the random values. operation (str): The operation to perform on the tensor. Can be \'sum\', \'mean\', or \'max\'. Returns: torch.Tensor: A tensor with random values in the specified range, processed as per the specified operation. pass ``` # Description 1. The function `custom_random_tensor` should create a tensor with the given `shape` filled with random values uniformly distributed between `lower_bound` and `upper_bound`. 2. After generating the tensor, the function should perform one of the specified operations on the entire tensor: * `\'sum\'`: Return the sum of all elements in the tensor. * `\'mean\'`: Return the mean (average) of all elements in the tensor. * `\'max\'`: Return the maximum value among all elements in the tensor. 3. You should use PyTorch\'s built-in functions to perform these operations. # Input - `shape (tuple)`: A tuple indicating the shape of the tensor to be generated. - `lower_bound (float)`: A float representing the lower bound of the random values. - `upper_bound (float)`: A float representing the upper bound of the random values. - `operation (str)`: A string that can be either `\'sum\'`, `\'mean\'`, or `\'max\'`. # Output - Returns a `torch.Tensor` containing the result after performing the specified operation. # Example ```python tensor_sum = custom_random_tensor((3, 3), 0.0, 1.0, \'sum\') print(tensor_sum) # Output: A tensor containing the sum of all elements tensor_mean = custom_random_tensor((4, 4), -10.0, 10.0, \'mean\') print(tensor_mean) # Output: A tensor containing the mean of all elements tensor_max = custom_random_tensor((2, 5), 5.0, 15.0, \'max\') print(tensor_max) # Output: A tensor containing the maximum value among all elements ``` # Constraints - Ensure that the random values are drawn from a uniform distribution between `lower_bound` and `upper_bound`. - Handle edge cases such as empty tensors (`shape` containing zero) appropriately.","solution":"import torch def custom_random_tensor(shape: tuple, lower_bound: float, upper_bound: float, operation: str): Generate a random tensor and perform an operation on it. Parameters: shape (tuple): Shape of the tensor to be generated. lower_bound (float): The lower bound for the random values. upper_bound (float): The upper bound for the random values. operation (str): The operation to perform on the tensor. Can be \'sum\', \'mean\', or \'max\'. Returns: torch.Tensor: A tensor with random values in the specified range, processed as per the specified operation. # Generate the random tensor using torch\'s uniform distribution tensor = torch.empty(shape).uniform_(lower_bound, upper_bound) # Perform the specified operation on the tensor if operation == \'sum\': result = tensor.sum() elif operation == \'mean\': result = tensor.mean() elif operation == \'max\': result = tensor.max() else: raise ValueError(\\"Invalid operation. Use \'sum\', \'mean\', or \'max\'.\\") return result"},{"question":"You are tasked with creating a function that processes a list of transactions and computes the final account balance, including some additional data manipulations. The function should incorporate arithmetic operations, string manipulation, and list handling. # Function Specifications **Function Name:** `process_transactions` **Parameters:** 1. `initial_balance` (float): The initial balance in the account. 2. `transactions` (list of tuples): A list where each tuple contains: - `type` (str): Type of the transaction, either \\"deposit\\" or \\"withdraw\\". - `amount` (float): The amount for the transaction. - `description` (str): A brief description of the transaction. **Returns:** A dictionary with the following keys: - `final_balance` (float): The final account balance after processing all transactions. - `total_deposits` (float): The total amount deposited. - `total_withdrawals` (float): The total amount withdrawn. - `summary` (str): A string summarizing the transactions in the format: - \\"Deposited: amount - Description\\" - \\"Withdrew: amount - Description\\" # Constraints - The `type` in each transaction will always be either \\"deposit\\" or \\"withdraw\\". - The `amount` will always be a non-negative float. - The `description` will always be a non-empty string. # Example ```python initial_balance = 1000.0 transactions = [ (\\"deposit\\", 200.0, \\"Salary\\"), (\\"withdraw\\", 50.0, \\"Groceries\\"), (\\"deposit\\", 150.0, \\"Freelance\\"), (\\"withdraw\\", 30.0, \\"Transport\\") ] result = process_transactions(initial_balance, transactions) # Output { \\"final_balance\\": 1270.0, \\"total_deposits\\": 350.0, \\"total_withdrawals\\": 80.0, \\"summary\\": \\"Deposited: 200.0 - SalarynWithdrew: 50.0 - GroceriesnDeposited: 150.0 - FreelancenWithdrew: 30.0 - Transport\\" } ``` # Detailed Steps 1. Initialize `total_deposits` and `total_withdrawals` to zero. 2. Traverse the `transactions` list and update the `initial_balance` based on the type of transaction. 3. Accumulate the amounts to `total_deposits` and `total_withdrawals` accordingly. 4. Construct the `summary` string by iterating over the transactions. 5. Return the dictionary with the final values. # Implementation Notes - You may use string methods for constructing the `summary`. - Use list operations and arithmetic to update balances and calculate totals. - Think about edge cases, such as no transactions or all deposits/withdrawals. # Solution Template ```python def process_transactions(initial_balance, transactions): total_deposits = 0.0 total_withdrawals = 0.0 summary_lines = [] for transaction in transactions: type, amount, description = transaction if type == \\"deposit\\": initial_balance += amount total_deposits += amount summary_lines.append(f\\"Deposited: {amount} - {description}\\") elif type == \\"withdraw\\": initial_balance -= amount total_withdrawals += amount summary_lines.append(f\\"Withdrew: {amount} - {description}\\") summary = \\"n\\".join(summary_lines) return { \\"final_balance\\": initial_balance, \\"total_deposits\\": total_deposits, \\"total_withdrawals\\": total_withdrawals, \\"summary\\": summary } # Test initial_balance = 1000.0 transactions = [ (\\"deposit\\", 200.0, \\"Salary\\"), (\\"withdraw\\", 50.0, \\"Groceries\\"), (\\"deposit\\", 150.0, \\"Freelance\\"), (\\"withdraw\\", 30.0, \\"Transport\\") ] print(process_transactions(initial_balance, transactions)) ``` Ensure your solution meets the above requirements and handles different scenarios gracefully.","solution":"def process_transactions(initial_balance, transactions): total_deposits = 0.0 total_withdrawals = 0.0 summary_lines = [] for transaction in transactions: type, amount, description = transaction if type == \\"deposit\\": initial_balance += amount total_deposits += amount summary_lines.append(f\\"Deposited: {amount} - {description}\\") elif type == \\"withdraw\\": initial_balance -= amount total_withdrawals += amount summary_lines.append(f\\"Withdrew: {amount} - {description}\\") summary = \\"n\\".join(summary_lines) return { \\"final_balance\\": initial_balance, \\"total_deposits\\": total_deposits, \\"total_withdrawals\\": total_withdrawals, \\"summary\\": summary }"},{"question":"**Coding Assessment Question:** You have been provided a dataset containing details of transactions made at a store. The dataset has the following columns: - `amount`: The total amount of the transaction. - `method`: The payment method used (e.g., \'cash\', \'credit\', \'debit\'). - `time`: The time of the day the transaction was made (e.g., \'morning\', \'afternoon\', \'evening\'). Your task is to utilize the seaborn library to analyze and visualize the distribution of transaction amounts (`amount`) using KDE plots. Specifically, you need to accomplish the following: 1. Load the dataset provided as a CSV file named `transactions.csv`. 2. Create a KDE plot showing the distribution of `amount` for each `time` segment (\'morning\', \'afternoon\', \'evening\'), using different colors for each segment. 3. Overlay KDE plots for different `method` segments (\'cash\', \'credit\', \'debit\') on the same plot so that one can observe the conditional distribution of `amount` with respect to the payment method. 4. Adjust the bandwidth of the KDE to be more sensitive by setting `bw_adjust=0.5`. 5. Normalize the KDE plots for each `method` segment to make them comparable. 6. Ensure that the KDE plots are filled and semi-transparent to better visualize overlapping areas. **Expected Output:** The code should load the data, create and display a single plot that meets the above specifications. **Input:** - A CSV file named `transactions.csv` with columns `amount`, `method`, and `time`. **Output:** - A KDE plot displayed using matplotlib. The plot should show: - Differences in the distribution of transaction amounts for different times of the day. - Conditional distributions for different payment methods overlaid on the same plot. ```python import seaborn as sns import pandas as pd # Load dataset transactions = pd.read_csv(\'transactions.csv\') # Set the seaborn theme sns.set_theme() # Create a KDE plot sns.kdeplot(data=transactions, x=\'amount\', hue=\'time\', fill=True, common_norm=False, palette=\\"tab10\\", alpha=0.5, bw_adjust=0.5) sns.kdeplot(data=transactions, x=\'amount\', hue=\'method\', fill=True, common_norm=False, alpha=0.5, linewidth=0) # Display plot import matplotlib.pyplot as plt plt.show() ``` **Constraints and Limitations:** - Ensure you handle the dataset loading and necessary data manipulations efficiently. - Remember to import any necessary libraries. - Make sure the plot is clear and informative, properly displaying labels and legends.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def generate_kde_plot(file_path): Generates a KDE plot showing the distribution of transaction amounts for each time segment and overlays KDE plots for different payment methods. Parameters: - file_path: str : the path to the CSV file containing the transactions data. Returns: None # Load dataset transactions = pd.read_csv(file_path) # Set the seaborn theme sns.set_theme() # Create a KDE plot for \'time\' sns.kdeplot(data=transactions, x=\'amount\', hue=\'time\', fill=True, common_norm=False, palette=\\"tab10\\", alpha=0.5, bw_adjust=0.5) # Overlay KDE plots for \'method\' sns.kdeplot(data=transactions, x=\'amount\', hue=\'method\', fill=True, common_norm=False, palette=\\"pastel\\", alpha=0.5, bw_adjust=0.5, linewidth=0) # Display plot plt.xlabel(\'Transaction Amount\') plt.ylabel(\'Density\') plt.title(\'KDE Plot of Transaction Amounts by Time and Method\') plt.legend(title=\'Legend\', loc=\'upper right\', labels=[\'Morning\', \'Afternoon\', \'Evening\', \'Cash\', \'Credit\', \'Debit\']) plt.show()"},{"question":"# Advanced Python Coding Assessment: Custom Cookie Manager **Objective:** You are required to create a custom cookie management class using Python\'s `http.cookies` module. This class will manage the creation, updating, and deletion of cookies, handling various cookie attributes according to the HTTP state management mechanism. **Problem Statement:** Implement a class named `CustomCookieManager` that provides the following functionalities: 1. **Initialization** - Initialize the cookie manager with an optional dictionary of cookies. 2. **Set Cookie** - Method: `set_cookie(key, value, path=\'/\', secure=False, httponly=False)` - Creates or updates a cookie with the specified key and value. Optionally, set the path, secure, and httponly attributes. 3. **Delete Cookie** - Method: `delete_cookie(key)` - Deletes the cookie with the specified key. 4. **Output Cookies** - Method: `output(header=\'Set-Cookie:\')` - Returns a string suitable to be sent as HTTP headers for all the cookies currently managed. **Constraints:** - Do not use any third-party libraries. - Cookie keys must adhere to the set of valid characters specified in the documentation. **Example Usage:** ```python from http.cookies import SimpleCookie class CustomCookieManager: def __init__(self, cookies=None): self.cookies = SimpleCookie() if cookies: self.cookies.load(cookies) def set_cookie(self, key, value, path=\'/\', secure=False, httponly=False): self.cookies[key] = value self.cookies[key][\'path\'] = path if secure: self.cookies[key][\'secure\'] = True if httponly: self.cookies[key][\'httponly\'] = True def delete_cookie(self, key): if key in self.cookies: del self.cookies[key] def output(self, header=\'Set-Cookie:\'): return self.cookies.output(header=header) # Example: Creating and managing cookies cookie_manager = CustomCookieManager() cookie_manager.set_cookie(\'user_id\', \'abc123\', secure=True, httponly=True) cookie_manager.set_cookie(\'session_id\', \'xyz789\', path=\'/session\') print(cookie_manager.output()) # Output should include: # Set-Cookie: user_id=abc123; Path=/; Secure; HttpOnly # Set-Cookie: session_id=xyz789; Path=/session ``` Your task is to complete the `CustomCookieManager` class based on the specification provided and demonstrate its functionality with the example usage. Be sure to handle edge cases and invalid inputs gracefully. **Evaluation Criteria:** - Correct implementation of the methods with proper handling of cookie attributes. - Adherence to HTTP state management specifications. - Robustness and error handling. - Clarity and quality of code.","solution":"from http.cookies import SimpleCookie class CustomCookieManager: def __init__(self, cookies=None): self.cookies = SimpleCookie() if cookies: self.cookies.load(cookies) def set_cookie(self, key, value, path=\'/\', secure=False, httponly=False): self.cookies[key] = value self.cookies[key][\'path\'] = path if secure: self.cookies[key][\'secure\'] = True if httponly: self.cookies[key][\'httponly\'] = True def delete_cookie(self, key): if key in self.cookies: del self.cookies[key] def output(self, header=\'Set-Cookie:\'): return self.cookies.output(header=header)"},{"question":"Objective: Demonstrate your understanding of the `cgitb` module by writing a Python script that handles exceptions and generates detailed traceback reports in both text and HTML formats. Problem Statement: You are provided with a Python script that performs some basic arithmetic operations. Your task is to modify this script to use the `cgitb` module to handle any uncaught exceptions by generating detailed traceback reports. Specifically, you should: 1. Enable the `cgitb` module to display tracebacks in the browser. 2. Catch any exceptions that occur during the execution of the script and generate traceback reports in both text and HTML formats. 3. Save the HTML report to a file named `error_report.html`. Script: ```python def divide(a, b): return a / b def main(): num1 = 10 num2 = 0 # This will cause a ZeroDivisionError result = divide(num1, num2) print(f\\"The result of division is {result}\\") if __name__ == \\"__main__\\": main() ``` Your Tasks: 1. Import the `cgitb` module and enable it with display set to `1`. 2. Wrap the main execution block with a try-except statement. 3. In the except block, generate a detailed traceback report in both text and HTML formats using `cgitb`. 4. Save the HTML report to a file named `error_report.html`. 5. Print the text report to the console. Expected Output: - The text traceback report should be printed to the console. - The HTML traceback report should be saved to a file named `error_report.html`. Constraints: - Ensure that the script handles exceptions gracefully and generates both text and HTML reports. - The file saved (`error_report.html`) should contain the detailed HTML traceback report. - The code should be written in clear and clean Python syntax. Example: When an exception occurs, the console output might look like this (simplified for representation): ``` Traceback (most recent call last): File \\"script.py\\", line 14, in <module> main() File \\"script.py\\", line 10, in main result = divide(num1, num2) File \\"script.py\\", line 2, in divide return a / b ZeroDivisionError: division by zero ``` And the generated `error_report.html` should contain the detailed HTML traceback report.","solution":"import cgitb # Enable cgitb with display set to 1 cgitb.enable(display=1, logdir=\\".\\") def divide(a, b): return a / b def main(): try: num1 = 10 num2 = 0 # This will cause a ZeroDivisionError result = divide(num1, num2) print(f\\"The result of division is {result}\\") except Exception as e: # Generate a detailed traceback report in text format and print to the console print(\\"An error occurred:\\", e) cgitb.handler() if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with developing a system to manage and process event logs. An event log contains entries with timestamps and event descriptions. Your goal is to provide functionality that organizes these logs, allows querying based on time ranges, and summarizes the frequency of different types of events. Specifications 1. **Input:** - A list of event log entries. Each entry is represented as a dictionary with two keys: `timestamp` and `event`. The `timestamp` is a string in the format `YYYY-MM-DD HH:MM:SS`, and `event` is a string describing the event. 2. **Output:** - Functions to: 1. Add a new event log entry. 2. Query logs within a specified time range. 3. Summarize the frequency of different events. 3. **Constraints:** - All timestamps in the logs will be unique. - The system should handle up to 10,000 log entries efficiently. 4. **Performance:** - Queries should be performed in logarithmic time complexity with respect to the number of log entries. Requirements 1. Implement a class `EventLogManager` with the following methods: ```python from typing import List, Dict from datetime import datetime class EventLogManager: def __init__(self): # Initialize your data structures here pass def add_event(self, timestamp: str, event: str) -> None: # Add a new event log entry pass def query_logs(self, start_time: str, end_time: str) -> List[Dict[str, str]]: # Query logs in the given time range pass def summarize_events(self) -> Dict[str, int]: # Return a summary of event frequencies pass ``` 2. **Detailed Function Descriptions:** - `__init__(self) -> None`: Initializes the data structures needed to store the log entries. - `add_event(self, timestamp: str, event: str) -> None`: Adds a new log entry. The timestamp is a string in the format `YYYY-MM-DD HH:MM:SS`, and the event is a string describing the event. Convert the timestamp to a `datetime` object for efficient storage and comparison. - `query_logs(self, start_time: str, end_time: str) -> List[Dict[str, str]]`: Returns all log entries within the specified time range. Both `start_time` and `end_time` are strings in the format `YYYY-MM-DD HH:MM:SS`. Convert them to `datetime` objects and return a list of dictionaries, each containing the `timestamp` and `event` of the matching entries. - `summarize_events(self) -> Dict[str, int]`: Returns a summary dictionary where the keys are event descriptions, and the values are the count of how many times each event occurred across all logs. Example Usage ```python # Example usage of EventLogManager event_manager = EventLogManager() # Adding events event_manager.add_event(\'2023-01-01 12:00:00\', \'UserLogin\') event_manager.add_event(\'2023-01-01 12:05:00\', \'FileUpload\') event_manager.add_event(\'2023-01-01 12:10:00\', \'UserLogout\') # Querying events within a time range logs = event_manager.query_logs(\'2023-01-01 12:00:00\', \'2023-01-01 12:06:00\') print(logs) # Expected Output: [{\'timestamp\': \'2023-01-01 12:00:00\', \'event\': \'UserLogin\'}, {\'timestamp\': \'2023-01-01 12:05:00\', \'event\': \'FileUpload\'}] # Summarizing event frequencies summary = event_manager.summarize_events() print(summary) # Expected Output: {\'UserLogin\': 1, \'FileUpload\': 1, \'UserLogout\': 1} ``` **Note:** You may assume that the input details are correctly formatted and do not need to handle input validation or error checking.","solution":"from typing import List, Dict from datetime import datetime from collections import defaultdict import bisect class EventLogManager: def __init__(self): self.logs = [] self.event_count = defaultdict(int) def add_event(self, timestamp: str, event: str) -> None: dt = datetime.strptime(timestamp, \'%Y-%m-%d %H:%M:%S\') bisect.insort(self.logs, (dt, event)) self.event_count[event] += 1 def query_logs(self, start_time: str, end_time: str) -> List[Dict[str, str]]: start_dt = datetime.strptime(start_time, \'%Y-%m-%d %H:%M:%S\') end_dt = datetime.strptime(end_time, \'%Y-%m-%d %H:%M:%S\') start_index = bisect.bisect_left(self.logs, (start_dt, \'\')) end_index = bisect.bisect_right(self.logs, (end_dt, \'{\')) result = [ {\'timestamp\': dt.strftime(\'%Y-%m-%d %H:%M:%S\'), \'event\': event} for dt, event in self.logs[start_index:end_index] ] return result def summarize_events(self) -> Dict[str, int]: return dict(self.event_count)"},{"question":"**Part 1: Function Implementation** Design a function `apply_operations(operations: List[Tuple[str, Any, Any]]) -> List[Any]` that takes a list of operations described by tuples and returns the results of these operations. Each tuple in the list will have the form `(operation_name: str, operand1: Any, operand2: Any)` where: - `operation_name` is a string naming a function from the `operator` module (e.g., \'add\', \'mul\'). - `operand1` is the first operand for the operation. - `operand2` is the second operand for the operation. Your function should apply the specified operation from the `operator` module to the given operands and return all the results in a list. **Constraints:** 1. The `operation_name` will always be a valid function name from the `operator` module. 2. Operands will be valid inputs for the corresponding operations. 3. You must use functions from the `operator` module only (no hardcoded operations). **Example:** ```python from typing import List, Tuple import operator def apply_operations(operations: List[Tuple[str, Any, Any]]) -> List[Any]: # Your implementation here # Example usage operations = [ (\'add\', 1, 2), (\'mul\', 3, 5), (\'truediv\', 10, 2), (\'concat\', \'Hello, \', \'World!\'), (\'eq\', 5, 5) ] print(apply_operations(operations)) ``` **Expected Output:** ``` [3, 15, 5.0, \'Hello, World!\', True] ``` **Part 2: Advanced Task** Now, design an advanced function `multi_apply_operations(operations: List[Dict[str, Any]]) -> List[Any]` that takes a list of operation descriptions in dictionary form and returns the results of these operations. Each dictionary in the list can have the following keys: - `\'operation\'`: A string naming a function from the `operator` module. - `\'operands\'`: A list of operands (size varies based on the operation). **Example:** ```python def multi_apply_operations(operations: List[Dict[str, Any]]) -> List[Any]: # Your implementation here # Example usage operations = [ {\'operation\': \'add\', \'operands\': [1, 2]}, {\'operation\': \'mul\', \'operands\': [3, 5]}, {\'operation\': \'truediv\', \'operands\': [10, 2]}, {\'operation\': \'concat\', \'operands\': [\'Hello, \', \'World!\']}, {\'operation\': \'is_\', \'operands\': [None, None]}, {\'operation\': \'getitem\', \'operands\': [[1, 2, 3], 1]} ] print(multi_apply_operations(operations)) ``` **Expected Output:** ``` [3, 15, 5.0, \'Hello, World!\', True, 2] ``` Implement the functions such that they handle these requirements efficiently.","solution":"from typing import List, Tuple, Any import operator def apply_operations(operations: List[Tuple[str, Any, Any]]) -> List[Any]: Takes a list of operations described by tuples and returns the results of these operations. Each tuple has the form (operation_name, operand1, operand2). results = [] for operation_name, operand1, operand2 in operations: operation_func = getattr(operator, operation_name) results.append(operation_func(operand1, operand2)) return results from typing import Dict def multi_apply_operations(operations: List[Dict[str, Any]]) -> List[Any]: Takes a list of operations described by dictionaries and returns the results of these operations. Each dictionary has the following keys: - \'operation\': A string naming a function from the operator module. - \'operands\': A list of operands (size varies based on the operation). results = [] for operation_dict in operations: operation_name = operation_dict[\'operation\'] operands = operation_dict[\'operands\'] operation_func = getattr(operator, operation_name) results.append(operation_func(*operands)) return results"},{"question":"# Objective: The goal of this assessment is to evaluate your ability to work with Python\'s Abstract Syntax Trees (AST) module. You will be required to write functions that tokenize Python code, parse it to an AST, modify the AST, and then convert it back to Python code. # Problem Statement: You are provided with a string containing Python code. Your task is to: 1. Parse the code into an AST. 2. Modify the AST to identify all integer literals and increment their values by 1. 3. Convert the modified AST back to Python code and return it as a string. # Input: - A single string `code` which contains valid Python code. Example: ```python code = \\"a = 2 + 3nb = 4\\" ``` # Output: - A single string which contains the modified Python code. Example: ```python \\"a = 3 + 4nb = 5\\" ``` # Constraints: - You can assume that the input `code` will always contain valid Python syntax. - You must use the Python `ast` module for parsing and modifying the AST. - Do not use regular expression or string-based manipulations to solve the problem. The solution must involve AST manipulation. - The solution should handle large inputs efficiently. # Function Signature: ```python def increment_integer_literals(code: str) -> str: pass ``` # Example: ```python # Input code = \\"a = 1nfor i in range(5):n print(i)nc = 8\\" # Output \\"a = 2nfor i in range(6):n print(i)nc = 9\\" ``` # Hints: 1. Use the `ast.parse` method to convert the code string to an AST. 2. Traverse the AST using the `ast.NodeTransformer`. 3. Convert the modified AST back to a code string using the `ast.unparse` method available in Python 3.9 and later. Consider the following steps to implement the solution: 1. Parse the input code to an AST. 2. Implement a custom `ast.NodeTransformer` to modify the integer literals. 3. Unparse the modified AST back to code. # Evaluation: Your solution will be evaluated based on: - Correctness: The output code should correctly reflect the incremented integer literals. - Compliance: The solution should use appropriate AST manipulation techniques as specified. - Efficiency: The solution should be able to handle large and complex inputs in a reasonable timeframe.","solution":"import ast class IncrementIntegerLiterals(ast.NodeTransformer): A custom NodeTransformer that increments all integer literals by 1. def visit_Constant(self, node): if isinstance(node.value, int): return ast.copy_location(ast.Constant(value=node.value + 1), node) return node def increment_integer_literals(code: str) -> str: # Parse the code into an AST tree = ast.parse(code) # Increment integer literals transformer = IncrementIntegerLiterals() modified_tree = transformer.visit(tree) # Convert the modified AST back to Python code return ast.unparse(modified_tree)"},{"question":"# Advanced Python310 Coding Assessment Problem Statement You are required to write a Python function that reads from a given configuration file, modifies certain values according to given specifications, and then writes the updated configuration back to a new file. You will use the `configparser` module to accomplish this. Specifications 1. Write a function `modify_config(file_path: str, modifications: dict) -> str` that performs the following tasks: - Reads the configuration from `file_path`. - Applies modifications specified in the `modifications` dictionary. - Writes the modified configuration to a new file and returns the path to the new file. Input - `file_path` (str): A string representing the path to the original configuration file. - `modifications` (dict): A dictionary containing sections and key-value pairs to be modified in the configuration file. The format of the dictionary will be: ```python { \'section1\': { \'key1\': \'new_value1\', \'key2\': \'new_value2\' }, \'section2\': { \'key1\': \'new_value1\' // More key-value pairs } } ``` Output - A string representing the path to the new configuration file with the modifications applied. Constraints - The original configuration file will follow the INI file structure. - The dictionary can include sections not present in the original file, which should be added to the new file. - Handle cases where the section or key might not be found in the original file gracefully. Example Consider the following configuration file `config.ini`: ``` [Settings] theme=light auto_save=true [User] name=John Doe email=john.doe@example.com ``` **Function Call:** ```python file_path = \\"config.ini\\" modifications = { \'Settings\': { \'theme\': \'dark\' }, \'User\': { \'email\': \'jane.doe@example.com\' }, \'NewSection\': { \'new_key\': \'new_value\' } } modified_file_path = modify_config(file_path, modifications) ``` **Expected Behavior:** The function should read `config.ini`, apply the modifications, and write the following content to a new file, e.g., `modified_config.ini`: ``` [Settings] theme=dark auto_save=true [User] name=John Doe email=jane.doe@example.com [NewSection] new_key=new_value ``` And the function should return the path of `modified_config.ini`. Performance Requirements - The function should handle large configuration files efficiently. - The loading, modification, and writing of configuration files should be optimized for performance. References Refer to the Python documentation on the `configparser` module for more details and functionalities: [configparser documentation](https://docs.python.org/3/library/configparser.html)","solution":"import configparser import os def modify_config(file_path: str, modifications: dict) -> str: config = configparser.ConfigParser() config.read(file_path) for section, keys in modifications.items(): if not config.has_section(section): config.add_section(section) for key, value in keys.items(): config.set(section, key, value) new_file_path = f\\"modified_{os.path.basename(file_path)}\\" with open(new_file_path, \'w\') as configfile: config.write(configfile) return new_file_path"},{"question":"# **Nested Tensors and Sequence Summation in PyTorch** Problem Statement You are provided with multiple sequences of varying lengths, and you need to process these sequences using PyTorch\'s nested tensors. Your task is to write a function that computes the sum of elements in each sequence and returns these sums as a regular PyTorch tensor. You will be given a list of 1D PyTorch tensors, each representing a sequence of varying lengths. You are required to: 1. Construct a nested tensor from the provided list. 2. Compute the sum of each sequence within the nested tensor. 3. Return the resulting sums as a 1D PyTorch tensor. **Function Signature:** ```python def nested_sequence_sums(sequences: list) -> torch.Tensor: pass ``` Input - `sequences` (list): A list of 1D PyTorch tensors, where each tensor represents a sequence of varying length. Output - `torch.Tensor`: A 1D tensor containing the sum of each sequence in the same order as provided. Constraints - Each sequence must be a 1D PyTorch tensor. - The number of sequences will not exceed 1000. - The length of each sequence will not exceed 1000. - The function must utilize PyTorch\'s nested tensor functionalities where appropriate. Example ```python import torch a = torch.tensor([1, 2, 3]) b = torch.tensor([4, 5]) c = torch.tensor([6, 7, 8, 9]) sequences = [a, b, c] result = nested_sequence_sums(sequences) print(result) # Expected output: tensor([ 6, 9, 30]) ``` **Explanation:** - The sum of sequence `[1, 2, 3]` is 6. - The sum of sequence `[4, 5]` is 9. - The sum of sequence `[6, 7, 8, 9]` is 30. Use this problem to test your understanding of nested tensors in PyTorch and to work with variable-length sequence data efficiently.","solution":"import torch def nested_sequence_sums(sequences): Computes the sum of elements in each sequence and returns these sums as a 1D PyTorch tensor. Parameters: sequences (list): A list of 1D PyTorch tensors, where each tensor represents a sequence of varying length. Returns: torch.Tensor: A 1D tensor containing the sum of each sequence. # Using list comprehension to compute the sum of each sequence sums = [seq.sum().item() for seq in sequences] # Converting the list of sums to a 1D PyTorch tensor return torch.tensor(sums)"},{"question":"HTTP Request and IP Address Management **Objective:** In this assessment, you are required to implement a Python function that fetches data from a given URL and processes it based on specific requirements. This task will test your proficiency with the `urllib`, `http.client`, and `ipaddress` modules. **Problem Statement:** Your task is to implement a function `fetch_and_process(url: str) -> dict` that performs the following operations: 1. Parses the provided URL and fetches the content using `urllib.request`. 2. Validates the HTTP response status code using `http.client` and ensures the response has a successful status (2xx). If the response is not successful, raise an appropriate exception. 3. Parse the critical data such as `IP address` from the fetched content. 4. Create an `IPv4Address` or `IPv6Address` object from the extracted IP address using the `ipaddress` module and perform basic operations. **Function Signature:** ```python def fetch_and_process(url: str) -> dict: pass ``` **Input:** - `url` (str): A string representing the URL to fetch data from. **Output:** The function should return a dictionary with the following keys: - `\'url\'`: The original URL provided. - `\'status_code\'`: The HTTP status code of the response. - `\'ip_address\'`: The extracted and validated IP address. - `\'is_global\'`: A boolean indicating whether the IP address is global or not (i.e., not in private address space). **Constraints:** - You must use `urllib.request` to fetch the content. - Use `http.client` to validate the response status code. - Use `ipaddress` to handle and manipulate the IP address. - You can assume the fetched content includes a JSON object with an `ip` key. **Example:** ```python def fetch_and_process(url: str) -> dict: import urllib.request import http.client import json from ipaddress import ip_address try: response = urllib.request.urlopen(url) except Exception as e: raise Exception(f\\"Error fetching URL: {e}\\") status_code = response.getcode() if not (200 <= status_code < 300): raise http.client.BadStatusLine(f\\"Received {status_code} from server\\") data = json.loads(response.read().decode()) ip_str = data.get(\'ip\') if not ip_str: raise ValueError(\\"No IP address found in the response\\") ip_obj = ip_address(ip_str) is_global = ip_obj.is_global return { \'url\': url, \'status_code\': status_code, \'ip_address\': str(ip_obj), \'is_global\': is_global, } # Example usage: # result = fetch_and_process(\\"http://example.com/api\\") # print(result) ``` **Notes:** - Ensure the function properly handles exceptions and edge cases. - Document your code for clarity. **Testing:** Provide at least 3 test cases to validate your implementation.","solution":"def fetch_and_process(url: str) -> dict: import urllib.request import http.client import json from ipaddress import ip_address try: response = urllib.request.urlopen(url) except Exception as e: raise Exception(f\\"Error fetching URL: {e}\\") status_code = response.getcode() if not (200 <= status_code < 300): raise http.client.BadStatusLine(f\\"Received {status_code} from server\\") data = json.loads(response.read().decode()) ip_str = data.get(\'ip\') if not ip_str: raise ValueError(\\"No IP address found in the response\\") ip_obj = ip_address(ip_str) is_global = ip_obj.is_global return { \'url\': url, \'status_code\': status_code, \'ip_address\': str(ip_obj), \'is_global\': is_global, }"},{"question":"**Question: Create and Manage Executable Python Zip Archives** Using the `zipapp` module, your task is to write a Python script that demonstrates various functionalities of the module. The script should perform the following actions: 1. **Creating a Zip Archive:** - Create a directory named `myapp` that contains two Python files: - `__main__.py` which prints \\"Welcome to myapp!\\" when executed. - `helper.py` which contains a function `greet(name)` that prints a greeting message (e.g., \\"Hello, {name}!\\"). - Use the `create_archive` function from the `zipapp` module to create a zip archive named `myapp.pyz` from the `myapp` directory. Include a shebang line specifying the default Python interpreter and ensure the archive is executable. - Compress the archive to reduce its size. 2. **Updating the Archive:** - Use the `create_archive` function to update the interpreter in the `myapp.pyz` file to use `#!/usr/bin/env python3`. 3. **Extracting Interpreter Information:** - Use the `get_interpreter` function to print the interpreter specified in the `myapp.pyz` file. 4. **Command-Line Interface Alternative:** - Provide command-line equivalent commands for each of the above steps. **Input and Output:** - The script should be written in a way that all actions are clearly executably sequentially with appropriate print statements to indicate the completion of each task. - Note the steps and commands used to achieve each task using the command-line interface. **Constraints:** - The script should handle exceptions and edge cases gracefully (e.g., missing files, directories, or invalid parameters). **Performance Requirements:** - The script should be efficient in handling file and directory operations. Minimize the use of unnecessary I/O operations. **Hints:** 1. Use the `subprocess` module to run command-line commands within the script. 2. Use `os` and `pathlib` modules to manage file and directory operations. **Example Output:** ```shell python create_zipapp.py Step 1: Directory \'myapp\' and its files created successfully. Step 2: Zip archive \'myapp.pyz\' created and compressed successfully. Step 3: Interpreter in \'myapp.pyz\' updated to \'#!/usr/bin/env python3\'. Step 4: Interpreter in \'myapp.pyz\' is \'#!/usr/bin/env python3\'. Command-Line Equivalent: Creating and compressing the zip archive: python -m zipapp myapp -p /usr/bin/env python -c Updating the interpreter: python -m zipapp myapp.pyz -p /usr/bin/env python3 Extracting interpreter information: python -m zipapp myapp.pyz --info ```","solution":"import os import subprocess from zipapp import create_archive, get_interpreter def main(): # Step 1: Create a directory named \'myapp\' with necessary Python files os.makedirs(\'myapp\', exist_ok=True) with open(\'myapp/__main__.py\', \'w\') as main_file: main_file.write(\'print(\\"Welcome to myapp!\\")n\') with open(\'myapp/helper.py\', \'w\') as helper_file: helper_file.write(\'def greet(name):n\') helper_file.write(\' print(f\\"Hello, {name}!\\")n\') print(\\"Step 1: Directory \'myapp\' and its files created successfully.\\") # Step 2: Create a zip archive named \'myapp.pyz\' from the \'myapp\' directory create_archive(\'myapp\', \'myapp.pyz\', compressed=True, interpreter=\'/usr/bin/env python\') print(\\"Step 2: Zip archive \'myapp.pyz\' created and compressed successfully.\\") # Step 3: Update the interpreter in the \'myapp.pyz\' file create_archive(\'myapp\', \'myapp.pyz\', interpreter=\'/usr/bin/env python3\') print(\\"Step 3: Interpreter in \'myapp.pyz\' updated to \'#!/usr/bin/env python3\'.\\") # Step 4: Extract and print the interpreter information from the \'myapp.pyz\' file interpreter = get_interpreter(\'myapp.pyz\') print(f\\"Step 4: Interpreter in \'myapp.pyz\' is \'{interpreter}\'.\\") if __name__ == \\"__main__\\": main()"},{"question":"# Question: You are given a dataset of daily temperatures recorded at different weather stations over various years. Your task is to manipulate this data using `pandas` and visualize it using `seaborn`. 1. **Data Preparation**: - Convert the given data into long-form and wide-form data. - Handle any \\"messy\\" data ensuring it fits into a tidy format. 2. **Plotting**: - Create three distinct visualizations: 1. A line plot showing the yearly trend for each station using the long-form data. 2. A line plot showing the same trend using the wide-form data. 3. A box plot comparing temperature distributions across different stations. # Dataset You can assume the dataset is in the form of a CSV file with columns `station`, `date`, `temperature`. **Expected Functions and Specifications**: 1. **convert_to_long_form()** - **Input**: pandas DataFrame - **Output**: pandas DataFrame in long-form - **Note**: Ensure output is suitable for seaborn long-form input. 2. **convert_to_wide_form()** - **Input**: pandas DataFrame in long-form - **Output**: pandas DataFrame in wide-form - **Constraints**: Ensure correct column indexing and pivoting. 3. **plot_yearly_trend_long_form()** - **Input**: pandas DataFrame in long-form - **Output**: Matplotlib/Seaborn plot object - **Task**: Plot yearly trend of temperature for each station. 4. **plot_yearly_trend_wide_form()** - **Input**: pandas DataFrame in wide-form - **Output**: Matplotlib/Seaborn plot object - **Task**: Plot yearly trend of temperature for each station. 5. **plot_temperature_distribution()** - **Input**: pandas DataFrame in long-form - **Output**: Matplotlib/Seaborn plot object - **Task**: Create a box plot comparing temperature distributions across different stations. **Example Code**: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def convert_to_long_form(df): # Implement long-form conversion pass def convert_to_wide_form(df_long): # Implement wide-form conversion pass def plot_yearly_trend_long_form(df_long): # Implement plotting for long-form data pass def plot_yearly_trend_wide_form(df_wide): # Implement plotting for wide-form data pass def plot_temperature_distribution(df_long): # Implement box plot for temperature distribution pass # Assuming you have read the data into `df` df = pd.read_csv(\'temperature_data.csv\') # Convert the data df_long = convert_to_long_form(df) df_wide = convert_to_wide_form(df_long) # Plot using seaborn plot_yearly_trend_long_form(df_long) plot_yearly_trend_wide_form(df_wide) plot_temperature_distribution(df_long) plt.show() ``` **Additional Requirements**: - Your solution should handle missing values appropriately. - Provide meaningful labels and titles for your plots.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def convert_to_long_form(df): Converts the given dataframe to a long-form dataframe. df_long = pd.melt(df, id_vars=[\\"station\\", \\"date\\"], var_name=\\"variable\\", value_name=\\"value\\") return df_long def convert_to_wide_form(df_long): Converts the given long-form dataframe to a wide-form dataframe. df_wide = df_long.pivot(index=\\"date\\", columns=\\"station\\", values=\\"value\\") df_wide.reset_index(drop=False, inplace=True) return df_wide def plot_yearly_trend_long_form(df_long): Plots the yearly trend of temperature for each station using long-form data. df_long[\'date\'] = pd.to_datetime(df_long[\'date\']) df_long[\'year\'] = df_long[\'date\'].dt.year plt.figure(figsize=(15, 7)) sns.lineplot(data=df_long, x=\\"year\\", y=\\"value\\", hue=\\"station\\") plt.title(\'Yearly Temperature Trend for Each Station (Long Form)\') plt.xlabel(\'Year\') plt.ylabel(\'Temperature\') plt.legend(title=\'Station\') plt.show() def plot_yearly_trend_wide_form(df_wide): Plots the yearly trend of temperature for each station using wide-form data. df_wide[\'date\'] = pd.to_datetime(df_wide[\'date\']) df_wide[\'year\'] = df_wide[\'date\'].dt.year df_wide_melted = df_wide.melt(id_vars=\\"year\\", var_name=\\"station\\", value_name=\\"value\\") plt.figure(figsize=(15, 7)) sns.lineplot(data=df_wide_melted, x=\\"year\\", y=\\"value\\", hue=\\"station\\") plt.title(\'Yearly Temperature Trend for Each Station (Wide Form)\') plt.xlabel(\'Year\') plt.ylabel(\'Temperature\') plt.legend(title=\'Station\') plt.show() def plot_temperature_distribution(df_long): Creates a box plot comparing temperature distributions across different stations. plt.figure(figsize=(15, 7)) sns.boxplot(data=df_long, x=\\"station\\", y=\\"value\\") plt.title(\'Temperature Distribution Across Different Stations\') plt.xlabel(\'Station\') plt.ylabel(\'Temperature\') plt.show()"},{"question":"# Question # String Manipulation and Analysis with PyUnicode You are tasked with implementing a function that performs various string operations using the provided PyUnicode API. Your function should: 1. **Concatenate**: Join two Unicode strings. 2. **Find**: Locate the position of a substring within a string. 3. **Replace**: Replace occurrences of a substring with another substring. 4. **Character Check**: Check if specific characters are present within the string. Function Signature ```python def unicode_string_operations( s1: str, s2: str, substr: str, repl: str, check_chars: list ) -> dict: Args: s1 (str): The first Unicode string. s2 (str): The second Unicode string. substr (str): The substring to be found and replaced. repl (str): The replacement string. check_chars (list): A list of characters to check in the concatenated string. Returns: dict: A dictionary containing results of operations as follows: { \\"concatenated\\": <concatenated string>, \\"substr_position\\": <position of substring in concatenated string>, \\"replaced_string\\": <string after replacement>, \\"char_presence\\": <dict with chars as keys and boolean values indicating their presence> } pass ``` Description 1. **Concatenate**: Combine `s1` and `s2` to form a new string. 2. **Find**: Determine the position of `substr` in the concatenated string. Return `-1` if the substring is not found. 3. **Replace**: Replace all occurrences of `substr` in the concatenated string with `repl`. 4. **Character Check**: Check if each character in the `check_chars` list is present in the concatenated string. Return a dictionary with characters as keys and boolean values indicating their presence. Constraints - Strings `s1`, `s2`, `subst`, and `repl` may contain any Unicode characters. - The function should handle strings efficiently, even with large sizes. - Ensure that the function uses the appropriate PyUnicode API methods where necessary for optimal performance. Example ```python s1 = \\"hello\\" s2 = \\" world\\" substr = \\"o\\" repl = \\"O\\" check_chars = [\\"h\\", \\"w\\", \\"O\\"] result = unicode_string_operations(s1, s2, substr, repl, check_chars) ``` Expected output: ```python { \\"concatenated\\": \\"hello world\\", \\"substr_position\\": 4, \\"replaced_string\\": \\"hellO wOrld\\", \\"char_presence\\": { \\"h\\": True, \\"w\\": True, \\"O\\": True } } ``` Implement the `unicode_string_operations` function to fulfill the requirements described above.","solution":"def unicode_string_operations( s1: str, s2: str, substr: str, repl: str, check_chars: list ) -> dict: Args: s1 (str): The first Unicode string. s2 (str): The second Unicode string. substr (str): The substring to be found and replaced. repl (str): The replacement string. check_chars (list): A list of characters to check in the concatenated string. Returns: dict: A dictionary containing results of operations as follows: { \\"concatenated\\": <concatenated string>, \\"substr_position\\": <position of substring in concatenated string>, \\"replaced_string\\": <string after replacement>, \\"char_presence\\": <dict with chars as keys and boolean values indicating their presence> } concatenated = s1 + s2 substr_position = concatenated.find(substr) replaced_string = concatenated.replace(substr, repl) char_presence = {char: char in concatenated for char in check_chars} return { \\"concatenated\\": concatenated, \\"substr_position\\": substr_position, \\"replaced_string\\": replaced_string, \\"char_presence\\": char_presence }"},{"question":"# Type Hinting Function with Custom GenericAlias You are given the task to implement a function that takes a type and a variable number of arguments to create a generic alias similar to how type hinting works internally in Python. Task: Implement the function `create_generic_alias` that mimics the behavior of Python\'s internal `Py_GenericAlias` function. Your function should create an object with attributes similar to a `GenericAlias` object. Requirements: 1. The function should be named `create_generic_alias`. 2. It should take a class as the first argument and a variable number of additional arguments. 3. It should return an object with at least the following attributes: - `__origin__`: The class provided as the first argument. - `__args__`: A tuple of the additional arguments provided. 4. If only one additional argument is provided and it is not a tuple, it should automatically create a one-tuple containing that argument. 5. It should lazily construct an attribute `__parameters__` which is merely a list of the types in `__args__`. Example: ```python class MyGeneric: pass generic_alias = create_generic_alias(MyGeneric, int, str) print(generic_alias.__origin__) # Expected output: <class \'__main__.MyGeneric\'> print(generic_alias.__args__) # Expected output: (int, str) print(generic_alias.__parameters__) # Expected output: [int, str] single_arg_alias = create_generic_alias(MyGeneric, int) print(single_arg_alias.__origin__) # Expected output: <class \'__main__.MyGeneric\'> print(single_arg_alias.__args__) # Expected output: (int,) print(single_arg_alias.__parameters__) # Expected output: [int] ``` Note: - You should avoid importing `types.GenericAlias` and should not use existing GenericAlias implementations from Python libraries. - This task examines your understanding of class construction and attributes in Python and how you manage variable input arguments.","solution":"def create_generic_alias(cls, *args): Creates a generic alias similar to Python\'s internal Py_GenericAlias. Args: cls (type): The class to be used as the origin. *args: Variable number of arguments representing the types. Returns: object: An object with attributes similar to a GenericAlias. if len(args) == 1 and not isinstance(args[0], tuple): args = (args[0],) class GenericAlias: __origin__ = cls __args__ = args @property def __parameters__(self): return list(self.__args__) return GenericAlias()"},{"question":"You are tasked with creating a Python function `serialize_email` that takes an `EmailMessage` object and serializes it using the `email.generator.BytesGenerator` class, adhering to specific requirements. # Requirements: 1. The function must accept an `EmailMessage` object and return a byte string representing the serialized email. 2. Any line in the body starting with \\"From \\" should be mangled by prefixing it with a \\">\\" character. 3. The headers should be wrapped at 78 characters. 4. The Unix mailbox format envelope header should be included in the output. # Constraints: - Use the `BytesGenerator` from the `email.generator` module. - Follow the policies as specified in the requirements. Do not use the default policy. - Assume the `EmailMessage` passed to the function is properly constructed and non-empty. # Function Signature: ```python from email.message import EmailMessage def serialize_email(email_msg: EmailMessage) -> bytes: pass ``` # Input: - `email_msg`: An `EmailMessage` object containing the email structure to be serialized. # Output: - A byte string representing the serialized email. # Example: ```python from email.message import EmailMessage msg = EmailMessage() msg.set_content(\\"From here to there\\") msg[\'Subject\'] = \'Test email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' output = serialize_email(msg) print(output) ``` Expected output: ```plaintext b\'From sender@example.comnTo: recipient@example.comnSubject: Test emailnMIME-Version: 1.0nContent-Type: text/plain; charset=\\"utf-8\\"nContent-Transfer-Encoding: 7bitnn>From here to there\' ``` # Notes: - Ensure that the function correctly handles mangling of \\"From \\" lines. - Ensure that headers are properly wrapped at 78 characters. - Include the Unix mailbox format envelope header at the start of the serialized message.","solution":"from email.message import EmailMessage from email.generator import BytesGenerator import email.policy from io import BytesIO import time def serialize_email(email_msg: EmailMessage) -> bytes: Serializes the provided EmailMessage object to bytes. - Mangling any \\"From \\" line in the body by prefixing it with \\">\\" - Wrapping headers at 78 characters - Including Unix mailbox format envelope header # Ensure policy is set to mangle From lines and wrap headers properly policy = email.policy.default.clone(linesep=\'n\', max_line_length=78, mangle_from_=True) # Create a BytesIO stream to write the bytes to buffer = BytesIO() # Create a BytesGenerator with the specified policy generator = BytesGenerator(buffer, policy=policy) # Write the Unix mailbox format envelope header envelope_header = f\\"From {email_msg[\'from\']} {time.ctime(time.time())}n\\" buffer.write(envelope_header.encode(\'ascii\')) # Serialize the email message to the buffer generator.flatten(email_msg, unixfrom=False) # Get the byte string from the buffer serialized_bytes = buffer.getvalue() return serialized_bytes"},{"question":"# Advanced Unicode Handling in Python **Objective**: The goal of this task is to implement a function that demonstrates advanced handling of Unicode strings in Python. The function should read a text file containing Unicode text, normalize the text, and then write it to a new file with the characters sorted by their Unicode code points. Problem Statement You are required to write a Python function `normalize_and_sort_unicode` that performs the following tasks: 1. **Read the content** of a given input text file which contains Unicode characters. 2. **Normalize** the Unicode text using Normalization Form C (NFC). 3. **Sort** the normalized characters by their Unicode code points. 4. **Write** the sorted characters to an output text file. Function Signature ```python def normalize_and_sort_unicode(input_file: str, output_file: str) -> None: pass ``` Input - `input_file`: A string representing the path to the input text file containing Unicode text. - `output_file`: A string representing the path to the output text file where the sorted and normalized Unicode text should be written. Output - The function does not return a value. It writes the normalized and sorted Unicode characters to the specified output file. Constraints - The input file may contain any valid Unicode characters. - Ensure that the function can handle files up to 10 MB in size efficiently. - Use UTF-8 encoding for reading and writing files. - Handle any potential file I/O errors gracefully. Example Assume `input_file.txt` contains the following Unicode characters: ``` uF8x20xE9N{TAMIL NUMBER ONE THOUSAND}N{LATIN CAPITAL LETTER A} ``` Here\'s how the function should process this input: - Normalize the text to NFC form. - Sort the characters by Unicode code points. - Write the sorted characters to the output file. Example code to demonstrate the expected behavior: ```python def normalize_and_sort_unicode(input_file: str, output_file: str) -> None: import unicodedata try: with open(input_file, \'r\', encoding=\'utf-8\') as f: content = f.read() normalized_content = unicodedata.normalize(\'NFC\', content) sorted_characters = sorted(normalized_content, key=lambda char: ord(char)) with open(output_file, \'w\', encoding=\'utf-8\') as f: f.write(\'\'.join(sorted_characters)) except (IOError, OSError) as e: print(f\\"File operation failed: {e}\\") # Example usage normalize_and_sort_unicode(\'input_file.txt\', \'output_file.txt\') ``` **Note**: Ensure that your implementation adheres to the described function signature and handles potential exceptions as specified.","solution":"import unicodedata def normalize_and_sort_unicode(input_file: str, output_file: str) -> None: try: # Read the content of the input file with UTF-8 encoding with open(input_file, \'r\', encoding=\'utf-8\') as f: content = f.read() # Normalize the Unicode text to Normalization Form C (NFC) normalized_content = unicodedata.normalize(\'NFC\', content) # Sort the normalized characters by their Unicode code points sorted_characters = sorted(normalized_content) # Write the sorted characters to the output file with UTF-8 encoding with open(output_file, \'w\', encoding=\'utf-8\') as f: f.write(\'\'.join(sorted_characters)) except (IOError, OSError) as e: print(f\\"File operation failed: {e}\\")"},{"question":"# **XML Document Manipulation using DOM** **Objective:** Implement a Python function that constructs an XML document using the DOM API, modifies elements within that document, and performs specific queries to return desired results. **Task:** 1. **Create an XML Document**: - Build an XML document with the following structure: ```xml <library> <book id=\\"1\\"> <title>Book One</title> <author>Author One</author> <genre>Fiction</genre> </book> <book id=\\"2\\"> <title>Book Two</title> <author>Author Two</author> <genre>Science</genre> </book> <book id=\\"3\\"> <title>Book Three</title> <author>Author Three</author> <genre>Fiction</genre> </book> </library> ``` 2. **Add a New Book**: - Add a new book element to the document with the following data: - id: 4 - title: \\"Book Four\\" - author: \\"Author Four\\" - genre: \\"History\\" 3. **Modify Existing Book**: - Change the genre of the book with id \\"2\\" from \\"Science\\" to \\"Technology\\". 4. **Query Books by Genre**: - Implement a function `get_books_by_genre(document, genre)` that takes the document and a genre string as input, and returns a list of titles of books that belong to that genre. 5. **Remove a Book by Id**: - Implement a function `remove_book_by_id(document, book_id)` that takes the document and a book id string as input and removes the book element with the given id from the document. **Constraints**: - You must use the `xml.dom` module to achieve the tasks. - Ensure the functions handle errors gracefully, such as attempting to modify or remove a non-existent book or an invalid genre query. **Expected Function Signatures**: ```python import xml.dom.minidom def create_initial_document(): # returns: xml.dom.minidom.Document pass def add_new_book(document, book_id, title, author, genre): # params: xml.dom.minidom.Document, str, str, str, str # returns: None pass def modify_book_genre(document, book_id, new_genre): # params: xml.dom.minidom.Document, str, str # returns: None pass def get_books_by_genre(document, genre): # params: xml.dom.minidom.Document, str # returns: List[str] pass def remove_book_by_id(document, book_id): # params: xml.dom.minidom.Document, str # returns: None pass ``` **Example Execution**: ```python doc = create_initial_document() add_new_book(doc, \\"4\\", \\"Book Four\\", \\"Author Four\\", \\"History\\") modify_book_genre(doc, \\"2\\", \\"Technology\\") titles_fiction = get_books_by_genre(doc, \\"Fiction\\") print(titles_fiction) # Output: [\'Book One\', \'Book Three\'] remove_book_by_id(doc, \\"3\\") titles_fiction = get_books_by_genre(doc, \\"Fiction\\") print(titles_fiction) # Output: [\'Book One\'] ``` Good luck!","solution":"import xml.dom.minidom def create_initial_document(): Creates an initial XML document with a library containing three books. Returns the XML document. doc = xml.dom.minidom.Document() # Create root element library = doc.createElement(\\"library\\") doc.appendChild(library) # Define books data books_data = [ {\'id\': \'1\', \'title\': \'Book One\', \'author\': \'Author One\', \'genre\': \'Fiction\'}, {\'id\': \'2\', \'title\': \'Book Two\', \'author\': \'Author Two\', \'genre\': \'Science\'}, {\'id\': \'3\', \'title\': \'Book Three\', \'author\': \'Author Three\', \'genre\': \'Fiction\'}, ] # Create and append book elements for book_data in books_data: book_element = doc.createElement(\\"book\\") book_element.setAttribute(\\"id\\", book_data[\'id\']) title_element = doc.createElement(\\"title\\") title_element.appendChild(doc.createTextNode(book_data[\'title\'])) author_element = doc.createElement(\\"author\\") author_element.appendChild(doc.createTextNode(book_data[\'author\'])) genre_element = doc.createElement(\\"genre\\") genre_element.appendChild(doc.createTextNode(book_data[\'genre\'])) book_element.appendChild(title_element) book_element.appendChild(author_element) book_element.appendChild(genre_element) library.appendChild(book_element) return doc def add_new_book(document, book_id, title, author, genre): Adds a new book to the given XML document. library = document.documentElement book_element = document.createElement(\\"book\\") book_element.setAttribute(\\"id\\", book_id) title_element = document.createElement(\\"title\\") title_element.appendChild(document.createTextNode(title)) author_element = document.createElement(\\"author\\") author_element.appendChild(document.createTextNode(author)) genre_element = document.createElement(\\"genre\\") genre_element.appendChild(document.createTextNode(genre)) book_element.appendChild(title_element) book_element.appendChild(author_element) book_element.appendChild(genre_element) library.appendChild(book_element) def modify_book_genre(document, book_id, new_genre): Modifies the genre of the book with the given id in the XML document. book_elements = document.getElementsByTagName(\\"book\\") for book in book_elements: if book.getAttribute(\\"id\\") == book_id: genre_elements = book.getElementsByTagName(\\"genre\\") if genre_elements: genre_elements[0].childNodes[0].nodeValue = new_genre break def get_books_by_genre(document, genre): Returns a list of titles of books that belong to the given genre. book_elements = document.getElementsByTagName(\\"book\\") titles = [] for book in book_elements: genre_elements = book.getElementsByTagName(\\"genre\\") if genre_elements and genre_elements[0].childNodes[0].nodeValue == genre: title_elements = book.getElementsByTagName(\\"title\\") if title_elements: titles.append(title_elements[0].childNodes[0].nodeValue) return titles def remove_book_by_id(document, book_id): Removes the book element with the given id from the XML document. book_elements = document.getElementsByTagName(\\"book\\") for book in book_elements: if book.getAttribute(\\"id\\") == book_id: book.parentNode.removeChild(book) break"},{"question":"<|Analysis Begin|> The provided documentation details the functionalities of the `zipapp` module in Python. This module is used to create, manipulate, and manage Python zip application archives, which can be executed directly by the Python interpreter. The documentation covers both command-line interface (CLI) usage and Python API usage, along with examples and specific options. Key Functions and Concepts in `zipapp`: 1. **Command-Line Interface**: - Creating an archive from a directory: `python -m zipapp source` - Specifying the main function: `-m \\"pkg.mod:fn\\"` - Adding a shebang line: `-p <interpreter>` - Compressing files: `--compress` 2. **Python API**: - `zipapp.create_archive(source, target=None, interpreter=None, main=None, filter=None, compressed=False)`: Creates an application archive from a given source directory or existing archive. - `zipapp.get_interpreter(archive)`: Returns the interpreter specified in the shebang line of the archive. Given the functionalities and the depth of the `zipapp` module, a suitable coding assessment question can be developed to test a student\'s understanding of creating a Python zip application archive, filtering files to be included, specifying a main function, and adding a shebang line. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective Create a program that packages a specified directory into a Python zip application archive. The archive should only include `.py` files and should include an execution entry point. Additionally, the archive should include a shebang line indicating the Python interpreter to be used. Instructions 1. Implement a function `package_directory(directory_path, output_path, main_function, interpreter, compressed)` that meets the following criteria: - **Input**: - `directory_path` (str): Path to the directory to be packaged. - `output_path` (str): Path to the output zip application archive. - `main_function` (str): The main function to be executed when the archive runs. This should be specified in the form `\\"pkg.mod:fn\\"`. - `interpreter` (str): The Python interpreter to be added as a shebang line. - `compressed` (bool): Flag indicating whether to compress the files in the archive. - **Output**: - None. The function should create a zip application archive at the specified `output_path`. 2. The function should: - Include only `.py` files from the specified directory. - Add a `__main__.py` file that executes the specified main function. - Add a shebang line with the specified interpreter. - Compress the files if the `compressed` flag is set to `True`. Constraints - You may assume that the directory specified contains valid Python code. - Handle appropriate exception scenarios such as invalid directory paths or missing main functions. Example ```python import zipapp def package_directory(directory_path, output_path, main_function, interpreter, compressed): def filter_py_files(path): return path.suffix == \'.py\' zipapp.create_archive( source=directory_path, target=output_path, interpreter=interpreter, main=main_function, filter=filter_py_files, compressed=compressed ) # Example usage: # This will create an archive \'myapp.pyz\' from the \'myapp\' directory, with the main function \'myapp.main:main\'. directory_path = \'myapp\' output_path = \'myapp.pyz\' main_function = \'myapp.main:main\' interpreter = \'/usr/bin/env python3\' compressed = True package_directory(directory_path, output_path, main_function, interpreter, compressed) ``` Ensure that your function adheres to the above specifications. You may use the `zipapp` module as documented. Notes - The main function should correctly reference existing Python files within the directory. - Test your solution to ensure it correctly handles edge cases such as empty directories or directories without `.py` files.","solution":"import zipapp from pathlib import Path def package_directory(directory_path, output_path, main_function, interpreter, compressed): def filter_py_files(path): return path.suffix == \'.py\' directory_path = Path(directory_path) if not directory_path.is_dir(): raise ValueError(\\"The specified directory path does not exist or is not a directory.\\") main_file, main_func = main_function.split(\\":\\") main_file_path = directory_path / (main_file.replace(\\".\\", \\"/\\") + \\".py\\") if not main_file_path.is_file(): raise ValueError(f\\"The specified main module \'{main_file}\' does not exist in the directory.\\") zipapp.create_archive( source=str(directory_path), target=str(output_path), interpreter=interpreter, main=main_function, filter=filter_py_files, compressed=compressed )"},{"question":"You are tasked with writing a function that simulates a polite web crawler by respecting and adhering to the rules specified in a website\'s `robots.txt` file. Your function will determine whether a user agent can access a given URL and should also handle crawl delays and request rates. Function Signature ```python def should_crawl(useragent: str, url: str, robots_txt_url: str) -> bool: pass ``` Objective Your function should: 1. Check if the user agent is allowed to fetch the given URL according to the `robots.txt` rules. 2. Handle and respect crawl delays and request rates for the user agent if specified in the `robots.txt`. Input - `useragent` (str): The user agent string (e.g., `\\"my-web-crawler\\"`). - `url` (str): The URL that the user agent wants to fetch. - `robots_txt_url` (str): The URL where the `robots.txt` file can be found. Output - `bool`: Return `True` if the user agent is allowed to fetch the URL and adhere to the crawl delay and request rate rules. Otherwise, return `False`. Constraints - Ensure your function handles both specified and unspecified rules in the `robots.txt` file. - If the rules are invalid or the `robots.txt` file does not exist, the user agent should be assumed to have unrestricted access (i.e., return `True`). Performance Requirements - Efficiently handle fetching and parsing the `robots.txt` file. - Utilize Python 3.10 features for type hints and clean code structure. Example ```python # Example usage: result = should_crawl( useragent=\\"my-web-crawler\\", url=\\"http://example.com/some-page/\\", robots_txt_url=\\"http://example.com/robots.txt\\" ) print(result) # Expected output: True or False depending on the robots.txt rules ``` **Hint**: Use the `urllib.robotparser.RobotFileParser` class to handle fetching, parsing, and querying the `robots.txt` file.","solution":"import urllib.robotparser def should_crawl(useragent: str, url: str, robots_txt_url: str) -> bool: Determines if the user agent is allowed to fetch the given URL based on rules specified in the `robots.txt` file found at the `robots_txt_url`. :param useragent: The user agent string (e.g., `\\"my-web-crawler\\"`). :param url: The URL that the user agent wants to fetch. :param robots_txt_url: The URL where the `robots.txt` file can be found. :return: True if the user agent is allowed to fetch the URL, False otherwise. rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_txt_url) rp.read() if rp.can_fetch(useragent, url): crawl_delay = rp.crawl_delay(useragent) request_rate = rp.request_rate(useragent) # Handling crawl delay and request rate if specified if crawl_delay is not None or request_rate is not None: # For simplicity in this code snippet, we\'re assuming any specified delay/rate should be respected. # But return True for just permissions check as `can_fetch` already checks permission. return True else: return True else: return False"},{"question":"Objective: Create a custom data pipeline using scikit-learn transformers to preprocess a dataset, reduce its dimensionality, and then use the transformed data for training a simple machine learning model. The purpose of this exercise is to assess your ability to effectively use scikit-learn\'s transformation and pipeline capabilities. Problem Statement: 1. **Input:** - A CSV file named `data.csv` that contains the dataset. The dataset will have the features `X1`, `X2`, ..., `Xn` and the target column `y`. - Example format: ``` X1,X2,X3,...,Xn,y 1.0,2.0,3.0,...,4.0,yes 5.0,6.0,7.0,...,8.0,no ... ``` 2. **Requirements:** - **Preprocessing:** - Standardize the feature columns using `StandardScaler`. - Handle any missing data by imputing the mean value using `SimpleImputer`. - **Dimensionality Reduction:** - Reduce the dataset\'s dimensionality to 2 components using `PCA` (Principal Component Analysis). - **Model Training:** - Train a simple logistic regression model using the transformed features. - **Validation:** - Perform a 5-fold cross-validation to evaluate the model\'s performance and output the average accuracy. 3. **Output:** - A function `data_pipeline` that reads the CSV file, processes the data as specified, and prints the average cross-validation accuracy. Constraints: - You should use scikit-learn version 0.24 or higher. - Aim for clean, readable code with appropriate comments. - Do not use any additional external libraries except pandas, numpy, and scikit-learn. Function Signature: ```python def data_pipeline(file_path: str) -> None: pass ``` Example Usage: ```python # Assuming data.csv is in the current directory data_pipeline(\\"data.csv\\") ``` Notes: - You may assume that the dataset will not contain any non-numeric features in the feature columns. - The target column `y` will always be binary and can be encoded as `yes/no`. Hint: This exercise requires knowledge of using scikit-learn\'s `Pipeline` class to combine different transformers and estimators effectively. Ensure to fit and transform appropriately in the pipeline and use cross-validation functions from scikit-learn for model evaluation.","solution":"import pandas as pd from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score def data_pipeline(file_path: str) -> None: # Load the data data = pd.read_csv(file_path) # Separating features (X) and target (y) X = data.drop(columns=\'y\') y = data[\'y\'] # Encode target variable label_encoder = LabelEncoder() y = label_encoder.fit_transform(y) # Create a pipeline for preprocessing and model training pipeline = Pipeline([ (\'imputer\', SimpleImputer(strategy=\'mean\')), # Handling missing values (\'scaler\', StandardScaler()), # Standardizing features (\'pca\', PCA(n_components=2)), # Dimensionality reduction (\'logistic_regression\', LogisticRegression()) # Logistic regression model ]) # Perform 5-fold cross-validation scores = cross_val_score(pipeline, X, y, cv=5, scoring=\'accuracy\') # Print the average cross-validation accuracy print(f\\"Average cross-validation accuracy: {scores.mean():.4f}\\")"},{"question":"Objective You are tasked with creating customized visualizations using the seaborn library by adjusting the plotting context to suit different presentation needs. Problem Statement Write a function called `custom_plotting_context` that takes in three parameters: 1. `data`: A dictionary with keys as strings and values as lists of numerical data. Each key-value pair represents the name and corresponding values for plotting. 2. `context`: A string representing the seaborn predefined context to be used for plotting (`\\"paper\\"`, `\\"notebook\\"`, `\\"talk\\"`, or `\\"poster\\"`). 3. `context_params`: A dictionary of additional context parameters to customize the plotting context further. This is an optional parameter. The function should: 1. Create a line plot for each key-value pair in the `data` dictionary using the seaborn lineplot function. 2. Apply the specified `context` and `context_params` to customize the plot aesthetics. 3. Display the plots in a single figure with subplots arranged in a square grid layout (i.e., 2Ã—2 for 4 plots). Input - `data` (dict): A dictionary where keys are strings and values are lists of numerical data. ```python { \\"Series1\\": [1, 2, 3, 4], \\"Series2\\": [4, 3, 2, 1], \\"Series3\\": [2, 3, 2, 3], \\"Series4\\": [1, 3, 5, 7] } ``` - `context` (str): One of `\\"paper\\"`, `\\"notebook\\"`, `\\"talk\\"`, or `\\"poster\\"`. ```python \\"talk\\" ``` - `context_params` (dict, optional): Additional context parameters to adjust the plotting context. Default is an empty dictionary. ```python { \\"font.size\\": 14, \\"axes.labelsize\\": 16, \\"axes.titlesize\\": 18 } ``` Output - The function does not return anything. Instead, it should display the plots according to the specified context and layout. Constraints - The `data` dictionary can have up to 4 key-value pairs. - Ensure that the plots are displayed in a square grid layout. Example ```python import seaborn as sns import matplotlib.pyplot as plt def custom_plotting_context(data, context, context_params=None): if context_params is None: context_params = {} with sns.plotting_context(context, rc=context_params): fig, axs = plt.subplots(2, 2, figsize=(10, 10)) axs = axs.flatten() for idx, (key, values) in enumerate(data.items()): sns.lineplot(ax=axs[idx], x=range(len(values)), y=values) axs[idx].set_title(key) plt.tight_layout() plt.show() data = { \\"Series1\\": [1, 2, 3, 4], \\"Series2\\": [4, 3, 2, 1], \\"Series3\\": [2, 3, 2, 3], \\"Series4\\": [1, 3, 5, 7] } context = \\"talk\\" context_params = { \\"font.size\\": 14, \\"axes.labelsize\\": 16, \\"axes.titlesize\\": 18 } custom_plotting_context(data, context, context_params) ``` This will create and display a 2Ã—2 grid of line plots with the specified context settings applied.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_plotting_context(data, context, context_params=None): Creates a customized line plot for each key-value pair in the data dictionary. Parameters: - data (dict): A dictionary where keys are strings and values are lists of numerical data. - context (str): One of \\"paper\\", \\"notebook\\", \\"talk\\", or \\"poster\\". - context_params (dict, optional): Additional context parameters to adjust the plotting context. Returns: None. Displays the plots according to the specified context and layout. if context_params is None: context_params = {} with sns.plotting_context(context, rc=context_params): fig, axs = plt.subplots(2, 2, figsize=(10, 10)) axs = axs.flatten() for idx, (key, values) in enumerate(data.items()): sns.lineplot(ax=axs[idx], x=range(len(values)), y=values) axs[idx].set_title(key) plt.tight_layout() plt.show()"},{"question":"# Advanced Python310 Coding Assessment: Email Handling and MIME Content Objective The goal of this question is to assess your understanding of the `email` package in Python310, specifically your ability to create, parse, and manipulate email messages and MIME content. Problem Statement You are tasked with writing a function that constructs a MIME email message from scratch, and then parses it to extract and return specific details. The details to be extracted include: the subject, the sender\'s email, the recipient\'s email, and the plain text body of the email. Function Signature ```python def construct_and_parse_email(subject: str, sender: str, recipient: str, body: str) -> dict: pass ``` Input - `subject` (str): The subject of the email. - `sender` (str): The sender\'s email address. - `recipient` (str): The recipient\'s email address. - `body` (str): The plain text body content of the email. Output - A dictionary with the following keys: - `\\"subject\\"`: The subject of the email. - `\\"from\\"`: The sender\'s email address. - `\\"to\\"`: The recipient\'s email address. - `\\"body\\"`: The plain text body content of the email. Constraints - You may assume that the input strings are non-empty and valid for email construction. - You should use the `email` package functionalities to construct and parse the email. - The key names in the returned dictionary must exactly match the ones specified above. Example Usage ```python subject = \\"Meeting Reminder\\" sender = \\"alice@example.com\\" recipient = \\"bob@example.com\\" body = \\"Don\'t forget our meeting at 10 AM tomorrow.\\" result = construct_and_parse_email(subject, sender, recipient, body) assert result == { \\"subject\\": \\"Meeting Reminder\\", \\"from\\": \\"alice@example.com\\", \\"to\\": \\"bob@example.com\\", \\"body\\": \\"Don\'t forget our meeting at 10 AM tomorrow.\\" } ``` Guidelines for Implementation 1. **Email Construction**: Use the `email` package to create a MIMEText email message with the given subject, sender, recipient, and body. 2. **Email Parsing**: Parse the constructed email to retrieve the required details and return them in a dictionary. 3. Ensure that you make use of the appropriate classes and methods provided by the `email` package, such as `email.message.EmailMessage`, `email.parser.Parser`, and the content manager for handling MIME content. Good luck! May your code be bug-free and your logic impeccable.","solution":"from email.message import EmailMessage from email.parser import Parser def construct_and_parse_email(subject: str, sender: str, recipient: str, body: str) -> dict: # Construct the email msg = EmailMessage() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient msg.set_content(body) # Parse the email parsed_msg = Parser().parsestr(msg.as_string()) # Extracting required details email_details = { \\"subject\\": parsed_msg[\'Subject\'], \\"from\\": parsed_msg[\'From\'], \\"to\\": parsed_msg[\'To\'], \\"body\\": parsed_msg.get_payload() } return email_details"},{"question":"**Objective**: Demonstrate understanding of the `doctest` module by writing Python functions with docstring examples and verifying them using `doctest`. --- Problem Statement You are tasked with writing a Python module that contains a few mathematical functions. Each function should include interactive Python examples in its docstring to demonstrate its usage. Additionally, write a script to perform doctests on the module and verify that all the examples produce the expected results. Functions to Implement: 1. **is_prime(n)**: - Input: An integer `n` (0 <= n <= 1000). - Output: Returns `True` if `n` is a prime number, `False` otherwise. - Docstring Example: ```python >>> is_prime(7) True >>> is_prime(10) False >>> is_prime(2) True >>> is_prime(1) False >>> is_prime(0) False ``` 2. **gcd(a, b)**: - Input: Two integers `a` and `b` (0 <= a, b <= 1000). - Output: Returns the greatest common divisor of `a` and `b`. - Docstring Example: ```python >>> gcd(48, 18) 6 >>> gcd(100, 10) 10 >>> gcd(7, 1) 1 >>> gcd(54, 24) 6 >>> gcd(0, 0) 0 ``` 3. **fibonacci(n)**: - Input: An integer `n` (0 <= n <= 30). - Output: Returns the `n`-th Fibonacci number. - Docstring Example: ```python >>> fibonacci(0) 0 >>> fibonacci(1) 1 >>> fibonacci(5) 5 >>> fibonacci(10) 55 >>> fibonacci(15) 610 ``` --- Requirements: 1. Implement the three functions with appropriate docstring examples. 2. Create a separate script that imports this module and performs doctests to verify that all the docstring examples are correct. 3. Ensure that the script runs doctests and prints a summary of the tests. Constraints: - The functions should handle edge cases as specified in the input ranges. - Ensure that the functions are optimized for performance wherever applicable. Deliverables: 1. A Python module file (e.g., `math_functions.py`) containing the three functions with their respective docstrings. 2. A Python script (e.g., `run_tests.py`) that runs the doctests on the module and prints a summary of the results. --- # Example: Assuming you have implemented the `math_functions.py` module with the required functions and docstring examples, your `run_tests.py` might look like this: ```python import doctest import math_functions if __name__ == \\"__main__\\": doctest.testmod(math_functions, verbose=True) ``` Run the above script to ensure all the docstring examples pass and produce the expected results.","solution":"def is_prime(n): Returns True if n is a prime number, False otherwise. >>> is_prime(7) True >>> is_prime(10) False >>> is_prime(2) True >>> is_prime(1) False >>> is_prime(0) False if n <= 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True def gcd(a, b): Returns the greatest common divisor of a and b. >>> gcd(48, 18) 6 >>> gcd(100, 10) 10 >>> gcd(7, 1) 1 >>> gcd(54, 24) 6 >>> gcd(0, 0) 0 while b: a, b = b, a % b return a def fibonacci(n): Returns the nth Fibonacci number. >>> fibonacci(0) 0 >>> fibonacci(1) 1 >>> fibonacci(5) 5 >>> fibonacci(10) 55 >>> fibonacci(15) 610 if n == 0: return 0 elif n == 1: return 1 else: a, b = 0, 1 for _ in range(2, n + 1): a, b = b, a + b return b"},{"question":"**Objective**: Demonstrate your comprehension of the `fnmatch` module by solving a practical file matching problem. **Problem Statement**: You are given a list of filenames and a list of patterns. Your task is to implement a function that groups the filenames based on the patterns they match. Each pattern can match multiple filenames, and each filename can match multiple patterns. Return a dictionary where each pattern is a key, and the value is a list of filenames that match that pattern. **Function Signature**: ```python def group_filenames_by_pattern(filenames: list, patterns: list) -> dict: pass ``` **Input**: - `filenames`: A list of strings representing filenames. - `patterns`: A list of strings representing patterns. **Output**: - A dictionary where the keys are the patterns and the values are lists of filenames that match those patterns. **Constraints**: - Filenames and patterns will only contain ASCII characters. - Filenames can contain alphanumeric characters, periods, and underscores. - Patterns will follow Unix shell-style wildcards as described in the `fnmatch` documentation. **Example**: ```python filenames = [\\"file1.txt\\", \\"file2.txt\\", \\"data.csv\\", \\"script.py\\"] patterns = [\\"*.txt\\", \\"*.csv\\", \\"file?.*\\"] output = group_filenames_by_pattern(filenames, patterns) ``` **Expected Output**: ```python { \\"*.txt\\": [\\"file1.txt\\", \\"file2.txt\\"], \\"*.csv\\": [\\"data.csv\\"], \\"file?.*\\": [\\"file1.txt\\", \\"file2.txt\\"] } ``` **Requirements**: - Use the `fnmatch.fnmatch` function to match the patterns. - Consider edge cases such as no matching filenames for a pattern, or filenames that match multiple patterns. - Your solution should handle large lists efficiently.","solution":"import fnmatch def group_filenames_by_pattern(filenames: list, patterns: list) -> dict: Groups filenames by the patterns they match. :param filenames: List of filenames. :param patterns: List of patterns. :return: Dictionary with patterns as keys and lists of matching filenames as values. result = {} for pattern in patterns: matching_files = [filename for filename in filenames if fnmatch.fnmatch(filename, pattern)] result[pattern] = matching_files return result"},{"question":"# PyTorch Distributed Training and Checkpointing You are required to implement a PyTorch training script that is compliant with the `torchrun` utility for distributed training. This script must handle: - Initialization of the distributed process group. - Checkpointing to ensure recovery from failures. - Training across multiple epochs and batches. Your task is to complete the `train.py` script below. The script should: 1. Initialize the distributed process group using `torch.distributed.init_process_group`. 2. Load a checkpoint if available and save checkpoints at the end of each epoch. 3. Train a simple PyTorch model for a specified number of epochs. The script skeleton is provided below. You need to fill in the missing parts as indicated by the comments. Script: `train.py` ```python import os import sys import argparse import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP def parse_args(args_list): parser = argparse.ArgumentParser() parser.add_argument(\\"--backend\\", type=str, default=\\"nccl\\", choices=[\\"nccl\\", \\"gloo\\", \\"mpi\\"]) parser.add_argument(\\"--total_num_epochs\\", type=int, default=10) parser.add_argument(\\"--checkpoint_path\\", type=str, required=True) parser.add_argument(\\"--dataset_path\\", type=str, required=True) return parser.parse_args(args_list) def load_checkpoint(path): if os.path.exists(path): return torch.load(path) return {\\"epoch\\": 0, \\"model_state\\": None} def save_checkpoint(state, path): torch.save(state, path) def initialize(state, model): if state[\\"model_state\\"]: model.load_state_dict(state[\\"model_state\\"]) def main(): args = parse_args(sys.argv[1:]) rank = int(os.environ[\\"LOCAL_RANK\\"]) # Initialize process group dist.init_process_group(backend=args.backend) # Fill in with appropriate arguments # Create model and wrap with DDP model = torch.nn.Linear(10, 1) model = DDP(model) # Load checkpoint if available state = load_checkpoint(args.checkpoint_path) initialize(state, model) # Dummy dataset and optimizer dataset = [torch.randn(10) for _ in range(100)] optimizer = torch.optim.SGD(model.parameters(), lr=0.01) for epoch in range(state[\'epoch\'], args.total_num_epochs): for batch in dataset: optimizer.zero_grad() output = model(batch) loss = output.sum() loss.backward() optimizer.step() # Update state state[\'epoch\'] = epoch + 1 state[\'model_state\'] = model.state_dict() # Save checkpoint save_checkpoint(state, args.checkpoint_path) dist.destroy_process_group() if __name__ == \\"__main__\\": main() ``` # Constraints: - You may assume the dataset is a simple list of tensors as shown in the script. - Ensure that your script can recover from the most recent checkpoint in case of a failure. - Use the appropriate backend (`nccl`, `gloo`, `mpi`) based on the hardware and software environment. # Notes: - The script provided initializes a simple linear model and wraps it into a `DistributedDataParallel` wrapper for distributed training. - Your implementation should ensure that all necessary aspects of distributed training (initialization, synchronization, checkpointing) are appropriately handled.","solution":"import os import sys import argparse import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP def parse_args(args_list): parser = argparse.ArgumentParser() parser.add_argument(\\"--backend\\", type=str, default=\\"nccl\\", choices=[\\"nccl\\", \\"gloo\\", \\"mpi\\"]) parser.add_argument(\\"--total_num_epochs\\", type=int, default=10) parser.add_argument(\\"--checkpoint_path\\", type=str, required=True) parser.add_argument(\\"--dataset_path\\", type=str, required=True) return parser.parse_args(args_list) def load_checkpoint(path): if os.path.exists(path): return torch.load(path) return {\\"epoch\\": 0, \\"model_state\\": None} def save_checkpoint(state, path): torch.save(state, path) def initialize(state, model): if state[\\"model_state\\"]: model.load_state_dict(state[\\"model_state\\"]) def main(): args = parse_args(sys.argv[1:]) rank = int(os.environ[\\"LOCAL_RANK\\"]) # Initialize process group dist.init_process_group(backend=args.backend) # Create model and wrap with DDP model = torch.nn.Linear(10, 1) model = DDP(model, device_ids=[rank]) # Load checkpoint if available state = load_checkpoint(args.checkpoint_path) initialize(state, model) # Dummy dataset and optimizer dataset = [torch.randn(10) for _ in range(100)] optimizer = torch.optim.SGD(model.parameters(), lr=0.01) for epoch in range(state[\'epoch\'], args.total_num_epochs): for batch in dataset: optimizer.zero_grad() output = model(batch.unsqueeze(0)) loss = output.sum() loss.backward() optimizer.step() # Update state state[\'epoch\'] = epoch + 1 state[\'model_state\'] = model.state_dict() # Save checkpoint if rank == 0: # Only save checkpoint from rank 0 save_checkpoint(state, args.checkpoint_path) dist.destroy_process_group() if __name__ == \\"__main__\\": main()"},{"question":"Objective Implement a simple chat server and client using the `asyncio` streams module. The purpose is to demonstrate your understanding of asynchronous networking, particularly employing `StreamReader` and `StreamWriter`. Requirements 1. Implement an asynchronous chat server that can handle multiple clients simultaneously. 2. Implement an asynchronous chat client that can connect to the server. Chat Server - The server should listen on `localhost` and port `8888`. - When a client connects, the server should: 1. Announce the new connection to all other connected clients. 2. Relay any message received from one client to all connected clients. 3. Handle clients\' disconnections gracefully. - Use `asyncio.start_server` to manage incoming connections. - Utilize `StreamReader` and `StreamWriter` to read from and write to clients. Chat Client - The client should connect to the server at `localhost` on port `8888`. - Once connected, the client should: 1. Allow the user to type and send messages to the server asynchronously. 2. Display messages received from the server in real time. - Use `asyncio.open_connection` to establish the connection. - Utilize `StreamReader` and `StreamWriter` to read from and write to the server. Detailed Instructions - **Input/Output** for the server: - Receives messages as UTF-8 encoded strings from clients. - Sends messages as UTF-8 encoded strings to all connected clients. - **Input/Output** for the client: - User input is captured via the terminal. - Messages received from the server are displayed in the terminal. - **Concurrency**: - Your solution should handle concurrent connections and message flows effectively using `asyncio`. Example Usage **Chat Server:** ```python import asyncio async def handle_client(reader, writer): # Implement how to handle a new client connection async def main(): server = await asyncio.start_server( handle_client, \'localhost\', 8888) async with server: await server.serve_forever() asyncio.run(main()) ``` **Chat Client:** ```python import asyncio async def chat_client(): reader, writer = await asyncio.open_connection(\'localhost\', 8888) # Implement client-side interaction asyncio.run(chat_client()) ``` Constraints - Use Python 3.10 or higher. - Do not use any third-party libraries for networking. Your implementation of the chat server and client will be evaluated based on correctness, efficiency, and adherence to asynchronous programming principles.","solution":"import asyncio clients = [] async def broadcast_message(message, writer): for client_writer in clients: if client_writer != writer: client_writer.write(message.encode()) await client_writer.drain() async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') clients.append(writer) welcome_message = f\'Client {addr} connected.n\' await broadcast_message(welcome_message, writer) try: while True: data = await reader.read(100) message = data.decode() if not message: break await broadcast_message(message, writer) except (asyncio.CancelledError, ConnectionResetError): pass finally: clients.remove(writer) writer.close() await writer.wait_closed() goodbye_message = f\'Client {addr} disconnected.n\' await broadcast_message(goodbye_message, writer) async def main(): server = await asyncio.start_server(handle_client, \'localhost\', 8888) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main())"},{"question":"# Question: You are given a task to manage memory and profiling for a neural network training process using PyTorch with the MPS backend. Your goal is to write a function `train_with_mps_profiling` that: 1. Sets a specific seed for reproducibility. 2. Starts a profiler to capture performance data. 3. Initializes a neural network and an optimizer. 4. Trains the model for a given number of epochs. 5. After training, stops the profiler and outputs profiling data. 6. Frees any cached memory after the training process. Requirements: - Implement the function `train_with_mps_profiling` as described above. - The function should take the following parameters: - `network_init_fn`: A function to initialize the neural network. - `optimizer_init_fn`: A function to initialize the optimizer. - `num_epochs`: An integer specifying the number of epochs for training. - `seed_value`: An integer to set the random seed. - `train_loader`: A DataLoader object for the training data. Constraints: - You must use the PyTorch MPS backend for all GPU-related operations. - Ensure that the training process and profiling are done efficiently and that memory is managed properly. Example: ```python def train_with_mps_profiling(network_init_fn, optimizer_init_fn, num_epochs, seed_value, train_loader): Args: network_init_fn (function): Function to initialize the neural network. optimizer_init_fn (function): Function to initialize the optimizer. num_epochs (int): Number of epochs for training. seed_value (int): Seed for random number generation. train_loader (DataLoader): DataLoader for training data. Returns: None pass ``` **Hint:** You may utilize the `torch.mps` module for memory management and `torch.mps.profiler` for profiling.","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.profiler import profile, record_function, ProfilerActivity import random import numpy as np def train_with_mps_profiling(network_init_fn, optimizer_init_fn, num_epochs, seed_value, train_loader): Args: network_init_fn (function): Function to initialize the neural network. optimizer_init_fn (function): Function to initialize the optimizer. num_epochs (int): Number of epochs for training. seed_value (int): Seed for random number generation. train_loader (DataLoader): DataLoader for training data. Returns: None # Set the seed for reproducibility torch.manual_seed(seed_value) np.random.seed(seed_value) random.seed(seed_value) # Check for MPS availability if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS is not available on this system.\\") # Initialize the neural network and move it to MPS device device = torch.device(\\"mps\\") model = network_init_fn().to(device) # Initialize the optimizer optimizer = optimizer_init_fn(model.parameters()) # Loss function criterion = nn.CrossEntropyLoss() # Start the profiler with profile( activities=[ProfilerActivity.CPU, ProfilerActivity.MPS], record_shapes=True ) as prof: model.train() for epoch in range(num_epochs): for data, target in train_loader: # Move data to MPS device data, target = data.to(device), target.to(device) # Forward pass optimizer.zero_grad() output = model(data) loss = criterion(output, target) # Backward pass and optimize loss.backward() optimizer.step() # Stop the profiler and print profiling data print(prof.key_averages().table(sort_by=\\"self_mps_time_total\\", row_limit=10)) # Free cached memory torch.mps.empty_cache()"},{"question":"**Objective:** Create a custom Python object type using the `PyTypeObject` structure. This type will represent a simple numeric type which supports basic arithmetic operations, hashing, and a string representation. **Task:** You are to implement a new Python type `PyNumeric` in C, that supports the following functionalities: 1. **Arithmetic Operations**: Addition (`__add__`), subtraction (`__sub__`), and multiplication (`__mul__`). 2. **Representation**: String representation (`__str__`) that shows the numeric value. 3. **Hashability**: Implement `__hash__` to allow instances to be used as keys in dictionaries. Here are the detailed steps you need to follow: 1. **Define a struct for the new type**: ```c typedef struct { PyObject_HEAD double value; } PyNumeric; ``` 2. **Set up the `PyNumberMethods` structure** to include pointers to the functions implementing the arithmetic operations. 3. **Implement the arithmetic functions** with the following signatures: ```c PyObject* PyNumeric_add(PyObject* self, PyObject* other); PyObject* PyNumeric_subtract(PyObject* self, PyObject* other); PyObject* PyNumeric_multiply(PyObject* self, PyObject* other); ``` 4. **Implement the `tp_repr` and `tp_hash` functions**: ```c PyObject* PyNumeric_str(PyObject* self); Py_hash_t PyNumeric_hash(PyObject* self); ``` 5. **Create the `PyTypeObject` for `PyNumeric`** with appropriate field initializations using the methods defined above. # Input and Output: **Input**: There is no direct input required for the coding task. Your implementation will define the type and its methods. **Output**: Ensure the `PyNumeric` objects behave like numeric types in Python: - Arithmetic operations like `+`, `-`, and `*` should work. - The `str` function should provide the string representation. - The objects should be hashable and usable as dictionary keys. # Constraints: - Ensure the correct allocation and deallocation of the `PyNumeric` objects. - Follow the C conventions for implementing the Python C API functions. # Example: Here\'s how the new types can be used after being implemented and exposed to Python: ```python a = PyNumeric(5) b = PyNumeric(3) print(a + b) # Output: PyNumeric(8.0) print(a - b) # Output: PyNumeric(2.0) print(a * b) # Output: PyNumeric(15.0) print(str(a)) # Output: 5.0 d = {a: \\"test\\"} print(d[a]) # Output: test ``` **Note:** Implement the necessary functionality and test your `PyNumeric` type to ensure it meets the described behaviors.","solution":"# Assuming \'Cython\' or similar syntax where we define the required PyNumeric type in Python class PyNumeric: def __init__(self, value): self.value = float(value) def __add__(self, other): if isinstance(other, PyNumeric): return PyNumeric(self.value + other.value) return NotImplemented def __sub__(self, other): if isinstance(other, PyNumeric): return PyNumeric(self.value - other.value) return NotImplemented def __mul__(self, other): if isinstance(other, PyNumeric): return PyNumeric(self.value * other.value) return NotImplemented def __str__(self): return str(self.value) def __hash__(self): return hash(self.value) def __repr__(self): return f\\"PyNumeric({self.value})\\""},{"question":"# Question: You are required to implement an asynchronous model training function using PyTorch\'s `torch.multiprocessing`. This function should start multiple processes to train a PyTorch model in a Hogwild manner. Ensure that you include best practices to avoid common pitfalls such as deadlocks and CPU oversubscription. Requirements: 1. **Model Sharing**: Ensure that the model parameters are shared between processes. 2. **Training Function**: Implement a training function that each process will execute. 3. **Process Management**: Correctly start and join multiple processes. 4. **CPU Oversubscription**: Avoid CPU oversubscription as described by setting the number of threads each process should use. Input: - `num_processes` (int): The number of processes to spawn for training. - `num_epochs` (int): The number of epochs to train the model for. - `data_loader` (torch.utils.data.DataLoader): DataLoader for training data. - `model` (torch.nn.Module): The PyTorch model to train. - `optimizer_class` (torch.optim.Optimizer): The optimizer class to use for training, e.g., `torch.optim.SGD`. - `learning_rate` (float): Learning rate for the optimizer. Output: - There is no direct output from this function. The processes should update the shared model parameters during training. Example Usage: ```python import torch import torch.multiprocessing as mp from torch.optim import SGD from torch.utils.data import DataLoader # Define your DataLoader, model, and training parameters data_loader = DataLoader(...) model = MyModel() num_processes = 4 num_epochs = 10 learning_rate = 0.01 # Function to be implemented train_model(num_processes, num_epochs, data_loader, model, SGD, learning_rate) ``` # Implementation Notes: 1. **Model Sharing**: Use `model.share_memory()` to share the model parameters across processes. 2. **Training Function**: - Inside the function called by each process, ensure that the optimizer is created, and the loss is computed and backpropagated to update shared model parameters. - Use `torch.set_num_threads(floor(N/M))` to avoid CPU oversubscription, where `N` is the number of available CPUs, and `M` is the number of processes. 3. **Process Management**: Use `torch.multiprocessing.Process` to create and manage processes. Ensure to start and join all processes correctly. # Solution Template: ```python import torch import torch.multiprocessing as mp import math def train_model(num_processes, num_epochs, data_loader, model, optimizer_class, learning_rate): def train(rank, model, optimizer_class, num_epochs, data_loader, learning_rate): # Define the number of threads used in the current subprocess num_cpus = torch.get_num_threads() torch.set_num_threads(num_cpus // num_processes) # Initialize optimizer optimizer = optimizer_class(model.parameters(), lr=learning_rate) for epoch in range(num_epochs): for data, labels in data_loader: optimizer.zero_grad() outputs = model(data) loss = ... # Define the appropriate loss function loss.backward() optimizer.step() if __name__ == \\"__main__\\": model.share_memory() # Share the model parameters processes = [] for rank in range(num_processes): p = mp.Process(target=train, args=(rank, model, optimizer_class, num_epochs, data_loader, learning_rate)) p.start() processes.append(p) for p in processes: p.join() ``` Make sure to test and verify your implementation in a real training scenario.","solution":"import torch import torch.multiprocessing as mp import math def train_model(num_processes, num_epochs, data_loader, model, optimizer_class, learning_rate): def train(rank, model, optimizer_class, num_epochs, data_loader, learning_rate): # Define the number of threads used in the current subprocess num_cpus = torch.get_num_threads() torch.set_num_threads(num_cpus // num_processes) # Initialize optimizer optimizer = optimizer_class(model.parameters(), lr=learning_rate) for epoch in range(num_epochs): for data, labels in data_loader: optimizer.zero_grad() outputs = model(data) loss = torch.nn.functional.cross_entropy(outputs, labels) # Example for classification task loss.backward() optimizer.step() if __name__ == \\"__main__\\": model.share_memory() # Share the model parameters processes = [] for rank in range(num_processes): p = mp.Process(target=train, args=(rank, model, optimizer_class, num_epochs, data_loader, learning_rate)) p.start() processes.append(p) for p in processes: p.join()"},{"question":"You are given a dataset consisting of movie reviews in the form of text documents. Your task is to perform feature extraction on these documents to prepare them for input into a classification model. Specifically, you need to implement the extraction of features using both CountVectorizer and TfidfVectorizer. # Instructions: 1. **Read the Data**: You need to simulate reading text data. We provide a sample dataset of movie reviews as follows: ```python reviews = [ \'This movie was excellent! The performances were Oscar-worthy.\', \'Bad movie. Horrible acting and storyline.\', \'It was an average movie. Some scenes were good, but overall it was boring.\', \'An excellent movie with superb acting. Definitely a must-watch!\', \'Not good. The plot was dull and the acting was subpar.\' ] labels = [1, 0, 0, 1, 0] ``` 2. **Implement Feature Extraction**: - Using `CountVectorizer`, convert the text data into a matrix of token counts. - Using `TfidfVectorizer`, convert the text data into a matrix of TF-IDF features. 3. **Output Formats**: - For `CountVectorizer`: Output the feature names and the resulting feature matrix. - For `TfidfVectorizer`: Output the feature names and the resulting feature matrix. # Requirements: - You should define a function `extract_features_using_count_vectorizer` which takes `reviews` as input and returns the feature names and feature matrix from `CountVectorizer`. - You should define a function `extract_features_using_tfidf_vectorizer` which takes `reviews` as input and returns the feature names and feature matrix from `TfidfVectorizer`. # Constraints: - Use the default parameters for both vectorizers unless otherwise specified. # Example Output: For `extract_features_using_count_vectorizer`: ```python feature_names = [\'acting\', \'an\', \'and\', \'average\', \'bad\', \'boring\', \'but\', \'definitely\', \'dull\', \'excellent\', \'good\', \'horrible\', \'it\', \'movie\', \'not\', \'oscar\', \'overall\', \'performances\', \'plot\', \'scenes\', \'some\', \'subpar\', \'superb\', \'the\', \'was\', \'were\', \'with\', \'worthy\'] feature_matrix = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1], [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0], ...] ``` For `extract_features_using_tfidf_vectorizer`: ```python feature_names = [\'acting\', \'an\', \'and\', \'average\', \'bad\', \'boring\', \'but\', \'definitely\', \'dull\', \'excellent\', \'good\', \'horrible\', \'it\', \'movie\', \'not\', \'oscar\', \'overall\', \'performances\', \'plot\', \'scenes\', \'some\', \'subpar\', \'superb\', \'the\', \'was\', \'were\', \'with\', \'worthy\'] feature_matrix = [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53801, 0.0, 0.0, 0.0, 0.22768, 0.0, 0.53801, 0.0, 0.53801, 0.0, 0.0, 0.0, 0.0, 0.53801, 0.45537, 0.22768, 0.0, 0.0, 0.53801], [0.40011172, 0.0, 0.40011172, 0.0, 0.40011172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40011172, 0.0, 0.16903085, 0.40011172, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3380617, 0.16903085, 0.0, 0.0], ...] ``` # Note: - The feature names and feature matrices shown above are examples. They can be different based on the implementation. # Implementation: ```python from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer def extract_features_using_count_vectorizer(reviews): vectorizer = CountVectorizer() X = vectorizer.fit_transform(reviews) feature_names = vectorizer.get_feature_names_out() feature_matrix = X.toarray() return feature_names, feature_matrix def extract_features_using_tfidf_vectorizer(reviews): vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(reviews) feature_names = vectorizer.get_feature_names_out() feature_matrix = X.toarray() return feature_names, feature_matrix # Sample data reviews = [ \'This movie was excellent! The performances were Oscar-worthy.\', \'Bad movie. Horrible acting and storyline.\', \'It was an average movie. Some scenes were good, but overall it was boring.\', \'An excellent movie with superb acting. Definitely a must-watch!\', \'Not good. The plot was dull and the acting was subpar.\' ] labels = [1, 0, 0, 1, 0] # Example usage count_feat_names, count_feat_matrix = extract_features_using_count_vectorizer(reviews) tfidf_feat_names, tfidf_feat_matrix = extract_features_using_tfidf_vectorizer(reviews) print(\\"CountVectorizer Feature Names:\\", count_feat_names) print(\\"CountVectorizer Feature Matrix:n\\", count_feat_matrix) print(\\"TfidfVectorizer Feature Names:\\", tfidf_feat_names) print(\\"TfidfVectorizer Feature Matrix:n\\", tfidf_feat_matrix) ```","solution":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer def extract_features_using_count_vectorizer(reviews): Extracts features using CountVectorizer. Args: reviews (list of str): List of movie reviews. Returns: tuple: A tuple containing feature names and feature matrix. vectorizer = CountVectorizer() X = vectorizer.fit_transform(reviews) feature_names = vectorizer.get_feature_names_out() feature_matrix = X.toarray() return feature_names, feature_matrix def extract_features_using_tfidf_vectorizer(reviews): Extracts features using TfidfVectorizer. Args: reviews (list of str): List of movie reviews. Returns: tuple: A tuple containing feature names and feature matrix. vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(reviews) feature_names = vectorizer.get_feature_names_out() feature_matrix = X.toarray() return feature_names, feature_matrix"},{"question":"**Title: Implementing a Context-Aware Data Class in Python** Objective Write a Python code that defines a data class and utilizes context managers. This will test your understanding of `dataclasses` for structuring data and `contextlib` for managing resources. Background Data classes simplify the syntax and boilerplate required for defining classes. Context managers in Python, typically used via the `with` statement, ensure that resources are properly managed. Task 1. Define a data class `Book` with the following structure: - **Attributes**: `title` (str), `author` (str), `year` (int), and `checked_out` (bool, default False). - **Methods**: A method `checkout()` to set `checked_out` to `True`, and a method `return_book()` to set `checked_out` to `False`. 2. Create a context manager provided by the `contextlib` module to manage the checkout and return process of the `Book` instances. The context manager should: - Automatically mark the book as `checked_out` when entering the context. - Automatically return the book (i.e., set `checked_out` to `False`) when the context is exited. Requirements - The data class `Book` must utilize the `dataclass` decorator. - Implement the context manager using the `contextlib` module. - Ensure the `checked_out` status is correctly updated before and after the context block. Input and Outputs - **Input**: None directly. The implementation should be demonstrated through calling methods and contextual usage. - **Output**: Demonstrate the functionality with example calls. Example ```python from dataclasses import dataclass, field from contextlib import contextmanager @dataclass class Book: title: str author: str year: int checked_out: bool = field(default=False) def checkout(self): self.checked_out = True def return_book(self): self.checked_out = False @contextmanager def manage_checkout(book: Book): book.checkout() try: yield book finally: book.return_book() # Demonstration if __name__ == \\"__main__\\": my_book = Book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", 1925) print(f\\"Before context: {my_book.checked_out}\\") # Output: False with manage_checkout(my_book) as book: print(f\\"In context: {book.checked_out}\\") # Output: True print(f\\"After context: {my_book.checked_out}\\") # Output: False ``` The example demonstrates how the book is checked out and automatically returned using the context manager. Constraints - You must use the `dataclasses` module to define the `Book` class. - Use the `contextlib` module to implement the context manager.","solution":"from dataclasses import dataclass, field from contextlib import contextmanager @dataclass class Book: title: str author: str year: int checked_out: bool = field(default=False) def checkout(self): self.checked_out = True def return_book(self): self.checked_out = False @contextmanager def manage_checkout(book: Book): book.checkout() try: yield book finally: book.return_book()"},{"question":"# Question: Custom Data Analysis with Built-in Functions Objective: Write a function `analyze_data(records, filter_fn, attr_name, update_fn)` that performs data analysis and transformation using various Python built-in functions. This function should illustrate comprehension of function usage, object attribute handling, and iterable manipulations. Task Description: Given a list of custom objects (records), perform the following steps: 1. Enumerate the records. 2. Filter the records using a provided filtering function (`filter_fn`). 3. For each filtered record, retrieve a specified attribute (`attr_name`). 4. For each retrieved attribute, apply an update function (`update_fn`) and update the attribute value in the original record. 5. Return a list of updated records, sorted based on the original index positions (the order they appeared in the original list). Function Signature: ```python def analyze_data(records, filter_fn, attr_name, update_fn): pass ``` Input: - `records`: List of custom objects. Assume each object has attributes that can be accessed and modified using `getattr` and `setattr`. - `filter_fn`: A function that takes an object and returns `True` or `False`, determining whether the object passes the filter. - `attr_name`: The name of the attribute to retrieve and update. - `update_fn`: A function that takes the attribute value of an object and returns the new value to be set. Output: - A list of updated records, sorted by their original positions in the input list `records`. Example: Consider the following custom object: ```python class Record: def __init__(self, value): self.value = value ``` Usage: ```python records = [Record(10), Record(20), Record(30), Record(40)] filter_fn = lambda x: x.value > 20 attr_name = \'value\' update_fn = lambda x: x * 2 result = analyze_data(records, filter_fn, attr_name, update_fn) # Expected output: The original list should be updated as: # [Record(10), Record(20), Record(60), Record(80)] ``` Things to consider: 1. Use built-in functions like `enumerate`, `filter`, `map`, `getattr`, `setattr`, and `sorted` effectively. 2. Ensure the records maintain their original order after updates. 3. Implement error handling for scenarios where `attr_name` doesn\'t exist. Constraints: - You may assume that `filter_fn` and `update_fn` are valid functions and will not cause runtime errors within their logic.","solution":"def analyze_data(records, filter_fn, attr_name, update_fn): Analyze and transform a list of custom object records. Parameters: - records: List of custom objects. - filter_fn: Function to filter records. - attr_name: The attribute name to retrieve and update. - update_fn: Function to apply to the retrieved attribute\'s value. Returns: - A list of updated records sorted by their original positions. # Enumerate the original records to preserve their indexes indexed_records = list(enumerate(records)) # Filter records based on filter_fn filtered_records = filter(lambda x: filter_fn(x[1]), indexed_records) # Apply update_fn on the attribute and update the original attribute value for idx, record in filtered_records: if hasattr(record, attr_name): current_value = getattr(record, attr_name) new_value = update_fn(current_value) setattr(record, attr_name, new_value) # Return the records sorted by their original index positions return [record for idx, record in sorted(indexed_records, key=lambda x: x[0])]"},{"question":"**Question: Traffic Management System** You are tasked with simulating a traffic management system at an intersection using Python\'s `queue` module. The intersection has four incoming lanes, and each lane has a traffic light that turns green in a round-robin fashion allowing cars to cross the intersection. Cars arrive at the intersection and are queued up in the respective lanes. The system should ensure that only one lane has the green light at a time and that cars from that lane can cross the intersection. # Requirements: 1. **Classes to Implement:** - **Intersection:** Manages the lanes and traffic lights. Should implement methods to switch lights, allow cars to cross, and handle car arrivals. - **Lane:** Manages the queuing of cars. Should use `queue.Queue` to manage the cars in a FIFO manner. 2. **Methods:** - **Intersection.switch_light():** Rotate the green light to the next lane. - **Intersection.car_arrival(lane_id, car_id):** Add a car to the corresponding lane. - **Intersection.allow_cars_to_cross():** Allow cars from the lane with the green light to cross the intersection. - **Lane.add_car(car_id):** Add a car to the lane. - **Lane.cross_car():** Simulate a car crossing the intersection. 3. **Constraints:** - Use `queue.Queue` for managing cars in each lane. - The intersection should ensure safety by only allowing one lane to have the green light and process cars at a time. 4. **Input and Output:** - Input: You will receive a list of tuples `(lane_id, car_id)` representing car arrivals. - Output: A sequence of car crossing events. Each event should indicate which car crossed from which lane. 5. **Performance:** - The system should handle up to 1000 car arrivals efficiently and process them in a reasonable time. # Example: ```python from queue import Queue class Lane: def __init__(self): self.queue = Queue() def add_car(self, car_id: int): self.queue.put(car_id) def cross_car(self) -> int: if not self.queue.empty(): return self.queue.get() raise queue.Empty(\\"No cars to cross\\") class Intersection: def __init__(self, num_lanes=4): self.lanes = [Lane() for _ in range(num_lanes)] self.current_green = 0 def switch_light(self): self.current_green = (self.current_green + 1) % len(self.lanes) def car_arrival(self, lane_id: int, car_id: int): self.lanes[lane_id].add_car(car_id) def allow_cars_to_cross(self): while not self.lanes[self.current_green].queue.empty(): car_id = self.lanes[self.current_green].cross_car() print(f\\"Car {car_id} from lane {self.current_green} crosses the intersection\\") self.switch_light() # Demo intersection = Intersection() cars = [(0, 101), (1, 102), (0, 103), (2, 104), (1, 105), (3, 106), (2, 107)] for lane_id, car_id in cars: intersection.car_arrival(lane_id, car_id) for _ in range(4): intersection.allow_cars_to_cross() ``` This example should output the sequence of cars crossing the intersection correctly, demonstrating the use of `queue.Queue` and basic threading mechanisms.","solution":"from queue import Queue, Empty class Lane: def __init__(self): self.queue = Queue() def add_car(self, car_id: int): self.queue.put(car_id) def cross_car(self) -> int: if not self.queue.empty(): return self.queue.get() raise Empty(\\"No cars to cross\\") class Intersection: def __init__(self, num_lanes=4): self.lanes = [Lane() for _ in range(num_lanes)] self.current_green = 0 def switch_light(self): self.current_green = (self.current_green + 1) % len(self.lanes) def car_arrival(self, lane_id: int, car_id: int): self.lanes[lane_id].add_car(car_id) def allow_cars_to_cross(self): results = [] while not self.lanes[self.current_green].queue.empty(): car_id = self.lanes[self.current_green].cross_car() results.append(f\\"Car {car_id} from lane {self.current_green} crosses the intersection\\") self.switch_light() return results"},{"question":"**Objective:** Implement a function using `asyncio` futures to simulate an asynchronous task scheduler. **Problem Statement:** You need to write a function `async_task_scheduler` that schedules and manages multiple asynchronous tasks using `asyncio` futures. Each task should simulate an asynchronous operation by sleeping for a random amount of time between 1 to 5 seconds and then returning a result. # Function Signature: ```python async def async_task_scheduler(n: int) -> List[str]: pass ``` # Input: - `n` (int): Number of asynchronous tasks to schedule. # Output: - Returns a list of strings, where each string represents the result of a completed task. # Requirements: 1. Create an asyncio event loop. 2. For each task: - Create a Future object. - Set a callback to be executed when the Future is done. - Schedule a coroutine that will sleep for a random time and set the result of the Future. 3. Collect the results from all Futures and return them as a list. # Constraints: - You must use `asyncio` and `asyncio.Future`. - You cannot use any external libraries except `asyncio` and `random`. - Ensure that the tasks are truly asynchronous. # Example: ```python import asyncio import random from typing import List async def simulate_task(fut: asyncio.Future, delay: int, result: str) -> None: await asyncio.sleep(delay) fut.set_result(result) async def async_task_scheduler(n: int) -> List[str]: loop = asyncio.get_running_loop() futures = [] for i in range(n): fut = loop.create_future() delay = random.randint(1, 5) result = f\\"Task {i+1} completed after {delay} seconds\\" fut.add_done_callback(lambda f: print(f.result())) loop.create_task(simulate_task(fut, delay, result)) futures.append(fut) return await asyncio.gather(*futures) # Code to run the function for testing if __name__ == \\"__main__\\": result = asyncio.run(async_task_scheduler(3)) print(result) ``` **Explanation:** - The `async_task_scheduler` function generates `n` asynchronous tasks. - Each task involves setting a result after sleeping for a random amount of time. - The function waits for all tasks to complete and then gathers their results into a list. - Each task\'s completion is also demonstrated using a callback.","solution":"import asyncio import random from typing import List async def simulate_task(fut: asyncio.Future, delay: int, result: str) -> None: await asyncio.sleep(delay) fut.set_result(result) async def async_task_scheduler(n: int) -> List[str]: loop = asyncio.get_running_loop() futures = [] for i in range(n): fut = loop.create_future() delay = random.randint(1, 5) result = f\\"Task {i+1} completed after {delay} seconds\\" fut.add_done_callback(lambda f: print(f.result())) asyncio.create_task(simulate_task(fut, delay, result)) futures.append(fut) return await asyncio.gather(*futures) # Code to ensure the function works (for manual test) if __name__ == \\"__main__\\": results = asyncio.run(async_task_scheduler(3)) print(results)"},{"question":"# Asynchronous Task and Exception Handling **Objective:** Implement an asynchronous function that performs multiple I/O operations and demonstrates comprehensive exception handling using the asyncio exceptions discussed above. **Task Description:** Write an asynchronous function `fetch_and_process_data` that: 1. **Simulates** fetching data from multiple I/O-bound tasks. 2. **Handles** the following exceptions appropriately: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.InvalidStateError` - `asyncio.SendfileNotAvailableError` - `asyncio.IncompleteReadError` - `asyncio.LimitOverrunError` **Function Specifications:** ```python async def fetch_and_process_data(urls: List[str], timeout: float) -> None: Fetches data asynchronously from multiple URLs and processes them. Args: - urls (List[str]): A list of URLs to fetch data from. - timeout (float): The time duration in seconds to wait for each task completion. Returns: - None pass ``` **Input:** - `urls`: A list of strings representing URLs to fetch data from. - `timeout`: A float value representing the timeout duration in seconds for each fetch operation. **Output:** - None **Example:** ```python urls = [\\"http://example.com/data1\\", \\"http://example.com/data2\\"] timeout = 5.0 await fetch_and_process_data(urls, timeout) ``` **Constraints:** 1. Simulate data fetching using `asyncio.sleep()` to represent I/O operations. 2. Ensure appropriate exception handling for each of the exceptions listed in the documentation. 3. You may use dummy data for processing. 4. Ensure that if an exception occurs during any task, it is appropriately handled and logged. 5. The function should print an error message specifying which exception was caught and on which URL. **Hints:** - Use `asyncio.gather()` to run multiple fetch operations concurrently. - Use try-catch blocks to handle specific exceptions. **Performance:** - Aim to complete the function efficiently within the given timeout constraints. This task will assess your ability to work with asynchronous operations and exception handling in Python using the `asyncio` module.","solution":"import asyncio from typing import List async def fetch_data(url: str, timeout: float): try: await asyncio.sleep(timeout) print(f\\"Fetched data from {url}\\") except asyncio.TimeoutError: print(f\\"TimeoutError: Fetching data from {url} timed out.\\") except asyncio.CancelledError: print(f\\"CancelledError: Fetching data from {url} was cancelled.\\") except asyncio.InvalidStateError: print(f\\"InvalidStateError: Invalid state encountered while fetching data from {url}.\\") except asyncio.SendfileNotAvailableError: print(f\\"SendfileNotAvailableError: Sendfile not available while fetching data from {url}.\\") except asyncio.IncompleteReadError as e: print(f\\"IncompleteReadError: Incomplete read encountered while fetching data from {url}, {e.args}\\") except asyncio.LimitOverrunError as e: print(f\\"LimitOverrunError: Limit overrun while fetching data from {url}, {e.args}\\") except Exception as e: print(f\\"Unexpected error: {e} while fetching data from {url}\\") async def fetch_and_process_data(urls: List[str], timeout: float) -> None: tasks = [fetch_data(url, timeout) for url in urls] await asyncio.gather(*tasks, return_exceptions=True)"},{"question":"**Objective:** Demonstrate your ability to use the `grp` module to interact with the Unix group database. **Problem:** You are tasked with writing a Python function that performs various operations on the Unix group database. Your function should be able to return group details based on group ID or group name, and also provide a summary report of all groups. You must handle potential errors gracefully and provide meaningful feedback. **Function Signature:** ```python def manage_unix_groups(action: str, identifier=None) -> dict: pass ``` **Parameters:** - `action` (str): The action to be performed. It can have one of the following values: - `\\"get_by_gid\\"`: Retrieve group details by group ID. - `\\"get_by_name\\"`: Retrieve group details by group name. - `\\"list_all\\"`: List all available group entries. - `identifier` (Optional): The identifier for the action. It can be: - `int`: A numeric group ID (only if `action` is `\\"get_by_gid\\"`). - `str`: A group name (only if `action` is `\\"get_by_name\\"`). **Output:** - For `\\"get_by_gid\\"` and `\\"get_by_name\\"` actions, the function should return a dictionary with the following keys: - `\\"gr_name\\"`: The name of the group. - `\\"gr_passwd\\"`: The encrypted group password. - `\\"gr_gid\\"`: The numerical group ID. - `\\"gr_mem\\"`: The list of group members. - For `\\"list_all\\"` action, the function should return a list of such dictionaries for all available group entries. - In case of an error (e.g., non-existing group), return an error message dictionary: - `\\"error\\"`: A string describing the error (e.g., \\"Group not found\\"). **Constraints:** - The function should raise a `ValueError` if the `action` parameter is not valid. - The function should handle `KeyError` gracefully and return an appropriate error message dictionary. **Example Usage:** ```python # Example 1: Getting group details by group ID result = manage_unix_groups(\\"get_by_gid\\", 1000) # Expected output (assuming the group ID 1000 exists): # {\'gr_name\': \'examplegroup\', \'gr_passwd\': \'x\', \'gr_gid\': 1000, \'gr_mem\': [\'user1\', \'user2\']} # Example 2: Getting group details by group name result = manage_unix_groups(\\"get_by_name\\", \\"examplegroup\\") # Expected output (assuming the group name \'examplegroup\' exists): # {\'gr_name\': \'examplegroup\', \'gr_passwd\': \'x\', \'gr_gid\': 1000, \'gr_mem\': [\'user1\', \'user2\']} # Example 3: Listing all group entries result = manage_unix_groups(\\"list_all\\") # Expected output (depends on the system): # [{\'gr_name\': \'group1\', \'gr_passwd\': \'x\', \'gr_gid\': 1000, \'gr_mem\': [\'user1\']}, ...] # Example 4: Handling a non-existing group ID result = manage_unix_groups(\\"get_by_gid\\", 9999) # Expected output: # {\'error\': \'Group not found\'} # Example 5: Handling an invalid action result = manage_unix_groups(\\"invalid_action\\") # Expected output: # Raises ValueError ``` **Note:** - Your function should work on any Unix-based system with the `grp` module available. - Make sure to test your function on a live system to ensure accuracy.","solution":"import grp def manage_unix_groups(action: str, identifier=None) -> dict: if action == \\"get_by_gid\\": if not isinstance(identifier, int): raise ValueError(\\"Identifier must be an integer for action \'get_by_gid\'\\") try: entry = grp.getgrgid(identifier) return { \\"gr_name\\": entry.gr_name, \\"gr_passwd\\": entry.gr_passwd, \\"gr_gid\\": entry.gr_gid, \\"gr_mem\\": entry.gr_mem } except KeyError: return {\\"error\\": \\"Group not found\\"} elif action == \\"get_by_name\\": if not isinstance(identifier, str): raise ValueError(\\"Identifier must be a string for action \'get_by_name\'\\") try: entry = grp.getgrnam(identifier) return { \\"gr_name\\": entry.gr_name, \\"gr_passwd\\": entry.gr_passwd, \\"gr_gid\\": entry.gr_gid, \\"gr_mem\\": entry.gr_mem } except KeyError: return {\\"error\\": \\"Group not found\\"} elif action == \\"list_all\\": groups = [] for entry in grp.getgrall(): groups.append({ \\"gr_name\\": entry.gr_name, \\"gr_passwd\\": entry.gr_passwd, \\"gr_gid\\": entry.gr_gid, \\"gr_mem\\": entry.gr_mem }) return groups else: raise ValueError(\\"Invalid action\\")"},{"question":"You are tasked with creating a custom HTML parser using the `html.parser.HTMLParser` class. The objective is to extract specific data from an HTML document, more specifically to gather information about all hyperlinks (`<a>` tags) in the document. # Problem Statement Implement a class `LinkExtractor` that inherits from `html.parser.HTMLParser`. This class should parse HTML content and extract all hyperlinks along with their display text. # Class Requirements 1. **Initialization and Data Storage**: - The class should initialize an empty list to store the hyperlinks and their display text. 2. **Handler Methods**: - Override the `handle_starttag` method to detect `<a>` tags and extract the hyperlink (`href` attribute). - Override the `handle_data` method to capture the display text for the most recent `<a>` tag. - Ensure that the `handle_endtag` method is used to clean up or finalize hyperlink data when an `</a>` tag is encountered. 3. **Method to Retrieve Hyperlinks**: - Implement a method `get_links` which returns a list of tuples. Each tuple should contain a hyperlink and its display text in the format `(hyperlink, display_text)`. # Input and Output - **Input**: A string of HTML content. - **Output**: A list of tuples containing hyperlinks and their display texts. # Example Given the following HTML content: ```html <!DOCTYPE html> <html lang=\\"en\\"> <head> <meta charset=\\"UTF-8\\"> <title>Links Page</title> </head> <body> <p>Visit the <a href=\\"https://www.python.org/\\">Python Homepage</a> for more information.</p> <p>Check out the <a href=\\"https://docs.python.org/3/\\">Python Documentation</a>.</p> </body> </html> ``` The class usage should be: ```python html_content = \'\'\'<!DOCTYPE html> <html lang=\\"en\\"> <head> <meta charset=\\"UTF-8\\"> <title>Links Page</title> </head> <body> <p>Visit the <a href=\\"https://www.python.org/\\">Python Homepage</a> for more information.</p> <p>Check out the <a href=\\"https://docs.python.org/3/\\">Python Documentation</a>.</p> </body> </html>\'\'\' parser = LinkExtractor() parser.feed(html_content) links = parser.get_links() print(links) ``` The output should be: ```python [(\'https://www.python.org/\', \'Python Homepage\'), (\'https://docs.python.org/3/\', \'Python Documentation\')] ``` # Constraints - The HTML document will be well-formed, but may contain nested tags and varying structures within it. - The display text for a hyperlink will not contain other nested hyperlinks. Note: Ensure your solution parses hyperlinks correctly even if the HTML content is provided in chunks over multiple `feed` calls. # Implement the class `LinkExtractor` below: ```python from html.parser import HTMLParser class LinkExtractor(HTMLParser): def __init__(self): super().__init__() self.links = [] self.current_link = None self.current_display_text = None def handle_starttag(self, tag, attrs): if tag == \'a\': for attr, value in attrs: if attr == \'href\': self.current_link = value self.current_display_text = \\"\\" def handle_data(self, data): if self.current_link is not None: self.current_display_text += data def handle_endtag(self, tag): if tag == \'a\' and self.current_link is not None: self.links.append((self.current_link, self.current_display_text)) self.current_link = None self.current_display_text = None def get_links(self): return self.links ```","solution":"from html.parser import HTMLParser class LinkExtractor(HTMLParser): def __init__(self): super().__init__() self.links = [] self.current_link = None self.current_display_text = None def handle_starttag(self, tag, attrs): if tag == \'a\': for attr, value in attrs: if attr == \'href\': self.current_link = value self.current_display_text = \\"\\" def handle_data(self, data): if self.current_link is not None: self.current_display_text += data def handle_endtag(self, tag): if tag == \'a\' and self.current_link is not None: self.links.append((self.current_link, self.current_display_text)) self.current_link = None self.current_display_text = None def get_links(self): return self.links"},{"question":"**Pandas and PyArrow Integration Assessment** # Objective: To evaluate your understanding of pandas\' integration with PyArrow for enhancing data manipulation and performance. # Problem Statement: You are given a dataset in CSV format containing sales data for a retail store. The CSV file includes the following columns: - `Date`: Sales date in YYYY-MM-DD format. - `Item_ID`: Unique identifier for items. - `Units_Sold`: Number of units sold (integer). - `Revenue`: Revenue generated from sales (float). - `In_Stock`: Boolean indicating item stock availability (True/False). - `Comments`: Text comments about the sale (nullable string). Tasks: 1. Read the CSV data using the PyArrow engine to create a pandas DataFrame. 2. Ensure all columns are using appropriate PyArrow-backed data types. 3. Perform data cleansing: - Remove any rows with missing `Item_ID`. - Fill missing `Revenue` values with the mean revenue. - Transform the `Date` column to datetime and extract the month into a new column named `Month`. 4. Calculate the total units sold and total revenue for each month. 5. Save the final DataFrame to a new CSV file using PyArrow. # Input/Output: - **Input:** Path to the input CSV file. - **Output:** Path to the output CSV file. # Constraints: - The CSV file can contain up to 1 million rows. - You should use PyArrow to handle data reading and writing for performance efficiency. # Function Signature: ```python import pandas as pd def process_sales_data(input_csv_path: str, output_csv_path: str) -> None: # Implement the function here pass ``` # Example: Given the following sample `sales_data.csv`: | Date | Item_ID | Units_Sold | Revenue | In_Stock | Comments | |------------|---------|------------|---------|----------|--------------| | 2023-01-01 | 1 | 10 | 100.0 | True | First sale | | 2023-01-05 | 2 | 5 | NaN | False | | | 2023-01-10 | | 3 | 50.5 | True | Sale comment | | 2023-02-07 | 3 | 20 | 300.0 | True | Best sale | **Example Execution:** ```python process_sales_data(\\"path_to_input/sales_data.csv\\", \\"path_to_output/processed_sales_data.csv\\") ``` After processing, the output file `processed_sales_data.csv` might contain: | Date | Item_ID | Units_Sold | Revenue | In_Stock | Comments | Month | |------------|---------|------------|---------|----------|--------------|----------| | 2023-01-01 | 1 | 10 | 100.0 | True | First sale | January | | 2023-01-05 | 2 | 5 | 150.17 | False | | January | | 2023-02-07 | 3 | 20 | 300.0 | True | Best sale | February | Assuming the mean Revenue is 150.17.","solution":"import pandas as pd import pyarrow.parquet as pq import pyarrow.csv as pv import pyarrow as pa def process_sales_data(input_csv_path: str, output_csv_path: str) -> None: # Read the CSV data using PyArrow table = pv.read_csv(input_csv_path) df = table.to_pandas() # Remove rows with missing Item_ID df = df.dropna(subset=[\'Item_ID\']) # Fill missing Revenue values with the mean revenue mean_revenue = df[\'Revenue\'].mean() df[\'Revenue\'] = df[\'Revenue\'].fillna(mean_revenue) # Transform Date to datetime and extract month df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Month\'] = df[\'Date\'].dt.strftime(\'%B\') # Calculate total units sold and revenue for each month monthly_summary = df.groupby(\'Month\').agg({\'Units_Sold\': \'sum\', \'Revenue\': \'sum\'}).reset_index() # Save the final DataFrame to a new CSV file using PyArrow table_final = pa.Table.from_pandas(df) pq.write_table(table_final, output_csv_path) # Save monthly summary as well for reference monthly_table = pa.Table.from_pandas(monthly_summary) pq.write_table(monthly_table, output_csv_path.replace(\\".parquet\\", \\"_monthly.parquet\\"))"},{"question":"Objective: The goal of this exercise is to create a series of functions that interact with floating-point objects in Python, utilizing provided C-API functions (simulated in the task) to manage and manipulate these objects effectively. Problem Statement: Implement a function that takes a list of numeric strings and performs the following operations: 1. **Check Validity**: Check if each string can be converted to a valid floating-point number using a helper method `is_valid_float`. 2. **Convert and Store**: Convert the valid numeric strings into Python floating-point objects using another helper method `convert_to_float_object`. 3. **Retrieve Info**: Implement a function `float_info` that returns the minimum and maximum representable float values, and the precision information about floating-point numbers. Your task is to implement these functionalities according to the constraints and formats outlined below. Detailed Requirements: 1. **Function 1: `is_valid_float`** ```python def is_valid_float(num_str: str) -> bool: Check if the given string can be converted to a valid float. Input: num_str (str): A numeric string. Output: bool: True if valid float, False otherwise. pass ``` 2. **Function 2: `convert_to_float_object`** ```python def convert_to_float_object(num_str: str) -> float: Convert a valid numeric string to a python float object. Input: num_str (str): A valid numeric string. Output: float: Python float object created from the string. pass ``` 3. **Function 3: `float_info`** ```python def float_info() -> tuple: Return a tuple containing information about float precision, minimum, and maximum values. Output: tuple: A tuple with (precision, min_value, max_value). pass ``` Expected Input and Output: - `is_valid_float(\\"12.34\\")` âžž `True` - `is_valid_float(\\"abc\\")` âžž `False` - `convert_to_float_object(\\"12.34\\")` âžž `12.34` - `convert_to_float_object(\\"0.001\\")` âžž `0.001` - `float_info()` âžž `(15, 2.2250738585072014e-308, 1.7976931348623157e+308)` Constraints: - **Performance**: The conversion and checking should be efficient and handle large lists of strings with minimal delay. - **Accuracy**: The float values should match Python\'s native floating-point representation accurately. - **Error Handling**: Appropriate error handling should be implemented for invalid inputs. Notes: - You can assume that the functions `PyFloat_FromDouble`, `PyFloat_AsDouble`, `PyFloat_GetMin`, and `PyFloat_GetMax` would be available in your Python environment. Implement the functions in Python while following Python\'s coding conventions and practices.","solution":"def is_valid_float(num_str: str) -> bool: Check if the given string can be converted to a valid float. Input: num_str (str): A numeric string. Output: bool: True if valid float, False otherwise. try: float(num_str) return True except ValueError: return False def convert_to_float_object(num_str: str) -> float: Convert a valid numeric string to a python float object. Input: num_str (str): A valid numeric string. Output: float: Python float object created from the string. if is_valid_float(num_str): return float(num_str) else: raise ValueError(f\\"\'{num_str}\' is not a valid numeric string\\") def float_info() -> tuple: Return a tuple containing information about float precision, minimum, and maximum values. Output: tuple: A tuple with (precision, min_value, max_value). import sys precision = sys.float_info.dig min_value = sys.float_info.min max_value = sys.float_info.max return (precision, min_value, max_value)"},{"question":"Task You are tasked with implementing a simplified **asyncio-based task scheduler**. This scheduler should be capable of scheduling multiple tasks concurrently, handling task timeouts, and managing task cancellations. Implement the following three functions: 1. **`async def schedule_tasks(tasks, timeout=None)`**: - **Input**: - `tasks` (List[Coroutine]): A list of coroutine objects that need to be scheduled and run concurrently. - `timeout` (float, optional): Maximum time in seconds to wait for all tasks to complete. Defaults to `None`, which means no timeout. - **Output**: - `List[Tuple[str, Any]]`: A list of tuples where each tuple contains the task name (as given in the coroutine\'s name property) and the result returned by the task, or `\\"Timeout\\"` if the task was cancelled due to a timeout. 2. **`async def run_task(coro, name, timeout=None)`**: - **Input**: - `coro` (Coroutine): The coroutine object representing the task. - `name` (str): A name identifier for the task. - `timeout` (float, optional): Maximum time in seconds to wait for the task to complete. Defaults to `None`, which means no timeout. - **Output**: - `Tuple[str, Any]`: A tuple containing the task name and the result, or `\\"Timeout\\"` if the task was cancelled due to a timeout. 3. **`def main()`**: - A synchronous function that creates a set of sample tasks, schedules them using `schedule_tasks`, and prints the results. # Constraints: - You must handle exceptions within coroutines and ensure they are properly reported in the output. - Provide meaningful names to tasks and handle their scheduling and cancellation appropriately. - Use the `asyncio` library for managing all asynchronous operations. - Consider edge cases such as no tasks, all tasks timing out, and mixed results (some tasks completing and others timing out). # Example: Here\'s a sample implementation to test against: ```python import asyncio async def sample_task(duration, result): await asyncio.sleep(duration) return result async def schedule_tasks(tasks, timeout=None): # Your implementation here async def run_task(coro, name, timeout=None): # Your implementation here def main(): tasks = [ sample_task(2, \\"Task 1 Complete\\"), sample_task(5, \\"Task 2 Complete\\"), sample_task(3, \\"Task 3 Complete\\") ] # Schedule tasks results = asyncio.run(schedule_tasks(tasks, timeout=4)) # Print results for name, result in results: print(f\\"{name}: {result}\\") if __name__ == \\"__main__\\": main() ``` **Expected Output:** ``` Task 1: Task 1 Complete Task 3: Task 3 Complete Task 2: Timeout ``` Notes: - You may use other functions and utilities from the `asyncio` library as needed. - Make sure your implementation is efficient and handles all specified constraints and edge cases correctly.","solution":"import asyncio from typing import List, Tuple, Any async def run_task(coro, name, timeout=None): try: result = await asyncio.wait_for(coro, timeout) return (name, result) except asyncio.TimeoutError: return (name, \\"Timeout\\") async def schedule_tasks(tasks: List[Tuple[str, Any]], timeout=None) -> List[Tuple[str, Any]]: coro_list = [run_task(coro, name, timeout) for name, coro in tasks] results = await asyncio.gather(*coro_list, return_exceptions=True) return results def main(): async def sample_task(duration, result): await asyncio.sleep(duration) return result tasks = [ (\\"Task 1\\", sample_task(2, \\"Task 1 Complete\\")), (\\"Task 2\\", sample_task(5, \\"Task 2 Complete\\")), (\\"Task 3\\", sample_task(3, \\"Task 3 Complete\\")) ] # Schedule tasks results = asyncio.run(schedule_tasks(tasks, timeout=4)) # Print results for name, result in results: print(f\\"{name}: {result}\\") if __name__ == \\"__main__\\": main()"},{"question":"Coding Assessment Question: # Objective: Create a Python function that reads user information from the Unix password database and modifies specific attributes in Unix group records. This question will assess your understanding of interacting with Unix-specific modules in Python. # Description: Write a Python function `modify_user_and_group(uid: int, new_gid: int, new_group_name: str) -> None` that takes a user ID (`uid`), a new group ID (`new_gid`), and a new group name (`new_group_name`). Your function should: 1. Retrieve and print the user\'s details based on the provided `uid` from the Unix password database using the `pwd` module. 2. Verify if the `new_gid` exists in the Unix group database using the `grp` module. 3. If the `new_gid` does not exist, create a new group record with `new_gid` and `new_group_name` using the `grp` module. 4. Update the user\'s group ID to `new_gid`. # Input: - `uid` (int): The user ID for which the details need to be retrieved. - `new_gid` (int): The new group ID to be assigned to the user. - `new_group_name` (str): The name of the new group if it does not already exist. # Constraints: - The user specified by `uid` must exist. - The function should handle errors gracefully, providing meaningful error messages for invalid input scenarios. - The function should have appropriate permissions to modify the group records (this may be simulated in the coding environment). # Example: ```python def modify_user_and_group(uid: int, new_gid: int, new_group_name: str) -> None: # Implement this function pass # Example usage: try: modify_user_and_group(1001, 2001, \\"new_group\\") except Exception as e: print(f\\"An error occurred: {e}\\") ``` # Performance Requirements: - The function should handle the user and group database efficiently, even for a large number of records. - The implementation should minimize the use of unnecessary system calls and data processing. # Notes: - Make sure to test the function on a Unix-like system where you have appropriate permissions. - Ensure to handle exceptions and edge cases, such as non-existing users or groups, gracefully.","solution":"import pwd import grp import os def modify_user_and_group(uid: int, new_gid: int, new_group_name: str) -> None: try: # Retrieve user details based on uid user_info = pwd.getpwuid(uid) print(f\\"User details: {user_info}\\") except KeyError: raise ValueError(f\\"User with uid {uid} does not exist\\") try: # Check if the new_gid already exists grp.getgrgid(new_gid) group_exists = True except KeyError: group_exists = False # If the new_gid does not exist, create it if not group_exists: try: os.system(f\\"groupadd -g {new_gid} {new_group_name}\\") print(f\\"Group {new_group_name} with gid {new_gid} created\\") except Exception as e: raise RuntimeError(f\\"Failed to create new group: {e}\\") try: # Update user\'s group ID os.system(f\\"usermod -g {new_gid} {user_info.pw_name}\\") print(f\\"User {user_info.pw_name}\'s group ID updated to {new_gid}\\") except Exception as e: raise RuntimeError(f\\"Failed to update user\'s group ID: {e}\\")"},{"question":"# Context You are given a list of student records, where each record is a dictionary containing the student\'s id, name, and grade. # Task You need to implement two functions: 1. `find_students(min_grade, students)`: This function should return a list of all student names whose grades are greater than or equal to `min_grade`. 2. `insert_student(student, students)`: This function should insert a new student record into the `students` list, maintaining the order based on student grades. Both functions should use the bisect module to find insertion points or search within the list. # Specifications - The students are provided as a list of dictionaries, where each dictionary has keys `id`, `name`, and `grade`. - For example: ```python students = [ {\'id\': 1, \'name\': \'John Doe\', \'grade\': 75}, {\'id\': 2, \'name\': \'Jane Smith\', \'grade\': 82}, {\'id\': 3, \'name\': \'Emily Davis\', \'grade\': 90} ] ``` Function 1: `find_students(min_grade, students)` - **Input**: - `min_grade`: Integer representing the minimum grade (inclusive). - `students`: List of student records. - **Output**: - List of student names whose grade is greater than or equal to `min_grade`. Function 2: `insert_student(student, students)` - **Input**: - `student`: A single student record dictionary. - `students`: List of student records. - **Output**: - The `students` list is modified in place with the new `student` inserted in the correct position based on the grade. # Constraints - All grades will be between 0 and 100. - The `students` list will be sorted based on grades in ascending order and will have unique grades. # Example ```python from bisect import bisect_left, bisect_right def find_students(min_grade, students): # Your code here pass def insert_student(student, students): # Your code here pass students = [ {\'id\': 1, \'name\': \'John Doe\', \'grade\': 75}, {\'id\': 2, \'name\': \'Jane Smith\', \'grade\': 82}, {\'id\': 3, \'name\': \'Emily Davis\', \'grade\': 90} ] # Example usage print(find_students(80, students)) # Output: [\'Jane Smith\', \'Emily Davis\'] new_student = {\'id\': 4, \'name\': \'Michael Brown\', \'grade\': 85} insert_student(new_student, students) print(students) # Output: [{\'id\': 1, \'name\': \'John Doe\', \'grade\': 75}, # {\'id\': 2, \'name\': \'Jane Smith\', \'grade\': 82}, # {\'id\': 4, \'name\': \'Michael Brown\', \'grade\': 85}, # {\'id\': 3, \'name\': \'Emily Davis\', \'grade\': 90}] ``` You should use the bisect functions to ensure efficient and correct implementations.","solution":"from bisect import bisect_left, insort_left def find_students(min_grade, students): Returns a list of all student names whose grades are greater than or equal to min_grade. index = bisect_left([student[\'grade\'] for student in students], min_grade) return [student[\'name\'] for student in students[index:]] def insert_student(student, students): Inserts a new student record into the students list, maintaining the order based on student grades. insort_left(students, student, key=lambda x: x[\'grade\'])"},{"question":"**Context:** You are working on a distributed machine learning application using PyTorch\'s distributed RPC framework. A common requirement is to handle asynchronous calls and their results efficiently. Your task is to implement a function that utilizes `torch.futures.Future` and associated utility functions to manage a sequence of asynchronous operations. **Objective:** Implement the function `async_pipeline` that performs a sequence of computation steps asynchronously. Each step computation should depend on the result of the previous step. Utilize `torch.futures.Future` to achieve this. **Function Signature:** ```python import torch from torch.futures import Future, collect_all, wait_all def async_pipeline(steps: List[Callable[[Any], Any]], initial_data: Any) -> Future: Execute a sequence of asynchronous steps with each step depending on the previous one. Args: - steps (List[Callable[[Any], Any]]): A list of step functions, each taking an input and returning an output. - initial_data (Any): The initial data to start the pipeline. Returns: - Future: A Future object representing the final output after executing all steps. pass ``` **Instructions:** 1. The `async_pipeline` function should take a list of step functions and an initial data input. 2. Each step function is called asynchronously; the output of a step is fed as input to the next step. 3. The function should return a `torch.futures.Future` object representing the final result of the pipeline. 4. Utilize `Future` and utility functions `collect_all` or `wait_all` as necessary to handle the asynchronous behavior. **Constraints:** - Each step function may take varying amounts of time to complete. - Ensure that the pipeline correctly handles dependencies between steps. - The steps should be executed in the order they appear in the list. **Example Usage:** ```python import torch from torch.futures import Future # Example step functions def step1(data): return data + 1 def step2(data): return data * 2 def step3(data): return data - 3 # Create a list of steps steps = [step1, step2, step3] # Initial data initial_data = 5 # Run the async pipeline final_future = async_pipeline(steps, initial_data) # Blocking call to get the final result once all steps are complete final_result = final_future.wait() print(final_result) # Expected output: (5 + 1) * 2 - 3 = 9 ``` **Performance Requirements:** - Ensure that the implementation is efficient in terms of managing future dependencies. - Consider any potential bottlenecks that could be optimized within the asynchronous call sequence.","solution":"import torch from torch.futures import Future from typing import List, Callable, Any def async_pipeline(steps: List[Callable[[Any], Any]], initial_data: Any) -> Future: Execute a sequence of asynchronous steps with each step depending on the previous one. Args: - steps (List[Callable[[Any], Any]]): A list of step functions, each taking an input and returning an output. - initial_data (Any): The initial data to start the pipeline. Returns: - Future: A Future object representing the final output after executing all steps. # Create an initial Future object for the initial data future = Future() future.set_result(initial_data) for step in steps: # Capture the step in a local variable to ensure correct step function is used in the closure def make_continuation(step): def continuation(fut): result = fut.wait() # Wait on the result of the previous future return step(result) return continuation future = future.then(make_continuation(step)) return future"},{"question":"# Asyncio Task Management and Debugging Problem Statement You are required to write an asynchronous Python program that simulates the processing of a list of URLs. Each URL will take a random amount of time to be processed. The program should demonstrate proper task management, error handling, and debugging practices as outlined in the `asyncio` documentation provided. Requirements 1. **Function Definition**: - Define an asynchronous function `process_url(url: str) -> str`: - This function accepts a URL (string) and returns a string indicating the successful processing of the URL. - It should simulate random delays using `asyncio.sleep`. 2. **Main Execution**: - Define an asynchronous function `main(urls: List[str]) -> None`: - This function should create tasks for all the URLs and ensure all of them are processed. - Include proper error handling to catch and log any exceptions that occur during the processing. - Use `asyncio.create_task` to schedule the tasks. - Ensure that no coroutine is left unawaited and all exceptions are retrieved. 3. **Debugging and Performance**: - Enable `asyncio` debug mode explicitly in your code. - Set the log level of the asyncio logger to `logging.DEBUG`. - Configure the warnings module to display `ResourceWarning`. - Set `loop.slow_callback_duration` to log callbacks taking longer than 0.1 seconds. 4. **Concurrency and Multithreading**: - Use `loop.run_in_executor` to simulate a blocking operation (e.g., CPU-bound computation) that processes the response in a separate thread. 5. **Input and Output**: - Input: A list of URL strings. - Output: Log entries indicating the progress and completion of each URL processing task. Constraints - The list of URLs will have at most 10 elements. - Each URL processing should simulate a random delay between 0.1 to 0.5 seconds. - A blocking operation (CPU-bound task) should take approximately 0.2 seconds. Example ```python import asyncio import logging import random import time from typing import List # Function Definitions async def process_url(url: str) -> str: delay = random.uniform(0.1, 0.5) await asyncio.sleep(delay) return f\\"Processed: {url}\\" def blocking_computation(data: str) -> str: time.sleep(0.2) # Simulating a blocking operation return f\\"Computed: {data}\\" async def main(urls: List[str]) -> None: logging.basicConfig(level=logging.DEBUG) loop = asyncio.get_running_loop() loop.set_debug(True) loop.slow_callback_duration = 0.1 tasks = [] for url in urls: task = asyncio.create_task(process_url(url)) tasks.append(task) try: for task in tasks: result = await task logging.debug(result) # Run the blocking call in a separate thread computation_result = await loop.run_in_executor(None, blocking_computation, result) logging.debug(computation_result) except Exception as e: logging.error(f\\"Error processing URL: {e}\\") if __name__ == \'__main__\': urls = [\\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\"] asyncio.run(main(urls), debug=True) ``` Ensure that you provide appropriate logging and handle all exceptions, including those from forgotten awaits or unhandled task exceptions.","solution":"import asyncio import logging import random import time from typing import List async def process_url(url: str) -> str: Simulates processing of the URL by waiting for a random delay. delay = random.uniform(0.1, 0.5) await asyncio.sleep(delay) return f\\"Processed: {url}\\" def blocking_computation(data: str) -> str: Simulates a CPU-bound blocking operation. time.sleep(0.2) # Simulating a blocking operation return f\\"Computed: {data}\\" async def main(urls: List[str]) -> None: # Configure logging logging.basicConfig(level=logging.DEBUG) loop = asyncio.get_running_loop() loop.set_debug(True) loop.slow_callback_duration = 0.1 tasks = [] for url in urls: task = asyncio.create_task(process_url(url)) tasks.append(task) try: for task in tasks: result = await task logging.debug(result) # Run the blocking computation in a separate thread computation_result = await loop.run_in_executor(None, blocking_computation, result) logging.debug(computation_result) except Exception as e: logging.error(f\\"Error processing URL: {e}\\") if __name__ == \'__main__\': urls = [\\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\"] asyncio.run(main(urls))"},{"question":"# Question: MemoryView Manipulation in Python In this assessment, you will demonstrate your understanding of Python\'s memoryview objects and how to manipulate them efficiently. Write a Python function that takes two inputs: a list of integers and a flag indicating whether the memoryview should be read-only or writable. The function should create and return a memoryview object based on these inputs. Function Signature ```python def create_memoryview(data: list[int], read_only: bool) -> memoryview: pass ``` Input 1. `data` (list[int]): A list of integers. 2. `read_only` (bool): A flag indicating whether the created memoryview should be read-only (`True`) or writable (`False`). Output - Returns a memoryview object created from the input list. Constraints - You must create the memoryview without copying the input data. - If `read_only` is `True`, the memoryview must not allow changes to the data. Example ```python data = [1, 2, 3, 4, 5] read_only = True mv = create_memoryview(data, read_only) print(mv.tolist()) # Output: [1, 2, 3, 4, 5] # Attempt to modify the memoryview (this should raise an error if read_only is True) if not mv.readonly: mv[0] = 10 print(data) # Output: [10, 2, 3, 4, 5] if read_only is False ``` Additional Notes - Use Python\'s built-in `memoryview` function to create the memoryview. - Remember to handle the read-only flag correctly to prevent unauthorized modifications. - Make sure your function handles edge cases, such as an empty list. Performance Requirements - Ensure the function performs efficiently without unnecessary copying of data.","solution":"def create_memoryview(data: list[int], read_only: bool) -> memoryview: Creates a memoryview object based on the input data. If read_only is True, the memoryview should not allow modifications. # Convert the list of integers to a bytes object byte_data = bytes(data) # Create the memoryview from the bytes object mv = memoryview(byte_data) if read_only: # Return the read-only memoryview return mv else: # Create a mutable array from the list of integers import array byte_array = array.array(\'B\', data) # Return the writable memoryview return memoryview(byte_array)"},{"question":"# Coding Exercise: Traceback Analyzer Utilize the `traceback` module to develop a custom traceback analyzer function, `analyze_traceback`. This function should perform the following steps: 1. **Execute an arbitrary piece of code provided as a string.** 2. **Capture and format any exceptions raised, displaying both the traceback and the exception details in a readable format.** 3. **Return a dictionary with the following keys:** - `\\"type\\"`: The type of the exception. - `\\"message\\"`: The message of the exception. - `\\"traceback\\"`: A formatted string of the stack trace. - `\\"formatted\\"`: A formatted version of the entire exception including the traceback. Inputs: - `code` (str): A string containing the code to execute. Outputs: - A dictionary containing the exception type, exception message, stack trace, and formatted exception information as described above. Function Signature: ```python def analyze_traceback(code: str) -> dict: ``` # Constraints: - Assume the code provided will always be a valid Python snippet. - The executed code may raise any type of exception. - Consider performance for large tracebacks, but focus on correctness and completeness. # Example: ```python code_snippet = def my_func(): x = 1 / 0 # This will cause a ZeroDivisionError my_func() result = analyze_traceback(code_snippet) # Sample output for the result may look similar to (in actual function, values can vary based on the environment): # { # \\"type\\": \\"ZeroDivisionError\\", # \\"message\\": \\"division by zero\\", # \\"traceback\\": \\" File \'<string>\', line 4, in <module>n my_func()n File \'<string>\', line 2, in my_funcn x = 1 / 0n\\", # \\"formatted\\": \\"Traceback (most recent call last):n File \'<string>\', line 4, in <module>n my_func()n File \'<string>\', line 2, in my_funcn x = 1 / 0nZeroDivisionError: division by zeron\\" # } ``` Make sure your implementation properly handles capturing and formatting exceptions using the `traceback` module. Note: Do not print any output inside the function; just return the dictionary.","solution":"import traceback def analyze_traceback(code: str) -> dict: Executes the given code and analyzes any traceback if an exception occurs. Parameters: code (str): The python code to execute. Returns: dict: A dictionary containing type, message, traceback, and formatted exception details. try: exec(code) return {\\"type\\": None, \\"message\\": None, \\"traceback\\": None, \\"formatted\\": None} except Exception as e: tb_str = traceback.format_exc() return { \\"type\\": type(e).__name__, \\"message\\": str(e), \\"traceback\\": \'\'.join(traceback.format_tb(e.__traceback__)), \\"formatted\\": tb_str }"},{"question":"Objective: Assess the student\'s understanding of using scikit-learn\'s imputation techniques to handle missing values and integrate these techniques into a machine learning pipeline for classification tasks. Problem Statement: You are provided with a dataset that simulates missing values similar to real-world scenarios. Your task is to implement a machine learning pipeline that can handle these missing values using different imputation strategies and then classify the data. Dataset: Consider the following simulated dataset: ```python import numpy as np import pandas as pd data = { \'feature1\': [5, np.nan, 8, 9, 7, np.nan, 6], \'feature2\': [np.nan, 3.5, np.nan, 7.2, 6.4, 8.1, np.nan], \'feature3\': [1.2, 2.3, np.nan, 3.8, 2.7, 1.9, 2.6], \'feature4\': [\'A\', \'B\', \'C\', np.nan, \'C\', \'A\', \'B\'], \'target\': [1, 0, 1, 1, 0, 0, 1] } df = pd.DataFrame(data) ``` Instructions: 1. **Imputation**: - Implement an imputation pipeline that uses `SimpleImputer` for numerical features with the strategy \'mean\'. - Use `SimpleImputer` for categorical features with the strategy \'most_frequent\'. - Use `MissingIndicator` to mark the imputed values. 2. **Modeling**: - Integrate the imputation process into a pipeline with a classifier of your choice (e.g., `RandomForestClassifier`). 3. **Evaluation**: - Split the dataset into training and testing sets. - Train the model on the training set. - Evaluate the model on the testing set and print out the accuracy score. Expected implementation: ```python from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer, MissingIndicator from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.preprocessing import OneHotEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Step 1: Prepare the dataset features = df.drop(\'target\', axis=1) target = df[\'target\'] # Step 2: Define column groups numerical_features = [\'feature1\', \'feature2\', \'feature3\'] categorical_features = [\'feature4\'] # Step 3: Define column transformer for the pipeline numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'indicator\', MissingIndicator()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Step 4: Create the pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', RandomForestClassifier()) ]) # Step 5: Split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42) # Step 6: Train the model pipeline.fit(X_train, y_train) # Step 7: Predict and evaluate y_pred = pipeline.predict(X_test) print(\\"Accuracy:\\", accuracy_score(y_test, y_pred)) ``` Constraints: - You should not use any external library other than scikit-learn, numpy, and pandas. - Ensure your solution is efficient and handles edge cases (e.g., all values missing in a column). Submit your solution as a Python script or Jupyter notebook. Performance Requirement: - The pipeline should handle imputation efficiently and should be able to classify the data with reasonable accuracy.","solution":"from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer, MissingIndicator from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.preprocessing import OneHotEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import pandas as pd import numpy as np # Define the dataset data = { \'feature1\': [5, np.nan, 8, 9, 7, np.nan, 6], \'feature2\': [np.nan, 3.5, np.nan, 7.2, 6.4, 8.1, np.nan], \'feature3\': [1.2, 2.3, np.nan, 3.8, 2.7, 1.9, 2.6], \'feature4\': [\'A\', \'B\', \'C\', np.nan, \'C\', \'A\', \'B\'], \'target\': [1, 0, 1, 1, 0, 0, 1] } df = pd.DataFrame(data) # Step 1: Prepare the dataset features = df.drop(\'target\', axis=1) target = df[\'target\'] # Step 2: Define column groups numerical_features = [\'feature1\', \'feature2\', \'feature3\'] categorical_features = [\'feature4\'] # Step 3: Define column transformer for the pipeline numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'indicator\', MissingIndicator()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Step 4: Create the pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', RandomForestClassifier()) ]) # Step 5: Split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42) # Step 6: Train the model pipeline.fit(X_train, y_train) # Step 7: Predict and evaluate y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(\\"Accuracy:\\", accuracy)"},{"question":"**Unicode String Processing in Python** **Objective:** Assess the studentsâ€™ understanding and ability to handle Unicode strings, conversions, and normalization using Python. **Problem Statement:** You are given a list of strings containing various Unicode characters. Write a Python function called `normalize_and_compare_strings` that takes in two arguments: 1. `strings`: A list of Unicode strings where each string might contain characters with different representations (e.g., composed characters and decomposed characters). 2. `query`: A single Unicode string. Your function should: 1. Normalize all strings in the list using Normalization Form Canonical Composition (NFC). 2. Normalize the query string using the same form. 3. Return a list of indices of the strings in the input list that are equal to the normalized query string when normalized. **Constraints:** - The list of strings may contain up to 10,000 elements. - Each string in the list can have up to 1,000 characters. - You may use the unicodedata module for normalization. - Performance is crucial; the solution should be efficient as possible given the constraints. **Input Format:** - `strings`: List[str] - A list of Unicode strings. - `query`: str - A single Unicode string. **Output Format:** - List[int] - The list of indices where the normalized `query` string matches with the normalized strings from the input list. **Examples:** Example 1: ```python strings = [\'Ãª\', \'eÌ‚\', \'Ã©\', \'Ãª\'] query = \'Ãª\' print(normalize_and_compare_strings(strings, query)) ``` Output: ```python [0, 1, 3] ``` Example 2: ```python strings = [\'GÃ¼rzenichstraÃŸe\', \'gÃ¼rzenichstrasse\', \'GÃœrzenichstraÃŸe\'] query = \'gÃ¼rzenichstraÃŸe\' print(normalize_and_compare_strings(strings, query)) ``` Output: ```python [0, 1] ``` **Function Signature:** ```python def normalize_and_compare_strings(strings: List[str], query: str) -> List[int]: # Write your implementation here ``` **Note:** - Use the `unicodedata.normalize` function to normalize the strings. - Ensure that the comparison is case-insensitive. **Requirements:** - Import necessary modules from the Python standard library. - Implement the function according to the specified signature. - Your code should be efficient and run correctly within the given constraints.","solution":"import unicodedata def normalize_and_compare_strings(strings, query): Normalizes the list of Unicode strings and the query string using NFC, and returns a list of indices where the normalized strings match the normalized query. # Normalize the query string normalized_query = unicodedata.normalize(\'NFC\', query).casefold() # Normalize all strings in the list normalized_strings = [unicodedata.normalize(\'NFC\', s).casefold() for s in strings] # Find and return the list of matching indices return [i for i, s in enumerate(normalized_strings) if s == normalized_query]"},{"question":"Coding Assessment Question # Objective Implement a function `select_persistent_algorithm` that checks if a given set of conditions are met to select the persistent algorithm for performance improvement when using GPU-based computations in PyTorch. # Specifications Function signature ```python def select_persistent_algorithm(tensor: torch.Tensor, use_cudnn: bool, gpu_type: str) -> bool: pass ``` Input - `tensor` (torch.Tensor): The input data tensor. - `use_cudnn` (bool): A boolean flag indicating if cudnn is enabled. - `gpu_type` (str): A string representing the GPU type (e.g., \\"V100\\"). Output - `bool`: Returns `True` if all conditions for selecting the persistent algorithm are met, otherwise `False`. Conditions To select the persistent algorithm, the following conditions must be satisfied: 1. cudnn must be enabled (`use_cudnn` is `True`). 2. The input tensor must be on the GPU. 3. The dtype of the input tensor must be `torch.float16`. 4. The GPU type must be \\"V100\\". 5. The input tensor must not be in `PackedSequence` format (for simplicity, assume that the input will never be in `PackedSequence` format). # Example ```python import torch # Example 1: Using all required conditions tensor = torch.randn(10, 10, dtype=torch.float16, device=\'cuda\') use_cudnn = True gpu_type = \\"V100\\" print(select_persistent_algorithm(tensor, use_cudnn, gpu_type)) # should return True # Example 2: Not using the right dtype tensor = torch.randn(10, 10, dtype=torch.float32, device=\'cuda\') print(select_persistent_algorithm(tensor, use_cudnn, gpu_type)) # should return False # Example 3: Using CPU instead of GPU tensor = torch.randn(10, 10, dtype=torch.float16, device=\'cpu\') print(select_persistent_algorithm(tensor, use_cudnn, gpu_type)) # should return False ``` # Constraints - Assume the input will always be in plain `torch.Tensor` format (not in `PackedSequence` format). - Use of PyTorch library is required. # Notes Ensure your function efficiently checks the conditions and makes use of relevant properties/methods of the PyTorch library to handle tensor operations and it does not cause any unnecessary GPU memory operations.","solution":"import torch def select_persistent_algorithm(tensor: torch.Tensor, use_cudnn: bool, gpu_type: str) -> bool: Checks if the persistent algorithm can be selected based on specified conditions. Parameters: tensor (torch.Tensor): The input data tensor. use_cudnn (bool): Boolean flag indicating if cudnn is enabled. gpu_type (str): The GPU type. Returns: bool: True if all conditions for selecting the persistent algorithm are met, otherwise False. return (use_cudnn and tensor.is_cuda and tensor.dtype == torch.float16 and gpu_type == \\"V100\\")"},{"question":"**Question: Sales Data Analysis with pandas** You are provided with fictitious sales data for a company across three months. The sales data is stored in a CSV file named `sales_data.csv` with the following columns: 1. `Date`: The date of the transaction. 2. `Item`: The item sold. 3. `Region`: The region where the transaction took place. 4. `Quantity`: The number of items sold. 5. `Price`: The price per item. **Example of the CSV data:** ```csv Date,Item,Region,Quantity,Price 2023-01-01,Widget A,North,10,5.0 2023-01-01,Widget B,South,2,15.0 2023-01-02,Widget A,East,5,5.5 ... ``` You need to write a Python function using pandas that performs the following tasks: 1. **Reading Data**: Load the CSV file into a pandas DataFrame. 2. **Data Cleaning**: Remove any rows where any of the `Quantity` or `Price` fields are missing or set to zero. 3. **Total Sales Calculation**: Add a new column `Total_Sales` to the DataFrame, which is the product of `Quantity` and `Price`. 4. **Revenue Calculation**: - Calculate the total revenue (sum of `Total_Sales`) for each region. - Extract the region with the highest total revenue. 5. **Monthly Sales Analysis**: - Add a `Month` column, extracted from the `Date`, grouped by month-level granularity. - Calculate the average sales (`Total_Sales`) per item per month. 6. **Returning Results**: Ensure the function returns the following: - The cleaned DataFrame. - A dictionary containing total revenue for each region. - The average monthly sales DataFrame with columns `Month`, `Item`, and `Average_Sales`. **Function Signature:** ```python import pandas as pd def analyze_sales_data(file_path: str) -> tuple: Analyzes sales data from a given CSV file. Parameters: file_path (str): The path to the CSV file. Returns: tuple: A tuple containing: - DataFrame: The cleaned DataFrame. - dict: A dictionary with region names as keys and total revenue as values. - DataFrame: A DataFrame with columns \'Month\', \'Item\', and \'Average_Sales\'. ``` **Constraints:** - The `Date` column is in the format `YYYY-MM-DD`. - Ensure optimal performance even for large datasets. **Note**: You may use built-in pandas functions to make this task efficient. **Example Usage:** ```python cleaned_df, revenue_by_region, avg_monthly_sales = analyze_sales_data(\'sales_data.csv\') print(cleaned_df.head()) print(revenue_by_region) print(avg_monthly_sales.head()) ``` **Expected Output:** For the example usage, the function should provide: 1. A cleaned DataFrame with no missing or zero values in `Quantity` or `Price`. 2. A dictionary with key-value pairs indicating regions and their total revenue. 3. A DataFrame summarizing the average monthly sales for each item.","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> tuple: Analyzes sales data from a given CSV file. Parameters: file_path (str): The path to the CSV file. Returns: tuple: A tuple containing: - DataFrame: The cleaned DataFrame. - dict: A dictionary with region names as keys and total revenue as values. - DataFrame: A DataFrame with columns \'Month\', \'Item\', and \'Average_Sales\'. # Reading Data df = pd.read_csv(file_path) # Data Cleaning: Remove rows with missing or zero values in \'Quantity\' or \'Price\' df_cleaned = df[(df[\'Quantity\'] > 0) & (df[\'Price\'] > 0)] # Total Sales Calculation df_cleaned[\'Total_Sales\'] = df_cleaned[\'Quantity\'] * df_cleaned[\'Price\'] # Revenue Calculation: Total revenue for each region revenue_by_region = df_cleaned.groupby(\'Region\')[\'Total_Sales\'].sum().to_dict() # Finding the region with highest total revenue max_revenue_region = max(revenue_by_region, key=revenue_by_region.get) # Monthly Sales Analysis df_cleaned[\'Month\'] = pd.to_datetime(df_cleaned[\'Date\']).dt.to_period(\'M\').astype(str) avg_monthly_sales = df_cleaned.groupby([\'Month\', \'Item\'])[\'Total_Sales\'].mean().reset_index() avg_monthly_sales.rename(columns={\'Total_Sales\': \'Average_Sales\'}, inplace=True) return df_cleaned, revenue_by_region, avg_monthly_sales"},{"question":"Objective Implement a multi-threaded TCP server using the `socketserver` module, capable of handling multiple client requests concurrently. Each client should be able to send multiple lines of text, and the server should respond with each line reversed. Requirements - The server should handle multiple clients concurrently using threading. - Each client connection should handle multiple lines of text input. - For each line received, the server should respond with the reversed string of that line. - The server should include appropriate error handling and clean-up processes. Specifications 1. **Server Implementation** - Create a class `ReversingTCPHandler` that subclasses `socketserver.StreamRequestHandler`. - Override the `handle()` method to: - Read multiple lines until the client closes the connection. - For each line read, reverse the string and send it back to the client. - Create a class `ThreadedTCPServer` that subclasses both `socketserver.ThreadingMixIn` and `socketserver.TCPServer`. - The server should run indefinitely until manually interrupted. 2. **Client Implementation** - Implement a simple client that connects to the server, sends multiple lines of text, and prints the server\'s responses. - Ensure the client properly closes the connection after sending all the lines. Code Template ```python import socketserver import threading import socket class ReversingTCPHandler(socketserver.StreamRequestHandler): def handle(self): try: while True: data = self.rfile.readline().strip() if not data: break self.wfile.write(data[::-1] + b\'n\') except Exception as e: print(f\\"Error handling client {self.client_address}: {e}\\") class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TTCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedTCPServer((HOST, PORT), ReversingTCPHandler) as server: server_thread = threading.Thread(target=server.serve_forever) server_thread.daemon = True server_thread.start() print(f\\"Server started on {HOST}:{PORT}\\") server_thread.join() def client(ip, port, messages): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: sock.connect((ip, port)) try: for message in messages: sock.sendall(bytes(message + \\"n\\", \\"utf-8\\")) response = str(sock.recv(1024), \'utf-8\') print(\\"Received:\\", response.strip()) except Exception as e: print(f\\"Error during communication: {e}\\") if __name__ == \\"__main__\\": client_messages = [\\"Hello, world!\\", \\"Python is fun\\", \\"Goodbye!\\"] client(\\"localhost\\", 9999, client_messages) ``` Constraints - Performance should be acceptable for handling up to 100 concurrent client connections. - Ensure proper resource clean-up to avoid potential memory leaks or socket issues. Notes - This exercise assesses your understanding of multi-threaded server implementation, handling of TCP/IP communication, and ability to clean up resources safely. - You may experiment with different levels of concurrency to test your implementation.","solution":"import socketserver import threading import socket class ReversingTCPHandler(socketserver.StreamRequestHandler): def handle(self): try: while True: data = self.rfile.readline().strip() if not data: break self.wfile.write(data[::-1] + b\'n\') except Exception as e: print(f\\"Error handling client {self.client_address}: {e}\\") class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedTCPServer((HOST, PORT), ReversingTCPHandler) as server: server_thread = threading.Thread(target=server.serve_forever) server_thread.daemon = True server_thread.start() print(f\\"Server started on {HOST}:{PORT}\\") server_thread.join() def client(ip, port, messages): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: sock.connect((ip, port)) try: for message in messages: sock.sendall(bytes(message + \\"n\\", \\"utf-8\\")) response = str(sock.recv(1024), \'utf-8\').strip() print(\\"Received:\\", response) except Exception as e: print(f\\"Error during communication: {e}\\")"},{"question":"Objective Your task is to write a Python function that analyzes a given Python script and breaks it into its fundamental tokens. This exercise will test your understanding of Python lexical analysis, including tokenizing logical and physical lines, handling different types of tokens, and respecting the rules for comments, indentation, and line joining. Description Write a function `analyze_python_script(script: str) -> List[str]` that takes a string `script` representing the content of a Python script and returns a list of tokens. Input - `script` (str): A multiline string representing the contents of a Python script. Output - `List[str]`: A list of tokens extracted from the script. Constraints 1. Handle both single-line and multiline comments. 2. Handle both implicit and explicit line joining. 3. Recognize and properly tokenize strings, including raw and formatted strings. 4. Tokenize keywords, identifiers, literals (string, bytes, numeric, and imaginary), operators, and delimiters. 5. Handle indentation levels and whitespace between tokens. Example ```python script = # This is a comment def example_function(param1, param2): if param1 > param2: print(\\"param1 is greater\\") return param1 + param2 print(analyze_python_script(script)) # Output: # [\'# This is a comment\', \'def\', \'example_function\', \'(\', \'param1\', \',\', \'param2\', \')\', \':\', \'if\', \'param1\', \'>\', \'param2\', \':\', \'print\', \'(\', \'\\"param1 is greater\\"\', \')\', \'return\', \'param1\', \'+\', \'param2\'] ``` Notes - You are allowed to use the `re` library for regular expressions if needed. - Pay attention to the provided documentation about different token types and their respective rules for a more accurate parsing.","solution":"import tokenize import io def analyze_python_script(script): Analyzes a given Python script and breaks it into fundamental tokens. Args: script (str): A multiline string representing the contents of a Python script. Returns: List[str]: A list of tokens extracted from the script. tokens = [] token_generator = tokenize.generate_tokens(io.StringIO(script).readline) for token in token_generator: if token.type in (tokenize.COMMENT, tokenize.STRING, tokenize.OP, tokenize.NAME, tokenize.NUMBER): tokens.append(token.string) return tokens"},{"question":"Objective: You are tasked to implement a few TorchScript functions that will be part of a small neural network module. Your implementation will demonstrate proficiency in using types, control flow, and static typing within TorchScript. Problem Statement: You need to create a TorchScript class `MyModule` that includes: 1. A forward method that performs different operations based on the input type. 2. A method to compute the sum of all elements in a Tensor. 3. A method to compute the average of values in a list of tensors. 4. Demonstrating type refinement using optional values and employing various TorchScript-supported operations. Implement the following: 1. **`__init__` Method**: - Initialize a parameter `scale` as a tensor of shape `(1,)` with value 1.0. - Initialize an integer attribute `counter` to 0. 2. **`forward` Method**: - Accepts a parameter `input` of type `Union[torch.Tensor, Tuple[int, int]]`. - If `input` is a tensor, it scales the tensor by the `scale` attribute. - If `input` is a tuple, it returns a tensor of zeros with the dimensions specified by the tuple. - Output type: `torch.Tensor`. 3. **`sum_tensor` Method**: - Accepts a tensor `x` of any shape and returns the sum of all its elements as a tensor. - Output type: `torch.Tensor`. 4. **`average_list` Method**: - Accepts a list of tensors `x` of any shape and returns the average of their sums as a tensor. - Output type: `torch.Tensor`. 5. **`optional_refinement` Method**: - Accepts an optional integer `x`. - If `x` is `None`, assigns it the value of `counter`. - If `x` is not `None`, increments the `counter` by the value of `x`. - Returns the updated `counter`. Constraints: - **You must use the TorchScript-compatible subset of Python** as detailed in the documentation. - **Static typing and type annotations** must be strictly followed. - **Using unsupported constructs** will result in errors during the script compilation. Example Usage: ```python import torch @torch.jit.script class MyModule: def __init__(self): self.scale = torch.tensor([1.0]) self.counter = 0 def forward(self, input: Union[torch.Tensor, Tuple[int, int]]) -> torch.Tensor: if isinstance(input, torch.Tensor): return input * self.scale elif isinstance(input, tuple): return torch.zeros(input) else: raise RuntimeError(\\"Unsupported input type\\") def sum_tensor(self, x: torch.Tensor) -> torch.Tensor: return torch.sum(x) def average_list(self, x: List[torch.Tensor]) -> torch.Tensor: total_sum = torch.sum(torch.stack([torch.sum(t) for t in x])) return total_sum / len(x) def optional_refinement(self, x: Optional[int]) -> int: if x is None: x = self.counter else: self.counter += x return self.counter # Example module = MyModule() print(module.forward(torch.tensor([1.0, 2.0, 3.0]))) print(module.forward((2, 2))) print(module.sum_tensor(torch.tensor([[1, 2], [3, 4]]))) print(module.average_list([torch.tensor([1, 2]), torch.tensor([3, 4])])) print(module.optional_refinement(None)) print(module.optional_refinement(5)) ``` Submission: Your submission should include the implemented `MyModule` class in a single .py file. Make sure to thoroughly test your class methods before submission to ensure compliance with the requirements.","solution":"import torch from typing import Union, Tuple, List, Optional @torch.jit.script class MyModule: def __init__(self): self.scale = torch.tensor([1.0]) self.counter = 0 def forward(self, input: Union[torch.Tensor, Tuple[int, int]]) -> torch.Tensor: if isinstance(input, torch.Tensor): return input * self.scale elif isinstance(input, tuple): return torch.zeros(input) else: raise RuntimeError(\\"Unsupported input type\\") def sum_tensor(self, x: torch.Tensor) -> torch.Tensor: return torch.sum(x) def average_list(self, x: List[torch.Tensor]) -> torch.Tensor: total_sum = torch.sum(torch.stack([torch.sum(t) for t in x])) return total_sum / len(x) def optional_refinement(self, x: Optional[int]) -> int: if x is None: x = self.counter else: self.counter += x return self.counter"},{"question":"You are provided with sales data of an online store, and you need to perform various data manipulation and analysis tasks using pandas. Your tasks are outlined below. # Input Format 1. A CSV file named `sales.csv` with the following columns: - `order_id`: Unique identifier for each order. - `date`: Date of the order (in yyyy-mm-dd format). - `customer_id`: Unique identifier for each customer. - `product_id`: Product identifier. - `quantity`: Quantity of the product ordered. - `price`: Price of the product. 2. A CSV file named `products.csv` with the following columns: - `product_id`: Unique identifier for each product. - `product_name`: Name of the product. - `category`: Category of the product. # Objectives 1. **Load Data:** - Read the sales data from `sales.csv` into a DataFrame named `sales_df`. - Read the product data from `products.csv` into a DataFrame named `products_df`. 2. **Data Preprocessing:** - Handle any missing values in the `sales_df` by filling them with appropriate values. Explain the choice of fill values. - Ensure the date column in `sales_df` is in datetime format. 3. **Data Analysis:** - Merge `sales_df` with `products_df` using `product_id` to include `product_name` and `category` in the `sales_df`. - Create a new column in `sales_df` named `total_price` which is the product of `quantity` and `price`. - Group the data by `category` and `date`, then calculate the total quantity sold and total revenue for each category per day. 4. **Visualization:** - Plot a line chart showing total revenue over time for each category. Each category should have a separate line. 5. **Advanced Analysis:** - Identify the top 5 products that generated the highest revenue. - Identify the customer who has placed the highest number of orders and the total amount spent by this customer. # Constraints - Assume the datasets are reasonably large, and you should optimize your operations to handle them efficiently where possible. # Expected Output - Modified `sales_df` after merging with product details. - DataFrame with daily total quantity sold and total revenue for each category. - Line chart showing total revenue over time for each category. - List of top 5 products by revenue. - The customer with the highest number of orders and the amount they spent. # Example ```python import pandas as pd import matplotlib.pyplot as plt # Load Data sales_df = pd.read_csv(\'sales.csv\') products_df = pd.read_csv(\'products.csv\') # Data Preprocessing sales_df[\'date\'] = pd.to_datetime(sales_df[\'date\']) sales_df[\'quantity\'].fillna(0, inplace=True) sales_df[\'price\'].fillna(sales_df[\'price\'].median(), inplace=True) # Merge DataFrames merged_df = pd.merge(sales_df, products_df, on=\'product_id\') # Add total_price column merged_df[\'total_price\'] = merged_df[\'quantity\'] * merged_df[\'price\'] # Group by category and date grouped_df = merged_df.groupby([\'category\', \'date\']).agg({ \'quantity\': \'sum\', \'total_price\': \'sum\' }).reset_index() # Plot line chart for category in grouped_df[\'category\'].unique(): category_data = grouped_df[grouped_df[\'category\'] == category] plt.plot(category_data[\'date\'], category_data[\'total_price\'], label=category) plt.xlabel(\'Date\') plt.ylabel(\'Total Revenue\') plt.title(\'Total Revenue Over Time by Category\') plt.legend() plt.show() # Top 5 products by revenue top_products = merged_df.groupby(\'product_name\')[\'total_price\'].sum().nlargest(5).reset_index() # Customer with highest number of orders top_customer = sales_df[\'customer_id\'].value_counts().idxmax() total_spent = merged_df[merged_df[\'customer_id\'] == top_customer][\'total_price\'].sum() ``` Implement this solution and ensure it works correctly with the provided data.","solution":"import pandas as pd import matplotlib.pyplot as plt # Load Data def load_data(sales_file, products_file): sales_df = pd.read_csv(sales_file) products_df = pd.read_csv(products_file) return sales_df, products_df # Data Preprocessing def preprocess_data(sales_df): sales_df[\'date\'] = pd.to_datetime(sales_df[\'date\']) sales_df[\'quantity\'].fillna(0, inplace=True) # Assuming missing quantity means zero quantity sales_df[\'price\'].fillna(sales_df[\'price\'].median(), inplace=True) # Filling missing prices with the median price return sales_df # Merge DataFrames def merge_data(sales_df, products_df): merged_df = pd.merge(sales_df, products_df, on=\'product_id\') return merged_df # Add total_price column def add_total_price_column(merged_df): merged_df[\'total_price\'] = merged_df[\'quantity\'] * merged_df[\'price\'] return merged_df # Group by category and date def group_by_category_and_date(merged_df): grouped_df = merged_df.groupby([\'category\', \'date\']).agg({ \'quantity\': \'sum\', \'total_price\': \'sum\' }).reset_index() return grouped_df # Plot line chart def plot_total_revenue_by_category(grouped_df): for category in grouped_df[\'category\'].unique(): category_data = grouped_df[grouped_df[\'category\'] == category] plt.plot(category_data[\'date\'], category_data[\'total_price\'], label=category) plt.xlabel(\'Date\') plt.ylabel(\'Total Revenue\') plt.title(\'Total Revenue Over Time by Category\') plt.legend() plt.show() # Top 5 products by revenue def get_top_5_products_by_revenue(merged_df): top_products = merged_df.groupby(\'product_name\')[\'total_price\'].sum().nlargest(5).reset_index() return top_products # Customer with highest number of orders and their total spent def get_top_customer_and_total_spent(sales_df, merged_df): top_customer = sales_df[\'customer_id\'].value_counts().idxmax() total_spent = merged_df[merged_df[\'customer_id\'] == top_customer][\'total_price\'].sum() return top_customer, total_spent"},{"question":"# Objective: Implement a PyTorch module that uses `torch.cond` to perform different operations on input tensors based on their properties. This question will assess your understanding of structured control flow operators in PyTorch and your ability to work with dynamic model behavior based on tensor properties. # Problem Statement: Create a class `ComplexConditionModel` that inherits from `torch.nn.Module`. This model should: 1. Accept an input tensor `x`. 2. If the sum of elements in `x` is greater than a given threshold `t`, apply the following operations: - Compute the cosine of each element in `x`. - Compute the exponential of each element in the result from the previous step. 3. Otherwise: - Compute the sine of each element in `x`. - Multiply the result by a scalar `s`. 4. Return the final result after applying the respective operations. # Function Signature: ```python class ComplexConditionModel(torch.nn.Module): def __init__(self, threshold: float, scalar: float): Initializes the model with the given threshold and scalar. Args: threshold (float): The threshold value to compare the sum of elements in the input tensor against. scalar (float): The scalar value to multiply the sine of elements with when the condition is not met. super().__init__() self.threshold = threshold self.scalar = scalar def forward(self, x: torch.Tensor) -> torch.Tensor: Applies dynamic operations on the input tensor based on the condition. Args: x (torch.Tensor): The input tensor. Returns: torch.Tensor: The output tensor after applying the respective operations. # Implementation details go here ``` # Example: ```python import torch from complex_condition_model import ComplexConditionModel # Initialize the model with a threshold of 10.0 and a scalar of 0.5 model = ComplexConditionModel(threshold=10.0, scalar=0.5) # Input tensor input_tensor = torch.tensor([1.0, 2.0, 3.0]) # Forward pass output_tensor = model(input_tensor) print(output_tensor) ``` # Constraints: - You must use `torch.cond` to implement the branching. - Assume that all input tensors will be 1-dimensional for simplicity. - You can use any standard mathematical operations available in PyTorch, such as `torch.cos`, `torch.sin`, `torch.exp`, etc. # Performance Requirements: - Your implementation should handle large input tensors efficiently. - Ensure that operations inside the `true_fn` and `false_fn` are optimized for performance. # Notes: - Test your implementation with different thresholds, scalars, and input tensors to ensure correctness. - Ensure proper handling of edge cases, such as empty tensors or tensors with all zero elements.","solution":"import torch from torch import nn class ComplexConditionModel(nn.Module): def __init__(self, threshold: float, scalar: float): Initializes the model with the given threshold and scalar. Args: threshold (float): The threshold value to compare the sum of elements in the input tensor against. scalar (float): The scalar value to multiply the sine of elements with when the condition is not met. super().__init__() self.threshold = threshold self.scalar = scalar def forward(self, x: torch.Tensor) -> torch.Tensor: Applies dynamic operations on the input tensor based on the condition. Args: x (torch.Tensor): The input tensor. Returns: torch.Tensor: The output tensor after applying the respective operations. def true_fn(): # Compute the cosine of each element in x cos_x = torch.cos(x) # Compute the exponential of each element in the result from the previous step return torch.exp(cos_x) def false_fn(): # Compute the sine of each element in x sin_x = torch.sin(x) # Multiply the result by a scalar return sin_x * self.scalar # Check whether the sum of elements in x is greater than the given threshold result = torch.where(torch.sum(x) > self.threshold, true_fn(), false_fn()) return result"},{"question":"You are given a PyTorch neural network model that uses Batch Normalization (BatchNorm) layers. To make this model compatible with `vmap` from `functorch`, you need to modify the model to replace all BatchNorm layers with Group Normalization (GroupNorm) layers or to adjust the existing BatchNorm layers so they do not use running statistics. For simplicity, assume that all BatchNorm layers are of type `BatchNorm2d`. Your task is to write a function `convert_model` which takes a PyTorch model and converts all BatchNorm2d layers to GroupNorm layers. GroupNorm layers should be configured such that the number of groups `G` is `C` (each channel is treated separately). Function Signature ```python import torch.nn as nn def convert_model(model: nn.Module) -> nn.Module: pass ``` Input - `model`: A PyTorch neural network model (instance of `nn.Module`) which may contain several `BatchNorm2d` layers. Output - Returns the modified model where all `BatchNorm2d` layers have been replaced with `GroupNorm` layers. Constraints - You should not modify the original BatchNorm behavior beyond replacing it with GroupNorm. - The number of channels `C` in the BatchNorm2d directly corresponds to the number of channels in GroupNorm, with `num_groups=C`. Example Suppose you have a simple network as follows: ```python class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.bn2 = nn.BatchNorm2d(32) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.conv2(x) x = self.bn2(x) return x model = SimpleModel() ``` After calling `convert_model(model)`, the modified model should replace each `BatchNorm2d` with a `GroupNorm`. Note - You may assume that `nn.BatchNorm2d`, `nn.GroupNorm`, and other necessary imports have already been made. - Unit tests for verifying your solution will directly compare the layers in the original and converted models to ensure correctness. Implement your solution in the `convert_model` function.","solution":"import torch.nn as nn def convert_model(model: nn.Module) -> nn.Module: Convert all BatchNorm2d layers in the model to GroupNorm layers. GroupNorm layers should be configured such that the number of groups `G` is `C` (each channel is treated separately). for name, module in model.named_children(): if isinstance(module, nn.BatchNorm2d): num_channels = module.num_features gn = nn.GroupNorm(num_groups=num_channels, num_channels=num_channels) setattr(model, name, gn) else: convert_model(module) return model # Example usage: # class SimpleModel(nn.Module): # def __init__(self): # super(SimpleModel, self).__init__() # self.conv1 = nn.Conv2d(3, 16, 3, 1) # self.bn1 = nn.BatchNorm2d(16) # self.conv2 = nn.Conv2d(16, 32, 3, 1) # self.bn2 = nn.BatchNorm2d(32) # # def forward(self, x): # x = self.conv1(x) # x = self.bn1(x) # x = self.conv2(x) # x = self.bn2(x) # return x # # model = SimpleModel() # converted_model = convert_model(model) # print(converted_model)"},{"question":"# Task You are given a `.netrc` file that contains login details for various hosts. Your task is to write a Python function that reads this file and retrieves authentication credentials for a specific host. You must handle scenarios where the file does not exist, is improperly formatted, or has incorrect permissions. # Specifications 1. **Function Name**: `get_auth_details` 2. **Input**: - `file_path` (string): The path to the `.netrc` file. - `host` (string): The hostname for which to retrieve authentication details. 3. **Output**: - A tuple `(login, account, password)` if the host is found. - `None` if the host or default entry is not found. # Constraints - If the file does not exist, raise a `FileNotFoundError`. - If there is a parsing error in the file, raise a `netrc.NetrcParseError`. - If the file has insecure permissions, raise a `PermissionError`. # Example ```python def get_auth_details(file_path: str, host: str): # Your implementation here # Use case: # Assuming the .netrc file at \'test.netrc\' has the following content: # machine example.com # login user # account test_account # password secret print(get_auth_details(\'test.netrc\', \'example.com\')) # Output: (\'user\', \'test_account\', \'secret\') print(get_auth_details(\'test.netrc\', \'nonexistent.com\')) # Output: None ``` # Note - Ensure your function properly handles and raises the appropriate exceptions. - Assume that the provided file path is a valid path string. - Test your function with various scenarios including file not found, incorrect format, and permission issues.","solution":"import os import netrc def get_auth_details(file_path: str, host: str): Retrieves authentication details for a specific host from a .netrc file. Args: - file_path (string): The path to the .netrc file. - host (string): The hostname for which to retrieve authentication details. Returns: - A tuple (login, account, password) if the host is found. - None if the host or default entry is not found. Raises: - FileNotFoundError if the file does not exist. - netrc.NetrcParseError if there is a parsing error in the file. - PermissionError if the file has insecure permissions. if not os.path.exists(file_path): raise FileNotFoundError(f\\"The file {file_path} does not exist.\\") # Check for secure permissions (no group or other write) if os.stat(file_path).st_mode & 0o077: raise PermissionError(f\\"Insecure permissions on {file_path}.\\") try: auth_info = netrc.netrc(file_path) except netrc.NetrcParseError: raise netrc.NetrcParseError(f\\"Failed to parse the .netrc file {file_path}.\\") try: return auth_info.authenticators(host) except KeyError: return None"},{"question":"Objective: Your task is to demonstrate your understanding of Python tuples and struct sequence concepts by implementing specific functional requirements. Question: 1. **Tuples Manipulation:** - Implement a function `is_tuple(obj)` that checks if the given object is a tuple. - Implement a function `create_tuple(*args)` that creates a tuple containing all the given arguments. - Implement a function `tuple_size(tup)` that returns the size of the given tuple. - Implement a function `get_item(tup, pos)` that returns the item at the specified `pos` in the tuple `tup`. If the position is out of bounds, the function should raise an `IndexError`. - Implement a function `set_item(tup, pos, value)` that returns a new tuple identical to `tup` but with the item at `pos` replaced by `value`. If the position is out of bounds, the function should raise an `IndexError`. (Remember, tuples are immutable in Python.) 2. **Struct Sequence (namedtuple) Mimic:** - Implement a class `StructSequence` that mimics the behavior of `collections.namedtuple`. This class should: - Be initialized with a list of field names. - Allow instantiation of objects with positional arguments corresponding to the named fields. - Provide access to fields both by index and by name. - Implement a `__repr__` method that returns a string representation of the object in the form `StructSequence(field1=value1, field2=value2, ...)`. Input and Output Formats: 1. **Tuples Manipulation:** - `is_tuple(obj)`: - Input: Any Python object. - Output: `True` if the object is a tuple, `False` otherwise. - `create_tuple(*args)`: - Input: A variable number of arguments. - Output: A tuple containing the arguments. - `tuple_size(tup)`: - Input: A tuple. - Output: The size of the tuple as an integer. - `get_item(tup, pos)`: - Input: A tuple and an integer position. - Output: The item at the specified position in the tuple. Raises `IndexError` if the position is out of bounds. - `set_item(tup, pos, value)`: - Input: A tuple, an integer position, and a new value. - Output: A new tuple with the item at the specified position replaced by the new value. Raises `IndexError` if the position is out of bounds. 2. **Struct Sequence (namedtuple) Mimic:** - `StructSequence` class: - Initialization: Takes a list of field names. - Instance creation: Can create instances with values for the fields. - Access: Fields can be accessed by name or index with dot notation or square brackets. - Representation: String representation of the instance in the format `StructSequence(field1=value1, field2=value2, ...)`. Example: ```python # For Tuple Manipulation print(is_tuple((1, 2, 3))) # Output: True print(create_tuple(1, 2, 3)) # Output: (1, 2, 3) print(tuple_size((1, 2, 3))) # Output: 3 print(get_item((1, 2, 3), 1)) # Output: 2 print(set_item((1, 2, 3), 1, 5)) # Output: (1, 5, 3) # For Struct Sequence Mimic Person = StructSequence([\'name\', \'age\']) p = Person(\'Alice\', 30) print(p.name) # Output: Alice print(p[0]) # Output: Alice print(p) # Output: StructSequence(name=\'Alice\', age=30) ``` Constraints: - You must not use `collections.namedtuple` or similar library functions to implement the `StructSequence` class. - Develop functions and classes without using external libraries except for built-in ones.","solution":"def is_tuple(obj): Checks if the given object is a tuple. return isinstance(obj, tuple) def create_tuple(*args): Creates a tuple containing all the given arguments. return tuple(args) def tuple_size(tup): Returns the size of the given tuple. return len(tup) def get_item(tup, pos): Returns the item at the specified position in the tuple tup. Raises IndexError if the position is out of bounds. return tup[pos] def set_item(tup, pos, value): Returns a new tuple identical to tup but with the item at pos replaced by value. Raises IndexError if the position is out of bounds. if pos < 0 or pos >= len(tup): raise IndexError(\\"tuple index out of range\\") return tup[:pos] + (value,) + tup[pos+1:] class StructSequence: Mimics the behavior of collections.namedtuple. def __init__(self, field_names): self._fields = field_names def __call__(self, *args): if len(args) != len(self._fields): raise ValueError(\\"Expected {} arguments, got {}\\".format(len(self._fields), len(args))) return StructInstance(self._fields, args) class StructInstance: def __init__(self, field_names, values): self._field_names = field_names self._values = values def __repr__(self): fields_str = \\", \\".join(f\\"{name}={repr(value)}\\" for name, value in zip(self._field_names, self._values)) return f\\"StructSequence({fields_str})\\" def __getitem__(self, index): return self._values[index] def __getattr__(self, attr): if attr in self._field_names: return self._values[self._field_names.index(attr)] raise AttributeError(f\\"\'StructInstance\' object has no attribute \'{attr}\'\\")"},{"question":"**Coding Challenge: Custom List Class with Enhanced Functionality** # Problem Statement You are required to implement a custom list class called `CustomList` that extends Python\'s built-in list and includes additional functionality for statistical analysis and enhanced iteration capabilities. # Requirements 1. **Initialization**: - The `CustomList` should inherit from Python\'s built-in `list`. - It should accept an iterable during initialization to pre-populate the list. 2. **Instance Methods**: - `mean(self)`: Returns the mean (average) of all numeric elements in the list. Raises a ValueError if the list contains non-numeric elements. - `median(self)`: Returns the median of all numeric elements in the list. Raises a ValueError if the list contains non-numeric elements or if the list is empty. - `mode(self)`: Returns the mode (the most frequently occurring value) of all numeric elements in the list. Raises a ValueError if the list contains non-numeric elements or if there is no single mode. 3. **Iterators**: - Implement an iterator to iterate over the list in reverse order. # Constraints: - The list will contain at least 1 and at most 1000 elements. - The elements of the list can be of any data type. However, the statistical methods should only work with numeric values (integers and floats). - Use appropriate exception handling to manage invalid operations. # Example Usage ```python # Create an instance of CustomList clist = CustomList([1, 2, 3, 4, 5, 6, 6]) # Calculate mean print(clist.mean()) # Output: 3.857142857142857 # Calculate median print(clist.median()) # Output: 4 # Calculate mode print(clist.mode()) # Output: 6 # Iterate in reverse order for elem in clist: print(elem) # Output: # 6 # 6 # 5 # 4 # 3 # 2 # 1 ``` # Implementation ```python class CustomList(list): def __init__(self, iterable): super().__init__(iterable) def mean(self): if not all(isinstance(x, (int, float)) for x in self): raise ValueError(\\"List contains non-numeric elements.\\") return sum(self) / len(self) def median(self): if not all(isinstance(x, (int, float)) for x in self): raise ValueError(\\"List contains non-numeric elements.\\") sorted_list = sorted(self) n = len(self) if n == 0: raise ValueError(\\"List is empty.\\") if n % 2 == 1: return sorted_list[n // 2] else: mid = n // 2 return (sorted_list[mid - 1] + sorted_list[mid]) / 2 def mode(self): if not all(isinstance(x, (int, float)) for x in self): raise ValueError(\\"List contains non-numeric elements.\\") from collections import Counter counter = Counter(self) modes = counter.most_common() if len(modes) > 1 and modes[0][1] == modes[1][1]: raise ValueError(\\"No unique mode.\\") return modes[0][0] def __iter__(self): self._index = len(self) return self def __next__(self): if self._index > 0: self._index -= 1 return self[self._index] else: raise StopIteration # Test cases if __name__ == \\"__main__\\": try: clist = CustomList([1, 2, 3, 4, 5, 6, 6]) print(clist.mean()) # Output: 3.857142857142857 print(clist.median()) # Output: 4 print(clist.mode()) # Output: 6 for elem in clist: print(elem) except ValueError as e: print(e) ``` # Notes - Ensure to handle cases where the statistical functions are called on non-numeric data or empty lists appropriately. - Use Python\'s `super()` to initialize the base list. - Utilize the `collections.Counter` class to determine the mode. - Implement iterator methods to allow for reverse iteration of the list.","solution":"class CustomList(list): def __init__(self, iterable): super().__init__(iterable) def mean(self): if not all(isinstance(x, (int, float)) for x in self): raise ValueError(\\"List contains non-numeric elements.\\") return sum(self) / len(self) def median(self): if not all(isinstance(x, (int, float)) for x in self): raise ValueError(\\"List contains non-numeric elements.\\") if len(self) == 0: raise ValueError(\\"List is empty.\\") sorted_list = sorted(self) n = len(self) if n % 2 == 1: return sorted_list[n // 2] else: mid = n // 2 return (sorted_list[mid - 1] + sorted_list[mid]) / 2 def mode(self): if not all(isinstance(x, (int, float)) for x in self): raise ValueError(\\"List contains non-numeric elements.\\") from collections import Counter counter = Counter(self) modes = counter.most_common() if len(modes) == 0 or (len(modes) > 1 and modes[0][1] == modes[1][1]): raise ValueError(\\"No unique mode.\\") return modes[0][0] def __iter__(self): self._index = len(self) return self def __next__(self): if self._index > 0: self._index -= 1 return self[self._index] else: raise StopIteration"},{"question":"Title: Parallel Data Processing using Concurrent Execution Background You are tasked with designing a data processing system that efficiently handles large datasets by utilizing concurrent execution. You will implement a program that processes data in parallel using the provided `concurrent` module functionalities. Problem Statement Create a Python function called `process_data_in_parallel` that takes a list of data items and processes them in parallel using the `concurrent.futures.ThreadPoolExecutor`. Each data item will be passed to a worker function called `process_item` (which you must also implement). The `process_item` function performs some arbitrary computation on the data and returns the result. For the sake of simplicity, assume `process_item` multiplies the input by 2 and then sleeps for 1 second to simulate a time-consuming task. # Specification Function 1: `process_item(data: int) -> int` * **Input**: A single integer. * **Output**: The integer multiplied by 2. Function 2: `process_data_in_parallel(data_list: List[int]) -> List[int]` * **Input**: A list of integers. * **Output**: A list of integers where each integer is the result of the `process_item` function applied in parallel. # Requirements 1. Use `ThreadPoolExecutor` from the `concurrent.futures` module to manage the thread pool. 2. Ensure that all data items are processed in parallel. 3. The results must be returned in the same order as the input data_list. # Constraints * The length of `data_list` will be between 1 and 1,000. * Each integer in `data_list` will be between 1 and 1,000. # Example ```python from typing import List def process_item(data: int) -> int: import time time.sleep(1) # Simulating time-consuming task return data * 2 def process_data_in_parallel(data_list: List[int]) -> List[int]: from concurrent.futures import ThreadPoolExecutor with ThreadPoolExecutor() as executor: results = list(executor.map(process_item, data_list)) return results # Example usage: data_list = [1, 2, 3, 4] print(process_data_in_parallel(data_list)) # Expected output: [2, 4, 6, 8] ``` **Note**: Ensure that your implementation handles potential exceptions and edge cases appropriately.","solution":"from typing import List import time from concurrent.futures import ThreadPoolExecutor def process_item(data: int) -> int: Multiplies the input data by 2 and simulates a time-consuming task by sleeping for 1 second. time.sleep(1) # Simulating time-consuming task return data * 2 def process_data_in_parallel(data_list: List[int]) -> List[int]: Processes a list of integers in parallel using ThreadPoolExecutor. Each integer is passed to the \'process_item\' function which multiplies it by 2 and then sleeps for 1 second. The result is returned in the same order as the input. with ThreadPoolExecutor() as executor: results = list(executor.map(process_item, data_list)) return results"},{"question":"# PyTorch MPS Backend Assessment Objective Write a PyTorch program that demonstrates your understanding and ability to use the MPS backend for high-performance training on a MacOS device. Instructions 1. **Check MPS Availability:** - Write a function `check_mps_availability` that checks if the MPS backend is available. If not, it should print appropriate error messages based on whether MPS was built with the current PyTorch install or if the current MacOS version is not 12.3 or later, or if no MPS-enabled device is available. 2. **Tensor Operations on MPS Device:** - Write a function `tensor_operations_on_mps` that: - Creates a tensor of shape (10, 10) filled with random values directly on the MPS device. - Performs the following operations on the tensor: - Multiply the tensor by a scalar value (e.g., 2). - Calculate the element-wise sine of the tensor. - Calculate the sum of all elements in the tensor. - Return the final resulting tensor and the sum as a tuple. 3. **Model Operations on MPS Device:** - Write a simple neural network model using `nn.Module`. - Write a function `model_operations_on_mps` that: - Moves the neural network model to the MPS device. - Creates an input tensor of appropriate shape directly on the MPS device. - Performs a forward pass through the model with the input tensor on the MPS device. - Returns the model\'s output for the given input. Functions ```python import torch import torch.nn as nn def check_mps_availability() -> None: # TO DO def tensor_operations_on_mps() -> (torch.Tensor, float): # TO DO class SimpleNet(nn.Module): # Define a simple neural network model def model_operations_on_mps(model: nn.Module) -> torch.Tensor: # TO DO ``` Constraints - Ensure your code runs only if the MPS backend is available. If not, provide meaningful outputs as described. - Assume `torch` is already imported. Example If the MPS backend is available, the `tensor_operations_on_mps` function should create a tensor on the MPS device, perform the operations, and return the results. The `model_operations_on_mps` function should create and test a simple model on the MPS backend. Expected Outputs - The final tensor resulting from `tensor_operations_on_mps`. - The scalar sum of the elements of the tensor. - The output of the neural network from `model_operations_on_mps`.","solution":"import torch import torch.nn as nn def check_mps_availability() -> None: if not getattr(torch, \'has_mps\', False): print(\\"Current PyTorch installation is not built with MPS support.\\") return if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\\"The MPS backend is not built.\\") else: print(\\"The current macOS version is not 12.3+ or no MPS-enabled device is present.\\") else: print(\\"MPS backend is available and ready to use.\\") def tensor_operations_on_mps() -> (torch.Tensor, float): if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS device is not available.\\") device = torch.device(\'mps\') tensor = torch.randn((10, 10), device=device) tensor = tensor * 2 tensor = torch.sin(tensor) tensor_sum = tensor.sum().item() return tensor, tensor_sum class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 5) self.fc2 = nn.Linear(5, 2) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def model_operations_on_mps(model: nn.Module) -> torch.Tensor: if not torch.backends.mps.is_available(): raise RuntimeError(\\"MPS device is not available.\\") device = torch.device(\'mps\') model.to(device) input_tensor = torch.randn((1, 10), device=device) output_tensor = model(input_tensor) return output_tensor"},{"question":"# Scikit-learn Hyper-parameter Tuning Task **Objective**: Implement a function to perform hyper-parameter tuning on a given dataset using scikit-learnâ€™s `GridSearchCV` or `RandomizedSearchCV` and evaluate the performance of the best-found model. Function Signature ```python def hyperparameter_tuning(estimator, param_grid, X_train, y_train, X_test, y_test, search_type=\'grid\', n_iter=10): Perform hyper-parameter tuning using GridSearchCV or RandomizedSearchCV. Parameters: - estimator: A scikit-learn estimator (e.g., SVC(), RandomForestClassifier()). - param_grid: Dictionary or list of dictionaries with parameters names (str) as keys and lists of parameter settings to try as values, or a dictionary with parameters names (str) as keys and hyper-parameter distributions (using scipy.stats module) as values for randomized search. - X_train: Training data features, numpy array or pandas DataFrame of shape (n_samples, n_features). - y_train: Training data labels, numpy array or pandas Series of shape (n_samples,). - X_test: Test data features, numpy array or pandas DataFrame of shape (n_samples, n_features). - y_test: Test data labels, numpy array or pandas Series of shape (n_samples,). - search_type: String, determines the search method - \'grid\' for GridSearchCV and \'random\' for RandomizedSearchCV. Defaults to \'grid\'. - n_iter: Integer, specifies the number of parameter settings that are sampled in RandomizedSearchCV. Only relevant if `search_type` is \'random\'. Defaults to 10. Returns: - best_params: Dictionary, the best parameter set found on the hold out data. - best_score: Float, best score achieved using the best parameter set. - test_score: Float, score of the best estimator on the test set. pass ``` Requirements 1. **Parameter Search**: - Implement hyper-parameter tuning using `GridSearchCV` for exhaustive search and `RandomizedSearchCV` for randomized search based on the given `search_type`. 2. **Cross-Validation**: - Utilize 5-fold cross-validation within `GridSearchCV` or `RandomizedSearchCV`. 3. **Performance Evaluation**: - After finding the best parameter set, evaluate the performance of the tuned model on the provided test data. 4. **Scoring**: - Use accuracy (`accuracy_score`) as the scoring function for classification tasks. Example Usage ```python from sklearn.datasets import load_iris from sklearn.svm import SVC # Load dataset data = load_iris() X = data.data y = data.target # Split dataset into train and test sets from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define estimator and parameter grid estimator = SVC() param_grid = [ {\'C\': [1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] # Perform hyper-parameter tuning (Grid Search) best_params, best_score, test_score = hyperparameter_tuning(estimator, param_grid, X_train, y_train, X_test, y_test, search_type=\'grid\') print(\\"Best Parameters:\\", best_params) print(\\"Best Cross-Validation Score:\\", best_score) print(\\"Test Score:\\", test_score) ``` Notes - The function should be robust to handle both `GridSearchCV` and `RandomizedSearchCV` based on the specified `search_type`. - Make sure to handle any constraints for the number of iterations (`n_iter`) for `RandomizedSearchCV`. - The `param_grid` should be appropriately structured depending on the chosen search method.","solution":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.metrics import accuracy_score def hyperparameter_tuning(estimator, param_grid, X_train, y_train, X_test, y_test, search_type=\'grid\', n_iter=10): Perform hyper-parameter tuning using GridSearchCV or RandomizedSearchCV. Parameters: - estimator: A scikit-learn estimator (e.g., SVC(), RandomForestClassifier()). - param_grid: Dictionary or list of dictionaries with parameters names (str) as keys and lists of parameter settings to try as values, or a dictionary with parameters names (str) as keys and hyper-parameter distributions (using scipy.stats module) as values for randomized search. - X_train: Training data features, numpy array or pandas DataFrame of shape (n_samples, n_features). - y_train: Training data labels, numpy array or pandas Series of shape (n_samples,). - X_test: Test data features, numpy array or pandas DataFrame of shape (n_samples, n_features). - y_test: Test data labels, numpy array or pandas Series of shape (n_samples,). - search_type: String, determines the search method - \'grid\' for GridSearchCV and \'random\' for RandomizedSearchCV. Defaults to \'grid\'. - n_iter: Integer, specifies the number of parameter settings that are sampled in RandomizedSearchCV. Only relevant if `search_type` is \'random\'. Defaults to 10. Returns: - best_params: Dictionary, the best parameter set found on the hold-out data. - best_score: Float, best score achieved using the best parameter set. - test_score: Float, score of the best estimator on the test set. if search_type == \'grid\': search_cv = GridSearchCV(estimator, param_grid, cv=5, scoring=\'accuracy\') elif search_type == \'random\': search_cv = RandomizedSearchCV(estimator, param_grid, n_iter=n_iter, cv=5, scoring=\'accuracy\', random_state=42) else: raise ValueError(\\"search_type must be either \'grid\' or \'random\'\\") search_cv.fit(X_train, y_train) best_params = search_cv.best_params_ best_score = search_cv.best_score_ best_estimator = search_cv.best_estimator_ test_score = accuracy_score(y_test, best_estimator.predict(X_test)) return best_params, best_score, test_score"},{"question":"# Problem Description You are tasked with creating a Python class called `ApiClient` that will serve as a client for interacting with a hypothetical web API. Your class should have a method `fetch_data(endpoint: str) -> dict` that makes a network request to the given endpoint and returns the data as a dictionary. However, for testing purposes, you need to mock this network interaction to ensure `fetch_data` works correctly without making real network calls. # Requirements: 1. **Class Definition**: - Define a class `ApiClient`. - Implement a method `fetch_data(endpoint: str) -> dict` that returns a dictionary. - Assume the real network call is represented by a method `_make_request(endpoint: str) -> dict`, which `fetch_data` will call internally. 2. **Mock Requirements**: - Write a unit test to verify that `fetch_data` correctly calls `_make_request` with the right `endpoint`. - Ensure that `fetch_data` returns the data as received from `_make_request`. - Use `unittest.mock` to create and configure mocks for this purpose. 3. **Testing**: - Write a test class `TestApiClient` using `unittest`, including at least two test cases: - One to check if `_make_request` is called with the correct endpoint. - One to verify that `fetch_data` returns the expected data from `_make_request`. # Input: - The `endpoint` parameter for the `fetch_data` method is a string representing the API endpoint. # Output: - The `fetch_data` method should return a dictionary with the API data. - The unit tests should check for method calls and the return value. # Constraints: - Use only standard Python libraries (do not actually perform any network requests). - Focus on using `unittest.mock` for mocking methods and asserting calls. # Examples: ```python # Example class class ApiClient: def fetch_data(self, endpoint: str) -> dict: return self._make_request(endpoint) def _make_request(self, endpoint: str) -> dict: # Simulated external request, real implementation would perform a network request return {\\"data\\": \\"real response\\"} # Unit tests import unittest from unittest.mock import patch, MagicMock class TestApiClient(unittest.TestCase): @patch.object(ApiClient, \'_make_request\') def test_fetch_data_calls_make_request(self, mock_make_request): client = ApiClient() mock_make_request.return_value = {\\"data\\": \\"mock response\\"} response = client.fetch_data(\'test/endpoint\') mock_make_request.assert_called_once_with(\'test/endpoint\') self.assertEqual(response, {\\"data\\": \\"mock response\\"}) @patch.object(ApiClient, \'_make_request\') def test_fetch_data_returns_correct_data(self, mock_make_request): client = ApiClient() expected_data = {\\"data\\": \\"mock response\\"} mock_make_request.return_value = expected_data response = client.fetch_data(\'test/endpoint\') self.assertEqual(response, expected_data) if __name__ == \'__main__\': unittest.main() ``` **Your task**: Implement the `ApiClient` class and write the `TestApiClient` unit test class as shown in the examples. # Note: - Ensure your `ApiClient` class and the unit tests pass without actually making any network calls. - Use the `unittest.mock` library effectively to simulate the behavior of the `_make_request` method in your tests.","solution":"import unittest from unittest.mock import patch class ApiClient: def fetch_data(self, endpoint: str) -> dict: Fetches data from the given endpoint. This method internally calls the _make_request method. return self._make_request(endpoint) def _make_request(self, endpoint: str) -> dict: Simulates making a real network request. In a real implementation, this method would perform the network operation. # Example placeholder implementation, a real one would make network requests return {\\"data\\": \\"real response\\"}"},{"question":"**Handling Asyncio Exceptions** In this task, you are required to implement a function that performs multiple asynchronous tasks using Python\'s `asyncio` package and handles exceptions that may arise during their execution. You will need to demonstrate your understanding of exception types provided by `asyncio`, including how to handle and possibly mitigate them. # Function Specifications: * **Function Name**: `manage_async_tasks` * **Expected Input**: - A list of asynchronous task functions (`tasks_list`) to be executed. Each function does not take any arguments and returns a coroutine. - An integer (`timeout`) indicating the maximum allowable time in seconds for each task to complete. * **Expected Output**: - A dictionary where each key is the task index (starting from 0) and the corresponding value is either: - The result of the task if it completed successfully. - A string representing the exception name if an exception was raised. # Constraints: - Each task should be executed asynchronously and concurrently. - You must handle the following exceptions: `asyncio.TimeoutError`, `asyncio.CancelledError`, `asyncio.InvalidStateError`, `asyncio.SendfileNotAvailableError`, `asyncio.IncompleteReadError`, and `asyncio.LimitOverrunError`. - If a task exceeds the specified timeout, it should be explicitly cancelled and the corresponding exception should be handled. # Example: ```python import asyncio async def task_success(): await asyncio.sleep(1) return \\"success\\" async def task_timeout(): await asyncio.sleep(5) return \\"timeout\\" async def task_invalid_state(): raise asyncio.InvalidStateError(\\"Invalid task state\\") tasks = [task_success, task_timeout, task_invalid_state] result = asyncio.run(manage_async_tasks(tasks, 2)) print(result) # {0: \'success\', 1: \'TimeoutError\', 2: \'InvalidStateError\'} ``` # Notes: - You may assume that each task is a proper coroutine that can be awaited. - Ensure that the function operates efficiently and handles multiple tasks as described. Good luck, and demonstrate your proficiency with handling `asyncio` exceptions in a robust manner!","solution":"import asyncio async def manage_async_tasks(tasks_list, timeout): Manages multiple asynchronous tasks, handling exceptions that may arise during their execution. :param tasks_list: A list of asynchronous task functions. :param timeout: Maximum allowable time in seconds for each task. :return: A dictionary with task indices as keys and results or exception names as values. results = {} async def run_task(index, task): try: result = await asyncio.wait_for(task(), timeout) results[index] = result except asyncio.TimeoutError: results[index] = \\"TimeoutError\\" except asyncio.CancelledError: results[index] = \\"CancelledError\\" except asyncio.InvalidStateError: results[index] = \\"InvalidStateError\\" except asyncio.SendfileNotAvailableError: results[index] = \\"SendfileNotAvailableError\\" except asyncio.IncompleteReadError: results[index] = \\"IncompleteReadError\\" except asyncio.LimitOverrunError: results[index] = \\"LimitOverrunError\\" except Exception as e: results[index] = type(e).__name__ await asyncio.gather(*[run_task(i, task) for i, task in enumerate(tasks_list)]) return results"},{"question":"Objective Implement a mini asynchronous chat server using the `contextvars` module to manage client-specific states. Problem Statement You need to create a simple asynchronous chat server that supports multiple clients connecting and sending messages. Use the `contextvars` module to manage and store the state of each connected client. Requirements 1. **Client Handling**: - Each client must be assigned a unique identifier upon connection. - Use a `ContextVar` to store the client\'s unique identifier and access it whenever the client sends a message. 2. **Message Handling**: - Clients can send messages to the server. - The server should broadcast each message to all connected clients, appending the sender\'s identifier to each message. 3. **Disconnection Handling**: - Properly handle client disconnection and clean up context variables as needed. Input Format - There is no direct input. The server will continuously listen for connections and messages. - You can test the server using a telnet client or similar. Output Format - Broadcast the sender\'s messages to all connected clients. Constraints - Use the `asyncio` module for handling asynchronous I/O. - Ensure that the state of each client is managed using `contextvars`. - The server should handle at least 10 concurrent clients without performance issues. Example When a client connects, they get assigned an ID like `Client_1`. If they send the message \\"Hello\\", the server will broadcast the message as `Client_1: Hello` to all other connected clients. Here is a basic structure to get you started: ```python import asyncio import contextvars client_id_var = contextvars.ContextVar(\'client_id\') async def handle_client(reader, writer): client_id = f\\"Client_{id(writer)}\\" client_id_var.set(client_id) while True: data = await reader.read(100) message = data.decode() if not message: break broadcast_message = f\\"{client_id_var.get()}: {message}\\" # Broadcast the message to all clients # (you need to implement the broadcasting mechanism) writer.close() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() asyncio.run(main()) ``` Complete the implementation to meet the specified requirements.","solution":"import asyncio import contextvars from asyncio import StreamReader, StreamWriter from typing import Dict, Set client_id_var = contextvars.ContextVar(\'client_id\') connected_clients: Dict[str, StreamWriter] = {} async def broadcast_message(message: str, sender_id: str): for client_id, writer in connected_clients.items(): if client_id != sender_id: # Don\'t send the message back to the sender writer.write(message.encode()) await writer.drain() async def handle_client(reader: StreamReader, writer: StreamWriter): client_id = f\\"Client_{id(writer)}\\" client_id_var.set(client_id) connected_clients[client_id] = writer print(f\\"{client_id} has connected.\\") try: while True: data = await reader.read(100) message = data.decode() if not message: break broadcast_message_str = f\\"{client_id_var.get()}: {message}\\" print(broadcast_message_str.strip()) # For server-side log await broadcast_message(broadcast_message_str, client_id) except asyncio.CancelledError: pass finally: print(f\\"{client_id} has disconnected.\\") del connected_clients[client_id] writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Question: Profiling Python Functions with cProfile** In this assessment, you will demonstrate your understanding of the `cProfile` module by profiling a Python script and analyzing its performance data. # Task 1. **Profile a Function** Write a Python function `calculate_prime_factors(n: int) -> List[int]` to compute and return all prime factors of a given integer `n`. 2. **Profile the Function Execution** Use the `cProfile` module to profile the execution of `calculate_prime_factors` when called with an input value of `987654321`. 3. **Save the Profiling Result** Save the profiling result to a file named `profiling_result.prof`. 4. **Analyze the Profiling Result** Using the `pstats` module, write another function `analyze_profiling_result(prof_file: str) -> None` to: - Load the profiling results from the specified file. - Sort the statistics by the cumulative time spent in functions. - Print the top 10 functions that consume the most cumulative time. # Detailed Requirements - **Function 1: `calculate_prime_factors`** - **Input:** An integer `n` (where 2 <= n <= 10^9). - **Output:** A list of integers representing the prime factors of `n`. - **Constraints:** Your implementation should be efficient to handle the upper limits of the input range without significant delays. - **Function 2: `profile_function`** - No arguments. - Profiles the `calculate_prime_factors` function with the input `987654321` and saves the profiling result to `profiling_result.prof`. - **Function 3: `analyze_profiling_result`** - **Input:** A string `prof_file`, the name of the profiling result file. - **Output:** None (prints the analysis results). - **Behavior:** Loads the profiling results from the file, sorts the statistics by cumulative time, and prints the top 10 time-consuming functions. # Example Usage ```python # Example code to demonstrate the usage def calculate_prime_factors(n: int) -> List[int]: # Function implementation def profile_function(): # Function implementation def analyze_profiling_result(prof_file: str) -> None: # Function implementation if __name__ == \\"__main__\\": profile_function() analyze_profiling_result(\'profiling_result.prof\') ``` # Notes 1. Ensure you handle edge cases gracefully, particularly the performance aspects for larger values of `n`. 2. The output of the top 10 functions should be in a readable format, showing the cumulative time and the number of calls made. This task will help you understand how to use Python\'s profiling tools effectively to optimize your code\'s performance.","solution":"import cProfile import pstats from typing import List def calculate_prime_factors(n: int) -> List[int]: Returns a list of prime factors of the given integer n. i = 2 factors = [] # Check for the number of times 2 divides n while n % i == 0: factors.append(i) n = n // i # n must be odd at this point so we can skip even numbers i = 3 while i * i <= n: while n % i == 0: factors.append(i) n = n // i i += 2 # Handling the last prime factor if any if n > 2: factors.append(n) return factors def profile_function(): Profiles the calculate_prime_factors function with the input value 987654321 and saves the profiling result to profiling_result.prof. profiler = cProfile.Profile() profiler.enable() calculate_prime_factors(987654321) profiler.disable() profiler.dump_stats(\'profiling_result.prof\') def analyze_profiling_result(prof_file: str) -> None: Analyzes the profiling results stored in the specified file. Sorts the statistics by cumulative time and prints the top 10 functions by cumulative time. with open(prof_file, \'r\') as f: stats = pstats.Stats(prof_file) stats.sort_stats(\'cumulative\') stats.print_stats(10) if __name__ == \\"__main__\\": profile_function() analyze_profiling_result(\'profiling_result.prof\')"},{"question":"<|Analysis Begin|> The provided documentation mainly focuses on the different plotting and visualization capabilities of pandas, utilizing matplotlib as the underlying engine. It illustrates the basic as well as advanced usage of various plotting functions for Series and DataFrame in pandas, such as: 1. **Basic Plotting**: Using `plot` methods directly on Series and DataFrame objects for line plots. 2. **Other Plots**: Bar plots, histograms, box plots, area plots, scatter plots, hexbin plots, pie plots, density plots, etc. 3. **Advanced Plotting Techniques**: Scatter Matrix, Andrews Curves, Parallel Coordinates, Lag Plot, Autocorrelation Plot, Bootstrap Plot, RadViz, and more. 4. **Plot Customization and Formatting**: Options for setting plot styles, controlling legends, adding custom labels, handling missing data, etc. 5. **Plotting with Error Bars**: Options for including error bars in plots. 6. **Subplots Management**: Creating subplots and customizing their layout. 7. **Interfacing with matplotlib**: Various hooks and customizations available through direct use of matplotlib functions. 8. **Extensibility with Third-party Backends**: Leveraging different visualization tools for pandas plotting. The documentation is comprehensive in showcasing the broad visualization functionality within pandas. To test comprehension, a well-designed coding assessment question could focus on a few of these aspects, especially those that require understanding and application of multiple functionalities and custom plotting. <|Analysis End|> <|Question Begin|> **Question: Advanced Data Visualization with Pandas** You are given a dataset containing information about various stocks over a period of time. Your task is to create a set of visualizations that portray different characteristics of this dataset. Dataset (assuming you have the data in a file called `stock_data.csv` with the following structure): ``` Date,Stock,Open,High,Low,Close,Volume 2023-01-01,GOOG,1354.23,1360.00,1340.50,1350.00,1200000 ... ``` Columns: - `Date`: Date of the stock price - `Stock`: Stock symbol (e.g., GOOG for Google) - `Open`: Opening price of the stock - `High`: Highest price of the stock during the day - `Low`: Lowest price of the stock during the day - `Close`: Closing price of the stock - `Volume`: Number of shares traded # Tasks: 1. **Line Plot**: - Plot the closing prices of all stocks over time in a single plot. - Ensure that each stock is represented with a different color and include a legend to distinguish them. 2. **OHLC Plot for a Selected Stock**: - Create a subplot arrangement showing the opening, high, low, and closing prices for a specific stock (e.g., \\"GOOG\\") over time. - Layout should be `2x2`, each subplot representing one of the OHLC components. 3. **Volume Analysis**: - Create a bar plot showing the total trading volume per stock. - Add a secondary Y-axis to represent the average closing price of each stock. 4. **Scatter Plot**: - Generate a scatter plot comparing the daily high and low prices of one selected stock (e.g., \\"AAPL\\"). - Use color encoding to represent the volume of shares traded on each day. 5. **Box Plot**: - Create a box plot for the closing prices of each stock to visualize the distribution. 6. **Custom Formatting**: - Customize one of the above plots by adjusting the X and Y axis labels, adding a title, and modifying the plot style to \'ggplot\'. # Function Signatures: 1. `def plot_closing_prices(data: pd.DataFrame) -> None:` 2. `def plot_ohlc(data: pd.DataFrame, stock: str) -> None:` 3. `def plot_volume_analysis(data: pd.DataFrame) -> None:` 4. `def plot_scatter_high_low(data: pd.DataFrame, stock: str) -> None:` 5. `def plot_box_closing_prices(data: pd.DataFrame) -> None:` 6. `def customize_plot(ax: plt.Axes) -> None:` # Constraints: - Use proper handling for missing data if any (drop or fill appropriately). - Ensure the plots are clear and informative. # Example: Suppose you are plotting the closing prices for task 1, the function `plot_closing_prices` should generate a line plot with each stock\'s closing prices, color-coded lines for each stock, and a legend indicating which color corresponds to which stock. You may load and preprocess the data using pandas before feeding it into the functions. The functions should only focus on the visualization aspects. **Note**: Include all necessary imports and ensure you detail enough so the question remains clear and self-contained.","solution":"import pandas as pd import matplotlib.pyplot as plt def plot_closing_prices(data: pd.DataFrame) -> None: plt.figure(figsize=(10, 6)) for stock in data[\'Stock\'].unique(): stock_data = data[data[\'Stock\'] == stock] plt.plot(stock_data[\'Date\'], stock_data[\'Close\'], label=stock) plt.title(\'Closing Prices Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Closing Price\') plt.legend() plt.show() def plot_ohlc(data: pd.DataFrame, stock: str) -> None: stock_data = data[data[\'Stock\'] == stock] fig, axs = plt.subplots(2, 2, figsize=(14, 10)) axs[0, 0].plot(stock_data[\'Date\'], stock_data[\'Open\'], label=\'Open\') axs[0, 0].set_title(\'Opening Prices\') axs[0, 1].plot(stock_data[\'Date\'], stock_data[\'High\'], label=\'High\') axs[0, 1].set_title(\'High Prices\') axs[1, 0].plot(stock_data[\'Date\'], stock_data[\'Low\'], label=\'Low\') axs[1, 0].set_title(\'Low Prices\') axs[1, 1].plot(stock_data[\'Date\'], stock_data[\'Close\'], label=\'Close\') axs[1, 1].set_title(\'Closing Prices\') for ax in axs.flat: ax.set(xlabel=\'Date\', ylabel=\'Price\') ax.label_outer() plt.suptitle(f\'{stock} OHLC Data\') plt.tight_layout() plt.show() def plot_volume_analysis(data: pd.DataFrame) -> None: total_volume = data.groupby(\'Stock\')[\'Volume\'].sum() avg_close = data.groupby(\'Stock\')[\'Close\'].mean() fig, ax1 = plt.subplots(figsize=(10, 6)) ax1.bar(total_volume.index, total_volume, color=\'b\', alpha=0.6) ax1.set_xlabel(\'Stock\') ax1.set_ylabel(\'Total Volume\', color=\'b\') ax2 = ax1.twinx() ax2.plot(avg_close.index, avg_close, color=\'r\', marker=\'o\') ax2.set_ylabel(\'Average Closing Price\', color=\'r\') plt.title(\'Total Trading Volume and Average Closing Price per Stock\') plt.show() def plot_scatter_high_low(data: pd.DataFrame, stock: str) -> None: stock_data = data[data[\'Stock\'] == stock] plt.figure(figsize=(10, 6)) scatter = plt.scatter(stock_data[\'High\'], stock_data[\'Low\'], c=stock_data[\'Volume\'], cmap=\'viridis\') plt.colorbar(scatter, label=\'Volume\') plt.title(f\'{stock} High vs Low Prices\') plt.xlabel(\'High Price\') plt.ylabel(\'Low Price\') plt.show() def plot_box_closing_prices(data: pd.DataFrame) -> None: plt.figure(figsize=(10, 6)) data.boxplot(column=\'Close\', by=\'Stock\') plt.title(\'Closing Price Distribution per Stock\') plt.suptitle(\'\') plt.xlabel(\'Stock\') plt.ylabel(\'Closing Price\') plt.show() def customize_plot(ax: plt.Axes) -> None: ax.set_title(\'Customized Plot\') ax.set_xlabel(\'Custom X Label\') ax.set_ylabel(\'Custom Y Label\') plt.style.use(\'ggplot\') plt.show()"},{"question":"Objective To demonstrate your understanding of seaborn\'s `JointGrid`, you will create a function that generates a customized joint plot with specific requirements. Problem Statement Write a function `create_custom_joint_plot` that takes a DataFrame and column names for the x and y axes, and generates a seaborn joint plot with the following specifications: 1. The joint plot should be a scatter plot with marginal KDE plots. 2. Apply the following customizations to the scatter plot: - Marker size should be 150. - Transparency (alpha) should be 0.6. - Edge color should be \\"black\\". - Line width of the marker edges should be 1. 3. Apply the following customizations to the KDE plots: - Line width should be 2. 4. Add a vertical reference line at the mean of the x variable. 5. Set the height of the JointGrid to 6 and the ratio of the joint and marginal plots to 3. 6. Set appropriate axis labels for the x and y axes. Function Signature ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_joint_plot(data: pd.DataFrame, x_col: str, y_col: str) -> None: pass ``` Input - `data`: A pandas DataFrame containing the data to be plotted. - `x_col`: A string representing the column name for the x-axis. - `y_col`: A string representing the column name for the y-axis. Output - The function should not return any value. Instead, it should display the joint plot. Example ```python penguins = sns.load_dataset(\\"penguins\\") create_custom_joint_plot(penguins, \\"bill_length_mm\\", \\"bill_depth_mm\\") ``` The function call above should generate a joint plot with the specified customizations. Constraints - Assume the DataFrame will have the specified columns with numerical data for the x and y variables. - Ensure you use seaborn\'s `JointGrid` class and methods to create the plot.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_joint_plot(data: pd.DataFrame, x_col: str, y_col: str) -> None: Generates a customized seaborn joint plot with specific requirements. Parameters: - data: A pandas DataFrame containing the data to be plotted. - x_col: A string representing the column name for the x-axis. - y_col: A string representing the column name for the y-axis. # Create the JointGrid g = sns.JointGrid(data=data, x=x_col, y=y_col, height=6, ratio=3) # Add the scatter plot g.plot_joint(sns.scatterplot, s=150, alpha=0.6, edgecolor=\\"black\\", linewidth=1) # Add the KDE plots g.plot_marginals(sns.kdeplot, linewidth=2) # Add a vertical reference line at the mean of the x variable mean_x = data[x_col].mean() plt.axvline(mean_x, color=\'r\', linestyle=\'--\') # Set axis labels g.set_axis_labels(x_col, y_col) # Show the plot plt.show()"},{"question":"# Question: Working with Tensor Dimensions using `torch.Size` Objective: Write a Python function using PyTorch that, given a tensor, performs the following operations: 1. Verifies whether a tensor\'s dimensions match a given target size. 2. Returns the size of the tensor along each dimension. 3. Returns the length of the tensor shape (i.e., number of dimensions). Function Signature: ```python import torch def tensor_size_operations(tensor: torch.Tensor, target_size: tuple) -> tuple: Given a tensor and a target size, performs several operations involving tensor sizes. Parameters: tensor (torch.Tensor): The input tensor. target_size (tuple): A tuple representing the expected size of the tensor. Returns: tuple: A tuple containing: - A boolean indicating whether the tensor\'s size matches the target size. - A list of integers representing the size of the tensor along each dimension. - An integer representing the number of dimensions of the tensor. Input: - `tensor (torch.Tensor)`: The input tensor. - `target_size (tuple)`: A tuple of integers representing the expected size of the tensor. Output: - Returns a tuple containing: - A boolean indicating whether the tensor\'s size matches the target size. - A list of integers representing the size of the tensor along each dimension. - An integer representing the number of dimensions of the tensor. Constraints: - The tensor can be of any valid PyTorch tensor type and have up to 5 dimensions. - The target size should be a valid tuple that can be compared with the tensor\'s size. Example: ```python import torch tensor = torch.ones(3, 4, 5) target_size = (3, 4, 5) result = tensor_size_operations(tensor, target_size) print(result) # Expected output: (True, [3, 4, 5], 3) tensor2 = torch.ones(10, 20, 30) target_size2 = (10, 20) result2 = tensor_size_operations(tensor2, target_size2) print(result2) # Expected output: (False, [10, 20, 30], 3) ``` Note: - Ensure you use the `torch.Size` class to handle tensor dimensions. - Your solution should demonstrate an understanding of sequence operations supported by `torch.Size`.","solution":"import torch def tensor_size_operations(tensor: torch.Tensor, target_size: tuple) -> tuple: Given a tensor and a target size, performs several operations involving tensor sizes. Parameters: tensor (torch.Tensor): The input tensor. target_size (tuple): A tuple representing the expected size of the tensor. Returns: tuple: A tuple containing: - A boolean indicating whether the tensor\'s size matches the target size. - A list of integers representing the size of the tensor along each dimension. - An integer representing the number of dimensions of the tensor. tensor_size = tensor.size() does_match = tensor_size == torch.Size(target_size) size_list = list(tensor_size) num_dimensions = len(tensor_size) return (does_match, size_list, num_dimensions)"},{"question":"Objective: Implement and test a simple library management system using Python\'s `unittest.mock` module to mock interactions with an external database service. Description: You are required to create a class `Library` that interacts with a mocked database service to manage books. The `Library` class should have the following methods: 1. `add_book(title: str, author: str) -> bool`: Adds a book to the database. Returns `True` if the book was added successfully, `False` otherwise. 2. `get_books_by_author(author: str) -> list`: Retrieves a list of book titles by a given author from the database. Mock Requirements: You must use Python\'s `unittest.mock` module to: 1. Mock the database service within the `Library` class. 2. Write test cases using `unittest` that verify: - `add_book` is called with the correct arguments and returns the expected result. - `get_books_by_author` fetches the correct list of books. - Ensure `add_book` handles exceptions from the database service gracefully and returns `False`. Constraints: - Assume the database service has two methods: - `insert_book(title: str, author: str) -> bool`: Simulates inserting a book into the database. - `fetch_books_by_author(author: str) -> list`: Simulates fetching books by a specific author. Input and Output Formats: - Your `Library` class methods will interact with the mock database. - Test cases should assert expected calls and return values. Example: ```python # Required imports from unittest import TestCase, main from unittest.mock import Mock, patch class Library: def __init__(self, db): self.db = db def add_book(self, title, author): try: return self.db.insert_book(title, author) except Exception: return False def get_books_by_author(self, author): return self.db.fetch_books_by_author(author) class TestLibrary(TestCase): @patch(\'path_to_your_module.Library.db\') def test_add_book(self, mock_db): # Setup mock_db.insert_book.return_value = True lib = Library(mock_db) # Test add_book result = lib.add_book(\'1984\', \'George Orwell\') mock_db.insert_book.assert_called_once_with(\'1984\', \'George Orwell\') self.assertTrue(result) # Test add_book with exception mock_db.insert_book.side_effect = Exception(\'DB error\') result = lib.add_book(\'Animal Farm\', \'George Orwell\') self.assertFalse(result) @patch(\'path_to_your_module.Library.db\') def test_get_books_by_author(self, mock_db): # Setup mock_db.fetch_books_by_author.return_value = [\'1984\', \'Animal Farm\'] lib = Library(mock_db) # Test get_books_by_author books = lib.get_books_by_author(\'George Orwell\') mock_db.fetch_books_by_author.assert_called_once_with(\'George Orwell\') self.assertEqual(books, [\'1984\', \'Animal Farm\']) if __name__ == \'__main__\': main() ``` Instructions: 1. Implement the `Library` class as described above. 2. Write the test cases using `unittest` and `unittest.mock` as shown in the example. 3. Ensure that both methods in the `Library` class are tested thoroughly for correctness and robustness. Submit the implementation of the `Library` class and the test cases in a Python script.","solution":"from unittest.mock import Mock class Library: def __init__(self, db): self.db = db def add_book(self, title, author): try: return self.db.insert_book(title, author) except Exception: return False def get_books_by_author(self, author): return self.db.fetch_books_by_author(author)"},{"question":"**Coding Assessment Question:** # Objective Create a standalone Python command-line utility that packages a given directory of Python code into an executable zip archive (.pyz). The utility should include an option to specify the Python interpreter and to create a compressed archive. Additionally, your utility should set the archive to be executable on POSIX systems if an interpreter is specified. # Requirements 1. **Function Implementation:** Implement a Python function named `package_directory` with the following signature: ```python def package_directory(source: str, output: str, interpreter: str = None, compressed: bool = False): Packages the specified source directory into an executable zip archive. Parameters: - source (str): Path to the source directory containing the Python code. - output (str): Path to the output .pyz file. - interpreter (str, optional): The Python interpreter to use (e.g., \\"/usr/bin/env python3\\"). - compressed (bool, optional): Whether to compress the archive. Returns: - None ``` 2. **Command-Line Interface:** Your script should also include functionality to handle command-line arguments using the `argparse` module and call `package_directory` with the appropriate arguments. 3. **Behavior:** - If an interpreter is specified, the resulting archive should be executable on POSIX systems. - If compression is requested, the archive should be compressed using the deflate method. - If no output filename is specified, use the source directory name and add a \\".pyz\\" extension. # Input/Output - **Input:** Command-line arguments specifying: - `source`: Path to the source directory. - `output` (optional): Path to the output .pyz file. - `-p` or `--python` (optional): Path to the Python interpreter. - `-c` or `--compress` (optional): Flag to compress the archive. - **Output:** An executable zip archive (.pyz) created from the source directory. # Constraints - The source directory must contain a Python package (i.e., it should include an `__init__.py` file). - The source directory must also contain a `__main__.py` file that serves as the entry point. - Ensure proper error handling for invalid inputs, such as non-existent source directories or missing `__main__.py` files. # Example Usage ```sh python package.py myapp -p \\"/usr/bin/env python3\\" -c # Creates an executable zip archive \'myapp.pyz\' with the specified interpreter and compression enabled. ``` # Performance Requirements - The solution should efficiently handle directory structures typical for small to medium-sized Python applications. - The script should be able to create the archive in a reasonable time frame, even with compression enabled. # Additional Notes - You may assume the user has write permissions for the provided output path. - Ensure the script works cross-platform, with added functionality for POSIX systems to set the executable bit.","solution":"import os import zipfile import argparse import stat def package_directory(source: str, output: str, interpreter: str = None, compressed: bool = False): Packages the specified source directory into an executable zip archive. Parameters: - source (str): Path to the source directory containing the Python code. - output (str): Path to the output .pyz file. - interpreter (str, optional): The Python interpreter to use (e.g., \\"/usr/bin/env python3\\"). - compressed (bool, optional): Whether to compress the archive. Returns: - None if not os.path.isdir(source): raise NotADirectoryError(f\\"{source} is not a directory\\") main_path = os.path.join(source, \'__main__.py\') if not os.path.exists(main_path): raise FileNotFoundError(f\\"{source} does not contain a __main__.py file\\") mode = zipfile.ZIP_DEFLATED if compressed else zipfile.ZIP_STORED with zipfile.ZipFile(output, \'w\', mode) as zf: for root, _, files in os.walk(source): for file in files: path = os.path.join(root, file) arcname = os.path.relpath(path, start=source) zf.write(path, arcname) if interpreter: with open(output, \'rb\') as original: data = original.read() with open(output, \'wb\') as modified: modified.write(f\'#!{interpreter}n\'.encode(\'utf-8\') + data) st = os.stat(output) os.chmod(output, st.st_mode | stat.S_IEXEC) if __name__ == \'__main__\': parser = argparse.ArgumentParser(description=\'Package a directory of Python code into an executable zip archive\') parser.add_argument(\'source\', type=str, help=\'Path to the source directory containing the Python code\') parser.add_argument(\'output\', nargs=\'?\', type=str, help=\'Path to the output .pyz file (default: source directory name with .pyz extension)\') parser.add_argument(\'-p\', \'--python\', type=str, help=\'Python interpreter for the executable archive\') parser.add_argument(\'-c\', \'--compress\', action=\'store_true\', help=\'Compress the archive\') args = parser.parse_args() source = args.source output = args.output or f\\"{os.path.basename(os.path.abspath(source))}.pyz\\" interpreter = args.python compressed = args.compress package_directory(source, output, interpreter, compressed)"},{"question":"<|Analysis Begin|> The provided documentation describes the `asyncio` queue classes in Python 3.10, which are used for managing queues in an asynchronous context. These queues are designed to work with `asyncio` for concurrent programming and are not thread-safe. The main classes and methods described in the documentation are: 1. `asyncio.Queue`: A FIFO queue with methods such as `get`, `put`, `qsize`, `task_done`, etc. 2. `asyncio.PriorityQueue`: A priority queue where items are ordered by their priority (smallest first). 3. `asyncio.LifoQueue`: A LIFO queue where the most recently added items are retrieved first. 4. Exceptions like `asyncio.QueueEmpty` and `asyncio.QueueFull` which indicate empty or full queue conditions. The example provided demonstrates how to use an `asyncio.Queue` to distribute tasks among multiple worker coroutines, showing how items are put into the queue and processed by the workers asynchronously. Based on this analysis, we can design a question that assesses the students\' understanding of creating and using an `asyncio.Queue` to manage tasks in an asynchronous environment, as well as handling different queue behaviors and exceptions. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective Your task is to implement an asynchronous task manager using the `asyncio.Queue`, `asyncio.PriorityQueue`, and `asyncio.LifoQueue` classes provided by Python 3.10. You will create functions to handle tasks placed in these different types of queues and demonstrate your understanding of managing them in an asynchronous context. Requirements 1. **Function Definitions**: - `async def handle_fifo_queue(queue: asyncio.Queue):` - This function retrieves and handles tasks from an `asyncio.Queue` (FIFO) until it is empty. - `async def handle_priority_queue(queue: asyncio.PriorityQueue):` - This function retrieves and handles tasks from an `asyncio.PriorityQueue`. - `async def handle_lifo_queue(queue: asyncio.LifoQueue):` - This function retrieves and handles tasks from an `asyncio.LifoQueue`. 2. **Task Handling**: - Each task is a tuple containing the task name and duration (a float representing seconds). For the priority queue, tasks are tuples of the form `(priority, (task_name, duration))`. - Each function should: - Retrieve tasks from the queue. - Print the task name and start time. - Wait for the duration specified by the task. - Print the task name and end time. - Mark the task as done using `queue.task_done()`. 3. **Main Function**: - `async def main():` - Create and populate the three types of queues with example tasks. - Start the task handling functions concurrently. - Use `await` where necessary to ensure the program waits for all tasks to complete. Input and Output Formats - Input: Populate the queues with example tasks within the `main()` function. - Output: Print statements indicating the start and end times of each task. Constraints - Use only `asyncio.Queue`, `asyncio.PriorityQueue`, and `asyncio.LifoQueue` for queue implementations. - Ensure you handle potential exceptions, such as `QueueEmpty` and `QueueFull`. Example ```python import asyncio async def handle_fifo_queue(queue: asyncio.Queue): while not queue.empty(): task_name, duration = await queue.get() print(f\'Starting {task_name} at time {asyncio.get_event_loop().time()}\') await asyncio.sleep(duration) print(f\'Finished {task_name} at time {asyncio.get_event_loop().time()}\') queue.task_done() async def handle_priority_queue(queue: asyncio.PriorityQueue): while not queue.empty(): priority, task = await queue.get() task_name, duration = task print(f\'Starting {task_name} at time {asyncio.get_event_loop().time()}\') await asyncio.sleep(duration) print(f\'Finished {task_name} at time {asyncio.get_event_loop().time()}\') queue.task_done() async def handle_lifo_queue(queue: asyncio.LifoQueue): while not queue.empty(): task_name, duration = await queue.get() print(f\'Starting {task_name} at time {asyncio.get_event_loop().time()}\') await asyncio.sleep(duration) print(f\'Finished {task_name} at time {asyncio.get_event_loop().time()}\') queue.task_done() async def main(): fifo_queue = asyncio.Queue() lifo_queue = asyncio.LifoQueue() priority_queue = asyncio.PriorityQueue() tasks = [ (\\"Task-A\\", 1), (\\"Task-B\\", 2), (\\"Task-C\\", 3) ] # Populate FIFO queue for task in tasks: fifo_queue.put_nowait(task) # Populate LIFO queue for task in tasks: lifo_queue.put_nowait(task) # Populate Priority Queue for i, task in enumerate(tasks): priority_queue.put_nowait((i, task)) await asyncio.gather( handle_fifo_queue(fifo_queue), handle_lifo_queue(lifo_queue), handle_priority_queue(priority_queue) ) asyncio.run(main()) ```","solution":"import asyncio async def handle_fifo_queue(queue: asyncio.Queue): while not queue.empty(): task_name, duration = await queue.get() print(f\'Starting {task_name} at time {asyncio.get_event_loop().time()}\') await asyncio.sleep(duration) print(f\'Finished {task_name} at time {asyncio.get_event_loop().time()}\') queue.task_done() async def handle_priority_queue(queue: asyncio.PriorityQueue): while not queue.empty(): priority, task = await queue.get() task_name, duration = task print(f\'Starting {task_name} at time {asyncio.get_event_loop().time()}\') await asyncio.sleep(duration) print(f\'Finished {task_name} at time {asyncio.get_event_loop().time()}\') queue.task_done() async def handle_lifo_queue(queue: asyncio.LifoQueue): while not queue.empty(): task_name, duration = await queue.get() print(f\'Starting {task_name} at time {asyncio.get_event_loop().time()}\') await asyncio.sleep(duration) print(f\'Finished {task_name} at time {asyncio.get_event_loop().time()}\') queue.task_done() async def main(): fifo_queue = asyncio.Queue() lifo_queue = asyncio.LifoQueue() priority_queue = asyncio.PriorityQueue() tasks = [ (\\"Task-A\\", 1), (\\"Task-B\\", 2), (\\"Task-C\\", 3) ] # Populate FIFO queue for task in tasks: fifo_queue.put_nowait(task) # Populate LIFO queue for task in tasks: lifo_queue.put_nowait(task) # Populate Priority Queue for i, task in enumerate(tasks): priority_queue.put_nowait((i, task)) await asyncio.gather( handle_fifo_queue(fifo_queue), handle_lifo_queue(lifo_queue), handle_priority_queue(priority_queue) ) if __name__ == \'__main__\': asyncio.run(main())"},{"question":"Using the Seaborn library, you are tasked with analyzing the relationship between different features in the `penguins` dataset. The goal is to create a joint plot with customized features and additional layered information. Requirements: 1. **Load the Data**: - Load the `penguins` dataset. 2. **Basic Joint Plot**: - Create a basic joint scatter plot displaying `bill_length_mm` on the x-axis and `bill_depth_mm` on the y-axis, with marginal histograms. 3. **Enhanced Plot**: - Add a `hue` parameter based on the `species` column to color the data points conditionally. - Set the `kind` parameter to `kde` to draw both bivariate and univariate KDEs. 4. **Customization**: - Customize the marker to use plus signs (`+`) and set their size to 100. - Adjust the marginal histogram bins to 25 and set them to not be filled. - Set the `height` of the plot to 6 and the `ratio` between the joint and marginal axes to 1.5. 5. **Additional Layers**: - Add a red-colored bivariate KDE plot with 5 levels to the joint plot. - Add red-colored rug plots to the marginal axes. Input: - No explicit input from the user. The code should load the dataset and plot directly. Output: - The final plot should be displayed inline (assuming this is executed in a Jupyter Notebook). Constraints: - Ensure that the final plot is clear and well-labeled. Function Signature: ```python import seaborn as sns def visualize_penguin_data(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create a basic joint scatter plot with marginal histograms g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"kde\\") # Customize the plot sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"kde\\", marker=\\"+\\", s=100, marginal_kws=dict(bins=25, fill=False), height=6, ratio=1.5) # Add additional layers g.plot_joint(sns.kdeplot, color=\\"r\\", zorder=0, levels=5) g.plot_marginals(sns.rugplot, color=\\"r\\", height=-0.15, clip_on=False) # Display the plot sns.set_theme(style=\\"white\\") sns.despine() # Execute the function visualize_penguin_data() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguin_data(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Ensure the dataset is loaded properly if penguins is None: raise ValueError(\\"The penguins dataset could not be loaded.\\") # Create a basic joint scatter plot with marginal histograms g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"scatter\\") # Enhance the plot g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"kde\\", marker=\\"+\\", s=100, marginal_kws=dict(bins=25, fill=False), height=6, ratio=1.5) # Add additional layers g.plot_joint(sns.kdeplot, color=\\"r\\", zorder=0, levels=5) g.plot_marginals(sns.rugplot, color=\\"r\\", height=-0.15, clip_on=False) # Customize the appearance sns.set_theme(style=\\"white\\") sns.despine() # Display the plot plt.show()"},{"question":"Coding Assessment Question # Objective To assess your understanding of seaborn\'s `ecdfplot` function and data visualization techniques. # Problem Statement You are given a dataset containing information about various penguin species. Your task is to visualize certain aspects of this dataset using seaborn\'s `ecdfplot` function. # Dataset Load the dataset using seaborn\'s `load_dataset` function: ```python penguins = sns.load_dataset(\\"penguins\\") ``` # Requirements 1. Plot an ECDF of the `flipper_length_mm` variable. 2. Create a second ECDF plot of the `bill_length_mm` variable while differentiating between species using the `hue` parameter. 3. Show the absolute counts for the ECDF plot of `bill_length_mm`, again differentiating between species. 4. Produce an ECDF plot for `bill_depth_mm` along the y-axis to demonstrate usage of y-axis plotting. 5. Create a complementary ECDF plot (1 - CDF) for the `bill_length_mm` variable, differentiating between species. # Constraints - Use seaborn for all visualizations. - Ensure all plots have appropriate titles for clarity. - Handle any missing data appropriately. # Expected Output Ensure your implementation meets the following criteria: - Each plot should be correctly titled and axis-labeled. - Distinguish between species using different colors in the hue parameter. - Properly display counts instead of proportions where specified. - Demonstrate correct use of complementary ECDF. # Example Code Below is an example code template you can follow. Complete the code to meet the requirements. ```python import seaborn as sns import matplotlib.pyplot as plt # Set theme sns.set_theme() # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Requirement 1: ECDF plot of flipper_length_mm plt.figure() sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"ECDF of Flipper Length (mm)\\") plt.show() # Requirement 2: ECDF plot of bill_length_mm with hue=\\"species\\" plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Bill Length (mm) by Species\\") plt.show() # Requirement 3: ECDF plot of bill_length_mm with hue=\\"species\\" with count plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.title(\\"Count-Based ECDF of Bill Length (mm) by Species\\") plt.show() # Requirement 4: ECDF plot of bill_depth_mm along y-axis plt.figure() sns.ecdfplot(data=penguins, y=\\"bill_depth_mm\\") plt.title(\\"ECDF of Bill Depth (mm)\\") plt.show() # Requirement 5: Complementary ECDF plot of bill_length_mm with hue=\\"species\\" plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\\"Complementary ECDF of Bill Length (mm) by Species\\") plt.show() ``` # Submission Submit your completed code and ensure all visualizations match the described requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_ecdf(): # Set theme sns.set_theme() # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Check to ensure that the dataset loaded correctly and has the necessary columns if not all(column in penguins.columns for column in [\\"flipper_length_mm\\", \\"bill_length_mm\\", \\"bill_depth_mm\\", \\"species\\"]): raise ValueError(\\"Dataset does not contain the required columns.\\") # Requirement 1: ECDF plot of flipper_length_mm plt.figure() sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"ECDF of Flipper Length (mm)\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"ECDF\\") plt.show() # Requirement 2: ECDF plot of bill_length_mm with hue=\\"species\\" plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Bill Length (mm) by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"ECDF\\") plt.show() # Requirement 3: ECDF plot of bill_length_mm with hue=\\"species\\" with count plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.title(\\"Count-Based ECDF of Bill Length (mm) by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() # Requirement 4: ECDF plot of bill_depth_mm along y-axis plt.figure() sns.ecdfplot(data=penguins, y=\\"bill_depth_mm\\") plt.title(\\"ECDF of Bill Depth (mm)\\") plt.xlabel(\\"ECDF\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.show() # Requirement 5: Complementary ECDF plot of bill_length_mm with hue=\\"species\\" plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\\"Complementary ECDF of Bill Length (mm) by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Complementary ECDF\\") plt.show()"},{"question":"**Question: Advanced KDE Plotting with Seaborn** You are given two datasets: `tips` and `geyser`. Your task is to create a combined visualization that demonstrates advanced usage of seaborn\'s `kdeplot` function. This requires you to plot both univariate and bivariate KDE plots on the same figure, with various customizations. # Input 1. The `tips` dataset available in seaborn. 2. The `geyser` dataset available in seaborn. # Tasks 1. Create a horizontal KDE plot of the `total_bill` column from the `tips` dataset. 2. Superimpose another KDE plot on the same figure showing the `total_bill` distribution, but with hue mapped to the `time` variable and using a fill. 3. On a separate subplot, create a bivariate KDE plot of the `waiting` and `duration` columns from the `geyser` dataset with conditional distribution mapped to the `kind` variable, using filled contours, and a custom colormap of your choice. 4. Modify the appearance of all plots to have a more appealing aesthetic. # Constraints - Use `seaborn` and `matplotlib` for the visualizations. - The figure should be well-organized and include appropriate titles and legends for clarity. - Ensure the plots are rendered with sufficient resolution and clarity for each aspect to be visually distinguishable. # Performance Requirements Not applicable. # Expected Output A figure containing the following: - A subplot with a horizontal KDE plot of `total_bill` from the `tips` dataset. - The same subplot includes a filled KDE plot of `total_bill` conditional on `time`. - A second subplot with a bivariate KDE plot of `waiting` and `duration` from the `geyser` dataset, conditional on `kind`, with filled contours and a custom colormap. - Appropriate titles, labels, and legends for each plot. # Example Solution ```python import seaborn as sns import matplotlib.pyplot as plt # Load the datasets tips = sns.load_dataset(\\"tips\\") geyser = sns.load_dataset(\\"geyser\\") # Set the theme for the plots sns.set_theme() # Create the figure and subplots fig, ax = plt.subplots(1, 2, figsize=(18, 8)) # Univariate KDE plot for total_bill from tips dataset sns.kdeplot(data=tips, y=\\"total_bill\\", ax=ax[0], label=\\"Total Bill\\") sns.kdeplot(data=tips, y=\\"total_bill\\", hue=\\"time\\", ax=ax[0], fill=True) ax[0].legend(title=\'Time\') ax[0].set_title(\'Total Bill Distribution by Time\') # Bivariate KDE plot for waiting and duration from geyser dataset sns.kdeplot( data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, cmap=\\"viridis\\", ax=ax[1] ) ax[1].set_title(\'Geyser Waiting vs Duration by Kind\') # Show the plots plt.tight_layout() plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the datasets tips = sns.load_dataset(\\"tips\\") geyser = sns.load_dataset(\\"geyser\\") # Set the theme for the plots sns.set_theme() # Create the figure and subplots fig, ax = plt.subplots(1, 2, figsize=(18, 8)) # Univariate KDE plot for total_bill from tips dataset sns.kdeplot(data=tips, y=\\"total_bill\\", ax=ax[0], label=\\"Total Bill\\") sns.kdeplot(data=tips, y=\\"total_bill\\", hue=\\"time\\", ax=ax[0], fill=True) ax[0].legend(title=\'Time\') ax[0].set_title(\'Total Bill Distribution by Time\') # Bivariate KDE plot for waiting and duration from geyser dataset sns.kdeplot( data=geyser, x=\\"waiting\\", y=\\"duration\\", hue=\\"kind\\", fill=True, cmap=\\"viridis\\", ax=ax[1] ) ax[1].set_title(\'Geyser Waiting vs Duration by Kind\') # Show the plots plt.tight_layout() plt.show()"},{"question":"# Complex Unicode String Operations **Objective:** Implement a Python function that performs the following tasks using the provided Unicode handling APIs: 1. Create a Unicode string from a given UTF-8 encoded byte string. 2. Convert this Unicode string to uppercase characters. 3. Perform a certain check on the characters and print the corresponding results. 4. Encode the result back to a UTF-8 encoded byte string. **Function Signature:** ```python def process_unicode_string(utf8_string: bytes) -> bytes: Perform complex Unicode string operations. Args: utf8_string (bytes): The input byte string encoded in UTF-8. Returns: bytes: The result byte string encoded in UTF-8. ``` **Steps:** 1. **Decode the input byte string (`utf8_string`) to create a Unicode string** using the `PyUnicode_DecodeUTF8` API. 2. **Convert** the resulting Unicode string to uppercase using the `Py_UNICODE_TOUPPER` API. 3. **Check each character in the string** to determine if it is a whitespace character using the `Py_UNICODE_ISSPACE` API. Print the result as a list of booleans corresponding to each character in the string. 4. **Encode** the modified Unicode string back to a UTF-8 encoded byte string using the `PyUnicode_AsUTF8String` API. **Example:** ```python input_byte_str = b\'this is a test\' output_byte_str = process_unicode_string(input_byte_str) print(output_byte_str) # Output should be the uppercase version of the input string ``` **Constraints:** - Do not use any Python built-in string methods for uppercasing or checking spaces. Operations should be performed using the provided Unicode APIs. - Assume the input byte string is always valid UTF-8 encoded text. **Hint:** Refer to the documentation for the correct usage of the specified APIs. The task involves handling the Unicode objects at a lower level, simulating some internal operations and ensuring correctness through API interactions.","solution":"import sys def process_unicode_string(utf8_string: bytes) -> bytes: Perform complex Unicode string operations. Args: utf8_string (bytes): The input byte string encoded in UTF-8. Returns: bytes: The result byte string encoded in UTF-8. # Decode the input byte string to create a Unicode string unicode_string = utf8_string.decode(\'utf-8\') # Convert the Unicode string to uppercase uppercase_unicode_string = \'\'.join(chr(char).upper() if char != \' \' else \' \' for char in map(ord, unicode_string)) # Check if each character is a whitespace character is_space_list = [char.isspace() for char in uppercase_unicode_string] print(is_space_list) # Encode the modified Unicode string back to a UTF-8 encoded byte string result_byte_string = uppercase_unicode_string.encode(\'utf-8\') return result_byte_string"},{"question":"Objective: Create a comprehensive KDE plot using Seaborn that demonstrates your mastery of both fundamental and advanced functionalities of the `sns.kdeplot` function. Task: 1. **Dataset**: - Use the `penguins` dataset included with Seaborn. Load it using `sns.load_dataset(\'penguins\')`. 2. **Plot Requirements**: - **Univariate KDE Plot**: - Plot the KDE distribution of the `body_mass_g` column. - Adjust the bandwidth to be less smooth by setting `bw_adjust` to 0.3. - **Bivariate KDE Plot**: - On the same figure, overlay a bivariate KDE plot between `body_mass_g` and `flipper_length_mm`. - Apply log scaling to the `body_mass_g` axis. - **Hue Mapping**: - Color the bivariate KDE plot based on the `species` column. - Ensure the plot displays filled contours. - **Normalization**: - Stack the KDE distributions of `body_mass_g` for the different `species` using `multiple=\'stack\'`. - **Appearance**: - Use the \'coolwarm\' palette and set transparency (`alpha`) to 0.6 for filled areas. 3. **Implementation**: - Write a function `plot_penguins_kde()` that performs the tasks described above. - Ensure the plot is well-labeled with an appropriate title and axis labels. Function Signature: ```python def plot_penguins_kde(): pass ``` Constraints: - Your solution should render a single comprehensive plot. - You must use the `seaborn.kdeplot` function to generate all the required plots. - Ensure your plot is clear, well-labeled, and visually distinguishes between the different species using color. Expected Output: A function `plot_penguins_kde()` that generates and displays a KDE plot meeting all the specified requirements. When the function is called, it should produce a plot that helps in visualizing the kernel density of body mass, the relationship between body mass and flipper length, and the distinctions between different species in terms of these variables.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguins_kde(): # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') fig, ax = plt.subplots() # Univariate KDE plot for body_mass_g sns.kdeplot(data=penguins, x=\'body_mass_g\', alpha=0.6, bw_adjust=0.3, ax=ax, multiple=\'stack\', palette=\'coolwarm\') # Bivariate KDE plot for body_mass_g and flipper_length_mm sns.kdeplot( data=penguins, x=\'body_mass_g\', y=\'flipper_length_mm\', hue=\'species\', fill=True, palette=\'coolwarm\', alpha=0.6, log_scale=(True, False), ax=ax ) # Set title and labels ax.set_title(\'KDE of Penguin Body Mass and Flipper Length\') ax.set_xlabel(\'Body Mass (g)\') ax.set_ylabel(\'Flipper Length (mm)\') # Display the plot plt.show()"},{"question":"**Objective**: Your task is to demonstrate your understanding and usage of pandas options and settings. You need to write a function that configures pandas display options, processes a DataFrame, and ensures the options are properly set and reset as required. **Problem Statement**: Write a function named `configure_and_process_dataframe` which accepts a DataFrame and several optional parameters to adjust pandas display settings. The function should perform the following steps: 1. Temporarily adjust pandas display options using the specified parameters. 2. Print the DataFrame\'s summary using `DataFrame.info`. 3. Check if the DataFrame contains any column names with more than 10 characters and modify `display.max_colwidth` accordingly. 4. Print the first 5 rows of the DataFrame. 5. Restore all options to their original settings. **Function Signature**: ```python def configure_and_process_dataframe(df: pd.DataFrame, max_rows: int = 60, max_columns: int = 20, precision: int = 4) -> None: pass ``` **Input**: - `df` (pd.DataFrame): The DataFrame to be processed. - `max_rows` (int, optional): Maximum number of rows to display. Default is 60. - `max_columns` (int, optional): Maximum number of columns to display. Default is 20. - `precision` (int, optional): The precision for floating-point numbers. Default is 4. **Output**: - No return value. The function prints to the console. **Constraints**: - Do not permanently alter any global settings. - Use the `pd.option_context` to temporarily set values. **Example**: Consider a DataFrame `df` created as below: ```python import pandas as pd import numpy as np data = np.random.randn(100, 25) df = pd.DataFrame(data, columns=[f\\"col_{i}\\" for i in range(25)]) ``` Call the function with: ```python configure_and_process_dataframe(df, max_rows=10, max_columns=15, precision=5) ``` **Expected Output**: 1. Temporary display settings are set. 2. Summary information of the DataFrame is printed. 3. Adjust `display.max_colwidth` if any column name exceeds 10 characters. 4. First 5 rows of the DataFrame are printed. 5. All display settings are reset. Make sure that your implementation handles all edge cases, such as DataFrames with very large or very small dimensions, and varying lengths for column names.","solution":"import pandas as pd def configure_and_process_dataframe(df: pd.DataFrame, max_rows: int = 60, max_columns: int = 20, precision: int = 4) -> None: # Save current settings with pd.option_context(\'display.max_rows\', max_rows, \'display.max_columns\', max_columns, \'display.precision\', precision): # Print DataFrame summary df.info() # Check for column names with more than 10 characters and adjust max_colwidth if any(len(col) > 10 for col in df.columns): with pd.option_context(\'display.max_colwidth\', None): print(df.head()) else: print(df.head())"},{"question":"# Coding Assessment: CSV Data Processing and Analysis **Objective:** Write a Python program that reads a CSV file containing information about a set of products, processes the data to perform some analysis, and writes the result to a new CSV file. **Input:** 1. A CSV file named `products.csv` with the following format: - The first row contains the column headers: `product_id`, `product_name`, `category`, `price`, `quantity_available`. - Each subsequent row contains the details of a product. 2. The CSV file follows the standard \'excel\' dialect. **Output:** 1. A CSV file named `summary.csv` having the following format: - Three columns: `category`, `total_quantity`, `total_value`. - Each row summarizes the total quantity and total value of products for each category. **Task Requirements:** 1. Read the `products.csv` file using the `csv.reader`. 2. Calculate the total quantity and total value of products for each category. 3. Write the computed summary to `summary.csv` using the `csv.writer`. **Specific Constraints:** 1. Ensure that the program handles any potential errors (e.g., missing fields, incorrect data types). 2. Use appropriate data structures to store intermediate values during the processing. 3. The program should be efficient, handling large CSV files within reasonable time and memory limits. **Expected Input File (`products.csv`) Example:** ``` product_id,product_name,category,price,quantity_available 1,Product A,Electronics,199.99,100 2,Product B,Clothing,29.99,200 3,Product C,Electronics,99.99,150 4,Product D,Home,59.99,300 5,Product E,Clothing,19.99,500 ``` **Expected Output File (`summary.csv`) Example:** ``` category,total_quantity,total_value Electronics,250,44997.5 Clothing,700,22993 Home,300,17997 ``` ```python import csv def read_products(file_path): products = [] try: with open(file_path, \'r\', newline=\'\', encoding=\'utf-8\') as csvfile: reader = csv.DictReader(csvfile) for row in reader: products.append(row) except csv.Error as e: print(f\\"Error reading CSV file at line {reader.line_num}: {e}\\") return products def summarize_products(products): summary = {} for product in products: try: category = product[\'category\'] price = float(product[\'price\']) quantity = int(product[\'quantity_available\']) if category not in summary: summary[category] = {\'total_quantity\': 0, \'total_value\': 0.0} summary[category][\'total_quantity\'] += quantity summary[category][\'total_value\'] += price * quantity except (ValueError, KeyError) as e: print(f\\"Error processing product data: {e}\\") return summary def write_summary(file_path, summary): try: with open(file_path, \'w\', newline=\'\', encoding=\'utf-8\') as csvfile: writer = csv.writer(csvfile) writer.writerow([\'category\', \'total_quantity\', \'total_value\']) for category, data in summary.items(): writer.writerow([category, data[\'total_quantity\'], data[\'total_value\']]) except csv.Error as e: print(f\\"Error writing CSV file: {e}\\") def main(): products = read_products(\'products.csv\') if products: summary = summarize_products(products) write_summary(\'summary.csv\', summary) if __name__ == \'__main__\': main() ``` **Explanation:** 1. The `read_products` function reads the `products.csv` file and returns a list of product dictionaries. 2. The `summarize_products` function processes the list of products to calculate the total quantity and total value for each category. 3. The `write_summary` function writes the computed summary to `summary.csv`. 4. The `main` function orchestrates the reading, processing, and writing of CSV data. **Performance Considerations:** - The program should efficiently handle large CSV files by processing rows sequentially and minimizing memory usage. **Error Handling:** - Properly handle CSV reading/writing errors and data conversion errors, logging appropriate messages to aid debugging.","solution":"import csv def read_products(file_path): products = [] try: with open(file_path, \'r\', newline=\'\', encoding=\'utf-8\') as csvfile: reader = csv.DictReader(csvfile) for row in reader: products.append(row) except csv.Error as e: print(f\\"Error reading CSV file at line {reader.line_num}: {e}\\") return products def summarize_products(products): summary = {} for product in products: try: category = product[\'category\'] price = float(product[\'price\']) quantity = int(product[\'quantity_available\']) if category not in summary: summary[category] = {\'total_quantity\': 0, \'total_value\': 0.0} summary[category][\'total_quantity\'] += quantity summary[category][\'total_value\'] += price * quantity except (ValueError, KeyError) as e: print(f\\"Error processing product data: {e}\\") return summary def write_summary(file_path, summary): try: with open(file_path, \'w\', newline=\'\', encoding=\'utf-8\') as csvfile: writer = csv.writer(csvfile) writer.writerow([\'category\', \'total_quantity\', \'total_value\']) for category, data in summary.items(): writer.writerow([category, data[\'total_quantity\'], data[\'total_value\']]) except csv.Error as e: print(f\\"Error writing CSV file: {e}\\") def main(): products = read_products(\'products.csv\') if products: summary = summarize_products(products) write_summary(\'summary.csv\', summary) if __name__ == \'__main__\': main()"},{"question":"# Question: Advanced Color Palettes in Seaborn Suppose you are a data scientist analyzing a dataset containing various numeric, categorical, and ordinal data. You are required to create a series of visualizations that effectively represent this data using seaborn\'s color palettes. Your task is to implement a function that plots three different subplots using qualitative, sequential, and diverging palettes. The function should demonstrate your understanding of seaborn\'s color palette selection and customization methods. Function Signature: ```python def plot_custom_palettes(dataset): pass ``` Input: - `dataset`: A pandas DataFrame that includes at least one numeric, one categorical, and one ordinal column. Requirements: 1. **Qualitative Palette Plot**: Create a scatter plot differentiating the data points by a categorical variable. Use a qualitative palette suitable for categorical data. 2. **Sequential Palette Plot**: Create a heatmap visualizing the correlation matrix of the numeric columns in the dataset. Use a sequential palette. 3. **Diverging Palette Plot**: Create a diverging palette plot that shows the data distribution of an ordinal variable. Constraints: - You must use at least one custom color palette in each of the three plots. - Each plot should be well-labeled and include an appropriate legend or color bar to make the plot interpretable. - Avoid using any built-in seaborn or matplotlib colormaps directly without modification. Example Output: Your function should produce a single figure with three subplots, each demonstrating a different type of palette (qualitative, sequential, and diverging). Notes: - You may assume that seaborn and other necessary libraries (such as pandas and matplotlib) are already imported. - Focus on the suitability of color usage for the type of data represented in each plot. - Document any assumptions you make about the input data. ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_custom_palettes(dataset): # extracting columns from dataset numeric_cols = dataset.select_dtypes(include=[\'number\']).columns categorical_col = dataset.select_dtypes(include=[\'category\']).columns[0] ordinal_col = dataset.select_dtypes(include=[\'category\']).columns[1] fig, axs = plt.subplots(1, 3, figsize=(18, 5)) # Qualitative Palette Plot qualitative_palette = sns.color_palette(\\"Set2\\") sns.scatterplot(data=dataset, x=numeric_cols[0], y=numeric_cols[1], hue=categorical_col, palette=qualitative_palette, ax=axs[0]) axs[0].set_title(\\"Qualitative Palette Scatter Plot\\") # Sequential Palette Plot sequential_palette = sns.cubehelix_palette(as_cmap=True) corr_matrix = dataset.corr() sns.heatmap(corr_matrix, cmap=sequential_palette, ax=axs[1]) axs[1].set_title(\\"Sequential Palette Heatmap\\") # Diverging Palette Plot diverging_palette = sns.diverging_palette(220, 20, as_cmap=True) sns.histplot(data=dataset, x=ordinal_col, hue=ordinal_col, palette=diverging_palette, ax=axs[2], multiple=\\"stack\\") axs[2].set_title(\\"Diverging Palette Distribution\\") plt.tight_layout() plt.show() ``` Write a `plot_custom_palettes` function as described. Demonstrate your comprehension of seaborn\'s advanced color palette concepts through your solution.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_custom_palettes(dataset): # Assume the dataset has columns: \'numeric_1\', \'numeric_2\' for numeric data # \'categorical\' for categorical data and \'ordinal\' for ordinal data fig, axs = plt.subplots(1, 3, figsize=(18, 5)) # Qualitative Palette Plot qualitative_palette = sns.color_palette(\\"Paired\\") sns.scatterplot(data=dataset, x=\'numeric_1\', y=\'numeric_2\', hue=\'categorical\', palette=qualitative_palette, ax=axs[0]) axs[0].set_title(\\"Qualitative Palette Scatter Plot\\") axs[0].legend(title=\'Category\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') # Sequential Palette Plot sequential_palette = sns.color_palette(\\"light:b\\", as_cmap=True) corr_matrix = dataset[[\'numeric_1\', \'numeric_2\']].corr() sns.heatmap(corr_matrix, cmap=sequential_palette, annot=True, ax=axs[1]) axs[1].set_title(\\"Sequential Palette Heatmap\\") # Diverging Palette Plot diverging_palette = sns.diverging_palette(220, 20, n=10) sns.histplot(data=dataset, x=\'ordinal\', hue=\'ordinal\', multiple=\'stack\', palette=diverging_palette, ax=axs[2]) axs[2].set_title(\\"Diverging Palette Distribution\\") plt.tight_layout() plt.show()"},{"question":"**Problem Statement:** During financial calculations, you might encounter precision issues with floating-point numbers. The inaccuracies can lead to significant errors in the results. This problem requires you to implement a function that avoids such errors by using exact arithmetic. **Task:** Write a Python function called `sum_exact` that takes a list of floating-point numbers and returns their exact sum using the `decimal` module. Ensure that the sum is accurate up to 10 decimal places. **Function Signature:** ```python def sum_exact(numbers: list[float]) -> float: pass ``` **Input:** - `numbers` (list[float]): A list of floating-point numbers. Length of the list is between 1 and 1000, inclusive. Each number will have at most 10 decimal places. **Output:** - (float): The exact sum of the input numbers, accurate up to 10 decimal places. **Constraints:** - Use the `decimal.Decimal` class for all arithmetic operations. - Your solution should be efficient and capable of handling up to 1000 numbers in a reasonable time frame. **Example:** ```python assert sum_exact([0.1, 0.1, 0.1]) == 0.3 assert sum_exact([1.23, 4.56, 7.89]) == 13.68 assert sum_exact([0.1, 0.2, 0.3, 0.4]) == 1.0 ``` **Notes:** - You should convert each floating-point number to a `decimal.Decimal` object before performing the summation. - To ensure the accuracy up to 10 decimal places, you might need to round the result appropriately before returning it.","solution":"from decimal import Decimal, getcontext def sum_exact(numbers: list[float]) -> float: Returns the exact sum of the list of floating-point numbers using decimal arithmetic. getcontext().prec = 15 # Set precision higher to handle intermediate calculations total = Decimal(\'0.0\') for number in numbers: total += Decimal(str(number)) # Convert each number to string then to Decimal to avoid rounding issues return round(float(total), 10) # Round the result to 10 decimal places"},{"question":"Command Line Argument Parser You are tasked with developing a Python function utilizing the `shlex` module. Your function will take a string formatted similar to command-line arguments and parse it into a dictionary where each flag (prefixed with `--` or `-`) is a key, and its corresponding value is the key\'s value. If a flag does not have a subsequent value, it should default to `True`. Function Signature ```python def parse_arguments(argument_string: str) -> dict: pass ``` Input - `argument_string` (str): A string containing command-line-like arguments. Each argument starts with a flag (prefixed with `--` or `-`) followed by its value if applicable. Output - Returns a dictionary where: - Keys are argument flags (without the prefixes `--` or `-`). - Values are the corresponding values from the input string or `True` if the flag has no value. Examples ```python argument_string = \\"--file example.txt --verbose -d --output output.txt -f\\" result = parse_arguments(argument_string) # Expected output: # { # \\"file\\": \\"example.txt\\", # \\"verbose\\": True, # \\"d\\": True, # \\"output\\": \\"output.txt\\", # \\"f\\": True # } argument_string = \\"-a -b -c value --long-arg SomeValue\\" result = parse_arguments(argument_string) # Expected output: # { # \\"a\\": True, # \\"b\\": True, # \\"c\\": \\"value\\", # \\"long-arg\\": \\"SomeValue\\" # } ``` Constraints - You should use the `shlex` module for parsing the input string. - Assume the input string is well-formatted with spaces separating each argument or its value. Notes - The `shlex.split()` function can be beneficial for splitting the input string while preserving shell-like syntax characteristics. - Make sure to handle flags with both single and double dashes. Implement the function `parse_arguments` to solve the problem.","solution":"import shlex def parse_arguments(argument_string: str) -> dict: Parses a command-line-like argument string into a dictionary. Parameters: argument_string (str): A string containing command-line-like arguments. Returns: dict: A dictionary with flags as keys (without the prefixes \'--\' or \'-\'), and their corresponding values from the input string or `True` if no value is provided. arguments = shlex.split(argument_string) argument_dict = {} i = 0 while i < len(arguments): arg = arguments[i] if arg.startswith(\'--\'): key = arg[2:] # Check if the next element exists and is not another flag if i + 1 < len(arguments) and not arguments[i + 1].startswith(\'-\'): argument_dict[key] = arguments[i + 1] i += 1 else: argument_dict[key] = True elif arg.startswith(\'-\'): key = arg[1:] # Check if the next element exists and is not another flag if i + 1 < len(arguments) and not arguments[i + 1].startswith(\'-\'): argument_dict[key] = arguments[i + 1] i += 1 else: argument_dict[key] = True i += 1 return argument_dict"},{"question":"# Resource Management in Python The `resource` module in Python provides mechanisms for measuring and controlling system resources utilized by a program. This module is crucial for writing programs that need to manage their resource consumption, such as limiting memory usage to avoid exhausting system resources. In this task, you will implement a function to manage resource usage for the current process. Your function should: 1. Retrieve the current resource limits for a specific resource. 2. Set new limits for that resource. 3. Retrieve the resource usage after some code execution. # Your Task Write a function `manage_resource_limits(resource_type, new_limits_func)` that: - Retrieves the current soft and hard limits for a specified `resource_type`. - Sets new soft and hard limits for the specified resource using the function `new_limits_func`. - `new_limits_func()` should return a tuple `(soft, hard)` representing the new limits. - Executes a resource-intensive task (e.g., looping a large number of iterations). - Returns the resource usage statistics after the task execution. # Input - `resource_type`: An integer representing the resource type (using constants like `resource.RLIMIT_CPU`, `resource.RLIMIT_RSS`, etc.). - `new_limits_func`: A function that returns a tuple `(soft, hard)` representing the new soft and hard limits for the resource. # Output - A tuple containing: - The original limits `(soft, hard)` before modification. - The new limits `(soft, hard)` after modification. - The resource usage statistics after task execution, as retrieved by `resource.getrusage()`. # Example ```python import resource def new_limits_func(): return (resource.RLIM_INFINITY, resource.RLIM_INFINITY) result = manage_resource_limits(resource.RLIMIT_CPU, new_limits_func) print(result) # Should print the original limits, the new limits, and resource usage statistics ``` # Constraints - You can assume that `new_limits_func` will always return valid limits that do not exceed the hard limits set by the system. - Handle possible exceptions like `ValueError` and `OSError` appropriately. # Implementation Notes - Use `resource.getrlimit(resource_type)` to get the current limits. - Use `resource.setrlimit(resource_type, (soft, hard))` to set new limits. - Use `resource.getrusage(resource.RUSAGE_SELF)` to get resource usage statistics for the current process. # Additional Information Refer to the `resource` module documentation for details on different resource types and their meanings.","solution":"import resource def manage_resource_limits(resource_type, new_limits_func): Manages resource limits for the current process. Args: resource_type (int): The resource type (e.g., resource.RLIMIT_CPU). new_limits_func (function): A function that returns a tuple (soft, hard) representing the new limits. Returns: tuple: A tuple containing original limits, new limits, and resource usage statistics. try: # Retrieve the current limits. original_limits = resource.getrlimit(resource_type) # Get new limits from the provided function. new_limits = new_limits_func() # Apply the new limits. resource.setrlimit(resource_type, new_limits) # Execute a resource-intensive task. for _ in range(10**6): pass # Retrieve resource usage statistics. usage_stats = resource.getrusage(resource.RUSAGE_SELF) return original_limits, new_limits, usage_stats except ValueError as ve: print(f\\"ValueError occurred: {ve}\\") except OSError as oe: print(f\\"OSError occurred: {oe}\\")"},{"question":"**Title: Validate Unicode String Using Stringprep Module** **Objective:** You are required to implement a function that validates an input Unicode string based on certain rules defined in the `stringprep` module. Specifically, the function will ensure that the input string does not contain any characters that are inappropriate for plain text (as defined in Table C.6 of RFC 3454), and it will map any characters that are commonly mapped to nothing (as defined in Table B.1) by removing them. **Function Signature:** ```python def validate_and_prepare_string(input_string: str) -> str: Validates and prepares the input Unicode string based on RFC 3454 rules. Parameters: input_string (str): The input Unicode string to be validated and prepared. Returns: str: A new string that is validated and prepared according to the specified rules. pass ``` **Input:** - `input_string` (str): A Unicode string containing characters to be validated and prepared. **Output:** - A new string that has been validated and prepared according to the following rules: 1. The string must not include any characters inappropriate for plain text (Table C.6). 2. Any characters commonly mapped to nothing (Table B.1) should be removed from the string. **Constraints:** - The input string length is at most (10^6) characters. - The function should run efficiently within a reasonable time for the given input size. **Example:** ```python assert validate_and_prepare_string(\\"Hellou2028World\\") == \\"HelloWorld\\" assert validate_and_prepare_string(\\"Invalidu202DControl\\") == \\"InvalidControl\\" assert validate_and_prepare_string(\\"Goodu00ADMorning\\") == \\"GoodMorning\\" ``` **Note:** - In the first example, the character `u2028` (Line Separator) is removed because it is inappropriate for plain text (Table C.6). - In the second example, the character `u202D` (Left-to-Right Override) is removed because it is inappropriate for plain text (Table C.6). - In the third example, the character `u00AD` (Soft Hyphen) is removed because it is commonly mapped to nothing (Table B.1). Implement this function using the functions provided by the `stringprep` module to complete this task.","solution":"import stringprep def validate_and_prepare_string(input_string: str) -> str: Validates and prepares the input Unicode string based on RFC 3454 rules. Parameters: input_string (str): The input Unicode string to be validated and prepared. Returns: str: A new string that is validated and prepared according to the specified rules. output = [] for ch in input_string: # Skip characters mapped to nothing (stringprep.in_table_b1) if stringprep.in_table_b1(ch): continue # Skip characters inappropriate for plain text (stringprep.in_table_c12) if stringprep.in_table_c12(ch): continue if stringprep.in_table_c21(ch): continue if stringprep.in_table_c22(ch): continue if stringprep.in_table_c3(ch): continue if stringprep.in_table_c4(ch): continue if stringprep.in_table_c5(ch): continue if stringprep.in_table_c6(ch): continue if stringprep.in_table_c7(ch): continue if stringprep.in_table_c8(ch): continue if stringprep.in_table_c9(ch): continue output.append(ch) return \'\'.join(output)"},{"question":"# Advanced Python Functional Programming: Assess Students\' Understanding of Iterators, Generators, and Higher-Order Functions Objective: Create a Python script that processes a log file and extracts information about user activity. Use iterators, generators, and functions from itertools and functools modules to accomplish this task in a functional programming style. Problem Statement: You are given a log file, `user_activity.log`, where each line consists of user activity in the format: ``` timestamp, user_id, action ``` Example: ``` 2023-10-01 12:00:00, user123, login 2023-10-01 12:05:00, user123, upload 2023-10-01 12:06:00, user456, login 2023-10-01 12:10:00, user123, logout 2023-10-01 12:15:00, user456, upload 2023-10-01 12:20:00, user456, logout ``` Your task is to implement the following functions: 1. **`log_iterator(file_path)`**: - Input: `file_path` (str) - Path to the log file. - Output: Iterator that yields each line of the log file. 2. **`parse_log(log_iter)`**: - Input: `log_iter` - Iterator from `log_iterator`. - Output: Iterator of tuples `(timestamp, user_id, action)` parsed from each line. 3. **`filter_by_action(log_iter, action_type)`**: - Input: - `log_iter` - Iterator from `parse_log`. - `action_type` (str) - Action type to filter by (e.g., \'login\', \'logout\', \'upload\'). - Output: Iterator of tuples `(timestamp, user_id, action)` filtered by the given action type. 4. **`group_by_user(log_iter)`**: - Input: `log_iter` - Iterator from `filter_by_action`. - Output: An iterator of tuples `(user_id, user_logs)` where `user_logs` is a list of `(timestamp, action)` tuples for each user. 5. **`user_activity_summary(file_path)`**: - Input: `file_path` (str) - Path to the log file. - Output: Dictionary where keys are user_ids and values are lists of actions. Implement the above functions, ensuring you utilize features like iterators, generators, itertools, functools, and list comprehensions. Constraints: - Use the functional programming style as much as possible. - Avoid using standard loops where iterators or generator expressions can be used. - The log file size can be significant; aim for an efficient solution that does not load the entire file into memory. Example usage: ```python if __name__ == \'__main__\': file_path = \'user_activity.log\' summary = user_activity_summary(file_path) for user_id, actions in summary.items(): print(f\\"User: {user_id}\\") for action in actions: print(f\\" - {action[0]}: {action[1]}\\") ``` Expected Output: For the example log file provided, the output should be: ``` User: user123 - 2023-10-01 12:00:00: login - 2023-10-01 12:05:00: upload - 2023-10-01 12:10:00: logout User: user456 - 2023-10-01 12:06:00: login - 2023-10-01 12:15:00: upload - 2023-10-01 12:20:00: logout ```","solution":"import itertools from collections import defaultdict def log_iterator(file_path): Yields each line in the given log file. with open(file_path, \'r\') as file: for line in file: yield line.strip() def parse_log(log_iter): Parses each line from log iterator into a tuple (timestamp, user_id, action). for line in log_iter: timestamp, user_id, action = line.split(\', \') yield timestamp, user_id, action def filter_by_action(log_iter, action_type): Filters the log iterator by the given action type. for log_entry in log_iter: if log_entry[2] == action_type: yield log_entry def group_by_user(log_iter): Groups log entries by user_id. user_logs = defaultdict(list) for timestamp, user_id, action in log_iter: user_logs[user_id].append((timestamp, action)) for user_id, logs in user_logs.items(): yield user_id, logs def user_activity_summary(file_path): Generates a summary of user activity from the log file. log_lines = log_iterator(file_path) parsed_logs = parse_log(log_lines) grouped_logs = group_by_user(parsed_logs) summary = {} for user_id, logs in grouped_logs: summary[user_id] = logs return summary"},{"question":"**Advanced Coding Assessment: Ridge Regression with Cross-Validation** **Objective:** Assess your knowledge and skills in using scikit-learn for Ridge Regression, including cross-validation for hyperparameter tuning and model evaluation. **Problem Statement:** You are provided with a dataset that contains features representing different variables and a target variable representing house prices. Your task is to implement a Ridge Regression model using scikit-learn, perform hyperparameter tuning using cross-validation to find the optimal regularization parameter (`alpha`), and evaluate the model\'s performance. **Requirements:** 1. Load the dataset from a CSV file `house_prices.csv` which contains the following columns: - `feature1`, `feature2`, ..., `featureN` (features representing different variables) - `target` (target variable representing house prices) 2. Implement a function `train_and_evaluate_ridge()` that: - Loads the dataset - Splits the dataset into training and testing sets (80% train, 20% test) - Uses `RidgeCV` to perform cross-validation and find the optimal `alpha`. Use `alphas=np.logspace(-6, 6, 13)` for the grid search. - Trains the Ridge Regression model using the optimal `alpha` on the training set - Evaluates the model\'s performance on the test set using Mean Squared Error (MSE) and R-squared score - Returns the MSE, R-squared score, and the optimal `alpha` value **Function Signature:** ```python def train_and_evaluate_ridge(csv_filepath: str) -> Tuple[float, float, float]: Trains and evaluates a Ridge Regression model using cross-validation. Parameters: - csv_filepath (str): The file path to the CSV file containing the dataset. Returns: - Tuple[float, float, float]: A tuple containing: - Mean Squared Error (MSE) on the test set - R-squared score on the test set - Optimal value of `alpha` found during cross-validation pass ``` **Constraints:** 1. Use `RidgeCV` from `sklearn.linear_model` for cross-validation. 2. Split the dataset in an 80-20 ratio for training and testing. 3. Use Mean Squared Error and R-squared score for evaluation. **Examples:** Assuming the dataset `house_prices.csv` contains: ``` feature1,feature2,...,featureN,target 0.5,1.0,...,3.5,200000 0.7,0.8,...,3.0,210000 ... ``` Your function should be able to load the data, train the Ridge Regression model with cross-validation, and return the evaluation metrics and optimal alpha. **Additional Notes:** - Handle any exceptions or errors that might occur during data loading or model training. - Ensure the function is efficient and well-documented. **Submission:** Submit the implemented function `train_and_evaluate_ridge()` along with necessary import statements and any additional helper functions or classes you define.","solution":"from typing import Tuple import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import RidgeCV from sklearn.metrics import mean_squared_error, r2_score def train_and_evaluate_ridge(csv_filepath: str) -> Tuple[float, float, float]: Trains and evaluates a Ridge Regression model using cross-validation. Parameters: - csv_filepath (str): The file path to the CSV file containing the dataset. Returns: - Tuple[float, float, float]: A tuple containing: - Mean Squared Error (MSE) on the test set - R-squared score on the test set - Optimal value of `alpha` found during cross-validation # Load dataset data = pd.read_csv(csv_filepath) # Split features and target X = data.drop(columns=[\'target\']) y = data[\'target\'] # Split data into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the range of alphas for RidgeCV alphas = np.logspace(-6, 6, 13) # Initialize RidgeCV with the specified alphas ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True) # Train the model ridge_cv.fit(X_train, y_train) # Predict on test set y_pred = ridge_cv.predict(X_test) # Calculate Mean Squared Error (MSE) and R-squared score on the test set mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) # Optimal alpha optimal_alpha = ridge_cv.alpha_ return mse, r2, optimal_alpha"},{"question":"Coding Assessment Question # Objective Write a program in Python using the `netrc` module to read and validate a `.netrc` file, extract authentication details for a given host, and ensure security compliance on POSIX systems. # Task 1. **Reading and Parsing a netrc File**: - Implement a function `read_netrc(file_path: str) -> netrc.netrc` that reads the specified netrc file. - Handle and log `FileNotFoundError` if the file does not exist. - Handle and log `netrc.NetrcParseError` for any parsing errors with appropriate details. 2. **Fetching Authentication Details**: - Implement a function `get_auth_details(netrc_obj: netrc.netrc, host: str) -> tuple` to fetch the authentication details `(login, account, password)` for a given host. - If the host is not present, return the details for the \'default\' entry. - If neither the host nor the default entry is found, return `None`. 3. **Security Compliance (POSIX only)**: - Implement a function `check_security(file_path: str) -> bool` that checks if the file ownership and permissions are secure. - The file should be owned by the user running the process and not accessible for read or write by any other user. - If the file is insecure, raise a `netrc.NetrcParseError` with a relevant message. # Input and Output Formats 1. Function `read_netrc(file_path: str) -> netrc.netrc`: - **Input**: A string, `file_path`, indicating the path to the netrc file. - **Output**: An instance of `netrc.netrc` encapsulating the parsed data, or raise appropriate exceptions. 2. Function `get_auth_details(netrc_obj: netrc.netrc, host: str) -> tuple`: - **Input**: An instance of `netrc.netrc`, and a string `host`. - **Output**: A tuple `(login, account, password)` or `None`. 3. Function `check_security(file_path: str) -> bool`: - **Input**: A string, `file_path`, indicating the path to the netrc file. - **Output**: A boolean indicating if the file meets security requirements, or raise a `netrc.NetrcParseError`. # Constraints - You may assume that the input file will be in a valid netrc format but should handle cases where the file has syntax errors. - Ensure your implementation is efficient and handles large netrc files gracefully. - Security check only applies to POSIX systems. # Example Usage ```python try: netrc_obj = read_netrc(\'/path/to/.netrc\') if check_security(\'/path/to/.netrc\'): auth_details = get_auth_details(netrc_obj, \'example.com\') print(auth_details) except (FileNotFoundError, netrc.NetrcParseError) as e: print(f\'Error: {e}\') ``` Good Luck!","solution":"import netrc import os import stat import logging def read_netrc(file_path: str) -> netrc.netrc: Reads and parses the specified netrc file. try: return netrc.netrc(file_path) except FileNotFoundError as e: logging.error(f\\"File not found: {file_path}\\") raise e except netrc.NetrcParseError as e: logging.error(f\\"Error parsing netrc file: {file_path} - {e}\\") raise e def get_auth_details(netrc_obj: netrc.netrc, host: str) -> tuple: Fetches authentication details for a given host. try: auth_details = netrc_obj.authenticators(host) if auth_details is None: auth_details = netrc_obj.authenticators(\'default\') return auth_details except KeyError: return None def check_security(file_path: str) -> bool: Checks if the file ownership and permissions are secure (POSIX systems only). st = os.stat(file_path) if (st.st_uid != os.getuid() or (st.st_mode & (stat.S_IRWXG | stat.S_IRWXO))): raise netrc.NetrcParseError(f\\"Insecure permissions on netrc file: {file_path}\\") return True"},{"question":"You have been assigned a task to manage and validate network configurations for a diverse set of devices. Using the `ipaddress` module, write a function that takes a list of IP address and network pairs, validates them, and returns a detailed report. The function should: 1. Validate the IP addresses and networks. If an address or network is invalid, it should be noted in the report. 2. Determine if each IP address belongs to its corresponding network. 3. Count the number of addresses in each network and include it in the report. 4. Handle both IPv4 and IPv6 addresses and networks. 5. Return a structured dictionary with the validation results. The function should have the following signature: ```python def validate_and_report(ip_network_pairs: List[Tuple[str, str]]) -> dict: pass ``` # Constraints - The IP address and network strings will be in valid formats but may not be logically correct (e.g., `192.168.1.300` is syntactically invalid). - You should raise an appropriate error if you encounter issues during processing. - Performance isn\'t a concern for this task, but the function should handle at least 100 pairs in a reasonable time. # Input: - `ip_network_pairs`: A list of tuples, where each tuple contains a pair of strings representing an IP address and a network, e.g., `[(\\"192.0.2.1\\", \\"192.0.2.0/24\\"), (\\"2001:db8::1\\", \\"2001:db8::/96\\")` # Output: - A dictionary with the following structure: ```python { \\"valid_pairs\\": List[Dict], # List of dictionaries for valid pairs with keys \'ip\', \'network\', and \'contains\'. \\"invalid_entries\\": List[Dict], # List of dictionaries for invalid entries with keys \'ip\', \'network\', and \'error\'. \\"network_sizes\\": Dict[str, int] # Dictionary with network as key and number of addresses in the network as value. } ``` # Example ```python ip_network_pairs = [ (\\"192.0.2.1\\", \\"192.0.2.0/24\\"), (\\"2001:db8::1\\", \\"2001:db8::/96\\"), (\\"192.0.2.1\\", \\"192.0.3.0/24\\"), (\\"invalid_ip\\", \\"192.0.2.0/24\\"), ] validate_and_report(ip_network_pairs) ``` Output: ```python { \\"valid_pairs\\": [ {\\"ip\\": \\"192.0.2.1\\", \\"network\\": \\"192.0.2.0/24\\", \\"contains\\": True}, {\\"ip\\": \\"2001:db8::1\\", \\"network\\": \\"2001:db8::/96\\", \\"contains\\": True}, {\\"ip\\": \\"192.0.2.1\\", \\"network\\": \\"192.0.3.0/24\\", \\"contains\\": False} ], \\"invalid_entries\\": [ {\\"ip\\": \\"invalid_ip\\", \\"network\\": \\"192.0.2.0/24\\", \\"error\\": \\"Invalid IP Address\\"} ], \\"network_sizes\\": { \\"192.0.2.0/24\\": 256, \\"2001:db8::/96\\": 4294967296, \\"192.0.3.0/24\\": 256 } } ```","solution":"from typing import List, Tuple, Dict import ipaddress def validate_and_report(ip_network_pairs: List[Tuple[str, str]]) -> dict: report = { \\"valid_pairs\\": [], \\"invalid_entries\\": [], \\"network_sizes\\": {} } for ip_str, network_str in ip_network_pairs: try: ip = ipaddress.ip_address(ip_str) except ValueError: report[\\"invalid_entries\\"].append({ \\"ip\\": ip_str, \\"network\\": network_str, \\"error\\": \\"Invalid IP Address\\" }) continue try: network = ipaddress.ip_network(network_str, strict=False) except ValueError: report[\\"invalid_entries\\"].append({ \\"ip\\": ip_str, \\"network\\": network_str, \\"error\\": \\"Invalid Network\\" }) continue contains = ip in network report[\\"valid_pairs\\"].append({ \\"ip\\": ip_str, \\"network\\": network_str, \\"contains\\": contains }) if network_str not in report[\\"network_sizes\\"]: report[\\"network_sizes\\"][network_str] = network.num_addresses return report"},{"question":"You are given a list of URLs to fetch their HTML content. Your task is to use the `ThreadPoolExecutor` to download these pages concurrently and count the number of occurrences of a specific word on each page. The results should be printed as soon as each download is complete. # Requirements: 1. Implement a function `fetch_and_count(urls: List[str], word: str) -> None` that takes: - `urls` : A list of strings representing the URLs. - `word`: A string representing the word to count. 2. Use `ThreadPoolExecutor` from the `concurrent.futures` module to perform the downloads concurrently. 3. Handle possible exceptions for each download gracefully and log any errors without terminating the entire process. 4. For each successfully downloaded page, count the occurrences of the given word in the HTML content (case-insensitive). 5. Print the URL followed by the word count as soon as the download and counting are complete. # Constraints: - You should use a maximum of 5 threads. - The URLs may include some that are invalid or slow to respond; ensure your implementation can handle these scenarios without hanging indefinitely. - If a page cannot be downloaded or processed, print the URL followed by \\"Error\\". # Example: Given the following URLs and word: ```python urls = [ \\"http://example.com\\", \\"http://invalid-url\\", \\"http://slow-page.com\\", \\"http://example.org\\" ] word = \\"example\\" ``` Your function might output (results may vary based on actual content): ``` http://example.com: 5 http://invalid-url: Error http://slow-page.com: Error http://example.org: 3 ``` # Boilerplate code: ```python from typing import List from concurrent.futures import ThreadPoolExecutor, as_completed import requests def fetch_and_count(urls: List[str], word: str) -> None: def load_url(url): try: response = requests.get(url, timeout=10) response.raise_for_status() return url, response.text except requests.RequestException as e: return url, None def count_word(html: str, word: str) -> int: if html is None: return None return html.lower().count(word.lower()) with ThreadPoolExecutor(max_workers=5) as executor: future_to_url = {executor.submit(load_url, url): url for url in urls} for future in as_completed(future_to_url): url = future_to_url[future] try: url, html = future.result() count = count_word(html, word) if count is not None: print(f\\"{url}: {count}\\") else: print(f\\"{url}: Error\\") except Exception as exc: print(f\\"{url}: Error\\") # Example usage: urls = [ \\"http://example.com\\", \\"http://invalid-url\\", \\"http://slow-page.com\\", \\"http://example.org\\" ] word = \\"example\\" fetch_and_count(urls, word) ```","solution":"from typing import List from concurrent.futures import ThreadPoolExecutor, as_completed import requests def fetch_and_count(urls: List[str], word: str) -> None: def load_url(url): try: response = requests.get(url, timeout=10) response.raise_for_status() return url, response.text except requests.RequestException: return url, None def count_word(html: str, word: str) -> int: if html is None: return None return html.lower().count(word.lower()) with ThreadPoolExecutor(max_workers=5) as executor: future_to_url = {executor.submit(load_url, url): url for url in urls} for future in as_completed(future_to_url): url = future_to_url[future] try: url, html = future.result() count = count_word(html, word) if count is not None: print(f\\"{url}: {count}\\") else: print(f\\"{url}: Error\\") except Exception: print(f\\"{url}: Error\\")"},{"question":"# Custom ZIP Importer You are required to create a custom ZIP importer class in Python that allows importing modules directly from ZIP files. Implement the `CustomZipImporter` class which should have the following functionalities: 1. **Initialization** - The constructor should accept the path to a ZIP file or a path within the ZIP file. - It should raise a `ZipImportError` if the provided path is invalid. 2. **Methods to implement**: - `find_spec(self, fullname, target=None)`: This method should find and return the specification of the module with the provided fully qualified name (`fullname`). If the module is not found, it should return `None`. - `get_code(self, fullname)`: This method should return the code object for the specified module. - `get_data(self, pathname)`: This method should return the data associated with the specified `pathname`. - `exec_module(self, module)`: This method should execute the module in its own namespace given the module object. - `invalidate_caches(self)`: This method should clear out the internal cache of information about files found within the ZIP archive. 3. **Testing your implementation**: - Write test cases to demonstrate the full functionality of your `CustomZipImporter` class. - Create a sample ZIP file containing a few Python modules and try importing them using your custom importer. # Constraints - The `find_spec` and `exec_module` methods should follow the semantics as described in the documentation. - Handle both `.py` and `.pyc` files. - Ensure proper exception handling, especially with `ZipImportError` and `OSError`. # Expected Input and Output ```python # Example usage: import sys zip_file_path = \'path/to/your/test.zip\' full_module_name = \'your_module_name\' try: custom_importer = CustomZipImporter(zip_file_path) sys.meta_path.insert(0, custom_importer) module_spec = custom_importer.find_spec(full_module_name) if module_spec is not None: custom_importer.exec_module(module_spec) imported_module = sys.modules[full_module_name] print(imported_module) else: print(\\"Module not found\\") except ZipImportError as e: print(\\"Error importing from zip file:\\", e) except Exception as e: print(\\"An unexpected error occurred:\\", e) ``` You must implement the `CustomZipImporter` class and demonstrate importing a Python module from a ZIP file using it.","solution":"import zipfile import importlib.abc import importlib.util import sys import types from importlib.machinery import ModuleSpec class ZipImportError(Exception): pass class CustomZipImporter(importlib.abc.MetaPathFinder, importlib.abc.Loader): def __init__(self, zip_path): self.zip_path = zip_path try: self.zip_file = zipfile.ZipFile(zip_path) except Exception as e: raise ZipImportError(f\\"Could not open ZIP file: {str(e)}\\") def find_spec(self, fullname, target=None): module_path = fullname.replace(\'.\', \'/\') + \'.py\' if module_path in self.zip_file.namelist(): return ModuleSpec(fullname, self) return None def get_code(self, fullname): source = self.get_source(fullname) if source is None: return None return compile(source, fullname, \'exec\') def get_source(self, fullname): module_path = fullname.replace(\'.\', \'/\') + \'.py\' try: with self.zip_file.open(module_path) as file: return file.read().decode(\'utf-8\') except KeyError: return None def get_data(self, pathname): try: with self.zip_file.open(pathname) as file: return file.read() except KeyError as e: raise OSError(f\\"Could not find file {pathname} in the ZIP archive: {str(e)}\\") def exec_module(self, module): code = self.get_code(module.__name__) exec(code, module.__dict__) def invalidate_caches(self): pass"},{"question":"You are required to write a function that processes a list of MIME types and filenames and returns a list of command lines for viewing these files using the `mailcap` module in Python. Each command line should be appropriate to the MIME type of the corresponding file. Function Signature ```python def get_view_commands(mime_filenames: list) -> list: Returns a list of command lines to view files of specific MIME types. Parameters: mime_filenames (list): A list of tuples where each tuple contains a MIME type (str) and a filename (str). Returns: list: A list of command lines (str) based on the MIME types and filenames provided. Example: >>> get_view_commands([(\'video/mpeg\', \'video1.mpeg\'), (\'text/html\', \'index.html\')]) [\'xmpeg video1.mpeg\', \'lynx index.html\'] # Example output based on hypothetical mailcap entries ``` Input - `mime_filenames`: A list of tuples, where each tuple contains: - A string representing the MIME type (e.g., \'video/mpeg\'). - A string representing the filename (e.g., \'video1.mpeg\'). Output - A list of strings representing the command lines that can be used to view the files. Each command line corresponds to the appropriate MIME type of the filename provided. If no matching MIME type is found, the output should include \\"No command found\\" in place of that command line. Constraints - Assume the list `mime_filenames` is non-empty and does not contain any invalid MIME types or filenames. - Your implementation should utilize the `mailcap.getcaps` and `mailcap.findmatch` functions as described in the provided documentation. Example ```python mime_filenames = [(\'video/mpeg\', \'video1.mpeg\'), (\'text/html\', \'index.html\')] result = get_view_commands(mime_filenames) print(result) # Expected example output: [\'xmpeg video1.mpeg\', \'lynx index.html\'] ``` Notes 1. Handle the case where the `findmatch` function returns `(None, None)` by appending `\\"No command found\\"` to the result list for that file. 2. Use the `getcaps` function to retrieve the dictionary of mailcap entries.","solution":"import mailcap def get_view_commands(mime_filenames): Returns a list of command lines to view files of specific MIME types. Parameters: mime_filenames (list): A list of tuples where each tuple contains a MIME type (str) and a filename (str). Returns: list: A list of command lines (str) based on the MIME types and filenames provided. commands = [] caps = mailcap.getcaps() for mime_type, filename in mime_filenames: command, _ = mailcap.findmatch(caps, mime_type, filename=filename) if command: commands.append(command) else: commands.append(\\"No command found\\") return commands"},{"question":"**Objective**: Implement a custom class hierarchy in Python demonstrating the usage of classes, inheritance, and iterators. **Problem Statement**: You are required to implement a class hierarchy to represent a collection of books in a library. The collection should allow iterating through the books based on certain criteria. 1. **Define a base class `Book`** with the following characteristics: - Attributes: - `title`: A string representing the title of the book. - `author`: A string representing the author of the book. - `year`: An integer representing the publication year of the book. - Method: - `__repr__()`: Returns a string representation of the book in the format `\\"{title} by {author} ({year})\\"`. 2. **Define a derived class `EBook`** that inherits from `Book`, with an additional attribute: - `file_size`: An integer representing the file size of the eBook in MB. 3. **Define a class `Library`** to manage a collection of books: - Attribute: - `books`: A list that stores instances of `Book` and `EBook`. - Methods: - `add_book(book)`: Adds an instance of `Book` or `EBook` to the `books` list. - `__iter__()`: Returns an iterator that yields books in the collection in the order they were added. - `__next__()`: Returns the next book in the collection. **Constraints**: - The `Library` class must allow iteration over the books using a `for` loop. - The iteration should stop when all books have been iterated over. - Implement a mechanism to reset the iteration order when the end is reached. **Example Usage**: ```python # Define some books book1 = Book(\\"1984\\", \\"George Orwell\\", 1949) ebook1 = EBook(\\"Digital Fortress\\", \\"Dan Brown\\", 1998, 2) # Create a library and add books to it library = Library() library.add_book(book1) library.add_book(ebook1) # Iterate through the books for book in library: print(book) # Expected Output: # 1984 by George Orwell (1949) # Digital Fortress by Dan Brown (1998) ``` **Submission Guidelines**: - Your code should define the classes `Book`, `EBook`, and `Library` as specified. - Ensure that the iteration functionality in the `Library` class works correctly. - Include docstrings and comments to enhance code readability.","solution":"class Book: def __init__(self, title, author, year): self.title = title self.author = author self.year = year def __repr__(self): return f\\"{self.title} by {self.author} ({self.year})\\" class EBook(Book): def __init__(self, title, author, year, file_size): super().__init__(title, author, year) self.file_size = file_size def __repr__(self): return super().__repr__() + f\\" [{self.file_size}MB]\\" class Library: def __init__(self): self.books = [] self._index = 0 def add_book(self, book): self.books.append(book) def __iter__(self): self._index = 0 return self def __next__(self): if self._index >= len(self.books): raise StopIteration book = self.books[self._index] self._index += 1 return book"},{"question":"Seaborn Visualization Coding Challenge # Objective You are tasked with demonstrating your understanding of seaborn by visualizing and transforming a dataset. You will work with both long-form (tidy) and wide-form data representations and produce visualizations using seaborn\'s `relplot` function. # Problem Statement Given a dataset containing monthly temperature data for two cities over several years, you will: 1. Load and display the initial dataset. 2. Transform the dataset from its original format to both long-form and wide-form. 3. Create visualizations using seaborn for both formats. # Steps 1. **Data Loading and Display**: - Load the dataset `city_temperature.csv` which contains average temperatures for two cities, \\"CityA\\" and \\"CityB\\", for each month from 2000 to 2020. The dataset has the following columns: `year`, `month`, `CityA`, `CityB`. - Display the first few rows of the dataset. 2. **Data Transformation**: - **Wide-form Data**: Directly use the initially loaded data. - **Long-form Data**: Transform the dataset into a long-form where each row represents one monthly observation with columns `year`, `month`, `city`, and `temperature`. 3. **Visualization**: - Create a line plot for the temperatures of both cities over time using the **wide-form** dataset. - Create a line plot for the temperatures of both cities over time using the **long-form** dataset. # Input - A CSV file named `city_temperature.csv` with columns: `year` (int), `month` (str: Jan, Feb, ..., Dec), `CityA` (float), `CityB` (float). # Output - Print the first few rows of the initially loaded dataset. - Print the first few rows of the transformed long-form dataset. - Generate and display two line plots, one for wide-form and another for long-form data. # Constraints - You must use seaborn and pandas for this task. - Ensure the plots are clear and labeled appropriately. # Example Code Template ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Step1: Load the dataset # df = pd.read_csv(\'path_to_your_file/city_temperature.csv\') # print(df.head()) # Step 2: Data Transformation # Transform to wide-form (already in wide-form) # wide_df = df.copy() # Transform to long-form # long_df = df.melt(id_vars=[\'year\', \'month\'], value_vars=[\'CityA\', \'CityB\'], # var_name=\'city\', value_name=\'temperature\') # print(long_df.head()) # Step 3: Visualization # Wide-form visualization # sns.relplot(data=wide_df, kind=\'line\') # plt.xlabel(\'Year\') # plt.ylabel(\'Temperature\') # plt.title(\'Monthly Temperatures of Two Cities (Wide-form Data)\') # plt.show() # Long-form visualization # sns.relplot(data=long_df, x=\'year\', y=\'temperature\', hue=\'city\', kind=\'line\') # plt.xlabel(\'Year\') # plt.ylabel(\'Temperature\') # plt.title(\'Monthly Temperatures of Two Cities (Long-form Data)\') # plt.show() ``` # Notes - Remember to handle any missing values appropriately. - Ensure that the plots are well-labeled and include titles and axes labels to make the interpretation easy.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_and_display_initial_dataset(file_path): df = pd.read_csv(file_path) print(\\"Initial Dataset:\\") print(df.head()) return df def transform_to_long_form(df): long_df = df.melt(id_vars=[\'year\', \'month\'], value_vars=[\'CityA\', \'CityB\'], var_name=\'city\', value_name=\'temperature\') print(\\"Long-form Data:\\") print(long_df.head()) return long_df def plot_wide_form(df): sns.relplot(data=df, kind=\'line\', x=\'year\', y=\'CityA\', label=\'CityA\') sns.lineplot(data=df, x=\'year\', y=\'CityB\', label=\'CityB\') plt.xlabel(\'Year\') plt.ylabel(\'Temperature\') plt.title(\'Monthly Temperatures of Two Cities (Wide-form Data)\') plt.legend() plt.show() def plot_long_form(df): sns.relplot(data=df, x=\'year\', y=\'temperature\', hue=\'city\', kind=\'line\') plt.xlabel(\'Year\') plt.ylabel(\'Temperature\') plt.title(\'Monthly Temperatures of Two Cities (Long-form Data)\') plt.show()"},{"question":"Objective You are required to implement and apply a causal bias matrix for a simple attention mechanism. This task focuses on your ability to understand how causal dependencies affect sequence-to-sequence modeling. Instructions 1. Implement a function `apply_causal_bias(attention_scores: torch.Tensor, causal_matrix: torch.Tensor) -> torch.Tensor` that takes in: * `attention_scores` (a 2D PyTorch tensor of shape (N, N), where N is the number of tokens). * `causal_matrix` (a 2D PyTorch tensor of shape (N, N) representing the bias to be applied). The function should apply the causal bias by masking the attention_scores as indicated by the causal_matrix, returning the modified attention scores. 2. Implement a simple case where the causal matrix can be generated using either `causal_lower_right` or `causal_upper_left` functions (implement these functions if necessary as per your understanding from the documentation). Input - `attention_scores`: a 2D tensor of shape (N, N) - `causal_matrix`: a 2D tensor of shape (N, N) Output - The function should output a 2D tensor of shape (N, N) with the causally modified attention scores. Constraints - Ensure that the function works for square matrices of any reasonable size (1 â‰¤ N â‰¤ 1000). - Utilize PyTorch operations to manipulate tensors efficiently. Example ```python import torch # Example attention scores and causal matrix attention_scores = torch.tensor([ [0.2, 0.5, 0.3], [0.4, 0.1, 0.5], [0.3, 0.4, 0.3] ]) # Causal matrix example (lower triangular causal bias) causal_matrix = torch.tril(torch.ones(3, 3)) # You need to implement the function `apply_causal_bias` modified_attention_scores = apply_causal_bias(attention_scores, causal_matrix) print(modified_attention_scores) ``` The `causal_lower_right` and `causal_upper_left` functions should align with creating causal dependencies as implied by their names. Do not worry about their exact form in the `torch.nn.attention.bias` namespace; for this task, define them as you see fit based on the given context.","solution":"import torch def apply_causal_bias(attention_scores: torch.Tensor, causal_matrix: torch.Tensor) -> torch.Tensor: Applies causal bias to the attention scores. Args: attention_scores (torch.Tensor): A 2D tensor of shape (N, N). causal_matrix (torch.Tensor): A 2D tensor of shape (N, N) representing the causal bias. Returns: torch.Tensor: The modified attention scores with causal bias applied. return attention_scores * causal_matrix def causal_lower_right(size: int) -> torch.Tensor: Creates a causal matrix with lower triangular ones (lower right causal matrix). Args: size (int): The size of the NxN causal matrix. Returns: torch.Tensor: The lower triangular causal matrix. return torch.tril(torch.ones(size, size)) def causal_upper_left(size: int) -> torch.Tensor: Creates a causal matrix with upper triangular ones (upper left causal matrix). Args: size (int): The size of the NxN causal matrix. Returns: torch.Tensor: The upper triangular causal matrix. return torch.triu(torch.ones(size, size))"},{"question":"# Question: Implementing a Simple Neural Network in TorchScript **Objective**: Demonstrate your understanding of TorchScript by implementing a simple neural network module and scripting it using TorchScript. # Instructions: 1. **Implement a Simple Neural Network**: - Create a class `SimpleNN` that inherits from `torch.nn.Module`. - The class should have two linear layers: - The first linear layer should have an input size of 10 and an output size of 20. - The second linear layer should have an input size of 20 and an output size of 1. - Implement the `forward` method that defines the forward pass through the network: - The forward pass should apply the first linear layer, followed by a ReLU activation function, and then the second linear layer. 2. **Script the Module using TorchScript**: - Use `torch.jit.script` to script the `SimpleNN` model. 3. **Implement a Function to Use the Model**: - Create a function `use_model` that takes an instance of the scripted model and a tensor of appropriate size (10) as input, and returns the model\'s output. 4. **Type Annotations**: - Ensure all functions/methods have appropriate type annotations as required by TorchScript. # Expected Input and Output: - The `SimpleNN` class should take no input during initialization. - The `forward` method of the `SimpleNN` should take a tensor of size (batch_size, 10) and return a tensor of size (batch_size, 1). # Example Usage: ```python import torch # Define the Neural Network class class SimpleNN(torch.nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = torch.nn.Linear(10, 20) self.fc2 = torch.nn.Linear(20, 1) def forward(self, x: torch.Tensor) -> torch.Tensor: x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Script the Neural Network scripted_model = torch.jit.script(SimpleNN()) # Function to use the model def use_model(model: torch.jit.ScriptModule, input_tensor: torch.Tensor) -> torch.Tensor: return model(input_tensor) # Create an instance of SimpleNN and an input tensor model = SimpleNN() input_tensor = torch.rand((1, 10)) # Use the scripted model output = use_model(scripted_model, input_tensor) print(output) ``` # Constraints: - All methods/functions must have appropriate type annotations. - You must use `torch.jit.script` to script your model and ensure it is compatible with TorchScript. # Grading Criteria: - Correct implementation of the `SimpleNN` class with two linear layers and a ReLU activation. - Proper type annotations at all required places. - Successful scripting of the `SimpleNN` class. - Correct implementation of the `use_model` function. - The code should run without errors and produce the correct output.","solution":"import torch class SimpleNN(torch.nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = torch.nn.Linear(10, 20) self.fc2 = torch.nn.Linear(20, 1) def forward(self, x: torch.Tensor) -> torch.Tensor: x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Script the Neural Network scripted_model = torch.jit.script(SimpleNN()) # Function to use the model def use_model(model: torch.jit.ScriptModule, input_tensor: torch.Tensor) -> torch.Tensor: return model(input_tensor)"},{"question":"**Coding Assessment Question: Seaborn Advanced Plotting** # Objective Demonstrate your understanding of the `seaborn` package, particularly its object-oriented interface, by manipulating a dataset and creating a customized area plot. # Problem Statement Given a dataset containing information about countries\' health expenditures over several years, your task is to load this dataset, preprocess it, and generate a customized area plot showing the spending trends for different countries. You will use the `seaborn.objects` module for this purpose. # Requirements: 1. Load the dataset from a CSV file named \\"healthexp.csv\\". This dataset contains columns: `Country`, `Year`, and `Spending_USD`. 2. Preprocess the data to: - Pivot the data to have `Year` as the index and `Country` as the columns, with the values being `Spending_USD`. - Interpolate missing values. - Stack the data to a long-form format. - Reset the index and sort by `Country`. 3. Create an area plot using the `seaborn.objects` module, with the following specifications: - The x-axis should represent `Year`, and the y-axis should represent `Spending_USD`. - Facet the plot by `Country`, wrapping the facets into 3 columns. - Fill the area under the plotted lines with color mapped from the `Country`. - Ensure that each country\'s data is represented with different colors. 4. Customize the plot further by adding a line on top of each area\'s shaded region. # Input Format: - A CSV file named \\"healthexp.csv\\" formatted as described. # Expected Output: - A matplotlib figure displaying the requested area plot with the specified customizations. # Constraints: - You should use the seaborn\'s object-oriented interface (`seaborn.objects` module). # Performance: - Your solution should handle the dataset efficiently and produce the plot without excessive memory or computational overhead. # Example Usage: Here is an example of how you could structure your solution: ```python import seaborn.objects as so import pandas as pd # Load the dataset healthexp = pd.read_csv(\\"healthexp.csv\\") # Preprocess the data healthexp_pivot = healthexp.pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") healthexp_interpolated = healthexp_pivot.interpolate() healthexp_stack = healthexp_interpolated.stack().rename(\\"Spending_USD\\").reset_index().sort_values(\\"Country\\") # Create the area plot p = so.Plot(healthexp_stack, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(color=\\"Country\\")).add(so.Line()) # Show the plot p.show() ``` **Note**: Include comments in your code to explain each step of the process.","solution":"import seaborn.objects as so import pandas as pd def load_and_preprocess_data(file_path): Loads and preprocesses the health expenditure data. Args: - file_path (str): The path to the CSV file containing the data. Returns: - pd.DataFrame: The preprocessed data suitable for plotting. # Load the dataset healthexp = pd.read_csv(file_path) # Pivot the data to have Year as index and Country as columns healthexp_pivot = healthexp.pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") # Interpolate missing values healthexp_interpolated = healthexp_pivot.interpolate() # Stack the data into long-form format healthexp_stack = healthexp_interpolated.stack().rename(\\"Spending_USD\\").reset_index() # Sort by Country healthexp_stack = healthexp_stack.sort_values(\\"Country\\") return healthexp_stack def create_area_plot(data): Creates and displays an area plot showing health spending trends for different countries. Args: - data (pd.DataFrame): The preprocessed data to plot. # Create the area plot plot = so.Plot(data, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) plot.add(so.Area(color=\\"Country\\")).add(so.Line()) # Show the plot plot.show()"},{"question":"**Problem Statement:** You are required to implement an HTTP client that interacts with a provided mock HTTP server to perform a series of tasks. The mock server has the following endpoints: - `/data` (GET): Returns a JSON array of data elements. Example: `[\\"element1\\", \\"element2\\", ..., \\"elementN\\"]`. - `/data` (POST): Accepts a JSON array of elements and returns a success message. - `/status` (GET): Returns the status of the server. Example response: `{\\"status\\": \\"running\\"}`. Your task is to create a Python class `CustomHTTPClient` using the `http.client` package. This class should be able to: 1. Initialize the client with the server host and port. 2. Fetch the data from `/data` using a GET request. 3. Post new data to `/data` using a POST request. 4. Check the server status using the `/status` endpoint. **Class Signature:** ```python class CustomHTTPClient: def __init__(self, host: str, port: int): pass def get_data(self) -> list: Fetches the data from the \'/data\' endpoint. Returns: list: JSON array of data elements pass def post_data(self, data: list) -> str: Posts a list of data elements to the \'/data\' endpoint. Args: data(list): List of data elements to be posted. Returns: str: Success message from the server. pass def get_status(self) -> dict: Fetches the status of the server from the \'/status\' endpoint. Returns: dict: JSON object containing the server status. pass ``` **Example Usage:** ```python client = CustomHTTPClient(\\"localhost\\", 8080) # Fetch data data = client.get_data() print(data) # Should print a list of data elements # Post new data response = client.post_data([\\"new_element1\\", \\"new_element2\\"]) print(response) # Should print a success message # Fetch server status status = client.get_status() print(status) # Should print a dictionary with the server status ``` **Constraints:** - You must use the `http.client` package for handling HTTP requests and responses. - Proper error handling should be implemented to manage potential HTTP exceptions. - Assume the server is always available at the provided host and port for the duration of the task. **Performance Requirements:** - Efficiently manage HTTP connections and response handling. - The class methods should perform HTTP requests and handle responses without unnecessary delays.","solution":"import http.client import json class CustomHTTPClient: def __init__(self, host: str, port: int): self.host = host self.port = port def get_data(self) -> list: connection = http.client.HTTPConnection(self.host, self.port) connection.request(\\"GET\\", \\"/data\\") response = connection.getresponse() if response.status == 200: data = json.loads(response.read().decode()) connection.close() return data else: connection.close() raise Exception(f\\"Failed to get data: {response.status} {response.reason}\\") def post_data(self, data: list) -> str: connection = http.client.HTTPConnection(self.host, self.port) headers = {\'Content-Type\': \'application/json\'} json_data = json.dumps(data) connection.request(\\"POST\\", \\"/data\\", body=json_data, headers=headers) response = connection.getresponse() if response.status == 200: message = response.read().decode() connection.close() return message else: connection.close() raise Exception(f\\"Failed to post data: {response.status} {response.reason}\\") def get_status(self) -> dict: connection = http.client.HTTPConnection(self.host, self.port) connection.request(\\"GET\\", \\"/status\\") response = connection.getresponse() if response.status == 200: status = json.loads(response.read().decode()) connection.close() return status else: connection.close() raise Exception(f\\"Failed to get status: {response.status} {response.reason}\\")"},{"question":"# Advanced Coding Assessment Question: Data Classes and Their Features **Objective:** To assess the student\'s understanding and implementation skills of the `dataclasses` module in Python, including creation, handling of mutable and immutable data, post-initialization processing, and default factories. **Problem Statement:** You are required to implement a data class representing a library system that tracks books and their authors. The library system has the following requirements: 1. A `Book` data class with the following attributes: - `title`: A string representing the title of the book. - `author`: A string representing the author\'s name. - `year`: An integer representing the year of publication. - `checked_out`: A boolean indicating if the book is checked out. Default is `False`. 2. A `Library` data class which: - Contains a list of `Book` instances. - Provides methods to: - Add a new book to the library. - Check out a book given its title (set its `checked_out` attribute to `True`). - Return a book given its title (set its `checked_out` attribute to `False`). - List all books currently checked out. 3. The `Library` class should handle mutable default values correctly. **Requirements:** - Use the `dataclass` decorator for both classes. - Implement post-init processing to ensure no duplicate books (based on title) are added to the library. - Ensure the `Library` class can be safely copied and reused without issues related to mutable default values. **Constraints:** - The title of a book is unique within the library. - If a book is not found for a check out or return operation, raise a `ValueError` with an appropriate message. **Function Signatures:** ```python from dataclasses import dataclass, field from typing import List @dataclass class Book: title: str author: str year: int checked_out: bool = False @dataclass class Library: books: List[Book] = field(default_factory=list) def add_book(self, book: Book) -> None: pass def check_out_book(self, title: str) -> None: pass def return_book(self, title: str) -> None: pass def list_checked_out_books(self) -> List[Book]: pass ``` **Expected Input and Output:** 1. `add_book(book: Book)`: - Input: An instance of `Book`. - Output: None. 2. `check_out_book(title: str)`: - Input: Title of the book to check out. - Output: None. (Raises `ValueError` if book not found.) 3. `return_book(title: str)`: - Input: Title of the book to return. - Output: None. (Raises `ValueError` if book not found.) 4. `list_checked_out_books()`: - Input: None. - Output: List of `Book` instances that are currently checked out. **Example:** ```python lib = Library() book1 = Book(title=\\"Book A\\", author=\\"Author A\\", year=2020) book2 = Book(title=\\"Book B\\", author=\\"Author B\\", year=2021) lib.add_book(book1) lib.add_book(book2) lib.check_out_book(\\"Book A\\") print(lib.list_checked_out_books()) # Output: [Book(title=\'Book A\', author=\'Author A\', year=2020, checked_out=True)] lib.return_book(\\"Book A\\") print(lib.list_checked_out_books()) # Output: [] ```","solution":"from dataclasses import dataclass, field from typing import List @dataclass class Book: title: str author: str year: int checked_out: bool = False @dataclass class Library: books: List[Book] = field(default_factory=list) def add_book(self, book: Book) -> None: if any(b.title == book.title for b in self.books): raise ValueError(f\\"Book with title \'{book.title}\' already exists.\\") self.books.append(book) def check_out_book(self, title: str) -> None: for book in self.books: if book.title == title: if book.checked_out: raise ValueError(f\\"Book \'{title}\' is already checked out.\\") book.checked_out = True return raise ValueError(f\\"Book \'{title}\' not found in the library.\\") def return_book(self, title: str) -> None: for book in self.books: if book.title == title: if not book.checked_out: raise ValueError(f\\"Book \'{title}\' is not checked out.\\") book.checked_out = False return raise ValueError(f\\"Book \'{title}\' not found in the library.\\") def list_checked_out_books(self) -> List[Book]: return [book for book in self.books if book.checked_out]"},{"question":"# Python310 Module Handling In this assessment, you are required to demonstrate your understanding of the `PyModule` API by writing a function that programmatically creates a new Python module and sets various attributes for it. Task Implement a Python function `create_custom_module(module_name: str, initial_objects: dict) -> PyObject` that: 1. Creates a new module object with the given `module_name`. 2. Sets the module\'s `__author__` attribute to \\"Python 310 Test\\". 3. Adds the initial objects (e.g., constants, dictionaries) provided in the `initial_objects` dictionary to the module\'s namespace. 4. The `initial_objects` dictionary keys are taken as attribute names, and the values are added to the module as module-level constants or objects. Input - `module_name` (str): The name of the module to be created. - `initial_objects` (dict): A dictionary where keys are attribute names and values are the objects to be added to the module. Output - Returns the newly created module object. Constraints - The function should handle common edge cases, such as invalid module names or unsupported object types in `initial_objects`. - Ensure that the module object is correctly referenced and not causing memory leaks. Example ```python module_name = \\"custom_module\\" initial_objects = { \\"PI\\": 3.14159, \\"message\\": \\"Hello, World!\\", } module = create_custom_module(module_name, initial_objects) # The following assertions should hold true assert module.PI == 3.14159 assert module.message == \\"Hello, World!\\" assert module.__author__ == \\"Python 310 Test\\" ``` **Note:** Consider using the relevant functions and methods from the `PyModule` API to achieve the desired outcome.","solution":"import types def create_custom_module(module_name: str, initial_objects: dict) -> types.ModuleType: Creates a new Python module and sets various attributes including initial objects. :param module_name: The name of the module to be created. :param initial_objects: A dictionary where keys are attribute names and values are the objects to be added to the module. :return: The newly created module object. # Step 1: Create a new module object new_module = types.ModuleType(module_name) # Step 2: Set the module\'s __author__ attribute new_module.__author__ = \\"Python 310 Test\\" # Step 3: Add the initial objects to the module\'s namespace for attr_name, obj in initial_objects.items(): setattr(new_module, attr_name, obj) return new_module"},{"question":"**Problem Statement:** You are provided with several dictionaries from the `html.entities` module that map between HTML character references and Unicode characters. Your task is to implement a Python function that takes a string containing HTML entities and returns a string with the HTML entities replaced by their equivalent Unicode characters. # Function Signature ```python def replace_html_entities(text: str) -> str: pass ``` # Input - `text`: A string containing HTML entities, such as `\\"&lt;div&gt;Hello &amp; welcome!&lt;/div&gt;\\"`. # Output - Returns a string with all HTML entities replaced by their corresponding Unicode characters. # Constraints - You can assume the `text` will contain only valid HTML entities present in the `html.entities` module. - The length of the input string `text` will not exceed 10^6 characters. - Each HTML entity will either be terminated by a semicolon (e.g., `&lt;`) or be represented without a semicolon if the entity is recognized both with and without a semicolon. # Example ```python print(replace_html_entities(\\"&lt;div&gt;Hello &amp; welcome!&lt;/div&gt;\\")) # Output: \\"<div>Hello & welcome!</div>\\" print(replace_html_entities(\\"This is 50&deg;C\\")) # Output: \\"This is 50Â°C\\" ``` # Notes - You can use the dictionaries from the `html.entities` module: `html5`, `entitydefs`, `name2codepoint`, and `codepoint2name`, to assist in the conversion process. - Consider edge cases where the text may contain multiple different entities or where entities are not terminated by a semicolon.","solution":"import html def replace_html_entities(text: str) -> str: Replaces HTML entities in the given text with their equivalent Unicode characters. return html.unescape(text)"},{"question":"**Coding Assessment Question: Custom HTML Link Extractor** **Objective:** Write a custom HTML parser that extracts and prints all the hyperlinks (i.e., elements with the `<a>` tag) from a given HTML document, including the hyperlink text and the href attribute. **Question:** You need to implement a class `LinkExtractor` that subclasses `html.parser.HTMLParser`. Your class should override the necessary methods to extract all hyperlinks (both the href attribute and the text between the start and end tags) from the input HTML. **Specification:** 1. Implement the `LinkExtractor` class inheriting from `HTMLParser`. 2. Override the methods required to handle start tags, end tags, and data. 3. For each `<a>` tag, store the href attribute and the enclosed text. 4. Ignore `<a>` tags that do not have an href attribute. 5. Write a method called `get_links` that returns a list of tuples, each containing the hyperlink text and the href value `(text, href)`. **Input:** - A string containing the HTML content to be parsed. **Output:** - A list of tuples where each tuple contains the hyperlink text and the href attribute value. **Constraints:** - The input HTML will be less than 10000 characters in length. - The HTML content can contain nested elements. **Performance Requirements:** - Your implementation should efficiently handle the input within a reasonable time frame. **Example:** ```python from html.parser import HTMLParser class LinkExtractor(HTMLParser): def __init__(self): super().__init__() self.links = [] self.current_text = \'\' self.is_link = False def handle_starttag(self, tag, attrs): if tag == \'a\': for attr in attrs: if attr[0] == \'href\': self.current_href = attr[1] self.is_link = True def handle_endtag(self, tag): if tag == \'a\' and self.is_link: self.links.append((self.current_text.strip(), self.current_href)) self.current_text = \'\' self.is_link = False def handle_data(self, data): if self.is_link: self.current_text += data def get_links(self): return self.links # Test the implementation html_content = \'\'\' <html> <head><title>Sample Document</title></head> <body> <p>This is a <a href=\\"https://example.com/page1\\">link to page 1</a>.</p> <div>Another <a href=\\"https://example.com/page2\\">link</a> here.</div> </body> </html> \'\'\' parser = LinkExtractor() parser.feed(html_content) print(parser.get_links()) ``` **Expected Output:** ``` [ (\\"link to page 1\\", \\"https://example.com/page1\\"), (\\"link\\", \\"https://example.com/page2\\") ] ```","solution":"from html.parser import HTMLParser class LinkExtractor(HTMLParser): def __init__(self): super().__init__() self.links = [] self.current_text = \'\' self.current_href = None self.is_link = False def handle_starttag(self, tag, attrs): if tag == \'a\': for attr in attrs: if attr[0] == \'href\': self.current_href = attr[1] self.is_link = True def handle_endtag(self, tag): if tag == \'a\' and self.is_link: self.links.append((self.current_text.strip(), self.current_href)) self.current_text = \'\' self.current_href = None self.is_link = False def handle_data(self, data): if self.is_link: self.current_text += data def get_links(self): return self.links"},{"question":"**Goal**: Assess the student\'s ability to manipulate datasets using pandas, including handling missing data, merging datasets, and performing groupby operations. **Problem Statement**: You are given two CSV files representing datasets of sales transactions and customer information. The datasets have the following structures: `sales.csv` | transaction_id | customer_id | product_id | sale_amount | date | |----------------|-------------|------------|-------------|------------| | 1 | 101 | A1 | 100.0 | 2022-01-01 | | 2 | 102 | A2 | 200.0 | 2022-01-02 | | 3 | 103 | A1 | NaN | 2022-01-01 | | ... | ... | ... | ... | ... | `customers.csv` | customer_id | customer_name | customer_age | country | |-------------|---------------|--------------|------------| | 101 | John Doe | 34 | USA | | 102 | Jane Smith | 29 | UK | | 103 | Ana Johnson | NaN | USA | | ... | ... | ... | ... | **Tasks**: 1. **Load the datasets**: Write a function `load_data()` that reads both `sales.csv` and `customers.csv` into pandas DataFrames. 2. **Handle Missing Data**: Write a function `clean_sales_data(sales_df)` that: * Fills missing `sale_amount` values with the mean sale amount. * Fills missing `customer_age` in the customer dataset with the median age. 3. **Merge the Datasets**: Write a function `merge_datasets(sales_df, customers_df)` that merges the `sales` and `customers` DataFrames on the `customer_id` column. 4. **Group Transactions by Country**: Write a function `group_sales_by_country(merged_df)` that returns a DataFrame with the total `sale_amount` grouped by `country`. 5. **Filter Customers by Age**: Write a function `filter_customers_by_age(df, min_age)` that returns a DataFrame of customers who are older than `min_age`. **Constraints**: - The CSV files could have additional columns not specified. Your code should handle such cases gracefully. - You should not use any global variables; all data should be passed through function parameters. - Assume that the `sales.csv` and `customers.csv` files are available in the current directory. **Expected Input and Output Formats**: ```python def load_data(): # Reads sales.csv and customers.csv into pandas DataFrames. pass def clean_sales_data(sales_df, customers_df): # Fills missing values in sales_df and customers_df. pass def merge_datasets(sales_df, customers_df): # Merges sales_df and customers_df on customer_id. pass def group_sales_by_country(merged_df): # Returns a DataFrame with total sale_amount grouped by country. pass def filter_customers_by_age(df, min_age): # Returns a DataFrame of customers older than min_age. pass ``` **Sample Output**: For the given sample data, after processing, the output might look like: - After merging the datasets: ```plaintext transaction_id customer_id product_id sale_amount date customer_name customer_age country 0 1 101 A1 100.0 2022-01-01 John Doe 34.0 USA 1 2 102 A2 200.0 2022-01-02 Jane Smith 29.0 UK 2 3 103 A1 150.0 2022-01-01 Ana Johnson 31.5 USA ``` - Grouped sales by country: ```plaintext country total_sales 0 UK 200.0 1 USA 250.0 ``` - Filtered customers older than 30: ```plaintext customer_id customer_name customer_age country 0 101 John Doe 34.0 USA 2 103 Ana Johnson 31.5 USA ``` Your implementation should be efficient and handle large datasets gracefully.","solution":"import pandas as pd def load_data(): sales_df = pd.read_csv(\'sales.csv\') customers_df = pd.read_csv(\'customers.csv\') return sales_df, customers_df def clean_sales_data(sales_df, customers_df): sales_df[\'sale_amount\'].fillna(sales_df[\'sale_amount\'].mean(), inplace=True) customers_df[\'customer_age\'].fillna(customers_df[\'customer_age\'].median(), inplace=True) return sales_df, customers_df def merge_datasets(sales_df, customers_df): merged_df = pd.merge(sales_df, customers_df, on=\'customer_id\', how=\'left\') return merged_df def group_sales_by_country(merged_df): grouped_df = merged_df.groupby(\'country\')[\'sale_amount\'].sum().reset_index() grouped_df.rename(columns={\'sale_amount\': \'total_sales\'}, inplace=True) return grouped_df def filter_customers_by_age(df, min_age): filtered_df = df[df[\'customer_age\'] > min_age].reset_index(drop=True) return filtered_df"},{"question":"# Question: Configuring PyTorch MPS Environment Variables for Optimal Performance Objective: You are required to implement a function that configures PyTorch\'s MPS (Metal Performance Shaders) backend environment variables to optimize memory allocation and performance based on the given input parameters. Function Signature: ```python def configure_mps_environment(high_watermark: float, low_watermark: float, enable_fast_math: bool, enable_cpu_fallback: bool): Configures PyTorch\'s MPS environment variables based on the given parameters. Args: high_watermark (float): The high watermark ratio for MPS allocator. Constraints: 0.0 <= high_watermark low_watermark (float): The low watermark ratio for MPS allocator. Constraints: 0.0 <= low_watermark <= high_watermark enable_fast_math (bool): If True, fast math will be enabled for MPS metal kernels. enable_cpu_fallback (bool): If True, enable fallback operations to CPU when MPS does not support them. Returns: dict: A dictionary containing the set environment variables for verification. pass ``` Constraints and requirements: 1. The function should set the appropriate environment variables using `os.environ`. 2. Ensure the ratios for the high and low watermarks follow the constraints. 3. Fast math option and CPU fallback option should be configurable. 4. The function should return a dictionary for verification purposes where the keys are the environment variable names, and the values are the values set by the function. Example Usage: ```python # Example usage of the configure_mps_environment function config = configure_mps_environment(1.5, 1.2, True, False) print(config) ``` Expected output for the given example: ``` { \'PYTORCH_MPS_HIGH_WATERMARK_RATIO\': \'1.5\', \'PYTORCH_MPS_LOW_WATERMARK_RATIO\': \'1.2\', \'PYTORCH_MPS_FAST_MATH\': \'1\', \'PYTORCH_ENABLE_MPS_FALLBACK\': \'0\' } ``` # Explanation: 1. The `configure_mps_environment` function will be used to dynamically configure the environment variables needed for MPS performance and memory management in PyTorch. 2. The function will ensure that the high watermark ratio is not exceeded by the low watermark ratio. 3. It will provide a clear output in the form of a dictionary to verify the environment variables have been set correctly. The provided function signature and example usage should help students focus on correctly implementing the environment configuration logic while understanding the significance and impact of these variables on PyTorch\'s MPS performance.","solution":"import os def configure_mps_environment(high_watermark: float, low_watermark: float, enable_fast_math: bool, enable_cpu_fallback: bool): Configures PyTorch\'s MPS environment variables based on the given parameters. Args: high_watermark (float): The high watermark ratio for MPS allocator. Constraints: 0.0 <= high_watermark low_watermark (float): The low watermark ratio for MPS allocator. Constraints: 0.0 <= low_watermark <= high_watermark enable_fast_math (bool): If True, fast math will be enabled for MPS metal kernels. enable_cpu_fallback (bool): If True, enable fallback operations to CPU when MPS does not support them. Returns: dict: A dictionary containing the set environment variables for verification. assert 0.0 <= low_watermark <= high_watermark, \\"Low watermark must be less than or equal to high watermark\\" os.environ[\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\'] = str(high_watermark) os.environ[\'PYTORCH_MPS_LOW_WATERMARK_RATIO\'] = str(low_watermark) os.environ[\'PYTORCH_MPS_FAST_MATH\'] = \'1\' if enable_fast_math else \'0\' os.environ[\'PYTORCH_ENABLE_MPS_FALLBACK\'] = \'1\' if enable_cpu_fallback else \'0\' return { \'PYTORCH_MPS_HIGH_WATERMARK_RATIO\': str(high_watermark), \'PYTORCH_MPS_LOW_WATERMARK_RATIO\': str(low_watermark), \'PYTORCH_MPS_FAST_MATH\': \'1\' if enable_fast_math else \'0\', \'PYTORCH_ENABLE_MPS_FALLBACK\': \'1\' if enable_cpu_fallback else \'0\' }"},{"question":"# **Python310 Coding Assessment Question** # Objective: You are to implement a class `CustomDict`, which utilizes the Python310 dictionary API to provide similar functionalities as Python\'s built-in `dict` but with additional custom constraints and operations. # **CustomDict Class Requirements:** 1. **Initialization**: - The class should be initialized with an optional dictionary. ```python def __init__(self, initial_dict: dict = None) ``` - If no initial dictionary is provided, an empty dictionary should be created. 2. **Get Value by Key**: - Implement a method `get_value(self, key)` that retrieves the value for a given key. If the key doesn\'t exist, return `None`. ```python def get_value(self, key) ``` 3. **Set Value by Key**: - Implement a method `set_value(self, key, value)` that sets a value for a given key. Ensure the keys are always strings. ```python def set_value(self, key: str, value) ``` 4. **Delete Key**: - Implement a method `delete_key(self, key)` that deletes a specified key from the dictionary. If the key doesnâ€™t exist, do nothing. ```python def delete_key(self, key) ``` 5. **Get All Keys**: - Implement a method `keys(self)` that returns a list of all keys in the dictionary. ```python def keys(self) -> list ``` 6. **Merge Dictionaries**: - Implement a method `merge_dict(self, new_dict)` that takes another dictionary and merges it with the existing dictionary. Existing keys should be updated with new values. ```python def merge_dict(self, new_dict: dict) ``` 7. **Get Dictionary Size**: - Implement a method `size(self)` that returns the number of key-value pairs in the dictionary. ```python def size(self) -> int ``` # Task: Implement the `CustomDict` class using Python310 dictionary functions provided in the document, not the native Python dictionary methods. # Constraints: - Keys must be strings and hashable. - Handling errors gracefully and not mutating the dictionary during iteration of methods. - Ensure the implementation does not leak memory or have performance bottlenecks for large dictionaries. # Example Usage: ```python # Initializing with a dictionary custom_dict = CustomDict({\\"a\\": 1, \\"b\\": 2}) # Getting value for a key print(custom_dict.get_value(\\"a\\")) # Output: 1 # Setting a new key-value pair custom_dict.set_value(\\"c\\", 3) print(custom_dict.get_value(\\"c\\")) # Output: 3 # Deleting a key custom_dict.delete_key(\\"a\\") print(custom_dict.get_value(\\"a\\")) # Output: None # Merging with another dictionary custom_dict.merge_dict({\\"b\\": 4, \\"d\\": 5}) print(custom_dict.get_value(\\"b\\")) # Output: 4 print(custom_dict.size()) # Output: 3 # Getting all keys print(custom_dict.keys()) # Output: [\\"b\\", \\"c\\", \\"d\\"] ```","solution":"class CustomDict: def __init__(self, initial_dict: dict = None): self._dict = initial_dict if initial_dict is not None else {} def get_value(self, key): Retrieves the value for a given key. If the key doesn\'t exist, return None. return self._dict.get(key) def set_value(self, key: str, value): Sets a value for a given key. Ensures the keys are always strings. if not isinstance(key, str): raise TypeError(\\"Keys must be strings.\\") self._dict[key] = value def delete_key(self, key): Deletes a specified key from the dictionary. If the key doesnâ€™t exist, do nothing. if key in self._dict: del self._dict[key] def keys(self) -> list: Returns a list of all keys in the dictionary. return list(self._dict.keys()) def merge_dict(self, new_dict: dict): Takes another dictionary and merges it with the existing dictionary. Existing keys should be updated with new values. self._dict.update(new_dict) def size(self) -> int: Returns the number of key-value pairs in the dictionary. return len(self._dict)"},{"question":"# Platform Information Fetcher Implement a function called `fetch_platform_info()` that gathers and formats detailed platform information using the `platform` module from Python\'s standard library. The function should return a dictionary containing the following keys and corresponding values: 1. **architecture**: A tuple with the bit architecture and linkage format of the executable. 2. **machine**: The machine type. 3. **node**: The computer\'s network name. 4. **platform**: A string identifying the underlying platform. 5. **processor**: The processor name. 6. **python_build**: A tuple with the Python build number and date. 7. **python_compiler**: A string identifying the compiler used for Python. 8. **python_implementation**: A string identifying the Python implementation. 9. **python_version**: The Python version. 10. **release**: The system\'s release. 11. **system**: The operating system name. 12. **uname**: A dictionary with keys \\"system\\", \\"node\\", \\"release\\", \\"version\\", \\"machine\\", and \\"processor\\" based on the `uname` function. Function Signature ```python def fetch_platform_info() -> dict: # Your implementation here pass ``` Expected Output Format The function should return a dictionary with the above keys and their corresponding values. For example: ```python { \'architecture\': (\'64bit\', \'ELF\'), \'machine\': \'x86_64\', \'node\': \'my-computer\', \'platform\': \'Linux-5.4.0-42-generic-x86_64-with-glibc2.29\', \'processor\': \'x86_64\', \'python_build\': (\'default\', \'Sep 7 2020 09:18:44\'), \'python_compiler\': \'GCC 7.5.0\', \'python_implementation\': \'CPython\', \'python_version\': \'3.8.2\', \'release\': \'5.4.0-42-generic\', \'system\': \'Linux\', \'uname\': { \'system\': \'Linux\', \'node\': \'my-computer\', \'release\': \'5.4.0-42-generic\', \'version\': \'#46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020\', \'machine\': \'x86_64\', \'processor\': \'x86_64\' } } ``` Constraints and Notes 1. The function should handle any platform where Python is executable. 2. The function should use the simplest and most effective way to fetch the information using the `platform` module\'s functions. 3. Handle cases gracefully where information cannot be determined, ensure values are still representative. # Performance Requirements - The function should run efficiently without needing extensive computational resources. - Work seamlessly across different operating systems (Linux, macOS, Windows, etc.). Implementing this function will assess students\' understanding and ability to work with the `platform` module comprehensively, demonstrating their grasp of fetching and handling system identification data.","solution":"import platform def fetch_platform_info(): Gather and format detailed platform information. Returns: dict: A dictionary containing detailed platform information. uname_info = platform.uname() return { \'architecture\': platform.architecture(), \'machine\': platform.machine(), \'node\': platform.node(), \'platform\': platform.platform(), \'processor\': platform.processor(), \'python_build\': platform.python_build(), \'python_compiler\': platform.python_compiler(), \'python_implementation\': platform.python_implementation(), \'python_version\': platform.python_version(), \'release\': platform.release(), \'system\': platform.system(), \'uname\': { \'system\': uname_info.system, \'node\': uname_info.node, \'release\': uname_info.release, \'version\': uname_info.version, \'machine\': uname_info.machine, \'processor\': uname_info.processor } }"},{"question":"# Advanced Pandas Coding Assessment **Objective:** Implement a function that processes a given DataFrame and provides detailed memory usage insights, handling of missing values, and accurate boolean operations. **Problem Statement:** You are provided with a DataFrame and need to implement a function `process_dataframe(df: pd.DataFrame) -> dict` that performs the following tasks: 1. **Memory Usage Report**: Compute the memory usage of the DataFrame and each of its columns. Include the index memory usage and provide a more accurate report using `memory_usage=\'deep\'`. 2. **Handle Missing Values**: Identify columns with missing values. Replace missing values in numerical columns with the mean of the column and in non-numerical columns with the string `\'missing\'`. 3. **Boolean Operations**: Create a new boolean column `has_large_numbers` which is `True` if any of the original numerical columns have values greater than 100, and `False` otherwise. **Input:** - `df`: A pandas DataFrame with at least one numerical and one non-numerical column. **Output:** - A dictionary with the following keys: - `memory_usage`: A pandas Series object showing memory usage of each column including the index with deep memory inspection. - `processed_df`: A pandas DataFrame with missing values handled and the new boolean column added. **Constraints:** - You must use pandas methods effectively to solve the problem. - You are not allowed to use any libraries other than `pandas` and `numpy`. **Example:** ```python import pandas as pd import numpy as np data = { \'A\': [1, 2, np.nan, 4], \'B\': [None, \'x\', \'y\', \'z\'], \'C\': [10, 200, 30, 40], } df = pd.DataFrame(data) result = process_dataframe(df) print(result[\'memory_usage\']) print(result[\'processed_df\']) ``` Expected Output: ``` Index 128 A 32 B 32 C 32 dtype: int64 A B C has_large_numbers 0 1.0 missing 10 False 1 2.0 x 200 True 2 2.333333333 \'missing\' 30 False 3 4.0 z 40 False ``` **Explanation**: - The `memory_usage` should reflect the deep memory usage estimation of each column including the index. - The `processed_df` should have missing values in column `A` replaced with the mean and in column `B` replaced with \'missing\'. - A new column `has_large_numbers` should indicate if any number in column `C` exceeds 100. **Notes**: - Ensure proper handling of missing data. Numerical missing values should be filled with the column mean, and object type missing values with the string \'missing\'. - Use the method `memory_usage(\'deep\')` for accurate memory reporting.","solution":"import pandas as pd import numpy as np def process_dataframe(df: pd.DataFrame) -> dict: # Compute the memory usage of the DataFrame and each of its columns memory_usage = df.memory_usage(deep=True) # Identify columns with missing values and fill them accordingly for column in df: if df[column].isnull().any(): if df[column].dtype in [np.float64, np.int64]: df[column].fillna(df[column].mean(), inplace=True) else: df[column].fillna(\'missing\', inplace=True) # Create a new boolean column `has_large_numbers` has_large_numbers = (df.select_dtypes(include=[np.number]) > 100).any(axis=1) df[\'has_large_numbers\'] = has_large_numbers return { \'memory_usage\': memory_usage, \'processed_df\': df }"},{"question":"# Question: Exception Details Formatter You are required to write a function in Python that captures the current exceptionâ€™s traceback and formats it in two distinct ways. The first format should mimic the default traceback printing of the Python interpreter, and the second format should provide a more detailed view that includes local variables within each frame. Your task is to implement the following function: ```python def format_current_exception_with_locals(): This function should perform the following steps: 1. Capture the current exception and its traceback. 2. Print the traceback in the default format used by Python. 3. Print a more detailed traceback that includes local variable state for each frame. pass ``` # Detailed Requirements 1. **Capture the Exception**: - When an exception occurs, utilize `sys.exc_info()` to retrieve exception information. 2. **Print Default Traceback**: - Use the `traceback.print_exc()` function to print the exception and traceback in the default format. 3. **Print Detailed Traceback with Locals**: - Use the `traceback.TracebackException` class to format the traceback. - Ensure the detailed traceback includes local variables in each frame. Utilize `StackSummary.extract()` with the `capture_locals=True` parameter. - Print the formatted detailed traceback. # Example Usage Here\'s how the function should work: ```python def faulty_function(): x = 10 y = 0 return x / y # This will raise a ZeroDivisionError try: faulty_function() except: format_current_exception_with_locals() ``` The output should contain: 1. The default exception traceback. 2. The detailed traceback that includes the values of `x` and `y` in each stack frame. # Constraints and Additional Instructions - You should use the `traceback` module functions and classes as documented. - The function should handle any exception that it captures. - Ensure your function prints the formatted output directly to `sys.stderr`. # Performance Considerations - Although exceptions and tracebacks handling might not be performance-critical, make sure your implementation does not introduce unnecessary overhead.","solution":"import sys import traceback def format_current_exception_with_locals(): This function captures the current exception and its traceback, prints the traceback in the default format used by Python, and prints a more detailed traceback that includes local variable state for each frame. exc_type, exc_value, exc_tb = sys.exc_info() # Print the default traceback print(\\"Default traceback:\\", file=sys.stderr) traceback.print_exc() # Print the detailed traceback with local variables print(\\"nDetailed traceback with local variables:\\", file=sys.stderr) formatted_details = traceback.TracebackException( exc_type, exc_value, exc_tb, limit=None, capture_locals=True ) for line in formatted_details.format(): print(line, file=sys.stderr)"},{"question":"**Broadcast Compatibility Checker and Pointwise Operation** **Objective:** Implement a function to determine if two given tensors are broadcastable according to PyTorch\'s broadcasting rules. If they are broadcastable, perform a specified element-wise operation and return the result. Otherwise, raise an appropriate error. **Function Signature:** ```python def broadcast_and_apply(tensor_a: torch.Tensor, tensor_b: torch.Tensor, operation: str) -> torch.Tensor: pass ``` **Input:** 1. `tensor_a` (torch.Tensor): The first input tensor. 2. `tensor_b` (torch.Tensor): The second input tensor. 3. `operation` (str): The element-wise operation to apply (\\"add\\", \\"subtract\\", \\"multiply\\", \\"divide\\"). **Output:** - Returns a torch.Tensor resulting from applying the specified operation to the broadcasted input tensors. - Raises a `ValueError` if the tensors are not broadcastable with an appropriate error message. **Constraints:** - The operation must be one of: \\"add\\", \\"subtract\\", \\"multiply\\", or \\"divide\\". - The tensors must follow the broadcasting rules as described in the provided documentation. **Example:** ```python import torch # Example 1 tensor_a = torch.ones(5, 1, 4, 1) tensor_b = torch.ones(3, 1, 1) operation = \\"add\\" result = broadcast_and_apply(tensor_a, tensor_b, operation) print(result.size()) # Output: torch.Size([5, 3, 4, 1]) # Example 2 tensor_a = torch.ones(5, 2, 4, 1) tensor_b = torch.ones(3, 1, 1) operation = \\"multiply\\" try: result = broadcast_and_apply(tensor_a, tensor_b, operation) except ValueError as e: print(e) # Output: Tensors are not broadcastable: mismatch in dimension 1. ``` **Notes:** - The function should handle any exceptions related to invalid operations or dimension mismatches gracefully, providing clear error messages. - The solution should not modify the input tensors in place; it should return a new tensor as the result. **Evaluation Criteria:** - Correctness of the implementation. - Proper handling of broadcasting rules. - Clarity and conciseness of error messages. - Adherence to best practices in PyTorch.","solution":"import torch def broadcast_and_apply(tensor_a: torch.Tensor, tensor_b: torch.Tensor, operation: str) -> torch.Tensor: Determines if tensor_a and tensor_b are broadcastable and applies the specified operation. Args: tensor_a (torch.Tensor): The first input tensor. tensor_b (torch.Tensor): The second input tensor. operation (str): The element-wise operation to apply (\\"add\\", \\"subtract\\", \\"multiply\\", \\"divide\\"). Returns: torch.Tensor: A tensor resulting from applying the specified operation. Raises: ValueError: If the tensors are not broadcastable or if the operation is invalid. # Ensure the tensors can be broadcasted try: broadcasted_a = torch.broadcast_tensors(tensor_a, tensor_b) except RuntimeError as e: raise ValueError(f\'Tensors are not broadcastable: {e}\') # Define the operation if operation == \\"add\\": result = tensor_a + tensor_b elif operation == \\"subtract\\": result = tensor_a - tensor_b elif operation == \\"multiply\\": result = tensor_a * tensor_b elif operation == \\"divide\\": result = tensor_a / tensor_b else: raise ValueError(f\'Invalid operation: {operation}. Must be \\"add\\", \\"subtract\\", \\"multiply\\", or \\"divide\\".\') return result"},{"question":"**Seaborn Styling and Plotting Assessment** You are required to create a Python script using the seaborn library to generate and customize different plots. The goal of this assessment is to evaluate your understanding of seaborn\'s styling and plotting capabilities. Follow the instructions carefully to complete the task. # Instructions 1. **Default Styling**: - Set the default seaborn style to `\\"whitegrid\\"`. - Create a bar plot for data with categories `[\'X\', \'Y\', \'Z\']` and corresponding values `[4, 7, 1]`. - Save the plot as `bar_plot.png`. 2. **Custom Styling**: - Change the seaborn style to `\\"darkgrid\\"`. - Customize the grid color to `\\".5\\"` and the grid line style to `\\"--\\"`. - Create a line plot for data points `{\'A\': 2, \'B\': 6, \'C\': 8, \'D\': 4}`. - Save the plot as `line_plot.png`. 3. **Advanced Plot with Customization**: - Use the seaborn `set_style` function to create a custom style with the following properties: - Background color: (`\'#f0f0f0\'`) - Grid color: (`\'#b0b0b0\'`) - Grid line style: (`\'--\'`) - Axis titles with a specific font size (`14`) and color (`\'#333333\'`) - Create a seaborn scatter plot using the data `x=[5, 10, 15, 20]` and `y=[20, 35, 30, 25]` with points colored in `\'blue\'` and sized `\'100\'`. - Add a title to the plot: `\\"Custom Styled Scatter Plot\\"` - Save the plot as `scatter_plot.png`. # Expected Output - Three image files: `bar_plot.png`, `line_plot.png`, and `scatter_plot.png`, each reflecting the specified styles and customizations. # Constraints - You must use the `seaborn` library for all plot creations and styling. - Ensure all customizations are applied as specified. - Include comments in your code to note where each part of the instructions is implemented. # Performance Requirements - Ensure the script runs without errors and generates the plots in a reasonable time frame (under 30 seconds). - Make sure the visual aspects of the plots clearly reflect the custom styles specified. # Submission Submit a single Python file named `seaborn_assessment.py` containing your code to generate and save the requested plots. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt # Function to create and save bar plot with default styling def create_bar_plot(): sns.set_style(\\"whitegrid\\") # Set default style to whitegrid categories = [\'X\', \'Y\', \'Z\'] values = [4, 7, 1] plt.figure(figsize=(6, 4)) sns.barplot(x=categories, y=values, palette=\\"viridis\\") plt.title(\\"Bar Plot with Whitegrid Style\\") plt.savefig(\\"bar_plot.png\\") plt.close() # Function to create and save line plot with custom darkgrid styling def create_line_plot(): sns.set_style(\\"darkgrid\\") sns.set_context(rc={\\"grid.linewidth\\": 1.5, \\"grid.color\\": \\".5\\", \\"grid.linestyle\\": \\"--\\"}) data = {\'A\': 2, \'B\': 6, \'C\': 8, \'D\': 4} plt.figure(figsize=(6, 4)) sns.lineplot(x=list(data.keys()), y=list(data.values()), marker=\'o\', color=\'g\') plt.title(\\"Line Plot with Custom Darkgrid Style\\") plt.savefig(\\"line_plot.png\\") plt.close() # Function to create and save advanced scatter plot with custom styling def create_custom_scatter_plot(): sns.set_style(\\"white\\") # Reset to a base style sns.set_style({\'axes.facecolor\': \'#f0f0f0\', \'grid.color\': \'#b0b0b0\', \'grid.linestyle\': \'--\', \'axes.titlesize\': 14, \'axes.titlecolor\': \'#333333\'}) x = [5, 10, 15, 20] y = [20, 35, 30, 25] plt.figure(figsize=(8, 6)) sns.scatterplot(x=x, y=y, color=\'blue\', s=100) plt.title(\\"Custom Styled Scatter Plot\\") plt.savefig(\\"scatter_plot.png\\") plt.close() # Generate and save all plots create_bar_plot() create_line_plot() create_custom_scatter_plot()"},{"question":"Implement a function `train_and_evaluate_rbm` that uses a Bernoulli Restricted Boltzmann Machine (RBM) from scikit-learn to learn features from a given binary dataset and evaluates its performance using a simple linear classifier (e.g., Logistic Regression). The function should return the accuracy of the classifier when using raw features versus learned features from the RBM. Function Signature ```python def train_and_evaluate_rbm(data: np.ndarray, labels: np.ndarray, rbm_components: int, rbm_learning_rate: float, rbm_iter: int) -> Tuple[float, float]: Trains a Bernoulli RBM on the provided data and evaluates the performance using both raw and learned features. Parameters: - data (np.ndarray): Binary input data, where each row represents a sample and each column represents a feature. - labels (np.ndarray): Corresponding binary labels for the input data. - rbm_components (int): Number of hidden units in the Bernoulli RBM. - rbm_learning_rate (float): Learning rate for training the RBM. - rbm_iter (int): Number of iterations for training the RBM. Returns: - Raw feature accuracy (float): Classification accuracy using raw features. - RBM feature accuracy (float): Classification accuracy using RBM-transformed features. pass ``` Input - `data`: A binary `numpy` array of shape `(n_samples, n_features)`, where each element should be either 0 or 1. - `labels`: A binary `numpy` array of shape `(n_samples,)`, containing the class labels for each sample. - `rbm_components`: An integer indicating the number of hidden units in the RBM. - `rbm_learning_rate`: A float representing the learning rate for the RBM training. - `rbm_iter`: An integer specifying the number of training iterations for the RBM. Output - Two floats representing: - The accuracy of a Logistic Regression classifier using the raw input features. - The accuracy of a Logistic Regression classifier using the features learned by the RBM. Example ```python from sklearn.datasets import fetch_openml import numpy as np # Load a binary classification dataset (e.g., digits 0 and 1 from MNIST) mnist = fetch_openml(\'mnist_784\') data, labels = mnist.data, mnist.target data = data / 255.0 binary_indices = np.isin(labels, [\'0\', \'1\']) data, labels = data[binary_indices], labels[binary_indices].astype(np.int32) # Binarize the data data = (data > 0.5).astype(np.float32) # Train and evaluate RBM raw_acc, rbm_acc = train_and_evaluate_rbm(data, labels, rbm_components=100, rbm_learning_rate=0.01, rbm_iter=10) print(f\\"Raw Accuracy: {raw_acc}\\") print(f\\"RBM Feature Accuracy: {rbm_acc}\\") ``` Constraints - The input data should contain only binary values (0 or 1). - The implementation should use `sklearn.neural_network.BernoulliRBM` for feature learning. - The logistic regression classifier should be from `sklearn.linear_model.LogisticRegression`. Additional Information - You may use default values for hyperparameters of the Logistic Regression classifier. - Ensure reproducibility by setting random states where applicable. - Do not use additional libraries outside of those in the standard Python and scikit-learn libraries.","solution":"import numpy as np from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from typing import Tuple def train_and_evaluate_rbm(data: np.ndarray, labels: np.ndarray, rbm_components: int, rbm_learning_rate: float, rbm_iter: int) -> Tuple[float, float]: Trains a Bernoulli RBM on the provided data and evaluates the performance using both raw and learned features. Parameters: - data (np.ndarray): Binary input data, where each row represents a sample and each column represents a feature. - labels (np.ndarray): Corresponding binary labels for the input data. - rbm_components (int): Number of hidden units in the Bernoulli RBM. - rbm_learning_rate (float): Learning rate for training the RBM. - rbm_iter (int): Number of iterations for training the RBM. Returns: - Raw feature accuracy (float): Classification accuracy using raw features. - RBM feature accuracy (float): Classification accuracy using RBM-transformed features. # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42) # Logistic Regression on raw data logistic = LogisticRegression(solver=\'lbfgs\', max_iter=1000, random_state=42) logistic.fit(X_train, y_train) raw_accuracy = accuracy_score(y_test, logistic.predict(X_test)) # RBM + Logistic Regression pipeline rbm = BernoulliRBM(n_components=rbm_components, learning_rate=rbm_learning_rate, n_iter=rbm_iter, random_state=42) logistic_rbm = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) logistic_rbm.fit(X_train, y_train) rbm_accuracy = accuracy_score(y_test, logistic_rbm.predict(X_test)) return raw_accuracy, rbm_accuracy"},{"question":"**Objective**: Demonstrate your ability to use the `itertools` module to create complex iterators and process sequences efficiently. **Problem Statement**: You are given two lists of integers, `list1` and `list2`, of potentially different lengths. Write a Python function `generate_combinations` that takes these two lists as inputs and returns all possible combinations of 3 elements where one element comes from `list1` and two elements come from `list2`. The combinations should be unique and should not contain repeated elements. # Function Signature ```python import itertools def generate_combinations(list1: List[int], list2: List[int]) -> List[Tuple[int, int, int]]: # Your code here ``` # Input - `list1`: A list of integers (1 â‰¤ length â‰¤ 100). - `list2`: A list of integers (2 â‰¤ length â‰¤ 100). # Output - A list of tuples, each containing 3 integers, representing all unique combinations of one element from `list1` and two elements from `list2`. # Constraints - Each combination should include exactly one integer from `list1` and two integers from `list2`. - The order of elements within the tuples does not matter, but the tuples should be unique. - The returned list of tuples should be sorted in ascending order based on the first element, and if there are ties, then by the second element, and so on. # Example ```python # Example 1 list1 = [1, 2] list2 = [3, 4, 5] generate_combinations(list1, list2) # Output: [(1, 3, 4), (1, 3, 5), (1, 4, 5), (2, 3, 4), (2, 3, 5), (2, 4, 5)] # Example 2 list1 = [1] list2 = [2, 3, 4] generate_combinations(list1, list2) # Output: [(1, 2, 3), (1, 2, 4), (1, 3, 4)] ``` # Additional Notes - You may use the `itertools` library functions such as `combinations` and `product` to solve this problem. - Ensure that your solution handles the edge cases mentioned in the constraints. - Pay attention to performance and ensure that your solution is efficient for the given input bounds.","solution":"import itertools from typing import List, Tuple def generate_combinations(list1: List[int], list2: List[int]) -> List[Tuple[int, int, int]]: # Generate all possible combinations of 2 elements from list2 list2_combinations = list(itertools.combinations(list2, 2)) # Combine each element from list1 with each combination from list2_combinations result = [ (x, y, z) for x in list1 for (y, z) in list2_combinations ] # Sort results as specified result.sort() return result"},{"question":"# Advanced Python Coding Assessment Objective To assess students\' understanding of handling Python 2 and Python 3 compatibility issues, focusing on text and binary data distinctions, feature detection, and implementing methods for integer division. Problem Statement You are required to implement a function `process_data` that processes text and binary data from a given source, ensuring compatibility for both Python 2 and Python 3. Your task is to: 1. Read data from a file, where the file contains both text and binary sections. 2. Depending on the section type (\'text\' or \'binary\'), decode/encode the data appropriately. 3. Perform division operations that are compatible with both Python 2 and Python 3. 4. Use feature detection to check if certain functions are available and import them accordingly. 5. Utilize `__future__` imports to ensure code compatibility. Function Signature ```python def process_data(file_path): Processes data from the given file path. Handles both text and binary data sections. :param file_path: str, path to the input file :return: dict, containing processed \'text\' and \'binary\' data ``` Input - `file_path`: A string representing the path to the file you need to read. The file content will be structured in sections separated by the keyword \'SECTION_TYPE: text\' or \'SECTION_TYPE: binary\'. Output - Returns a dictionary with keys \'text\' and \'binary\', each containing the respective processed data. Example Given an input file `input.txt` with the following content: ``` SECTION_TYPE: text Hello, this is a text section. SECTION_TYPE: binary 01001100 01001001 01000010 01000101 01010010 01010100 01011001 ``` The function should return: ```python { \'text\': \'Hello, this is a text section.\', \'binary\': [76, 73, 66, 69, 82, 84, 89] } ``` Constraints - Ensure compatibility with both Python 2 (>=2.7) and Python 3. - Use `__future__` imports where necessary. - Use io.open() for file operations to ensure compatibility. - Perform integer division appropriately. Special Notes - Binary data in the file is represented as space-separated binary numbers. Convert these to their respective integer representations in the output. - Text data should be returned as a string. - Handle any potential exceptions that might occur during file processing gracefully. Requirements - Implement the function strictly according to the specified signature and requirements. - Demonstrate good coding practices, including writing clear and maintainable code. - Avoid using deprecated libraries or methods that are not compatible with the future versions of Python. This question assesses your understanding of handling text and binary data types, ensuring compatibility between Python 2 and 3, and writing robust, future-proof code. Hints 1. To read a file in a way that is compatible across Python versions, consider using the `io` module. 2. Use the `from __future__ import division` to handle the division operation compatibly. 3. For feature detection, use try/except blocks instead of checking the Python version directly.","solution":"from __future__ import division import io def process_data(file_path): Processes data from the given file path. Handles both text and binary data sections. :param file_path: str, path to the input file :return: dict, containing processed \'text\' and \'binary\' data processed_data = {\'text\': \'\', \'binary\': []} with io.open(file_path, mode=\'r\', encoding=\'utf-8\') as file: current_section = None for line in file: if line.startswith(\'SECTION_TYPE: text\'): current_section = \'text\' elif line.startswith(\'SECTION_TYPE: binary\'): current_section = \'binary\' else: if current_section == \'text\': processed_data[\'text\'] += line.strip() elif current_section == \'binary\': binary_values = line.strip().split() for binary_value in binary_values: processed_data[\'binary\'].append(int(binary_value, 2)) return processed_data"},{"question":"**Objective:** Create a Python script that automates the creation of a source distribution for a given Python package. The script should allow specifying custom files to be included, the archive format, and other options described in the documentation. This will demonstrate your understanding of packaging Python code and handling source distributions. **Problem Statement:** You are tasked with writing a Python script named `create_distribution.py`. This script should: 1. Accept two command-line arguments: - `--formats`: A comma-separated string of formats for the source distribution (e.g., `gztar,zip`). - `--manifest-content`: A string representing the content of the `MANIFEST.in` file, specifying additional files to include. 2. Generate a `setup.py` script for a simple Python package if it does not exist. The generated `setup.py` should minimally include the package name and version. 3. Create or update the `MANIFEST.in` file with the provided content. 4. Use the `sdist` command to create the source distribution archives, with the specified formats. 5. Print a success message after the archives are created. **Input and Output:** - **Input:** - `--formats`: A string specifying the formats for the source distribution, separated by commas. Example: `gztar,zip`. - `--manifest-content`: A string specifying the content for the `MANIFEST.in` file. Each line should be separated by `n`. Example: `include *.txtnrecursive-include src *.py`. - **Output:** - The script should generate the specified source distribution archives in the current directory. - Print a success message indicating the names of the created archives. **Constraints:** 1. If the `setup.py` file already exists, do not overwrite it. 2. If the `MANIFEST.in` file already exists, append the provided content to it. 3. Handle any exceptions that may occur during the generation of the distributions and print a relevant error message. **Example Usage:** ```shell python create_distribution.py --formats=gztar,zip --manifest-content=\\"include *.txtnrecursive-include src *.py\\" ``` **Expected Behavior:** 1. Create or update the `MANIFEST.in` file with the specified content. 2. Generate source distribution archives in both `tar.gz` and `zip` formats. 3. Print a success message indicating the names of the created archives. **Hints:** - Utilize the `subprocess` module to run the `python setup.py sdist` command. - Use the `argparse` module to handle command-line arguments. **Assessment Criteria:** - Correct handling of command-line arguments. - Proper generation and update of `setup.py` and `MANIFEST.in` files. - Successful creation of the specified source distribution archives. - Robust error handling and informative messages.","solution":"import os import argparse import subprocess def create_setup_py(): if not os.path.exists(\'setup.py\'): with open(\'setup.py\', \'w\') as f: setup_content = from setuptools import setup, find_packages setup( name=\'example_package\', version=\'0.1\', packages=find_packages(), ) f.write(setup_content) def update_manifest_in(manifest_content): with open(\'MANIFEST.in\', \'a\') as f: f.write(manifest_content + \'n\') def create_distribution(formats): formats = formats.split(\',\') cmd = [\'python\', \'setup.py\', \'sdist\', \'--formats=\' + \',\'.join(formats)] try: subprocess.check_call(cmd) print(f\\"Successfully created distributions: {formats}\\") except subprocess.CalledProcessError as e: print(f\\"Failed to create distributions: {e}\\") def main(): parser = argparse.ArgumentParser(description=\'Create source distribution archives for a Python package.\') parser.add_argument(\'--formats\', required=True, help=\'Comma-separated string of formats for the source distribution (e.g., gztar,zip).\') parser.add_argument(\'--manifest-content\', required=True, help=\'String representing the content of the MANIFEST.in file, specifying additional files to include.\') args = parser.parse_args() create_setup_py() update_manifest_in(args.manifest_content) create_distribution(args.formats) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Coding Assessment: Secure Authentication in Python **Objective**: Assess your understanding of secure input handling and user authentication using Python\'s `getpass` module. **Problem Statement**: You are tasked with implementing a simple command-line authentication system for a secure application. The system should: 1. Prompt the user to enter their username and password securely. 2. Check the entered credentials against a predefined set of valid usernames and passwords. 3. Grant access if the credentials are correct or display an error message otherwise. # Requirements: 1. Implement the function `authenticate_user()`, which: - Prompts the user for their username using the `input()` function. - Uses `getpass.getpass()` to securely prompt the user for their password. - Verifies the entered credentials against a predefined dictionary of username-password pairs. - Predefined valid credentials: ```python valid_credentials = { \'user1\': \'password123\', \'admin\': \'adminpass\', \'guest\': \'guestpassword\' } ``` - Returns a string `\'Access Granted\'` if the credentials are valid. - Returns a string `\'Access Denied\'` if the credentials are invalid. 2. Implement proper exception handling for input and any unforeseen errors during the authentication process. # Function Signature: ```python import getpass def authenticate_user(): # Your code here pass ``` # Constraints: - Do not print any auxiliary information besides user prompts and the final result. - You must use `getpass.getpass()` for password input to ensure security. - You cannot hard-code the prompts within `input()` or `getpass.getpass()`; they must use the function\'s default prompts. # Example: ```python valid_credentials = { \'user1\': \'password123\', \'admin\': \'adminpass\', \'guest\': \'guestpassword\' } # Example 1: # Input: # (Assume input is provided as below) # Username: user1 # Password: password123 # Output: # \'Access Granted\' # Example 2: # Input: # (Assume input is provided as below) # Username: admin # Password: wrongpass # Output: # \'Access Denied\' ``` Note: The example inputs and outputs are illustrative; the actual implementation will involve secure input handling as per the requirements.","solution":"import getpass valid_credentials = { \'user1\': \'password123\', \'admin\': \'adminpass\', \'guest\': \'guestpassword\' } def authenticate_user(): try: username = input(\\"Username: \\") password = getpass.getpass() if username in valid_credentials and valid_credentials[username] == password: return \\"Access Granted\\" else: return \\"Access Denied\\" except Exception as e: return f\\"An error occurred: {e}\\""},{"question":"# PyTorch Coding Assessment Question Objective The objective of this question is to assess your understanding of PyTorch tensors, their operations, and automatic differentiation. Problem Statement You are required to implement a function using PyTorch that performs the following tasks: 1. **Tensor Creation**: - Create a 3x3 tensor initialized with random values from a normal distribution. - Create a 3x3 identity tensor. 2. **Tensor Operations**: - Perform element-wise addition of the two tensors. - Perform matrix multiplication of the resulting tensor from the addition with itself. - Compute the sum of all elements in the final tensor. 3. **Automatic Differentiation**: - Enable gradient tracking for the initial random tensor created. - Compute the gradient of the sum of all elements in the final tensor with respect to the initial random tensor. 4. **Output**: - Return the final tensor from the matrix multiplication. - Return the computed gradient of the sum with respect to the initial random tensor. Function Signature ```python import torch def tensor_operations(): Performs specified tensor operations and computes gradients. Returns: torch.Tensor: The resulting tensor after matrix multiplication. torch.Tensor: The gradient of the sum of elements of the resulting tensor with respect to the initial random tensor. # Step 1: Tensor Creation random_tensor = torch.randn(3, 3, requires_grad=True) identity_tensor = torch.eye(3) # Step 2: Tensor Operations added_tensor = random_tensor + identity_tensor multiplied_tensor = torch.matmul(added_tensor, added_tensor) element_sum = multiplied_tensor.sum() # Step 3: Automatic Differentiation element_sum.backward() gradient = random_tensor.grad # Step 4: Output return multiplied_tensor, gradient ``` Expected Output The function should return the following: - The resulting tensor after performing the matrix multiplication. - The computed gradient of the sum with respect to the initial random tensor. Constraints - Do not use any library other than PyTorch. - Ensure that the gradient computation is enabled only when necessary. - The function should adhere to the given function signature. Performance Requirements - The operations should be efficient and perform appropriately for the given tensor sizes. Hint - Use `torch.randn` for creating a tensor with random values from a normal distribution. - Use `torch.eye` for creating an identity tensor. - Make use of the `requires_grad` parameter to enable gradient tracking.","solution":"import torch def tensor_operations(): Performs specified tensor operations and computes gradients. Returns: torch.Tensor: The resulting tensor after matrix multiplication. torch.Tensor: The gradient of the sum of elements of the resulting tensor with respect to the initial random tensor. # Step 1: Tensor Creation random_tensor = torch.randn(3, 3, requires_grad=True) identity_tensor = torch.eye(3) # Step 2: Tensor Operations added_tensor = random_tensor + identity_tensor multiplied_tensor = torch.matmul(added_tensor, added_tensor) element_sum = multiplied_tensor.sum() # Step 3: Automatic Differentiation element_sum.backward() gradient = random_tensor.grad # Step 4: Output return multiplied_tensor, gradient"},{"question":"# Question: Advanced Data Processing with Python Expressions You are tasked with processing a complex data structure using various elements of Python expressions including comprehensions and generator functions. The data structure is a nested dictionary with lists of numerical values. You must implement a function that will: 1. Flatten the dictionary into a list of tuples, each containing the dictionary keys leading to a list and the values from that list. 2. Apply a transformation to each value using a generator expression. 3. Aggregate the results using a specified aggregation function. Implement the function `process_data(data: dict, transform: callable, aggregate: callable) -> any` where: - `data` is a nested dictionary with lists of numerical values. - `transform` is a function that takes a numerical value and returns a transformed value. - `aggregate` is a function that takes a list of transformed values and returns a single aggregated value. # Input Format: - `data`: A nested dictionary with lists of numerical values, e.g., ```python { \'a\': { \'x\': [1, 2, 3], \'y\': [4, 5] }, \'b\': { \'z\': [6, 7], \'w\': [8, 9] } } ``` - `transform`: A function, e.g., ```python def square(n): return n * n ``` - `aggregate`: A function, e.g., ```python def sum_all(values): return sum(values) ``` # Output Format: - The function should return the aggregated result of all transformed values from the nested dictionary. # Function Signature: ```python def process_data(data: dict, transform: callable, aggregate: callable) -> any: ``` # Example: Given the following input: ```python data = { \'a\': { \'x\': [1, 2, 3], \'y\': [4, 5] }, \'b\': { \'z\': [6, 7], \'w\': [8, 9] } } transform = lambda x: x * x aggregate = sum ``` Calling `process_data(data, transform, aggregate)` should return `285` (since all values squared are `[1, 4, 9, 16, 25, 36, 49, 64, 81]` and their sum is `285`). # Constraints: - The keys of the dictionary will be strings. - The lists within the dictionary will contain only numerical values. - The functions `transform` and `aggregate` will be valid and provided. **Note:** Ensure the function efficiently handles the nested dictionary without unnecessary nested loops.","solution":"def process_data(data: dict, transform: callable, aggregate: callable) -> any: Processes a nested dictionary of numerical values by flattening, transforming each value, and aggregating the results. :param data: Nested dictionary with lists of numerical values. :param transform: A function to transform each numerical value. :param aggregate: A function to aggregate the transformed values. :return: An aggregated value of the transformed values. # Flatten the dictionary and collect all values flattened_values = (value for sub_dict in data.values() for lst in sub_dict.values() for value in lst) # Apply the transform function to each value transformed_values = (transform(val) for val in flattened_values) # Aggregate the transformed values result = aggregate(transformed_values) return result"},{"question":"As a programming instructor, you have been tasked with designing a challenging yet insightful question to assess students\' understanding of the `scikit-learn` package, focusing on Support Vector Machines (SVM). Objective: To implement an SVM-based classifier and evaluate its performance on a given dataset. Task: 1. **Load Data**: Load and preprocess a dataset (e.g., the Iris dataset available in `scikit-learn`). 2. **Model Implementation**: Implement an SVM classifier using `SVC` from `scikit-learn`. 3. **Parameter Tuning**: Perform grid search with cross-validation to find the optimal parameters for `C` and `gamma` when using the RBF kernel. 4. **Performance Evaluation**: Evaluate the performance of the trained classifier using metrics such as accuracy, precision, recall, and F1-score. Description: 1. **Data Loading and Preprocessing**: - Load the Iris dataset from `sklearn.datasets`. - Split the dataset into training and testing sets (80% training, 20% testing). - Standardize the features using `StandardScaler`. 2. **Model Implementation**: - Implement an SVM classifier using the `SVC` class with the RBF kernel. - Perform grid search with cross-validation (`GridSearchCV`) to find the best `C` and `gamma` values. Use a 5-fold cross-validation strategy. 3. **Parameter Tuning**: - Define a parameter grid for `C` (e.g., `[0.1, 1, 10, 100]`) and `gamma` (e.g., `[1, 0.1, 0.01, 0.001]`). - Use the parameter grid in `GridSearchCV` to find the optimal parameters. 4. **Performance Evaluation**: - Train an SVM classifier using the optimal parameters found from the grid search. - Evaluate the classifier on the test set and calculate accuracy, precision, recall, and F1-score. Constraints: - The input dataset is the Iris dataset from `sklearn.datasets`. - You must use the RBF kernel for the SVM classifier. - Use 5-fold cross-validation for parameter tuning. - Ensure that the `random_state` parameter is set to a fixed number to ensure reproducibility. Expected Functions: - `load_data()`: Load and split the Iris dataset. - `standardize_data(X_train, X_test)`: Standardize the feature set. - `tune_parameters(X_train, y_train)`: Perform grid search to find the optimal `C` and `gamma`. - `evaluate_model(clf, X_test, y_test)`: Evaluate the classifier on the test data. - `main()`: The main function to execute the above steps. Example Usage: ```python def main(): X_train, X_test, y_train, y_test = load_data() X_train, X_test = standardize_data(X_train, X_test) best_params = tune_parameters(X_train, y_train) clf = svm.SVC(kernel=\'rbf\', C=best_params[\'C\'], gamma=best_params[\'gamma\']) clf.fit(X_train, y_train) evaluate_model(clf, X_test, y_test) if __name__ == \\"__main__\\": main() ``` This question tests the student\'s ability to implement and tune an SVM classifier and understand the key aspects of SVMs and their application in a real-world dataset.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn import svm from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_data(): Loads the Iris dataset and splits it into training and testing sets. Returns: X_train, X_test, y_train, y_test (tuple of np.ndarray): Training and testing data and labels. data = load_iris() X, y = data.data, data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def standardize_data(X_train, X_test): Standardizes the features by removing the mean and scaling to unit variance. Args: X_train (np.ndarray): Training data. X_test (np.ndarray): Testing data. Returns: X_train_std, X_test_std (tuple of np.ndarray): Standardized training and testing data. scaler = StandardScaler() X_train_std = scaler.fit_transform(X_train) X_test_std = scaler.transform(X_test) return X_train_std, X_test_std def tune_parameters(X_train, y_train): Performs grid search with cross-validation to find the best hyperparameters for SVM. Args: X_train (np.ndarray): Training data. y_train (np.ndarray): Training labels. Returns: dict: Best parameters found by GridSearchCV. param_grid = { \'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001] } grid_search = GridSearchCV(svm.SVC(kernel=\'rbf\'), param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_train, y_train) return grid_search.best_params_ def evaluate_model(clf, X_test, y_test): Evaluates the classifier on the test data. Args: clf: Trained SVM classifier. X_test (np.ndarray): Testing data. y_test (np.ndarray): Testing labels. Returns: dict: A dictionary containing accuracy, precision, recall, and F1-score. y_pred = clf.predict(X_test) scores = { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred, average=\'macro\'), \'recall\': recall_score(y_test, y_pred, average=\'macro\'), \'f1_score\': f1_score(y_test, y_pred, average=\'macro\') } return scores def main(): X_train, X_test, y_train, y_test = load_data() X_train, X_test = standardize_data(X_train, X_test) best_params = tune_parameters(X_train, y_train) clf = svm.SVC(kernel=\'rbf\', C=best_params[\'C\'], gamma=best_params[\'gamma\']) clf.fit(X_train, y_train) scores = evaluate_model(clf, X_test, y_test) print(scores) if __name__ == \\"__main__\\": main()"},{"question":"**Problem Statement:** You are given a 1D signal represented as a PyTorch tensor. Your task is to apply different window functions from the `torch.signal.windows` module to this signal and analyze their effects on the signal by performing a Fast Fourier Transform (FFT). After applying each window function and performing the FFT, you should compare the results by plotting the magnitude spectrum. # Requirements: 1. Implement a function `apply_window_and_fft` with the following signature: ```python def apply_window_and_fft(signal: torch.Tensor, window_function: str) -> torch.Tensor: ``` - `signal`: a 1D PyTorch tensor representing the input signal. - `window_function`: a string representing the window function to be applied. It can be one of the following: \'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\'. The function should: - Apply the specified window function to the signal. - Perform an FFT on the windowed signal. - Return the magnitude spectrum of the transformed signal as a 1D PyTorch tensor. 2. Implement a function `compare_window_functions` with the following signature: ```python def compare_window_functions(signal: torch.Tensor, window_functions: List[str]) -> None: ``` - `signal`: a 1D PyTorch tensor representing the input signal. - `window_functions`: a list of strings, each representing a window function to be applied. The function should: - Use the `apply_window_and_fft` function to get the magnitude spectrum for each window function. - Plot the magnitude spectra of all specified window functions on the same plot for comparison. # Example: ```python import torch import matplotlib.pyplot as plt # Example signal signal = torch.sin(torch.linspace(0, 2 * 3.14159 * 5, 500)) # Window functions to compare window_functions = [\'hann\', \'hamming\', \'blackman\'] # Your implementation compare_window_functions(signal, window_functions) ``` # Constraints: - The input signal tensor will have a length of at least 100. - You may assume valid inputs for the window function strings. # Performance Requirements: - The implementation should handle signals up to length 10000 efficiently. - Use PyTorch operations to leverage the optimized performance. # Additional Information: - Documentation and examples for each window function can be found in the `torch.signal.windows` module documentation. **Note:** Ensure you have the necessary imports, especially for the plotting library (e.g., matplotlib).","solution":"import torch import torch.fft import matplotlib.pyplot as plt import numpy as np def apply_window_and_fft(signal: torch.Tensor, window_function: str) -> torch.Tensor: Applies a specified window function to the input signal and performs a Fast Fourier Transform (FFT). Args: signal (torch.Tensor): 1D input signal tensor. window_function (str): Name of the window function. Returns: torch.Tensor: Magnitude spectrum of the transformed signal. window_funcs = { \'bartlett\': torch.bartlett_window, \'blackman\': torch.blackman_window, \'cosine\': lambda x: torch.hann_window(x, periodic=True, dtype=torch.float32).mul(torch.cos(torch.linspace(0, np.pi, x))), \'hamming\': torch.hamming_window, \'hann\': torch.hann_window, \'kaiser\': lambda x: torch.kaiser_window(x, beta=14), \'nuttall\': lambda x: torch.nuttall_window(x, dtype=torch.float32) } if window_function not in window_funcs: raise ValueError(f\\"Unsupported window function: {window_function}\\") window = window_funcs[window_function](signal.size(0)) windowed_signal = signal * window fft_spectrum = torch.fft.fft(windowed_signal) magnitude_spectrum = torch.abs(fft_spectrum) return magnitude_spectrum def compare_window_functions(signal: torch.Tensor, window_functions: list) -> None: Compares the magnitude spectra of different window functions applied to the input signal. Args: signal (torch.Tensor): 1D input signal tensor. window_functions (list): List of window functions to compare. plt.figure(figsize=(10, 6)) for win_func in window_functions: magnitude_spectrum = apply_window_and_fft(signal, win_func) freqs = torch.fft.fftfreq(len(signal)) plt.plot(freqs[:len(signal)//2], magnitude_spectrum[:len(signal)//2].numpy(), label=win_func) plt.title(\'Magnitude Spectrum Comparison\') plt.xlabel(\'Frequency\') plt.ylabel(\'Magnitude\') plt.legend() plt.grid() plt.show()"},{"question":"Tuning Decision Thresholds with `TunedThresholdClassifierCV` Objective: Demonstrate your understanding of tuning decision thresholds in classification models by implementing a classification pipeline using scikit-learn. This exercise will require you to use the `TunedThresholdClassifierCV` to optimize a specific metric and evaluate its performance. Problem Statement: You are given a dataset generated for a binary classification problem. Your task is to train a classifier and optimize its decision threshold using `TunedThresholdClassifierCV` to maximize the recall score. Specifically, you need to: 1. Generate a binary classification dataset. 2. Train a base classifier. 3. Wrap the base classifier using `TunedThresholdClassifierCV` to find the optimal decision threshold that maximizes the recall score. 4. Evaluate the performance of both the base classifier and the tuned threshold classifier using recall, precision, and accuracy. Constraints: - Use the `make_classification` function to generate the dataset with 1000 samples and a class imbalance (90% of one class, 10% of the other). - The base classifier to use is `LogisticRegression`. - Use default parameters for `TunedThresholdClassifierCV` except for the scoring parameter, which should be set to maximize the recall score. - Use a train-test split of 75%-25% for evaluation. Input: - None (You will need to generate the data). Output: - Print the recall, precision, and accuracy scores for both the base and the tuned threshold classifiers. Code Template: ```python import numpy as np from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import recall_score, precision_score, accuracy_score from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import make_scorer # Step 1: Generate a binary classification dataset with class imbalance X, y = make_classification(n_samples=1000, weights=[0.1, 0.9], random_state=0) # Step 2: Split the dataset into training and testing sets (75%-25%) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) # Step 3: Train the base Logistic Regression classifier base_classifier = LogisticRegression(random_state=0) base_classifier.fit(X_train, y_train) # Step 4: Evaluate the base classifier y_pred_base = base_classifier.predict(X_test) print(\\"Base Classifier - Recall:\\", recall_score(y_test, y_pred_base)) print(\\"Base Classifier - Precision:\\", precision_score(y_test, y_pred_base)) print(\\"Base Classifier - Accuracy:\\", accuracy_score(y_test, y_pred_base)) # Step 5: Define a scorer to maximize recall score recall_scorer = make_scorer(recall_score) # Step 6: Wrap the base classifier using TunedThresholdClassifierCV and tune the threshold tuned_classifier = TunedThresholdClassifierCV(base_classifier, scoring=recall_scorer) tuned_classifier.fit(X_train, y_train) # Step 7: Evaluate the tuned threshold classifier y_pred_tuned = tuned_classifier.predict(X_test) print(\\"Tuned Classifier - Recall:\\", recall_score(y_test, y_pred_tuned)) print(\\"Tuned Classifier - Precision:\\", precision_score(y_test, y_pred_tuned)) print(\\"Tuned Classifier - Accuracy:\\", accuracy_score(y_test, y_pred_tuned)) ``` Notes: - Ensure that you can explain what each step in the code does and why it is necessary. - Pay particular attention to the step where you wrap the base classifier with `TunedThresholdClassifierCV` and how you set the scoring parameter.","solution":"import numpy as np from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import recall_score, precision_score, accuracy_score from sklearn.model_selection import GridSearchCV from sklearn.base import BaseEstimator, ClassifierMixin class TunedThresholdClassifierCV(BaseEstimator, ClassifierMixin): def __init__(self, base_classifier, scoring=None): self.base_classifier = base_classifier self.scoring = scoring self.best_threshold = None def fit(self, X, y): self.base_classifier.fit(X, y) y_probs = self.base_classifier.predict_proba(X)[:, 1] thresholds = np.linspace(0, 1, 101) best_score = -np.inf for threshold in thresholds: y_pred = (y_probs >= threshold).astype(int) score = self.scoring(y, y_pred) if score > best_score: best_score = score self.best_threshold = threshold def predict(self, X): y_probs = self.base_classifier.predict_proba(X)[:, 1] return (y_probs >= self.best_threshold).astype(int) def recall_scorer(y_true, y_pred): return recall_score(y_true, y_pred) # Step 1: Generate a binary classification dataset with class imbalance X, y = make_classification(n_samples=1000, weights=[0.1, 0.9], random_state=0) # Step 2: Split the dataset into training and testing sets (75%-25%) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) # Step 3: Train the base Logistic Regression classifier base_classifier = LogisticRegression(random_state=0) base_classifier.fit(X_train, y_train) # Step 4: Evaluate the base classifier y_pred_base = base_classifier.predict(X_test) print(\\"Base Classifier - Recall:\\", recall_score(y_test, y_pred_base)) print(\\"Base Classifier - Precision:\\", precision_score(y_test, y_pred_base)) print(\\"Base Classifier - Accuracy:\\", accuracy_score(y_test, y_pred_base)) # Step 5: Wrap the base classifier using TunedThresholdClassifierCV and tune the threshold tuned_classifier = TunedThresholdClassifierCV(base_classifier, scoring=recall_scorer) tuned_classifier.fit(X_train, y_train) # Step 6: Evaluate the tuned threshold classifier y_pred_tuned = tuned_classifier.predict(X_test) print(\\"Tuned Classifier - Recall:\\", recall_score(y_test, y_pred_tuned)) print(\\"Tuned Classifier - Precision:\\", precision_score(y_test, y_pred_tuned)) print(\\"Tuned Classifier - Accuracy:\\", accuracy_score(y_test, y_pred_tuned))"},{"question":"# PyTorch Coding Assessment Question Objective Implement a Custom Self-Attention Layer using the experimental `torch.nn.attention` module. Problem Description You are required to create a Custom Self-Attention Layer in PyTorch. This layer will process a batch of sequences and compute self-attention for each sequence in the batch. Task 1. Implement a class `CustomSelfAttention` that extends `torch.nn.Module`. 2. Your class should contain the following components: - `init` method to initialize the weights for query, key, and value matrices. - `forward` method to compute the self-attention scores and return the weighted sum of values. 3. The input to the `forward` method will be a 3D tensor of shape `(batch_size, sequence_length, embedding_dim)`. 4. The output should also be a 3D tensor of shape `(batch_size, sequence_length, embedding_dim)`. Input and Output Formats - Input: A 3D tensor of shape `(batch_size, sequence_length, embedding_dim)`. - Output: A 3D tensor of the same shape as the input. Constraints - You should use the experimental `torch.nn.attention` module for incorporating the attention mechanism. - Ensure matrix multiplications and any other tensor operations are efficient and leverage PyTorch\'s capabilities. Sample Code Template ```python import torch import torch.nn as nn import torch.nn.functional as F class CustomSelfAttention(nn.Module): def __init__(self, embedding_dim): super(CustomSelfAttention, self).__init__() # Initialize the weights for query, key, and value matrices self.query = nn.Linear(embedding_dim, embedding_dim) self.key = nn.Linear(embedding_dim, embedding_dim) self.value = nn.Linear(embedding_dim, embedding_dim) def forward(self, x): # Compute queries, keys, and values queries = self.query(x) keys = self.key(x) values = self.value(x) # Compute attention scores attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (keys.size(-1) ** 0.5) attention_weights = F.softmax(attention_scores, dim=-1) # Compute the weighted sum of values weighted_sum = torch.matmul(attention_weights, values) return weighted_sum # Example usage batch_size = 2 sequence_length = 5 embedding_dim = 10 input_tensor = torch.rand((batch_size, sequence_length, embedding_dim)) attention_layer = CustomSelfAttention(embedding_dim) output_tensor = attention_layer(input_tensor) print(output_tensor.shape) # Should print: torch.Size([2, 5, 10]) ``` Implement the details of the Custom Self-Attention Layer. Performance Requirements - Ensure your implementation is efficient and leverages the capabilities of PyTorch. - The forward pass should be able to handle large batch sizes and sequence lengths within a reasonable time frame. Notes - Remember to test your implementation thoroughly with different input shapes. - Since the attention APIs are experimental, ensure backward compatibility in your implementation if needed.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomSelfAttention(nn.Module): def __init__(self, embedding_dim): super(CustomSelfAttention, self).__init__() # Initialize the weights for query, key, and value matrices self.query = nn.Linear(embedding_dim, embedding_dim) self.key = nn.Linear(embedding_dim, embedding_dim) self.value = nn.Linear(embedding_dim, embedding_dim) def forward(self, x): # Compute queries, keys, and values queries = self.query(x) keys = self.key(x) values = self.value(x) # Compute attention scores attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (keys.size(-1) ** 0.5) attention_weights = F.softmax(attention_scores, dim=-1) # Compute the weighted sum of values weighted_sum = torch.matmul(attention_weights, values) return weighted_sum"},{"question":"**Objective:** The objective of this assessment is to test your understanding of file system operations, environment variable handling, and process management using the `os` module in Python. **Problem Statement:** You are required to develop a Python script that performs the following tasks: 1. **Directory Traversal and File Counting**: - Implement a function `count_files(dir_path)` that takes a directory path (`dir_path`) as input and returns a dictionary where keys are directory paths and values are the number of non-directory files in those directories. - The function should perform a recursive traversal of the entire directory tree rooted at `dir_path`. 2. **Environment Variables Manipulation**: - Implement a function `set_environment_variable(var_name, var_value)` that sets an environment variable `var_name` to `var_value`. - Implement a function `get_environment_variable(var_name)` that retrieves the value of the environment variable `var_name`. If the environment variable does not exist, the function should return `None`. 3. **Process Management and Execution**: - Implement a function `run_command(command)` that takes a string `command`, executes it in a subprocess, and returns the standard output and standard error as a tuple `(stdout, stderr)`. - Ensure the function handles any exceptions that might occur during the command execution and returns an appropriate error message. **Input and Output Formats:** 1. **Directory Traversal and File Counting**: - Input: `count_files(\'/path/to/directory\')` - Output: `{\'/path/to/directory\': 5, \'/path/to/directory/subdir1\': 3, ...}` 2. **Environment Variables Manipulation**: - Input: ```python set_environment_variable(\'TEST_VAR\', \'123\') get_environment_variable(\'TEST_VAR\') get_environment_variable(\'NON_EXISTENT_VAR\') ``` - Output: ```python # After calling set_environment_variable Environment variable TEST_VAR set to 123 # After calling get_environment_variable \'123\' None ``` 3. **Process Management and Execution**: - Input: `run_command(\'ls -l /path/to/directory\')` - Output: `(\'total 0n-rw-r--r-- 1 user user 0 Jan 1 00:00 file1n\', \'\')` **Constraints**: 1. Handle cases where the directory does not exist by raising an appropriate exception. 2. Ensure that setting environment variables is done in a way that subsequent child processes inherit these variables. 3. The function `run_command` should be platform-independent, i.e., it should work on both Unix and Windows systems. **Performance Requirements**: 1. The `count_files` function should handle large directory trees efficiently without excessive memory usage. 2. The `run_command` function should manage command execution within a reasonable time frame, avoiding hanging commands. **Submission**: Submit your Python script containing the following functions: - `count_files(dir_path)` - `set_environment_variable(var_name, var_value)` - `get_environment_variable(var_name)` - `run_command(command)`","solution":"import os import subprocess def count_files(dir_path): Returns a dictionary with the number of non-directory files in each directory, starting from the given directory path dir_path. file_count = {} for root, dirs, files in os.walk(dir_path): file_count[root] = len(files) return file_count def set_environment_variable(var_name, var_value): Sets the environment variable var_name to var_value. os.environ[var_name] = var_value def get_environment_variable(var_name): Retrieves the value of the environment variable var_name. Returns None if the environment variable does not exist. return os.environ.get(var_name) def run_command(command): Executes the given command in a subprocess and returns its output and error as a tuple. try: result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True) return (result.stdout, result.stderr) except subprocess.CalledProcessError as e: return (e.stdout, e.stderr) except Exception as e: return (str(e), \\"\\")"},{"question":"You are tasked with implementing a function that will configure a `RobotFileParser` instance to read a given `robots.txt` file and analyze its contents. The function will check several aspects of the `robots.txt` rules for different user agents and URLs. Function Signature ```python def analyze_robots_txt(url: str, user_agents: list, urls: list) -> dict: Analyzes the given robots.txt file for various user agents and URLs. Args: - url (str): The URL of the robots.txt file to be analyzed. - user_agents (list): A list of user agent strings to test. - urls (list): A list of URLs to check for access permissions. Returns: - dict: A dictionary with the following keys: - \'can_fetch\': A nested dictionary where `user_agents` are keys and their values are dictionaries with `urls` as keys and booleans as values. Indicating whether each user agent can fetch each URL. - \'crawl_delays\': A dictionary where `user_agents` are keys and their values are either the crawl delay (int) or None. - \'request_rates\': A dictionary where `user_agents` are keys and their values are either a tuple `(requests, seconds)` or None. - \'sitemaps\': A list of sitemap URLs (strings) found in the robots.txt file. pass ``` Input - `url` (string): The URL of the `robots.txt` file to be processed. - `user_agents` (list of strings): A list of user agent identifiers to verify crawling permissions and parameters. - `urls` (list of strings): A list of URLs to be checked against the rules in `robots.txt`. Output - A dictionary containing: - `\'can_fetch\'`: A nested dictionary where each key is a user agent, and its value is another dictionary mapping each URL to a boolean indicating if it can be fetched by that user agent. - `\'crawl_delays\'`: A dictionary mapping each user agent to its crawl delay (int) or `None` if not specified. - `\'request_rates\'`: A dictionary mapping each user agent to a tuple `(requests, seconds)` or `None` if not specified. - `\'sitemaps\'`: A list of all sitemap URLs found in the `robots.txt` file. Example ```python url = \\"http://example.com/robots.txt\\" user_agents = [\\"*\\", \\"mybot\\"] urls = [\\"http://example.com/index.html\\", \\"http://example.com/admin/\\"] result = analyze_robots_txt(url, user_agents, urls) # Expected Output Example: # { # \\"can_fetch\\": { # \\"*\\": { # \\"http://example.com/index.html\\": True, # \\"http://example.com/admin/\\": False # }, # \\"mybot\\": { # \\"http://example.com/index.html\\": True, # \\"http://example.com/admin/\\": False # } # }, # \\"crawl_delays\\": { # \\"*\\": 10, # \\"mybot\\": None # }, # \\"request_rates\\": { # \\"*\\": (1, 5), # \\"mybot\\": None # }, # \\"sitemaps\\": [ # \\"http://example.com/sitemap1.xml\\", # \\"http://example.com/sitemap2.xml\\" # ] # } ``` Constraints - The function should handle errors gracefully. If the `robots.txt` file cannot be read or parsed, return an empty dictionary. - Assume the URL for the `robots.txt` file is well-formed and accessible. Implement the function `analyze_robots_txt` that meets the above specifications and constraints.","solution":"from urllib.robotparser import RobotFileParser def analyze_robots_txt(url: str, user_agents: list, urls: list) -> dict: Analyzes the given robots.txt file for various user agents and URLs. Args: - url (str): The URL of the robots.txt file to be analyzed. - user_agents (list): A list of user agent strings to test. - urls (list): A list of URLs to check for access permissions. Returns: - dict: A dictionary with the following keys: - \'can_fetch\': A nested dictionary where `user_agents` are keys and their values are dictionaries with `urls` as keys and booleans as values. Indicating whether each user agent can fetch each URL. - \'crawl_delays\': A dictionary where `user_agents` are keys and their values are either the crawl delay (int) or None. - \'request_rates\': A dictionary where `user_agents` are keys and their values are either a tuple `(requests, seconds)` or None. - \'sitemaps\': A list of sitemap URLs (strings) found in the robots.txt file. result = { \\"can_fetch\\": {}, \\"crawl_delays\\": {}, \\"request_rates\\": {}, \\"sitemaps\\": [] } try: parser = RobotFileParser() parser.set_url(url) parser.read() except Exception as e: return {} # Fetch access permissions for URLs for user_agent in user_agents: result[\\"can_fetch\\"][user_agent] = {u: parser.can_fetch(user_agent, u) for u in urls} # Fetch crawl delay and request rate for user_agent in user_agents: crawl_delay = parser.crawl_delay(user_agent) result[\\"crawl_delays\\"][user_agent] = crawl_delay request_rate = parser.request_rate(user_agent) if request_rate: result[\\"request_rates\\"][user_agent] = (request_rate.requests, request_rate.seconds) else: result[\\"request_rates\\"][user_agent] = None # Fetch sitemaps result[\\"sitemaps\\"] = parser.site_maps() or [] return result"},{"question":"# Python Coding Assessment **Objective:** In this task, you will demonstrate your understanding of the `site` module in Python 3.10 by implementing a function that manages the inclusion of site-specific directory paths based on specific conditions. **Problem:** Write a function `configure_site_paths` that takes three arguments: - `base_path` (str): The base directory path that should be included in the `sys.path`. - `use_user_site` (bool): A flag indicating whether the user site directory should be considered. - `include_rlcompleter` (bool): A flag indicating whether the readline and rlcompleter configurations should be enabled for interactive sessions. The function should perform the following tasks: 1. Add the `base_path` to the `sys.path` if it exists. 2. If `use_user_site` is `True`, add the user site-packages directory (given by `site.getusersitepackages()`) to the `sys.path` if it exists. 3. If `include_rlcompleter` is `True`, configure the readline and rlcompleter modules to enable tab-completion and use `~/.python_history` as the history file when in an interactive session. 4. Return the updated `sys.path`. **Constraints:** - Do not use any external libraries other than the ones mentioned in the documentation. - Assume the function is called in an environment that supports the readline module when needed. - The function should be efficient and handle any errors gracefully. **Example Usage:** ```python def configure_site_paths(base_path: str, use_user_site: bool, include_rlcompleter: bool) -> list: import sys import os import site # Task 1: Add base_path to sys.path if it exists if os.path.exists(base_path) and base_path not in sys.path: sys.path.append(base_path) # Task 2: Add user site-packages directory if use_user_site is True if use_user_site: user_site_path = site.getusersitepackages() if os.path.exists(user_site_path) and user_site_path not in sys.path: sys.path.append(user_site_path) # Task 3: Configure readline and rlcompleter if include_rlcompleter is True if include_rlcompleter and (not hasattr(sys, \'ps1\') or hasattr(sys, \'ps2\')): import readline import rlcompleter readline.parse_and_bind(\'tab: complete\') history_file = os.path.expanduser(\'~/.python_history\') try: readline.read_history_file(history_file) except FileNotFoundError: pass import atexit atexit.register(readline.write_history_file, history_file) # Return the updated sys.path return sys.path # Example Test Case updated_sys_path = configure_site_paths(\'/usr/local/my_custom_path\', use_user_site=True, include_rlcompleter=True) print(updated_sys_path) ``` **Expected Output:** The function should print the updated `sys.path` which includes the provided base path, user site-packages directory if applicable, and configure rlcompleter for tab-completion if required.","solution":"def configure_site_paths(base_path: str, use_user_site: bool, include_rlcompleter: bool) -> list: import sys import os import site # Task 1: Add base_path to sys.path if it exists if os.path.exists(base_path) and base_path not in sys.path: sys.path.append(base_path) # Task 2: Add user site-packages directory if use_user_site is True if use_user_site: user_site_path = site.getusersitepackages() if os.path.exists(user_site_path) and user_site_path not in sys.path: sys.path.append(user_site_path) # Task 3: Configure readline and rlcompleter if include_rlcompleter is True if include_rlcompleter and hasattr(sys, \'ps1\'): import readline import rlcompleter readline.parse_and_bind(\'tab: complete\') history_file = os.path.expanduser(\'~/.python_history\') try: readline.read_history_file(history_file) except FileNotFoundError: pass import atexit atexit.register(readline.write_history_file, history_file) # Return the updated sys.path return sys.path"},{"question":"# Problem Description You are provided with a list of URLs that need to be fetched asynchronously. There is also a requirement to process the fetched data and save the results to a file. To handle this scenario, you need to implement an asynchronous function in Python using the asyncio module for fetching the URLs, processing the data concurrently, and writing the results to a specified file. Your goal is to implement a function `fetch_and_process(urls, process_func, output_file)` where: - `urls` (List[str]): A list of URLs to fetch data from. - `process_func` (Callable[[str, str], str]): A function that takes a URL and its fetched content as input and returns a processed string. - `output_file` (str): The file path where the processed results should be written. This function should: 1. Fetch the data from each URL concurrently using asyncio. 2. Process the fetched data using `process_func`. 3. Write the processed results to the specified `output_file`. # Constraints - Each URL fetch should have a timeout of 5 seconds. - Maximum of 10 concurrent fetch operations should be allowed. - The process_func is provided and guaranteed to be a coroutine. - Ensure that the function handles potential errors (like timeouts and network issues) gracefully, and continues fetching other URLs even if one fails. - Write each result to the output file as soon as it is processed. # Function Signature ```python import aiohttp import asyncio from typing import List, Callable async def fetch_and_process(urls: List[str], process_func: Callable[[str, str], str], output_file: str): # Your implementation here ``` # Example Usage ```python import asyncio async def sample_process_func(url, content): return f\\"Processed content from {url}n\\" urls = [\\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\"] asyncio.run(fetch_and_process(urls, sample_process_func, \\"output.txt\\")) ``` In this example, `fetch_and_process` should fetch the contents from the URLs, process them using `sample_process_func`, and save the results to `\\"output.txt\\"`. # Notes - Ensure proper use of asyncio concepts such as `async def`, `await`, `asyncio.create_task`, and `aiohttp.ClientSession`. - Handle exceptions like timeouts and network errors appropriately to ensure other URLs are still processed. - Write results to the file incrementally to avoid keeping all results in memory.","solution":"import aiohttp import asyncio from typing import List, Callable async def fetch(url: str) -> str: Fetches the content of the given URL. Returns the content as a string, or raises an exception if the fetch fails. async with aiohttp.ClientSession() as session: try: async with session.get(url, timeout=5) as response: response.raise_for_status() return await response.text() except Exception as e: return f\\"Error fetching {url}: {str(e)}\\" async def fetch_and_process(urls: List[str], process_func: Callable[[str, str], str], output_file: str): async def fetch_and_process_single(url: str): content = await fetch(url) return await process_func(url, content) # Limit the number of concurrent fetches semaphore = asyncio.Semaphore(10) async def sem_fetch_and_process_single(url: str): async with semaphore: return await fetch_and_process_single(url) tasks = [sem_fetch_and_process_single(url) for url in urls] with open(output_file, \'w\') as f: for result in asyncio.as_completed(tasks): processed_result = await result f.write(processed_result + \'n\')"},{"question":"**Custom Distance Metric and Kernel Matrix Calculation** # Objective: Implement a custom Manhattan distance metric and a custom Linear kernel function, then compute the pairwise distance and kernel matrices for given datasets and verify these calculations against scikit-learn\'s built-in functions. # Instructions: 1. Implement the Manhattan distance metric as a function `manhattan_distance(a, b)` that takes two 1D numpy arrays `a` and `b`, and returns the Manhattan distance between them. 2. Implement the Linear kernel function as a function `linear_kernel(a, b)` that takes two 1D numpy arrays `a` and `b`, and returns the Linear kernel between them. 3. Using these functions and numpy, write a function `pairwise_manhattan_distances(X)` that takes a 2D numpy array `X` (shape: n_samples x n_features), and returns the pairwise Manhattan distance matrix for the samples in `X`. 4. Using these functions and numpy, write a function `pairwise_linear_kernels(X)` that takes a 2D numpy array `X` (shape: n_samples x n_features), and returns the pairwise Linear kernel matrix for the samples in `X`. 5. Use scikit-learn\'s `pairwise_distances` and `pairwise_kernels` to compute the pairwise Manhattan distance matrix and the pairwise Linear kernel matrix for the dataset `X` (provided below) to verify your custom implementations. # Constraints: - Do not use scipy or any other external libraries for the custom implementations (numpy only). # Performance Requirements: - Your implementations should handle datasets with up to 1000 samples efficiently. # Input: - `X`: A 2D numpy array of shape (n_samples, n_features). # Output: - `pairwise_manhattan_distances(X)`: A 2D numpy array of shape (n_samples, n_samples) representing the pairwise Manhattan distance matrix. - `pairwise_linear_kernels(X)`: A 2D numpy array of shape (n_samples, n_samples) representing the pairwise Linear kernel matrix. # Example dataset: ```python import numpy as np X = np.array([[2, 3], [3, 5], [5, 8], [1, 0], [2, 1]]) ``` # Example usage: ```python custom_manhattan_distances = pairwise_manhattan_distances(X) custom_linear_kernels = pairwise_linear_kernels(X) # Verification using scikit-learn from sklearn.metrics import pairwise_distances from sklearn.metrics.pairwise import pairwise_kernels sklearn_manhattan_distances = pairwise_distances(X, metric=\'manhattan\') sklearn_linear_kernels = pairwise_kernels(X, metric=\'linear\') # The results should be the same print(np.allclose(custom_manhattan_distances, sklearn_manhattan_distances)) # Should print True print(np.allclose(custom_linear_kernels, sklearn_linear_kernels)) # Should print True ``` # Submission: Submit a single Python file or Jupyter notebook containing: 1. The implementation of the functions described above. 2. The verification code to compare custom results with scikit-learn results as shown in the example usage.","solution":"import numpy as np def manhattan_distance(a, b): Returns the Manhattan distance between two 1D numpy arrays a and b. return np.sum(np.abs(a - b)) def linear_kernel(a, b): Returns the linear kernel (dot product) between two 1D numpy arrays a and b. return np.dot(a, b) def pairwise_manhattan_distances(X): Returns the pairwise Manhattan distance matrix for the samples in 2D numpy array X. n_samples = X.shape[0] distances = np.zeros((n_samples, n_samples)) for i in range(n_samples): for j in range(n_samples): distances[i, j] = manhattan_distance(X[i], X[j]) return distances def pairwise_linear_kernels(X): Returns the pairwise linear kernel matrix for the samples in 2D numpy array X. n_samples = X.shape[0] kernels = np.zeros((n_samples, n_samples)) for i in range(n_samples): for j in range(n_samples): kernels[i, j] = linear_kernel(X[i], X[j]) return kernels"},{"question":"Objective: This question evaluates your understanding of the seaborn library, specifically its object-oriented interface and data visualization capabilities. Problem Statement: You are provided with the \'diamonds\' dataset from seaborn. Your task is to create and customize several plots to analyze the \'cut\' of the diamonds and their respective \'price\' distributions. Each step should build upon the previous one to form a comprehensive analysis. Requirements: 1. **Load the Diamonds Dataset:** - Write a function `load_diamonds()` that: - Loads the \'diamonds\' dataset using the seaborn library. - Returns the dataset. ```python import seaborn as sns import pandas as pd def load_diamonds() -> pd.DataFrame: # Your code here ``` 2. **Create a Log-Scaled Dot Plot:** - Write a function `log_scale_dot_plot(diamonds: pd.DataFrame)` that: - Takes the diamonds dataset as an input. - Creates a dot plot with \'cut\' on the x-axis and \'price\' on the y-axis. - Applies a logarithmic scale to the y-axis. - Returns the plot object. ```python import seaborn.objects as so def log_scale_dot_plot(diamonds: pd.DataFrame) -> so.Plot: # Your code here ``` 3. **Percentiles-Based Visualization:** - Extend the `log_scale_dot_plot` function to additionally: - Compute quartiles and min/max percentiles for the price. - Show the computed percentiles using the `so.Dot()` and `so.Perc()` methods. ```python def log_scale_dot_plot(diamonds: pd.DataFrame) -> so.Plot: # Your code here ``` 4. **Custom Percentiles Plot:** - Write a function `custom_percentiles_plot(diamonds: pd.DataFrame, percentiles: list)` that: - Takes the diamonds dataset and a list of custom percentiles as inputs. - Creates a dot plot similar to the previous with custom percentiles displayed. - Returns the plot object. ```python def custom_percentiles_plot(diamonds: pd.DataFrame, percentiles: list) -> so.Plot: # Your code here ``` Example: ```python diamonds = load_diamonds() plot = log_scale_dot_plot(diamonds) plot.show() custom_plot = custom_percentiles_plot(diamonds, [10, 25, 50, 75, 90]) custom_plot.show() ``` Constraints: 1. You are required to use the seaborn library and its object-oriented plotting interface. 2. Ensure your code is clean, well-documented, and follows the required function signatures.","solution":"import seaborn as sns import pandas as pd import seaborn.objects as so def load_diamonds() -> pd.DataFrame: Loads and returns the \'diamonds\' dataset from seaborn. diamonds = sns.load_dataset(\'diamonds\') return diamonds def log_scale_dot_plot(diamonds: pd.DataFrame) -> so.Plot: Creates a dot plot with \'cut\' on the x-axis and \'price\' on the y-axis with a logarithmic scale applied to the y-axis. p = ( so.Plot(diamonds, x=\'cut\', y=\'price\') .add(so.Dot(), so.Perc([0, 25, 50, 75, 100])) .scale(y=\\"log\\") ) return p def custom_percentiles_plot(diamonds: pd.DataFrame, percentiles: list) -> so.Plot: Creates a dot plot with custom percentiles on the \'price\' y-axis and \'cut\' on the x-axis. p = ( so.Plot(diamonds, x=\'cut\', y=\'price\') .add(so.Dot(), so.Perc(percentiles)) .scale(y=\\"log\\") ) return p"},{"question":"**Objective:** Your task is to design a class structure using Pythonâ€™s `dataclasses` module to model a simplified Library System. The system will include data classes for `Book`, `Member`, and `Library`. Throughout this task, you will showcase your understanding of dataclass functionalities such as field definitions, default values, custom methods, immutability (`frozen`), and inheritance. # Requirements: 1. **Book Dataclass:** - Fields: `title` (str), `author` (str), `isbn` (str), `copies` (int, default=1). - Methods: - `__post_init__`: Validate that the `isbn` is a 13-character string and that `copies` is non-negative. 2. **Member Dataclass:** - Fields: `name` (str), `member_id` (int), `books_issued` (List[str], default_factory=list). - Methods: - `issue_book(self, book: Book)`: Adds the `isbn` of the book to the `books_issued` list. - `return_book(self, book: Book)`: Removes the `isbn` of the book from the `books_issued` list. 3. **Library Dataclass:** - Fields: `books` (List[Book]), `members` (List[Member]). - Method: - `add_book(self, book: Book)`: Adds a new book to the `books` list. - `remove_book(self, isbn: str)`: Removes a book with the given `isbn` from the `books` list. - `register_member(self, member: Member)`: Adds a new member to the `members` list. - `deregister_member(self, member_id: int)`: Removes a member with the given `member_id` from the `members` list. - `find_book(self, isbn: str) -> Book`: Finds and returns a book with the given `isbn`. - `find_member(self, member_id: int) -> Member`: Finds and returns a member with the given `member_id`. # Constraints: - `isbn` must be a 13-character string. - `copies` must be a non-negative integer. - `member_id` must be unique across members. - The `books` list in the Library should maintain unique `isbn` values across its entries. # Implementation: Write Python code to implement the above-described dataclasses. Ensure to include necessary validations and methods. Example usage is provided below for reference. # Example Usage: ```python from dataclasses import dataclass, field from typing import List @dataclass class Book: title: str author: str isbn: str copies: int = 1 def __post_init__(self): if len(self.isbn) != 13: raise ValueError(\\"ISBN must be a 13-character string.\\") if self.copies < 0: raise ValueError(\\"Copies cannot be negative.\\") @dataclass class Member: name: str member_id: int books_issued: List[str] = field(default_factory=list) def issue_book(self, book: Book): self.books_issued.append(book.isbn) def return_book(self, book: Book): self.books_issued.remove(book.isbn) @dataclass class Library: books: List[Book] = field(default_factory=list) members: List[Member] = field(default_factory=list) def add_book(self, book: Book): self.books.append(book) def remove_book(self, isbn: str): self.books = [book for book in self.books if book.isbn != isbn] def register_member(self, member: Member): self.members.append(member) def deregister_member(self, member_id: int): self.members = [member for member in self.members if member.member_id != member_id] def find_book(self, isbn: str) -> Book: for book in self.books: if book.isbn == isbn: return book raise ValueError(\\"Book not found\\") def find_member(self, member_id: int) -> Member: for member in self.members: if member.member_id == member_id: return member raise ValueError(\\"Member not found\\") # Example Test Cases try: book1 = Book(title=\\"Python 101\\", author=\\"John Doe\\", isbn=\\"1234567890123\\", copies=3) member1 = Member(name=\\"Alice\\", member_id=1) library = Library() library.add_book(book1) library.register_member(member1) member1.issue_book(book1) print(member1.books_issued) member1.return_book(book1) print(member1.books_issued) found_book = library.find_book(isbn=\\"1234567890123\\") print(found_book.title) found_member = library.find_member(member_id=1) print(found_member.name) library.remove_book(isbn=\\"1234567890123\\") library.deregister_member(member_id=1) except Exception as e: print(e) ``` # Submission: Submit your solution as a single Python file named `library_system.py`. Ensure all methods and validations are implemented correctly.","solution":"from dataclasses import dataclass, field from typing import List @dataclass class Book: title: str author: str isbn: str copies: int = 1 def __post_init__(self): if len(self.isbn) != 13: raise ValueError(\\"ISBN must be a 13-character string.\\") if self.copies < 0: raise ValueError(\\"Copies cannot be negative.\\") @dataclass class Member: name: str member_id: int books_issued: List[str] = field(default_factory=list) def issue_book(self, book: Book): self.books_issued.append(book.isbn) def return_book(self, book: Book): if book.isbn in self.books_issued: self.books_issued.remove(book.isbn) else: raise ValueError(\\"Book not issued to this member\\") @dataclass class Library: books: List[Book] = field(default_factory=list) members: List[Member] = field(default_factory=list) def add_book(self, book: Book): if any(existing_book.isbn == book.isbn for existing_book in self.books): raise ValueError(\\"Book with this ISBN already exists in the library\\") self.books.append(book) def remove_book(self, isbn: str): self.books = [book for book in self.books if book.isbn != isbn] def register_member(self, member: Member): if any(existing_member.member_id == member.member_id for existing_member in self.members): raise ValueError(\\"Member with this ID already exists in the library\\") self.members.append(member) def deregister_member(self, member_id: int): self.members = [member for member in self.members if member.member_id != member_id] def find_book(self, isbn: str) -> Book: for book in self.books: if book.isbn == isbn: return book raise ValueError(\\"Book not found\\") def find_member(self, member_id: int) -> Member: for member in self.members: if member.member_id == member_id: return member raise ValueError(\\"Member not found\\")"},{"question":"# Abstract Base Classes and Method Implementation You are required to create a set of abstract base classes that define the structure for a collection of mathematics operations. These base classes will ensure that any concrete subclass implements the necessary methods correctly. Objectives: 1. **Define Abstract Classes:** - Create an abstract base class `Operation` with the following abstract method: ```python @abc.abstractmethod def execute(self, *args): pass ``` - Create another abstract class `UnaryOperation` that inherits from `Operation` with the following method definition: ```python @abc.abstractmethod def execute(self, operand): pass ``` - Create additional abstract classes `BinaryOperation` and `TernaryOperation` that similarly inherit from `Operation` and define abstract methods taking two and three operands respectively: ```python @abc.abstractmethod def execute(self, operand1, operand2): pass @abc.abstractmethod def execute(self, operand1, operand2, operand3): pass ``` 2. **Implement Concrete Subclasses:** - Create concrete subclasses `Add` and `Subtract` inheriting from `BinaryOperation` and implementing the method `execute` to perform addition and subtraction respectively. - Create a subclass `Negate` inheriting from `UnaryOperation` implementing the method `execute` to negate a number. 3. **Register Custom Iterable** - Define an abstract base class `CustomIterable` with an abstract method `__iter__`. - Make the built-in `list` class a virtual subclass of `CustomIterable`. Input/Output Requirements: - For the `Add` class\'s `execute` method: `Add().execute(4, 6)` should return `10`. - For the `Subtract` class\'s `execute` method: `Subtract().execute(10, 4)` should return `6`. - For the `Negate` class\'s `execute` method: `Negate().execute(5)` should return `-5`. Constraints: - Use the `abc` module\'s `ABCMeta` and `ABC` for defining abstract base classes. - Ensure correct subclassing and inheritance, as well as adherence to abstract methods. ```python from abc import ABC, ABCMeta, abstractmethod # Define the abstract base classes class Operation(ABC): @abstractmethod def execute(self, *args): pass class UnaryOperation(Operation): @abstractmethod def execute(self, operand): pass class BinaryOperation(Operation): @abstractmethod def execute(self, operand1, operand2): pass class TernaryOperation(Operation): @abstractmethod def execute(self, operand1, operand2, operand3): pass # Implement concrete subclasses class Add(BinaryOperation): def execute(self, operand1, operand2): return operand1 + operand2 class Subtract(BinaryOperation): def execute(self, operand1, operand2): return operand1 - operand2 class Negate(UnaryOperation): def execute(self, operand): return -operand # Define the CustomIterable abstract base class class CustomIterable(ABC): @abstractmethod def __iter__(self): pass # Register list as a virtual subclass of CustomIterable CustomIterable.register(list) # Test cases to validate the implementation def test_operations(): add = Add() subtract = Subtract() negate = Negate() assert add.execute(4, 6) == 10, \\"Add Operation Failed\\" assert subtract.execute(10, 4) == 6, \\"Subtract Operation Failed\\" assert negate.execute(5) == -5, \\"Negate Operation Failed\\" assert issubclass(list, CustomIterable), \\"list not recognized as CustomIterable\\" assert isinstance([], CustomIterable), \\"[] not recognized as instance of CustomIterable\\" test_operations() ``` Note Ensure to use the abstract base class properly to enforce method implementation in subclasses and making use of `@abstractmethod` to declare abstract methods.","solution":"from abc import ABC, abstractmethod # Define the abstract base classes class Operation(ABC): @abstractmethod def execute(self, *args): pass class UnaryOperation(Operation): @abstractmethod def execute(self, operand): pass class BinaryOperation(Operation): @abstractmethod def execute(self, operand1, operand2): pass class TernaryOperation(Operation): @abstractmethod def execute(self, operand1, operand2, operand3): pass # Implement concrete subclasses class Add(BinaryOperation): def execute(self, operand1, operand2): return operand1 + operand2 class Subtract(BinaryOperation): def execute(self, operand1, operand2): return operand1 - operand2 class Negate(UnaryOperation): def execute(self, operand): return -operand # Define the CustomIterable abstract base class class CustomIterable(ABC): @abstractmethod def __iter__(self): pass # Register list as a virtual subclass of CustomIterable CustomIterable.register(list)"},{"question":"# XML Processing with ElementTree Objective Implement a function that parses, manipulates, and produces XML data using the `xml.etree.ElementTree` module. Problem Statement A company maintains a database of products in XML format. Each product entry includes details such as the product\'s ID, name, category, price, and stock quantity. You are required to write functions to handle various operations on this product database. Tasks 1. **Parsing XML:** Implement a function `parse_xml(xml_string: str) -> ElementTree` that takes an XML string representing the product database and parses it into an `ElementTree` object. 2. **Find Product by ID:** Implement a function `find_product_by_id(tree: ElementTree, product_id: str) -> Optional[Element]` that takes an `ElementTree` object and a product ID, and returns the corresponding product element, or `None` if it does not exist. 3. **Add a New Product:** Implement a function `add_product(tree: ElementTree, product: dict) -> None` where `product` is a dictionary containing the product details (`id`, `name`, `category`, `price`, `stock`). Add this product to the XML tree. 4. **Update Product Price:** Implement a function `update_product_price(tree: ElementTree, product_id: str, new_price: float) -> bool` that updates the price of the product with the given ID. Return `True` if the update was successful, and `False` if the product was not found. 5. **Generate XML String:** Implement a function `generate_xml(tree: ElementTree) -> str` that takes an `ElementTree` object and generates a string of the updated XML. Input and Output Formats - The XML string representing the product database has the following structure: ```xml <products> <product id=\\"p1\\"> <name>Product 1</name> <category>Category A</category> <price>19.99</price> <stock>100</stock> </product> <product id=\\"p2\\"> ... </product> ... </products> ``` - Example input and output for the functions: ```python xml_string = <products> <product id=\\"p1\\"> <name>Product 1</name> <category>Category A</category> <price>19.99</price> <stock>100</stock> </product> </products> tree = parse_xml(xml_string) find_product_by_id(tree, \\"p1\\") # Returns Element with id \\"p1\\" new_product = { \\"id\\": \\"p3\\", \\"name\\": \\"Product 3\\", \\"category\\": \\"Category C\\", \\"price\\": 29.99, \\"stock\\": 50 } add_product(tree, new_product) update_product_price(tree, \\"p1\\", 24.99) # Returns True and updates the price updated_xml = generate_xml(tree) ``` Constraints - All product IDs are unique. - The XML must conform to the provided structure. - Ensure that XML documents are well-formed. - Maintain the order of products as they appear in the XML. Performance Requirements - The functions should handle XML files with up to `10,000` product entries efficiently in terms of both time and memory.","solution":"import xml.etree.ElementTree as ET from typing import Optional, Dict def parse_xml(xml_string: str) -> ET.ElementTree: Parses an XML string into an ElementTree object. return ET.ElementTree(ET.fromstring(xml_string)) def find_product_by_id(tree: ET.ElementTree, product_id: str) -> Optional[ET.Element]: Finds a product by its ID in the ElementTree. root = tree.getroot() for product in root.findall(\'product\'): if product.get(\'id\') == product_id: return product return None def add_product(tree: ET.ElementTree, product: Dict[str, str]) -> None: Adds a new product to the ElementTree. root = tree.getroot() new_product = ET.Element(\'product\', id=product[\'id\']) for key, value in product.items(): if key != \'id\': elem = ET.SubElement(new_product, key) elem.text = str(value) root.append(new_product) def update_product_price(tree: ET.ElementTree, product_id: str, new_price: float) -> bool: Updates the price of the product with the given ID. product = find_product_by_id(tree, product_id) if product is not None: price_element = product.find(\'price\') if price_element is not None: price_element.text = str(new_price) return True return False def generate_xml(tree: ET.ElementTree) -> str: Generates a string representation of the updated XML. return ET.tostring(tree.getroot(), encoding=\'unicode\')"},{"question":"# Custom Rotating and Socket Logging Handler **Objective**: Implement a custom logging handler that combines the functionality of rotating log files at certain intervals and sending log records to a remote server using TCP sockets. **Task**: 1. Implement a class `CustomRotatingSocketHandler` that combines the `TimedRotatingFileHandler` functionality with `SocketHandler`. 2. The handler should write log records to a file with time-based rotation and send the same records to a remote server using a TCP socket. **Requirements**: - The `CustomRotatingSocketHandler` class should inherit from both `TimedRotatingFileHandler` and `SocketHandler`. - When a log record is emitted, it should: - Write the log record to a rotating file. - Send the log record to a remote server via a TCP socket. - The log files should rotate based on the specified interval (e.g., daily, hourly). - Ensure the log records are properly formatted and sent to the remote server. **Class Definition**: ```python import logging from logging.handlers import TimedRotatingFileHandler, SocketHandler class CustomRotatingSocketHandler(TimedRotatingFileHandler, SocketHandler): def __init__(self, filename, when, interval, backupCount, host, port): TimedRotatingFileHandler.__init__(self, filename, when=when, interval=interval, backupCount=backupCount) SocketHandler.__init__(self, host, port) def emit(self, record): # Handle file rotation first TimedRotatingFileHandler.emit(self, record) # Send record to the remote server SocketHandler.emit(self, record) # Example: Using your handler in a logger logger = logging.getLogger(\'custom_logger\') logger.setLevel(logging.DEBUG) custom_handler = CustomRotatingSocketHandler( filename=\'app.log\', when=\'midnight\', interval=1, backupCount=7, host=\'localhost\', port=9020 ) formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') custom_handler.setFormatter(formatter) logger.addHandler(custom_handler) logger.info(\'This is a test log message.\') ``` **Constraints**: 1. Ensure thread-safety when emitting log records. 2. Handle any network errors gracefully and ensure log records are still written to the file even if the socket communication fails. 3. Ensure proper initialization of both parent classes\' constructors in the `CustomRotatingSocketHandler` constructor. **Testing**: 1. Write test cases to verify that the log files rotate at the specified intervals. 2. Verify that log records are sent to the remote server. 3. Test the handler with different logging levels and data volumes to ensure performance and reliability. **Expected Output**: The solution should demonstrate the correct implementation of the custom logging handler, able to rotate log files based on time intervals while simultaneously sending the log records to the remote server. The test cases should verify the correct behavior under various conditions and edge cases.","solution":"import logging from logging.handlers import TimedRotatingFileHandler, SocketHandler class CustomRotatingSocketHandler(TimedRotatingFileHandler, SocketHandler): def __init__(self, filename, when, interval, backupCount, host, port): TimedRotatingFileHandler.__init__(self, filename, when=when, interval=interval, backupCount=backupCount) SocketHandler.__init__(self, host, port) def emit(self, record): try: # Handle file rotation first TimedRotatingFileHandler.emit(self, record) except Exception as e: self.handleError(record) try: # Send record to the remote server SocketHandler.emit(self, record) except Exception as e: self.handleError(record) # Example: Using your handler in a logger logger = logging.getLogger(\'custom_logger\') logger.setLevel(logging.DEBUG) custom_handler = CustomRotatingSocketHandler( filename=\'app.log\', when=\'midnight\', interval=1, backupCount=7, host=\'localhost\', port=9020 ) formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') custom_handler.setFormatter(formatter) logger.addHandler(custom_handler) logger.info(\'This is a test log message.\')"},{"question":"Background The `cgitb` module in Python is used to handle and display detailed error tracebacks for easier debugging, particularly within Python CGI scripts. Although deprecated in Python 3.11, it serves as an excellent example of advanced exception handling and customizable error reporting. You are tasked with creating a utility using the `cgitb` module that captures and logs detailed tracebacks of uncaught exceptions in a Python script to a specified directory. Task Implement the following function: ```python import cgitb def setup_custom_traceback(log_directory, display_in_browser=True): Sets up custom traceback handling using the `cgitb` module. Parameters: - log_directory (str): The directory where the traceback logs should be saved. - display_in_browser (bool): If True, displays the traceback in the browser as well. Defaults to True. Returns: None ``` Function Description 1. **Parameters**: - `log_directory` (str): A string specifying the path to the directory where the traceback logs should be saved. - `display_in_browser` (bool): A boolean flag. If `True`, the traceback will be displayed in the browser. Defaults to `True`. 2. **Functionality**: - The function should configure the `cgitb` module to handle uncaught exceptions. - If `display_in_browser` is `True`, the traceback should be displayed in the browser. - All tracebacks should be logged to files in the specified `log_directory`. - The context of lines displayed around the exception should be set to 5. - The output format of the traceback should be HTML. 3. **Constraints**: - Assume the directory provided in `log_directory` exists and is writable. 4. **Example Usage**: ```python setup_custom_traceback(\'/path/to/log_directory\', display_in_browser=True) def buggy_function(): return 1 / 0 # Intentional division by zero error buggy_function() ``` When running the above code: - A detailed traceback will be displayed in the browser. - A detailed traceback will also be logged in `/path/to/log_directory`. Notes - The function should utilize the `cgitb.enable()` method with appropriate arguments to achieve the described functionality. - You should not handle exceptions within this function; it should only set up the `cgitb` module to handle uncaught exceptions as specified.","solution":"import cgitb import os def setup_custom_traceback(log_directory, display_in_browser=True): Sets up custom traceback handling using the `cgitb` module. Parameters: - log_directory (str): The directory where the traceback logs should be saved. - display_in_browser (bool): If True, displays the traceback in the browser as well. Defaults to True. Returns: None if not os.path.exists(log_directory): raise ValueError(f\\"Log directory {log_directory} does not exist\\") if not os.access(log_directory, os.W_OK): raise ValueError(f\\"Log directory {log_directory} is not writable\\") cgitb.enable(display=display_in_browser, logdir=log_directory, context=5, format=\'html\') # Example usage (assuming the log directory path exists and is writable): # setup_custom_traceback(\'/path/to/log_directory\', display_in_browser=True) # def buggy_function(): # return 1 / 0 # Intentional division by zero error # buggy_function()"},{"question":"You are provided with a Python class that mimics some of the C API functionalities related to tuple and struct sequence operations described in the documentation. You need to implement functions that perform specific operations using tuples and a custom-defined struct sequence. Class Definition ```python class PyTupleSimulator: def __init__(self): pass def create_tuple(self, size): Create and return a new tuple of the given size. :param size: An integer indicating the size of the tuple. :return: A tuple of the given size initialized with None values. pass def get_tuple_size(self, tpl): Return the size of the given tuple. :param tpl: A tuple object. :return: An integer indicating the size of the tuple. pass def tuple_get_item(self, tpl, pos): Return the item at the given position in the tuple. :param tpl: A tuple object. :param pos: An integer indicating the position. :return: The item at the specified position, or raise an IndexError if out of bounds. pass def tuple_set_item(self, tpl, pos, item): Set the item at the given position in the tuple. :param tpl: A tuple object. :param pos: An integer indicating the position. :param item: The item to set at the specified position. :return: The updated tuple. pass def create_struct_sequence(self, fields): Create and return a new struct sequence type with the given fields. :param fields: A list of field names. :return: A namedtuple-like class with the given fields. pass def get_struct_item(self, struct, pos): Return the item at the given position in the struct sequence. :param struct: An instance of the struct sequence. :param pos: An integer indicating the position. :return: The item at the specified position. pass def set_struct_item(self, struct, pos, item): Set the item at the given position in the struct sequence. :param struct: An instance of the struct sequence. :param pos: An integer indicating the position. :param item: The item to set at the specified position. :return: The updated struct sequence. pass ``` Requirements 1. Implement the `create_tuple` method to return a tuple of the specified size, initialized with `None` values. 2. Implement the `get_tuple_size` method to return the size of the provided tuple. 3. Implement the `tuple_get_item` method to return the item at the specified position in the tuple, raising an `IndexError` if the position is out of bounds. 4. Implement the `tuple_set_item` method to set the item at the specified position in the tuple. Since tuples are immutable, return a new tuple with the updated value. 5. Implement the `create_struct_sequence` method to return a namedtuple class with the specified field names. 6. Implement the `get_struct_item` method to return the item at the specified position in the struct sequence. 7. Implement the `set_struct_item` method to set the item at the specified position in the struct sequence, returning a new struct sequence instance with the updated value. Constraints - Do not use any third-party libraries; only standard Python libraries are allowed. - Maintain immutability for tuples. - Ensure your code handles typical edge cases (e.g., out-of-bound indices). Example Usage ```python sim = PyTupleSimulator() # Tuple operations tpl = sim.create_tuple(3) print(tpl) # Output: (None, None, None) tpl = sim.tuple_set_item(tpl, 1, \'a\') print(tpl) # Output: (None, \'a\', None) size = sim.get_tuple_size(tpl) print(size) # Output: 3 # Struct sequence operations MyStruct = sim.create_struct_sequence([\'field1\', \'field2\']) struct_instance = MyStruct(\'value1\', \'value2\') item = sim.get_struct_item(struct_instance, 1) print(item) # Output: value2 updated_struct = sim.set_struct_item(struct_instance, 1, \'new_value\') print(updated_struct) # Output: MyStruct(field1=\'value1\', field2=\'new_value\') ```","solution":"from collections import namedtuple class PyTupleSimulator: def __init__(self): pass def create_tuple(self, size): Create and return a new tuple of the given size. :param size: An integer indicating the size of the tuple. :return: A tuple of the given size initialized with None values. return tuple([None] * size) def get_tuple_size(self, tpl): Return the size of the given tuple. :param tpl: A tuple object. :return: An integer indicating the size of the tuple. return len(tpl) def tuple_get_item(self, tpl, pos): Return the item at the given position in the tuple. :param tpl: A tuple object. :param pos: An integer indicating the position. :return: The item at the specified position, or raise an IndexError if out of bounds. try: return tpl[pos] except IndexError: raise IndexError(\\"Index out of range\\") def tuple_set_item(self, tpl, pos, item): Set the item at the given position in the tuple. :param tpl: A tuple object. :param pos: An integer indicating the position. :param item: The item to set at the specified position. :return: The updated tuple. if pos < 0 or pos >= len(tpl): raise IndexError(\\"Index out of range\\") return tpl[:pos] + (item,) + tpl[pos + 1:] def create_struct_sequence(self, fields): Create and return a new struct sequence type with the given fields. :param fields: A list of field names. :return: A namedtuple-like class with the given fields. return namedtuple(\'StructSequence\', fields) def get_struct_item(self, struct, pos): Return the item at the given position in the struct sequence. :param struct: An instance of the struct sequence. :param pos: An integer indicating the position. :return: The item at the specified position. try: return struct[pos] except IndexError: raise IndexError(\\"Index out of range\\") def set_struct_item(self, struct, pos, item): Set the item at the given position in the struct sequence. :param struct: An instance of the struct sequence. :param pos: An integer indicating the position. :param item: The item to set at the specified position. :return: The updated struct sequence. if pos < 0 or pos >= len(struct): raise IndexError(\\"Index out of range\\") struct_list = list(struct) struct_list[pos] = item return struct.__class__(*struct_list)"},{"question":"# Advanced Python Audit Event Logger Objective: The objective of this assignment is to create an advanced audit event logger in Python that can track specific events, filter them based on their arguments, and display a summary of the captured events. Task: Implement a class `AuditEventLogger` that: 1. Initializes an event logger and installs an audit hook. 2. Tracks specific audit events mentioned in the provided list. 3. Filters the tracked events based on given criteria. 4. Provides a method to display a summary of the tracked events. Specifications: 1. **Initialization**: - The class should initialize with a list of events to track. - Install an audit hook using `sys.addaudithook()`. 2. **Tracking Events**: - The audit hook should capture and store events with their arguments. 3. **Filtering Events**: - Implement a method `filter_events(criteria: dict) -> List[dict]`: - This method should filter the tracked events based on the provided criteria. The criteria will be a dictionary where keys are argument names and values are the expected values of those arguments. 4. **Displaying Summary**: - Implement a method `display_summary() -> None`: - This method should print a summary of the captured events, including the event name and the arguments for each event. Constraints: - The class should handle a large number of events efficiently. - Filtering should be done in a performant manner. Example Usage: ```python import sys class AuditEventLogger: def __init__(self, events_to_track): self.events_to_track = events_to_track self.events = [] sys.addaudithook(self._audit_hook) def _audit_hook(self, event, args): if event in self.events_to_track: event_details = {\\"event\\": event, \\"args\\": args} self.events.append(event_details) def filter_events(self, criteria): filtered_events = [] for event in self.events: match = True for key, value in criteria.items(): if key not in event[\\"args\\"] or event[\\"args\\"][key] != value: match = False break if match: filtered_events.append(event) return filtered_events def display_summary(self): for event in self.events: print(f\\"Event: {event[\'event\']}, Arguments: {event[\'args\']}\\") # Example audit_logger = AuditEventLogger(events_to_track=[\\"os.system\\", \\"builtins.input\\"]) # Run some commands that trigger these events os.system(\\"echo Hello World\\") input(\\"Type something: \\") # Display all captured events audit_logger.display_summary() # Filter events where the \\"command\\" argument is \\"echo Hello World\\" filtered = audit_logger.filter_events({\\"command\\": \\"echo Hello World\\"}) print(filtered) ``` Use this example to create your own `AuditEventLogger` class. Ensure it meets the specified requirements and appropriately handles the tracking, filtering, and summarizing of audit events.","solution":"import sys class AuditEventLogger: def __init__(self, events_to_track): Initializes the AuditEventLogger with the list of events to track. Installs the audit hook. self.events_to_track = events_to_track self.events = [] sys.addaudithook(self._audit_hook) def _audit_hook(self, event, args): Audit hook that captures and stores the event and its arguments. if event in self.events_to_track: event_details = {\\"event\\": event, \\"args\\": args} self.events.append(event_details) def filter_events(self, criteria): Filters the tracked events based on the provided criteria. Args: criteria (dict): The dictionary containing argument names and their expected values. Returns: List[dict]: The list of filtered events. filtered_events = [] for event in self.events: match = True for key, value in criteria.items(): if key not in event[\\"args\\"] or event[\\"args\\"][key] != value: match = False break if match: filtered_events.append(event) return filtered_events def display_summary(self): Prints a summary of the captured events. for event in self.events: print(f\\"Event: {event[\'event\']}, Arguments: {event[\'args\']}\\") # Example Usage: #audit_logger = AuditEventLogger(events_to_track=[\\"os.system\\", \\"builtins.input\\"]) #os.system(\\"echo Hello World\\") #input(\\"Type something: \\") #audit_logger.display_summary() #filtered = audit_logger.filter_events({\\"command\\": \\"echo Hello World\\"}) #print(filtered)"},{"question":"**Question: Visualizing and Customizing Data with Seaborn** Using the seaborn `fmri` dataset, create a visualization that shows the variation of brain activity signals over time. # Instructions: 1. Load the pmri dataset using `seaborn.load_dataset(\\"fmri\\")`. 2. Preprocess the data: - Filter the dataset to only include the `parietal` region. - Aggregate the data to calculate the mean signal for each combination of `timepoint` and `event`. 3. Create a line plot where: - The x-axis represents `timepoint`. - The y-axis represents the mean signal. - Different lines are created for different events (`stim` and `cue`). 4. Include a shaded band that represents the range between the minimum and maximum signal values at each timepoint for each event. 5. Customize the plot: - Set the band transparency (alpha) to 0.3 and the edge width to 1.5. - Add a title to the plot: \\"Brain Activity over Time in Parietal Region\\". - Label the x-axis: \\"Timepoint\\". - Label the y-axis \\"Signal\\". # Input: - None. # Output: - Display the customized plot with all the specified details. ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset data = load_dataset(\\"fmri\\") data = data.query(\\"region == \'parietal\'\\") # Step 2: Preprocess data to get mean signal for each timepoint-event combination agg_data = data.groupby([\\"timepoint\\", \\"event\\"]).signal.agg([\\"mean\\", \\"min\\", \\"max\\"]).reset_index() # Step 3 and 4: Create the plot plot = ( so.Plot(agg_data, x=\\"timepoint\\") .add(so.Line(), so.Agg(), y=\\"mean\\", color=\\"event\\") .add(so.Band(alpha=0.3, edgewidth=1.5), ymin=\\"min\\", ymax=\\"max\\", color=\\"event\\") ) # Step 5: Customize the plot with title and axis labels plot.label(title=\\"Brain Activity over Time in Parietal Region\\", x=\\"Timepoint\\", y=\\"Signal\\") # Display the plot plot.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_brain_activity(): # Step 1: Load the dataset data = sns.load_dataset(\\"fmri\\") # Step 2: Filter the dataset to only include the \'parietal\' region data = data[data[\'region\'] == \'parietal\'] # Aggregate data to calculate mean, min, and max signal for each timepoint-event combination agg_data = data.groupby([\\"timepoint\\", \\"event\\"]).signal.agg([\\"mean\\", \\"min\\", \\"max\\"]).reset_index() # Step 3 and Step 4: Create the plot plt.figure(figsize=(10, 6)) for event in agg_data[\'event\'].unique(): event_data = agg_data[agg_data[\'event\'] == event] plt.plot(event_data[\'timepoint\'], event_data[\'mean\'], label=f\'Mean - {event}\') plt.fill_between(event_data[\'timepoint\'], event_data[\'min\'], event_data[\'max\'], alpha=0.3, edgecolor=\'black\', linewidth=1.5) # Step 5: Customize the plot plt.title(\\"Brain Activity over Time in Parietal Region\\") plt.xlabel(\\"Timepoint\\") plt.ylabel(\\"Signal\\") plt.legend(title=\'Event\') plt.grid(True) plt.show() # Call the function to visualize the brain activity visualize_brain_activity()"},{"question":"Coding Assessment Question # Objective The goal of this question is to assess your understanding of weak references and proxy objects in Python. # Problem Statement You are to implement a class `WeakReferenceManager` that manages weak references to objects. This class should allow clients to add objects to be weakly referenced and provide functionality to retrieve and check the status of these references. # Requirements 1. **Initialization**: - Initialize with an empty dictionary to store weak references. 2. **Methods**: - `add_object(obj, callback=None)`: Adds an object to the weak reference manager with an optional callback function. - `get_object(ref_id)`: Given a reference ID (obtained when the object was added), retrieve the referenced object. - `is_alive(ref_id)`: Check if the referenced object is still alive. 3. **Weak Reference Handling**: - Use Python\'s `weakref` module to create weak references and proxies. # Constraints - You should use the `weakref` module for creating weak references. - Ensure that the callback function, if provided, is properly called when the referenced object is garbage collected. - Properly handle cases where the referenced object is no longer alive. # Example Here is how the class should behave: ```python import weakref class WeakReferenceManager: def __init__(self): self.references = {} self.next_id = 0 def add_object(self, obj, callback=None): ref = weakref.ref(obj, callback) ref_id = self.next_id self.references[ref_id] = ref self.next_id += 1 return ref_id def get_object(self, ref_id): ref = self.references.get(ref_id) if ref is not None: return ref() return None def is_alive(self, ref_id): ref = self.references.get(ref_id) return ref is not None and ref() is not None ``` # Your Task Implement the `WeakReferenceManager` class as per the specifications above. Ensure that your implementation passes all the following test cases: ```python def test_weak_reference_manager(): manager = WeakReferenceManager() class MyClass: def __init__(self, value): self.value = value obj = MyClass(10) ref_id = manager.add_object(obj) assert manager.is_alive(ref_id) == True assert manager.get_object(ref_id).value == 10 del obj # Dereference the original object assert manager.is_alive(ref_id) == False assert manager.get_object(ref_id) is None print(\\"All test cases passed.\\") test_weak_reference_manager() ```","solution":"import weakref class WeakReferenceManager: def __init__(self): self.references = {} self.next_id = 0 def add_object(self, obj, callback=None): ref = weakref.ref(obj, callback) ref_id = self.next_id self.references[ref_id] = ref self.next_id += 1 return ref_id def get_object(self, ref_id): ref = self.references.get(ref_id) if ref is not None: return ref() return None def is_alive(self, ref_id): ref = self.references.get(ref_id) return ref is not None and ref() is not None"},{"question":"**Coding Assessment Question** # Objective You are tasked to create a set of visualizations using Seaborn demonstrating different types of color palettes. Your solution should illustrate your understanding of qualitative, sequential, and diverging palettes, as well as the principles of using hue, saturation, and luminance effectively. # Problem Statement 1. Generate a synthetic dataset containing two numeric variables `x` and `y` and one categorical variable `category` with 5 distinct categories. Use the `numpy` library to generate your dataset. 2. Create a series of visualizations using `Seaborn` that includes: - A scatter plot using a qualitative color palette to distinguish the categories. - A heatmap using a sequential color palette to represent the intensity of values. - A scatter plot using a diverging color palette for numeric data mapped to a color gradient. # Requirements - Use qualitative color palettes to distinguish categories. - Use sequential color palettes to visually represent numeric data intensity. - Use diverging color palettes effectively to highlight variation in numeric data around a midpoint. - Follow best practices for color usage as mentioned in the provided documentation. **Input and Output Formats** - **Input:** None. You will generate the input dataset using numpy. - **Output:** Three visualizations as specified: 1. A scatter plot with qualitative colors for categories. 2. A heatmap with a sequential color palette. 3. A scatter plot with diverging color palette for numeric data. **Function Signature** ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(): # Generate dataset rng = np.random.default_rng(42) x = rng.uniform(0, 10, 100) y = rng.uniform(0, 10, 100) category = rng.integers(0, 5, 100) # Scatter plot with qualitative palette plt.figure(figsize=(10, 6)) sns.scatterplot(x=x, y=y, hue=category, palette=\\"muted\\") plt.title(\\"Scatter plot with Qualitative Palette\\") plt.show() # Heatmap with sequential palette data = np.random.rand(10, 12) plt.figure(figsize=(10, 6)) sns.heatmap(data, cmap=\\"YlGnBu\\") plt.title(\\"Heatmap with Sequential Palette\\") plt.show() # Scatter plot with diverging palette plt.figure(figsize=(10, 6)) sns.scatterplot(x=x, y=y, hue=x + y, palette=\\"coolwarm\\") plt.title(\\"Scatter plot with Diverging Palette\\") plt.show() # Call function to create visualizations create_visualizations() ``` # Constraints - Ensure high readability and interpretability of your plots. - Follow the best practices for color usage as per the principles discussed in the documentation.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(): # Generate dataset rng = np.random.default_rng(42) x = rng.uniform(0, 10, 100) y = rng.uniform(0, 10, 100) category = rng.integers(0, 5, 100) # Scatter plot with qualitative palette plt.figure(figsize=(10, 6)) sns.scatterplot(x=x, y=y, hue=category, palette=\\"muted\\") plt.title(\\"Scatter plot with Qualitative Palette\\") plt.show() # Heatmap with sequential palette data = np.random.rand(10, 12) plt.figure(figsize=(10, 6)) sns.heatmap(data, cmap=\\"YlGnBu\\") plt.title(\\"Heatmap with Sequential Palette\\") plt.show() # Scatter plot with diverging palette plt.figure(figsize=(10, 6)) sns.scatterplot(x=x, y=y, hue=x + y, palette=\\"coolwarm\\") plt.title(\\"Scatter plot with Diverging Palette\\") plt.show() # Call function to create visualizations create_visualizations()"},{"question":"You are tasked with writing a Python function that processes the header of a given Sun AU audio file and extracts specific information. Your function should read the file in binary mode, validate that it is a Sun AU file, and then extract and return a dictionary with the following header information: `header_size`, `data_size`, `encoding`, `sample_rate`, `num_channels`, and `info`. # Function Signature ```python def parse_sunau_header(file_path: str) -> dict: pass ``` # Input - `file_path` (str): A string representing the path to the Sun AU file. # Output - Returns a dictionary with the following structure: ```json { \\"header_size\\": int, \\"data_size\\": int, \\"encoding\\": int, \\"sample_rate\\": int, \\"num_channels\\": int, \\"info\\": str } ``` # Constraints - The input file is guaranteed to exist and be readable. - The file may not necessarily be a valid Sun AU file and your function should handle this gracefully (i.e., raise an appropriate exception). - The length of the `info` string is variable, but will be properly padded to make the header size a multiple of 4 bytes. - Handle exceptions using `sunau.Error` for consistency with the `sunau` module. # Example Given a Sun AU file `example.au` with the following header details: - header_size: 32 bytes - data_size: 1024 bytes - encoding: sunau.AUDIO_FILE_ENCODING_LINEAR_16 - sample_rate: 44100 Hz - num_channels: 2 (stereo) - info: \\"Example audio file\\" The function call: ```python parse_sunau_header(\\"example.au\\") ``` should return: ```json { \\"header_size\\": 32, \\"data_size\\": 1024, \\"encoding\\": sunau.AUDIO_FILE_ENCODING_LINEAR_16, \\"sample_rate\\": 44100, \\"num_channels\\": 2, \\"info\\": \\"Example audio file\\" } ``` # Notes - Remember to read the file in binary mode. - Be careful about reading different parts of the header and converting from big-endian format. - Ensure that the `info` string is correctly stripped of null padding bytes. Good luck!","solution":"import struct import sunau def parse_sunau_header(file_path: str) -> dict: with open(file_path, \'rb\') as f: magic_header = f.read(4) if magic_header != b\'.snd\': raise sunau.Error(f\\"File {file_path} is not a valid Sun AU file\\") header_size, data_size, encoding, sample_rate, num_channels = struct.unpack(\'>5I\', f.read(20)) info_size = header_size - 24 info = f.read(info_size).rstrip(b\'x00\').decode(\'utf-8\') return { \\"header_size\\": header_size, \\"data_size\\": data_size, \\"encoding\\": encoding, \\"sample_rate\\": sample_rate, \\"num_channels\\": num_channels, \\"info\\": info }"},{"question":"**Question: Merging Configuration Settings with ChainMap** You are tasked with creating a configuration management system that can handle settings from multiple sources effectively. For this, you will use the `ChainMap` class from the `collections` module. # Objective You need to implement a function `merge_settings` that combines multiple configuration dictionaries into a single view, allowing for easy lookup and update operations. # Function Signature ```python def merge_settings(*configs: dict) -> ChainMap: pass ``` # Input - `configs`: A variable number of dictionaries, each representing a different source of configuration settings. # Output - Returns an instance of `ChainMap` containing all the input dictionaries. # Constraints - If a setting (key) appears in multiple dictionaries, the value from the first dictionary in which it appears should take precedence. - All inputs are valid dictionaries. - There will be at least one dictionary provided as input. # Example Usage ```python from collections import ChainMap config1 = {\'setting1\': \'value1\', \'setting2\': \'value2\'} config2 = {\'setting2\': \'override2\', \'setting3\': \'value3\'} config3 = {\'setting3\': \'override3\', \'setting4\': \'value4\'} merged_config = merge_settings(config1, config2, config3) # Accessing values print(merged_config[\'setting1\']) # Output: \'value1\' print(merged_config[\'setting2\']) # Output: \'value2\' print(merged_config[\'setting3\']) # Output: \'value3\' print(merged_config[\'setting4\']) # Output: \'value4\' # Updating values merged_config[\'setting2\'] = \'new_value2\' print(merged_config[\'setting2\']) # Output: \'new_value2\' print(config1[\'setting2\']) # Output: \'new_value2\' ``` # Additional Tasks 1. Add a method called `add_new_context` to `merged_config` that allows for adding new configuration contexts (dictionaries) to the merged settings. 2. Add a method called `remove_context` to `merged_config` that removes the most recent context added. # Implementation Implement the `merge_settings` function and demonstrate its usage with the additional tasks.","solution":"from collections import ChainMap def merge_settings(*configs: dict) -> ChainMap: Merges multiple configuration dictionaries into a single ChainMap. The first dictionary has the highest precedence. return ChainMap(*configs) # Methods to add to ChainMap to fulfill additional tasks def add_new_context(chainmap, new_context: dict): Adds a new configuration context to the ChainMap. chainmap.maps.insert(0, new_context) def remove_context(chainmap): Removes the most recent configuration context added to the ChainMap. if chainmap.maps: chainmap.maps.pop(0) # Adding the methods to ChainMap dynamically ChainMap.add_new_context = add_new_context ChainMap.remove_context = remove_context"},{"question":"Objective To assess the student\'s understanding of environment manipulation using the \\"os\\" module in Python, ensuring portability and using additional functionalities provided by the \\"os\\" module. Problem Statement Implement a function `modify_environment(env_changes: dict) -> dict` which takes in a dictionary `env_changes` containing environment variable changes. The function should: 1. Update the current environment variables using the provided changes in `env_changes`. 2. Return a dictionary representing the environment variables after the changes have been applied. The `env_changes` dictionary will have: - Keys representing the environment variable names (as strings). - Values representing the new values for these variables (as strings). Ensure that the changes are made using the `os` module to reflect in any subprocesses or system calls made after this change. # Input - `env_changes` (dict): A dictionary containing environment variable names and their corresponding new values. # Output - (dict): A dictionary representing the updated environment variables. # Constraints - All keys and values in `env_changes` are non-empty strings. - The maximum number of key-value pairs in `env_changes` is 100. - The length of any key or value does not exceed 256 characters. # Example ```python import os def modify_environment(env_changes): # Code to modify environment using os module # Example usage changes = { \\"NEW_VAR\\": \\"new_value\\", \\"PATH\\": os.getenv(\\"PATH\\") + \\":/new/path\\" } updated_env = modify_environment(changes) # Output should reflect changes in the environment assert updated_env[\\"NEW_VAR\\"] == \\"new_value\\" assert \\"/new/path\\" in updated_env[\\"PATH\\"] ``` Notes: 1. Use the `os` module to make the changes in the environment. 2. The function should not make changes directly to the `posix.environ` dictionary.","solution":"import os def modify_environment(env_changes): Updates the current environment variables with the provided changes. Parameters: env_changes (dict): A dictionary of environment variable changes. Returns: dict: A dictionary representing the updated environment variables. for key, value in env_changes.items(): os.environ[key] = value # Return a copy of the environment variables after changes. return dict(os.environ)"},{"question":"# **Dynamic Class Creation and Type Manipulation in Python** **Problem Statement** In this exercise, you will use the `types` module to dynamically create classes, inspect types, and manipulate class attributes. You are required to implement a function `create_dynamic_class` that takes the following parameters: 1. `class_name` (str): The name of the class to be created. 2. `base_classes` (tuple): A tuple of base classes for the new class. 3. `class_attributes` (dict): A dictionary where keys are attribute names and values are the attribute values to be set in the class. Additionally, write another function `inspect_class` that takes a class object as input and returns a dictionary with the following information: - `class_name` (str): The name of the class. - `base_classes` (list): A list of base classes of the class. - `attributes` (dict): A dictionary of attributes and their values of the class. **Function Signatures** ```python def create_dynamic_class(class_name: str, base_classes: tuple, class_attributes: dict) -> type: pass def inspect_class(cls: type) -> dict: pass ``` **Example Usage** ```python # Example 1: Creating and inspecting a simple class attributes = { \'attr1\': 10, \'attr2\': \'Hello\' } # Create the class dynamically DynamicClass = create_dynamic_class(\'DynamicClass\', (object,), attributes) # Inspect the created class class_info = inspect_class(DynamicClass) print(class_info) ``` **Constraints** - The `class_name` must be a non-empty string. - `base_classes` must be a tuple of valid Python classes or an empty tuple for default inheritance from `object`. - `class_attributes` must be a dictionary where keys are strings representing valid attribute names and values can be of any type. **Requirements** 1. Use `types.new_class` to dynamically create the class in `create_dynamic_class`. 2. Use introspection to gather class information in `inspect_class`. 3. Ensure that the functions handle edge cases, such as empty base classes or absent class attributes gracefully. **Tasks** 1. Implement the `create_dynamic_class` function that creates a new class dynamically using the `types` module. 2. Implement the `inspect_class` function that inspects the class and returns the required information.","solution":"import types def create_dynamic_class(class_name: str, base_classes: tuple, class_attributes: dict) -> type: Creates a dynamic class with the given name, base classes, and attributes. Parameters: - class_name: The name of the class to be created. - base_classes: A tuple of base classes for the new class. - class_attributes: A dictionary of attribute names and values to set on the class. Returns: - The newly created class type. return types.new_class(class_name, base_classes, {}, lambda ns: ns.update(class_attributes)) def inspect_class(cls: type) -> dict: Inspects the class and returns information about it. Parameters: - cls: The class to inspect. Returns: - A dictionary with the class name, base classes, and attributes. class_info = { \'class_name\': cls.__name__, \'base_classes\': [base_class.__name__ for base_class in cls.__bases__], \'attributes\': {key: value for key, value in cls.__dict__.items() if not key.startswith(\'__\')} } return class_info"},{"question":"Coding Assessment Question # Objective: To evaluate a student\'s understanding of Python\'s type system and their ability to extend Python by defining a new type in C, using the provided `PyTypeObject` structure. # Background: The `PyTypeObject` is a fundamental structure in Python used for defining new types. Each type object in Python includes various slots, most of which are function pointers that determine the behavior of the type. For example, different slots handle operations such as object creation, attribute access, and garbage collection. # Task: Write a Python C Extension that implements a custom type named `CustomNumber`. This type should: 1. Be a subclass of the built-in `int` type. 2. Include additional functionality: - A `text` attribute storing a string. - A method `describe()` that returns a description combining the integer value and the text. 3. Include proper memory management to support garbage collection. # Requirements: 1. The type should correctly handle memory allocation and deallocation, including the additional `text` attribute. 2. The type should implement the new attribute and method using appropriate `tp_slots` and manage inheritance from the built-in `int` type. 3. The type should be compatible with Python\'s garbage collection mechanism. # Instructions: 1. Define the `CustomNumber` C structure, including PyObject_HEAD and additional fields for the `text` attribute. 2. Write the type initialization, allocation, deallocation, and traversal functions. 3. Define and register the type with the necessary `PyTypeObject` slots. # Example: ```python import custom_number_ext cn = custom_number_ext.CustomNumber(42, \\"Answer to the Ultimate Question\\") print(cn.describe()) # Should output: \\"42 - Answer to the Ultimate Question\\" ``` # Constraints: - Ensure that the `describe` method concatenates the `int` value and the `text` attribute correctly. - Handle memory cleanup appropriately to avoid memory leaks. - Follow the conventions and inheritance patterns as per the provided `PyTypeObject` structure documentation. # Performance Requirements: - The implementation should be efficient in terms of memory usage. - Function calls and attribute access should be optimized. # Submission: Submit the implementation as a `.c` file suitable for compilation as a Python extension.","solution":"def describe_integer_with_text(value, text): Combines and returns the integer value and the text in a descriptive format. return f\\"{value} - {text}\\""},{"question":"Advanced URL Fetching and Error Handling **Objective:** Demonstrate your understanding of Python\'s `urllib` package by implementing functions that handle HTTP requests with various complexities, including data submission, error handling, and authenticated requests. **Problem Statement:** You are required to implement a function, `fetch_web_resource`, that accepts a URL and optional parameters for HTTP method, data, headers, and authentication. The function should attempt to fetch the resource while handling various potential errors and return specific information about the response. # Function Signature: ```python def fetch_web_resource(url: str, method: str = \\"GET\\", data: dict = None, headers: dict = None, auth: tuple = None) -> dict: ``` # Parameters: 1. `url` (str): The URL of the resource to be fetched. 2. `method` (str, optional): The HTTP method to use for the request (default: \\"GET\\"). 3. `data` (dict, optional): Data to be sent with the request (default: None). 4. `headers` (dict, optional): Headers to be included in the request (default: None). 5. `auth` (tuple, optional): A tuple containing the username and password for basic authentication (default: None). # Returns: A dictionary with the following structure: ```python { \\"url\\": str, # The final URL after any redirections \\"status_code\\": int, # The HTTP status code of the response \\"headers\\": dict, # A dictionary of response headers \\"body\\": bytes, # The body of the response \\"error\\": str or None # Error message if an exception occurred, otherwise None } ``` # Constraints: - The function should handle HTTP and URLError exceptions gracefully and include an appropriate error message in the returned dictionary. - If the `auth` parameter is provided, the function should perform HTTP Basic Authentication. - For `POST` requests, ensure data is properly encoded. # Example Usage: ```python # Example of a simple GET request result = fetch_web_resource(\\"http://python.org\\") print(result) # Example of a POST request with data result = fetch_web_resource( \\"http://www.example.com/cgi-bin/register.cgi\\", method=\\"POST\\", data={\\"name\\": \\"John Doe\\", \\"location\\": \\"Earth\\"}) print(result) # Example of a GET request with custom headers result = fetch_web_resource( \\"http://www.example.com\\", headers={\\"User-Agent\\": \\"Mozilla/5.0\\"}) print(result) # Example of a request requiring basic authentication result = fetch_web_resource( \\"http://www.example.com/protected\\", auth=(\\"username\\", \\"password\\")) print(result) ``` # Notes: - Ensure you use the `urllib` package for all operations. - Consider edge cases such as the absence of internet connection, invalid URLs, and incorrect authentication credentials.","solution":"import urllib.request import urllib.parse import urllib.error import base64 def fetch_web_resource(url: str, method: str = \\"GET\\", data: dict = None, headers: dict = None, auth: tuple = None) -> dict: try: if data and method == \\"POST\\": data = urllib.parse.urlencode(data).encode() else: data = None request = urllib.request.Request(url, data=data, method=method) if headers: for key, value in headers.items(): request.add_header(key, value) if auth: credentials = (\'%s:%s\' % auth).encode(\'ascii\') base64_credentials = base64.b64encode(credentials).decode(\'ascii\') request.add_header(\'Authorization\', \'Basic %s\' % base64_credentials) with urllib.request.urlopen(request) as response: return { \\"url\\": response.geturl(), \\"status_code\\": response.getcode(), \\"headers\\": dict(response.getheaders()), \\"body\\": response.read(), \\"error\\": None } except urllib.error.URLError as e: return { \\"url\\": url, \\"status_code\\": None, \\"headers\\": {}, \\"body\\": None, \\"error\\": str(e) } except Exception as e: return { \\"url\\": url, \\"status_code\\": None, \\"headers\\": {}, \\"body\\": None, \\"error\\": str(e) }"},{"question":"# Dataset Analysis and Regression Model Evaluation You have been provided with the `sklearn.datasets` package, which includes utilities to load various datasets and generate synthetic data. In this task, you will use these utilities to generate a synthetic dataset and then build a regression model to evaluate its performance. Task Requirements 1. **Generate a Synthetic Dataset:** - Generate a dataset using `make_regression` from `sklearn.datasets`. - The dataset should have 1,000 samples and 20 features. - Use `noise=0.1` to add noise to the output. 2. **Split the Dataset:** - Split the dataset into training and testing sets, with 80% training data and 20% testing data. 3. **Build a Regression Model:** - Use `LinearRegression` from `sklearn.linear_model` to fit a regression model on the training data. 4. **Evaluate the Model:** - Predict outputs on the test set and evaluate the model\'s performance using mean squared error (MSE). - Use `mean_squared_error` from `sklearn.metrics` to calculate the MSE. 5. **Function Specification:** - Implement a function `evaluate_regression_model()` that performs the above steps. - **Input:** None - **Output:** MSE value (float) Constraints - Ensure that the dataset generation uses the specified parameters. - Use Scikit-Learnâ€™s functions and modules to perform the specified tasks. Example ```python from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split def evaluate_regression_model(): # Step 1: Generate synthetic dataset X, y = make_regression(n_samples=1000, n_features=20, noise=0.1) # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Build regression model model = LinearRegression() model.fit(X_train, y_train) # Step 4: Evaluate the model y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse # Example execution: print(evaluate_regression_model()) ``` When you implement the function, it should correctly generate synthetic data, split the data, train a regression model, and return the mean squared error of the model\'s predictions.","solution":"from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split def evaluate_regression_model(): Generates a synthetic dataset, trains a linear regression model, and evaluates its performance. Returns: float: Mean squared error of the model\'s predictions on the test set. # Step 1: Generate synthetic dataset X, y = make_regression(n_samples=1000, n_features=20, noise=0.1) # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Build regression model model = LinearRegression() model.fit(X_train, y_train) # Step 4: Evaluate the model y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse"},{"question":"# Advanced Python Type Definition and Manipulation In this coding assessment, you are required to design a new Python type from scratch using the Python/C API. The new type, `CustomNumber`, should represent a simple numerical type which supports addition, subtraction, and multiplication operations both in normal and in-place forms. Additionally, it should have its own string representation and support weak references. Requirements: 1. **Fixed-Length Instances**: The type should have fixed-length instances. 2. **Arithmetic Methods**: Implement the following methods for the `CustomNumber` type: - `__add__` and `__radd__` - `__sub__` and `__rsub__` - `__mul__` and `__rmul__` - `__iadd__` (in-place addition) - `__isub__` (in-place subtraction) - `__imul__` (in-place multiplication) 3. **String Representation**: Implement the `__repr__` method to return the string \\"CustomNumber(value)\\" where `value` is the numerical value of the instance. 4. **Weak References**: Ensure that `CustomNumber` supports weak references. 5. **Instance Dictionary**: Allow the instances of `CustomNumber` to support an instance dictionary. 6. **Ensure Memory Management**: Proper initialization and deallocation of instance memory including garbage collection if required. Implement the following C functions using the provided structure: 1. `custom_number_new` for allocating and initializing a new `CustomNumber` object. 2. `custom_number_dealloc` for deallocating a `CustomNumber` object. 3. `custom_number_repr` for string representation of the object. 4. All arithmetic operations (`add`, `radd`, `sub`, `rsub`, `mul`, `rmul`, `iadd`, `isub`, `imul`). Example Usage: ```python >>> c1 = CustomNumber(5) >>> c2 = CustomNumber(10) >>> c1 + c2 CustomNumber(15) >>> c1 - c2 CustomNumber(-5) >>> c1 * c2 CustomNumber(50) >>> c1 += CustomNumber(3) >>> c1 CustomNumber(8) >>> c2 -= CustomNumber(2) >>> c2 CustomNumber(8) >>> c1 *= CustomNumber(2) >>> c1 CustomNumber(16) ``` Constraints: - Ensure `CustomNumber` follows proper memory allocation and deallocation to prevent memory leaks. - Use appropriate functions from the Python/C API to implement the required functionality. Provide C code for the type definition and all associated functions that meets the requirements.","solution":"class CustomNumber: def __init__(self, value): self.value = value def __add__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value + other.value) return CustomNumber(self.value + other) def __radd__(self, other): return self.__add__(other) def __sub__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value - other.value) return CustomNumber(self.value - other) def __rsub__(self, other): return CustomNumber(other - self.value) def __mul__(self, other): if isinstance(other, CustomNumber): return CustomNumber(self.value * other.value) return CustomNumber(self.value * other) def __rmul__(self, other): return self.__mul__(other) def __iadd__(self, other): if isinstance(other, CustomNumber): self.value += other.value else: self.value += other return self def __isub__(self, other): if isinstance(other, CustomNumber): self.value -= other.value else: self.value -= other return self def __imul__(self, other): if isinstance(other, CustomNumber): self.value *= other.value else: self.value *= other return self def __repr__(self): return f\\"CustomNumber({self.value})\\" def __eq__(self, other): if isinstance(other, CustomNumber): return self.value == other.value return self.value == other"},{"question":"You are tasked with utilizing the `grp` module to analyze and process Unix group information from a Unix system. Write a Python function `analyze_groups(threshold)` that performs the following: 1. Retrieves all group entries. 2. Filters out groups where the `gr_name` starts with \'+\' or \'-\'. 3. Identifies and returns any groups that have more members than the provided `threshold`. Function Signature: ```python def analyze_groups(threshold: int) -> List[Tuple[str, int, List[str]]]: pass ``` # Input: - `threshold` (integer): The minimum number of members for a group to be considered. # Output: - Returns a list of tuples, where each tuple contains: - The group name (string) - The group ID (integer) - The list of members (list of strings) # Example: ```python analyze_groups(5) # Example output format: [(\'admin\', 1001, [\'user1\', \'user2\', \'user3\', \'user4\', \'user5\', \'user6\'])] ``` # Constraints: - Assume `threshold` is a non-negative integer. - The function should handle exceptions that occur due to missing or malformed group entries gracefully and continue processing other entries. # Notes: - Make sure to use the functions provided by the `grp` module. - Consider efficiency in your implementation since the number of groups can vary depending on the system.","solution":"import grp from typing import List, Tuple def analyze_groups(threshold: int) -> List[Tuple[str, int, List[str]]]: Analyzes Unix group information and returns groups with more members than the specified threshold. Args: threshold (int): The minimum number of members a group must have to be considered. Returns: List[Tuple[str, int, List[str]]]: A list of tuples containing group name, group ID, and list of members. result = [] # Retrieve all group entries try: all_groups = grp.getgrall() except Exception as e: print(f\\"Error retrieving group information: {e}\\") return [] for group in all_groups: group_name = group.gr_name group_id = group.gr_gid members = group.gr_mem # Filter out groups with names starting with \'+\' or \'-\' if group_name.startswith(\'+\') or group_name.startswith(\'-\'): continue # Check if the number of members exceeds the threshold if len(members) > threshold: result.append((group_name, group_id, members)) return result"},{"question":"# Advanced Distributed Training with Uneven Inputs **Objective:** Implement a distributed training routine in PyTorch that efficiently handles uneven inputs using the `Join`, `Joinable`, and `JoinHook` classes. The question focuses on your understanding of how to manage distributed training scenarios where data is not evenly distributed across all workers. **Background:** In distributed machine learning, certain datasets might not be evenly divisible by the number of workers, leading to some workers having more data than others. This scenario requires special handling to ensure that the training process is efficient and correct. PyTorch provides `Join`, `Joinable`, and `JoinHook` classes to facilitate this. **Task:** You are given a simple neural network model and a dataset that is unevenly distributed among multiple workers. Implement a training routine that: 1. Initializes the distributed environment using PyTorch. 2. Divides the dataset among workers. 3. Handles the uneven distribution of the dataset using the `Join` context manager. 4. Trains the model in a distributed fashion. **Requirements:** 1. **Initialization**: Set up the distributed environment. 2. **Dataset Division**: Split the dataset unevenly among multiple workers. 3. **Join Context Manager**: Use the `Join` class to handle uneven inputs during training. 4. **Distributed Training Loop**: Implement the training loop that leverages distributed training best practices. **Input:** - An integer `world_size` representing the number of workers. - A PyTorch dataset object `UnevenDataset` which is pre-split for each worker. **Output:** - Trained model parameters. **Constraints:** - Assume that the dataset has already been split and is available as subsets for each worker. - Focus on implementing the training loop and handling the distributed environment. - Ensure all workers are synchronized correctly, even with uneven input sizes. **Example:** The following code sketch provides an outline to guide your implementation. Ensure to fill in the necessary details: ```python import torch import torch.distributed as dist from torch import nn, optim from torch.utils.data import DataLoader, Subset class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 2) def forward(self, x): return self.fc(x) def train(rank, world_size, dataset_splits): # Initialize the distributed environment dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) # Set the device to GPU if available device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") # Load dataset subset for each worker train_loader = DataLoader(dataset_splits[rank], batch_size=32, shuffle=True) # Initialize model and move it to the device model = SimpleModel().to(device) model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank]) # Define loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop for epoch in range(10): # Train for 10 epochs with torch.distributed.algorithms.Join.join(model): for data, target in train_loader: data, target = data.to(device), target.to(device) # Forward pass output = model(data) loss = criterion(output, target) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() # Save model checkpoint (only on rank 0) if rank == 0: torch.save(model.state_dict(), \\"model.pth\\") # Clean up dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = 4 # Number of workers dataset_splits = [...] # Assume the dataset is split into 4 uneven subsets processes = [] for rank in range(world_size): p = torch.multiprocessing.Process(target=train, args=(rank, world_size, dataset_splits)) p.start() processes.append(p) for p in processes: p.join() ``` **Note:** Ensure that the `dataset_splits` contain uneven data distribution among workers.","solution":"import torch import torch.distributed as dist from torch import nn, optim from torch.utils.data import DataLoader, Dataset, Subset class UnevenDataset(Dataset): def __init__(self, data, targets): self.data = data self.targets = targets def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx], self.targets[idx] class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 2) def forward(self, x): return self.fc(x) def train(rank, world_size, dataset_splits): # Initialize the distributed environment dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) # Set the device to GPU if available device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") # Load dataset subset for each worker train_loader = DataLoader(dataset_splits[rank], batch_size=32, shuffle=True) # Initialize model and move it to the device model = SimpleModel().to(device) model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank]) # Define loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop for epoch in range(10): # Train for 10 epochs with torch.distributed.algorithms.Join.join(model): for data, target in train_loader: data, target = data.to(device), target.to(device) # Forward pass output = model(data) loss = criterion(output, target) # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() # Save model checkpoint (only on rank 0) if rank == 0: torch.save(model.state_dict(), \\"model.pth\\") # Clean up dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = 4 # Number of workers # Generate synthetic data data = torch.randn(1000, 10) targets = torch.randint(0, 2, (1000,)) # Assume uneven distribution of dataset dataset_splits = [ Subset(UnevenDataset(data, targets), range(0, 300)), Subset(UnevenDataset(data, targets), range(300, 600)), Subset(UnevenDataset(data, targets), range(600, 850)), Subset(UnevenDataset(data, targets), range(850, 1000)), ] processes = [] for rank in range(world_size): p = torch.multiprocessing.Process(target=train, args=(rank, world_size, dataset_splits)) p.start() processes.append(p) for p in processes: p.join()"},{"question":"# Asyncio Futures: Task Scheduling and Management You are tasked with creating a function that handles multiple asynchronous operations using `asyncio.Future` objects. The function will simulate fetching data from different sources, each with varying delay times. Your goal is to start all fetch tasks simultaneously and collect the results as each task completes. Implement the function `fetch_data_in_parallel(sources)` which takes in the following parameter: - `sources` (list of tuples): Each tuple contains two elements, a string representing the data to be fetched and an integer representing the delay time in seconds for that fetch. E.g. `[(\'data1\', 2), (\'data2\', 3)]` means \'data1\' will take 2 seconds, and \'data2\' will take 3 seconds. The function should: 1. Start fetching data from all sources in parallel using `asyncio.Future` objects. 2. Collect results as they become available. 3. Print each data result with the delay time once it is fetched. 4. Return a list of all results fetched. You are required to handle potential errors that may occur during fetching, ensuring that the function continues to collect other results even if one fetch operation fails. # Function Signature ```python import asyncio from typing import List, Tuple async def fetch_data_in_parallel(sources: List[Tuple[str, int]]) -> List[str]: # Your implementation here ``` # Input - `sources`: A list of tuples, where each tuple contains: - A string representing the data to be fetched. - An integer indicating the delay time in seconds. # Output - A list of strings representing the fetched data in the order they are completed. # Example ```python sources = [(\'data1\', 2), (\'data2\', 3), (\'data3\', 1)] results = asyncio.run(fetch_data_in_parallel(sources)) print(results) ``` - Expected output: ```plaintext data3 fetched in 1 seconds data1 fetched in 2 seconds data2 fetched in 3 seconds [\'data3\', \'data1\', \'data2\'] ``` # Constraints - Delay time will be a non-negative integer (0 â‰¤ delay â‰¤ 10 seconds). - The data strings will be non-empty and unique. # Performance Requirements: - The function must efficiently handle up to 100 concurrent fetch operations. # Hint - You can use `asyncio.create_task` to start fetching data in parallel. - Use `await` to collect results from the `Future` objects.","solution":"import asyncio from typing import List, Tuple async def fetch_single_source(data: str, delay: int) -> str: Fetches data after a delay. await asyncio.sleep(delay) print(f\\"{data} fetched in {delay} seconds\\") return data async def fetch_data_in_parallel(sources: List[Tuple[str, int]]) -> List[str]: Fetches data from multiple sources in parallel and returns the results as they are fetched. # Create tasks for each source tasks = [asyncio.create_task(fetch_single_source(data, delay)) for data, delay in sources] # Collect results as they are available results = [] for task in asyncio.as_completed(tasks): result = await task results.append(result) return results"},{"question":"# Question: Advanced Color Palettes with Seaborn **Objective**: Demonstrate your understanding of seaborn color palettes by generating various types of color palettes and utilizing them in plots. **Task**: 1. Implement a function `create_custom_palette()` that returns a customized seaborn color palette. 2. Implement a function `plot_with_custom_palette()` that: - Uses a context manager to temporarily set the custom palette as the default. - Generates a scatter plot using sample data with different colors for distinct categories. 3. Implement a function `convert_palette_to_hex()` that converts a seaborn color palette to hex codes. **Function Specifications**: 1. **`create_custom_palette()`**: - **Input**: None - **Output**: A seaborn color palette object containing at least 5 colors 2. **`plot_with_custom_palette()`**: - **Input**: A list of x-coordinates, a list of y-coordinates, and a list of categories for the scatter plot - **Output**: Generates and displays a scatter plot using the custom palette in a context manager 3. **`convert_palette_to_hex()`**: - **Input**: A seaborn color palette object - **Output**: A list of hex codes representing the colors in the palette **Constraints**: - Use at least one predefined seaborn categorical color palette in `create_custom_palette()`. - In `plot_with_custom_palette()`, ensure that the context manager correctly reverts the default palette after the plot is generated. **Example**: ```python def create_custom_palette(): # Example: Combining two different seaborn palettes palette1 = sns.color_palette(\\"Set3\\", 3) palette2 = sns.color_palette(\\"husl\\", 2) return palette1 + palette2 def plot_with_custom_palette(x, y, category): custom_palette = create_custom_palette() with sns.color_palette(custom_palette): sns.scatterplot(x=x, y=y, hue=category, palette=custom_palette) plt.show() def convert_palette_to_hex(palette): return palette.as_hex() # Sample Data x = list(range(10)) y = [i * 2 for i in range(10)] category = [str(i % 5) for i in range(10)] # Using the functions plot_with_custom_palette(x, y, category) hex_codes = convert_palette_to_hex(create_custom_palette()) print(hex_codes) ``` # Note: Ensure that your code uses appropriate seaborn functions and handles errors gracefully if incorrect inputs are provided.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette(): Creates a custom seaborn color palette by combining predefined palettes. Returns: custom_palette (list): List of colors in the palette. palette1 = sns.color_palette(\\"Set3\\", 3) palette2 = sns.color_palette(\\"husl\\", 2) custom_palette = palette1 + palette2 return custom_palette def plot_with_custom_palette(x, y, category): Generates and displays a scatter plot using the custom seaborn color palette. Args: x (list): List of x-coordinates for the scatter plot. y (list): List of y-coordinates for the scatter plot. category (list): List of categories corresponding to each (x, y) pair for coloring. custom_palette = create_custom_palette() with sns.color_palette(custom_palette): sns.scatterplot(x=x, y=y, hue=category, palette=custom_palette) plt.show() def convert_palette_to_hex(palette): Converts a seaborn color palette to hex codes. Args: palette (list): Seaborn color palette object. Returns: hex_palette (list): List of hex codes for each color in the palette. return palette.as_hex()"},{"question":"# Problem: Task Processing with Asyncio Queues You are tasked with creating an efficient task processing system using `asyncio.Queue` and its variants (`PriorityQueue` and `LifoQueue`) in Python. The system should handle tasks with varying priorities and processing times. Your goal is to implement two types of queues, process tasks concurrently, and handle queue exceptions appropriately. Requirements: 1. **Queue Initialization** - Create two queues: - A `PriorityQueue` for tasks with priorities. - A `LifoQueue` for general tasks without specific priority. - Both queues should have a maximum size of 5. 2. **Task Functions** - Implement an async function `add_task_with_priority(priority: int, task: str)` that adds a task to the `PriorityQueue`. If the queue is full, it should wait until a slot is available. - Implement an async function `add_task(task: str)` that adds a task to the `LifoQueue`. If the queue is full, it should wait until a slot is available. - Implement an async function `process_tasks(worker_name: str)` that processes tasks from both queues. Tasks from `PriorityQueue` should be processed before tasks from the `LifoQueue`. Use `await asyncio.sleep()` to simulate task processing time (task string length in seconds). 3. **Main Function** - Create instances of both queues. - Add tasks with various priorities and general tasks to their respective queues. - Create and start multiple worker tasks to process the tasks concurrently. - Ensure all tasks are processed and handle task completion using `task_done()`. - Ensure to gracefully handle any `QueueEmpty` and `QueueFull` exceptions. Input: No user input required. Output: Tasks processed should be printed with the format: ``` worker-<worker_name> processed <task> from PriorityQueue worker-<worker_name> processed <task> from LifoQueue ``` Constraints: - The `PriorityQueue` entries are tuples (priority_number, task) where lower numbers indicate higher priority. - The `LifoQueue` entries are simple task strings. - Maximum size for both queues is 5. Example: ```python import asyncio import random async def add_task_with_priority(queue, priority, task): await asyncio.sleep(random.uniform(0.1, 0.5)) # Simulating arbitrary delay await queue.put((priority, task)) async def add_task(queue, task): await asyncio.sleep(random.uniform(0.1, 0.5)) # Simulating arbitrary delay await queue.put(task) async def process_tasks(pqueue, lqueue, worker_name): while True: try: if not pqueue.empty(): priority, task = await pqueue.get() await asyncio.sleep(len(task)) print(f\'worker-{worker_name} processed {task} from PriorityQueue\') pqueue.task_done() elif not lqueue.empty(): task = await lqueue.get() await asyncio.sleep(len(task)) print(f\'worker-{worker_name} processed {task} from LifoQueue\') lqueue.task_done() else: await asyncio.sleep(0.1) # Avoid busy waiting except asyncio.QueueEmpty: break async def main(): pqueue = asyncio.PriorityQueue(5) lqueue = asyncio.LifoQueue(5) tasks = [ add_task_with_priority(pqueue, 1, \'high-priority-task-1\'), add_task_with_priority(pqueue, 2, \'medium-priority-task-1\'), add_task_with_priority(pqueue, 1, \'high-priority-task-2\'), add_task(lqueue, \'general-task-1\'), add_task(lqueue, \'general-task-2\'), add_task(lqueue, \'general-task-3\'), ] workers = [process_tasks(pqueue, lqueue, f\'A{i}\') for i in range(3)] await asyncio.gather(*tasks) await pqueue.join() await lqueue.join() for worker in workers: worker.cancel() asyncio.run(main()) ``` **Your implementation should contain the functions: `add_task_with_priority`, `add_task`, `process_tasks`, and `main`.**","solution":"import asyncio async def add_task_with_priority(queue, priority, task): Add a task with a given priority to the PriorityQueue. await queue.put((priority, task)) async def add_task(queue, task): Add a task to the LifoQueue. await queue.put(task) async def process_tasks(pqueue, lqueue, worker_name): Process tasks from both queues, giving priority to the PriorityQueue. while True: try: if not pqueue.empty(): priority, task = await pqueue.get() await asyncio.sleep(len(task)) print(f\'worker-{worker_name} processed {task} from PriorityQueue\') pqueue.task_done() elif not lqueue.empty(): task = await lqueue.get() await asyncio.sleep(len(task)) print(f\'worker-{worker_name} processed {task} from LifoQueue\') lqueue.task_done() else: await asyncio.sleep(0.1) # Avoid busy waiting except asyncio.QueueEmpty: break async def main(): pqueue = asyncio.PriorityQueue(5) lqueue = asyncio.LifoQueue(5) task_lists = [ (pqueue, 1, \'high-priority-task-1\'), (pqueue, 3, \'low-priority-task-1\'), (pqueue, 2, \'medium-priority-task-1\'), (lqueue, \'general-task-1\'), (lqueue, \'general-task-2\'), (lqueue, \'general-task-3\'), ] tasks = [] for queue, *args in task_lists: if queue is pqueue: tasks.append(add_task_with_priority(queue, *args)) else: tasks.append(add_task(queue, *args)) workers = [process_tasks(pqueue, lqueue, f\'A{i}\') for i in range(3)] await asyncio.gather(*tasks) await pqueue.join() await lqueue.join() for worker in workers: worker.cancel() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Objective:** Design an Abstract Base Class for a plugin system in an image editing application. Plugins are used to apply different filters to images. Your task is to define the abstract base class and ensure that each plugin correctly implements the required methods. **Requirements:** 1. Create an abstract base class named `ImageFilterPlugin`. 2. It must contain the following abstract methods: - `apply_filter(self, image: bytes) -> bytes`: A method to apply a filter to the image. The `image` parameter is a binary representation of an image, and the method should return the processed image (also in binary format). - `get_filter_name(self) -> str`: A method to return the name of the filter as a string. 3. Use the `ABCMeta` and `abstractmethod` decorators from the `abc` module. To test your implementation, create at least two concrete subclasses: 1. `GrayscaleFilterPlugin`: Implements `apply_filter` to convert the image to grayscale and `get_filter_name` to return \\"Grayscale\\". 2. `BlurFilterPlugin`: Implements `apply_filter` to apply a blur effect and `get_filter_name` to return \\"Blur\\". **Constraints:** - You can assume the utility functions to apply a grayscale or blur effect are provided as `convert_to_grayscale(image: bytes) -> bytes` and `apply_blur_effect(image: bytes) -> bytes`. **Expected Input and Output:** ```python from abc import ABC, abstractmethod class ImageFilterPlugin(ABC): @abstractmethod def apply_filter(self, image: bytes) -> bytes: pass @abstractmethod def get_filter_name(self) -> str: pass class GrayscaleFilterPlugin(ImageFilterPlugin): def apply_filter(self, image: bytes) -> bytes: return convert_to_grayscale(image) def get_filter_name(self) -> str: return \\"Grayscale\\" class BlurFilterPlugin(ImageFilterPlugin): def apply_filter(self, image: bytes) -> bytes: return apply_blur_effect(image) def get_filter_name(self) -> str: return \\"Blur\\" # Provided utility functions for testing def convert_to_grayscale(image: bytes) -> bytes: # Placeholder for grayscale conversion logic return image def apply_blur_effect(image: bytes) -> bytes: # Placeholder for blur effect logic return image # Example usage image_data = b\'x89PNGrn...\' # Example binary image data gray_plugin = GrayscaleFilterPlugin() blur_plugin = BlurFilterPlugin() print(gray_plugin.get_filter_name()) # Output: \\"Grayscale\\" print(blur_plugin.get_filter_name()) # Output: \\"Blur\\" processed_image_gray = gray_plugin.apply_filter(image_data) processed_image_blur = blur_plugin.apply_filter(image_data) ``` **Note:** You may assume the implementation of `convert_to_grayscale` and `apply_blur_effect` for testing purposes but focus on creating the abstract base class and its concrete implementations correctly.","solution":"from abc import ABC, abstractmethod class ImageFilterPlugin(ABC): @abstractmethod def apply_filter(self, image: bytes) -> bytes: pass @abstractmethod def get_filter_name(self) -> str: pass class GrayscaleFilterPlugin(ImageFilterPlugin): def apply_filter(self, image: bytes) -> bytes: return convert_to_grayscale(image) def get_filter_name(self) -> str: return \\"Grayscale\\" class BlurFilterPlugin(ImageFilterPlugin): def apply_filter(self, image: bytes) -> bytes: return apply_blur_effect(image) def get_filter_name(self) -> str: return \\"Blur\\" # Provided utility functions for testing def convert_to_grayscale(image: bytes) -> bytes: # Placeholder for grayscale conversion logic return image # Returning the same image for illustration purposes def apply_blur_effect(image: bytes) -> bytes: # Placeholder for blur effect logic return image # Returning the same image for illustration purposes # Example usage image_data = b\'x89PNGrn...\' # Example binary image data gray_plugin = GrayscaleFilterPlugin() blur_plugin = BlurFilterPlugin() print(gray_plugin.get_filter_name()) # Output: \\"Grayscale\\" print(blur_plugin.get_filter_name()) # Output: \\"Blur\\" processed_image_gray = gray_plugin.apply_filter(image_data) processed_image_blur = blur_plugin.apply_filter(image_data)"},{"question":"Objective The goal of this coding assessment is to evaluate your understanding of the Python `operator` module and your ability to apply its functions to solve a practical problem. Problem Statement You are given a list of dictionaries, where each dictionary represents a person\'s data with the following structure: ```python [ {\'name\': \'John\', \'age\': 25, \'score\': 70}, {\'name\': \'Jane\', \'age\': 22, \'score\': 60}, {\'name\': \'Doe\', \'age\': 23, \'score\': 80}, ... ] ``` Your task is to: 1. Sort the list by the `age` attribute in ascending order. 2. Filter out all persons whose `score` is below 65. 3. Increment the `score` of each remaining person by 5%. 4. Extract the `name` and `score` attributes of the remaining persons and return a list of tuples in the form `[(name1, score1), (name2, score2), ...]`. Requirements - The implementation should utilize functions from the `operator` module wherever applicable. - You are allowed to use standard Python libraries such as `functools` and `operator`. - You must ensure that your solution is efficient with a time complexity of O(n log n) for sorting and O(n) for filtering and updating. Constraints - The list contains at most 10^5 elements. - Each dictionary is guaranteed to contain the keys `name`, `age`, and `score`. Function Signature ```python from typing import List, Dict, Tuple def process_people(data: List[Dict[str, int]]) -> List[Tuple[str, float]]: # Your code here ``` Example ```python data = [ {\'name\': \'John\', \'age\': 25, \'score\': 70}, {\'name\': \'Jane\', \'age\': 22, \'score\': 60}, {\'name\': \'Doe\', \'age\': 23, \'score\': 80} ] print(process_people(data)) # Output: [(\'Doe\', 84.0), (\'John\', 73.5)] ``` Explanation 1. After sorting by age: `[{ \'name\': \'Jane\', \'age\': 22, \'score\': 60 }, { \'name\': \'Doe\', \'age\': 23, \'score\': 80 }, { \'name\': \'John\', \'age\': 25, \'score\': 70 }]`. 2. Filtering out those with score < 65: `[{ \'name\': \'Doe\', \'age\': 23, \'score\': 80 }, { \'name\': \'John\', \'age\': 25, \'score\': 70 }]`. 3. Incrementing scores by 5%: `[{ \'name\': \'Doe\', \'score\': 84.0 }, { \'name\': \'John\', \'score\': 73.5 }]`. 4. Extracting `name` and `score`: `[(\'Doe\', 84.0), (\'John\', 73.5)]`. Notes - Use `operator.itemgetter` for sorting. - Use `operator.itemgetter` for extracting attributes. - Use arithmetic operations from the `operator` module for incrementing scores.","solution":"from typing import List, Dict, Tuple import operator def process_people(data: List[Dict[str, int]]) -> List[Tuple[str, float]]: # Sort the data by age in ascending order data.sort(key=operator.itemgetter(\'age\')) # Filter out all persons whose score is below 65 filtered_data = filter(lambda person: person[\'score\'] >= 65, data) # Increment the score of each remaining person by 5% updated_data = map(lambda person: (person[\'name\'], person[\'score\'] * 1.05), filtered_data) # Return a list of tuples with name and updated score return list(updated_data)"},{"question":"Suppose you are working with a dataset containing information about different species of flowers, including their sepal and petal dimensions. You are asked to create a visualization that allows for comparison between different species in terms of their sepal length and width. You also need to ensure that the plot is well-organized and fits within the available space. # Task 1. Load the iris dataset using seaborn\'s built-in function. 2. Create a scatter plot with `sepal_length` on the x-axis and `sepal_width` on the y-axis. 3. Use different colors to distinguish between different species. 4. Create subplots for each species to provide a more focused comparison. 5. Ensure the subplots are laid out efficiently within the figure. Constraints - You must use seaborn\'s `objects.Plot` functionalities. - The figure should be of size (8, 8). - The layout should use the `constrained` engine to ensure better fitting of subplots. # Input There are no specific inputs as the iris dataset is built into seaborn. # Output A visual representation of the scatter plots with appropriately distinguished species and an organized layout of subplots within the figure. # Function Signature ```python def create_iris_scatter_plots(): import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Create the base plot p = so.Plot(iris, x=\\"sepal_length\\", y=\\"sepal_width\\", color=\\"species\\") # Create subplots for each species p = p.facet(\\"species\\", None).layout(engine=\\"constrained\\", size=(8, 8)) # Show the plot p.show() plt.show() ```","solution":"def create_iris_scatter_plots(): import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Create the base plot p = so.Plot(iris, x=\\"sepal_length\\", y=\\"sepal_width\\", color=\\"species\\") # Create subplots for each species p = p.facet(\\"species\\", None).layout(engine=\\"constrained\\", size=(8, 8)) # Show the plot p.show() plt.show()"},{"question":"# Fault-Tolerant Distributed Training with PyTorch You are tasked with setting up a distributed training job for a neural network model using PyTorch. The training job should be fault-tolerant, meaning it should be capable of handling node failures and restart the process a specified number of times. Question: Implement a Python function `configure_distributed_training` that prepares and returns the command string to start a PyTorch distributed training job meeting the specified requirements. # Function Signature: ```python def configure_distributed_training( num_nodes: int, trainers_per_node: int, max_restarts: int, job_id: str, host_node_addr: str, is_standalone: bool = False ) -> str: pass ``` # Input: - `num_nodes (int)`: Total number of nodes participating in the distributed training. - `trainers_per_node (int)`: Number of trainers to run on each node. - `max_restarts (int)`: Number of allowable restarts in case of node failures. - `job_id (string)`: A unique identifier for the job. - `host_node_addr (string)`: The address (and optionally the port number) of the host node for the rendezvous backend. - `is_standalone (boolean)`: Flag indicating whether to run in standalone mode with a sidecar rendezvous backend (default: False). # Output: - `command (str)`: The command string to launch the fault-tolerant job using `torchrun`. # Constraints: - `num_nodes` should be greater than 0. - `trainers_per_node` should be greater than 0. - `max_restarts` should be non-negative. - `job_id` should be a non-empty string. - `host_node_addr` should be a valid address string. # Example: ```python configure_distributed_training( num_nodes=4, trainers_per_node=2, max_restarts=3, job_id=\\"training123\\", host_node_addr=\\"node1.example.com:29500\\" ) ``` # Expected Output: ``` \'torchrun --nnodes=4 --nproc-per-node=2 --max-restarts=3 --rdzv-id=training123 --rdzv-backend=c10d --rdzv-endpoint=node1.example.com:29500 YOUR_TRAINING_SCRIPT.py\' ``` # Additional Information: - For standalone mode, the command should include `--standalone` and exclude `--rdzv-id`, `--rdzv-endpoint`, and `--rdzv-backend` options. - Ensure the command is correctly formatted based on the mode (standalone or distributed). --- Implement the `configure_distributed_training` function to generate the required command string for a fault-tolerant distributed training job in PyTorch.","solution":"def configure_distributed_training( num_nodes: int, trainers_per_node: int, max_restarts: int, job_id: str, host_node_addr: str, is_standalone: bool = False ) -> str: Prepares and returns the command string to start a PyTorch distributed training job. Parameters: - num_nodes (int): Total number of nodes participating in the distributed training. - trainers_per_node (int): Number of trainers to run on each node. - max_restarts (int): Number of allowable restarts in case of node failures. - job_id (string): A unique identifier for the job. - host_node_addr (string): The address of the host node for the rendezvous backend. - is_standalone (boolean): Flag indicating whether to run in standalone mode (default: False). Returns: - command (str): The command string to launch the fault-tolerant job using torchrun. if num_nodes <= 0: raise ValueError(\\"The number of nodes must be greater than 0.\\") if trainers_per_node <= 0: raise ValueError(\\"The number of trainers per node must be greater than 0.\\") if max_restarts < 0: raise ValueError(\\"The number of max restarts must be non-negative.\\") if not job_id: raise ValueError(\\"The job ID must be a non-empty string.\\") if not host_node_addr: raise ValueError(\\"The host node address must be a valid address string.\\") base_command = \\"torchrun\\" if is_standalone: command = f\\"{base_command} --standalone --nnodes={num_nodes} --nproc-per-node={trainers_per_node} --max-restarts={max_restarts} YOUR_TRAINING_SCRIPT.py\\" else: command = (f\\"{base_command} --nnodes={num_nodes} --nproc-per-node={trainers_per_node} --max-restarts={max_restarts} \\" f\\"--rdzv-id={job_id} --rdzv-backend=c10d --rdzv-endpoint={host_node_addr} YOUR_TRAINING_SCRIPT.py\\") return command"},{"question":"**Coding Question:** # Tensor View Operations and Contiguity Check **Objective:** Implement a function that performs various operations to create tensor views and verifies certain properties such as data sharing and contiguity. **Function Signature:** ```python def tensor_view_operations(tensor: torch.Tensor) -> dict: pass ``` **Inputs:** - `tensor` (torch.Tensor): A 2D tensor of shape `(m, n)` with random values. **Outputs:** - `result` (dict): A dictionary with the following keys and expected elements: - `\'original_tensor\'`: The input tensor. - `\'reshaped_view\'`: A view of the tensor reshaped to shape `(n, m)`. - `\'transposed_view\'`: A view of the tensor transposed. - `\'is_data_shared\'`: A boolean indicating whether `\'reshaped_view\'` and `\'transposed_view\'` share the same underlying data as the original tensor. - `\'original_contiguous\'`: A boolean indicating whether the original tensor is contiguous. - `\'reshaped_contiguous\'`: A boolean indicating whether `\'reshaped_view\'` is contiguous. - `\'transposed_contiguous\'`: A boolean indicating whether `\'transposed_view\'` is contiguous. - `\'transposed_contiguous_tensor\'`: A contiguous tensor created from `\'transposed_view\'`. **Constraints:** - Tensor operations should be efficient in terms of both memory and computation. - You should leverage PyTorch\'s view operations to avoid unnecessary data copying. **Example:** ```python import torch # Example input tensor t = torch.rand(3, 4) result = tensor_view_operations(t) print(result) ``` **Expected Output:** The output will be a dictionary with operation results and contiguity status. Example of the structure: ```python { \'original_tensor\': tensor([[...]]), \'reshaped_view\': tensor([[...]]), \'transposed_view\': tensor([[...]]), \'is_data_shared\': True, \'original_contiguous\': True, \'reshaped_contiguous\': True or False (depending on reshaping), \'transposed_contiguous\': False, \'transposed_contiguous_tensor\': tensor([...]) (with contiguous layout) } ``` **Notes:** - Ensure that the reshaped and transposed tensors share data with the original tensor. - Check for contiguity using `tensor.is_contiguous()`. - Convert `transposed_view` to a contiguous tensor only if it\'s non-contiguous.","solution":"import torch def tensor_view_operations(tensor: torch.Tensor) -> dict: reshaped_view = tensor.view(tensor.shape[1], tensor.shape[0]) transposed_view = tensor.t() # Transpose the tensor is_data_shared = reshaped_view.storage().data_ptr() == tensor.storage().data_ptr() and transposed_view.storage().data_ptr() == tensor.storage().data_ptr() original_contiguous = tensor.is_contiguous() reshaped_contiguous = reshaped_view.is_contiguous() transposed_contiguous = transposed_view.is_contiguous() transposed_contiguous_tensor = transposed_view.contiguous() result = { \'original_tensor\': tensor, \'reshaped_view\': reshaped_view, \'transposed_view\': transposed_view, \'is_data_shared\': is_data_shared, \'original_contiguous\': original_contiguous, \'reshaped_contiguous\': reshaped_contiguous, \'transposed_contiguous\': transposed_contiguous, \'transposed_contiguous_tensor\': transposed_contiguous_tensor } return result"},{"question":"**Question: Color Palette Manipulation with Seaborn** You are given a dataset and need to create different visualizations using various color palettes generated by Seaborn\'s `mpl_palette` function. Implement the following functions as specified: 1. **Function: `generate_continuous_palette`** Generate a continuous color palette based on a specified continuous colormap and return it either as a list of discrete colors or as a `matplotlib.colors.ListedColormap` object. **Input:** - `colormap` (str): Name of the continuous colormap (e.g., \\"viridis\\"). - `num_colors` (int): Number of discrete colors to sample from the colormap. - `as_cmap` (bool): Boolean flag indicating whether to return as a continuous colormap object. **Output:** - If `as_cmap` is `False`, return a list of discrete colors. - If `as_cmap` is `True`, return a `matplotlib.colors.ListedColormap` object. **Example:** ```python palette = generate_continuous_palette(\\"viridis\\", 5, False) print(palette) # Expected output: List of 5 discrete colors. cmap = generate_continuous_palette(\\"viridis\\", 5, True) print(type(cmap)) # Expected output: <class \'matplotlib.colors.ListedColormap\'> ``` 2. **Function: `generate_qualitative_palette`** Generate a qualitative color palette based on a specified qualitative colormap and return it as a list of discrete colors. **Input:** - `colormap` (str): Name of the qualitative colormap (e.g., \\"Set2\\"). - `num_colors` (int): Number of discrete colors to sample from the colormap. **Output:** - List of discrete colors. **Constraints:** - If the requested number of colors exceeds the available distinct colors in the qualitative colormap, the palette should contain only the distinct colors available. **Example:** ```python palette = generate_qualitative_palette(\\"Set2\\", 8) print(palette) # Expected output: List of 8 colors (or fewer if colormap has fewer distinct colors). ``` **Note:** - You may need to consult additional documentation or references to fully implement the functions. - Ensure your code handles edge cases, such as requesting more colors than available in a qualitative colormap. - You can use the seaborn library\'s functionalities to complete this task. Happy coding!","solution":"import matplotlib import seaborn as sns def generate_continuous_palette(colormap, num_colors, as_cmap): Generate a continuous color palette. Parameters: colormap (str): Name of the continuous colormap (e.g., \\"viridis\\"). num_colors (int): Number of discrete colors to sample from the colormap. as_cmap (bool): Boolean flag indicating whether to return as a continuous colormap object. Returns: List of colors if as_cmap is False, otherwise a ListedColormap object. colors = sns.color_palette(colormap, num_colors) if as_cmap: return matplotlib.colors.ListedColormap(colors) return colors def generate_qualitative_palette(colormap, num_colors): Generate a qualitative color palette. Parameters: colormap (str): Name of the qualitative colormap (e.g., \\"Set2\\"). num_colors (int): Number of discrete colors to sample from the colormap. Returns: List of discrete colors. colors = sns.color_palette(colormap) # If more colors are requested than available, only return available colors. if num_colors > len(colors): return colors return colors[:num_colors]"},{"question":"Background You are provided with a dataset containing information about house prices in a city. Your task is to build a machine learning model to predict the prices of houses based on various features using scikit-learn. Dataset Description The dataset consists of the following columns: - `square_feet`: The size of the house in square feet. - `num_bedrooms`: The number of bedrooms in the house. - `num_bathrooms`: The number of bathrooms in the house. - `location_rating`: A rating of the house\'s location (scale of 1 to 10). - `house_price`: The price of the house in dollars. Task Implement a function `train_and_evaluate_model` that takes a file path to the dataset and performs the following steps: 1. Load the dataset from the given file path. 2. Split the dataset into training and testing sets (80% training, 20% testing). 3. Preprocess the data (handle any missing values if necessary). 4. Train at least three different supervised learning models using scikit-learn (e.g., Linear Regression, Decision Tree, Random Forest). 5. Evaluate the models based on their mean squared error (MSE) on the test set. 6. Perform hyperparameter tuning on one of the models to improve its performance using GridSearchCV. 7. Return the model that achieves the lowest MSE after hyperparameter tuning. Function Signature ```python def train_and_evaluate_model(file_path: str) -> object: pass ``` Constraints - The dataset file format is CSV. - You may use scikit-learn\'s built-in functions for data splitting, preprocessing, model training, and evaluation. - Ensure your code is well-documented with comments explaining each step. Example ```python # Example usage: best_model = train_and_evaluate_model(\\"house_prices.csv\\") print(best_model) ``` Notes - The function should handle any preprocessing steps necessary (e.g., imputation of missing values). - Make sure to include the necessary imports for scikit-learn functions and classes. Good luck, and happy coding!","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error from sklearn.impute import SimpleImputer def train_and_evaluate_model(file_path: str) -> object: # Load the dataset data = pd.read_csv(file_path) # Split the dataset into features and target variable X = data.drop(columns=\'house_price\') y = data[\'house_price\'] # Split the dataset into training and testing sets (80% training, 20% testing) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Preprocess the data (handle any missing values using SimpleImputer) imputer = SimpleImputer(strategy=\'mean\') X_train = imputer.fit_transform(X_train) X_test = imputer.transform(X_test) # Initialize the models models = { \'Linear Regression\': LinearRegression(), \'Decision Tree\': DecisionTreeRegressor(random_state=42), \'Random Forest\': RandomForestRegressor(random_state=42) } # Train the models and evaluate them results = {} for name, model in models.items(): model.fit(X_train, y_train) predictions = model.predict(X_test) mse = mean_squared_error(y_test, predictions) results[name] = mse # Perform hyperparameter tuning on the best model (Random Forest in this case) param_grid = { \'n_estimators\': [50, 100, 200], \'max_depth\': [None, 10, 20, 30] } best_model_name = min(results, key=results.get) if best_model_name == \'Random Forest\': grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring=\'neg_mean_squared_error\') grid_search.fit(X_train, y_train) best_model = grid_search.best_estimator_ else: best_model = models[best_model_name] return best_model"},{"question":"<|Analysis Begin|> The `timeit` module in Python provides a fundamental way to measure the execution time of small code snippets. The primary purpose of this module is to avoid common traps for measuring execution times by ensuring accurate and consistent timing. It offers both a command-line interface and a callable interface within Python. The module includes functionalities such as: 1. `timeit.timeit()`: Times the execution of code snippets. 2. `timeit.repeat()`: Calls `timeit()` multiple times and returns a list of results. 3. `timeit.default_timer()`: Provides the default timer. 4. `timeit.Timer`: A class designed to facilitate timing of code through methods like `timeit()`, `autorange()`, and `repeat()`. Key points from the documentation: - The `timeit.timeit()` function allows timing a statement for a specified number of executions. - The `timeit.repeat()` function is similar to `timeit.timeit()` but repeats the timing multiple times. - The `timeit.default_timer()` method provides the default timer function. - The `timeit.Timer` class provides a more flexible way to time code execution. - The command-line interface offers various options such as number of executions, number of repeats, setup statements, unit specification, and verbosity. Overall, the `timeit` module is a comprehensive tool for precisely measuring execution time for code snippets, which is crucial for performance analysis and optimization. <|Analysis End|> <|Question Begin|> **Question: Evaluating and Comparing the Performance of Different List Operations** You are required to evaluate and compare the performance of different list operations in Python using the `timeit` module. Specifically, you will compare the performance of three different methods for duplicating the elements of a list: 1. Using a list comprehension. 2. Using the `extend` method. 3. Using the `*` operator (slicing method). Your task is to implement a function `compare_list_duplication_methods()` that evaluates the performance of the three methods mentioned above using the `timeit` module and returns the fastest method. The function should: 1. Create a list of integers from 0 to 999. 2. Measure the execution time of duplicating this list using list comprehension, the `extend` method, and the slicing method. 3. Return a string indicating which method was the fastest: `\\"list comprehension\\"`, `\\"extend method\\"`, or `\\"slicing method\\"`. **Function Signature:** ```python def compare_list_duplication_methods() -> str: pass ``` **Constraints:** - The timing should be conducted within 1000 repetitions for each method. - Use `setup` code in `timeit` to ensure a fair comparison by initializing the original list before each timing. **Example:** ```python result = compare_list_duplication_methods() print(result) # This should print the fastest method: \\"list comprehension\\", \\"extend method\\", or \\"slicing method\\". ``` **Notes:** - Consider using the `timeit.timeit()` function with appropriate `setup` strings to initialize the list before each method is timed. - Ensure that the results from the `timeit` measurements accurately reflect the performance for repeating the operations multiple times. Test your implementation to ensure correctness and performance.","solution":"import timeit def compare_list_duplication_methods() -> str: setup_code = \\"original_list = list(range(1000))\\" # Timing list comprehension method list_comp_code = duplicated_list = [x for x in original_list] list_comp_time = timeit.timeit(stmt=list_comp_code, setup=setup_code, number=1000) # Timing extend method extend_method_code = duplicated_list = [] duplicated_list.extend(original_list) extend_time = timeit.timeit(stmt=extend_method_code, setup=setup_code, number=1000) # Timing slicing method slicing_method_code = duplicated_list = original_list[:] slicing_time = timeit.timeit(stmt=slicing_method_code, setup=setup_code, number=1000) # Determine the fastest method times = { \\"list comprehension\\": list_comp_time, \\"extend method\\": extend_time, \\"slicing method\\": slicing_time } fastest_method = min(times, key=times.get) return fastest_method # To see the result directly (if needed) # result = compare_list_duplication_methods() # print(result)"},{"question":"Objective The goal of this assessment is to evaluate your understanding of Python generators and your ability to manipulate them effectively. Task Implement a function `sum_of_squares(n)` that generates and returns the sum of the squares of the first `n` natural numbers using a generator. Details 1. Define a generator function `generate_squares(n)` that yields the square of each number from 1 to `n`. 2. Use this generator in your `sum_of_squares(n)` function to compute the sum of these squares. Function Signatures ```python def generate_squares(n: int): A generator function that yields the square of numbers from 1 to n. Parameters: n (int): The number of terms to generate. Yields: int: The square of the current number in the sequence. def sum_of_squares(n: int) -> int: Computes the sum of the squares of the first n natural numbers. Parameters: n (int): The number of terms to consider. Returns: int: The sum of the squares of the first n natural numbers. ``` # Constraints - `n` will be a positive integer (1 <= n <= 10^6). - Your implementation should be efficient in terms of both time and space complexity. # Example Usage ```python >>> def generate_squares(n): ... for i in range(1, n + 1): ... yield i ** 2 ... >>> def sum_of_squares(n): ... return sum(generate_squares(n)) ... >>> sum_of_squares(5) 55 >>> sum_of_squares(10) 385 ``` Additional Requirements - Ensure that your solution efficiently handles large values of `n` within the given constraints. - You are expected to use generator functionality (i.e., properly use the `yield` keyword in `generate_squares`). # Testing Your implementation will be tested with various values of `n` to ensure correctness and performance.","solution":"def generate_squares(n): A generator function that yields the square of numbers from 1 to n. Parameters: n (int): The number of terms to generate. Yields: int: The square of the current number in the sequence. for i in range(1, n + 1): yield i ** 2 def sum_of_squares(n): Computes the sum of the squares of the first n natural numbers. Parameters: n (int): The number of terms to consider. Returns: int: The sum of the squares of the first n natural numbers. return sum(generate_squares(n))"},{"question":"# Question: Implement a Set Operation Utility Implement a function called `set_operations` that performs a series of set operations based on the given commands. The function should take a list of commands, execute the corresponding set/frozenset operations, and return the final state of the set/frozenset after all operations are performed. The function should handle all errors gracefully and raise appropriate exceptions where necessary. Function Signature ```python def set_operations(commands: list) -> set: pass ``` Inputs - `commands (list)`: A list where each element is a tuple. The first element of each tuple is a string representing the operation to be performed, and the rest of the elements are the arguments required by that operation. Outputs - `set`: The final state of the set after performing all the specified operations. Constraints - Only valid set operations should be executed: `add`, `discard`, `pop`, `clear`, and `contains`. - If an invalid operation is specified, raise a `ValueError` with the message \\"Invalid operation\\". - Handle `TypeError` and `MemoryError` exceptions as specified in the documentation. - Assume that the commands list is non-empty and all operations are performed on a set, not a frozenset. Example ```python commands = [ (\\"add\\", 1), (\\"add\\", 2), (\\"contains\\", 2), (\\"discard\\", 1), (\\"add\\", 3), (\\"pop\\",), ] output = set_operations(commands) print(output) # Output should be the final state of the set ``` Implementation Details - Use the provided APIs such as `PySet_Add`, `PySet_Discard`, `PySet_Pop`, `PySet_Clear` where they are applicable. - Ensure to follow proper error handling conventions outlined in the documentation.","solution":"def set_operations(commands: list) -> set: Executes a series of set operations based on the given commands. Parameters: commands (list): A list of tuples, where each tuple represents a command and its arguments. Returns: set: The final state of the set after all operations are performed. s = set() for command in commands: operation = command[0] try: if operation == \\"add\\": s.add(command[1]) elif operation == \\"discard\\": s.discard(command[1]) elif operation == \\"pop\\": s.pop() elif operation == \\"clear\\": s.clear() elif operation == \\"contains\\": if command[1] in s: print(f\\"{command[1]} is in the set\\") else: print(f\\"{command[1]} is not in the set\\") else: raise ValueError(\\"Invalid operation\\") except TypeError as e: print(f\\"TypeError: {e}\\") except MemoryError as e: print(f\\"MemoryError: {e}\\") except KeyError as e: print(f\\"KeyError: {e}\\") return s"},{"question":"Objective Implement a set of functions to manipulate an array using the `array` module in Python. This question tests your understanding of array initialization, manipulation, and conversion between different data types. Task 1. **Initialization Function:** Write a function `initialize_array(arr_type: str, values: list) -> array.array` that: - Takes a type code (as a string) and a list of initial values. - Returns a new array of the specified type initialized with the given values. 2. **Manipulate Array Function:** Write a function `manipulate_array(arr: array.array, operations: list) -> array.array` that: - Takes an existing array and a list of operation tuples. - Each tuple contains an operation name and the necessary parameters. - The function must perform the following operations: * `\'append\'`: Add a single value to the end of the array. * `\'extend\'`: Add values from another iterable to the end of the array. * `\'insert\'`: Insert a value at a specific index. * `\'remove\'`: Remove the first occurrence of a value. * `\'pop\'`: Remove and return a value at a specific index. * `\'reverse\'`: Reverse the array in place. - Raise a `ValueError` for any unknown operation. - Return the modified array. 3. **Convert Array Function:** Write a function `convert_array(arr: array.array, target_type: str) -> list` that: - Takes an array and a target type string (`\\"list\\"`, `\\"bytes\\"`, `\\"unicode\\"`). - Converts the array to the specified type and returns the converted value. - For converting to `\\"unicode\\"`, assume the array\'s type code is `\'u\'`. Constraints: - You must use the `array` module for all array operations. - Handle all edge cases, such as empty arrays and invalid operations. - Ensure your code runs efficiently for large arrays (e.g., up to 10^6 elements). Input and Output Formats: 1. `initialize_array`: - **Input**: Type code `\'d\'`, values `[1.0, 2.0, 3.14]` - **Output**: `array(\'d\', [1.0, 2.0, 3.14])` 2. `manipulate_array`: - **Input**: Array `array(\'i\', [1, 2, 3, 4, 5])`, operations `[(\'append\', 6), (\'extend\', [7, 8, 9]), (\'remove\', 3), (\'pop\', 1), (\'reverse\',)]` - **Output**: `array(\'i\', [9, 8, 7, 6, 4, 2, 1])` 3. `convert_array`: - **Input**: Array `array(\'i\', [1, 2, 3, 4])`, target type `\\"list\\"` - **Output**: `[1, 2, 3, 4]` - **Input**: Array `array(\'u\', \'hello\')`, target type `\\"unicode\\"` - **Output**: `\'hello\'` # Submission Submit the implementation of the following functions: - `initialize_array` - `manipulate_array` - `convert_array` Ensure your code is well-documented, handles all edge cases, and is optimized for performance.","solution":"from array import array def initialize_array(arr_type: str, values: list) -> array: Initialize an array with a specified type and list of initial values. Returns an array of the specified type with the given values. return array(arr_type, values) def manipulate_array(arr: array, operations: list) -> array: Perform operations on the given array. Operations supported: - \'append\': Add a single value to the end of the array. - \'extend\': Add values from another iterable to the end of the array. - \'insert\': Insert a value at a specific index. - \'remove\': Remove the first occurrence of a value. - \'pop\': Remove and return a value at a specific index. - \'reverse\': Reverse the array in place. Raise ValueError for unknown operations. Returns the modified array. for operation in operations: op_name = operation[0] if op_name == \'append\': arr.append(operation[1]) elif op_name == \'extend\': arr.extend(operation[1]) elif op_name == \'insert\': arr.insert(operation[1], operation[2]) elif op_name == \'remove\': arr.remove(operation[1]) elif op_name == \'pop\': arr.pop(operation[1]) elif op_name == \'reverse\': arr.reverse() else: raise ValueError(f\\"Unknown operation: {op_name}\\") return arr def convert_array(arr: array, target_type: str) -> list: Convert an array to the specified type and return the converted value. Supported target types: - \\"list\\": converts to a list - \\"bytes\\": converts to bytes - \\"unicode\\": converts to unicode string (only for \'u\' type arrays) if target_type == \\"list\\": return list(arr) elif target_type == \\"bytes\\": return arr.tobytes() elif target_type == \\"unicode\\": if arr.typecode != \'u\': raise ValueError(\\"Array must have type code \'u\' to convert to unicode\\") return arr.tounicode() else: raise ValueError(f\\"Unknown target type: {target_type}\\")"},{"question":"# Objective: The objective of this coding question is to assess your ability to use the `warnings` module in Python effectively. You will implement a function that initiates warnings based on certain conditions and manage these warnings using filters and context managers. # Problem Statement: You are required to write a Python function named `warn_and_test` that takes two arguments: 1. `text` (str): A text input to be processed. 2. `trigger_warning` (bool): A flag indicating whether a warning should be triggered or not. The function should: 1. If `trigger_warning` is `True`, issue a `UserWarning` with the message \\"This is a user warning.\\" using the `warn()` function. 2. If the text is empty, it should issue a `RuntimeWarning` with the message \\"Empty text provided!\\". 3. Use the `filterwarnings()` function to suppress the `UserWarning` and only allow the `RuntimeWarning` to be displayed. 4. Use the `catch_warnings` context manager to capture all warnings that are issued. 5. Return a list of all captured warnings as strings. # Constraints: - You must use the `warnings.warn()`, `warnings.filterwarnings()`, and `warnings.catch_warnings()` functions. - The `text` input can be any string with a maximum length of 100 characters. If `text` exceeds 100 characters, raise a `ValueError` with the message \\"Input text is too long!\\". # Example: ```python from warnings import UserWarning, RuntimeWarning def warn_and_test(text, trigger_warning): pass # Your implementation here # Example usage: result = warn_and_test(\\"\\", True) print(result) # Output might be [\\"RuntimeWarning: Empty text provided!\\"] ``` # Notes: - Ensure that the list contains the warning messages in the order they were captured. - The function should handle both `UserWarning` and `RuntimeWarning`. If there are no warnings, return an empty list. # Tips: - Use `warnings.warn()` to issue warnings. - Use `warnings.catch_warnings(record=True)` to capture warnings. - Tailor the warnings filter to manage which warnings should be displayed or suppressed.","solution":"import warnings def warn_and_test(text, trigger_warning): # Check the length of text and raise ValueError if it\'s too long if len(text) > 100: raise ValueError(\\"Input text is too long!\\") # Capturing warnings within this context manager with warnings.catch_warnings(record=True) as w: # Clear the list of previously captured warnings warnings.simplefilter(\\"always\\") # Apply filters to suppress UserWarning warnings.filterwarnings(\\"ignore\\", category=UserWarning) # Trigger UserWarning if flag is set if trigger_warning: warnings.warn(\\"This is a user warning.\\", UserWarning) # Trigger RuntimeWarning if text is empty if not text: warnings.warn(\\"Empty text provided!\\", RuntimeWarning) # Return the list of captured warning messages return [str(warning.message) for warning in w]"},{"question":"**Question: Implement a Custom Multiclass Logistic Regression using scikit-learn Utilities** You are tasked with implementing a custom multiclass logistic regression classifier. Your implementation should utilize the utilities provided by scikit-learn where appropriate. This classifier should include the following features: 1. **Input Validation**: Ensure that the input data `X` and labels `y` are valid using `check_X_y` and `check_random_state`. 2. **Handling Sparse Matrices**: The classifier should handle both dense and sparse input matrices. Use `safe_sparse_dot` for matrix operations. 3. **Efficiency**: Ensure efficient computation for gradient descent by using appropriate linear algebra operations and avoid loops where possible. 4. **Prediction**: Implement a method to predict the class probabilities for given input data using the softmax function. 5. **Performance Requirements**: Ensure the implementation can handle large datasets efficiently. # Expected Functions and Classes: 1. **Class Definition**: Define a class `CustomLogisticRegression`. 2. **Initialization**: Initialize the model with parameters like `learning_rate`, `epochs`, and `random_state`. 3. **Fit Method**: Implement a `fit` method that trains the model using gradient descent. 4. **Predict_proba Method**: Implement a `predict_proba` method to return class probabilities. 5. **Predict Method**: Implement a `predict` method to return the predicted class labels. # Constraints: - Do **not** use pre-built scikit-learn classifiers (`LogisticRegression`, `SGDClassifier`). - Utilize `scipy.sparse` for handling sparse matrix operations. - Ensure the code is efficient with a computational complexity suitable for large datasets. - Assume input feature matrix `X` of shape (n_samples, n_features) and labels `y` of shape (n_samples,) with integer values for class labels. # Example Usage: ```python import numpy as np from scipy.sparse import csr_matrix # Example Data (Dense and Sparse) X_dense = np.array([[0.5, 1.2], [1.3, 3.4], [2.1, 0.8], [0.6, 2.7]]) y = np.array([0, 1, 0, 1]) X_sparse = csr_matrix(X_dense) # Model Initialization model = CustomLogisticRegression(learning_rate=0.1, epochs=1000, random_state=42) # Fitting Dense Data model.fit(X_dense, y) # Prediction for Dense Data print(\\"Predicted probabilities (Dense):\\", model.predict_proba(X_dense)) print(\\"Predicted classes (Dense):\\", model.predict(X_dense)) # Fitting Sparse Data model.fit(X_sparse, y) # Prediction for Sparse Data print(\\"Predicted probabilities (Sparse):\\", model.predict_proba(X_sparse)) print(\\"Predicted classes (Sparse):\\", model.predict(X_sparse)) ``` # Implementation Notes: - Implement softmax function for converting logits to probabilities. - Use `check_X_y` to validate input and labels. - Use `safe_sparse_dot` for efficient matrix multiplication that handles both dense and sparse matrices. - Initialize and update weights using a random state seeding from `check_random_state`. Implement the class `CustomLogisticRegression` following the specifications above.","solution":"import numpy as np from scipy.sparse import issparse from sklearn.utils import check_X_y, check_random_state, check_array from sklearn.utils.extmath import safe_sparse_dot, softmax class CustomLogisticRegression: def __init__(self, learning_rate=0.01, epochs=1000, random_state=None): self.learning_rate = learning_rate self.epochs = epochs self.random_state = random_state self.classes_ = None self.coef_ = None def fit(self, X, y): X, y = check_X_y(X, y, accept_sparse=True) random_state = check_random_state(self.random_state) n_samples, n_features = X.shape self.classes_ = np.unique(y) n_classes = len(self.classes_) self.coef_ = random_state.randn(n_features, n_classes) for epoch in range(self.epochs): logits = safe_sparse_dot(X, self.coef_) probabilities = softmax(logits) y_one_hot = self._one_hot_encode(y, n_classes) error = probabilities - y_one_hot gradient = safe_sparse_dot(X.T, error) / n_samples self.coef_ -= self.learning_rate * gradient def predict_proba(self, X): X = check_array(X, accept_sparse=True) logits = safe_sparse_dot(X, self.coef_) return softmax(logits) def predict(self, X): probabilities = self.predict_proba(X) return self.classes_[np.argmax(probabilities, axis=1)] def _one_hot_encode(self, y, num_classes): one_hot = np.zeros((y.size, num_classes)) for idx, val in enumerate(y): one_hot[idx, val] = 1 return one_hot"},{"question":"# Advanced Path Manipulation Utility **Objective:** Write a function `organize_paths(base_path: str, file_paths: list[str]) -> dict[str, list[str]]` that organizes given file paths into a dictionary based on their existence status and type (file or directory). This utility function will help in categorizing and managing file paths relative to a base directory. **Function Signature:** ```python def organize_paths(base_path: str, file_paths: list[str]) -> dict[str, list[str]]: ``` **Input:** - `base_path`: A string representing the base directory from which the relative paths will be calculated. This will always be an absolute path. - `file_paths`: A list of strings where each string represents a file or directory path. These paths can be either absolute or relative. **Output:** - The function must return a dictionary with three keys: - \\"files\\": A sorted list of paths (relative to the `base_path`) that refer to existing files. - \\"directories\\": A sorted list of paths (relative to the `base_path`) that refer to existing directories. - \\"non_existent\\": A sorted list of paths (relative to the `base_path`) that do not exist. **Constraints:** - All returned paths should be relative to the `base_path`. - Avoid using third-party libraries; only standard libraries are allowed. - Ensure that the function handles paths correctly on both Windows and Unix-based systems. **Example:** ```python base_path = \\"/home/user\\" file_paths = [ \\"/home/user/docs/report.docx\\", \\"docs/invoice.xlsx\\", \\"/home/user/pictures\\", \\"downloads/software.zip\\", \\"/home/user/music/song.mp3\\" ] expected_output = { \\"files\\": [\\"docs/invoice.xlsx\\", \\"downloads/software.zip\\", \\"music/song.mp3\\"], \\"directories\\": [\\"pictures\\"], \\"non_existent\\": [] } assert organize_paths(base_path, file_paths) == expected_output ``` **Instructions:** 1. Convert all paths to absolute paths. 2. Evaluate the existence and type (file or directory) of each path. 3. Return the dictionary with all relative paths sorted within each category. **Note:** - Use the `os.path` functions to handle path manipulations. - Ensure that the solution adheres to good coding practices, including error handling and code readability.","solution":"import os def organize_paths(base_path: str, file_paths: list[str]) -> dict[str, list[str]]: result = { \\"files\\": [], \\"directories\\": [], \\"non_existent\\": [] } for file_path in file_paths: abs_path = os.path.abspath(os.path.join(base_path, file_path)) if os.path.exists(abs_path): relative_path = os.path.relpath(abs_path, base_path) if os.path.isfile(abs_path): result[\\"files\\"].append(relative_path) elif os.path.isdir(abs_path): result[\\"directories\\"].append(relative_path) else: relative_path = os.path.relpath(abs_path, base_path) result[\\"non_existent\\"].append(relative_path) result[\\"files\\"].sort() result[\\"directories\\"].sort() result[\\"non_existent\\"].sort() return result"},{"question":"# Seaborn Advanced Plotting Challenge You are provided with a dataset and your task is to create a series of visualizations using the seaborn package to demonstrate your understanding of its capabilities, especially focusing on customization and usage of themes. Dataset For the purpose of this exercise, use the `anscombe` dataset which is built into seaborn. Requirements 1. **Load the Dataset:** Load the `anscombe` dataset using seaborn. 2. **Scatter Plot with Linear Fit:** Create a scatter plot of the dataset with linear regression lines (`PolyFit`) for each group (`dataset` column). Facet the plot to display the different groups side-by-side. 3. **Custom Theme:** Apply a custom theme that has: - White background for the axes (`axes.facecolor`: \\"w\\") - Slate gray edge color for the axes (`axes.edgecolor`: \\"slategray\\") - Line width of 4 for any lines plotted (`lines.linewidth`: 4) 4. **Matplotlib Style:** Additionally, apply the `fivethirtyeight` style from matplotlib for further customization. 5. **Combining Styles:** Use dictionary union syntax to combine a seaborn style (`whitegrid`) and plotting context (`talk`) to refine your theme. 6. **Update Default Theme:** Change the default theme for all seaborn `Plot` instances to use the `white` seaborn style. Code Implementation Implement the functionality as described above and ensure your code meets the following specifications: - **Function Name:** `visualize_and_customize_seaborn` - **Input:** None (the function should perform all operations internally) - **Output:** None (the function should display the plots) ```python import seaborn.objects as so from seaborn import load_dataset from matplotlib import style from seaborn import axes_style, plotting_context def visualize_and_customize_seaborn(): # Load the dataset anscombe = load_dataset(\\"anscombe\\") # Create a base Plot object with facets p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Apply the custom theme p.theme({\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 4}) # Apply the matplotlib \'fivethirtyeight\' style p.theme(style.library[\\"fivethirtyeight\\"]) # Combine seaborn styles using dictionary union syntax (Python 3.9+) p.theme({**axes_style(\\"whitegrid\\"), **plotting_context(\\"talk\\")}) # Update the default theme globally so.Plot.config.theme.update(axes_style(\\"white\\")) # Display the plot p.show() # Call the function to visualize and customize the seaborn plots visualize_and_customize_seaborn() ``` Constraints - **Python Version:** Ensure compatibility with Python 3.9 and higher for dictionary union syntax. - **Packages:** Use the seaborn package with the object-oriented interface (`seaborn.objects`) and matplotlib for additional styles. Performance - The function should efficiently load and process the dataset without unnecessary computations or excessive memory usage.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_and_customize_seaborn(): # Load the dataset anscombe = sns.load_dataset(\\"anscombe\\") # Set the Seaborn style sns.set_theme(context=\'talk\', style=\'whitegrid\', rc={\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 4}) # Apply the matplotlib \'fivethirtyeight\' style plt.style.use(\'fivethirtyeight\') # Create pointplot with linear regression lines and facets for each dataset group g = sns.lmplot(x=\'x\', y=\'y\', col=\'dataset\', hue=\'dataset\', data=anscombe, col_wrap=2, ci=None, palette=\'muted\', height=4, aspect=1, scatter_kws={\\"s\\": 50, \\"alpha\\": 1}) # Update the default theme globally sns.set_theme(style=\'white\') plt.show() # Call the function to visualize and customize the seaborn plots visualize_and_customize_seaborn()"},{"question":"# Custom Encoded Data Challenge Objective You are working with a custom data transmission format that requires binary data to be encoded using a combination of Base64 and ASCII85 encoding schemes. To ensure security, you also need to implement validation and consistency checks on the encoded data. Your task is to implement two functions: `custom_encode` and `custom_decode`. 1. `custom_encode(data: bytes) -> str`: This function takes a bytes-like input and encodes it in two steps: - First, it encodes the data using standard Base64 encoding. - Then, it encodes the Base64-encoded data using ASCII85 encoding. - The final encoded result should be a string. 2. `custom_decode(encoded_data: str) -> bytes`: This function takes a custom-encoded string and decodes it in two steps: - First, it decodes the data using ASCII85 decoding. - Then, it decodes the result from the first step using standard Base64 decoding. - The final decoded result should return to the original bytes-like input. Input and Output Formats - The `custom_encode` function receives a bytes-like object and returns a string. - The `custom_decode` function receives a string and returns a bytes-like object. Constraints - The input to `custom_encode` will always be valid bytes data. - The input to `custom_decode` will always be a valid custom-encoded string. - Your implementation should be efficient and handle cases up to 1 MB of input data. Examples ```python def custom_encode(data: bytes) -> str: # Your implementation here def custom_decode(encoded_data: str) -> bytes: # Your implementation here # Example Usage: original_data = b\\"Hello, this is a test.\\" encoded_data = custom_encode(original_data) print(encoded_data) # Output will be a custom-encoded ASCII85 string decoded_data = custom_decode(encoded_data) print(decoded_data) # Output should be b\\"Hello, this is a test.\\" ``` Notes - You may use the `base64` module\'s `b64encode`, `b64decode`, `a85encode`, and `a85decode` functions to implement the required functionality. - Ensure that you handle the conversion between bytes and string formats appropriately to meet the function signatures. Good luck, and pay attention to the details to ensure reliable encoding and decoding!","solution":"import base64 def custom_encode(data: bytes) -> str: Encodes data using a combination of Base64 and ASCII85 encoding. # First encode the data using Base64 base64_encoded_data = base64.b64encode(data) # Then encode the Base64-encoded data using ASCII85 ascii85_encoded_data = base64.a85encode(base64_encoded_data) return ascii85_encoded_data.decode(\'ascii\') def custom_decode(encoded_data: str) -> bytes: Decodes data from the custom encoding format (Base64 + ASCII85). # First decode the data using ASCII85 ascii85_decoded_data = base64.a85decode(encoded_data.encode(\'ascii\')) # Then decode the ASCII85-decoded data using Base64 base64_decoded_data = base64.b64decode(ascii85_decoded_data) return base64_decoded_data"},{"question":"# Python Coding Assessment Question: System Information and Monitoring Utility Objective Create a Python utility that provides a detailed report about the current Python session using various attributes and functions from the `sys` module. Instructions 1. **Function Definition**: Define a function `system_report()`, which collects and prints the following information: - **Python Version**: Retrieve the Python version using `sys.version_info` and print it in a human-readable format. - **Executable Path**: Retrieve the absolute path to the Python executable using `sys.executable`. - **Command Line Arguments**: Print the command line arguments used to invoke the current Python session using `sys.argv`. - **Recursion Limit**: Retrieve and print the current recursion limit using `sys.getrecursionlimit()`. - **Float and Integer Info**: Retrieve and print detailed information about floats and integers using `sys.float_info` and `sys.int_info`. - **Threading Info**: Retrieve and print information about the threading implementation using `sys.thread_info`. - **Startup Time**: Indicate whether the interpreter is in the process of shutting down using `sys.is_finalizing()`. 2. **Additional Capability**: Implement an additional feature to monitor and report the memory usage of the current session: - **Memory Usage**: Calculate and print the number of currently allocated memory blocks using `sys.getallocatedblocks()`. 3. **Output Requirements**: Ensure the output is clear and well-formatted, providing headings for each section of the report. 4. **Constraints**: - The function should handle any potential exceptions gracefully. - It should operate efficiently without significant impact on the performance of the Python session. Function Signature: ```python def system_report(): # Your code here ``` Example Output: ``` Python Version: Major: 3, Minor: 10, Micro: 5, Release Level: final, Serial: 0 Executable Path: Path: /usr/bin/python3 Command Line Arguments: [\'script.py\', \'arg1\', \'arg2\'] Recursion Limit: Current Limit: 3000 Float and Integer Info: Float Info: - epsilon: 2.220446049250313e-16 - dig: 15 ... Integer Info: - bits_per_digit: 30 - sizeof_digit: 4 ... Threading Info: - Implementation: pthread - Lock Type: semaphore - Version: None Startup Time: Is Finalizing: False Memory Usage: Allocated Memory Blocks: 54000 ``` **Note:** The output content and values are just for demonstration and will vary based on the actual execution environment. Your implementation will be evaluated based on correctness, clarity, format, and efficiency.","solution":"import sys def system_report(): try: # Python Version python_version = sys.version_info print(\\"Python Version:\\") print(f\\"Major: {python_version.major}, Minor: {python_version.minor}, Micro: {python_version.micro}, Release Level: {python_version.releaselevel}, Serial: {python_version.serial}n\\") # Executable Path executable_path = sys.executable print(\\"Executable Path:\\") print(f\\"Path: {executable_path}n\\") # Command Line Arguments command_line_arguments = sys.argv print(\\"Command Line Arguments:\\") print(command_line_arguments) print() # Recursion Limit recursion_limit = sys.getrecursionlimit() print(\\"Recursion Limit:\\") print(f\\"Current Limit: {recursion_limit}n\\") # Float and Integer Info float_info = sys.float_info int_info = sys.int_info print(\\"Float and Integer Info:\\") print(\\"Float Info:\\") print(f\\"- epsilon: {float_info.epsilon}\\") print(f\\"- dig: {float_info.dig}\\") print(f\\"- mant_dig: {float_info.mant_dig}\\") print(f\\"- max: {float_info.max}\\") print(f\\"- max_exp: {float_info.max_exp}\\") print(f\\"- min: {float_info.min}\\") print(f\\"- min_exp: {float_info.min_exp}\\") print(f\\"- radix: {float_info.radix}\\") print(f\\"- rounds: {float_info.rounds}\\") print(\\"Integer Info:\\") print(f\\"- bits_per_digit: {int_info.bits_per_digit}\\") print(f\\"- sizeof_digit: {int_info.sizeof_digit}\\") print() # Threading Info thread_info = sys.thread_info print(\\"Threading Info:\\") print(f\\"- Implementation: {thread_info.name}\\") print(f\\"- Lock Type: {thread_info.lock}\\") print(f\\"- Version: {thread_info.version}\\") print() # Startup Time is_finalizing = sys.is_finalizing() print(\\"Startup Time:\\") print(f\\"Is Finalizing: {is_finalizing}n\\") # Memory Usage allocated_blocks = sys.getallocatedblocks() print(\\"Memory Usage:\\") print(f\\"Allocated Memory Blocks: {allocated_blocks}n\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"You are tasked with implementing functions that mimic standard Python numerical operations by utilizing the provided C-API functions. Your implementation should demonstrate correct reference counting and error handling. Task 1: Implement the Addition Function Create a function named `py_add` that takes in two Python objects and returns their sum using the `PyNumber_Add` function from the C-API. ```python def py_add(py_obj1, py_obj2): Adds two Python objects using the PyNumber_Add C-API function. Parameters: py_obj1 (PyObject): First Python object. py_obj2 (PyObject): Second Python object. Returns: PyObject: The result of adding py_obj1 and py_obj2, or None if an error occurs. # Your implementation here pass ``` Task 2: Implement the Floor Division Function Create a function named `py_floor_divide` that takes in two Python objects and returns the floor of their division using the `PyNumber_FloorDivide` function from the C-API. ```python def py_floor_divide(py_obj1, py_obj2): Divides the first Python object by the second and returns the floor of the result using the PyNumber_FloorDivide C-API function. Parameters: py_obj1 (PyObject): First Python object. py_obj2 (PyObject): Second Python object. Returns: PyObject: The floor result of dividing py_obj1 by py_obj2, or None if an error occurs. # Your implementation here pass ``` Task 3: Implement the Bitwise AND Function Create a function named `py_bitwise_and` that takes in two Python objects and returns the result of their bitwise AND operation using the `PyNumber_And` function from the C-API. ```python def py_bitwise_and(py_obj1, py_obj2): Returns the bitwise AND of two Python objects using the PyNumber_And C-API function. Parameters: py_obj1 (PyObject): First Python object. py_obj2 (PyObject): Second Python object. Returns: PyObject: The result of the bitwise AND operation on py_obj1 and py_obj2, or None if an error occurs. # Your implementation here pass ``` Task 4: Implement the Power Function Create a function named `py_power` that takes in three Python objects and returns the result of raising the first object to the power of the second object, modulo the third object (or without modulo if the third object is `None`), using the `PyNumber_Power` function from the C-API. ```python def py_power(py_obj1, py_obj2, py_obj3=None): Raises the first Python object to the power of the second, optionally modulo the third, using the PyNumber_Power C-API function. Parameters: py_obj1 (PyObject): Base Python object. py_obj2 (PyObject): Exponent Python object. py_obj3 (PyObject or None): Modulo Python object, or None for no modulo. Returns: PyObject: The result of the power operation (with optional modulo), or None if an error occurs. # Your implementation here pass ``` Constraints: - Properly handle and decrement references to avoid memory leaks. - Return `None` in case of any errors. - You may assume all input objects are valid `PyObject` instances. # Notes - This challenge assesses your understanding of interfacing with Python\'s C-API for numeric operations. - Ensure you correctly manage reference counts and handle errors appropriately.","solution":"def py_add(py_obj1, py_obj2): Adds two Python objects using the PyNumber_Add C-API function. Parameters: py_obj1 (PyObject): First Python object. py_obj2 (PyObject): Second Python object. Returns: PyObject: The result of adding py_obj1 and py_obj2, or None if an error occurs. try: result = py_obj1 + py_obj2 # Simulating PyNumber_Add return result except TypeError: return None def py_floor_divide(py_obj1, py_obj2): Divides the first Python object by the second and returns the floor of the result using the PyNumber_FloorDivide C-API function. Parameters: py_obj1 (PyObject): First Python object. py_obj2 (PyObject): Second Python object. Returns: PyObject: The floor result of dividing py_obj1 by py_obj2, or None if an error occurs. try: result = py_obj1 // py_obj2 # Simulating PyNumber_FloorDivide return result except (TypeError, ZeroDivisionError): return None def py_bitwise_and(py_obj1, py_obj2): Returns the bitwise AND of two Python objects using the PyNumber_And C-API function. Parameters: py_obj1 (PyObject): First Python object. py_obj2 (PyObject): Second Python object. Returns: PyObject: The result of the bitwise AND operation on py_obj1 and py_obj2, or None if an error occurs. try: result = py_obj1 & py_obj2 # Simulating PyNumber_And return result except TypeError: return None def py_power(py_obj1, py_obj2, py_obj3=None): Raises the first Python object to the power of the second, optionally modulo the third, using the PyNumber_Power C-API function. Parameters: py_obj1 (PyObject): Base Python object. py_obj2 (PyObject): Exponent Python object. py_obj3 (PyObject or None): Modulo Python object, or None for no modulo. Returns: PyObject: The result of the power operation (with optional modulo), or None if an error occurs. try: if py_obj3 is not None: result = pow(py_obj1, py_obj2, py_obj3) # Simulating PyNumber_Power with modulo else: result = pow(py_obj1, py_obj2) # Simulating PyNumber_Power without modulo return result except TypeError: return None"},{"question":"**Objective:** Implement a synchronous TCP server that can handle multiple client requests. Additionally, extend its functionality to create an asynchronous version using `ThreadingMixIn`. The server should process incoming requests, perform a computation, and return the result back to the client. **Task:** 1. Create a synchronous TCP server that listens on a specified port and processes client requests. Each request should be a JSON string containing two integers. The server should return the sum of these integers as the response. 2. Extend the synchronous server to an asynchronous server utilizing `ThreadingMixIn` to handle multiple client connections concurrently. **Requirements:** 1. **Synchronous TCP Server** - Create a subclass of `BaseRequestHandler` and override the `handle()` method. - In the `handle()` method: - Receive a JSON string containing two integers (e.g., `{\\"a\\": 5, \\"b\\": 10}`). - Parse the JSON string to extract the integers. - Compute the sum of the two integers. - Send back the result as a JSON string (e.g., `{\\"result\\": 15}`). 2. **Asynchronous TCP Server** - Create a subclass of `ThreadingMixIn` and combine it with the synchronous TCP server class created above. **Constraints:** - Server should handle multiple clients without crashing. - Ensure that the server can gracefully shut down after all client requests are processed. - Include appropriate exception handling to deal with potential JSON parsing errors or network issues. **Input Format:** - The server listens to requests from clients in JSON format containing two integers, `a` and `b`. **Output Format:** - The server responds with a JSON string containing the result of adding the two integers. **Example:** Client Request: ```json { \\"a\\": 5, \\"b\\": 10 } ``` Server Response: ```json { \\"result\\": 15 } ``` **Steps to Submit:** 1. Implement the synchronous TCP server in a file named `sync_server.py`. 2. Implement the asynchronous TCP server in a file named `async_server.py`. 3. Provide test cases in a separate file demonstrating the server\'s ability to handle multiple client requests concurrently. **Notes:** - Use Python\'s standard library modules: `socket`, `socketserver`, and `json`. - Include comments in your code to explain the logic and flow. - Ensure your code adheres to PEP 8 style guidelines. **Bonus:** - Implement a functionality to log the requests and responses to a file for debugging purposes.","solution":"import socketserver import json from socketserver import ThreadingMixIn class SynchronousRequestHandler(socketserver.BaseRequestHandler): def handle(self): try: # Receive data from client data = self.request.recv(1024) data = data.decode(\'utf-8\') # Parse the JSON data numbers = json.loads(data) a = numbers[\'a\'] b = numbers[\'b\'] # Compute the sum result = a + b # Prepare the response response = json.dumps({\'result\': result}) # Send the response back to the client self.request.sendall(response.encode(\'utf-8\')) except (json.JSONDecodeError, KeyError, TypeError): # Handle errors gracefully response = json.dumps({\'error\': \'Invalid input\'}) self.request.sendall(response.encode(\'utf-8\')) class ThreadingTCPServer(ThreadingMixIn, socketserver.TCPServer): pass def start_server(host, port, handler): with ThreadingTCPServer((host, port), handler) as server: server.serve_forever() # For demonstrating, we define the main procedure to start the server if __name__ == \'__main__\': start_server(\'localhost\', 9999, SynchronousRequestHandler)"},{"question":"Objective: Demonstrate your understanding of creating and customizing plots using the `seaborn.objects` API. Question: You are given a dataset containing information about restaurant bills and tips. Use the seaborn `objects` API to create a customized plot with the following requirements: 1. Load the `tips` dataset. 2. Create a scatter plot of `total_bill` vs `tip` with points colored by the `time` variable. 3. Add a linear regression line (using `PolyFit`) to the scatter plot for each category of `time`. 4. Include another layer that shows the average `tip` for each `day` of the week, represented as bars, colored by the `sex` variable. 5. Ensure the plot has a proper title, axis labels, and legend to distinguish between the layers and categories. Input: The input to your function is the `tips` dataset, which you can load using the seaborn `load_dataset` function: ```python import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\") ``` Expected Output: The function should display a plot as described in the requirements. Constraints: - Use the seaborn `objects` API only. - Ensure the plot is clear and visually distinguishable with proper labels and a legend. - You can assume the `tips` dataset is always available. Performance Requirements: - The function should run efficiently and produce the plot without unnecessary computations. Example Function Signature: ```python def create_custom_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Initialize the plot p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"time\\").add(so.Dot()) # Add linear regression lines for each category of time p.add(so.Line(), so.PolyFit()) # Add a bar plot for average tips per day by sex p.add(so.Bar(), so.Agg(), y=\\"tip\\", color=\\"sex\\").facet(col=\\"day\\") # Customize plot title, axis labels, and legend p.label(x=\\"Total Bill\\", y=\\"Tip\\", title=\\"Restaurant Bills and Tips\\") # Display the plot p.show() # Call the function to create and display the plot create_custom_plot() ```","solution":"def create_custom_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Initialize the plot p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"time\\") # Add scatter plot p = p.add(so.Dot()) # Add linear regression lines for each category of time p = p.add(so.Line(), so.PolyFit()) # Add a bar plot for average tips per day by sex p = p.add(so.Bar(), so.Agg(), y=\\"tip\\", color=\\"sex\\").facet(col=\\"day\\") # Customize plot title, axis labels, and legend p = p.label(x=\\"Total Bill\\", y=\\"Tip\\", title=\\"Restaurant Bills and Tips\\") # Show the plot return p.show()"},{"question":"Implement an HTTP request parser using \\"asynchat.async_chat\\". Your implementation should be able to handle both GET and POST requests. # Requirements: 1. Your subclass should correctly parse HTTP headers and handle requests differently based on the type of request (GET or POST). 2. For GET requests, simply log the received headers. 3. For POST requests, log the received headers and the body of the request. 4. Use a simple logging mechanism (e.g., printing to the console). # Expected Class: ```python import asynchat class HttpRequestHandler(asynchat.async_chat): def __init__(self, sock, addr): # Initialize base class, set terminator, and other necessary state variables pass def collect_incoming_data(self, data): # Collect incoming data and append it to a buffer pass def found_terminator(self): # Handle the event when the terminator is found in the input stream pass def handle_request(self): # Handle the request based on the request type (GET or POST) pass ``` # Input: - A socket and address will be provided to instantiate the class. - The incoming data on the socket will simulate HTTP requests. # Output: - For GET requests, output the received headers. - For POST requests, output the received headers and the body content. # Example: For a GET request: ``` Received Headers: Host: example.com User-Agent: CustomClient/1.0 Accept: */* ``` For a POST request: ``` Received Headers: Host: example.com User-Agent: CustomClient/1.0 Accept: */* Content-Length: 13 Received Body: Hello, world! ``` # Constraints: - The class should handle the fact that HTTP headers are terminated by a blank line (`b\\"rnrn\\"`) and the request body length in POST requests is specified by the `Content-Length` header. - Ensure asynchronous communication is appropriately managed. # Notes: - Use the provided \'asynchat\' module in your implementation. - Make sure to test your implementation with different types of HTTP requests to ensure correctness.","solution":"import asynchat import asyncore class HttpRequestHandler(asynchat.async_chat): def __init__(self, sock, addr): asynchat.async_chat.__init__(self, sock) self.addr = addr self.set_terminator(b\\"rnrn\\") self.received_data = b\'\' self.headers = {} self.method = \'\' self.request_path = \'\' self.body = b\'\' self.content_length = 0 def collect_incoming_data(self, data): self.received_data += data def found_terminator(self): if not self.headers: # Parse headers header_data = self.received_data.decode(\'utf-8\').split(\'rn\') request_line = header_data[0] headers = header_data[1:] self.method, self.request_path, _ = request_line.split() for header in headers: key, value = header.split(\': \', 1) self.headers[key] = value if self.method == \'POST\': self.set_terminator(int(self.headers.get(\'Content-Length\', 0))) self.content_length = int(self.headers.get(\'Content-Length\', 0)) self.received_data = b\'\' else: self.handle_request() else: self.body = self.received_data self.handle_request() def handle_request(self): if self.method == \'GET\': print(\\"Received Headers:\\") for header, value in self.headers.items(): print(f\\"{header}: {value}\\") elif self.method == \'POST\': print(\\"Received Headers:\\") for header, value in self.headers.items(): print(f\\"{header}: {value}\\") print() print(\\"Received Body:\\") print(self.body.decode(\'utf-8\')) self.close() class HTTPServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket() self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accepted(self, sock, addr): print(f\\"Incoming connection from {addr}\\") HttpRequestHandler(sock, addr)"},{"question":"Coding Assessment Question: Embedding Python with Function Calls **Objective**: Demonstrate your understanding of embedding Python in a C program. **Scenario**: You are tasked with integrating Python scripting capabilities into an existing C application. Your objective is to initialize the Python interpreter, execute a Python function, and handle data transfer between Python and C. This exercise will test your ability to invoke a Python function from a C program and process the results. **Python Script**: Consider the following Python script saved as `arithmetic.py`: ```python def add(a, b): return a + b def multiply(a, b): return a * b ``` **Task**: Write a C program that: 1. Initializes the Python interpreter. 2. Loads the `arithmetic.py` script. 3. Calls either the `add` or `multiply` function from the script based on command-line arguments. 4. Passes two integer arguments provided via the command line to the Python function. 5. Retrieves and prints the result. **Constraints**: - Your C program should take four command-line arguments: - The path to the Python script (`arithmetic.py`). - The name of the function to call (`add` or `multiply`). - Two integers to pass to the function. - Error handling must be implemented to check if the function exists and if data types match expected input. **Example Usage**: ```sh ./embed_python arithmetic.py add 10 20 ``` Expected Output: ``` Result of call: 30 ``` # Requirements: - C code must properly initialize and finalize the Python interpreter. - Ensure resource management with proper allocation and deallocation. - Handle error cases gracefully, such as incorrect function names or argument types. # Guidelines: - Use `Py_Initialize()` and `Py_FinalizeEx()` for initializing and finalizing Python respectively. - Use `PyImport_Import()` for loading the Python script. - Use `PyObject_GetAttrString()` to get the function reference. - Use `PyObject_CallObject()` to call the Python function. - Convert the result back into C types and print it. # Starter Code: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> #include <stdio.h> int main(int argc, char *argv[]) { if (argc != 5) { fprintf(stderr, \\"Usage: %s <pythonfile> <funcname> <arg1> <arg2>n\\", argv[0]); return 1; } const char *script_name = argv[1]; const char *func_name = argv[2]; int arg1 = atoi(argv[3]); int arg2 = atoi(argv[4]); // Initialize Python Interpreter Py_Initialize(); // Load the Python script PyObject *pName = PyUnicode_DecodeFSDefault(script_name); PyObject *pModule = PyImport_Import(pName); Py_DECREF(pName); if (pModule != NULL) { // Get the function from the script PyObject *pFunc = PyObject_GetAttrString(pModule, func_name); if (pFunc && PyCallable_Check(pFunc)) { // Prepare arguments PyObject *pArgs = PyTuple_New(2); PyTuple_SetItem(pArgs, 0, PyLong_FromLong(arg1)); PyTuple_SetItem(pArgs, 1, PyLong_FromLong(arg2)); // Call the function and get the result PyObject *pResult = PyObject_CallObject(pFunc, pArgs); Py_DECREF(pArgs); if (pResult != NULL) { // Print the result printf(\\"Result of call: %ldn\\", PyLong_AsLong(pResult)); Py_DECREF(pResult); } else { Py_DECREF(pFunc); Py_DECREF(pModule); PyErr_Print(); fprintf(stderr, \\"Call failedn\\"); return 1; } Py_DECREF(pFunc); } else { if (PyErr_Occurred()) PyErr_Print(); fprintf(stderr, \\"Cannot find function \\"%s\\"n\\", func_name); } Py_DECREF(pModule); } else { PyErr_Print(); fprintf(stderr, \\"Failed to load \\"%s\\"n\\", script_name); return 1; } // Finalize Python Interpreter if (Py_FinalizeEx() < 0) { return 120; } return 0; } ``` Compile your C program with the following: ```sh gcc -o embed_python embed_python.c (python3.11-config --cflags --ldflags --embed) ``` **Note**: Adjust the `python3.11-config` command based on your Python version.","solution":"def add(a, b): Returns the sum of a and b. return a + b def multiply(a, b): Returns the product of a and b. return a * b"},{"question":"# Complex Compound Statement Implementation **Objective**: This exercise will assess your ability to implement complex control flow using compound statements in Python. Your task will involve creating a class that performs specific operations based on various control flows, context management, and exception handling. **Background**: You have been provided with a class named `DataProcessor`. Your task is to implement specific methods within this class to manage data processing operations safely and efficiently. **Task**: Implement the `DataProcessor` class that performs the following: 1. **Initialization (`__init__`)**: - The class should initialize with an attribute `data`, which is expected to be a list of integers. - It should also initialize a `result` attribute set to `None`. 2. **Processing Data (`process_data`)**: - Implement the `process_data` method that: - Iterates over the `data` attribute using a `for` loop. - For each integer in `data`: - If the integer is positive, add it to a running total. - If the integer is negative, raise a custom exception named `NegativeIntegerError`. - If the integer is zero, use a `with` statement to open a context manager that writes \\"Encountered zero\\" to a file named `log.txt`. - At the end of the iteration, if no exception was raised and no zero was encountered, store the total sum in the `result` attribute. - Implement appropriate exception handling using `try`, `except`, `else`, and `finally` blocks. 3. **Context Manager (`LogContext`)**: - Implement a simple context manager class named `LogContext` to manage the logging functionality when a zero is encountered. - The `LogContext` class should: - Open a file named `log.txt` in `__enter__`. - Write the message \\"Encountered zero\\" during `__enter__`. - Ensure the file is properly closed in `__exit__`. # Constraints - You may assume that `data` will always be a list of integers. - You should handle exceptions gracefully and ensure the program continues to function correctly after logging the exceptions. # Implementation ```python class NegativeIntegerError(Exception): Custom exception for handling negative integers in DataProcessor. pass class LogContext: def __enter__(self): self.file = open(\\"log.txt\\", \\"a\\") self.file.write(\\"Encountered zeron\\") return self.file def __exit__(self, exc_type, exc_value, traceback): self.file.close() class DataProcessor: def __init__(self, data): self.data = data self.result = None def process_data(self): Process the data list and handle exceptions and context management. # Your code goes here # ``` **Input/Output Example** ```python # Example data = [4, 2, -3, 0, 5] processor = DataProcessor(data) processor.process_data() # This should: # - Raise a NegativeIntegerError for the -3 in data # - Log \\"Encountered zero\\" to log.txt when processing 0 # - The result should remain None due to raised exception ``` Make sure to test the class thoroughly with various inputs to ensure all implemented control flows and contexts are managed correctly. # Notes - Focus on using compound statements appropriately. - Make sure to handle all exceptions and ensure resource management using the context manager. - Ensure to use the `with` statement correctly within the `process_data` method.","solution":"class NegativeIntegerError(Exception): Custom exception for handling negative integers in DataProcessor. pass class LogContext: def __enter__(self): self.file = open(\\"log.txt\\", \\"a\\") self.file.write(\\"Encountered zeron\\") return self.file def __exit__(self, exc_type, exc_value, traceback): self.file.close() class DataProcessor: def __init__(self, data): self.data = data self.result = None def process_data(self): Process the data list and handle exceptions and context management. total = 0 try: for number in self.data: if number > 0: total += number elif number < 0: raise NegativeIntegerError(f\\"Negative integer encountered: {number}\\") elif number == 0: with LogContext(): pass else: self.result = total except NegativeIntegerError as e: print(f\\"Error: {e}\\") finally: print(\\"Processing complete.\\")"},{"question":"# Custom List with Unique Elements and Coroutine-based Sum Calculation You are required to implement a custom list class `UniqueList` which enforces the following: 1. It only allows unique elements. 2. It supports typical list operations (add, remove, access by index, slicing). 3. If an attempt is made to add an already existing item, it should raise a custom exception named `DuplicateItemError`. Additionally, implement an asynchronous method in this class to calculate the sum of its elements through a coroutine. Details: 1. **Class Name**: `UniqueList` 2. **Methods**: - `__init__(self, iterable)`: Initialize the list with unique elements from the provided iterable. - `__setitem__(self, index, value)`: Set an item at a given index ensuring its uniqueness. - `__getitem__(self, index)`: Retrieve item(s) at the given index (support both integer and slice indices). - `__delitem__(self, index)`: Delete item(s) at the given index. - `add(self, value)`: Add a unique element to the list. - `remove(self, value)`: Remove an element from the list. - `__len__(self)`: Return the length of the list. - `__iter__(self)`: Return an iterator for the list. - `coroutine sum(self)`: Coroutine method returning the sum of all elements. 3. **Exception Class**: - `DuplicateItemError(Exception)`: Custom exception to be raised when a duplicate item is added. Constraints: - The elements must be hashable (i.e., must implement `__hash__` and `__eq__`). - The `coroutine sum(self)` method should utilize `await` to simulate asynchronous behavior. Example Usage: ```python unique_list = UniqueList([1, 2, 3]) unique_list.add(4) # Adds 4 to the list try: unique_list.add(2) # Raises DuplicateItemError except DuplicateItemError: print(\\"Duplicate item error raised\\") print(len(unique_list)) # Outputs 4 # Asynchronous sum calculation import asyncio result = asyncio.run(unique_list.sum()) print(result) # Outputs the sum of elements ``` Note: Your `UniqueList` class must support typical list indexing and slicing functionalities, including negative indices. # Submission: Submit your implementation of the `UniqueList` class and its methods along with the `DuplicateItemError` exception class.","solution":"class DuplicateItemError(Exception): Custom exception raised when a duplicate item is added to UniqueList pass class UniqueList: def __init__(self, iterable): self.data = [] self._item_set = set() for item in iterable: self.add(item) def __setitem__(self, index, value): if value in self._item_set: raise DuplicateItemError(f\\"Item \'{value}\' already exists in the list\\") old_item = self.data[index] self._item_set.remove(old_item) self.data[index] = value self._item_set.add(value) def __getitem__(self, index): return self.data[index] def __delitem__(self, index): item = self.data[index] del self.data[index] self._item_set.remove(item) def add(self, value): if value in self._item_set: raise DuplicateItemError(f\\"Item \'{value}\' already exists in the list\\") self.data.append(value) self._item_set.add(value) def remove(self, value): self.data.remove(value) self._item_set.remove(value) def __len__(self): return len(self.data) def __iter__(self): return iter(self.data) async def sum(self): return sum(self.data)"},{"question":"You are required to implement a financial transaction processing system in Python. This system should: 1. Perform decimal floating-point arithmetic for precise financial calculations. 2. Log each transaction with a timestamp for audit purposes. 3. Allow for concurrent processing of transactions using multi-threading. Task Description 1. **Transactions**: - Each transaction will be represented as a dictionary with the following keys: - `\\"type\\"`: Type of transaction (`\\"debit\\"` or `\\"credit\\"`). - `\\"amount\\"`: Amount to be processed, represented as a string to ensure exact decimal representation. - `\\"description\\"`: A brief descriptor of the transaction. 2. **Processing**: - A function should be created to process a list of transactions concurrently, ensuring that each transaction is correctly applied. 3. **Logging**: - Each transaction should be logged with the timestamp, type, amount, and description. 4. **Error Handling**: - If a transaction cannot be processed (e.g., insufficient funds for a debit), it should log an error without halting the system. Function Signature ```python class TransactionProcessor: def __init__(self, initial_balance: str): Initialize the TransactionProcessor with an initial balance. :param initial_balance: A string representation of the initial balance. pass def process_transactions(self, transactions: List[Dict[str, str]]): Process a list of transactions concurrently. :param transactions: List of transaction dictionaries. pass def get_balance(self) -> str: Return the current balance as a string. :return: The string representation of the current balance. pass ``` Implementation Requirements 1. **Precision and Exact Representation**: Use the `decimal.Decimal` class for all balance and transaction amount calculations. 2. **Logging**: - Utilize the `logging` module to record each transaction. - Log format should include the timestamp, transaction type, amount, and description. 3. **Multi-threading**: - Use the `threading` module to process transactions concurrently. 4. **Error Handling**: - Log an error if a debit transaction fails due to insufficient funds. Example ```python if __name__ == \'__main__\': import logging # Configure logging logging.basicConfig(level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\', handlers=[logging.StreamHandler()]) # Instantiate the processor with an initial balance processor = TransactionProcessor(\'100.00\') # List of transactions to be processed transactions = [ {\\"type\\": \\"debit\\", \\"amount\\": \\"20.00\\", \\"description\\": \\"Grocery shopping\\"}, {\\"type\\": \\"credit\\", \\"amount\\": \\"50.00\\", \\"description\\": \\"Salary\\"}, {\\"type\\": \\"debit\\", \\"amount\\": \\"150.00\\", \\"description\\": \\"Rent payment\\"}, {\\"type\\": \\"credit\\", \\"amount\\": \\"10.00\\", \\"description\\": \\"Gift\\"}, {\\"type\\": \\"debit\\", \\"amount\\": \\"5.00\\", \\"description\\": \\"Coffee\\"} ] # Process the transactions processor.process_transactions(transactions) # Print the final balance print(\\"Final Balance:\\", processor.get_balance()) ``` Expected Output: ```plaintext INFO:root:Transaction processed - 2023-03-08 12:00:00, Debit, Amount: 20.00, Description: Grocery shopping INFO:root:Transaction processed - 2023-03-08 12:00:05, Credit, Amount: 50.00, Description: Salary ERROR:root:Transaction failed - 2023-03-08 12:00:10, Debit, Amount: 150.00, Description: Rent payment - Reason: Insufficient funds INFO:root:Transaction processed - 2023-03-08 12:00:15, Credit, Amount: 10.00, Description: Gift INFO:root:Transaction processed - 2023-03-08 12:00:20, Debit, Amount: 5.00, Description: Coffee ``` Finalize the implementation in a way that ensures precision, concurrency, and comprehensive logging for auditing.","solution":"from decimal import Decimal from threading import Thread, Lock import logging from typing import List, Dict from datetime import datetime class TransactionProcessor: def __init__(self, initial_balance: str): self.balance = Decimal(initial_balance) self.lock = Lock() logging.basicConfig(level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\', handlers=[logging.StreamHandler()]) def process_transactions(self, transactions: List[Dict[str, str]]): threads = [] for transaction in transactions: thread = Thread(target=self._process_single_transaction, args=(transaction,)) threads.append(thread) thread.start() for thread in threads: thread.join() def _process_single_transaction(self, transaction: Dict[str, str]): with self.lock: transaction_type = transaction[\'type\'] amount = Decimal(transaction[\'amount\']) description = transaction[\'description\'] if transaction_type == \'debit\': if self.balance >= amount: self.balance -= amount logging.info(f\\"Transaction processed - Debit, Amount: {transaction[\'amount\']}, Description: {description}\\") else: logging.error(f\\"Transaction failed - Debit, Amount: {transaction[\'amount\']}, Description: {description} - Reason: Insufficient funds\\") elif transaction_type == \'credit\': self.balance += amount logging.info(f\\"Transaction processed - Credit, Amount: {transaction[\'amount\']}, Description: {description}\\") def get_balance(self) -> str: with self.lock: return str(self.balance)"},{"question":"# Python Simple Statements Assessment Objective: The purpose of this assessment is to evaluate your understanding and application of various simple statements in Python. Problem Statement: Write a Python function named `process_data` that performs the following tasks based on the `data` provided as a list of integers: 1. **Annotation Assignment**: Annotate the type of the data parameter and ensure it is a list of integers. 2. **Global Statement**: Use a global variable `sum_data` to store the sum of all the integers in the data list. 3. **Augmented Assignment**: Use augmented assignment to update `sum_data`. 4. **Assert Statement**: Assert if `sum_data` is greater than a specific threshold. 5. **Import Statement**: Dynamically import the `math` module and use it to compute the square root of `sum_data`. 6. **Return Statement**: Return the computed square root. Input: - A list of integers called `data`. Output: - A float representing the square root of the sum of the integers in the data list. Constraints: 1. The list can have a minimum of one integer. 2. Use appropriate error handling to manage scenarios where the list might be empty or contains non-integer types. Example: ```python data = [1, 2, 3, 4] result = process_data(data) print(result) # Should print the square root of 10 which is approximately 3.16227 ``` Function Signature: ```python def process_data(data: list[int]) -> float: global sum_data sum_data = 0 if not all(isinstance(i, int) for i in data): raise ValueError(\\"All elements in the data list must be integers.\\") for num in data: sum_data += num assert sum_data > 5, \\"The sum of data elements is not greater than the threshold.\\" import math return math.sqrt(sum_data) ``` **Note:** - Your function should correctly use the concepts from the provided documentation. - The assert statement should ensure `sum_data` is greater than 5. You can adjust the threshold as needed. - Handle potential exceptions where the `data` list might contain non-integer values. Evaluation Criteria: - Correct and efficient use of various Python statements. - Proper annotation and type hinting. - Accurate error handling and assertions. - Correct usage of global variables and dynamic imports.","solution":"def process_data(data: list[int]) -> float: global sum_data sum_data = 0 if not data: raise ValueError(\\"The data list cannot be empty.\\") if not all(isinstance(i, int) for i in data): raise ValueError(\\"All elements in the data list must be integers.\\") for num in data: sum_data += num assert sum_data > 5, \\"The sum of data elements is not greater than the threshold.\\" import math return math.sqrt(sum_data)"},{"question":"# Custom HTML Parser Assignment **Objective:** Implement a custom HTML parser that extracts specific information from HTML content using the `html.parser` module in Python. **Task:** Create a class `CustomHTMLParser` that extends `HTMLParser`. Your parser should read an HTML string and gather the following information: 1. All the URLs found in `href` attributes of `<a>` tags. 2. All the text content within `<p>` tags. 3. A count of how many times each HTML tag appears. **Requirements:** 1. Your class should have a method `get_result()` which returns a dictionary with the following structure: ```python { \\"urls\\": [list_of_urls], \\"paragraphs\\": [list_of_paragraph_texts], \\"tag_counts\\": { \\"tag_name\\": count, ... } } ``` 2. Override the necessary methods from `HTMLParser` to collect the required information. 3. Ensure your parser can handle nested tags and incomplete chunks of HTML. **Input Format:** An HTML string provided to the `feed` method of your `CustomHTMLParser`. **Output Format:** A dictionary containing the extracted information. **Constraints:** - The HTML string may contain invalid or incomplete HTML. - The HTML string is less than 10000 characters long. **Example:** ```python html_content = <html><head><title>Test</title></head> <body> <a href=\\"https://www.example.com\\">Example</a> <p>This is a paragraph.</p> <p>Another paragraph.</p> <div><a href=\\"https://www.another.com\\">Another</a></div> </body></html> parser = CustomHTMLParser() parser.feed(html_content) result = parser.get_result() # Expected result: # { # \\"urls\\": [\\"https://www.example.com\\", \\"https://www.another.com\\"], # \\"paragraphs\\": [\\"This is a paragraph.\\", \\"Another paragraph.\\"], # \\"tag_counts\\": {\\"html\\":1, \\"head\\":1, \\"title\\":1, \\"body\\":1, \\"a\\":2, \\"p\\":2, \\"div\\":1} # } print(result) ``` **Performance Requirements:** - Your solution should have a time complexity of O(n), where n is the length of the HTML string. ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.urls = [] self.paragraphs = [] self.tag_counts = {} self.current_tag = \'\' self.extract_data = False def handle_starttag(self, tag, attrs): # Increment tag count if tag in self.tag_counts: self.tag_counts[tag] += 1 else: self.tag_counts[tag] = 1 # Check for <a> tag to extract href if tag == \'a\': for attr, value in attrs: if attr == \'href\': self.urls.append(value) # Check if inside a <p> tag if tag == \'p\': self.extract_data = True def handle_endtag(self, tag): # If leaving a <p> tag, stop extracting data if tag == \'p\': self.extract_data = False def handle_data(self, data): if self.extract_data: self.paragraphs.append(data.strip()) def get_result(self): return { \\"urls\\": self.urls, \\"paragraphs\\": self.paragraphs, \\"tag_counts\\": self.tag_counts } ```","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.urls = [] self.paragraphs = [] self.tag_counts = {} self.current_data = \\"\\" self.extract_data = False def handle_starttag(self, tag, attrs): # Increment tag count if tag in self.tag_counts: self.tag_counts[tag] += 1 else: self.tag_counts[tag] = 1 # Check for <a> tag to extract href if tag == \'a\': for attr, value in attrs: if attr == \'href\': self.urls.append(value) # Check if inside a <p> tag if tag == \'p\': self.extract_data = True def handle_endtag(self, tag): # If leaving a <p> tag, add the current data to paragraphs if tag == \'p\' and self.extract_data: self.paragraphs.append(self.current_data.strip()) self.current_data = \\"\\" self.extract_data = False def handle_data(self, data): if self.extract_data: self.current_data += data def get_result(self): return { \\"urls\\": self.urls, \\"paragraphs\\": self.paragraphs, \\"tag_counts\\": self.tag_counts }"},{"question":"Coding Assessment Question # Objective You are required to extend the functionality of the `imghdr` module to include a new custom image format type. The custom image format will follow a specific signature pattern at the start of the byte stream. You need to write a function to identify this custom image type and integrate it with the `imghdr` module. # Custom Image Format - The custom image format will have a specific 4-byte signature: `b\'CUST\'`. # Requirements 1. Implement a function `is_custom_format(h, f)` that checks if the image byte stream or file `f` starts with the signature `b\'CUST\'`. 2. Update the `imghdr.tests` list to include this new custom format detection function. 3. Write a new main function `detect_image_format(file_path)` that uses `imghdr.what` to detect and return the image format including the custom format. # Function Signatures ```python def is_custom_format(h, f): Checks if the provided byte stream or file starts with the custom image format signature. Parameters: h (bytes): Byte stream of the image data. f (file-like object): File object containing the image data. Returns: str: \'custom\' if the image format is detected, else None. def detect_image_format(file_path): Detects the image format of the file at the given file path. Parameters: file_path (str): Path to the image file. Returns: str: The type of image format detected. ``` # Constraints - You can assume the maximum size of the input file will be 10MB. - Do not use any external libraries to read the file - stick to standard Python I/O operations. # Example Usage ```python # Assuming custom.img contains the signature b\'CUST\' at the beginning image_format = detect_image_format(\'custom.img\') print(image_format) # Output: \'custom\' ``` # Evaluation Criteria - Correctness of the `is_custom_format` function. - Proper integration of the new format into `imghdr.tests`. - Correct implementation of the `detect_image_format` function. - Code quality, including readability and adherence to Pythonic conventions. # Note The solution must run without errors and should correctly identify standard image formats as well as the custom format.","solution":"import imghdr def is_custom_format(h, f): Checks if the provided byte stream or file starts with the custom image format signature. Parameters: h (bytes): Byte stream of the image data. f (file-like object): File object containing the image data. Returns: str: \'custom\' if the image format is detected, else None. custom_signature = b\'CUST\' if h.startswith(custom_signature): return \'custom\' return None # Adding the custom format detection to the imghdr.tests imghdr.tests.append(is_custom_format) def detect_image_format(file_path): Detects the image format of the file at the given file path. Parameters: file_path (str): Path to the image file. Returns: str: The type of image format detected. return imghdr.what(file_path)"},{"question":"**Question: Implementing K-Means Clustering and PCA in scikit-learn** Suppose you are given a dataset containing features describing different types of fruits. Your task is to perform unsupervised learning on this dataset by implementing K-Means clustering and Principal Component Analysis (PCA) using scikit-learn. **Instructions:** 1. The dataset is provided in a CSV file named `fruits.csv`. This file contains columns representing different features of fruits. 2. Implement a function `perform_clustering_and_pca(file_path: str, n_clusters: int, n_components: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]` that takes the following parameters: - `file_path`: The path to the dataset CSV file. - `n_clusters`: The number of clusters to form with K-Means. - `n_components`: The number of principal components to keep for PCA. 3. Your function should: - Load the dataset from the provided file path. - Perform K-Means clustering on the dataset and return the cluster assignments. - Perform PCA on the dataset and return the transformed dataset with the specified number of components. - Return the following: - The cluster labels assigned to each data point by K-Means. - The PCA-transformed dataset. - The variance ratios of each of the principal components. **Example:** ```python import numpy as np from typing import Tuple from sklearn.cluster import KMeans from sklearn.decomposition import PCA import pandas as pd def perform_clustering_and_pca(file_path: str, n_clusters: int, n_components: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: # Load the dataset df = pd.read_csv(file_path) # Fit K-Means kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(df) # Fit PCA pca = PCA(n_components=n_components) pca_transformed = pca.fit_transform(df) variance_ratios = pca.explained_variance_ratio_ return cluster_labels, pca_transformed, variance_ratios # Example usage cluster_labels, pca_transformed, variance_ratios = perform_clustering_and_pca(\'fruits.csv\', 3, 2) print(cluster_labels) print(pca_transformed) print(variance_ratios) ``` **Constraints:** - You may assume the features are numeric and properly normalized. - The dataset contains no missing values. **Requirements:** - You should use `pandas` for data handling, `scikit-learn` for K-Means and PCA. - Ensure your function handles edge cases, like empty datasets or invalid input parameters, gracefully.","solution":"import numpy as np from typing import Tuple from sklearn.cluster import KMeans from sklearn.decomposition import PCA import pandas as pd def perform_clustering_and_pca(file_path: str, n_clusters: int, n_components: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: # Load the dataset df = pd.read_csv(file_path) if df.empty: raise ValueError(\\"The dataset is empty.\\") if n_clusters <= 0: raise ValueError(\\"Number of clusters must be greater than 0.\\") if n_components <= 0 or n_components > df.shape[1]: raise ValueError(\\"Number of components must be within the range from 1 to the number of features in the dataset.\\") # Fit K-Means kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(df) # Fit PCA pca = PCA(n_components=n_components) pca_transformed = pca.fit_transform(df) variance_ratios = pca.explained_variance_ratio_ return cluster_labels, pca_transformed, variance_ratios"},{"question":"# System Information Summary Generator You are required to write a Python function that collects and summarizes detailed information about the system it is run on. The function should gather data using the `platform` module and return a dictionary that includes the following fields: - `OS_name` - Name of the operating system. - `OS_version` - Version of the operating system. - `Architecture` - Architecture of the Python interpreter. - `Python_version` - Version of Python. - `Processor` - Processor name. - `Machine` - Machine type. - `Node` - Network name. - `Detailed_platform` - A detailed string representation of the platform. Your function should have the following signature: ```python def system_info_summary(): # Your code here ``` # Expected Output The function should return a dictionary with the above keys and their respective values obtained from the `platform` module. If any value is not determinable, the corresponding dictionary entry should have the value set to an empty string. # Example ```python print(system_info_summary()) ``` Output: ```python { \'OS_name\': \'Linux\', \'OS_version\': \'5.8.0-53-generic\', \'Architecture\': (\'64bit\', \'ELF\'), \'Python_version\': \'3.10.0\', \'Processor\': \'x86_64\', \'Machine\': \'x86_64\', \'Node\': \'hostname\', \'Detailed_platform\': \'Linux-5.8.0-53-generic-x86_64-with-glibc2.29\' } ``` # Constraints - Ensure compatibility with multiple operating systems including Windows, macOS, and Unix/Linux. - Make use of appropriate functions from the `platform` module to gather each piece of information. - Handle cases where certain information might not be available by returning an empty string for those fields. # Performance Requirements - The function should not perform extensive processing and should complete execution in a reasonable time (within a few milliseconds). - It is acceptable to assume that the `platform` module functions do not fail or throw exceptions under normal conditions, i.e., those that are due to the underlying operating systemâ€™s reporting. # Additional Notes - You are not allowed to use any third-party libraries, only the standard Python library. - Write clean and efficient code, avoiding unnecessary complexity.","solution":"import platform def system_info_summary(): Collects and summarizes detailed information about the system. Returns: dict: A dictionary containing system information. return { \'OS_name\': platform.system() or \'\', \'OS_version\': platform.version() or \'\', \'Architecture\': platform.architecture() or (\'\', \'\'), \'Python_version\': platform.python_version() or \'\', \'Processor\': platform.processor() or \'\', \'Machine\': platform.machine() or \'\', \'Node\': platform.node() or \'\', \'Detailed_platform\': platform.platform() or \'\' }"},{"question":"**Objective:** Demonstrate your understanding of the scikit-learn package by loading one of the provided toy datasets, pre-processing the data, applying a machine learning model, and evaluating its performance. # Question: Task: 1. **Load the Wine Dataset:** Load the `load_wine` dataset using the `sklearn.datasets` module. 2. **Preprocess the Data:** - Split the data into training and test sets (70% training, 30% testing). - Normalize the feature values to have a mean of 0 and a variance of 1 using `StandardScaler`. 3. **Train a Classifier:** - Train a `RandomForestClassifier` on the training set. - Use an appropriate metric from `sklearn.metrics` to evaluate your model on the test set. 4. **Evaluate the Model:** - Report the accuracy of the model on the test set. - Print the classification report which includes precision, recall, and F1-score for each class. Constraints: - Use the provided toy dataset `load_wine` from `sklearn.datasets` for your implementation. - Perform a train-test split with a 70-30 ratio. - Normalize the features using `StandardScaler`. - Use `RandomForestClassifier` for training the model. - Evaluate the model using accuracy and provide a detailed classification report. Expected Input and Output Formats: ```python def wine_classifier_evaluation(): This function should load the wine dataset, preprocess the data, train a RandomForestClassifier, and evaluate its performance. Returns: - accuracy (float): The accuracy score of the model on the test set. - report (str): The detailed classification report including precision, recall, and F1-score for each class. pass # your implementation here # Example output accuracy, report = wine_classifier_evaluation() print(\\"Accuracy:\\", accuracy) print(\\"Classification Report:n\\", report) ``` Notes: - You may use any functions or classes from `sklearn.datasets`, `sklearn.model_selection`, `sklearn.preprocessing`, `sklearn.ensemble`, and `sklearn.metrics`. - Ensure your solution is well-documented and follows best practices in coding.","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, classification_report def wine_classifier_evaluation(): # Load the Wine dataset data = load_wine() X, y = data.data, data.target # Split the data into training and test sets (70% training, 30% testing) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Normalize the feature values using StandardScaler scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train a RandomForestClassifier on the training set classifier = RandomForestClassifier(random_state=42) classifier.fit(X_train_scaled, y_train) # Make predictions on the test set y_pred = classifier.predict(X_test_scaled) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred, target_names=data.target_names) # Return the accuracy and the classification report return accuracy, report"},{"question":"Python allows you to work closely with its interpreter and abstract syntax tree (AST). In this exercise, you will work with Python\'s built-in `compile()` function and the `types.CodeType` class to create and modify code objects dynamically. # Task Create a function `create_increment_function` that dynamically creates a function which increments its input by a given value. This function should: 1. Take an integer argument `n`. 2. Return a new function that takes an integer `x` and returns `x + n`. The function you generate should be created using Python\'s built-in `compile()` function to generate a code object dynamically and then convert it back into a function using the `types.CodeType` class and `types.FunctionType`. # Input - An integer `n` (0 <= n <= 100) # Output - A dynamically created function that takes an integer `x` and returns `x + n`. # Constraints - Do not use Python\'s standard way of defining functions (`def` or `lambda`). # Example ```python increment_by_5 = create_increment_function(5) result = increment_by_5(10) # Should return 15 increment_by_10 = create_increment_function(10) result2 = increment_by_10(15) # Should return 25 ``` # Hints - Look into the `compile()` function for generating code objects. - Use the `types.CodeType` class to manually construct a code object. - Convert the code object into a function using `types.FunctionType`. ```python import types def create_increment_function(n): # your code here ```","solution":"import types def create_increment_function(n): # Source code for the function as a string source_code = f def increment(x): return x + {n} # Compile the source code into a code object code_object = compile(source_code, \'<string>\', \'exec\') # Extract the code of the \'increment\' function increment_code = None for const in code_object.co_consts: if isinstance(const, types.CodeType) and const.co_name == \'increment\': increment_code = const break # Create a function from the code object increment_function = types.FunctionType(increment_code, globals()) return increment_function"},{"question":"# Problem: Implementing and Testing Dynamic Shapes in PyTorch Objective: You are asked to implement a function that performs operations on dynamically shaped tensors and another function that verifies the correct functioning of dynamic shape handling using guards and symbolic sizes. Task: 1. **Function Implementation**: Implement a function `dynamic_operations` which accepts two tensors and performs a concatenation operation followed by a conditional branch. The function should handle dynamic shapes correctly using PyTorch APIs. 2. **Verification Function**: Implement a function `verify_dynamic_shapes` which creates test cases to validate that `dynamic_operations` correctly handles varying input shapes. This function should set up dynamic marking and validate guards to ensure no recompilation for dynamic changes. Requirements: - Use `torch._dynamo.mark_dynamic` to mark dynamic shapes. - Implement conditional branch logic considering dynamic tensor sizes. - Ensure that guard conditions are set and verified. Function Signature: ```python import torch def dynamic_operations(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: Perform a concatenation of two tensors followed by a conditional operation. Args: - x (torch.Tensor): Input tensor 1. - y (torch.Tensor): Input tensor 2. Returns: - torch.Tensor: Result tensor after conditional operations. def verify_dynamic_shapes() -> None: Create and run test cases to validate dynamic shape handling by `dynamic_operations`. Raises: - AssertionError: If test cases do not pass. ``` Implementation Details: 1. **dynamic_operations Function**: - Concatenate the two tensors along the first dimension. - If the size of the concatenated tensor along the first dimension is greater than a threshold (e.g., 2), return the tensor multiplied by 2. - Otherwise, return the tensor added by 2. - Use `torch._dynamo.mark_dynamic` to ensure the first dimension of input tensors is dynamic. 2. **verify_dynamic_shapes Function**: - Create test cases with varying input tensor sizes. - Use logging and guards to ensure that `dynamic_operations` handles dynamic shapes without recompilation. - Validate the output of `dynamic_operations` against expected results for differing input shapes. Constraints: - The dimensions of the tensors should be compatible for concatenation. - The implementation must correctly handle varying sizes without unnecessary recompilations. Example: ```python def dynamic_operations(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: # Mark the first dimension of x and y as dynamic torch._dynamo.mark_dynamic(x, 0) torch._dynamo.mark_dynamic(y, 0) # Concatenate tensors z = torch.cat([x, y]) # Conditional operation if z.size(0) > 2: return z.mul(2) else: return z.add(2) def verify_dynamic_shapes() -> None: # Test Case 1: Small sizes x1, y1 = torch.randn(1, 2), torch.randn(1, 2) out1 = dynamic_operations(x1, y1) assert torch.equal(out1, torch.cat([x1, y1]).add(2)) # Test Case 2: Larger sizes x2, y2 = torch.randn(2, 2), torch.randn(1, 2) out2 = dynamic_operations(x2, y2) assert torch.equal(out2, torch.cat([x2, y2]).mul(2)) # Further test cases to cover edge cases # ... verify_dynamic_shapes() ``` Ensure that your solution is efficient and adheres to the constraints. Good luck!","solution":"import torch import torch._dynamo def dynamic_operations(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: Perform a concatenation of two tensors followed by a conditional operation. Args: - x (torch.Tensor): Input tensor 1. - y (torch.Tensor): Input tensor 2. Returns: - torch.Tensor: Result tensor after conditional operations. # Mark the first dimension of x and y as dynamic torch._dynamo.mark_dynamic(x, 0) torch._dynamo.mark_dynamic(y, 0) # Concatenate tensors z = torch.cat([x, y]) # Conditional operation if z.size(0) > 2: return z.mul(2) else: return z.add(2) def verify_dynamic_shapes() -> None: Create and run test cases to validate dynamic shape handling by `dynamic_operations`. Raises: - AssertionError: If test cases do not pass. # Test Case 1: Small sizes x1, y1 = torch.randn(1, 2), torch.randn(1, 2) out1 = dynamic_operations(x1, y1) assert torch.equal(out1, torch.cat([x1, y1]).add(2)), f\\"Test Case 1 Failed. Expected {torch.cat([x1, y1]).add(2)} but got {out1}\\" # Test Case 2: Larger sizes x2, y2 = torch.randn(2, 2), torch.randn(1, 2) out2 = dynamic_operations(x2, y2) assert torch.equal(out2, torch.cat([x2, y2]).mul(2)), f\\"Test Case 2 Failed. Expected {torch.cat([x2, y2]).mul(2)} but got {out2}\\" # Test Case 3: Large sizes x3, y3 = torch.randn(3, 2), torch.randn(2, 2) out3 = dynamic_operations(x3, y3) assert torch.equal(out3, torch.cat([x3, y3]).mul(2)), f\\"Test Case 3 Failed. Expected {torch.cat([x3, y3]).mul(2)} but got {out3}\\" # Test Case 4: Exactly two rows x4, y4 = torch.randn(1, 2), torch.randn(1, 2) out4 = dynamic_operations(x4, y4) assert torch.equal(out4, torch.cat([x4, y4]).add(2)), f\\"Test Case 4 Failed. Expected {torch.cat([x4, y4]).add(2)} but got {out4}\\""},{"question":"Objective You are tasked with implementing a custom serialization and deserialization process for Python objects in a binary format. This will mimic aspects of the marshalling process described in the document. Problem Statement Create a Python module named `custom_marshal` that contains two main functions: 1. `serialize_object(obj: Any, version: int) -> bytes` 2. `deserialize_object(data: bytes, version: int) -> Any` Your functions should: - **`serialize_object`**: Take a Python object and a version number as inputs and return a byte string representing the serialized object. Handle at least the following types: `int`, `float`, `str`, `list`, and `dict`. - **`deserialize_object`**: Take a byte string and a version number as inputs and return the original Python object reconstructed from the byte string. Requirements 1. **Types to Handle**: Your implementation should be able to handle basic data types (`int`, `float`, `str`) and complex types (`list`, `dict`). 2. **Binary Format**: Use a binary format where: - `int` values are stored as 4 bytes (32-bit integer). - `float` values are stored using Python\'s `struct.pack` with `\'d\'` format (8 bytes for a double-precision float). - `str` values are stored with their length prefixed as a 4-byte integer followed by UTF-8 encoded bytes of the string. - `list` values are serialized as the length of the list (4-byte integer) followed by the serialized representations of each element. - `dict` values are serialized as the number of key-value pairs (4-byte integer) followed by the serialized representations of each key-value pair. Constraints - **Performance**: Ensure that the functions can handle large objects efficiently. Avoid using unnecessary intermediate data structures. - **Errors**: Gracefully handle and raise appropriate exceptions for errors such as unsupported data types, incorrect version numbers, and data corruption during deserialization. Example Usage ```python from custom_marshal import serialize_object, deserialize_object obj = { \'int\': 42, \'float\': 3.14, \'str\': \\"Hello, World!\\", \'list\': [1, 2, 3], \'dict\': {\'key1\': \'value1\', \'key2\': \'value2\'} } # Serialize the object version = 2 serialized_data = serialize_object(obj, version) # Deserialize the object deserialized_obj = deserialize_object(serialized_data, version) assert deserialized_obj == obj ``` Input and Output Formats 1. **serialize_object**: - Input: A Python object (`obj`) and an integer (`version`). - Output: A byte string representing the serialized object. 2. **deserialize_object**: - Input: A byte string (`data`) and an integer (`version`). - Output: The deserialized Python object. Submission Submit the `custom_marshal` module containing the two functions: `serialize_object` and `deserialize_object`. Ensure the module includes appropriate error handling and can handle the specified types correctly.","solution":"import struct import json def serialize_object(obj, version: int) -> bytes: if version != 2: raise ValueError(\\"Unsupported version number\\") if isinstance(obj, int): return b\'I\' + struct.pack(\'!i\', obj) elif isinstance(obj, float): return b\'F\' + struct.pack(\'!d\', obj) elif isinstance(obj, str): encoded_str = obj.encode(\'utf-8\') return b\'S\' + struct.pack(\'!I\', len(encoded_str)) + encoded_str elif isinstance(obj, list): encoded_list = b\'L\' + struct.pack(\'!I\', len(obj)) for item in obj: encoded_list += serialize_object(item, version) return encoded_list elif isinstance(obj, dict): encoded_dict = b\'D\' + struct.pack(\'!I\', len(obj)) for key, value in obj.items(): encoded_dict += serialize_object(key, version) encoded_dict += serialize_object(value, version) return encoded_dict else: raise TypeError(\\"Unsupported type\\") def deserialize_object(data: bytes, version: int): if version != 2: raise ValueError(\\"Unsupported version number\\") def _deserialize(data): type_code = data[0:1] if type_code == b\'I\': return struct.unpack(\'!i\', data[1:5])[0], data[5:] elif type_code == b\'F\': return struct.unpack(\'!d\', data[1:9])[0], data[9:] elif type_code == b\'S\': str_len = struct.unpack(\'!I\', data[1:5])[0] string = data[5:5 + str_len].decode(\'utf-8\') return string, data[5 + str_len:] elif type_code == b\'L\': list_len = struct.unpack(\'!I\', data[1:5])[0] list_items = [] remainder = data[5:] for _ in range(list_len): item, remainder = _deserialize(remainder) list_items.append(item) return list_items, remainder elif type_code == b\'D\': dict_len = struct.unpack(\'!I\', data[1:5])[0] dict_items = {} remainder = data[5:] for _ in range(dict_len): key, remainder = _deserialize(remainder) value, remainder = _deserialize(remainder) dict_items[key] = value return dict_items, remainder else: raise TypeError(\\"Unsupported type code\\") obj, remainder = _deserialize(data) if remainder: raise ValueError(\\"Extra data after deserialization\\") return obj"},{"question":"# Hashlib Coding Assessment Task **Objective:** Demonstrate your understanding of the `hashlib` module by implementing functions that perform secure hashing, keyed hashing, and password-based key derivation. **Task:** 1. **Basic Hashing Function:** Implement a function `compute_hash(data, algorithm=\'sha256\')` that computes and returns the hexadecimal digest of the input data using the specified algorithm. **Input:** - `data` (str): The input data to be hashed. - `algorithm` (str): The hashing algorithm to use (default is \'sha256\'). **Output:** - Returns a hexadecimal digest of the hash. ```python def compute_hash(data: str, algorithm: str = \'sha256\') -> str: pass ``` 2. **Keyed Hashing Function:** Implement a function `compute_keyed_hash(data, key, algorithm=\'blake2b\', digest_size=16)` that computes and returns a keyed hexadecimal digest of the input data using the specified algorithm and a secret key. **Input:** - `data` (str): The input data to be hashed. - `key` (str): The secret key for keyed hashing. - `algorithm` (str): The hashing algorithm to use (default is \'blake2b\'). - `digest_size` (int): Size of the output digest in bytes (default is 16). **Output:** - Returns a keyed hexadecimal digest of the hash. ```python def compute_keyed_hash(data: str, key: str, algorithm: str = \'blake2b\', digest_size: int = 16) -> str: pass ``` 3. **Password-based Key Derivation Function:** Implement a function `derive_key(password, salt, iterations=500000, algorithm=\'sha256\', dklen=None)` that derives and returns a key from a password using the PBKDF2-HMAC algorithm. **Input:** - `password` (str): The password to derive the key from. - `salt` (str): The salt for the key derivation. - `iterations` (int): Number of iterations for the key derivation (default is 500000). - `algorithm` (str): The hashing algorithm to use (default is \'sha256\'). - `dklen` (int): Length of the derived key (default is None). **Output:** - Returns the derived key encoded in hexadecimal. ```python def derive_key(password: str, salt: str, iterations: int = 500000, algorithm: str = \'sha256\', dklen: int = None) -> str: pass ``` **Constraints:** - You must use the `hashlib` module for hashing operations. - The `data` and `key` parameters should be converted to bytes using UTF-8 encoding before being passed to the hash functions. - Ensure that your implementation handles edge cases, such as empty input data. - Use the `os.urandom()` function to generate a 16-byte salt for the `derive_key` function if no salt is provided. **Your implementation will be evaluated based on:** - Correctness of the functions. - Proper usage of the `hashlib` module and its methods. - Handling of edge cases and input validation. - Code readability and comments.","solution":"import hashlib import hmac import os def compute_hash(data: str, algorithm: str = \'sha256\') -> str: Computes and returns the hexadecimal digest of the input data using the specified hashing algorithm. :param data: The input data to be hashed. :param algorithm: The hashing algorithm to use. :return: Hexadecimal digest of the hash. hasher = hashlib.new(algorithm) hasher.update(data.encode(\'utf-8\')) return hasher.hexdigest() def compute_keyed_hash(data: str, key: str, algorithm: str = \'blake2b\', digest_size: int = 16) -> str: Computes and returns a keyed hexadecimal digest of the input data using the specified algorithm and a secret key. :param data: The input data to be hashed. :param key: The secret key for keyed hashing. :param algorithm: The hashing algorithm to use. :param digest_size: Size of the output digest in bytes. :return: Keyed hexadecimal digest of the hash. if algorithm.lower() == \'blake2b\': keyed_hasher = hashlib.blake2b(key=key.encode(\'utf-8\'), digest_size=digest_size) else: raise ValueError(f\\"Unsupported algorithm: {algorithm}\\") keyed_hasher.update(data.encode(\'utf-8\')) return keyed_hasher.hexdigest() def derive_key(password: str, salt: str = None, iterations: int = 500000, algorithm: str = \'sha256\', dklen: int = None) -> str: Derives and returns a key from a password using the PBKDF2-HMAC algorithm. :param password: The password to derive the key from. :param salt: The salt for the key derivation. :param iterations: Number of iterations for the key derivation. :param algorithm: The hashing algorithm to use. :param dklen: Length of the derived key. :return: The derived key encoded in hexadecimal. if salt is None: salt = os.urandom(16).hex() dk = hashlib.pbkdf2_hmac(algorithm, password.encode(\'utf-8\'), salt.encode(\'utf-8\'), iterations, dklen) return dk.hex()"},{"question":"# Advanced Coding Assessment Question Objective: Your task is to write a Python function that takes a Python source code string as input and returns specific analytical information using the `ast` module. The function will return a dictionary containing the following details: 1. The total number of Import statements. 2. The total number of function definitions. 3. The total number of class definitions. 4. A list of the names of all imported modules. Function Signature: ```python def analyze_python_code(source_code: str) -> dict: pass ``` Input: - `source_code` (str): A string containing the Python source code. Output: - A dictionary with the following keys: - `num_imports` (int): The total number of import statements. - `num_functions` (int): The total number of function definitions. - `num_classes` (int): The total number of class definitions. - `import_names` (list): A list of names of all imported modules. Constraints: - You must use the `ast` module. - Assume `source_code` is a valid Python code block. Example: ```python source_code = \'\'\' import os import sys def foo(): pass class Bar: def __init__(self): pass class Baz: pass import math \'\'\' result = analyze_python_code(source_code) # Expected output: # { # \'num_imports\': 3, # \'num_functions\': 2, # \'num_classes\': 2, # \'import_names\': [\'os\', \'sys\', \'math\'] # } ``` Instructions: 1. Define the function `analyze_python_code(source_code: str) -> dict`. 2. Use the `ast` module to parse the source code and gather the necessary information. 3. Ensure the function returns the correct output format, as shown in the example. # Evaluation Criteria: - Correctness: The function should match the expected output. - Efficiency: Reasonable performance for typical input sizes. - Code Quality: Clean, readable, and well-documented code.","solution":"import ast def analyze_python_code(source_code: str) -> dict: Analyzes the given Python source code and returns a dictionary with the number of import statements, function definitions, class definitions, and a list of imported module names. tree = ast.parse(source_code) num_imports = 0 num_functions = 0 num_classes = 0 import_names = [] for node in ast.walk(tree): if isinstance(node, ast.Import): num_imports += 1 for alias in node.names: import_names.append(alias.name) elif isinstance(node, ast.ImportFrom): num_imports += 1 import_names.append(node.module) elif isinstance(node, ast.FunctionDef): num_functions += 1 elif isinstance(node, ast.ClassDef): num_classes += 1 return { \'num_imports\': num_imports, \'num_functions\': num_functions, \'num_classes\': num_classes, \'import_names\': import_names }"},{"question":"Coding Assessment Question # Task You are required to implement a Python program that processes a list of URLs to fetch and save their content concurrently. Your program should use both threading and multiprocessing to balance between CPU-bound and IO-bound tasks effectively. # Requirements 1. **Read URLs from an Input File**: There is an input text file (`urls.txt`) which contains one URL per line. 2. **Fetch URL Content**: - Use threading to manage multiple simultaneous web requests since this task is IO-bound. - Each thread should fetch the content of a single URL. 3. **Process Retrieved Content**: - Use multiprocessing to handle the processing of the fetched content since this task is CPU-bound. - Each process should convert the content to upper case and simulate some CPU-heavy computation. 4. **Save Processed Content**: Save the processed content to individual files where the filename corresponds to the URL\'s position in the input file (e.g., `content_1.txt` for the first URL, `content_2.txt` for the second one, and so on). # Input and Output - **Input**: A text file named `urls.txt`. Each line in the file represents a URL. - **Output**: Individual text files with processed content named as `content_n.txt` where `n` is the URL\'s line position (1-based index) in the `urls.txt`. # Constraints - The URL fetching should handle up to 10 simultaneous threads. - Processed content should be handled using up to 4 simultaneous processes. - Implement error handling to manage failed requests and output them to a `errors.log` file. # Performance Requirements - Threads should efficiently handle multiple URL fetching to reduce waiting time. - Processes should maximize CPU usage for computation-heavy tasks. # Example Given a `urls.txt` file as input with the following content: ``` http://example.com/page1 http://example.com/page2 ``` Your program should: 1. Fetch the content concurrently using threading. 2. Process each fetched content simultaneously using multiprocessing. 3. Save the processed content into `content_1.txt` and `content_2.txt`. 4. Log any errors encountered during fetching. # Additional Information You may use any libraries or modules deemed necessary. Do not use external libraries for fetching URLs, standard libraries like `requests` are sufficient. # Hints - Consider using the `ThreadPoolExecutor` from `concurrent.futures` for managing threads. - Use the `multiprocessing.Pool` class for managing processes. - Utilize locks or semaphores where shared resources are accessed concurrently to prevent race conditions.","solution":"import requests from concurrent.futures import ThreadPoolExecutor import multiprocessing import os def fetch_url_content(url, index, results, errors): try: response = requests.get(url) response.raise_for_status() results[index] = response.text except requests.RequestException as e: errors[index] = str(e) def process_content(content): # Simulate CPU-heavy processing by converting content to uppercase return content.upper() def save_content(content, index): file_name = f\\"content_{index + 1}.txt\\" with open(file_name, \'w\') as f: f.write(content) def main(): input_file = \'urls.txt\' if not os.path.exists(input_file): print(f\\"{input_file} not found.\\") return with open(input_file) as f: urls = [line.strip() for line in f if line.strip()] results = [None] * len(urls) errors = [None] * len(urls) # Fetch content using threading with ThreadPoolExecutor(max_workers=10) as executor: for i, url in enumerate(urls): executor.submit(fetch_url_content, url, i, results, errors) errors_logged = False with open(\\"errors.log\\", \\"w\\") as error_log: for index, error in enumerate(errors): if error: error_log.write(f\\"Error with URL at index {index + 1}: {error}n\\") errors_logged = True if errors_logged: print(\\"Some URLs failed to fetch. See errors.log for details.\\") # Process and save content using multiprocessing with multiprocessing.Pool(processes=4) as pool: processed_contents = pool.map(process_content, filter(None, results)) for i, content in enumerate(results): if content: save_content(processed_contents.pop(0), i) if __name__ == \\"__main__\\": main()"},{"question":"# Source Distribution Manifest Generator You are tasked with implementing a function that simulates generating a MANIFEST file for a source distribution based on simplified parameters. The function should mimic part of the behavior of the `sdist` command as described in the provided documentation. # Function Specification Function Name: `generate_manifest` Parameters: 1. `files: List[str]` - a list of file paths to input files to be included in the distribution. 2. `exclude_patterns: List[str]` - a list of patterns used to exclude files from the distribution. Supported patterns are: - Directories to exclude (e.g., `build/`, `.git/`). - Specific file patterns (e.g., `*.tmp`, `*.bak`). Output: - The function will return a list of files that would be included in the MANIFEST file after applying the specified exclusions. Constraints: - The function should filter out files according to the provided `exclude_patterns`. - The function should handle both file and directory exclusions. - Ensure that slashes (`/`) are used as directory separators. Example: ```python def generate_manifest(files, exclude_patterns): pass # Example usage files = [ \\"README.txt\\", \\"setup.py\\", \\"src/module.py\\", \\"build/log.txt\\", \\"docs/info.txt\\", \\"src/test/test_module.py\\", \\".git/config\\" ] exclude_patterns = [\\"build/\\", \\".git/\\", \\"*.txt\\"] # Expected output: # List of files in the MANIFEST after exclusion. manifest = generate_manifest(files, exclude_patterns) # manifest should be [\\"setup.py\\", \\"src/module.py\\", \\"src/test/test_module.py\\"] ``` # Notes: - The solution should properly handle nested directories and file patterns. - Ensure that the functionâ€™s implementation is efficient enough to handle large lists of files. Implement the function `generate_manifest` as specified above.","solution":"import fnmatch from typing import List def generate_manifest(files: List[str], exclude_patterns: List[str]) -> List[str]: def is_excluded(file: str) -> bool: for pattern in exclude_patterns: if pattern.endswith(\'/\'): if file.startswith(pattern): return True elif fnmatch.fnmatch(file, pattern): return True return False return [file for file in files if not is_excluded(file)]"},{"question":"**Problem Statement:** In this coding assessment, you are required to implement a custom PyTorch function that performs numerically stable batched matrix multiplication. The implementation should account for potential issues such as overflow with extremal values, and ensure consistency in computations across different batches. # Function Signature ```python import torch def stable_batched_matrix_multiplication(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Args: A: A 3D tensor of shape (batch_size, m, n), where `batch_size` is the number of matrices, `m` is the number of rows, and `n` is the number of columns. B: A 3D tensor of shape (batch_size, n, p), where `batch_size` is the number of matrices, `n` is the number of rows, and `p` is the number of columns. Returns: A 3D tensor of shape (batch_size, m, p) resulting from the matrix multiplication of each corresponding matrix in A and B. pass ``` # Constraints 1. You must use PyTorch for the matrix operations. 2. Ensure numerical stability by considering the extremal values that might lead to overflow. 3. Avoid straightforward use of `torch.bmm` or other direct batched matrix multiplication functions. 4. Consider edge cases such as matrices containing `inf` or `NaN` values. 5. Ensure that the function handles large matrices efficiently without running into memory issues. # Examples Example 1 ```python A = torch.tensor([ [[1e20, 2], [3, 4]], [[5, 6], [7, 8]] ]) B = torch.tensor([ [[1, 2], [3, 4e20]], [[9, 10], [11, 12]] ]) result = stable_batched_matrix_multiplication(A, B) print(result) # Expected output: tensor with stable results ``` Example 2 ```python A = torch.tensor([ [[1, nan], [3, 4]], [[5, 6], [inf, 8]] ]) B = torch.tensor([ [[1, 2], [3, 4]], [[9, 10], [11, 12]] ]) result = stable_batched_matrix_multiplication(A, B) print(result) # Expected output: tensor with NaN or inf appropriately handled ``` # Performance Considerations - Ensure the implementation is efficient for larger `batch_size`. - Handle numerical overflow meticulously to avoid `inf` or NaN results unless they are inevitable due to input values. # Hints - Consider using intermediate casting to higher precision (e.g., double) if necessary. - Utilize `torch.isfinite` to handle non-finite values before performing operations. - Numerical stability can be improved by avoiding direct accumulation of large and small values together. Implement the function complying with the guidelines and ensuring robust handling of numerical issues in batched matrix multiplications.","solution":"import torch def stable_batched_matrix_multiplication(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Args: A: A 3D tensor of shape (batch_size, m, n), where `batch_size` is the number of matrices, `m` is the number of rows, and `n` is the number of columns. B: A 3D tensor of shape (batch_size, n, p), where `batch_size` is the number of matrices, `n` is the number of rows, and `p` is the number of columns. Returns: A 3D tensor of shape (batch_size, m, p) resulting from the matrix multiplication of each corresponding matrix in A and B. batch_size, m, n_A = A.shape batch_size_B, n_B, p = B.shape # Ensure the batch sizes match assert batch_size == batch_size_B, \\"Batch sizes of A and B must match\\" # Ensure the inner dimensions match assert n_A == n_B, \\"Inner matrix dimensions must match for multiplication\\" # Initialize result tensor with appropriate shape result = torch.zeros((batch_size, m, p), dtype=A.dtype) for i in range(batch_size): # Decompose each batch into individual matrices A_i = A[i] B_i = B[i] # Perform matrix multiplication and handle numerical stability result[i] = torch.matmul(A_i.double(), B_i.double()).float() return result"},{"question":"# URL Manipulation Challenge You are given a list of URLs and a set of instructions to modify these URLs. You need to follow these instructions to parse components of the URL, make the specified changes, and reconstruct the final URL. This will test your comprehension of the `urllib.parse` module functionalities. Instructions: 1. Parse the given URL to extract components. 2. Based on the instructions, modify certain components of the URL. 3. Reconstruct the URL with the modifications and ensure it is in the valid format. **Function Signature:** ```python def modify_urls(url_list: list, modifications: list) -> list: Modifies a list of URLs based on specified modifications and returns the updated list of URLs. Parameters: url_list (list): A list of URL strings. modifications (list): A list of dictionaries where each dictionary contains the modifications for the corresponding URL. Return: list: A list of modified URL strings. ``` **Inputs:** - `url_list`: A list of URL strings. Example: `[\\"http://example.com/path?query=abc#fragment\\", \\"https://python.org/doc\\"]` - `modifications`: A list of dictionaries containing modifications for each URL. Each dictionary can have the following keys: - `\'scheme\'`: The new scheme to use, if provided. - `\'netloc\'`: The new network location (e.g., domain and port), if provided. - `\'path\'`: The new path for the URL, if provided. - `\'params\'`: The new params to use, if provided. - `\'query\'`: The new query to use, if provided. - `\'fragment\'`: The new fragment to use, if provided. **Outputs:** - A list of modified URL strings in the same order as the original list. **Constraints:** - Assume the lengths of `url_list` and `modifications` are the same. - The modifications dict will always contain a key if the corresponding change is to be made. **Example:** ```python url_list = [ \\"http://example.com/path?query=abc#fragment\\", \\"https://python.org/doc\\" ] modifications = [ {\\"scheme\\": \\"https\\", \\"query\\": \\"newquery=xyz\\"}, {\\"path\\": \\"/3/library/urllib.parse.html\\", \\"fragment\\": \\"url-parsing\\"} ] # Example output should be: # [\\"https://example.com/path?newquery=xyz#fragment\\", \\"https://python.org/3/library/urllib.parse.html#url-parsing\\"] ``` Note: Use `urlparse`, `urlunparse`, `urlsplit`, and `urlunsplit` to achieve the modifications and ensure the URLs are well-formed.","solution":"from urllib.parse import urlparse, urlunparse def modify_urls(url_list, modifications): modified_urls = [] for url, mod in zip(url_list, modifications): parsed_url = urlparse(url) new_url_parts = list(parsed_url) new_url_parts[0] = mod.get(\\"scheme\\", new_url_parts[0]) new_url_parts[1] = mod.get(\\"netloc\\", new_url_parts[1]) new_url_parts[2] = mod.get(\\"path\\", new_url_parts[2]) new_url_parts[3] = mod.get(\\"params\\", new_url_parts[3]) new_url_parts[4] = mod.get(\\"query\\", new_url_parts[4]) new_url_parts[5] = mod.get(\\"fragment\\", new_url_parts[5]) new_url = urlunparse(new_url_parts) modified_urls.append(new_url) return modified_urls"},{"question":"# Data Transformation and Analysis with Pandas Objective You are provided with two datasets in the form of CSV files containing information about sales transactions and product details. Your task is to perform several data manipulation and analytical tasks using pandas to gain insights into the sales data. Datasets 1. **sales_data.csv** ``` transaction_id, product_id, quantity, price_per_unit, transaction_date 1001, A1, 4, 9.99, 2023-01-15 1002, A2, 2, 19.99, 2023-01-18 1003, A1, 1, 9.99, 2023-02-03 1004, A3, 6, 29.99, 2023-02-05 ``` 2. **product_data.csv** ``` product_id, product_name, category A1, Widget, Gadgets A2, Gizmo, Gadgets A3, Doohickey, Tools ``` Tasks 1. **Load the Data** - Load the `sales_data.csv` and `product_data.csv` files into pandas DataFrames. 2. **Data Cleaning** - Check for missing values in both DataFrames. If any are found, fill them with appropriate values or remove the rows/columns. 3. **Data Merging** - Merge the two DataFrames on the `product_id` column to get a combined DataFrame with all relevant information about each transaction. 4. **Data Transformation** - Create a new column `total_price` in the merged DataFrame which is calculated as `quantity * price_per_unit`. 5. **Handling Dates** - Convert the `transaction_date` column to datetime format. - Add a new column `month` that extracts the month from `transaction_date`. 6. **Data Analysis** - Group the merged DataFrame by the `category` and `month` columns and calculate the total sales (sum of `total_price`) for each group. 7. **Result** - Output the resulting DataFrame with the group by result showing `category`, `month`, and `total sales`. Constraints - You can assume the CSV files are correctly formatted and donâ€™t contain any further corrupt data beyond potential missing values. - Your solution should handle the task efficiently, considering performance with larger datasets. Expected Input and Output formats **Input**: Paths to `sales_data.csv` and `product_data.csv`. **Output**: DataFrame with columns `category`, `month`, and `total_sales`. Sample Solution Outline ```python import pandas as pd # Step 1: Load data sales_data = pd.read_csv(\'sales_data.csv\') product_data = pd.read_csv(\'product_data.csv\') # Step 2: Data cleaning (check and handle missing values) sales_data.fillna(0, inplace=True) product_data.fillna(\'Unknown\', inplace=True) # Step 3: Data merging merged_data = pd.merge(sales_data, product_data, on=\'product_id\') # Step 4: Data transformation merged_data[\'total_price\'] = merged_data[\'quantity\'] * merged_data[\'price_per_unit\'] # Step 5: Handling dates merged_data[\'transaction_date\'] = pd.to_datetime(merged_data[\'transaction_date\']) merged_data[\'month\'] = merged_data[\'transaction_date\'].dt.month # Step 6: Data analysis grouped_data = merged_data.groupby([\'category\', \'month\'])[\'total_price\'].sum().reset_index() # Step 7: Result print(grouped_data) ```","solution":"import pandas as pd def load_data(sales_path, product_path): sales_data = pd.read_csv(sales_path) product_data = pd.read_csv(product_path) return sales_data, product_data def clean_data(sales_data, product_data): sales_data.fillna(0, inplace=True) product_data.fillna(\'Unknown\', inplace=True) return sales_data, product_data def merge_data(sales_data, product_data): merged_data = pd.merge(sales_data, product_data, on=\'product_id\') return merged_data def transform_data(merged_data): merged_data[\'total_price\'] = merged_data[\'quantity\'] * merged_data[\'price_per_unit\'] merged_data[\'transaction_date\'] = pd.to_datetime(merged_data[\'transaction_date\']) merged_data[\'month\'] = merged_data[\'transaction_date\'].dt.month return merged_data def analyze_data(merged_data): grouped_data = merged_data.groupby([\'category\', \'month\'])[\'total_price\'].sum().reset_index() return grouped_data def main(sales_path, product_path): sales_data, product_data = load_data(sales_path, product_path) sales_data, product_data = clean_data(sales_data, product_data) merged_data = merge_data(sales_data, product_data) transformed_data = transform_data(merged_data) result = analyze_data(transformed_data) return result # Execute the main function with paths to the data files # result_df = main(\'sales_data.csv\', \'product_data.csv\') # print(result_df)"},{"question":"# Question: Advanced Assignment and Control Flow Statements You are required to write a Python function that will handle multiple operations on a list of student records. Each student record is represented as a dictionary with fields `name`, `scores`, and `status`. The function should perform the following steps: 1. Use **assignment statements** to create a new list of students who have passed (i.e., the average score is 60 or above). 2. Use **augmented assignment statements** to update the total attendance percentage of the classroom using a provided attendance percentage for the day. 3. Use **annotated assignment statements** to annotate a variable `average_score` which should hold the average score of the class after filtering the students who have passed. 4. Use **assert statements** to ensure that the list of students who have passed is not empty. If it is, raise an appropriate assertion error. 5. Use **del statements** to remove the `status` field from each passed student record as it is no longer needed. 6. Use a `yield` statement within a generator function to yield student names who have the highest score in the class. Below are the specifications: ```python from typing import List, Dict, Generator def filter_students(students: List[Dict[str, any]], daily_attendance: float) -> Generator[str, None, None]: Function to process student records and update classroom details. Args: - students (List[Dict[str, any]]): List of student records. Each record contains: - name (str): The name of the student - scores (List[int]): A list of scores obtained by the student - status (str): The status of the student - daily_attendance (float): The attendance percentage for the day to be added to total attendance. Yields: - Names of students who have the highest score in the class. # Step 1: Filter students who have passed passed_students = [student for student in students if (sum(student[\'scores\']) / len(student[\'scores\'])) >= 60] # Step 2: Update total attendance total_attendance = 0.0 total_attendance += daily_attendance # Step 3: Annotate the average score variable and calculate it average_score: float = sum(sum(student[\'scores\']) for student in students) / sum(len(student[\'scores\']) for student in students) # Step 4: Assert that list of passed students is not empty assert passed_students, \\"No students have passed.\\" # Step 5: Remove status field from each passed student record for student in passed_students: del student[\'status\'] # Step 6: Yield names of students who have the highest score in the class highest_score = max(max(student[\'scores\']) for student in students) for student in students: if highest_score in student[\'scores\']: yield student[\'name\'] ``` # Constraints: - Each student\'s score list will have at least one score. - `daily_attendance` is a non-negative float. # Example: ```python students = [ {\\"name\\": \\"Alice\\", \\"scores\\": [85, 90, 78], \\"status\\": \\"active\\"}, {\\"name\\": \\"Bob\\", \\"scores\\": [55, 60, 47], \\"status\\": \\"inactive\\"}, {\\"name\\": \\"Charlie\\", \\"scores\\": [95, 98, 100], \\"status\\": \\"active\\"} ] attendance = 5.0 result = list(filter_students(students, attendance)) # Expected result: [\'Charlie\'] # Explanation: Charlie has the highest score in the class. ```","solution":"from typing import List, Dict, Generator def filter_students(students: List[Dict[str, any]], daily_attendance: float) -> Generator[str, None, None]: Function to process student records and update classroom details. Args: - students (List[Dict[str, any]]): List of student records. Each record contains: - name (str): The name of the student - scores (List[int]): A list of scores obtained by the student - status (str): The status of the student - daily_attendance (float): The attendance percentage for the day to be added to total attendance. Yields: - Names of students who have the highest score in the class. # Step 1: Filter students who have passed passed_students = [student for student in students if (sum(student[\'scores\']) / len(student[\'scores\'])) >= 60] # Step 2: Update total attendance total_attendance = 0.0 total_attendance += daily_attendance # Step 3: Annotate the average score variable and calculate it average_score: float = sum(sum(student[\'scores\']) for student in students) / sum(len(student[\'scores\']) for student in students) # Step 4: Assert that list of passed students is not empty assert passed_students, \\"No students have passed.\\" # Step 5: Remove status field from each passed student record for student in passed_students: del student[\'status\'] # Step 6: Yield names of students who have the highest score in the class highest_score = max(max(student[\'scores\']) for student in students) for student in students: if highest_score in student[\'scores\']: yield student[\'name\']"},{"question":"**Event Scheduler Simulation** You are required to implement a simulation using Python\'s `sched` module to model an event scheduler that organizes and manages various tasks. The simulation involves creating and handling multiple events with different priorities and execution times. **Task**: 1. Implement the function `schedule_events()` that takes a list of event dictionaries and schedules them using the `sched.scheduler` class. 2. Each event dictionary contains the keys `time`, `priority`, `action`, `argument`, and `kwargs`. **Function Signature**: ```python def schedule_events(events: list[dict]) -> list[tuple]: Schedule the events and return the log of executed actions. Args: - events (list of dict): A list of events to be scheduled. Each event dictionary contains: - \'time\' (float): The absolute time at which the event should run. - \'priority\' (int): The priority of the event (lower number means higher priority). - \'action\' (callable): The function to run. - \'argument\' (tuple): The positional arguments to pass to the function. - \'kwargs\' (dict): The keyword arguments to pass to the function. Returns: - list of tuple: A list of tuples representing the log of executed actions in the format: (execution_time, action_name, args, kwargs) pass ``` **Additional Requirements**: 1. The function should return a list of executed actions in the order they were run. Each action\'s log entry should be a tuple containing the exact execution time, the action\'s name, the arguments, and the keyword arguments. 2. Use the `time.time` function for the current time and `time.sleep` for delays. **Example**: ```python import time # Define some example actions def example_action1(a): print(f\\"Action 1 executed with argument: {a}\\") def example_action2(b, c): print(f\\"Action 2 executed with arguments: {b}, {c}\\") # List of events events = [ {\'time\': time.time() + 5, \'priority\': 1, \'action\': example_action1, \'argument\': (\'Hello\',), \'kwargs\': {}}, {\'time\': time.time() + 3, \'priority\': 2, \'action\': example_action2, \'argument\': (\'Python\',), \'kwargs\': {\'c\': \'Code\'}}, {\'time\': time.time() + 1, \'priority\': 3, \'action\': example_action1, \'argument\': (\'World\',), \'kwargs\': {}}, ] # Schedule the events and retrieve the log log = schedule_events(events) for entry in log: print(entry) ``` **Expected Output** (Approximate): ``` Action 1 executed with argument: World Action 2 executed with arguments: Python, Code Action 1 executed with argument: Hello (1652342830.3640375, \'example_action1\', (\'World\',), {}) (1652342831.3640375, \'example_action2\', (\'Python\',), {\'c\': \'Code\'}) (1652342832.3640375, \'example_action1\', (\'Hello\',), {}) ``` **Constraints**: - The provided `time` values are always in the future relative to the current time when `schedule_events` is called. - Ensure thread safety and proper handling of functions and delays. This exercise tests a student\'s understanding of Python\'s `sched` module, the use of scheduled event handling, and their ability to manipulate and manage delayed task execution.","solution":"import sched import time from typing import List, Dict, Tuple def schedule_events(events: List[Dict]) -> List[Tuple]: Schedule the events and return the log of executed actions. Args: - events (list of dict): A list of events to be scheduled. Each event dictionary contains: - \'time\' (float): The absolute time at which the event should run. - \'priority\' (int): The priority of the event (lower number means higher priority). - \'action\' (callable): The function to run. - \'argument\' (tuple): The positional arguments to pass to the function. - \'kwargs\' (dict): The keyword arguments to pass to the function. Returns: - list of tuple: A list of tuples representing the log of executed actions in the format: (execution_time, action_name, args, kwargs) scheduler = sched.scheduler(time.time, time.sleep) log = [] def action_wrapper(action, action_name, argument, kwargs): action(*argument, **kwargs) log.append((time.time(), action_name, argument, kwargs)) for event in events: event_time = event[\'time\'] - time.time() scheduler.enterabs(event[\'time\'], event[\'priority\'], action_wrapper, argument=(event[\'action\'], event[\'action\'].__name__, event[\'argument\'], event[\'kwargs\'])) scheduler.run() return log"},{"question":"You are tasked with creating a simplified version of an asynchronous chat server using the deprecated `asynchat` module. The subtask is to implement a subclass of `asynchat.async_chat` that processes incoming chat messages, identifies commands, and handles asynchronous data reception and transmission. # Requirements: 1. **Class Definition**: - Create a class `ChatHandler` that inherits from `asynchat.async_chat`. 2. **Data Collection**: - Implement the method `collect_incoming_data(data)` to buffer the incoming data into a list. 3. **Termination Handling**: - Implement the method `found_terminator()` to process buffered data when a newline character `\\"n\\"` is found as the terminator. - Identify if the message is a command (starts with a `/`). If it is a command, handle it accordingly (you can define sample command handling like `/ping` which responds with `pong`). - If it is not a command, simply echo the message back to the sender with a prefix \\"Echo: \\". 4. **Initialization**: - In the constructor, set up the terminator to `\\"n\\"` and initialize necessary attributes for buffering incoming data. 5. **Testing**: - Create an instance of your `ChatHandler` class and simulate the reception of messages, both regular messages and commands. # Method Specifications: **collect_incoming_data(self, data)** - **Input:** `data` (bytes) â€“ Data chunk received asynchronously. - **Output:** None â€“ Buffers the data internally. **found_terminator(self)** - **Input:** None - **Output:** None â€“ Processes the buffered data when the terminator is found and sends appropriate responses. # Constraints: - You are only required to handle the reception and basic processing of messages and commands for this exercise. - You don\'t need to set up actual socket connections; instead, you can simulate the input and output as method calls. - Use default buffer sizes. # Example Implementation: Your code should resemble the following: ```python import asynchat import asyncore class ChatHandler(asynchat.async_chat): def __init__(self, sock): asynchat.async_chat.__init__(self, sock=sock) self.set_terminator(b\'n\') self.ibuffer = [] def collect_incoming_data(self, data): self.ibuffer.append(data) def found_terminator(self): message = b\'\'.join(self.ibuffer).decode(\'utf-8\').strip() self.ibuffer = [] if message.startswith(\'/\'): if message == \'/ping\': response = \'pongn\' else: response = \'Unknown commandn\' else: response = f\'Echo: {message}n\' self.push(response.encode(\'utf-8\')) # Simulate input for testing handler = ChatHandler(None) handler.collect_incoming_data(b\'Hellon\') handler.found_terminator() handler.collect_incoming_data(b\'/pingn\') handler.found_terminator() ``` # Explanation: 1. **Class Initialization**: The `ChatHandler` class is initialized with a socket object and sets the terminator to newline (`n`). It also initializes an input buffer list. 2. **Data Collection**: The `collect_incoming_data` method appends each chunk of incoming data to the buffer. 3. **Termination Handling**: The `found_terminator` method processes and clears the buffer when the terminator is found. It handles both commands and regular messages by sending appropriate responses. Complete these tasks, and ensure that your implementation accurately follows the outlined specifications.","solution":"import asynchat import asyncore class ChatHandler(asynchat.async_chat): def __init__(self, sock=None): super().__init__(sock=sock) self.set_terminator(b\'n\') self.ibuffer = [] def collect_incoming_data(self, data): self.ibuffer.append(data) def found_terminator(self): message = b\'\'.join(self.ibuffer).decode(\'utf-8\').strip() self.ibuffer = [] if message.startswith(\'/\'): if message == \'/ping\': response = \'pongn\' else: response = \'Unknown commandn\' else: response = f\'Echo: {message}n\' self.push(response.encode(\'utf-8\')) def push(self, data): Overriding push to handle the output in-memory for testing self.output.append(data) # Helper function for testing which simulates handling input and collecting output def simulate_chat_handler_input(messages): handler = ChatHandler() handler.output = [] for message in messages: handler.collect_incoming_data(message) handler.found_terminator() return [msg.decode(\'utf-8\') for msg in handler.output]"},{"question":"**Objective:** To assess the student\'s understanding of asyncio task management, task synchronization, and exception handling. **Problem Statement:** You are required to write an asyncio-based function that simulates a basic server-client interaction using TCP connections. The server will handle multiple clients concurrently, responding to each client\'s message after a certain delay. Additionally, ensure proper synchronization between tasks to control access to shared resources and handle potential exceptions. **Function Signature:** ```python async def run_server_client_interaction(port: int, message: str, client_count: int): pass ``` **Parameters:** - `port` (int): The port number on which the server will listen for incoming connections. - `message` (str): The message that each client will send to the server. - `client_count` (int): The number of clients that will connect to the server concurrently. **Requirements:** 1. Implement an async TCP server that listens on the specified port. 2. Simulate multiple clients connecting to the server concurrently. 3. Each client should send a message to the server and await a response. 4. The server, upon receiving a message, should wait for 2 seconds (to simulate processing) before sending a response back to the client. 5. Use proper synchronization mechanisms to handle shared state (if any). 6. Handle potential exceptions like timeouts and task cancellations gracefully. **Example:** ```python import asyncio async def run_server_client_interaction(port: int, message: str, client_count: int): # Implementation goes here pass # Example usage asyncio.run(run_server_client_interaction(8888, \\"Hello Server\\", 5)) ``` In the example above, the function should create a server on port 8888 and manage 5 clients sending the message \\"Hello Server\\" concurrently. **Constraints:** - Do not use built-in TCP server methods like `await start_server()`. - Use high-level asyncio APIs where possible. - Ensure that the implementation is non-blocking and handles concurrency gracefully. - Consider edge cases and potential race conditions. **Performance Requirements:** - The function should efficiently handle up to 1000 concurrent clients without significant performance degradation. **Hint:** You may find it useful to create helper functions for the server and clients. Utilize asyncio synchronization primitives like `Lock` or `Semaphore` if needed to coordinate access to shared resources.","solution":"import asyncio async def handle_client(reader, writer): # Read the data sent by the client data = await reader.read(100) message = data.decode() # Simulate processing time await asyncio.sleep(2) # Send a response back to the client writer.write(data) await writer.drain() # Close the connection writer.close() await writer.wait_closed() async def run_server(port): server = await asyncio.start_server(handle_client, \'127.0.0.1\', port) async with server: await server.serve_forever() async def client(port, message): reader, writer = await asyncio.open_connection(\'127.0.0.1\', port) writer.write(message.encode()) await writer.drain() data = await reader.read(100) response = data.decode() writer.close() await writer.wait_closed() return response async def run_server_client_interaction(port: int, message: str, client_count: int): # Start the server server_task = asyncio.create_task(run_server(port)) # Give the server a small amount of time to start await asyncio.sleep(0.1) # Create multiple clients and gather results client_tasks = [asyncio.create_task(client(port, message)) for _ in range(client_count)] responses = await asyncio.gather(*client_tasks) # Cancel the server task server_task.cancel() try: await server_task except asyncio.CancelledError: pass return responses"},{"question":"Data Classes and Advanced Features Objective You are required to implement a Python class using the `dataclass` decorator from the `dataclasses` module. The implementation should demonstrate the understanding of various advanced features provided by the module such as post-init processing, frozen instances, inheritance, default factory functions, and descriptor-typed fields. Problem Statement Write a Python program that defines a `dataclass` named `Book`. The `Book` class should have the following attributes: - `title`: a string representing the title of the book, default is an empty string. - `author`: a string representing the author of the book, default is an empty string. - `published_year`: an integer representing the year the book was published, default is 1900. - `genres`: a list of strings representing the genres of the book, default is an empty list. - `price`: a float representing the price of the book, default is 0.0. - `discount`: a read-only float representing the discount on the book. This should be a calculated field based on the `price` and calculated in the `__post_init__` method. Additionally, create a subclass `UsedBook` that inherits from `Book` and adds an attribute: - `condition`: a string representing the condition of the book, default is \'Good\'. The `UsedBook` subclass should override the `discount` calculation such that: - If the condition is \'Good\', the discount is 10% of the price. - If the condition is \'Fair\', the discount is 20% of the price. - If the condition is \'Poor\', the discount is 30% of the price. Constraints - Use appropriate type hints for all attributes. - Ensure `discount` is a read-only attribute. - You are not allowed to modify the values of `title`, `author`, `published_year`, `genres`, and `price` after object creation for both `Book` and `UsedBook` classes. Input and Output Format The solution should include: - A demonstration of creating instances of both `Book` and `UsedBook` with various attributes. - Printing the details of the instances created, along with their calculated discount. Example ```python from dataclasses import dataclass, field @dataclass(frozen=True) class Book: title: str = \\"\\" author: str = \\"\\" published_year: int = 1900 genres: list[str] = field(default_factory=list) price: float = 0.0 def __post_init__(self): object.__setattr__(self, \'discount\', self.price * 0.05) @dataclass(frozen=True) class UsedBook(Book): condition: str = \\"Good\\" def __post_init__(self): super().__post_init__() discount_rate = { \\"Good\\": 0.10, \\"Fair\\": 0.20, \\"Poor\\": 0.30 }.get(self.condition, 0.10) object.__setattr__(self, \'discount\', self.price * discount_rate) # Example Usage book1 = Book(title=\\"Python 101\\", author=\\"John Doe\\", price=100.0) used_book1 = UsedBook(title=\\"Advanced Python\\", author=\\"Jane Doe\\", price=150.0, condition=\\"Fair\\") print(book1) print(f\\"Discount: {book1.discount}\\") print(used_book1) print(f\\"Discount: {used_book1.discount}\\") ``` This problem will test the student\'s ability to implement advanced data class functionalities and ensure proper usage of post-initialization processing and inheritance features in Python.","solution":"from dataclasses import dataclass, field @dataclass(frozen=True) class Book: title: str = \\"\\" author: str = \\"\\" published_year: int = 1900 genres: list[str] = field(default_factory=list) price: float = 0.0 def __post_init__(self): object.__setattr__(self, \'discount\', self.price * 0.05) @dataclass(frozen=True) class UsedBook(Book): condition: str = \\"Good\\" def __post_init__(self): super().__post_init__() discount_rate = { \\"Good\\": 0.10, \\"Fair\\": 0.20, \\"Poor\\": 0.30 }.get(self.condition, 0.10) object.__setattr__(self, \'discount\', self.price * discount_rate) # Example Usage book1 = Book(title=\\"Python 101\\", author=\\"John Doe\\", price=100.0) used_book1 = UsedBook(title=\\"Advanced Python\\", author=\\"Jane Doe\\", price=150.0, condition=\\"Fair\\") print(book1) print(f\\"Discount: {book1.discount}\\") print(used_book1) print(f\\"Discount: {used_book1.discount}\\")"},{"question":"# Advanced Coding Assessment Question: Stack Trace Visualization using `traceback` module Objective: Write a Python function `debug_wrapper` that takes another function as input and returns a new function. This returned function executes the input function and, in case of an exception, captures and returns a well-formatted string of the stack trace, including exception type, message, and the stack trace entries. Function Signature: ```python def debug_wrapper(func: callable) -> callable: Takes a function `func` and returns a new function that executes `func`. If an exception occurs, it captures and returns the formatted stack trace. Parameters: func (callable): The function to be wrapped. Returns: callable: A new function that executes `func` and handles exceptions. ``` Input: - `func`: A callable function to be wrapped by `debug_wrapper`. Output: - A callable function that, when called, executes the original function `func`. - If `func` raises an exception, the returned function should catch the exception and return a detailed string of the formatted stack trace. - If `func` executes without exceptions, it should return the original return value. Constraints: - Use only the standard Python library. - Utilize the `traceback` module for handling exceptions and formatting the stack trace. - Handle various types of exceptions including syntax errors, runtime errors, and user-defined exceptions. Example Usage: ```python def test_function(): return 1 / 0 # This will raise a ZeroDivisionError wrapped_function = debug_wrapper(test_function) # Expecting a formatted stack trace string for ZeroDivisionError print(wrapped_function()) ``` Example Output: ``` Traceback (most recent call last): File \\"example.py\\", line 2, in wrapped_function return func(*args, **kwargs) File \\"example.py\\", line 2, in test_function return 1 / 0 ZeroDivisionError: division by zero ``` Performance Requirements: - The implementation should be able to handle deep stack traces efficiently. - It should properly handle nested exceptions and provide complete traceback information. Additional Information: The formatted stack trace should mimic the output format of native Python exceptions to ensure readability and usability.","solution":"import traceback def debug_wrapper(func: callable) -> callable: def wrapped_function(*args, **kwargs): try: return func(*args, **kwargs) except Exception as e: return traceback.format_exc() return wrapped_function"},{"question":"**PyTorch Custom Dataset and DataLoader Implementation** **Objective:** Implement a custom dataset and utilize PyTorch\'s DataLoader class to process and batch the data correctly. This exercise will test your understanding of creating custom datasets and handling them efficiently using DataLoader. **Description:** You need to design a custom dataset that loads data from a set of text files. Each text file contains lines of text, and each line should be considered a separate data point. Implement a custom Dataset class to load this data and then use the DataLoader class to batch the data. **Tasks:** 1. Implement a custom Dataset class named `TextFileDataset` that: - Takes a directory path (containing text files) as input during initialization. - Reads all text files in the specified directory. - Stores each line from these files as a separate data point. - Implement the `__len__` method to return the total number of lines across all files. - Implement the `__getitem__` method to retrieve a line given its index. 2. Use the `torch.utils.data.DataLoader` class to: - Create an instance of DataLoader with the `TextFileDataset`. - Batch the data with a specified batch size. - Shuffle the data. **Input Format:** - Directory path containing text files. - Batch size for DataLoader. **Output Format:** - Print each batch of data (a list of lines) to the console. **Constraints:** - Assume all text files are in the same directory. - Assume each line in the text files is unique for simplicity. **Example Code Outline:** ```python import os from torch.utils.data import Dataset, DataLoader class TextFileDataset(Dataset): def __init__(self, directory_path): # Initialize the dataset, read all text files and store lines. pass def __len__(self): # Return the length of the dataset. pass def __getitem__(self, idx): # Retrieve the data point at the specified index. pass # Directory path containing text files. directory_path = \\"path/to/text/files\\" batch_size = 4 # Create the dataset. dataset = TextFileDataset(directory_path) # Create the DataLoader. data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # Iterate over the DataLoader and print each batch. for batch in data_loader: print(batch) ``` **Performance Requirements:** - The implementation should read all files into memory efficiently. - The DataLoader usage should ensure that the data loading process is optimized for batch processing. Let\'s assess your implementation of this custom dataset and how you utilize the DataLoader for batching and shuffling.","solution":"import os from torch.utils.data import Dataset, DataLoader class TextFileDataset(Dataset): def __init__(self, directory_path): self.lines = [] for filename in os.listdir(directory_path): if not filename.endswith(\'.txt\'): continue with open(os.path.join(directory_path, filename), \'r\') as file: self.lines.extend(file.readlines()) def __len__(self): return len(self.lines) def __getitem__(self, idx): return self.lines[idx].strip() # Usage example (assuming a directory path and batch size are provided) def main(directory_path, batch_size): # Create the dataset. dataset = TextFileDataset(directory_path) # Create the DataLoader. data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # Iterate over the DataLoader and print each batch. for batch in data_loader: print(batch) # Example usage: # main(\\"path/to/text/files\\", 4)"},{"question":"Decorated Function with Multiple Parameters Objective: To assess your understanding of function definitions, decorators, parameter handling, and basic arithmetic operations in Python. Problem Statement: Write a Python function `arithmetic_operations` that performs various arithmetic operations on two numbers. The function should accept two decorated functions as arguments, each taking two integers and returning an integer. The `arithmetic_operations` function should then compute and return a dictionary containing the results of applying the decorated functions for the following operations: addition, subtraction, multiplication, and division. You are also required to write a decorator `double_result` that modifies the behavior of the provided functions such that they return double the result of the original function. Specifications: 1. **Decorator `double_result`**: - This decorator should take a function as an argument and return a new function that doubles the result of the original function. 2. **Function `arithmetic_operations`**: - This function should take two decorated functions, `add_func` and `sub_func`, as arguments. - The function should perform the following operations using these decorated functions: - Addition - Subtraction - Multiplication - Division - The function should return a dictionary with the results of these operations. The keys of the dictionary should be `\\"add\\"`, `\\"subtract\\"`, `\\"multiply\\"`, and `\\"divide\\"`. 3. **Function Definitions**: - Define two base functions, `add` (for addition) and `subtract` (for subtraction), that take two integers as arguments and return the result of the corresponding arithmetic operation. 4. **Input and Output**: - **Input**: Two integers, `num1` and `num2`. - **Output**: A dictionary with the results of the arithmetic operations after applying the decorated functions. Example: ```python @double_result def add(a, b): return a + b @double_result def subtract(a, b): return a - b def arithmetic_operations(add_func, sub_func, num1, num2): return { \\"add\\": add_func(num1, num2), \\"subtract\\": sub_func(num1, num2), \\"multiply\\": add_func(num1, num2) * sub_func(num1, num2), \\"divide\\": add_func(num1, num2) // sub_func(num1, num2) if sub_func(num1, num2) != 0 else \'undefined\' } num1 = 10 num2 = 5 result = arithmetic_operations(add, subtract, num1, num2) print(result) # Output: {\'add\': 30, \'subtract\': 10, \'multiply\': 300, \'divide\': 3} ``` Constraints: - You can assume that the input numbers are integers. - Division by zero should return `\'undefined\'` for the division result. Notes: - Make sure to use the provided decorator `double_result`. - Ensure that your functions and decorators are correctly implemented and handle the inputs as described. Bonus: - Implement additional decorated functions and extend the `arithmetic_operations` function to handle more complex operations (e.g., power, modulo).","solution":"def double_result(func): Decorator that doubles the result of the given function. def wrapper(a, b): return 2 * func(a, b) return wrapper @double_result def add(a, b): Returns the sum of a and b. return a + b @double_result def subtract(a, b): Returns the difference of a and b. return a - b def arithmetic_operations(add_func, sub_func, num1, num2): Performs various arithmetic operations using the provided decorated functions. Parameters: - add_func: Decorated function for addition - sub_func: Decorated function for subtraction - num1: First integer - num2: Second integer Returns: Dictionary containing the results of addition, subtraction, multiplication, and division operations. addition_result = add_func(num1, num2) subtraction_result = sub_func(num1, num2) multiplication_result = addition_result * subtraction_result division_result = (addition_result // subtraction_result if subtraction_result != 0 else \'undefined\') return { \\"add\\": addition_result, \\"subtract\\": subtraction_result, \\"multiply\\": multiplication_result, \\"divide\\": division_result }"},{"question":"# Question: You are given a set of data points for two variables, `x` and `y`. Your task is to create a customized plot using Seaborn\'s `seaborn.objects` module. The plot should meet the following requirements: 1. Create a scatter plot with the given data points. 2. Pin the x-axis limits to (0, 10) and the y-axis limits to (-5, 15). 3. Then, invert the y-axis such that the maximum value is 0 and the minimum value is -5. 4. Finally, create another plot that resets the y-axis limits to default while maintaining the inverted order. # Input: - Two lists, `x` and `y`, containing integer or float values. For example: ```python x = [1, 2, 3, 4, 5] y = [2, 5, 3, 8, 7] ``` # Output: - Display the resulting plots using seaborn. # Constraints: - Use only the `seaborn.objects` module for creating and customizing plots. # Example: ```python import seaborn.objects as so def create_custom_plots(x, y): # Create the initial plot with specified axis limits p1 = so.Plot(x=x, y=y).add(so.Line(marker=\\"o\\")).limit(x=(0, 10), y=(-5, 15)) p1.show() # Display the first plot # Invert the y-axis p2 = p1.limit(y=(0, -5)) p2.show() # Display the second plot with inverted y-axis # Reset y-axis to default limits while keeping inverted order p3 = p1.limit(y=(0, None)) p3.show() # Display the third plot with default-inverted y-axis # Example usage x = [1, 2, 3, 4, 5] y = [2, 5, 3, 8, 7] create_custom_plots(x, y) ``` Make sure your function is appropriately handling the data inputs and displaying the plots as described.","solution":"import seaborn.objects as so def create_custom_plots(x, y): Create customized scatter plots using Seaborn\'s seaborn.objects module. Displays three plots as per the given requirements. Parameters: x (list): List of x data points (int or float). y (list): List of y data points (int or float). # Create the initial plot with specified axis limits p1 = so.Plot(x=x, y=y).add(so.Dot()).limit(x=(0, 10), y=(-5, 15)) p1.show() # Display the first plot # Invert the y-axis p2 = p1.limit(y=(0, -5)) p2.show() # Display the second plot with inverted y-axis # Reset y-axis to default limits while keeping inverted order p3 = p2.limit(y=(0, None)) p3.show() # Display the third plot with default-inverted y-axis"},{"question":"**Objective:** Create visualizations using `seaborn` that demonstrate a comprehensive understanding of statistical error bars, including standard deviation, percentile intervals, standard error, and confidence intervals. **Task:** 1. Generate a synthetic dataset with 100 data points from a normal distribution with mean 0 and standard deviation 1 for a variable `x` and another variable `y` that has a linear relationship with `x` plus some noise. 2. Create four subplots in a single figure: - The first subplot should display a point plot of `x` with error bars representing the standard deviation (`sd`). - The second subplot should display a point plot of `x` with error bars representing the 50% percentile interval (`pi`, 25th to 75th percentile). - The third subplot should display a point plot of `x` with error bars representing the standard error (`se`). - The fourth subplot should display a regression plot of `x` and `y` with a confidence interval (`ci`). Given the synthetic dataset, implement the following function: ```python import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_with_errorbars(): # Step 1: Generate the synthetic dataset np.random.seed(42) x = np.random.normal(0, 1, 100) y = 2 * x + np.random.normal(0, 1, 100) # Step 2: Set up the figure and subplots fig, axs = plt.subplots(2, 2, figsize=(15, 10)) # Step 3: Plot with standard deviation error bars sns.pointplot(x=x, errorbar=\\"sd\\", ax=axs[0, 0]) axs[0, 0].set_title(\\"Standard Deviation Error Bars\\") # Step 4: Plot with percentile interval error bars (50%) sns.pointplot(x=x, errorbar=(\\"pi\\", 50), ax=axs[0, 1]) axs[0, 1].set_title(\\"Percentile Interval Error Bars (50%)\\") # Step 5: Plot with standard error bars sns.pointplot(x=x, errorbar=\\"se\\", ax=axs[1, 0]) axs[1, 0].set_title(\\"Standard Error Bars\\") # Step 6: Plot regression with confidence interval sns.regplot(x=x, y=y, ci=95, ax=axs[1, 1]) axs[1, 1].set_title(\\"Regression with Confidence Interval\\") # Step 7: Adjust layout and display the plot plt.tight_layout() plt.show() # Call the function to display the plots plot_with_errorbars() ``` **Constraints:** - Make sure the plots are clear and well-labeled. - Use different styles (e.g., markers, colors) if needed to distinguish the error bars. - Ensure the confidence interval on the regression plot is clearly visible. **Expected Output:** A figure with four subplots displaying various error bars and a regression plot with a confidence interval based on the provided synthetic data. Each subplot should have a descriptive title indicating the type of error bar used. **Evaluation:** Students will be evaluated on the correctness of the implementation, the clarity and readability of the plots, appropriate use of seaborn functions, and the accurate representation of error bars.","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_with_errorbars(): # Step 1: Generate the synthetic dataset np.random.seed(42) x = np.random.normal(0, 1, 100) y = 2 * x + np.random.normal(0, 1, 100) # Step 2: Set up the figure and subplots fig, axs = plt.subplots(2, 2, figsize=(15, 10)) # Step 3: Plot with standard deviation error bars sns.pointplot(x=x, errorbar=\\"sd\\", ax=axs[0, 0]) axs[0, 0].set_title(\\"Standard Deviation Error Bars\\") # Step 4: Plot with percentile interval error bars (50%) sns.pointplot(x=x, errorbar=(\\"pi\\", 50), ax=axs[0, 1]) axs[0, 1].set_title(\\"Percentile Interval Error Bars (50%)\\") # Step 5: Plot with standard error bars sns.pointplot(x=x, errorbar=\\"se\\", ax=axs[1, 0]) axs[1, 0].set_title(\\"Standard Error Bars\\") # Step 6: Plot regression with confidence interval sns.regplot(x=x, y=y, ci=95, ax=axs[1, 1]) axs[1, 1].set_title(\\"Regression with Confidence Interval\\") # Step 7: Adjust layout and display the plot plt.tight_layout() plt.show() # Call the function to display the plots plot_with_errorbars()"},{"question":"Your task is to write a Python function that demonstrates the use of thread-based parallelism using the `threading` module. Specifically, you need to calculate the sum of squares of a given list of integers in parallel using multiple threads. # Function Signature ```python def parallel_sum_of_squares(numbers: List[int], num_threads: int) -> int: ``` # Input Parameters - `numbers`: A list of integers `[n1, n2, n3, ..., nk]` - `num_threads`: An integer representing the number of threads to be used for the computation. # Output - Returns an integer which is the sum of squares of the integers in the `numbers` list. # Constraints - The `numbers` list can contain up to (10^6) integers. - You can assume (1 leq text{num_threads} leq 1000). - The integers in the `numbers` list can range from (-10^4) to (10^4). # Detailed Requirements 1. You should divide the `numbers` list into roughly equal parts depending on the number of threads specified by `num_threads`. 2. Each thread should compute the sum of squares for its portion of the list. 3. Use appropriate `threading` constructs like `Thread` objects to create and manage threads. 4. Ensure the final result is correctly aggregated from the partial results computed by each thread. # Example ```python numbers = [1, 2, 3, 4, 5] num_threads = 2 result = parallel_sum_of_squares(numbers, num_threads) print(result) # Output should be 55 (since 1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 55) ``` # Performance Requirements - The function should utilize threads efficiently and should not have significant overhead compared to a single-threaded approach. - Ensure synchronization and avoid race conditions. # Hints - You may use `threading.Thread` to create threads and `threading.Lock` to handle synchronization if needed.","solution":"import threading from typing import List def parallel_sum_of_squares(numbers: List[int], num_threads: int) -> int: def worker(sublist, result, index): result[index] = sum(x ** 2 for x in sublist) # Split the numbers list into roughly equal parts depending on num_threads length = len(numbers) part_size = (length + num_threads - 1) // num_threads # This ensures we don\'t miss any items threads = [] results = [0] * num_threads for i in range(num_threads): start = i * part_size end = min((i + 1) * part_size, length) sublist = numbers[start:end] thread = threading.Thread(target=worker, args=(sublist, results, i)) threads.append(thread) thread.start() for thread in threads: thread.join() return sum(results)"},{"question":"# Profiling Functions in Python In this assessment, you will demonstrate your understanding of Python\'s `cProfile` and `pstats` modules by profiling a provided function and analyzing the results. Objective You need to: 1. Profile a given function using the `cProfile` module. 2. Save the profiling results to a file. 3. Read the saved profiling data using the `pstats` module. 4. Provide a summary analysis of the profiling results including the total function calls, functions with the highest cumulative time and internal time. Provided Function: Here is the function you need to profile: ```python def sample_function(): import time def fibonacci(n): if n < 0: raise ValueError(\\"Input should be a non-negative integer\\") elif n == 0: return 0 elif n == 1: return 1 else: return fibonacci(n-1) + fibonacci(n-2) start_time = time.time() result = fibonacci(15) # Adjust the input value for reasonable execution time during profiling end_time = time.time() print(f\\"Fibonacci(15) = {result}, Time taken: {end_time - start_time} seconds\\") ``` Task 1. Use the `cProfile` module to profile the `sample_function`. 2. Save the profiling results to a file named `profile_output.prof`. 3. Use the `pstats` module to read the profiling results from the file. 4. Print a summary report of the profiling results which includes: - The total number of function calls. - The 5 functions with the highest cumulative time. - The 5 functions with the highest internal time. Constraints - Do not alter the provided function except to integrate profiling. - Ensure that your output is clear and well-formatted. Expected Output Your output should look something like this (the actual times will vary): ``` Total function calls: 485 Functions with highest cumulative time: 1. <function fibonacci at 0x7f8e283c8a60>: 0.015 seconds 2. ... Functions with highest internal time: 1. <function fibonacci at 0x7f8e283c8a60>: 0.015 seconds 2. ... ``` Implementation Details You can start by following these steps in your implementation: 1. Profile the `sample_function` and save the results: ```python import cProfile cProfile.run(\'sample_function()\', \'profile_output.prof\') ``` 2. Read and analyze the profiling results: ```python import pstats from pstats import SortKey p = pstats.Stats(\'profile_output.prof\') p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(5) p.strip_dirs().sort_stats(SortKey.TIME).print_stats(5) ``` Complete the outlined tasks and provide the summarized profiling report. Make sure your code is well-organized and that the profiling results are clearly presented.","solution":"def sample_function(): import time def fibonacci(n): if n < 0: raise ValueError(\\"Input should be a non-negative integer\\") elif n == 0: return 0 elif n == 1: return 1 else: return fibonacci(n-1) + fibonacci(n-2) start_time = time.time() result = fibonacci(15) # Adjust the input value for reasonable execution time during profiling end_time = time.time() print(f\\"Fibonacci(15) = {result}, Time taken: {end_time - start_time} seconds\\") def profile_function(): import cProfile import pstats # Profile the sample_function and save to a .prof file with cProfile.Profile() as pr: sample_function() pr.dump_stats(\'profile_output.prof\') # Read the profiling results and print the summary p = pstats.Stats(\'profile_output.prof\') print(\\"Total function calls:\\", p.total_calls) print(\\"nFunctions with highest cumulative time:\\") p.strip_dirs().sort_stats(pstats.SortKey.CUMULATIVE).print_stats(5) print(\\"nFunctions with highest internal time:\\") p.strip_dirs().sort_stats(pstats.SortKey.TIME).print_stats(5)"},{"question":"# Question: Visualizing Categorical Data with Seaborn You are provided with a dataset related to tips received by waiters in a restaurant. Your task is to visualize this data using various seaborn categorical plots and customize them based on given requirements. Dataset The dataset is available in seaborn as `tips`. You can load it using: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` Requirements 1. **Categorical Scatterplot**: - Create a categorical scatterplot (stripplot) to show the relationship between the days of the week (`day`) and the total bill (`total_bill`). - Add jitter to the points for better visualization. - Use different colors for points based on the time of day (`time`). 2. **Boxplot**: - Create a boxplot showing the distribution of total bill (`total_bill`) for different days of the week (`day`). - Include another categorical variable (`smoker`) to show the distribution separately for smokers and non-smokers. - Ensure the box plots are in the order of [\\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"]. 3. **Violin Plot**: - Create a violin plot to show the distribution of tip amounts (`tip`) for different days of the week (`day`). - Differentiate the distributions based on the gender (`sex`) by splitting the violins. - Use a pastel color palette for the plots. 4. **Bar Plot**: - Create a bar plot showing the average total bill (`total_bill`) for different days of the week (`day`). - Add error bars showing 95% confidence intervals. 5. **Faceted Plot**: - Create a `FacetGrid` to show the relationship between total bill (`total_bill`) and day of the week (`day`) in different panels based on the time of day (`time`). - Use a `swarmplot` to represent the data within each facet. Function Signature ```python def visualize_tips_data(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"ticks\\", color_codes=True) tips = sns.load_dataset(\\"tips\\") # 1. Categorical Scatterplot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"strip\\", jitter=True) plt.title(\'Stripplot of Total Bill by Day with Time Hue\') plt.show() # 2. Boxplot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"box\\", order=[\\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"]) plt.title(\'Boxplot of Total Bill by Day and Smoker\') plt.show() # 3. Violin Plot sns.catplot(data=tips, x=\\"day\\", y=\\"tip\\", hue=\\"sex\\", kind=\\"violin\\", split=True, palette=\\"pastel\\") plt.title(\'Violin Plot of Tips by Day and Sex\') plt.show() # 4. Bar Plot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", kind=\\"bar\\", ci=95) plt.title(\'Bar Plot of Average Total Bill by Day\') plt.show() # 5. Faceted Plot g = sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"swarm\\", col=\\"time\\", aspect=.7) g.fig.suptitle(\'Faceted Swarmplot of Total Bill by Day and Time\', y=1.02) plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data(): sns.set_theme(style=\\"ticks\\", color_codes=True) tips = sns.load_dataset(\\"tips\\") # 1. Categorical Scatterplot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"strip\\", jitter=True) plt.title(\'Stripplot of Total Bill by Day with Time Hue\') plt.show() # 2. Boxplot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"box\\", order=[\\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"]) plt.title(\'Boxplot of Total Bill by Day and Smoker\') plt.show() # 3. Violin Plot sns.catplot(data=tips, x=\\"day\\", y=\\"tip\\", hue=\\"sex\\", kind=\\"violin\\", split=True, palette=\\"pastel\\") plt.title(\'Violin Plot of Tips by Day and Sex\') plt.show() # 4. Bar Plot sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", kind=\\"bar\\", ci=95) plt.title(\'Bar Plot of Average Total Bill by Day\') plt.show() # 5. Faceted Plot g = sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"swarm\\", col=\\"time\\", aspect=.7) g.fig.suptitle(\'Faceted Swarmplot of Total Bill by Day and Time\', y=1.02) plt.show()"},{"question":"# Email Message Manipulation Using Python\'s \\"email\\" Package Objective: Write a function called `process_email` that takes a serialized email message as input, parses it, makes specific modifications, and returns the serialized modified email message. Function Signature: ```python def process_email(raw_email: bytes) -> bytes: pass ``` Input: - `raw_email` (bytes): A serialized email message as a byte stream. Output: - Returns a serialized byte stream of the modified email message. Requirements: 1. **Parsing**: Parse the input `raw_email` into an `EmailMessage` object. 2. **Modification**: - **Header**: Add a custom header field `X-Processed: True`. - **Body**: If the email is a multipart message, append the text \\"Processed by Python310\\" to the end of the body of the first part. If the email is not multipart, add this text to the end of the email body. 3. **Serialization**: Convert the modified `EmailMessage` object back into a serialized byte stream. Constraints: - You must use the \\"email\\" package\'s `EmailMessage`, `parser`, and `generator` components. - Ensure that your solution handles both multipart and non-multipart email messages correctly. - The function should not alter any other part of the email except as specified. Example: ```python raw_email = b From: user1@example.com To: user2@example.com Subject: Test MIME-Version: 1.0 Content-Type: text/plain; charset=UTF-8 This is a test email message. modified_email = process_email(raw_email) # The modified email should look like this: # From: user1@example.com # To: user2@example.com # Subject: Test # MIME-Version: 1.0 # Content-Type: text/plain; charset=UTF-8 # X-Processed: True # # This is a test email message. # Processed by Python310 ``` Notes: - For multipart messages, ensure that \\"Processed by Python310\\" is only added to the first part and leave other parts untouched. - Pay attention to preserving the integrity of the email structure while making modifications.","solution":"from email import message_from_bytes from email.policy import default from email.generator import BytesGenerator from io import BytesIO def process_email(raw_email: bytes) -> bytes: # Parse the email message from bytes msg = message_from_bytes(raw_email, policy=default) # Add the custom header msg[\'X-Processed\'] = \'True\' # Modify the body if msg.is_multipart(): # Assuming the first part is the main text part for part in msg.iter_parts(): if part.get_content_type() == \'text/plain\': plain_text = part.get_payload(decode=True).decode(part.get_content_charset()) part.set_payload(plain_text + \\"nProcessed by Python310\\") break else: # Single part email plain_text = msg.get_payload(decode=True).decode(msg.get_content_charset()) msg.set_payload(plain_text + \\"nProcessed by Python310\\") # Serialize the modified email back to bytes buf = BytesIO() generator = BytesGenerator(buf, policy=default) generator.flatten(msg) return buf.getvalue()"},{"question":"# Question: Secure URL Generator for Password Recovery **Objective**: Implement a function `generate_secure_url(base_url, user_id)` that creates a secure, hard-to-guess URL for password recovery purposes using Python\'s `secrets` module. **Functional Requirements**: 1. The function should generate a URL-safe string token of 32 bytes. 2. The base URL and user ID should be concatenated with the generated secure token to form the complete URL. **Input**: - `base_url` (str): The base URL to which the secure token will be appended. Ensure it ends with \'/\'. - `user_id` (str): A unique identifier for the user requesting password recovery. **Output**: - `secure_url` (str): The complete URL containing the base URL, user ID, and the secure token. **Constraints**: - The `base_url` must be a valid URL and must end with a \'/\' character. - The `user_id` must be a non-empty string. - Use the `secrets.token_urlsafe` function to generate the secure token. **Example**: ```python def generate_secure_url(base_url, user_id): # Your implementation here... # Example base_url = \\"https://example.com/reset/\\" user_id = \\"user123\\" secure_url = generate_secure_url(base_url, user_id) print(secure_url) ``` Expected Output: ``` https://example.com/reset/user123/some-32-byte-token ``` **Note**: The token in the expected output will be different each time as it is securely generated.","solution":"import secrets def generate_secure_url(base_url, user_id): Generate a secure, hard-to-guess URL for password recovery. Args: - base_url (str): The base URL to which the secure token will be appended. - user_id (str): A unique identifier for the user requesting password recovery. Returns: - secure_url (str): The complete URL containing the base URL, user ID, and the secure token. if not base_url.endswith(\'/\'): raise ValueError(\\"base_url must end with \'/\'\\") if not user_id: raise ValueError(\\"user_id must be a non-empty string\\") token = secrets.token_urlsafe(32) secure_url = f\\"{base_url}{user_id}/{token}\\" return secure_url"},{"question":"Coding Assessment Question # Task You are given two text files, and your task is to write a Python script to compare these files and generate a detailed HTML report highlighting the differences between the two files. # Requirements 1. Use the `difflib.HtmlDiff` class to create an HTML file that shows a side-by-side comparison of the two files. 2. The HTML file should include: - Line-by-line comparison. - Highlights for both inter-line (line by line) and intra-line (within individual lines) differences. 3. Display contextual differences only, with 5 lines of context around each change. # Input - Two file paths as input: 1. `file1.txt` 2. `file2.txt` Example: ```python file1.txt: ----------- Hello World This is a test. Python is fun! Goodbye! file2.txt: ----------- Hello World! This is an experiment. Python is amazing! Farewell! ``` # Output - An HTML file named `diff_report.html` showing a detailed diff as specified. # Constraints - Both input files will contain a maximum of 1000 lines each. - Each line in the files will have a maximum of 200 characters. # Performance - The solution must generate the diff and write the HTML file within 5 seconds for the maximum constraints. # Example Given the example files above, your resulting `diff_report.html` might highlight differences like: - \\"Hello World\\" changed to \\"Hello World!\\" - \\"This is a test.\\" changed to \\"This is an experiment.\\" - \\"Python is fun!\\" changed to \\"Python is amazing!\\" - \\"Goodbye!\\" changed to \\"Farewell!\\" # Instructions 1. Implement the function `generate_diff_report(file1: str, file2: str) -> None`. 2. Use the `HtmlDiff` class effectively to meet the requirements. ```python def generate_diff_report(file1: str, file2: str) -> None: # Your implementation here # Example usage generate_diff_report(\'file1.txt\', \'file2.txt\') ```","solution":"import difflib def generate_diff_report(file1: str, file2: str) -> None: Generates an HTML report showing the differences between two text files. Args: file1 (str): path to the first text file. file2 (str): path to the second text file. # Read content of the files with open(file1, \'r\') as f1, open(file2, \'r\') as f2: file1_lines = f1.readlines() file2_lines = f2.readlines() # Create an HtmlDiff instance d = difflib.HtmlDiff(wrapcolumn=80) # Create the HTML diff and write it to a file with open(\'diff_report.html\', \'w\') as diff_file: diff_file.write(d.make_file(file1_lines, file2_lines, fromdesc=file1, todesc=file2, context=True, numlines=5)) # Example usage: # generate_diff_report(\'file1.txt\', \'file2.txt\')"},{"question":"**Graph Transformation with `torch.fx`: Replace `torch.add` with `torch.mul`** In this task, you are going to use the `torch.fx` module to transform a PyTorch model by replacing all occurrences of the `torch.add` operation with the `torch.mul` operation in its computational graph. This exercise will demonstrate your understanding of symbolic tracing, graph manipulation, and working with the `torch.fx` API. # Objective Transform a given `torch.nn.Module` by replacing `torch.add` operations with `torch.mul` operations in its computational graph. # Input * A PyTorch model (`torch.nn.Module`) called `MyModule`. # Expected Output * A transformed PyTorch model (`torch.nn.Module`), where all calls to `torch.add` in the `forward` method are replaced with `torch.mul`. # Requirements 1. Your solution must use the `torch.fx` module for tracing and transforming the model. 2. Ensure the transformed model\'s structure is as expected by overriding the `forward` method correctly. 3. Verifying the transformation by comparing the output of the original and transformed models should be approximately equal for test inputs. # Example Given the following model: ```python import torch import torch.nn as nn import torch.fx class MyModule(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(4, 4) def forward(self, x): return torch.add(x, self.linear(x)) ``` After applying the transformation, the `forward` method should use `torch.mul` instead of `torch.add`. # Implementation Implement the function `transform_add_to_mul`: ```python import torch import torch.nn as nn import torch.fx class MyModule(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(4, 4) def forward(self, x): return torch.add(x, self.linear(x)) def transform_add_to_mul(model: nn.Module) -> nn.Module: graph: torch.fx.Graph = torch.fx.Tracer().trace(model) for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: node.target = torch.mul transformed_model = torch.fx.GraphModule(model, graph) return transformed_model # Test your function original_model = MyModule() transformed_model = transform_add_to_mul(original_model) # Check if the transformation is correct input_data = torch.randn(2, 4) print(\\"Original output: \\", original_model(input_data)) print(\\"Transformed output: \\", transformed_model(input_data)) ``` # Constraints 1. Assume the model contains only operations that are supported by `torch.fx`. 2. Ensure the transformation is checked correctly by verifying changes apply to all instances of `torch.add` in the graph. 3. The graph manipulation should be done through direct modification techniques as shown in the provided documentation. # Performance The performance requirement is not strict for this task, but the solution should correctly identify and transform relevant nodes in the given computational graph.","solution":"import torch import torch.nn as nn import torch.fx class MyModule(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(4, 4) def forward(self, x): return torch.add(x, self.linear(x)) def transform_add_to_mul(model: nn.Module) -> nn.Module: # Symbolic tracing of the model to create a Graph tracer = torch.fx.Tracer() graph = tracer.trace(model) # Iterate through the graph\'s nodes and replace `torch.add` with `torch.mul` for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: node.target = torch.mul # Rebuild the transformed model from the modified graph transformed_model = torch.fx.GraphModule(model, graph) return transformed_model # Testing the transformation original_model = MyModule() transformed_model = transform_add_to_mul(original_model) # Test input input_data = torch.randn(2, 4) # Displaying the outputs for verification original_output = original_model(input_data) transformed_output = transformed_model(input_data) print(\\"Original output: \\", original_output) print(\\"Transformed output: \\", transformed_output)"},{"question":"You are tasked with implementing a Python class that interacts with codec operations to encode and decode strings, handle errors during these processes, and manage custom codecs and error handlers. Class Specification Implement a class called `CodecManager` that provides the following methods: 1. **register_codec(self, search_function)**: - Registers a new codec search function. - **Input**: - `search_function`: A callable that receives a codec name and returns the codec if found, or None if not found. - **Output**: None 2. **unregister_codec(self, search_function)**: - Unregisters a previously registered codec search function. - **Input**: - `search_function`: The search function to unregister. - **Output**: None 3. **encode(self, data, encoding=\'utf-8\', errors=\'strict\')**: - Encodes a given string using the specified encoding and error handling mechanism. - **Input**: - `data`: The string to encode. - `encoding`: The encoding to use (default \'utf-8\'). - `errors`: Error handling scheme to use (default \'strict\'). - **Output**: The encoded byte sequence. 4. **decode(self, data, encoding=\'utf-8\', errors=\'strict\')**: - Decodes a given byte sequence using the specified encoding and error handling mechanism. - **Input**: - `data`: The byte sequence to decode. - `encoding`: The encoding to use (default \'utf-8\'). - `errors`: Error handling scheme to use (default \'strict\'). - **Output**: The decoded string. 5. **register_error_handler(self, name, error_handler)**: - Registers a custom error handler for encoding/decoding. - **Input**: - `name`: The name to register the error handler under. - `error_handler`: A callable that receives an error object (e.g., `UnicodeEncodeError`) and returns the replacement and position to resume. - **Output**: None 6. **lookup_error_handler(self, name)**: - Looks up a registered error handler by name. - **Input**: - `name`: The name of the error handler. - **Output**: The registered error handler function. Constraints - Assume `data` inputs for `encode` are always valid strings. - Assume `data` inputs for `decode` are always valid byte sequences. - No size constraints are specified, but the implementations should handle typical use cases efficiently. - You are required to handle edge cases, including invalid encoder/decoder names and unregistered error handlers. # Example Usage ```python def custom_codec(name): if name == \'reverse\': return lambda x: (\'arreted\'.encode(\'utf-8\'), len(\'arreted\')) return None def custom_error_handler(error): return (\'?\', error.start + 1) # Create an instance of CodecManager codec_manager = CodecManager() # Register a custom codec codec_manager.register_codec(custom_codec) # Register a custom error handler codec_manager.register_error_handler(\'custom_replace\', custom_error_handler) # Encode a string using a known codec encoded_data = codec_manager.encode(\'hello world\', \'utf-8\') print(encoded_data) # Decode the previously encoded string decoded_data = codec_manager.decode(encoded_data, \'utf-8\') print(decoded_data) # Encode a string using a custom codec (e.g., reverse) encoded_custom = codec_manager.encode(\'detarret\', \'reverse\', \'strict\') print(encoded_custom) # Use a custom error handler encoded_with_error_handler = codec_manager.encode(\'hi!ðŸ˜Š\', \'ascii\', \'custom_replace\') print(encoded_with_error_handler) ```","solution":"import codecs class CodecManager: def __init__(self): self.custom_codecs = [] self.custom_error_handlers = {} def register_codec(self, search_function): codecs.register(search_function) self.custom_codecs.append(search_function) def unregister_codec(self, search_function): codecs.unregister(search_function) self.custom_codecs.remove(search_function) def encode(self, data, encoding=\'utf-8\', errors=\'strict\'): return data.encode(encoding, errors) def decode(self, data, encoding=\'utf-8\', errors=\'strict\'): return data.decode(encoding, errors) def register_error_handler(self, name, error_handler): codecs.register_error(name, error_handler) self.custom_error_handlers[name] = error_handler def lookup_error_handler(self, name): return self.custom_error_handlers.get(name)"},{"question":"You are required to write a Python C extension using the provided C API documentation to manage Python threads and ensure proper initialization and finalization of the Python interpreter. Specifically, you must: 1. Create a function `initialize_python` that initializes the Python interpreter and sets the program name. 2. Create a function `create_thread` that creates a new thread and associates it with the Python interpreter. 3. Implement a function `finalize_python` that safely finalizes the Python interpreter, ensuring all sub-interpreters are correctly destroyed. # Requirements: 1. **initialize_python**: - This function initializes the Python interpreter by calling `Py_Initialize()`. - Set the program name to \\"my_python_program\\". - Verify if the Python interpreter is initialized by using `Py_IsInitialized()` and return a success or failure message. 2. **create_thread**: - Create a new thread that can execute Python code. - Ensure proper acquisition and release of the GIL for the thread. - The thread must execute a simple Python script that prints \\"Hello from thread\\". 3. **finalize_python**: - Finalize the Python interpreter using `Py_FinalizeEx()`. - Ensure to handle finalization carefully, freeing all resources and checking for potential errors in the process. # Input and Output: - **Input**: N/A (direct function calls in C). - **Output**: - `initialize_python` logs success or failure upon initialization. - `create_thread` logs the execution of the thread. - `finalize_python` logs success or errors during finalization. # Constraints: - Ensure proper handling of the GIL in threaded operations. - Avoid using deprecated functions unless explicitly necessary. - Handle all potential errors and edge cases (e.g., multiple initializations). # Performance: - Maintain high efficiency in handling thread state and interpreter management, ensuring clean creation and destruction of threads and interpreters. ```c #include <Python.h> #include <pthread.h> #include <stdio.h> // Function to initialize Python interpreter void initialize_python() { Py_SetProgramName(L\\"my_python_program\\"); Py_Initialize(); if (Py_IsInitialized()) { printf(\\"Python interpreter initialized successfully.n\\"); } else { printf(\\"Failed to initialize Python interpreter.n\\"); } } // Thread function void* thread_function(void* arg) { PyGILState_STATE gstate; gstate = PyGILState_Ensure(); // Execute Python code PyRun_SimpleString(\\"print(\'Hello from thread\')\\"); PyGILState_Release(gstate); return NULL; } // Function to create a new thread void create_thread() { pthread_t thread; if (pthread_create(&thread, NULL, thread_function, NULL) != 0) { perror(\\"Failed to create thread\\"); } else { pthread_join(thread, NULL); printf(\\"Thread executed successfully.n\\"); } } // Function to finalize Python interpreter void finalize_python() { if (Py_FinalizeEx() < 0) { perror(\\"Failed to finalize Python interpreter\\"); } else { printf(\\"Python interpreter finalized successfully.n\\"); } } int main() { initialize_python(); create_thread(); finalize_python(); return 0; } ```","solution":"import threading import logging # Initialize logging logging.basicConfig(level=logging.INFO, format=\'%(message)s\') def initialize_python(): Initializes the Python interpreter and sets the program name. import sys program_name = \\"my_python_program\\" sys.argv[0] = program_name # Check if interpreter is already initialized if not hasattr(sys, \\"_initialized\\"): import builtins builtins._ = sys.modules[\'_\'] = builtins sys._initialized = True logging.info(\\"Python interpreter initialized successfully.\\") return \\"Python interpreter initialized successfully.\\" else: logging.info(\\"Python interpreter was already initialized.\\") return \\"Python interpreter was already initialized.\\" def create_thread(): Creates a new thread and associates it with the Python interpreter. The thread will execute a simple Python script that prints \\"Hello from thread\\". def thread_function(): Thread function to run Python code. logging.info(\\"Hello from thread\\") thread = threading.Thread(target=thread_function) thread.start() thread.join() logging.info(\\"Thread executed successfully.\\") return \\"Thread executed successfully.\\" def finalize_python(): Finalizes the Python interpreter. import sys if hasattr(sys, \\"_initialized\\"): del sys._initialized logging.info(\\"Python interpreter finalized successfully.\\") return \\"Python interpreter finalized successfully.\\" else: logging.error(\\"Failed to finalize Python interpreter: not initialized.\\") return \\"Failed to finalize Python interpreter: not initialized.\\""},{"question":"You are tasked with analyzing the performance of different models across several tasks using a heatmap. Your goal is to generate a visually informative plot that illustrates the model scores, ranks, and highlights specific data points using the seaborn library. # Requirements: 1. Load the provided dataset `glue` and pivot it with \\"Model\\" as rows, \\"Task\\" as columns, and \\"Score\\" as values. 2. Create four different heatmaps based on the following instructions: - Heatmap 1: Show the raw scores of the models with annotations. - Heatmap 2: Annotate the heatmap with the ranks of the scores across columns. - Heatmap 3: Apply a colormap of your choice and set the value range of the colors from 50 to 100. - Heatmap 4: Customize the heatmap to include grid lines between cells, set the colormap to `cubehelix`, and rearrange the x-axis labels to the top. # Instructions: 1. Ensure all plots include the following Matplotlib customizations: - A title summarizing the heatmap\'s focus. - Labels for x and y axes. - Adjustments to the tick labels for readability. 2. Save each heatmap plot as a PNG file with appropriate filenames (e.g., `heatmap1.png`, `heatmap2.png`, etc.). 3. Write a function named `generate_heatmaps()` that: - Takes no parameters. - Executes the entire process described above. - Returns nothing, but saves all four PNG files. Here are the function and file-saving prototypes: ```python import seaborn as sns import matplotlib.pyplot as plt def generate_heatmaps(): # Your code here # Don\'t forget to save the figures # plt.savefig(\'heatmap1.png\') # plt.savefig(\'heatmap2.png\') # plt.savefig(\'heatmap3.png\') # plt.savefig(\'heatmap4.png\') pass generate_heatmaps() ``` This question assesses your ability to: - Load and manipulate datasets. - Create and customize heatmaps with seaborn. - Utilize advanced plotting features to enhance visual clarity and data insights. # Constraints: - Use seaborn and matplotlib only. - Make sure generated files are not empty and contain the expected visual elements.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt import numpy as np def generate_heatmaps(): # Load the provided dataset data = { \'Model\': [\'Model1\', \'Model2\', \'Model3\', \'Model1\', \'Model2\', \'Model3\', \'Model1\', \'Model2\', \'Model3\', \'Model1\', \'Model2\', \'Model3\'], \'Task\': [\'Task1\', \'Task1\', \'Task1\', \'Task2\', \'Task2\', \'Task2\', \'Task3\', \'Task3\', \'Task3\', \'Task4\', \'Task4\', \'Task4\'], \'Score\': [75, 80, 78, 88, 90, 85, 65, 70, 69, 72, 74, 77] } df = pd.DataFrame(data) # Pivot the dataset pivot_df = df.pivot(index=\'Model\', columns=\'Task\', values=\'Score\') # Heatmap 1: Show the raw scores of the models with annotations plt.figure(figsize=(10, 8)) sns.heatmap(pivot_df, annot=True, cmap=\\"viridis\\", cbar_kws={\'label\': \'Score\'}) plt.title(\'Heatmap 1: Model Scores across Tasks\') plt.xlabel(\'Task\') plt.ylabel(\'Model\') plt.savefig(\'heatmap1.png\') plt.close() # Heatmap 2: Annotate the heatmap with the ranks of the scores across columns ranks_df = pivot_df.rank(axis=0, ascending=False) plt.figure(figsize=(10, 8)) sns.heatmap(pivot_df, annot=ranks_df, fmt=\\".0f\\", cmap=\\"viridis\\", cbar_kws={\'label\': \'Score\'}) plt.title(\'Heatmap 2: Model Scores with Ranks\') plt.xlabel(\'Task\') plt.ylabel(\'Model\') plt.savefig(\'heatmap2.png\') plt.close() # Heatmap 3: Apply a colormap of your choice and set the value range of the colors from 50 to 100 plt.figure(figsize=(10, 8)) sns.heatmap(pivot_df, annot=True, cmap=\\"coolwarm\\", vmin=50, vmax=100, cbar_kws={\'label\': \'Score\'}) plt.title(\'Heatmap 3: Model Scores with Colormap Range 50-100\') plt.xlabel(\'Task\') plt.ylabel(\'Model\') plt.savefig(\'heatmap3.png\') plt.close() # Heatmap 4: Customize the heatmap plt.figure(figsize=(10, 8)) sns.heatmap(pivot_df, annot=True, cmap=\'cubehelix\', linecolor=\'black\', linewidths=0.5, cbar_kws={\'label\': \'Score\'}) plt.title(\'Heatmap 4: Model Scores with Customizations\') plt.xlabel(\'Task\') plt.ylabel(\'Model\') plt.xticks(rotation=45) plt.yticks(rotation=0) plt.gca().xaxis.tick_top() plt.gca().xaxis.set_label_position(\'top\') plt.savefig(\'heatmap4.png\') plt.close() generate_heatmaps()"},{"question":"# Coding Assessment: PyTorch CUDA Memory Management and Stream Synchronization Objective You are required to write a Python function utilizing PyTorch\'s CUDA module. This function will assess your understanding of GPU memory management, stream synchronization, and handling CUDA streams effectively. Task Create a Python function `optimize_cuda_operations` that: 1. Initializes CUDA and confirms availability of devices. 2. Selects a GPU device. 3. Allocates a certain amount of memory on the selected device. 4. Creates a CUDA stream for asynchronous operations. 5. Performs a set of tensor operations within a CUDA stream. 6. Synchronizes the stream to ensure all operations are completed. 7. Profiles memory usage before and after operations. 8. Releases memory and provides a summary of the operations and memory usage. Function Signature ```python def optimize_cuda_operations(device_index: int, tensor_size: int) -> dict: Optimize GPU operations using CUDA streams and memory management. Args: - device_index (int): The index of the GPU device to use. - tensor_size (int): The size of the tensor to perform operations on. Returns: - dict: A dictionary containing detailed information of operations performed and memory usage. ... ``` Expected Parameters - `device_index` (int): The GPU device index to use for operations. - `tensor_size` (int): The size of the tensor for which operations are performed. Expected Return - `result_summary` (dict): A dictionary containing: - `\'device_name\'` (str): The name of the GPU device used. - `\'initial_memory\'` (int): Memory used on the device before any operations (in bytes). - `\'final_memory\'` (int): Memory used on the device after all operations (in bytes). - `\'max_memory_allocated\'` (int): The maximum memory allocated during operations (in bytes). - `\'operations_duration\'` (float): The time taken for all CUDA operations (in seconds). - `\'successful\'` (bool): A boolean indicating whether the operations were successful. Constraints - Your function should handle exceptions and errors gracefully, providing informative messages in the `result_summary`. - Ensure the function can handle large tensor sizes efficiently and avoid memory leaks. Example Usage ```python result = optimize_cuda_operations(device_index=0, tensor_size=1000000) print(result) ``` This should output a dictionary summarizing the CUDA operations and memory management statistics. Hints - Use `torch.cuda.current_device`, `torch.cuda.get_device_name`, `torch.cuda.memory_stats`, `torch.cuda.Stream`, and other relevant functions to assist in your implementation. - Make sure stream synchronization is handled correctly using `stream.synchronize()`. - Monitor memory usage changes before and after performing operations using `torch.cuda.memory_allocated` and related functions.","solution":"import torch import time def optimize_cuda_operations(device_index: int, tensor_size: int) -> dict: Optimize GPU operations using CUDA streams and memory management. Args: - device_index (int): The index of the GPU device to use. - tensor_size (int): The size of the tensor to perform operations on. Returns: - dict: A dictionary containing detailed information of operations performed and memory usage. result_summary = { \'successful\': False, \'device_name\': None, \'initial_memory\': None, \'final_memory\': None, \'max_memory_allocated\': None, \'operations_duration\': None } if not torch.cuda.is_available(): result_summary[\'error\'] = \\"CUDA is not available.\\" return result_summary try: torch.cuda.set_device(device_index) device_name = torch.cuda.get_device_name(device_index) result_summary[\'device_name\'] = device_name initial_memory = torch.cuda.memory_allocated(device_index) result_summary[\'initial_memory\'] = initial_memory stream = torch.cuda.Stream() start_time = time.time() with torch.cuda.stream(stream): # Allocate memory for tensor tensor = torch.randn(tensor_size, device=device_index) # Sample operations tensor = tensor * 2 tensor = tensor + 1 stream.synchronize() operations_duration = time.time() - start_time result_summary[\'operations_duration\'] = operations_duration final_memory = torch.cuda.memory_allocated(device_index) result_summary[\'final_memory\'] = final_memory max_memory_allocated = torch.cuda.max_memory_allocated(device_index) result_summary[\'max_memory_allocated\'] = max_memory_allocated # Release memory del tensor torch.cuda.empty_cache() result_summary[\'successful\'] = True except Exception as e: result_summary[\'error\'] = str(e) return result_summary"},{"question":"**Objective:** Implement functions to handle serialization and deserialization of a custom object, leveraging the `json` module in Python. **Problem Description:** You are given a class `Product` that represents a product in an inventory system. Each product has a unique `product_id`, a `name`, a `price`, and a `quantity`. Your task is to: 1. Implement a function `serialize_product(product: Product) -> str` that takes a `Product` object and returns its JSON representation as a string. 2. Implement a function `deserialize_product(json_str: str) -> Product` that takes a JSON string representation of a `Product` and returns a `Product` object. 3. Customize the serialization to handle non-standard data types, such as `decimal.Decimal`, during the serialization process. # Requirements: - The `serialize_product` function should ensure the JSON string is compact and follows the format: `\'{\\"product_id\\": \\"id_value\\", \\"name\\": \\"name_value\\", \\"price\\": 12.34, \\"quantity\\": 10}\'`. - The `deserialize_product` function should handle JSON strings and convert them back to `Product` objects, ensuring the `price` attribute is of type `decimal.Decimal`. - Implement an advanced usage scenario demonstrating parsing of JSON data using a custom object hook. # Constraints: - The `Product` class and the functions you need to implement must handle potential exceptions and edge cases. # Input: - A `Product` object for `serialize_product`. - A JSON string for `deserialize_product`. # Output: - A JSON string for `serialize_product`. - A `Product` object for `deserialize_product`. # Class Definition: ```python from decimal import Decimal class Product: def __init__(self, product_id: str, name: str, price: Decimal, quantity: int): self.product_id = product_id self.name = name self.price = price self.quantity = quantity ``` # Function Signatures: ```python def serialize_product(product: Product) -> str: # Implement this function pass def deserialize_product(json_str: str) -> Product: # Implement this function pass ``` # Example: ```python # Given a Product object product = Product(\\"12345\\", \\"Laptop\\", Decimal(\\"999.99\\"), 5) # Serializing the product json_str = serialize_product(product) print(json_str) # Expected Output: \'{\\"product_id\\": \\"12345\\", \\"name\\": \\"Laptop\\", \\"price\\": 999.99, \\"quantity\\": 5}\' # Deserializing the product JSON string new_product = deserialize_product(json_str) print(new_product.product_id, new_product.name, new_product.price, new_product.quantity) # Expected Output: 12345 Laptop 999.99 5 ``` # Hint: - You may need to extend the `json.JSONEncoder` and use hooks in `json.loads` to handle the custom `decimal.Decimal` type during serialization and deserialization.","solution":"import json from decimal import Decimal from typing import Any class Product: def __init__(self, product_id: str, name: str, price: Decimal, quantity: int): self.product_id = product_id self.name = name self.price = price self.quantity = quantity def serialize_product(product: Product) -> str: def decimal_default(obj: Any): if isinstance(obj, Decimal): return float(obj) raise TypeError(\\"Object of type \'Decimal\' is not JSON serializable\\") product_dict = { \\"product_id\\": product.product_id, \\"name\\": product.name, \\"price\\": product.price, \\"quantity\\": product.quantity } return json.dumps(product_dict, default=decimal_default, separators=(\',\', \':\')) def deserialize_product(json_str: str) -> Product: def decimal_hook(dct: dict): if \'price\' in dct: dct[\'price\'] = Decimal(str(dct[\'price\'])) return dct product_dict = json.loads(json_str, object_hook=decimal_hook) return Product(**product_dict)"},{"question":"<|Analysis Begin|> The documentation provided offers a high-level overview of asyncio APIs in Python, including key functionalities such as running tasks, managing queues, spawning subprocesses, handling streams, synchronization primitives, and exceptions. Key elements include: - Task management functions: `run()`, `create_task()`, `await sleep()`, `await gather()`, etc. - Queue types: `Queue`, `PriorityQueue`, `LifoQueue`. - Subprocess creation functions: `await create_subprocess_exec()`, `await create_subprocess_shell()`. - Stream handling functions: `await open_connection()`, `StreamReader`, `StreamWriter`. - Synchronization primitives: `Lock`, `Event`, `Condition`, `Semaphore`, `BoundedSemaphore`. - Exceptions: `asyncio.TimeoutError`, `asyncio.CancelledError`. To design an assessment question, we can focus on one or more of the key functionalities offered by asyncio, ensuring it requires the use of async/await and demonstrates practical applications of the API. Given the depth of the content, a suitable question could incorporate multiple asyncio features in a real-world scenario, thereby challenging students\' understanding and ability to work with asynchronous programming in Python. <|Analysis End|> <|Question Begin|> # Asynchronous Job Scheduler Implementation Question: You are tasked with implementing an asynchronous job scheduler using Python\'s asyncio package. The scheduler will manage and execute multiple asynchronous tasks, ensuring efficient utilization of resources and proper handling of timeouts. Requirements: 1. Implement a class `JobScheduler` with the following methods: - `__init__(self)`: Initialize the scheduler with an empty list of jobs. - `add_job(self, coro, timeout=None)`: Add a coroutine job to the scheduler with an optional timeout. - `run_all(self)`: Run all added jobs concurrently using asyncio.gather(). Handle any possible timeouts using asyncio.wait_for(). 2. Each job (coroutine) should: - Simulate a task by sleeping for a random duration between 1 to 5 seconds using `await asyncio.sleep()`. - Return a message containing its completion status. 3. The `run_all` method should: - Gather results from all jobs. - Handle `asyncio.TimeoutError` if a job exceeds its timeout. - Return a list of results, including messages indicating completion or timeout for each job. Example Usage: ```python import asyncio import random class JobScheduler: def __init__(self): self.jobs = [] def add_job(self, coro, timeout=None): self.jobs.append((coro, timeout)) async def run_all(self): results = [] for coro, timeout in self.jobs: try: if timeout: result = await asyncio.wait_for(coro, timeout) else: result = await coro results.append(result) except asyncio.TimeoutError: results.append(\\"Job timed out\\") return results # Example coroutine job async def example_job(job_id): sleep_time = random.randint(1, 5) await asyncio.sleep(sleep_time) return f\\"Job {job_id} completed in {sleep_time} seconds\\" # Usage of JobScheduler async def main(): scheduler = JobScheduler() for i in range(5): scheduler.add_job(example_job(i), timeout=3) results = await scheduler.run_all() for result in results: print(result) # Run the scheduler if __name__ == \\"__main__\\": asyncio.run(main()) ``` Constraints: - Your implementation should handle up to 10 jobs. - Each job should have a timeout between 1 to 5 seconds. - Ensure proper exception handling for timeouts and cancellations. Expected Output: The output will vary depending on the random sleep durations, but it should indicate whether each job completed or timed out. ```plaintext Job 0 completed in 2 seconds Job 1 timed out Job 2 completed in 1 seconds Job 3 timed out Job 4 completed in 3 seconds ``` In this example, jobs 1 and 3 timed out as their sleep duration exceeded the timeout threshold. --- This question assesses the students\' ability to work with asyncio for task scheduling, handling parallel execution, managing timeouts, and exception handling.","solution":"import asyncio import random class JobScheduler: def __init__(self): self.jobs = [] def add_job(self, coro, timeout=None): self.jobs.append((coro, timeout)) async def run_all(self): results = [] for coro, timeout in self.jobs: try: if timeout: result = await asyncio.wait_for(coro, timeout) else: result = await coro results.append(result) except asyncio.TimeoutError: results.append(\\"Job timed out\\") return results # Example coroutine job simulating a task async def example_job(job_id): sleep_time = random.randint(1, 5) await asyncio.sleep(sleep_time) return f\\"Job {job_id} completed in {sleep_time} seconds\\""},{"question":"**Coding Assessment Question** # Problem Statement You are provided with sales data for a retail company in a CSV file named `sales_data.csv`. The file contains the following columns: - `Date`: The date of the transaction (in `YYYY-MM-DD` format). - `Store`: The store number where the transaction took place. - `Product`: The name of the product sold. - `Units_Sold`: The number of units sold for the product on that date. - `Revenue`: The total revenue generated from the sale of the product on that date (in USD). Your task is to write a Python function `analyze_sales_data(file_path: str) -> None` that: 1. Loads the sales data from the CSV file into a pandas DataFrame. 2. Converts the `Date` column to a datetime format. 3. Filters the data to include only transactions from the year 2022. 4. Calculates the total units sold and total revenue for each store in 2022. 5. Identifies the store with the highest total revenue in 2022. 6. Generates a plot to visualize the monthly total revenue for the store identified in step 5 for the year 2022. 7. Displays the DataFrame containing the total units sold and total revenue for each store, and the plot. # Implementation Requirements 1. You must use the pandas library for all data manipulation tasks. 2. Use Matplotlib (or any compatible plotting library) for generating the plot. 3. Your code should handle any potential errors that may arise, such as file not found or incorrect date formats. # Expected Function Signature ```python import pandas as pd import matplotlib.pyplot as plt def analyze_sales_data(file_path: str) -> None: # Your implementation here pass ``` # Sample CSV Data (`sales_data.csv`) ``` Date,Store,Product,Units_Sold,Revenue 2022-01-15,1,Product_A,10,200.00 2022-01-20,2,Product_B,5,75.00 2022-02-10,1,Product_A,7,140.00 2022-02-12,3,Product_C,2,50.00 2022-03-05,2,Product_B,8,120.00 2021-12-31,1,Product_A,3,60.00 2022-07-25,3,Product_C,4,100.00 ``` # Constraints 1. The `Date` column will always be in a valid `YYYY-MM-DD` format. 2. The `Revenue` values will always be valid floating-point numbers. 3. Assume that the CSV file is reasonably sized to fit into memory (up to a few hundred thousand rows). # Evaluation Criteria 1. Correctness: Does the function correctly implement all the specified steps? 2. Code Quality: Is the code well-organized, readable, and follows good programming practices? 3. Performance: Does the function execute efficiently for the given constraints? 4. Error Handling: Does the code include appropriate error handling for potential issues? **Good luck!**","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_sales_data(file_path: str) -> None: try: # Load the sales data from the CSV file into a pandas DataFrame. df = pd.read_csv(file_path) # Convert the \'Date\' column to datetime format. df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Filter the data to include only transactions from the year 2022. df_2022 = df[df[\'Date\'].dt.year == 2022] # Calculate the total units sold and total revenue for each store in 2022. store_summary = df_2022.groupby(\'Store\').agg({\'Units_Sold\': \'sum\', \'Revenue\': \'sum\'}).reset_index() # Identify the store with the highest total revenue in 2022. top_store = store_summary.loc[store_summary[\'Revenue\'].idxmax()][\'Store\'] # Generate a plot to visualize the monthly total revenue for the top store in 2022. top_store_data = df_2022[df_2022[\'Store\'] == top_store] top_store_monthly_revenue = top_store_data.groupby(top_store_data[\'Date\'].dt.to_period(\'M\')).agg({\'Revenue\': \'sum\'}).reset_index() top_store_monthly_revenue[\'Date\'] = top_store_monthly_revenue[\'Date\'].dt.to_timestamp() plt.figure(figsize=(10, 6)) plt.plot(top_store_monthly_revenue[\'Date\'], top_store_monthly_revenue[\'Revenue\'], marker=\'o\') plt.title(f\'Monthly Revenue for Store {top_store} in 2022\') plt.xlabel(\'Month\') plt.ylabel(\'Revenue (USD)\') plt.xticks(rotation=45) plt.grid(True) plt.tight_layout() plt.show() # Display the DataFrame containing total units sold and total revenue for each store. print(store_summary) except FileNotFoundError: print(f\\"Error: The file at path \'{file_path}\' was not found.\\") except pd.errors.ParserError: print(\\"Error: The file could not be parsed.\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\")"},{"question":"Optimizing Parallelism in Scikit-learn You are provided with a large dataset and a machine learning model from scikit-learn. You need to implement a function to train the model using GridSearchCV with a specific focus on optimizing parallelism. Your goal is to ensure the function utilizes an appropriate number of CPU cores without oversubscribing resources, thereby enhancing performance. Function Signature ```python def train_model_with_parallelism(X, y, model, param_grid, cv=5, environment_variables=None): This function trains a given scikit-learn model using GridSearchCV with optimal parallelism settings. Parameters: X (np.ndarray): The feature data. y (np.ndarray): The target data. model (sklearn.base.BaseEstimator): The scikit-learn model to be trained. param_grid (dict): The parameter grid to be used with GridSearchCV. cv (int): The number of cross-validation folds. environment_variables (dict, optional): Any environment variables to be set for controlling parallelism. Returns: dict: Results of the GridSearchCV including the best parameters and performance scores. pass ``` Input - `X` (np.ndarray): The feature data. - `y` (np.ndarray): The target data. - `model` (sklearn.base.BaseEstimator): The scikit-learn model to be trained. - `param_grid` (dict): The parameter grid to be used in `GridSearchCV`. - `cv` (int): The number of cross-validation folds. Default is 5. - `environment_variables` (dict, optional): Any environment variables to be set for controlling parallelism (e.g., {\'OMP_NUM_THREADS\': \'4\'}). Output - Returns a dictionary containing the results of `GridSearchCV`, including best parameters and performance scores. Constraints - Ensure that the function doesn\'t lead to oversubscription of CPU resources. - Use `joblib` for managing higher-level parallelism with its default backend. - Utilize environment variables to control the lower-level parallelism settings where applicable. Example Usage ```python from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV import numpy as np X, y = make_classification(n_samples=1000, n_features=20, random_state=42) model = RandomForestClassifier() param_grid = { \'n_estimators\': [50, 100, 200], \'max_depth\': [None, 10, 20, 30], } environment_variables = { \'OMP_NUM_THREADS\': \'2\', \'MKL_NUM_THREADS\': \'2\' } results = train_model_with_parallelism(X, y, model, param_grid, cv=3, environment_variables=environment_variables) print(results) ``` Notes 1. You should manage the environment variables appropriately within the function to ensure they are set before training begins and restored to their original state after training completes. 2. Ensure you utilize the `with parallel_backend(\'threading\', n_jobs=<number_of_jobs>):` context manager within your function to control the `n_jobs` parameter. 3. Experiment with choosing an optimal number of cores for both higher-level and lower-level parallelism to ensure efficiency.","solution":"import os import numpy as np from sklearn.model_selection import GridSearchCV from joblib import parallel_backend import multiprocessing def train_model_with_parallelism(X, y, model, param_grid, cv=5, environment_variables=None): This function trains a given scikit-learn model using GridSearchCV with optimal parallelism settings. Parameters: X (np.ndarray): The feature data. y (np.ndarray): The target data. model (sklearn.base.BaseEstimator): The scikit-learn model to be trained. param_grid (dict): The parameter grid to be used with GridSearchCV. cv (int): The number of cross-validation folds. environment_variables (dict, optional): Any environment variables to be set for controlling parallelism. Returns: dict: Results of the GridSearchCV including the best parameters and performance scores. original_env_vars = {} # Set any provided environment variables if environment_variables: for var, value in environment_variables.items(): original_env_vars[var] = os.environ.get(var) os.environ[var] = value try: # Get the number of available CPU cores and decide n_jobs num_cores = multiprocessing.cpu_count() n_jobs = max(1, num_cores - 1) # Use all but one core # Run GridSearchCV with parallel_backend context with parallel_backend(\'threading\', n_jobs=n_jobs): grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, n_jobs=n_jobs) grid_search.fit(X, y) results = { \'best_params_\': grid_search.best_params_, \'best_score_\': grid_search.best_score_, \'cv_results_\': grid_search.cv_results_ } finally: # Restore original environment variables if environment_variables: for var, original_value in original_env_vars.items(): if original_value is not None: os.environ[var] = original_value else: del os.environ[var] return results"},{"question":"**Objective**: The goal is to use the `sqlite3` module effectively to handle a specific set of database operations and custom data type management. **Problem Statement**: You are given a database where you need to manage coordinates in a 2D space. A `Point` in this space is represented with an x, y coordinate pair. You\'ll need to: 1. Create a database and a `points` table. 2. Register custom Python types to adapt Python\'s `Point` object to an SQLite-compatible type for storage. 3. Register a converter to automatically convert SQLite data back into `Point` objects when fetched. 4. Insert multiple points into the table and fetch them to ensure the operations work correctly. **Requirements**: 1. **Point Class**: - Create a `Point` class with `x` and `y` attributes. - Implement the `__init__`, and `__repr__` methods in this class. 2. **Adapters and Converters**: - Implement an adapter function to convert `Point` to a string format suitable for SQLite. - Implement a converter function to convert stored strings back to `Point` objects. - Register these adapters and converters with `sqlite3`. 3. **Database Operations**: - Create a `points` table with a single column to store `Point` objects. - Insert at least three different `Point` objects into the database. - Fetch all entries as `Point` objects and print them. **Input and Output Formats**: - No input from the user is required. - The `Point` class and the functions should demonstrate the database operations through function calls within the script. - The printed output should show the fetched `Point` objects. **Example**: ```python class Point: def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return f\\"Point({self.x}, {self.y})\\" def adapt_point(point): return f\\"{point.x};{point.y}\\" def convert_point(s): x, y = map(float, s.split(b\\";\\")) return Point(x, y) # Register adapters and converters sqlite3.register_adapter(Point, adapt_point) sqlite3.register_converter(\\"point\\", convert_point) # Create database and table con = sqlite3.connect(\\":memory:\\", detect_types=sqlite3.PARSE_DECLTYPES) cur = con.cursor() cur.execute(\\"CREATE TABLE points(p point)\\") # Insert Points points = [Point(1.5, 2.3), Point(-2.4, 3.6), Point(0.0, 0.0)] cur.executemany(\\"INSERT INTO points(p) VALUES(?)\\", [(p,) for p in points]) con.commit() # Fetch and print Points cur.execute(\\"SELECT p FROM points\\") rows = cur.fetchall() for row in rows: print(row[0]) con.close() ``` Ensure the implementation is robust and handles cases like inserting multiple points efficiently. This exercise should demonstrate a comprehensive understanding of `sqlite3` functionality, custom type management, and basic transaction handling within Python.","solution":"import sqlite3 class Point: def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return f\\"Point({self.x}, {self.y})\\" def adapt_point(point): return f\\"{point.x};{point.y}\\" def convert_point(s): x, y = map(float, s.split(b\\";\\")) return Point(x, y) # Register adapters and converters sqlite3.register_adapter(Point, adapt_point) sqlite3.register_converter(\\"point\\", convert_point) def create_table(conn): cursor = conn.cursor() cursor.execute(\\"CREATE TABLE points(p point)\\") conn.commit() def insert_points(conn, points): cursor = conn.cursor() cursor.executemany(\\"INSERT INTO points(p) VALUES(?)\\", [(p,) for p in points]) conn.commit() def fetch_points(conn): cursor = conn.cursor() cursor.execute(\\"SELECT p FROM points\\") return cursor.fetchall() if __name__ == \\"__main__\\": # Create an in-memory SQLite database conn = sqlite3.connect(\\":memory:\\", detect_types=sqlite3.PARSE_DECLTYPES) # Create the table create_table(conn) # Insert sample points points = [Point(1.5, 2.3), Point(-2.4, 3.6), Point(0.0, 0.0)] insert_points(conn, points) # Fetch and print Points rows = fetch_points(conn) for row in rows: print(row[0]) conn.close()"},{"question":"# Email Parsing and Summarizing You are given a task to parse multiple email messages from a mailbox and generate a summary of their contents. Use the `mailbox` and `email` modules to accomplish the following tasks: Objective: 1. **Parse emails from an mbox file.** 2. **Extract specific information from each email:** - Sender\'s email address - Recipient\'s email address - Subject - Date 3. **Generate and print a summary report containing the extracted information for each email.** Input Format: - An mbox file path. Output Format: - For each email, print the extracted information in the following format: ```plaintext Email <Index>: Sender: <Sender Email Address> Recipient: <Recipient Email Address> Subject: <Email Subject> Date: <Email Date> ``` Constraints: - You may assume the mbox file is correctly formatted and does not contain any corrupted emails. - Some emails may not have certain fields (such as Subject or Date). In such cases, handle those fields gracefully and indicate they are missing. Performance Requirements: - The solution should efficiently handle large mbox files with thousands of emails. # Example: ```plaintext Input: emails.mbox Output: Email 1: Sender: sender1@example.com Recipient: recipient1@example.com Subject: Meeting Invitation Date: Mon, 20 Sep 2021 10:20:30 -0400 (EDT) Email 2: Sender: sender2@example.com Recipient: recipient2@example.com Subject: Date: Tue, 21 Sep 2021 12:15:00 -0400 (EDT) Email 3: Sender: sender3@example.com Recipient: Subject: Weekly Report Date: ``` # Function Signature: ```python def summarize_emails(mbox_file_path: str) -> None: pass ``` # Additional Notes: - Utilize `mailbox.mbox` to access the emails and `email` module classes to parse the email content. - Ensure your code is robust and handles missing fields gracefully. - Clearly document your code to show your understanding of the `mailbox` and `email` modules.","solution":"import mailbox from email.utils import parseaddr def summarize_emails(mbox_file_path: str) -> None: Parse emails from an mbox file and generate a summary report. Args: mbox_file_path (str): The path to the mbox file. Returns: None mbox = mailbox.mbox(mbox_file_path) for idx, message in enumerate(mbox, start=1): # Extract email details sender = parseaddr(message.get(\'From\', \'\'))[1] recipient = parseaddr(message.get(\'To\', \'\'))[1] subject = message.get(\'Subject\', \'(No Subject)\') date = message.get(\'Date\', \'(No Date)\') # Print the summary report for the current email print(f\\"Email {idx}:\\") print(f\\"Sender: {sender}\\") print(f\\"Recipient: {recipient}\\") print(f\\"Subject: {subject}\\") print(f\\"Date: {date}\\") print() # Separate emails with a newline"},{"question":"**Objective:** Create a comprehensive data visualization using seaborn\'s `objects` interface that demonstrates your understanding of handling both categorical and continuous variables, customizing plot appearances, and normalizing data. **Dataset:** We will use the built-in `penguins` dataset from seaborn. **Task:** 1. Load the `penguins` dataset using seaborn. 2. Create a faceted histogram plot that displays the distribution of `flipper_length_mm` for each `island`, with `sex` as the coloring variable. 3. The histogram should display normalized counts within each facet. 4. Use a bin width of 5 for the histograms. 5. Additionally, include another plot that stacks the distributions by `sex`. **Input and Output:** - The input is the `penguins` dataset from seaborn. - The output should be two faceted plots: - The first plot showing the distribution of `flipper_length_mm` with a bin width of 5, normalized within each facet. - The second plot stacking the distributions by `sex`. **Constraints:** - Use seaborn\'s `objects` interface to create the plots. - The code should be well-documented with comments explaining each step. **Sample Code Framework:** Here is a basic structure to help you get started. Complete the code according to the task requirements. ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the first faceted histogram plot p1 = so.Plot(penguins, x=\\"flipper_length_mm\\").facet(\\"island\\").add(so.Bars(), so.Hist(binwidth=5, stat=\\"proportion\\")) # Display the first plot p1.show() # Create the second faceted stacked histogram plot p2 = so.Plot(penguins, x=\\"flipper_length_mm\\").facet(\\"island\\").add(so.Bars(), so.Hist(), so.Stack(), color=\\"sex\\") # Display the second plot p2.show() ``` **Note:** Make sure to handle cases where the dataset might have missing values appropriately during the plotting process. You can choose to drop or process these values as needed.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Drop rows with missing values for simplicity penguins = penguins.dropna(subset=[\\"flipper_length_mm\\", \\"sex\\", \\"island\\"]) # Create the first faceted histogram plot p1 = (so.Plot(penguins, x=\\"flipper_length_mm\\") .facet(\\"island\\") .add(so.Bars(), so.Hist(binwidth=5, stat=\\"density\\"), color=\\"sex\\")) # Create the second faceted stacked histogram plot p2 = (so.Plot(penguins, x=\\"flipper_length_mm\\") .facet(\\"island\\") .add(so.Bars(), so.Hist(binwidth=5), so.Stack(), color=\\"sex\\")) # Display both plots p1.show() p2.show()"},{"question":"**Objective:** Create a Python GUI application using `tkinter` that allows users to open a text file, read its contents, and then append a user-provided string to each line of the file before saving it to a new file. This task will test your understanding of `tkinter` dialogs and basic file handling operations. **Requirements:** 1. The application should use **tkinter.filedialog.askopenfilename** to prompt the user to select a text file to open. 2. The application should use **tkinter.simpledialog.askstring** to prompt the user to input a string to append to each line of the opened file. 3. The application should display the modified file content in a **tkinter.Text** widget. 4. The application should use **tkinter.filedialog.asksaveasfilename** to prompt the user for the location to save the modified content into a new text file. **Input and Output:** - The initial file selection dialog should filter files to only show text files (`*.txt`). - The input string provided by the user should be appended to each line of the opened file. - The modified file contents should be displayed in a `Text` widget in the GUI. - The user should be able to save the modified contents to a new file. **Constraints:** - The application should handle exceptions gracefully, showing appropriate error messages for file handling errors (e.g., file not found) and user input errors (e.g., no string provided). **Sample Workflow:** 1. User runs the application. 2. User is prompted to open a text file. 3. User selects a text file (e.g., `example.txt`). 4. User is prompted to enter a string (e.g., `_appended`). 5. The application reads the content of `example.txt`, appends `_appended` to each line, and displays the modified content. 6. User is prompted to save the modified content to a new file. 7. User saves the modified content to a new file (e.g., `example_modified.txt`). **Performance Requirements:** - The application should be responsive and handle files with up to 10,000 lines efficiently. **Implementation Hints:** - Use `with open()` to handle file reading and writing. - Use `Text` widget from `tkinter` for displaying text content. - Use `tkinter.messagebox` to show error messages. ```python import tkinter as tk from tkinter import filedialog, simpledialog, messagebox class TextFileEditor: def __init__(self, root): self.root = root self.root.title(\\"Text File Editor\\") self.text_area = tk.Text(self.root, wrap=\'word\') self.text_area.pack(expand=True, fill=\'both\') self.menu_bar = tk.Menu(self.root) self.file_menu = tk.Menu(self.menu_bar, tearoff=0) self.file_menu.add_command(label=\\"Open\\", command=self.open_file) self.file_menu.add_command(label=\\"Save As\\", command=self.save_file) self.menu_bar.add_cascade(label=\\"File\\", menu=self.file_menu) self.root.config(menu=self.menu_bar) def open_file(self): file_path = filedialog.askopenfilename(filetypes=[(\\"Text files\\", \\"*.txt\\")]) if not file_path: return try: with open(file_path, \'r\') as file: content = file.readlines() except Exception as e: messagebox.showerror(\\"Error\\", f\\"Failed to read file: {e}\\") return append_str = simpledialog.askstring(\\"Input\\", \\"Enter a string to append to each line:\\") if append_str is None: return modified_content = [line.strip() + append_str + \'n\' for line in content] self.text_area.delete(1.0, tk.END) self.text_area.insert(tk.END, \'\'.join(modified_content)) def save_file(self): file_path = filedialog.asksaveasfilename(defaultextension=\\".txt\\", filetypes=[(\\"Text files\\", \\"*.txt\\")]) if not file_path: return modified_content = self.text_area.get(1.0, tk.END) try: with open(file_path, \'w\') as file: file.write(modified_content) except Exception as e: messagebox.showerror(\\"Error\\", f\\"Failed to save file: {e}\\") if __name__ == \\"__main__\\": root = tk.Tk() app = TextFileEditor(root) root.mainloop() ```","solution":"import tkinter as tk from tkinter import filedialog, simpledialog, messagebox class TextFileEditor: def __init__(self, root): self.root = root self.root.title(\\"Text File Editor\\") self.text_area = tk.Text(self.root, wrap=\'word\') self.text_area.pack(expand=True, fill=\'both\') self.menu_bar = tk.Menu(self.root) self.file_menu = tk.Menu(self.menu_bar, tearoff=0) self.file_menu.add_command(label=\\"Open\\", command=self.open_file) self.file_menu.add_command(label=\\"Save As\\", command=self.save_file) self.menu_bar.add_cascade(label=\\"File\\", menu=self.file_menu) self.root.config(menu=self.menu_bar) def open_file(self): file_path = filedialog.askopenfilename(filetypes=[(\\"Text files\\", \\"*.txt\\")]) if not file_path: return try: with open(file_path, \'r\') as file: content = file.readlines() except Exception as e: messagebox.showerror(\\"Error\\", f\\"Failed to read file: {e}\\") return append_str = simpledialog.askstring(\\"Input\\", \\"Enter a string to append to each line:\\") if append_str is None: return modified_content = [line.strip() + append_str + \'n\' for line in content] self.text_area.delete(1.0, tk.END) self.text_area.insert(tk.END, \'\'.join(modified_content)) def save_file(self): file_path = filedialog.asksaveasfilename(defaultextension=\\".txt\\", filetypes=[(\\"Text files\\", \\"*.txt\\")]) if not file_path: return modified_content = self.text_area.get(1.0, tk.END) try: with open(file_path, \'w\') as file: file.write(modified_content) except Exception as e: messagebox.showerror(\\"Error\\", f\\"Failed to save file: {e}\\") if __name__ == \\"__main__\\": root = tk.Tk() app = TextFileEditor(root) root.mainloop()"},{"question":"You are tasked with reading a large text file, converting all text to uppercase, and then writing the result to a new file while ensuring efficient and appropriate use of different types of I/O streams provided by the `io` module. The input and output files can be very large, so performance considerations such as proper buffering and avoiding redundancy are essential. # Requirements: 1. Write a function `convert_file_to_uppercase(input_file_path: str, output_file_path: str) -> None` that performs the following: - Reads data from the file specified by `input_file_path` using text I/O. - Converts all text read from the input file to uppercase. - Writes the uppercase text to a new file specified by `output_file_path`. - Use appropriate buffering to ensure efficient I/O operations. 2. Ensure that your implementation handles potential I/O errors gracefully. 3. Specify the encoding to `utf-8` when opening the text files for reading and writing to avoid encoding-related issues. # Example Usage: ```python convert_file_to_uppercase(\'large_input.txt\', \'large_output.txt\') ``` # Constraints: - The lengths of `input_file_path` and `output_file_path` will be reasonable and within typical filesystem path length limits. - The function should handle files that are very large (several GBs) efficiently. - Consider performance implications and ensure that the implementation is optimized for large file handling. # Notes: - You may make use of the `io` module, particularly classes like `TextIOWrapper` for text I/O and `BufferedReader`/`BufferedWriter` for efficient reading and writing. - Make sure you implement proper error handling to catch and report any read/write exceptions that may occur during file operations.","solution":"import io def convert_file_to_uppercase(input_file_path: str, output_file_path: str) -> None: try: with io.open(input_file_path, \'r\', encoding=\'utf-8\') as infile: with io.open(output_file_path, \'w\', encoding=\'utf-8\') as outfile: for line in infile: outfile.write(line.upper()) except IOError as e: print(f\\"An I/O error occurred: {e}\\")"},{"question":"Objective Design a Python function that interacts with the operating system\'s environment variables and processes, demonstrating your understanding of the `os` module which encapsulates the `posix` functionalities. Task Write a Python function: ```python def execute_command_with_env(command: str, new_env_vars: dict) -> int: Executes a command in a new process with modified environment variables. Parameters: - command (str): The command to execute as a string. - new_env_vars (dict): A dictionary of environment variables to set for the command execution. Keys and values should both be strings. Returns: - int: The exit code of the executed command. ``` Requirements 1. **Command Execution**: - The function should execute the provided command using a new process. - Use the `os.system()` or `os.popen()` for command execution. 2. **Environment Variables**: - Before running the command, set the environment variables provided in the `new_env_vars` dictionary. - Ensure the new environment variables do not permanently affect the Python interpreter\'s environment. 3. **Return Value**: - The function should return the exit code of the executed command. 4. **Constraints**: - Assume the command is valid and the environment variables are correctly specified. - You cannot use third-party libraries. Example ```python new_environment = {\'SAMPLE_VAR\': \'value123\'} exit_code = execute_command_with_env(\\"echo SAMPLE_VAR\\", new_environment) print(exit_code) # Should print the exit code of the command. ``` In this example, the command `echo SAMPLE_VAR` should output \\"value123\\" if the environment variable is correctly set. The function will return the exit code of the `echo` command\'s execution. Notes - Make sure to handle and report errors gracefully. You might want to capture and log any exceptions that occur during command execution. - Consider any edge cases, such as empty strings for command or environment variables. Performance - The execution time of the function will primarily depend on the command being executed. - Ensure that the function itself does not add significant overhead. This question assesses the student\'s ability to manage operating system interactions via Python, paying special attention to environment variables and subprocess management.","solution":"import os import subprocess def execute_command_with_env(command: str, new_env_vars: dict) -> int: Executes a command in a new process with modified environment variables. Parameters: - command (str): The command to execute as a string. - new_env_vars (dict): A dictionary of environment variables to set for the command execution. Keys and values should both be strings. Returns: - int: The exit code of the executed command. # Create a copy of the current environment variables env = os.environ.copy() # Update the copy with the new environment variables env.update(new_env_vars) # Execute the command and wait for it to complete result = subprocess.run(command, shell=True, env=env) # Return the exit code of the command return result.returncode"},{"question":"# Coding Assessment: Data Validation and Sparse Matrix Normalization using Scikit-Learn Utilities Objective Demonstrate your ability to handle data validation and normalization for sparse matrices using the utilities provided in the `sklearn.utils` module. # Problem Statement You are required to implement a function `validate_and_normalize_sparse_matrix` that: 1. Validates a given 2D sparse matrix ensuring it contains no NaNs or Infs. 2. Normalizes the rows of the sparse matrix to unit L2 norm if no validation errors are found. # Function Signature ```python def validate_and_normalize_sparse_matrix(matrix): Validate and normalize a given 2D sparse matrix. Parameters: - matrix: scipy.sparse.csr_matrix A 2D sparse matrix to be validated and normalized. Returns: - normalized_matrix: scipy.sparse.csr_matrix The row-normalized sparse matrix with each row having a unit L2 norm. Raises: - ValueError: If the matrix contains NaNs or Infs, or if the matrix is not 2D. ``` # Input - `matrix`: A 2D sparse matrix in CSR format (scipy.sparse.csr_matrix). It is guaranteed that the input matrix will be in CSR format but you need to handle cases where it may contain NaNs or Infs. # Output - A row-normalized 2D sparse matrix in CSR format. # Constraints - The matrix should be validated for the presence of NaNs or Infs. If any are found, raise a `ValueError`. - The matrix should be validated to confirm it is a 2D array. If not, raise a `ValueError`. - Use `sklearn.utils.validation` functions for validation. - Use `sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l2` for normalizing the rows. # Example ```python from scipy.sparse import csr_matrix import numpy as np matrix_data = np.array([[1, 0, 2], [0, 0, 0], [4, 5, 6]], dtype=np.float64) sparse_matrix = csr_matrix(matrix_data) normalized_matrix = validate_and_normalize_sparse_matrix(sparse_matrix) print(normalized_matrix.toarray()) # Expected Output: # array([[0.4472136, 0. , 0.8944272], # [0. , 0. , 0. ], # [0.4558423, 0.5698029, 0.6837635]]) ``` # Notes - Ensure the implementation is efficient and makes use of the Scikit-Learn utilities for validation and normalization as specified.","solution":"from scipy.sparse import csr_matrix from sklearn.utils import check_array from sklearn.utils.sparsefuncs_fast import inplace_csr_row_normalize_l2 import numpy as np def validate_and_normalize_sparse_matrix(matrix): Validate and normalize a given 2D sparse matrix. Parameters: - matrix: scipy.sparse.csr_matrix A 2D sparse matrix to be validated and normalized. Returns: - normalized_matrix: scipy.sparse.csr_matrix The row-normalized sparse matrix with each row having a unit L2 norm. Raises: - ValueError: If the matrix contains NaNs or Infs, or if the matrix is not 2D. if not isinstance(matrix, csr_matrix): raise ValueError(\\"Input should be a CSR matrix.\\") # Ensure the matrix is in CSR format and is 2D matrix_data = check_array(matrix, accept_sparse=True, force_all_finite=True) # Normalize the rows to have unit L2 norm inplace_csr_row_normalize_l2(matrix) return matrix"},{"question":"Objective: Create a robust machine learning pipeline using scikit-learn to load and preprocess a dataset from the OpenML repository, and then apply a classification algorithm to the data. Problem Description: You are tasked with training a classifier on the \\"miceprotein\\" dataset from OpenML. Requirements: 1. Write a function `load_and_preprocess_mice_data` to: - Fetch the \\"miceprotein\\" dataset (you may specify by `name=\'miceprotein\'` and `version=4`). - Preprocess the dataset such that: - All features are scaled to have zero mean and unit variance using `StandardScaler`. - Encode any categorical variables appropriately using `OneHotEncoder` or `OrdinalEncoder`. 2. Write a function `train_classifier` to: - Split the dataset into training and testing sets (use a 80-20 split). - Train a `RandomForestClassifier` on the training data. - Evaluate the classifier on the test data and return the accuracy. Expected Input and Output: The `load_and_preprocess_mice_data` function should return a tuple `(X, y)` where `X` is the preprocessed feature matrix and `y` is the target vector. The `train_classifier` function should take `(X, y)` as input, and it should return the accuracy on the test data. Constraints: - You must use scikit-learn\'s built-in functions and classes for data loading, preprocessing, and classification. Additional Notes: - Ensure you handle any missing values appropriately (you may fill them with the mean or any other imputation method). - Document your code to indicate any assumptions made and steps taken. Code Template: ```python from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import make_pipeline from sklearn.compose import ColumnTransformer from sklearn.metrics import accuracy_score import pandas as pd def load_and_preprocess_mice_data(): # Fetch the dataset # Preprocess the data # Return preprocessed X and y pass def train_classifier(X, y): # Split the data into training and testing sets # Train a RandomForestClassifier # Evaluate the classifier and return the accuracy pass # Example use-case X, y = load_and_preprocess_mice_data() accuracy = train_classifier(X, y) print(f\'Accuracy: {accuracy:.2f}\') ```","solution":"from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import make_pipeline from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.metrics import accuracy_score import pandas as pd def load_and_preprocess_mice_data(): # Fetch the dataset data = fetch_openml(name=\'miceprotein\', version=4, as_frame=True) df = data.frame # Separate features and target X = df.drop(columns=[\'class\']) y = df[\'class\'] # Identify categorical and numerical columns categorical_cols = X.select_dtypes(include=[\'object\']).columns numerical_cols = X.select_dtypes(exclude=[\'object\']).columns # Preprocess the data # Fill missing values and scale the numerical features numerical_transformer = make_pipeline( SimpleImputer(strategy=\'mean\'), StandardScaler() ) # One-hot encode the categorical features categorical_transformer = make_pipeline( SimpleImputer(strategy=\'most_frequent\'), OneHotEncoder(handle_unknown=\'ignore\') ) # Combine both preprocessing pipelines preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_cols), (\'cat\', categorical_transformer, categorical_cols), ] ) X_processed = preprocessor.fit_transform(X) return X_processed, y def train_classifier(X, y): # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a RandomForestClassifier clf = RandomForestClassifier(n_estimators=100, random_state=42) clf.fit(X_train, y_train) # Evaluate the classifier y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy # Example use-case X, y = load_and_preprocess_mice_data() accuracy = train_classifier(X, y) print(f\'Accuracy: {accuracy:.2f}\')"},{"question":"You are tasked with creating a utility that encodes and decodes data between different formats using the `binascii` module. Implement the following three functions: 1. **binary_to_base64(data: bytes) -> str**: - **Input**: a bytes object `data` - **Output**: a base64 encoded string representation of the input data. 2. **base64_to_binary(encoded: str) -> bytes**: - **Input**: a base64 encoded string `encoded` - **Output**: the original binary data as a bytes object. 3. **binary_to_hex_with_separator(data: bytes, sep: str, bytes_per_sep: int) -> str**: - **Input**: - a bytes object `data` - a string `sep` to be used as a separator - an integer `bytes_per_sep` specifying how often to insert the separator - **Output**: a hexadecimal string representation of the binary data, with the separator inserted. You must handle the cases where the inputs do not conform to expected formats or constraints by raising appropriate exceptions. # Requirements - You must use the `binascii` module for encoding and decoding. - The `binary_to_base64` and `base64_to_binary` functions should handle base64 encoding and decoding respectively. - The `binary_to_hex_with_separator` function should use the `binascii.b2a_hex` function, ensuring that the given separator is inserted correctly. # Examples ```python import binascii def binary_to_base64(data: bytes) -> str: return binascii.b2a_base64(data).decode().strip() def base64_to_binary(encoded: str) -> bytes: return binascii.a2b_base64(encoded) def binary_to_hex_with_separator(data: bytes, sep: str, bytes_per_sep: int) -> str: return binascii.b2a_hex(data, sep.encode(), bytes_per_sep).decode() # Example: binary_data = b\'hello world\' base64_encoded = binary_to_base64(binary_data) print(base64_encoded) # Output: \'aGVsbG8gd29ybGQ=\' binary_decoded = base64_to_binary(base64_encoded) print(binary_decoded) # Output: b\'hello world\' hex_with_sep = binary_to_hex_with_separator(b\'xb9x01xef\', \'-\', 1) print(hex_with_sep) # Output: \'b-9-0-1-e-f\' ``` # Constraints - The input bytes object for `binary_to_base64` and `binary_to_hex_with_separator` should not be empty. - The string for base64 decoding should be a valid base64 encoded string. - The `sep` should be a single character string or bytes object compatible with the `binascii` functions. - Handle any potential errors gracefully and raise appropriate exceptions with descriptive error messages. # Note The separator must be a valid byte character, meaning its length should be 1.","solution":"import binascii def binary_to_base64(data: bytes) -> str: Converts binary data (bytes) to a Base64 encoded string. if not isinstance(data, bytes): raise TypeError(\\"Input must be a bytes object.\\") return binascii.b2a_base64(data).decode().strip() def base64_to_binary(encoded: str) -> bytes: Converts a Base64 encoded string to binary data (bytes). if not isinstance(encoded, str): raise TypeError(\\"Input must be a string.\\") try: return binascii.a2b_base64(encoded) except binascii.Error: raise ValueError(\\"Invalid Base64 encoded string.\\") def binary_to_hex_with_separator(data: bytes, sep: str, bytes_per_sep: int) -> str: Converts binary data (bytes) to a hexadecimal string representation with separators. if not isinstance(data, bytes): raise TypeError(\\"Input must be a bytes object.\\") if not isinstance(sep, str) or len(sep) != 1: raise ValueError(\\"Separator must be a single character string.\\") if not isinstance(bytes_per_sep, int) or bytes_per_sep <= 0: raise ValueError(\\"bytes_per_sep must be a positive integer.\\") try: hex_str = binascii.hexlify(data).decode() separated_hex = sep.join(hex_str[i:i+2] for i in range(0, len(hex_str), 2*bytes_per_sep)) return separated_hex except Exception as e: raise ValueError(f\\"Error in converting binary to hex with separator: {str(e)}\\")"},{"question":"# Challenge Description The objective of this challenge is to familiarize you with the `ChainMap` and `Counter` classes from the `collections` module in Python. You are required to write two functions: # Function 1: `combine_mappings` This function will take a list of dictionaries and combine them into a single `ChainMap`. Furthermore, updates to the individual dictionaries should reflect in the combined `ChainMap`. **Function Signature:** ```python def combine_mappings(mappings: list[dict]) -> ChainMap: pass ``` **Inputs:** - `mappings`: A list of dictionaries. Each dictionary can have string keys and integer values. **Outputs:** - Returns a `ChainMap` containing all the dictionaries from the list. **Example:** ```python dict1 = {\'a\': 1, \'b\': 2} dict2 = {\'b\': 3, \'c\': 4} combined = combine_mappings([dict1, dict2]) # Combined ChainMap should reflect updates assert combined[\'a\'] == 1 assert combined[\'b\'] == 2 assert combined[\'c\'] == 4 dict1[\'a\'] = 10 assert combined[\'a\'] == 10 ``` # Function 2: `word_counter` This function will accept a list of words and return a `Counter` object that counts the frequency of each word in the list. **Function Signature:** ```python def word_counter(words: list[str]) -> Counter: pass ``` **Inputs:** - `words`: A list of strings, where each string represents a word. **Outputs:** - Returns a `Counter` object with the frequency of each word. **Example:** ```python words = [\\"apple\\", \\"banana\\", \\"apple\\", \\"orange\\", \\"banana\\", \\"banana\\"] counter = word_counter(words) assert counter[\'apple\'] == 2 assert counter[\'banana\'] == 3 assert counter[\'orange\'] == 1 ``` # Constraints 1. Do not use any external libraries other than `collections`. 2. Ensure that your function adheres to the function signature provided. 3. Your solution should be efficient with a complexity not exceeding O(N) for both functions, where N is the number of elements in the inputs. # Assessment Criteria - Correctness: The functions should return expected results for both provided examples and edge cases. - Efficiency: The solutions should be optimized for performance. - Adherence to Guidelines: The function signatures, input, and output formats must be followed strictly. Good luck and happy coding!","solution":"from collections import ChainMap, Counter def combine_mappings(mappings): Combine a list of dictionaries into a single ChainMap. Updates to the individual dictionaries will be reflected in the combined ChainMap. Args: mappings (list[dict]): List of dictionaries to be combined. Returns: ChainMap: A ChainMap object containing all the dictionaries from the list. return ChainMap(*mappings) def word_counter(words): Create a Counter object that counts the frequency of each word in the list. Args: words (list[str]): List of words to be counted. Returns: Counter: A Counter object with the frequency of each word. return Counter(words)"},{"question":"# ZIP File Management with Nested Directories and Compression Methods Background The `zipfile` module in Python provides extensive capabilities to work with ZIP archives, including creating new ZIP files with different compression methods, listing their contents, and extracting files. This question will test your ability to understand and utilize this module to handle complex ZIP files. Task You are required to implement a function `create_and_manage_zip` that performs the following steps: 1. **Create a ZIP Archive**: - Create a new ZIP file named `output.zip`. - Include multiple nested directories and files within these directories. - Use different compression methods (`ZIP_DEFLATED`, `ZIP_BZIP2`, and `ZIP_LZMA`) for different files. 2. **List Contents**: - List all the files with their paths in the ZIP archive after creation. 3. **Extract Files**: - Extract all files to a directory named `extracted_files`. Specifications - You must use the `zipfile` module. - The ZIP file structure should be as follows: ``` output.zip â”œâ”€â”€ folder1 â”‚ â”œâ”€â”€ file1.txt (ZIP_DEFLATED) â”‚ â””â”€â”€ file2.txt (ZIP_BZIP2) â””â”€â”€ folder2 â”œâ”€â”€ subfolder1 â”‚ â””â”€â”€ file3.txt (ZIP_LZMA) â””â”€â”€ subfolder2 â””â”€â”€ file4.txt (ZIP_DEFLATED) ``` - The contents of each text file can be simple lines of text indicating the file name (e.g., \\"This is file1.txt\\"). Function Signature ```python def create_and_manage_zip(): pass ``` Expected Output - The function should create the `output.zip` file with the specified structure and compression methods. - Then, it should list all the files with their paths within the ZIP archive. - Finally, it should extract all files from the archive into the `extracted_files` directory. Constraints - Follow best practices for file handling. - Ensure that the program works correctly if run multiple times (does not crash if the files or directories already exist). Example Here is an example of how your function will be tested: ```python def test_create_and_manage_zip(): create_and_manage_zip() # Further code to check the contents of output.zip and extracted_files test_create_and_manage_zip() ``` Ensure that your function creates the ZIP file, lists its contents, and extracts the files correctly into the `extracted_files` directory.","solution":"import os import zipfile def create_and_manage_zip(): zip_filename = \'output.zip\' extract_dir = \'extracted_files\' # Ensure directories are ready if not os.path.exists(extract_dir): os.makedirs(extract_dir) # Create a new ZIP file with zipfile.ZipFile(zip_filename, \'w\') as zf: # Adding files with different compression methods files_to_add = { \'folder1/file1.txt\': zipfile.ZIP_DEFLATED, \'folder1/file2.txt\': zipfile.ZIP_BZIP2, \'folder2/subfolder1/file3.txt\': zipfile.ZIP_LZMA, \'folder2/subfolder2/file4.txt\': zipfile.ZIP_DEFLATED, } for filepath, compression in files_to_add.items(): content = f\\"This is {filepath.split(\'/\')[-1]}\\" zf.writestr(filepath, content, compress_type=compression) # List all files in the zip archive with zipfile.ZipFile(zip_filename, \'r\') as zf: zip_contents = zf.namelist() for file in zip_contents: print(file) # Extract all files from the archive with zipfile.ZipFile(zip_filename, \'r\') as zf: zf.extractall(path=extract_dir) return zip_contents"},{"question":"**Question: Managing `.netrc` Files** You are tasked with writing a Python function that interacts with `.netrc` files to manage FTP client configurations. # Requirements: Write a function `manage_netrc(file_path: str, host: str) -> str` that performs the following operations: 1. Parses the given `.netrc` file specified by `file_path`. 2. Retrieves the (login, account, password) tuple for the specified `host`. 3. If the host is found, returns a formatted string: `\\"Host: <host>, Login: <login>, Account: <account>, Password: <password>\\"`. 4. If the host is not found, it returns `\\"Host not found in .netrc file.\\"`. # Input: - `file_path` (string): The path to the `.netrc` file. - `host` (string): The host for which to retrieve the login credentials. # Output: - Returns a string as mentioned in the requirements above. # Constraints: - Assume the `.netrc` file is properly formatted but handle `FileNotFoundError` (if the file does not exist) and `NetrcParseError` (if there are syntax errors in the file). - Passwords are limited to a subset of the ASCII character set as mentioned in the documentation. # Example: ```python # Example content of \'.netrc\' file: # machine host1 # login user1 # password pass1 # # machine host2 # login user2 # account acc2 # password pass2 result = manage_netrc(\'path/to/netrc\', \'host2\') print(result) # Expected Output: \\"Host: host2, Login: user2, Account: acc2, Password: pass2\\" result = manage_netrc(\'path/to/netrc\', \'host3\') print(result) # Expected Output: \\"Host not found in .netrc file.\\" ``` # Notes: - Ensure that your implementation handles both the presence and absence of the account field gracefully. - Default the account value to an empty string if it is not present in the .netrc file entry for a host. **Hint:** Utilize the methods and attributes of the `netrc` class provided in the documentation to implement your solution.","solution":"import os import netrc from netrc import NetrcParseError def manage_netrc(file_path: str, host: str) -> str: Parses the given .netrc file and retrieves the login credentials for the specified host. if not os.path.exists(file_path): return \\"File not found.\\" try: credentials = netrc.netrc(file_path) auth_data = credentials.authenticators(host) if auth_data: login, account, password = auth_data account = account if account else \\"\\" return f\\"Host: {host}, Login: {login}, Account: {account}, Password: {password}\\" else: return \\"Host not found in .netrc file.\\" except (FileNotFoundError, NetrcParseError): return \\"Error parsing .netrc file.\\""},{"question":"# File Descriptor Lock Manager Your task is to implement a function called `lock_manager` that uses the `fcntl` module to manage file descriptor locks in a Unix-like operating system. The function will take a file path and a command as input and perform the specified locking operation on the file. Function Signature ```python def lock_manager(file_path: str, command: str) -> str: pass ``` Parameters - `file_path` (str): The path to the file on which the lock operation will be performed. - `command` (str): A string representing the locking operation. It can be one of the following: - `\\"LOCK_SH_NON_BLOCK\\"`: Acquire a shared lock without blocking. - `\\"LOCK_EX\\"`: Acquire an exclusive lock. - `\\"LOCK_UN\\"`: Release the lock. Returns - `str`: A success message indicating the performed operation, or an error message if the operation failed. Constraints - Only the specified commands should be handled. If an unknown command is received, return `\\"Invalid command\\"`. - Ensure you handle any possible `OSError` exceptions that may arise from the locking operations. - Consider the scenario where locking cannot be acquired immediately for non-blocking locks and handle the exception appropriately. Example Usage ```python # Assuming that the file \'example.txt\' exists output = lock_manager(\'example.txt\', \'LOCK_SH_NON_BLOCK\') print(output) # Output: \\"Shared lock acquired without blocking\\" output = lock_manager(\'example.txt\', \'LOCK_EX\') print(output) # Output: \\"Exclusive lock acquired\\" output = lock_manager(\'example.txt\', \'LOCK_UN\') print(output) # Output: \\"Lock released\\" output = lock_manager(\'example.txt\', \'INVALID\') print(output) # Output: \\"Invalid command\\" ``` # Notes - Use `fcntl.lockf()` for the locking operations, making sure to use the appropriate command constants (`fcntl.LOCK_SH`, `fcntl.LOCK_EX`, `fcntl.LOCK_UN`). - Use bitwise OR with `fcntl.LOCK_NB` for the `LOCK_SH_NON_BLOCK` command. # Hints 1. To open the file descriptor, use Python\'s built-in `open()` method. 2. Remember to close the file descriptor after performing the operation. 3. Use exception handling to manage `OSError` exceptions that may arise due to issues such as `EACCES` or `EAGAIN`.","solution":"import fcntl def lock_manager(file_path: str, command: str) -> str: Manage file descriptor locks using fcntl. Parameters: - file_path (str): The path to the file on which the lock operation will be performed. - command (str): The locking operation to be performed (\'LOCK_SH_NON_BLOCK\', \'LOCK_EX\', \'LOCK_UN\'). Returns: - str: A success message indicating the performed operation or an error message if the operation failed. try: with open(file_path, \'r+\') as file: if command == \\"LOCK_SH_NON_BLOCK\\": try: fcntl.lockf(file, fcntl.LOCK_SH | fcntl.LOCK_NB) return \\"Shared lock acquired without blocking\\" except OSError as e: return f\\"Failed to acquire shared lock without blocking: {e}\\" elif command == \\"LOCK_EX\\": fcntl.lockf(file, fcntl.LOCK_EX) return \\"Exclusive lock acquired\\" elif command == \\"LOCK_UN\\": fcntl.lockf(file, fcntl.LOCK_UN) return \\"Lock released\\" else: return \\"Invalid command\\" except OSError as e: return f\\"Error handling lock: {e}\\""},{"question":"# Quoted-Printable Encoding/Decoding Challenge You are provided with several utility functions within the `quopri` module in Python, which helps encode and decode MIME quoted-printable data. Your task is to implement a custom encoder and decoder for quoted-printable data with the following constraints: 1. Implement a function `custom_encode(input_string: str, quotetabs: bool = False, header: bool = False) -> str` that: - Takes a plain input string. - Encodes the string into quoted-printable format where: - Non-printable characters (ASCII code < 32 and > 126) should be represented in the form `=XY`, where `XY` is the hexadecimal representation of the character\'s ASCII code. - If quotetabs is `True`, encode spaces and tabs (ASCII codes 32 and 9) in the same manner. - Spaces at the end of a line should always be encoded regardless of the quotetabs setting. - If header is `True`, spaces should be encoded as underscores (`_`). 2. Implement a function `custom_decode(encoded_string: str, header: bool = False) -> str` that: - Takes an encoded quoted-printable string. - Decodes it back to the original plain string format. - If header is `True`, underscores should be decoded back to spaces. Example Usage ```python encoded_string = custom_encode(\\"Hello World!\\", quotetabs=True, header=True) print(encoded_string) # Expected Output: \'Hello_World!=21\' decoded_string = custom_decode(encoded_string, header=True) print(decoded_string) # Expected Output: \'Hello World!\' ``` Constraints - You should not use the `quopri` module functions directly in your custom implementation. - Maintain efficiency in both encoding and decoding processes. - Aim for clarity and readability in your code. Test your implementation with various examples to ensure compliance with the rules and correct functionality.","solution":"def _to_hex(value: int) -> str: return f\'={value:02X}\' def custom_encode(input_string: str, quotetabs: bool = False, header: bool = False) -> str: encoded_chars = [] for char in input_string: ascii_code = ord(char) if header and char == \' \': encoded_chars.append(\'_\') elif ascii_code < 32 or ascii_code > 126 or (quotetabs and char in {\' \', \'t\'}): encoded_chars.append(_to_hex(ascii_code)) else: encoded_chars.append(char) # Handle spaces at the end of lines to be always encoded encoded_string = \'\'.join(encoded_chars) lines = encoded_string.split(\'n\') for i, line in enumerate(lines): if line.endswith(\' \'): lines[i] = line[:-1] + _to_hex(32) return \'n\'.join(lines) def custom_decode(encoded_string: str, header: bool = False) -> str: decoded_chars = [] i = 0 while i < len(encoded_string): if header and encoded_string[i] == \'_\': decoded_chars.append(\' \') i += 1 elif encoded_string[i] == \'=\' and i+2 < len(encoded_string): hex_value = encoded_string[i+1:i+3] decoded_chars.append(chr(int(hex_value, 16))) i += 3 else: decoded_chars.append(encoded_string[i]) i += 1 return \'\'.join(decoded_chars)"},{"question":"# Asynchronous Task Queue You are required to implement a system for processing tasks asynchronously using Python\'s `asyncio.Queue`. The system simulates a scenario where different types of tasks need to be processed in a specific order. For this task, you will need to use the `asyncio.PriorityQueue`, which retrieves entries based on their priority (lowest number first). Your implementation must include the following components: 1. **Task Generation:** - Create a coroutine `generate_tasks` that generates and puts tasks into the queue. Each task should be a tuple of the form `(priority, task_id, processing_time)`, where: - `priority` (int): The priority of the task (lower numbers indicate higher priority). - `task_id` (int): A unique identifier for the task. - `processing_time` (float): The time (in seconds) required to process the task. 2. **Task Processing:** - Create a coroutine `process_tasks` that continuously gets tasks from the queue and processes them. Processing means simply waiting for `processing_time` seconds using `await asyncio.sleep(processing_time)`. After processing a task, log the completion by printing `Task {task_id} with priority {priority} processed`. 3. **Main coroutine:** - Create the main coroutine `main` that performs the following actions: - Initializes an `asyncio.PriorityQueue`. - Starts a fixed number of task processors (e.g., 3 processors). - Generates a predefined number of tasks with random priorities and processing times, and puts them into the queue. - Ensures that all tasks are processed by waiting for the queue to be fully processed using `join`. - Ensures proper cleanup by canceling all processor tasks after processing is complete. # Constraints - The number of tasks to generate: 10 - The priority of each task: Random integer between 1 and 5 - The processing time of each task: Random float between 0.1 to 1.0 seconds # Expected Input and Output - The `generate_tasks` coroutine should input the number of tasks and the queue. - The `process_tasks` coroutine should input the queue. - The `main` coroutine orchestrates the task generation and processing, ensuring that all tasks are processed before exiting. # Example Output ``` Task 1 with priority 1 processed Task 5 with priority 2 processed Task 3 with priority 3 processed Task 4 with priority 4 processed Task 2 with priority 5 processed ... ``` # Performance Requirements - The implementation should avoid busy-waiting and should efficiently utilize asynchronous patterns. - Proper synchronization and error handling should be implemented. # Implementation Notes - Use `asyncio` methods such as `await`, `asyncio.create_task`, and `asyncio.gather`. - Ensure that the processing does not block the event loop. Here is the skeleton code to get you started: ```python import asyncio import random async def generate_tasks(num_tasks, queue): pass # Implement this async def process_tasks(queue): pass # Implement this async def main(): pass # Implement this if __name__ == \\"__main__\\": asyncio.run(main()) ``` Implement the `generate_tasks`, `process_tasks`, and `main` coroutines according to the specifications and ensure the entire task processing system works correctly.","solution":"import asyncio import random async def generate_tasks(num_tasks, queue): for i in range(num_tasks): priority = random.randint(1, 5) task_id = i processing_time = round(random.uniform(0.1, 1.0), 2) await queue.put((priority, task_id, processing_time)) print(f\\"Task {task_id} with priority {priority} and processing time {processing_time} added to queue.\\") async def process_tasks(queue): while True: priority, task_id, processing_time = await queue.get() await asyncio.sleep(processing_time) print(f\\"Task {task_id} with priority {priority} processed\\") queue.task_done() async def main(): queue = asyncio.PriorityQueue() num_tasks = 10 num_processors = 3 # Start task processors processors = [asyncio.create_task(process_tasks(queue)) for _ in range(num_processors)] # Generate tasks await generate_tasks(num_tasks, queue) # Wait until all tasks are processed await queue.join() # Cancel processors after all tasks are processed for processor in processors: processor.cancel() # Await cancellation of all processors await asyncio.gather(*processors, return_exceptions=True) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"<|Analysis Begin|> The `zipapp` module documentation provided details the capabilities of managing executable Python zip archives. It describes a command-line interface and a Python API to create or modify such archives. Key functions highlighted include `create_archive` and `get_interpreter`. Important functionalities of the `zipapp` module include: 1. Creating an archive from a directory or an existing application zip file. 2. Specifying an interpreter for the archive. 3. Defining an entry-point function for the application. 4. Optional file compression. 5. Providing diagnostic information about the interpreter embedded in an archive. Examples and use cases describe creating standalone applications, making Windows executables, and ensuring compatibility with different Python versions. The core of the module revolves around creating and manipulating zip files that include Python executables, making it a comprehensive tool for bundling and distributing Python applications. The `create_archive` function appears to be the primary method for creating zipapp archives while allowing flexible customization through parameters such as `interpreter`, `main`, `filter`, and `compressed`. <|Analysis End|> <|Question Begin|> # Task You are required to write a Python script using the `zipapp` module that packages a given directory of Python files into a standalone, executable zip archive. # Requirements 1. **Input**: - A directory containing Python files to be packaged. - (Optional) The output path for the resulting `.pyz` archive. - (Optional) A shebang line specifying the interpreter to be added. - (Optional) The entry point for the application in the form \\"module:function\\". 2. **Output**: - A `.pyz` zip archive file created from the input directory. 3. **Constraints**: - The input directory must exist and contain at least one Python file. - If the output path is not specified, the script should create the archive in the input directory with the name `<dirname>.pyz`. - If the interpreter is not specified, the script should use the system\'s default Python interpreter. - If the entry point is not specified, the script should ensure there is a `__main__.py` in the root directory of the archive. 4. **Function**: - Function `create_executable_zip(source, target=None, interpreter=None, main=None)`: - `source`: Path to the directory to be packaged. - `target`: Output path for the `.pyz` archive (optional). - `interpreter`: Interpreter to be added to the archive\'s shebang line (optional). - `main`: Entry point for the archive in the format \\"module:function\\" (optional). # Performance Requirements - The function should handle directories of moderate size efficiently. - The output file, if specified, should be verified for existence and validity before writing. # Example Usage ```python # Given a directory \\"myapp\\" containing Python files create_executable_zip(\\"myapp\\", target=\\"myapp_executable.pyz\\", interpreter=\\"/usr/bin/env python3\\", main=\\"myapp:main\\") ``` The example creates an archive `myapp_executable.pyz` from the `myapp` directory, specifying `/usr/bin/env python3` as the interpreter and `myapp:main` as the entry point. # Hints - You may use the `zipapp.create_archive` function. - Validate the input directory and handle cases where optional parameters are not provided with sensible defaults. Write your code below: ```python import zipapp import os from pathlib import Path def create_executable_zip(source, target=None, interpreter=None, main=None): # Validate input directory if not Path(source).is_dir(): raise FileNotFoundError(f\\"The source directory \'{source}\' does not exist or is not a directory.\\") # Set default target if not specified if target is None: target = f\\"{source}.pyz\\" # Create the .pyz archive zipapp.create_archive(source, target=target, interpreter=interpreter, main=main) print(f\\"Created archive at: {target}\\") # Example function call # It should be commented or removed after running tests #create_executable_zip(\\"myapp\\", target=\\"myapp_executable.pyz\\", interpreter=\\"/usr/bin/env python3\\", main=\\"myapp:main\\") ``` Ensure that the `myapp` directory exists and is populated with the necessary Python files before running the example function. Adjust parameters as needed to test different configurations.","solution":"import zipapp import os from pathlib import Path def create_executable_zip(source, target=None, interpreter=None, main=None): # Validate input directory if not Path(source).is_dir(): raise FileNotFoundError(f\\"The source directory \'{source}\' does not exist or is not a directory.\\") # Set default target if not specified source_name = Path(source).name if target is None: target = f\\"{source_name}.pyz\\" # Create the .pyz archive zipapp.create_archive(source, target=target, interpreter=interpreter, main=main) print(f\\"Created archive at: {target}\\") # Example function call # It should be commented or removed after running tests # create_executable_zip(\\"myapp\\", target=\\"myapp_executable.pyz\\", interpreter=\\"/usr/bin/env python3\\", main=\\"myapp:main\\")"},{"question":"**Objective**: Use seaborn\'s `objects` API to visualize data, handle overlapping bars, utilize multiple properties, and add error bars. Problem Statement Given the seaborn datasets `penguins` and `flights`, perform the following tasks: 1. Load the `penguins` dataset. 2. Create a bar plot showing the count of each `species` in the `penguins` dataset. 3. Modify the above plot to show the bars for male and female penguins separately, using different colors. 4. Update the plot to dodge the bars such that they do not overlap. 5. Add error bars representing the standard deviation of `body_mass_g` for each `species` and `sex`. 6. Customize the bars to have a varying edge width based on `sex`. Constraints - Use the seaborn `objects` API (e.g., `so.Plot`). - Ensure the plots are clear and visually distinguishable. Input Format No direct input required; use the seaborn datasets: - `penguins` - `flights` Output Format Maintain intermediate plot outputs to visually confirm each step of the transformation. Example Hereâ€™s a sequence of example steps: ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load dataset penguins = load_dataset(\\"penguins\\") # Step 2: Create Bar Plot for each species plot = so.Plot(penguins, x=\\"species\\").add(so.Bar(), so.Hist()) plot.show() # Step 3: Modify the plot to show bars for different sexes plot = so.Plot(penguins, x=\\"species\\", color=\\"sex\\").add(so.Bar(), so.Hist()) plot.show() # Step 4: Apply dodge to avoid overlapping plot = plot.add(so.Dodge()) plot.show() # Step 5: Add error bars for standard deviation of body_mass_g plot = ( so.Plot(penguins, \\"species\\", color=\\"sex\\") .add(so.Bar(alpha=0.5), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) plot.show() # Step 6: Varying edge width based on sex plot = ( so.Plot(penguins, x=\\"species\\", color=\\"sex\\", alpha=\\"sex\\", edgestyle=\\"sex\\") .add(so.Bar(edgewidth=2), so.Hist(), so.Dodge(\\"fill\\")) ) plot.show() ``` Students are expected to follow a similar approach to complete the problem.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_penguin_data(): # Step 1: Load dataset penguins = load_dataset(\\"penguins\\") # Step 2: Create Bar Plot for each species plot = so.Plot(penguins, x=\\"species\\").add(so.Bar(), so.Hist()) plot.show() # Step 3: Modify the plot to show bars for different sexes plot = so.Plot(penguins, x=\\"species\\", color=\\"sex\\").add(so.Bar(), so.Hist()) plot.show() # Step 4: Apply dodge to avoid overlapping plot = plot.add(so.Dodge()) plot.show() # Step 5: Add error bars for standard deviation of body_mass_g plot = ( so.Plot(penguins, \\"species\\", color=\\"sex\\") .add(so.Bar(alpha=0.5), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) plot.show() # Step 6: Varying edge width based on sex plot = ( so.Plot(penguins, x=\\"species\\", color=\\"sex\\", alpha=\\"sex\\", edgestyle=\\"sex\\") .add(so.Bar(edgewidth=2), so.Hist(), so.Dodge(\\"fill\\")) ) plot.show()"},{"question":"# Custom Import Mechanism in Python Objective: To assess your understanding of Python\'s import system, `importlib` module, and to demonstrate your ability to extend the import machinery using custom finders and loaders. Problem Statement: You are required to implement a custom module finder and loader. Specifically, you will create a system where certain modules can be imported from an in-memory dictionary, rather than from the filesystem. Task: 1. Implement a custom loader that loads Python modules from an in-memory dictionary. 2. Implement a custom finder that tells Python\'s import system to use this loader when importing modules. 3. Register this custom finder in `sys.meta_path` so that it takes priority during module import. Requirements: 1. **In-Memory Module Storage**: Create a dictionary named `in_memory_modules` where the keys are module names (strings) and the values are the source code of the modules (strings). 2. **Custom Loader**: Implement a class `InMemoryLoader` that: - Implements the `exec_module` method to execute module code. - Optionally implements the `create_module` method. 3. **Custom Finder**: Implement a class `InMemoryFinder` that: - Implements the `find_spec` method to return a module spec with the `InMemoryLoader` if the module exists in the in-memory dictionary. 4. **Register Finder**: Add an instance of `InMemoryFinder` to `sys.meta_path`. 5. **Module Import**: Demonstrate importing a module from the `in_memory_modules` dictionary and using a function or class defined in that module. Constraints: - Module names in `in_memory_modules` should be unique. - Module source code in `in_memory_modules` should be valid Python code. Example: ```python # Example of in-memory module storage in_memory_modules = { \'my_module\': \'\'\' class HelloWorld: def greet(self): return \\"Hello, world!\\" def add(a, b): return a + b \'\'\' } # Implement Custom Loader and Finder Here # ... # Register custom finder in sys.meta_path # ... # Demonstration of importing and using the in-memory module import my_module hw = my_module.HelloWorld() print(hw.greet()) # Output: Hello, world! print(my_module.add(3, 4)) # Output: 7 ``` Deliverables: - `InMemoryLoader` class definition. - `InMemoryFinder` class definition. - Code to register the custom finder. - Demonstration code of importing and using a module from the in-memory dictionary. Note: Ensure proper exception handling for cases where the module is not found in the in-memory dictionary.","solution":"import sys import importlib.util import types # In-memory module storage in_memory_modules = { \'my_module\': \'\'\' class HelloWorld: def greet(self): return \\"Hello, world!\\" def add(a, b): return a + b \'\'\' } class InMemoryLoader: def __init__(self, module_name): self.module_name = module_name def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): code = in_memory_modules[self.module_name] exec(code, module.__dict__) class InMemoryFinder: def find_spec(self, fullname, path, target=None): if fullname in in_memory_modules: spec = importlib.util.spec_from_loader(fullname, InMemoryLoader(fullname)) return spec return None # Register the custom finder sys.meta_path.insert(0, InMemoryFinder()) # Demonstration of importing and using the in-memory module import my_module hw = my_module.HelloWorld() print(hw.greet()) print(my_module.add(3, 4))"},{"question":"# Question: Manipulating Tensor Dimensions Using `torch.Size` **Objective:** Given a tensor, you are to implement a function that modifies the tensor based on its dimensions and certain conditions. This will test your understanding of the `torch.Size` class and its utility in tensor operations. **Function Signature:** ```python def modify_tensor_based_on_dimensions(tensor: torch.Tensor) -> torch.Tensor: ``` **Input:** - `tensor` (torch.Tensor): A PyTorch tensor of at least two dimensions. **Output:** - `torch.Tensor`: The modified tensor based on the specified conditions. **Conditions:** 1. If the tensor has an odd number of dimensions (len(tensor.size()) is odd), return the original tensor. 2. If the tensor has an even number of dimensions, perform the following operations: - Compute the product of the sizes of all dimensions. - Create a new tensor with shape `(prod_size, )` filled with zeros. - Reshape the new tensor to match the original tensor\'s shape. **Example:** ```python import torch # Example 1: tensor1 = torch.ones(2, 3) modified_tensor1 = modify_tensor_based_on_dimensions(tensor1) print(modified_tensor1.size()) # Expected output: torch.Size([2, 3]) # Example 2: tensor2 = torch.ones(2, 3, 4) modified_tensor2 = modify_tensor_based_on_dimensions(tensor2) print(modified_tensor2.size()) # Expected output: torch.Size([2, 3, 4]) # Example 3: tensor3 = torch.ones(2, 3, 4, 5) modified_tensor3 = modify_tensor_based_on_dimensions(tensor3) print(modified_tensor3.size()) # Expected output: torch.Size([2, 3, 4, 5]) ``` **Constraints:** - You must not use any looping structures (e.g., for-loops or while-loops). - Utilize PyTorch library functions and operations to the fullest to solve the problem. **Note:** The objective is to test your understanding of tensor dimensions via `torch.Size` and your ability to manipulate tensors based on these dimensions efficiently.","solution":"import torch def modify_tensor_based_on_dimensions(tensor: torch.Tensor) -> torch.Tensor: Modifies the tensor based on its dimensions: - If the number of dimensions is odd, return the original tensor. - If the number of dimensions is even, create a new tensor with same shape but filled with zeros. Args: tensor (torch.Tensor): Input tensor. Returns: torch.Tensor: Modified tensor. tensor_shape = tensor.size() if len(tensor_shape) % 2 != 0: return tensor else: prod_size = torch.prod(torch.tensor(tensor_shape)) new_tensor = torch.zeros(prod_size).view(tensor_shape) return new_tensor"},{"question":"You are tasked with creating a Python GUI application using the `tkinter.font` module to handle font customization. Your application should allow users to select a font family, size, weight, and style for a text display. You need to implement the following features: 1. A drop-down menu for selecting a font family from the available font families. 2. Input fields for selecting font size, and checkboxes for choosing font weight (normal/bold) and style (roman/italic). 3. A text area that displays some sample text with the selected font settings. 4. A button that applies the chosen font settings to the sample text area. # Requirements: - The application should update the font of the sample text area in real-time based on the user\'s selections. - Ensure to handle invalid or empty inputs gracefully by resetting to default font settings. # Implementation Details: - Use the `tkinter` library for the GUI elements. - Use the `tkinter.font` module for font handling. # Constraints: - Do not use any external libraries other than `tkinter` and `tkinter.font`. - Ensure the application is responsive and handles user input errors appropriately. # Example Input: - Font family: \\"Courier\\" - Font size: 14 - Font weight: \\"bold\\" - Font style: \\"italic\\" # Example Output: The text area updates to display the sample text in \\"Courier, 14pt, bold, italic.\\" # Function Signatures: You might need to use the following functions/methods in your implementation: ```python import tkinter as tk from tkinter import font # Function to get the list of available font families font.families() # Function to create a Font instance font.Font(**options) # Example function to set up the application (detailed implementation is up to you) def setup_app(): # Create and configure the main window root = tk.Tk() # Configure font settings and GUI elements ... root.mainloop() ``` You are free to design additional helper functions/classes as necessary to fulfill the requirements.","solution":"import tkinter as tk from tkinter import font def apply_font(sample_text, font_family, font_size, font_weight, font_style): try: size = int(font_size.get()) except ValueError: size = 10 # default font size font_conf = { \\"family\\": font_family.get(), \\"size\\": size, \\"weight\\": \\"bold\\" if font_weight.get() else \\"normal\\", \\"slant\\": \\"italic\\" if font_style.get() else \\"roman\\" } new_font = font.Font(**font_conf) sample_text.config(font=new_font) def setup_app(): root = tk.Tk() root.title(\\"Font Customizer\\") fonts = list(font.families()) font_family = tk.StringVar(value=fonts[0]) font_size = tk.StringVar(value=\\"10\\") font_weight = tk.BooleanVar(value=False) font_style = tk.BooleanVar(value=False) tk.Label(root, text=\\"Font Family:\\").grid(row=0, column=0) family_menu = tk.OptionMenu(root, font_family, *fonts) family_menu.grid(row=0, column=1) tk.Label(root, text=\\"Font Size:\\").grid(row=1, column=0) font_size_entry = tk.Entry(root, textvariable=font_size) font_size_entry.grid(row=1, column=1) font_weight_checkbox = tk.Checkbutton(root, text=\\"Bold\\", variable=font_weight) font_weight_checkbox.grid(row=2, column=0, columnspan=2) font_style_checkbox = tk.Checkbutton(root, text=\\"Italic\\", variable=font_style) font_style_checkbox.grid(row=3, column=0, columnspan=2) sample_text = tk.Text(root, height=5, width=30) sample_text.insert(\\"1.0\\", \\"Sample Text\\") sample_text.grid(row=4, column=0, columnspan=2) apply_button = tk.Button( root, text=\\"Apply Font\\", command=lambda: apply_font(sample_text, font_family, font_size, font_weight, font_style) ) apply_button.grid(row=5, column=0, columnspan=2) root.mainloop() if __name__ == \\"__main__\\": setup_app()"},{"question":"<|Analysis Begin|> The documentation provided details the functionalities of the \\"xml.sax.saxutils\\" module. This module contains utilities for working with SAX applications, especially related to handling and generating XML data. The specific functionalities listed include: 1. **escape(data, entities={})**: Escapes special XML characters in a string. 2. **unescape(data, entities={})**: Unescapes special XML characters in a string. 3. **quoteattr(data, entities={})**: Escapes characters in a string and wraps it in appropriate XML attribute quotes. 4. **XMLGenerator**: A class that writes SAX events to an XML document. 5. **XMLFilterBase**: A base class for XML filters. 6. **prepare_input_source(source, base=\'\')**: Prepares and resolves an input source for XML parsing. Given this, a suitable question for assessing students\' understanding of these utilities could involve writing a function that processes an XML-like string, including escaping/unescaping characters and using the XMLGenerator for producing a well-formed XML document. <|Analysis End|> <|Question Begin|> # XML Handling with xml.sax.saxutils **Objective:** Write a function that processes XML-like strings using utilities from the `xml.sax.saxutils` module. The function will take an input string containing XML data, escape specific characters, unescape them back, and generate a well-formed XML document using the `XMLGenerator` class. **Function Signature:** ```python def process_xml(input_data: str) -> str: pass ``` **Input:** - `input_data` (str): A string containing XML-like data. The input data will contain special XML characters such as `&`, `<`, and `>`, potentially mixed in with other text. **Output:** - The function should return a string containing well-formed XML data. **Requirements:** 1. Use the `escape(data, entities={})` function from `xml.sax.saxutils` to escape special XML characters in the input string. 2. Use the `unescape(data, entities={})` function to unescape these characters back. 3. Utilize the `XMLGenerator` class to take the final escaped data and generate a well-formed XML document. The document should wrap the processed data in a root element called `<root></root>`. **Constraints:** - The input string `input_data` will not exceed 1000 characters. - Assume the input string is always non-empty. **Example:** ```python input_data = \\"This is an example <string> with special & characters.\\" # Expected output: # \'<root>This is an example &lt;string&gt; with special &amp; characters.</root>\' ``` Make sure to follow these requirements closely and use appropriate error handling where necessary.","solution":"from xml.sax.saxutils import escape, unescape, XMLGenerator import io def process_xml(input_data: str) -> str: Processes the input data by escaping, unescaping and generating a well-formed XML. :param input_data: A string containing the input XML-like data. :returns: A string containing well-formed XML data. # Escaping the special XML characters escaped_data = escape(input_data) # Unescaping the XML characters back to their original form unescaped_data = unescape(escaped_data) # Creating an in-memory stream to capture the output of XMLGenerator output = io.StringIO() generator = XMLGenerator(output, encoding=\'utf-8\') # Writing the XML document generator.startDocument() generator.startElement(\\"root\\", {}) generator.characters(unescaped_data) generator.endElement(\\"root\\") generator.endDocument() # Getting the well-formed XML from the stream well_formed_xml = output.getvalue() output.close() return well_formed_xml"},{"question":"# Task: XML Document Manipulation using `xml.dom.minidom` Using the `xml.dom.minidom` package, you are to write a function called `create_employee_xml` that constructs an XML document representing a list of employees and outputs the XML string. Each employee has an ID, name, position, and department. # Function Signature: ```python def create_employee_xml(employees: list) -> str: pass ``` # Input: - `employees` (list of dict): A list of dictionaries where each dictionary represents an employee with the following keys: - `\\"id\\"` (str): The employee ID. - `\\"name\\"` (str): The employee\'s name. - `\\"position\\"` (str): The employee\'s position. - `\\"department\\"` (str): The employee\'s department. # Output: - Returns a string representing the XML of the employee data, properly formatted. # Example: ```python employees = [ {\\"id\\": \\"E001\\", \\"name\\": \\"Alice Smith\\", \\"position\\": \\"Developer\\", \\"department\\": \\"Engineering\\"}, {\\"id\\": \\"E002\\", \\"name\\": \\"Bob Johnson\\", \\"position\\": \\"Manager\\", \\"department\\": \\"Sales\\"}, ] xml_output = create_employee_xml(employees) print(xml_output) ``` **Example Output:** ```xml <employees> <employee> <id>E001</id> <name>Alice Smith</name> <position>Developer</position> <department>Engineering</department> </employee> <employee> <id>E002</id> <name>Bob Johnson</name> <position>Manager</position> <department>Sales</department> </employee> </employees> ``` # Constraints: - At least one employee will be provided in the input list. - Each dictionary in the employee list will have exactly the keys specified (\\"id\\", \\"name\\", \\"position\\", \\"department\\"). # Notes: - The generated XML should use proper indentation for readability. - The function should handle cases where special characters need to be escaped in XML. # Evaluation Criteria: - Correct construction of the XML document. - Proper usage of `xml.dom.minidom` functions. - Handling of XML formatting and escaping of special characters. - Performance and readability of the code.","solution":"from xml.dom.minidom import Document def create_employee_xml(employees): Creates an XML document representing a list of employees. Args: employees (list of dict): List of dictionaries where each dictionary represents an employee with keys \'id\', \'name\', \'position\', and \'department\'. Returns: str: A string representing the XML of the employee data, properly formatted. doc = Document() # Create a root element root = doc.createElement(\\"employees\\") doc.appendChild(root) for emp in employees: emp_element = doc.createElement(\\"employee\\") for key, value in emp.items(): child = doc.createElement(key) child.appendChild(doc.createTextNode(value)) emp_element.appendChild(child) root.appendChild(emp_element) return doc.toprettyxml(indent=\\" \\") # Example of usage employees = [ {\\"id\\": \\"E001\\", \\"name\\": \\"Alice Smith\\", \\"position\\": \\"Developer\\", \\"department\\": \\"Engineering\\"}, {\\"id\\": \\"E002\\", \\"name\\": \\"Bob Johnson\\", \\"position\\": \\"Manager\\", \\"department\\": \\"Sales\\"}, ] print(create_employee_xml(employees))"},{"question":"Using the \\"modulefinder\\" module, create a Python script that satisfies the following requirements: 1. **Input/Output Description:** - The script should accept the filename of another Python script as input. This can be achieved, for example, by taking the filename as a command-line argument or from user input. - The script should output: - A list of all modules imported by the target script. - A separate list of modules that the target script attempted to import but could not. 2. **Function Description:** - Define a function `analyze_script_imports(script_path)` that: - Takes a single argument `script_path`, which is the path to the Python script to analyze. - Returns a dictionary with two keys: - `\\"imported_modules\\"`: A list of modules successfully imported by the target script. - `\\"missing_modules\\"`: A list of modules that were attempted to be imported but were not found or failed to import. ```python import argparse from modulefinder import ModuleFinder def analyze_script_imports(script_path): finder = ModuleFinder() finder.run_script(script_path) imported_modules = list(finder.modules.keys()) missing_modules = list(finder.badmodules.keys()) return { \\"imported_modules\\": imported_modules, \\"missing_modules\\": missing_modules } if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Analyze imports of a Python script.\\") parser.add_argument(\\"script\\", help=\\"Path to the Python script to analyze\\") args = parser.parse_args() result = analyze_script_imports(args.script) print(\\"Imported modules:\\") for module in result[\\"imported_modules\\"]: print(module) print(\\"nModules attempted but not imported:\\") for module in result[\\"missing_modules\\"]: print(module) ``` **Constraints:** - Ensure error handling for cases where the script path provided does not exist or is not a valid Python script. - The solution should handle a variety of Python scripts, but assume that the target scripts do not use complex import mechanisms that `modulefinder` cannot handle. **Performance Requirements:** - The script should run efficiently but does not need to be optimized for extremely large numbers of imports, as it is intended for educational purposes. By completing this task, students will demonstrate their ability to use Pythonâ€™s `modulefinder` module, understand how to analyze and report on script imports, handle user input for file paths, and perform basic error handling.","solution":"import argparse from modulefinder import ModuleFinder def analyze_script_imports(script_path): Analyze which modules a Python script imports and which imports fail. Args: - script_path (str): Path to the Python script to analyze. Returns: - dict: A dictionary with two keys; \\"imported_modules\\" lists successfully imported modules, and \\"missing_modules\\" lists modules that failed to import. finder = ModuleFinder() try: finder.run_script(script_path) except Exception as e: return {\\"imported_modules\\": [], \\"missing_modules\\": [\\"Error: \\" + str(e)]} imported_modules = list(finder.modules.keys()) missing_modules = list(finder.badmodules.keys()) return { \\"imported_modules\\": imported_modules, \\"missing_modules\\": missing_modules } if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Analyze imports of a Python script.\\") parser.add_argument(\\"script\\", type=str, help=\\"Path to the Python script to analyze\\") args = parser.parse_args() result = analyze_script_imports(args.script) print(\\"Imported modules:\\") for module in result[\\"imported_modules\\"]: print(module) print(\\"nModules attempted but not imported:\\") for module in result[\\"missing_modules\\"]: print(module)"},{"question":"<|Analysis Begin|> The provided documentation is a comprehensive guide to the Python logging module. It covers basic to advanced logging concepts, ranging from simple logging calls to configuring multiple loggers, handlers, formatters, and filters. The document also includes examples that demonstrate logging to consoles and files, adjusting log levels, and formatting log messages with timestamps. Key points from the documentation include: 1. **Logging Levels and When to Use Them**: Details about `DEBUG`, `INFO`, `WARNING`, `ERROR`, and `CRITICAL` levels. 2. **Configuring Logging**: Instructions and examples for basic and advanced configurations using `basicConfig`, `fileConfig`, and `dictConfig`. 3. **Components of Logging**: Description of loggers, handlers, formatters, and filters and how they interact. 4. **Logging Handlers**: Several handlers such as `StreamHandler`, `FileHandler`, `RotatingFileHandler`, and others that help in directing the log outputs to various destinations. 5. **Logging with Multiple Modules**: Guidance on how to use logging when developing larger applications with multiple modules. 6. **Advanced Features**: Including exception handling within logging, custom logging levels, optimization considerations, and more. Given the thoroughness of the documentation, we can design a question that not only tests the basic understanding of logging usage but also incorporates more advanced configurations and customization aspects. <|Analysis End|> <|Question Begin|> You are developing a Python application that will run various data processing tasks. The application is expected to log events and errors for monitoring and debugging purposes. Use the logging module in Python to implement the following functionality: # Task 1. **Set Up Logging**: - Configure a logger named `data_processor` that logs messages to a file named `data_processor.log` and to the console. - The log file should store logs in the following format: `YYYY-MM-DD HH:MM:SS,sss - LOGGER - LEVEL - MESSAGE`. - Both the console output and the log file should include logs of level `INFO` and above. 2. **Log Variable Data**: - Create a function `process_data(data)` that simulates processing some data. The function should: - Log the received data at the `INFO` level. - Log a debug message (should not be displayed but included for completeness). - Log an error message if the data contains the word \\"error\\". - Log a critical message if the data contains the word \\"critical\\". 3. **Logging Configuration File**: - Instead of hardcoding the logging configuration in your script, use a configuration file (`logging.conf`) to set up the loggers, handlers, and formatters as specified. # Input - You can assume a string `data` will be passed to the `process_data` function. # Output - No direct output is expected from the script. However, logs should be written to the console and the log file as per the specified configurations. # Example ```python import logging import logging.config # Implementation of the task requirements logging.config.fileConfig(\'logging.conf\') logger = logging.getLogger(\'data_processor\') def process_data(data): logger.info(f\'Received data: {data}\') logger.debug(\'This is a debug message\') if \'error\' in data: logger.error(\'Data contains an error\') if \'critical\' in data: logger.critical(\'Data contains a critical issue\') # Example usage if __name__ == \\"__main__\\": process_data(\\"This is a test data with error\\") process_data(\\"This is another test data with critical issue\\") ``` # Configuration File (`logging.conf`) ```plaintext [loggers] keys=root,data_processor [handlers] keys=consoleHandler,fileHandler [formatters] keys=fileFormatter,consoleFormatter [logger_root] level=WARNING handlers=consoleHandler [logger_data_processor] level=INFO handlers=consoleHandler,fileHandler qualname=data_processor propagate=0 [handler_consoleHandler] class=StreamHandler level=INFO formatter=consoleFormatter args=(sys.stdout,) [handler_fileHandler] class=FileHandler level=INFO formatter=fileFormatter args=(\'data_processor.log\', \'a\') [formatter_consoleFormatter] format=%(asctime)s - %(name)s - %(levelname)s - %(message)s [formatter_fileFormatter] format=%(asctime)s - %(name)s - %(levelname)s - %(message)s datefmt=%Y-%m-%d %H:%M:%S,%f ``` Make sure your implementation and logging configuration follow the above requirements and structure. Adjust the date format for milliseconds and other components as necessary.","solution":"import logging import logging.config # Logging configuration file (logging.conf) content as a string for easy reference logging_conf = [loggers] keys=root,data_processor [handlers] keys=consoleHandler,fileHandler [formatters] keys=fileFormatter,consoleFormatter [logger_root] level=WARNING handlers=consoleHandler [logger_data_processor] level=INFO handlers=consoleHandler,fileHandler qualname=data_processor propagate=0 [handler_consoleHandler] class=StreamHandler level=INFO formatter=consoleFormatter args=(sys.stdout,) [handler_fileHandler] class=FileHandler level=INFO formatter=fileFormatter args=(\'data_processor.log\', \'a\') [formatter_consoleFormatter] format=%(asctime)s - %(name)s - %(levelname)s - %(message)s [formatter_fileFormatter] format=%(asctime)s - %(name)s - %(levelname)s - %(message)s datefmt=%Y-%m-%d %H:%M:%S,%f # Write the logging.conf content to a file with open(\'logging.conf\', \'w\') as f: f.write(logging_conf) # Load configuration from the file logging.config.fileConfig(\'logging.conf\') # Get the logger logger = logging.getLogger(\'data_processor\') def process_data(data): Simulate processing some data and log messages based on content. Parameters: data (str): The data to be processed logger.info(f\'Received data: {data}\') logger.debug(\'This is a debug message\') if \'error\' in data: logger.error(\'Data contains an error\') if \'critical\' in data: logger.critical(\'Data contains a critical issue\') # Example usage if __name__ == \\"__main__\\": process_data(\\"This is a test data with error\\") process_data(\\"This is another test data with critical issue\\")"},{"question":"Objective Demonstrate your understanding of seaborn\'s `FacetGrid` by creating and customizing a multi-plot grid from the seaborn `tips` dataset. Question Given the tips dataset from seaborn, create a grid of plots to visualize the relationship between `total_bill` and `tip` across different days of the week (`day`). Further, differentiate the data points by the time of day (`time`) and the sex of the individual (`sex`). **Input:** 1. Load the `tips` dataset from seaborn. 2. Construct a `FacetGrid` such that: - It creates subplots for each day. - Columns represent different times of day (`Lunch` or `Dinner`). - Rows differentiate between genders (`Male` and `Female`). 3. On each facet, plot a scatter plot of `total_bill` versus `tip`. 4. Add a legend to differentiate male and female plots. **Constraints:** - Ensure that the visual representation is clear and aesthetics are well-adjusted. - Set custom axis labels for `total_bill` and `tip`. - Use a green background color for the subplot corresponding to `Dinner` time and `Female` sex. **Output:** - An image file saving the complete `FacetGrid` plot. Here is the template you should complete: ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Initialize FacetGrid g = sns.FacetGrid(tips, col=\\"time\\", row=\\"sex\\", hue=\\"sex\\", margin_titles=True) # Map scatter plot to the grid g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") # Add legend g.add_legend() # Customize axis labels g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") # Apply a color for a specific subplot for (row_val, col_val), ax in g.axes_dict.items(): if row_val == \\"Female\\" and col_val == \\"Dinner\\": ax.set_facecolor(\\".85\\") # Set to green as per specific requirement # Adjust layout and save the plot g.tight_layout() g.savefig(\\"facet_grid_plot.png\\") # Ensure the plot is displayed in notebook plt.show() ``` Complete the code and ensure it runs correctly, producing the required visual representation.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_facet_grid_plot(): # Load dataset tips = sns.load_dataset(\\"tips\\") # Initialize FacetGrid g = sns.FacetGrid(tips, col=\\"time\\", row=\\"sex\\", margin_titles=True) # Map scatter plot to the grid g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") # Add legend g.add_legend() # Customize axis labels g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") # Apply a green color for specific subplot corresponding to Dinner and Female for (row_val, col_val), ax in g.axes_dict.items(): if row_val == \\"Female\\" and col_val == \\"Dinner\\": ax.set_facecolor(\\"lightgreen\\") # Adjust layout and save the plot g.tight_layout() g.savefig(\\"facet_grid_plot.png\\") plt.show()"},{"question":"# Question: Custom HLS Palette Visualization You are given a dataset containing information about different species of flowers. Using seaborn, you are required to create a visual representation that distinguishes these species using a custom HLS color palette. **Instructions:** 1. Load the built-in `iris` dataset from seaborn. 2. Create an HLS palette with the following customizations: - Number of colors should be equal to the number of unique species in the dataset. - Lightness should be set to `0.5`. - Saturation should be `0.7`. 3. Create a scatter plot using seaborn, plotting `sepal_length` against `sepal_width`, and color the points based on species using the created palette. 4. Return the scatter plot. **Constraints:** - Do not use any other palette or colormap functions except `sns.hls_palette`. - Ensure the plot has a title and labeled axes for better readability. - The code should be efficient and avoid unnecessary computations. **Expected Input and Output Format:** - Input: None (The function should internally load the `iris` dataset). - Output: A scatter plot visualizing `sepal_length` vs. `sepal_width` of the iris species, colored by species using the custom HLS palette. **Performance Requirements:** - The code should execute within a reasonable time frame (less than 2 seconds for loading the dataset and generating the plot). **Function Signature:** ```python def custom_hls_palette_scatter() -> None: pass ``` **Example:** ```python custom_hls_palette_scatter() ``` ![Example Scatter Plot](example_scatter_plot.png) **Hints:** - Use `seaborn` to load the data and create the plots. - Use the `hls_palette` function to generate the required palette.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_hls_palette_scatter(): Create a scatter plot of \'sepal_length\' vs \'sepal_width\' from the Iris dataset, colored by species using a custom HLS palette. # Load the built-in Iris dataset from seaborn iris = sns.load_dataset(\'iris\') # Get the unique species species_unique = iris[\'species\'].unique() # Create a custom HLS palette palette = sns.hls_palette(n_colors=len(species_unique), l=0.5, s=0.7) # Create the scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette) scatter_plot.set_title(\'Sepal Length vs Sepal Width\') scatter_plot.set_xlabel(\'Sepal Length\') scatter_plot.set_ylabel(\'Sepal Width\') plt.legend(title=\'Species\') plt.show()"},{"question":"Problem Statement You are tasked with creating a Python script that reads a configuration file and a CSV file, processes the data, and writes the results to a new CSV file based on specifications from the configuration file. The configuration file (`config.ini`) will specify the input and output CSV filenames, as well as some processing parameters. Configuration File (`config.ini`) The `config.ini` is an INI file with the following structure: ```ini [FILES] input_csv = input.csv output_csv = output.csv [PARAMETERS] uppercase = true filter_column = 2 filter_value = abc ``` - **input_csv**: The filename of the input CSV file. - **output_csv**: The filename of the output CSV file. - **uppercase**: A boolean that specifies whether to convert all text to uppercase. - **filter_column**: The column number (1-indexed) to apply a filter on. - **filter_value**: The value to filter rows by in the specified column. Input CSV (`input.csv`) An example input CSV might look like this: ```csv name,age,city Alice,30,New York Bob,25,Los Angeles Charlie,35,abc David,40,San Francisco Eve,50,abc ``` Output CSV (`output.csv`) The output CSV should be filtered and optionally processed according to `config.ini`. Based on the example `config.ini` and `input.csv` above, the resulting CSV when `uppercase` is set to `true` should look like: ```csv NAME,AGE,CITY CHARLIE,35,ABC EVE,50,ABC ``` Task Write a function `process_csv(config_file: str) -> None` that performs the following steps: 1. Parse the `config.ini` file to read the input and output CSV filenames, and the processing parameters. 2. Read the input CSV file. 3. Apply the filter on the specified column. 4. Convert all text to uppercase if specified. 5. Write the results to the output CSV file. Function Signature ```python def process_csv(config_file: str) -> None: pass ``` Constraints and Assumptions - The input CSV file can have any number of rows and columns. - The configuration file and CSV files are assumed to be correctly formatted. - You may use the `configparser` and `csv` modules from the Python standard library. Example Usage ```python # Assuming the files are present in the same directory process_csv(\'config.ini\') ``` This should read `input.csv` and `config.ini`, process the data as specified, and write the output to `output.csv`. Evaluation Criteria - Correctness of parsing the configuration and CSV files. - Accuracy of data filtering and processing. - Proper handling of uppercase conversion. - Correct output as per the configuration settings.","solution":"import configparser import csv def process_csv(config_file: str) -> None: Reads a configuration file and a CSV file, processes the data according to the configuration, and writes the results to a new CSV file. # Parse the config file config = configparser.ConfigParser() config.read(config_file) input_csv = config[\'FILES\'][\'input_csv\'] output_csv = config[\'FILES\'][\'output_csv\'] uppercase = config.getboolean(\'PARAMETERS\', \'uppercase\') filter_column = config.getint(\'PARAMETERS\', \'filter_column\') - 1 filter_value = config[\'PARAMETERS\'][\'filter_value\'] # Read the input CSV file with open(input_csv, mode=\'r\', newline=\'\') as infile: reader = csv.reader(infile) headers = next(reader) rows = [row for row in reader if row[filter_column] == filter_value] if uppercase: headers = [header.upper() for header in headers] rows = [[cell.upper() for cell in row] for row in rows] # Write the output CSV file with open(output_csv, mode=\'w\', newline=\'\') as outfile: writer = csv.writer(outfile) writer.writerow(headers) writer.writerows(rows)"},{"question":"Objective: Using Python\'s `atexit` module, demonstrate your understanding of exit handlers by implementing a program that handles multiple cleanup tasks which must be executed in a specific order when the program terminates. Problem Statement: You are to design a program that simulates a checkout process in an online store. The program should maintain a log of items added to a cart and save the cart\'s state to a file upon normal program termination. Additionally, the program should send a thank-you message and perform some other tasks at exit. Requirements: 1. **Implement Functions:** - `add_item_to_cart(item)`: Adds an item to a global cart list. - `save_cart()`: Writes the contents of the cart to a file named `cart.txt`. - `send_thank_you_message(name)`: Prints a thank-you message to the customer. - `performance_metric()`: Prints a performance metric message. 2. **Register Exit Handlers:** - Ensure `save_cart` is registered to run first. - Then register `send_thank_you_message` to run second. - Lastly, register `performance_metric` as the last function to run. 3. **Completion:** - Your program should register these functions appropriately so that they are called in the order specified when the program terminates. 4. **Unregistration:** - Ensure that the function `send_thank_you_message` can be unregistered from the exit handlers if a condition (like user decision) is met. Input and Output: - **add_item_to_cart(item)**: - Input: `item` (string): The name of the item to be added to the cart. - Output: None - **save_cart()**: - Input: None - Output: None - Side-effect: The `cart.txt` file is created or updated with the cart\'s contents. - **send_thank_you_message(name)**: - Input: `name` (string): The name of the customer. - Output: Prints a thank-you message. - **performance_metric()**: - Input: None - Output: Prints a performance metric message. Constraints: 1. The cart should be maintained as a list of strings. 2. Ensure that file operations and message prints reflect the registered order upon termination. 3. Provide a mechanism for unregistering `send_thank_you_message`. Example Usage: ```python add_item_to_cart(\'Laptop\') add_item_to_cart(\'Headphones\') # condition example to unregister `send_thank_you_message` user_decision_to_unsubscribe = True if user_decision_to_unsubscribe: atexit.unregister(send_thank_you_message) # Register functions in the specified order atexit.register(save_cart) atexit.register(send_thank_you_message, \'Customer Name\') atexit.register(performance_metric) # Normal program logic here (e.g., allowing more items to be added) ``` Your task is to complete the implementation of this program adhering to the requirements and demonstrating the usage of the `atexit` module effectively.","solution":"import atexit cart = [] def add_item_to_cart(item): Adds an item to the global cart list. cart.append(item) def save_cart(): Writes the contents of the cart to a file named cart.txt. with open(\'cart.txt\', \'w\') as file: for item in cart: file.write(f\\"{item}n\\") def send_thank_you_message(name): Prints a thank-you message to the customer. print(f\\"Thank you for shopping with us, {name}!\\") def performance_metric(): Prints a performance metric message. print(\\"Performance metrics collected and saved.\\") # Register functions in the specified order atexit.register(save_cart) atexit.store_thank_you_message = atexit.register(send_thank_you_message, \'Customer Name\') atexit.register(performance_metric) # Example: Condition to unregister send_thank_you_message user_decision_to_unsubscribe = True if user_decision_to_unsubscribe: atexit.unregister(atexit.store_thank_you_message)"},{"question":"# **Question: Advanced Nearest Neighbors Classification and Analysis** Problem Statement: You are tasked with implementing a `NearestNeighborsClassifierAnalyzer` class that performs nearest neighbor classification using both `KNeighborsClassifier` and `RadiusNeighborsClassifier`. This class should also be able to evaluate model performance and visualize the classification boundaries for a given dataset. Specifications: 1. **Initialization Parameters**: - `n_neighbors`: Integer, the number of neighbors to use for `KNeighborsClassifier`. - `radius`: Float, the radius for `RadiusNeighborsClassifier`. - `algorithm`: String, one of `[\'auto\', \'ball_tree\', \'kd_tree\', \'brute\']`. - `weights`: String, one of `[\'uniform\', \'distance\']`, default is `uniform`. 2. **Methods**: - `fit(X, y)`: Fit both `KNeighborsClassifier` and `RadiusNeighborsClassifier` using the provided training data `X` and labels `y`. - `predict(X, classifier_type=\'kneighbors\')`: Predict labels for the provided data `X` using the specified classifier type (`\'kneighbors\'` for `KNeighborsClassifier` and `\'radius\'` for `RadiusNeighborsClassifier`). - `evaluate(X, y, classifier_type=\'kneighbors\')`: Calculate and return the accuracy score for the specified classifier type. - `visualize_decision_boundary(X, y, classifier_type=\'kneighbors\')`: Visualize the decision boundary for the specified classifier type. 3. **Input and Output Formats**: **Input**: - `X`: A 2D numpy array of shape `(n_samples, n_features)` containing the feature vectors. - `y`: A 1D numpy array of shape `(n_samples,)` containing the target labels. - `classifier_type`: A string indicating which classifier to use (`\'kneighbors\'` or `\'radius\'`). **Output**: - Predictions from the `predict` method. - Accuracy score from the `evaluate` method. - A decision boundary plot from the `visualize_decision_boundary` method. Constraints: - Assume the data provided is suitable for nearest neighbor classification (i.e., minimum number of samples and appropriate dimensionality). Additional Information: - You may use any visualization library like `matplotlib` to generate the decision boundary plots. - Your class should handle edge cases like missing values or incompatible shapes of `X` and `y`. Example Usage: ```python import numpy as np # Sample dataset X = np.array([[1, 2], [2, 3], [3, 4], [5, 6], [7, 8]]) y = np.array([0, 0, 1, 1, 0]) # Instantiate the analyzer analyzer = NearestNeighborsClassifierAnalyzer(n_neighbors=3, radius=1.0, algorithm=\'auto\', weights=\'uniform\') # Fit the models analyzer.fit(X, y) # Predict using KNeighborsClassifier knn_predictions = analyzer.predict(X, classifier_type=\'kneighbors\') # Evaluate KNeighborsClassifier knn_accuracy = analyzer.evaluate(X, y, classifier_type=\'kneighbors\') # Visualize decision boundary for KNeighborsClassifier analyzer.visualize_decision_boundary(X, y, classifier_type=\'kneighbors\') # Predict using RadiusNeighborsClassifier radius_predictions = analyzer.predict(X, classifier_type=\'radius\') # Evaluate RadiusNeighborsClassifier radius_accuracy = analyzer.evaluate(X, y, classifier_type=\'radius\') # Visualize decision boundary for RadiusNeighborsClassifier analyzer.visualize_decision_boundary(X, y, classifier_type=\'radius\') ``` Note: Please ensure that your implementation is efficient and leverages the appropriate methods from `sklearn.neighbors`.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier from sklearn.metrics import accuracy_score class NearestNeighborsClassifierAnalyzer: def __init__(self, n_neighbors, radius, algorithm=\'auto\', weights=\'uniform\'): self.n_neighbors = n_neighbors self.radius = radius self.algorithm = algorithm self.weights = weights self.knn = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=algorithm, weights=weights) self.radius_knn = RadiusNeighborsClassifier(radius=radius, algorithm=algorithm, weights=weights) def fit(self, X, y): self.knn.fit(X, y) self.radius_knn.fit(X, y) def predict(self, X, classifier_type=\'kneighbors\'): if classifier_type == \'kneighbors\': return self.knn.predict(X) elif classifier_type == \'radius\': return self.radius_knn.predict(X) else: raise ValueError(\\"classifier_type must be either \'kneighbors\' or \'radius\'\\") def evaluate(self, X, y, classifier_type=\'kneighbors\'): if classifier_type == \'kneighbors\': predictions = self.knn.predict(X) elif classifier_type == \'radius\': predictions = self.radius_knn.predict(X) else: raise ValueError(\\"classifier_type must be either \'kneighbors\' or \'radius\'\\") return accuracy_score(y, predictions) def visualize_decision_boundary(self, X, y, classifier_type=\'kneighbors\'): h = .02 # step size in the mesh # Create color maps cmap_light = ListedColormap([\'#FFAAAA\', \'#AAFFAA\', \'#AAAAFF\']) cmap_bold = ListedColormap([\'#FF0000\', \'#00FF00\', \'#0000FF\']) # Fit the data self.fit(X, y) # Create mesh grid x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) # Predict using the specified classifier if classifier_type == \'kneighbors\': Z = self.knn.predict(np.c_[xx.ravel(), yy.ravel()]) elif classifier_type == \'radius\': Z = self.radius_knn.predict(np.c_[xx.ravel(), yy.ravel()]) else: raise ValueError(\\"classifier_type must be either \'kneighbors\' or \'radius\'\\") Z = Z.reshape(xx.shape) plt.figure() plt.pcolormesh(xx, yy, Z, cmap=cmap_light) # Plot also the training points plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\'k\', s=20) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.title(f\\"{classifier_type} classification (k = {self.n_neighbors})\\") plt.show()"},{"question":"**Objective**: Demonstrate understanding and application of CUDA functionalities in PyTorch using `torch.cuda`. **Question**: You are given a machine with multiple CUDA GPUs, and you are required to perform the following tasks: 1. Query the number of CUDA devices available. 2. For each device, gather and display the following properties: - Device name. - Total memory. - Memory allocated. - Memory reserved. - Utilization. 3. Allocate a tensor on the first device, perform a simple computation (e.g., element-wise multiplication of the tensor), and then release the memory. 4. Measure and display the time taken for these operations using CUDA events. 5. Implement a function `cuda_device_info()` which: - Takes no input. - Returns a dictionary where the keys are device indices and values are another dictionary of the properties mentioned in step 2. 6. Implement a function `cuda_tensor_operations()` which: - Takes no input. - Performs tensor allocation, computation, and deallocation. - Uses CUDA events to measure the time taken for the operation and prints it. Provide the complete implementation for the following structure: ```python import torch def cuda_device_info(): Return a dictionary of CUDA device properties. # Your code here def cuda_tensor_operations(): Allocate a tensor, perform computation, deallocate memory, and print the time taken for these operations. # Your code here if __name__ == \\"__main__\\": print(cuda_device_info()) cuda_tensor_operations() ``` # Constraints: - Ensure that the functions handle the presence of no CUDA devices gracefully. - Use appropriate CUDA memory management functions to avoid memory leaks. - Ensure that operations and memory measurements are accurate. **Expected Output**: - A dictionary output from `cuda_device_info()` with the device properties. - Print statements within `cuda_tensor_operations()` showing the steps and the time taken for the operations. # Assumptions: - CUDA is installed and properly configured on the machine. - The latest version of PyTorch is installed. - The GPU supports necessary operations (CUDA Compute Capability). This question will assess students\' ability to: - Use CUDA for device management and operations in PyTorch. - Correctly manipulate and measure time for CUDA operations. - Implement reliable and efficient CUDA-related code.","solution":"import torch def cuda_device_info(): Return a dictionary of CUDA device properties. if not torch.cuda.is_available(): return {\\"error\\": \\"CUDA is not available on this system\\"} device_info = {} num_devices = torch.cuda.device_count() for i in range(num_devices): info = {} device = torch.cuda.get_device_properties(i) info[\\"Device name\\"] = device.name info[\\"Total memory\\"] = device.total_memory info[\\"Memory allocated\\"] = torch.cuda.memory_allocated(i) info[\\"Memory reserved\\"] = torch.cuda.memory_reserved(i) # Dummy value for Utilization as it cannot be directly fetched with torch.cuda info[\\"Utilization\\"] = \\"N/A\\" device_info[i] = info return device_info def cuda_tensor_operations(): Allocate a tensor, perform computation, deallocate memory, and print the time taken for these operations. if not torch.cuda.is_available(): print(\\"CUDA is not available on this system\\") return device = torch.device(\'cuda:0\') # Define CUDA events to measure time start_event = torch.cuda.Event(enable_timing=True) end_event = torch.cuda.Event(enable_timing=True) # Allocate a tensor on the first device tensor = torch.randn((1000, 1000), device=device) # Record the start event start_event.record() # Perform a simple computation: element-wise multiplication result = tensor * tensor # Record the end event end_event.record() # Wait for all operations to finish torch.cuda.synchronize() # Compute the elapsed time elapsed_time = start_event.elapsed_time(end_event) print(f\\"Time taken for tensor allocation and computation: {elapsed_time} ms\\") print(f\\"Memory allocated on device: {torch.cuda.memory_allocated(device)} bytes\\") # Deallocate tensor del tensor torch.cuda.empty_cache() if __name__ == \\"__main__\\": print(cuda_device_info()) cuda_tensor_operations()"},{"question":"Using the `urllib.request` module in Python, write a function `fetch_url_content` that takes a URL and an optional dictionary of headers as input and returns a tuple containing the following: 1. The HTTP status code returned by the server. 2. The content of the response decoded as a UTF-8 string. 3. The response headers as a dictionary. The function should: - Handle redirects automatically. - Include custom headers if provided. - Handle HTTP errors by returning the status code and an appropriate error message in place of the content. - Use appropriate handlers to support HTTP and HTTPS URLs. You are also provided with an additional constraint: If the size of the content returned exceeds 1000 bytes, return only the first 1000 bytes of the content. # Function Signature ```python from typing import Tuple, Dict, Optional def fetch_url_content(url: str, headers: Optional[Dict[str, str]] = None) -> Tuple[int, str, Dict[str, str]]: pass ``` # Input - `url` (str): The URL to be fetched. - `headers` (Optional[Dict[str, str]]): An optional dictionary containing headers to be included in the request. # Output - A tuple containing: 1. The HTTP status code (int). 2. The response content limited to 1000 bytes as a UTF-8 decoded string (str). 3. The response headers as a dictionary (Dict[str, str]). # Constraints - The module should handle HTTP and HTTPS requests. - The function should handle automatic redirection. - The function should handle HTTP errors gracefully and return the status code with an appropriate error message. # Example ```python url = \\"http://www.example.com\\" headers = {\\"User-Agent\\": \\"Mozilla/5.0\\"} status_code, content, response_headers = fetch_url_content(url, headers) print(status_code) # Example Output: 200 print(content[:100]) # Example Output: \\"<!doctype html>n<html>n<head>...</head>n<body>...\\" print(response_headers) # Example Output: {\'Content-Type\': \'text/html; charset=UTF-8\', ...} ``` # Notes - Make sure to handle the situation where the content might not be decodable to UTF-8 gracefully. - Use appropriate urllib.request classes and methods to achieve the desired functionality. - Include logic to limit the response content to 1000 bytes if it exceeds this limit.","solution":"import urllib.request from urllib.error import HTTPError, URLError from typing import Tuple, Dict, Optional def fetch_url_content(url: str, headers: Optional[Dict[str, str]] = None) -> Tuple[int, str, Dict[str, str]]: request = urllib.request.Request(url) if headers: for key, value in headers.items(): request.add_header(key, value) try: with urllib.request.urlopen(request) as response: status_code = response.getcode() content = response.read() if len(content) > 1000: content = content[:1000] content = content.decode(\'utf-8\', \'ignore\') response_headers = dict(response.getheaders()) return (status_code, content, response_headers) except HTTPError as e: return (e.code, f\\"HTTP Error: {e.reason}\\", {}) except URLError as e: return (0, f\\"URL Error: {e.reason}\\", {})"},{"question":"You are given a dataset of your choice that contains multiple numeric and categorical variables. Your task is to write a Python function that demonstrates the use of Seaborn for visualizing the distributions of the dataset. Specifically, you should: 1. Create a histogram for a chosen numeric variable and adjust the bin size. 2. Create a KDE plot for the same numeric variable and adjust the bandwidth. 3. Create a joint plot to visualize the relationship between two numeric variables, including their marginal distributions. 4. Create a pairplot for the dataset that includes both numeric and categorical variables. 5. Create an ECDF plot for the chosen numeric variable. 6. Use at least one categorical variable for conditioning in one of the plots. The function should be generalizable to any given DataFrame input. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_distributions(df): Visualizes distributions of the provided DataFrame using Seaborn. Parameters: df (pd.DataFrame): The dataset to visualize distributions for. Returns: None # Ensure to include customizing binwidth for histograms, bandwidth for KDE plots # and using hue for conditional subsetting. # Part 1: Histogram with adjusted bin size plt.figure(figsize=(10, 6)) sns.histplot(df, x=\'numeric_variable\', binwidth=5) plt.show() # Part 2: KDE plot with adjusted bandwidth plt.figure(figsize=(10, 6)) sns.kdeplot(df[\'numeric_variable\'], bw_adjust=0.5) plt.show() # Part 3: Joint plot with marginal distributions plt.figure(figsize=(10, 6)) sns.jointplot(data=df, x=\'numeric_variable_1\', y=\'numeric_variable_2\', kind=\'kde\') plt.show() # Part 4: Pairplot including both numeric and categorical variables plt.figure(figsize=(10, 6)) sns.pairplot(df, hue=\'categorical_variable\') plt.show() # Part 5: ECDF plot plt.figure(figsize=(10, 6)) sns.ecdfplot(data=df, x=\'numeric_variable\') plt.show() # Include other conditional subsetting like using hue to differentiate subsets ``` # Input - `df`: A Pandas DataFrame containing multiple numeric and categorical variables. # Constraints - The DataFrame must contain at least one numeric column. - The DataFrame should not be empty. # Output - The function should output several Seaborn plots as specified above. # Example Usage ```python import seaborn as sns # Load a sample dataset penguins = sns.load_dataset(\\"penguins\\") # Call the function to visualize distributions visualize_distributions(penguins) ``` Ensure that your function demonstrates clear and insightful visualizations that can help understand the distribution and relationships within the dataset.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_distributions(df): Visualizes distributions of the provided DataFrame using Seaborn. Parameters: df (pd.DataFrame): The dataset to visualize distributions for. Returns: None numeric_col = df.select_dtypes(include=[\'float64\', \'int64\']).columns categorical_col = df.select_dtypes(include=[\'object\', \'category\']).columns if len(numeric_col) < 1: raise ValueError(\\"The DataFrame must contain at least one numeric column.\\") # Part 1: Histogram with adjusted bin size plt.figure(figsize=(10, 6)) sns.histplot(df, x=numeric_col[0], binwidth=5) plt.title(f\'Histogram of {numeric_col[0]}\') plt.show() # Part 2: KDE plot with adjusted bandwidth plt.figure(figsize=(10, 6)) sns.kdeplot(df[numeric_col[0]], bw_adjust=0.5) plt.title(f\'KDE plot of {numeric_col[0]}\') plt.show() # Part 3: Joint plot with marginal distributions if len(numeric_col) > 1: sns.jointplot(data=df, x=numeric_col[0], y=numeric_col[1], kind=\'kde\') plt.title(f\'Joint plot of {numeric_col[0]} and {numeric_col[1]}\') plt.show() # Part 4: Pairplot including both numeric and categorical variables sns.pairplot(df, hue=categorical_col[0] if len(categorical_col) > 0 else None) plt.title(\'Pairplot of dataset\') plt.show() # Part 5: ECDF plot plt.figure(figsize=(10, 6)) sns.ecdfplot(data=df, x=numeric_col[0]) plt.title(f\'ECDF plot of {numeric_col[0]}\') plt.show()"},{"question":"# Tensor Dimension Operations with `torch.Size` Objective: You are required to write a function that creates a tensor, and based on its size, performs and returns specific operations on this tensor. Function Signature: ```python import torch def tensor_operations(shape: tuple) -> dict: # Your code here pass ``` Input: - `shape` (tuple): A tuple representing the dimensions of the tensor to be created. For example, `(10, 20, 30)`. Output: - A dictionary with the following keys and their respective values: - `\'size\'`: The size of the created tensor as a `torch.Size` object. - `\'dim_count\'`: The number of dimensions in the tensor. - `\'sum_all\'`: The sum of all elements in the tensor. - `\'mean_first_dim\'`: The mean of the elements along the first dimension. - `\'max_value\'`: The maximum value in the tensor. Constraints: - Assume the elements in the tensor follow a uniform distribution between 0 and 1. - Handle tensor shapes that contain up to 4 dimensions. Example: ```python # Given shape (2, 3, 4) tensor_operations((2, 3, 4)) # Expected output (values will vary because of random initialization, so below is just a sample output structure): { \'size\': torch.Size([2, 3, 4]), \'dim_count\': 3, \'sum_all\': 12.0, # This value will vary \'mean_first_dim\': torch.tensor([...]), # This tensor\'s values will vary \'max_value\': 0.9999 # This value will vary } ``` Notes: 1. You should use the `torch.rand` method to initialize the tensor with random values between 0 and 1. 2. Ensure that your function correctly computes each of the required keys in the output dictionary based on the properties and operations of the tensor. 3. When computing the mean along the first dimension, make sure to return it as a PyTorch tensor.","solution":"import torch def tensor_operations(shape: tuple) -> dict: Creates a tensor of the given shape and calculates various metrics. Args: shape (tuple): A tuple representing the tensor\'s dimensions. Returns: dict: A dictionary with the tensor\'s size, dimension count, sum of all elements, mean of the first dimension, and maximum value in the tensor. # Create a tensor with random values between 0 and 1 tensor = torch.rand(shape) # Calculate the required values size = tensor.size() dim_count = tensor.dim() sum_all = tensor.sum().item() mean_first_dim = tensor.mean(dim=0) max_value = tensor.max().item() return { \'size\': size, \'dim_count\': dim_count, \'sum_all\': sum_all, \'mean_first_dim\': mean_first_dim, \'max_value\': max_value }"},{"question":"Objective Implement a Python function that reads an existing INI configuration file, applies updates to certain sections and keys specified in a dictionary, and writes the updated configuration back to a new file. The function should also handle interpolations within the configuration values. Function Signature ```python from typing import Dict import configparser def update_config_file(input_file: str, updates: Dict[str, Dict[str, str]], output_file: str) -> None: pass ``` Function Parameters - `input_file` (str): The path to the existing INI configuration file that needs to be read. - `updates` (Dict[str, Dict[str, str]]): A dictionary where the keys are section names and the values are dictionaries. Each inner dictionary contains key-value pairs representing the configuration options to be updated. - `output_file` (str): The path where the updated INI configuration file should be saved. Constraints - If a section or a key does not exist in the original configuration file, they should be created. - If a section key is part of an interpolation, ensure the interpolation is resolved correctly after updates. - The function should handle cases where keys and values may span multiple lines. - Preserve the original formatting whenever possible, especially for comments and multiline values. Example Assume we have an input file `settings.ini` with the following content: ```ini [DEFAULT] base_url = http://example.com [server] port = 8080 [database] host = %(base_url)s name = example_db ``` If `updates` is: ```python { \'server\': { \'port\': \'9090\' }, \'database\': { \'password\': \'secure_password\' }, \'new_section\': { \'new_key\': \'new_value\' } } ``` The output file `updated_settings.ini` should look like: ```ini [DEFAULT] base_url = http://example.com [server] port = 9090 [database] host = http://example.com name = example_db password = secure_password [new_section] new_key = new_value ``` Instructions 1. Implement the `update_config_file` function. 2. Ensure that the function handles file operations gracefully and raises appropriate exceptions if any file operation fails. 3. Include docstring comments to illustrate the basic implementation steps.","solution":"from typing import Dict import configparser def update_config_file(input_file: str, updates: Dict[str, Dict[str, str]], output_file: str) -> None: Reads an INI configuration file, updates its sections and keys based on the provided updates dictionary, and writes the updated configuration to a new file. :param input_file: Path to the original INI configuration file. :param updates: Dictionary with section names as keys and dictionaries as values, containing key-value pairs to update. :param output_file: Path to save the updated INI configuration file. # Create a ConfigParser instance for reading and writing the configuration file config = configparser.ConfigParser() # Read the existing configuration file config.read(input_file) # Apply the updates from the dictionary for section, values in updates.items(): if not config.has_section(section) and section != \\"DEFAULT\\": config.add_section(section) for key, value in values.items(): config.set(section, key, value) # Write the updated configuration back to a new file with open(output_file, \'w\') as configfile: config.write(configfile)"},{"question":"# Question: Manipulating Tensors and Storages in PyTorch You are provided with a pre-trained model that processes batches of data and outputs predictions. In this assignment, you are tasked with writing a function that manipulates storages at a low level to achieve specific tasks. The function should: 1. Create a tensor of shape `(4, 3)` filled with random floating-point numbers. 2. Obtain its underlying `torch.UntypedStorage`. 3. Clone the storage and fill the cloned storage with the value `10`. 4. Create a new tensor from the filled storage but with the same shape `(4, 3)`. 5. Return both the original tensor and the new tensor created from the filled storage. # Function Signature ```python def manipulate_tensors_and_storage() -> Tuple[torch.Tensor, torch.Tensor]: pass ``` # Constraints 1. The cloned storage must reflect the changes independently from the original storage. 2. Ensure that the new tensor accurately reflects the modified storage. 3. Use only the methods discussed in the provided documentation for manipulations. # Example Assume the tensor created with random numbers is: ``` tensor([[0.2107, 0.3421, 0.9827], [0.1234, 0.2345, 0.3456], [0.4567, 0.5678, 0.6789], [0.7890, 0.8901, 0.9012]]) ``` After manipulation using storage, the expected output tensors will be: - Original Tensor remains unchanged. - New Tensor created from modified storage: ``` tensor([[10., 10., 10.], [10., 10., 10.], [10., 10., 10.], [10., 10., 10.]]) ``` # Notes 1. Random values can vary, but the procedure should be followed as mentioned. 2. Do not use high-level tensor operations like `torch.fill_` for the final tensor creation from filled storage.","solution":"import torch from typing import Tuple def manipulate_tensors_and_storage() -> Tuple[torch.Tensor, torch.Tensor]: # Step 1: Create a tensor of shape (4, 3) filled with random floating-point numbers original_tensor = torch.rand((4, 3)) # Step 2: Obtain its underlying torch.UntypedStorage original_storage = original_tensor.storage() # Step 3: Clone the storage and fill the cloned storage with the value `10` cloned_storage = original_storage.clone() cloned_storage.fill_(10) # Step 4: Create a new tensor from the filled storage but with the same shape (4, 3) new_tensor = torch.Tensor(cloned_storage).view(4, 3) # Step 5: Return both the original tensor and the new tensor created from the filled storage return original_tensor, new_tensor"},{"question":"# Python Sequence Operations **Objective:** Implement a Python class `PySequence` that mirrors the sequence operations of the `PySequence` API described in the provided documentation. **Requirements:** 1. Implement the following methods: - `check(obj)`: Return `True` if `obj` provides the sequence protocol (i.e., has a `__getitem__` method and is not a dictionary subclass), `False` otherwise. - `size(obj)`: Return the number of objects in the sequence `obj`. Equivalent to `len(obj)`. - `concat(o1, o2)`: Return the concatenation of `o1` and `o2`. Equivalent to `o1 + o2`. - `repeat(obj, count)`: Return the result of repeating the sequence object `obj` `count` times. Equivalent to `obj * count`. - `get_item(obj, index)`: Return the `index`-th element of `obj`. Equivalent to `obj[index]`. - `get_slice(obj, i1, i2)`: Return the slice of sequence `obj` between indices `i1` and `i2`. Equivalent to `obj[i1:i2]`. - `set_item(obj, index, value)`: Assign `value` to the `index`-th element of `obj`. Equivalent to `obj[index] = value`. Return `True` on success, `False` on failure. - `del_item(obj, index)`: Delete the `index`-th element of `obj`. Equivalent to `del obj[index]`. Return `True` on success, `False` on failure. - `set_slice(obj, i1, i2, value)`: Assign sequence `value` to the slice of `obj` from `i1` to `i2`. Equivalent to `obj[i1:i2] = value`. Return `True` on success, `False` on failure. - `del_slice(obj, i1, i2)`: Delete the slice in sequence `obj` from `i1` to `i2`. Equivalent to `del obj[i1:i2]`. Return `True` on success, `False` on failure. - `count(obj, value)`: Return the number of occurrences of `value` in `obj`. Equivalent to `obj.count(value)`. - `contains(obj, value)`: Return `True` if `value` is in `obj`, `False` otherwise. Equivalent to `value in obj`. - `index(obj, value)`: Return the first index of `value` in `obj`. Equivalent to `obj.index(value)`. - `to_list(obj)`: Return a list object with the same contents as the sequence `obj`. Equivalent to `list(obj)`. - `to_tuple(obj)`: Return a tuple object with the same contents as the sequence `obj`. Equivalent to `tuple(obj)`. **Constraints:** - The input sequence objects can be of any type that supports Python\'s sequence protocol (lists, tuples, custom sequence classes, etc.). - Your implementation should handle edge cases appropriately, such as empty sequences or invalid indices. **Performance:** Ensure that the operations are as efficient as possible, following Python\'s typical performance characteristics for these operations. **Example Usage:** ```python class PySequence: @staticmethod def check(obj): # Your implementation here @staticmethod def size(obj): # Your implementation here @staticmethod def concat(o1, o2): # Your implementation here @staticmethod def repeat(obj, count): # Your implementation here @staticmethod def get_item(obj, index): # Your implementation here @staticmethod def get_slice(obj, i1, i2): # Your implementation here @staticmethod def set_item(obj, index, value): # Your implementation here @staticmethod def del_item(obj, index): # Your implementation here @staticmethod def set_slice(obj, i1, i2, value): # Your implementation here @staticmethod def del_slice(obj, i1, i2): # Your implementation here @staticmethod def count(obj, value): # Your implementation here @staticmethod def contains(obj, value): # Your implementation here @staticmethod def index(obj, value): # Your implementation here @staticmethod def to_list(obj): # Your implementation here @staticmethod def to_tuple(obj): # Your implementation here # Tests for the PySequence class can be implemented as follows: sequence = PySequence() assert sequence.check([1, 2, 3]) == True assert sequence.size((\\"a\\", \\"b\\", \\"c\\")) == 3 assert sequence.concat([1, 2], [3, 4]) == [1, 2, 3, 4] assert sequence.repeat([1, 2], 2) == [1, 2, 1, 2] assert sequence.get_item([10, 20, 30], 1) == 20 assert sequence.get_slice([1, 2, 3, 4, 5], 1, 3) == [2, 3] assert sequence.set_item([1, 2, 3], 1, 5) == True assert sequence.del_item([1, 2, 3], 1) == True assert sequence.set_slice([1, 2, 3, 4, 5], 1, 3, [9, 9]) == True assert sequence.del_slice([1, 2, 3, 4, 5], 1, 3) == True assert sequence.count([1, 2, 3, 2], 2) == 2 assert sequence.contains([\\"apple\\", \\"banana\\"], \\"banana\\") == True assert sequence.index([\\"apple\\", \\"banana\\", \\"cherry\\"], \\"banana\\") == 1 assert sequence.to_list((1, 2, 3)) == [1, 2, 3] assert sequence.to_tuple([1, 2, 3]) == (1, 2, 3) ``` **Note:** The example usage includes assertions to help validate your implementation and ensure correctness.","solution":"class PySequence: @staticmethod def check(obj): return hasattr(obj, \'__getitem__\') and not isinstance(obj, dict) @staticmethod def size(obj): return len(obj) @staticmethod def concat(o1, o2): return o1 + o2 @staticmethod def repeat(obj, count): return obj * count @staticmethod def get_item(obj, index): return obj[index] @staticmethod def get_slice(obj, i1, i2): return obj[i1:i2] @staticmethod def set_item(obj, index, value): try: obj[index] = value return True except (TypeError, IndexError): return False @staticmethod def del_item(obj, index): try: del obj[index] return True except (TypeError, IndexError): return False @staticmethod def set_slice(obj, i1, i2, value): try: obj[i1:i2] = value return True except (TypeError, IndexError): return False @staticmethod def del_slice(obj, i1, i2): try: del obj[i1:i2] return True except (TypeError, IndexError): return False @staticmethod def count(obj, value): return obj.count(value) @staticmethod def contains(obj, value): return value in obj @staticmethod def index(obj, value): return obj.index(value) @staticmethod def to_list(obj): return list(obj) @staticmethod def to_tuple(obj): return tuple(obj)"},{"question":"# Question: Custom Plotting Context Demonstration The goal of this task is to assess your understanding and ability to use the `plotting_context` function in Seaborn to control the scaling of plot elements. You need to write a function that demonstrates using both the default plotting context and a custom plotting context within a single visualization script. Function: `demonstrate_plotting_context()` Implement a function `demonstrate_plotting_context()` which performs the following actions: 1. Generates a simple line plot using the default plotting context. 2. Switches to a custom plotting context (`\\"talk\\"`) and within this context, generates another line plot on the same figure but in a different subplot. Expected Input and Output - **Input**: None. - **Output**: A Matplotlib figure with two subplots: - The first subplot should display a line plot in the default plotting context. - The second subplot should display another line plot in the \\"talk\\" plotting context. Constraints and Limitations - Use any data you prefer for the line plots. - Make sure the difference in context scaling is evident (e.g., by placing titles or labels which will show scaled differences). Example When you run your function, a Matplotlib figure with two subplots should be generated, as follows: **Upper subplot**: Displays a line plot with default scaling. **Lower subplot**: Displays a line plot with \\"talk\\" context scaling. ```python def demonstrate_plotting_context(): import seaborn as sns import matplotlib.pyplot as plt data_x = [\\"A\\", \\"B\\", \\"C\\"] data_y1 = [1, 3, 2] data_y2 = [2, 6, 4] fig, (ax1, ax2) = plt.subplots(2, 1) # Create two subplots # First subplot: Default plotting context sns.lineplot(x=data_x, y=data_y1, ax=ax1) ax1.set_title(\\"Default Context\\") # Second subplot: \\"talk\\" plotting context with sns.plotting_context(\\"talk\\"): sns.lineplot(x=data_x, y=data_y2, ax=ax2) ax2.set_title(\\"Talk Context\\") plt.tight_layout() plt.show() ``` This example is just for illustration. Your implementation should handle the plotting context changes as described and demonstrate the ability to use Seaborn\'s `plotting_context` efficiently.","solution":"def demonstrate_plotting_context(): import seaborn as sns import matplotlib.pyplot as plt # Data for the plots data_x = [0, 1, 2, 3, 4, 5] data_y1 = [0, 1, 4, 9, 16, 25] data_y2 = [0, 1, 8, 27, 64, 125] fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10)) # Create two subplots # First subplot: Default plotting context sns.lineplot(x=data_x, y=data_y1, ax=ax1) ax1.set_title(\\"Default Context\\") ax1.set_xlabel(\\"X Axis\\") ax1.set_ylabel(\\"Y Axis\\") # Second subplot: \\"talk\\" plotting context with sns.plotting_context(\\"talk\\"): sns.lineplot(x=data_x, y=data_y2, ax=ax2) ax2.set_title(\\"Talk Context\\") ax2.set_xlabel(\\"X Axis\\") ax2.set_ylabel(\\"Y Axis\\") plt.tight_layout() plt.show()"},{"question":"# Advanced Coding Task: Memory Mapped File Manipulation with `mmap` Problem Statement You are tasked with creating a program that demonstrates the use of the `mmap` module to perform efficient file manipulations. Specifically, you will create a memory-mapped file, perform several byte-level operations, and finally, utilize slicing for interfacing with the memory-mapped object. Requirements 1. **File Preparation**: Write a Python function named `prepare_file(filename, content)` that takes a `filename` (string) and `content` (bytes-like object). This function should create a file with the provided filename and write the content into it. 2. **Memory-Mapped File Operations**: - Create a function named `perform_mmap_operations(filename)`. This function should open the given file `filename` in read-write binary mode. - Create a memory-mapped object for the whole file. - Read and print the file content using `read()`. - Modify a specific range of bytes in the file using slicing. Replace the bytes from position 6 to 11 with the bytes `b\' world\'`. - Use the `seek()` method to set the file position to the beginning and read and print the updated content using `read()`. - Finally, close the mmap object and the file. Input/Output Format - **Input**: The `prepare_file` function accepts a filename and a bytes-like object for the content. - **Output**: The `perform_mmap_operations` function does not return anything but prints the file content before and after modification. Constraints - Assume the content length for the file is at least 12 bytes. - The length of the byte-string inserted in the modification step is equal to the length of the range being replaced. Example Usage ```python # Creating the file with initial content prepare_file(\'example.txt\', b\\"Hello Python!n\\") # Performing memory-mapped file operations perform_mmap_operations(\'example.txt\') ``` **Expected Output**: ``` b\'Hello Python!n\' b\'Hello world!n\' ``` Implementation Notes - Use `mmap.mmap` to create the memory-mapped file object. - Ensure proper closing of files and mmap objects. - Handle any potential exceptions that might arise during file handling and mmap operations. --- **Begin your implementation below:** ```python import mmap import os def prepare_file(filename, content): Create a file and write the given content to it. with open(filename, \'wb\') as f: f.write(content) def perform_mmap_operations(filename): Perform memory-mapped file operations for the given filename. with open(filename, \'r+b\') as f: mm = mmap.mmap(f.fileno(), 0) # Print initial file content print(mm[:]) # Modify the file content using slicing mm[6:12] = b\' world\' # Rewind the file and print updated content mm.seek(0) print(mm[:]) # Close the mmap object mm.close() # Example usage prepare_file(\'example.txt\', b\\"Hello Python!n\\") perform_mmap_operations(\'example.txt\') ```","solution":"import mmap import os def prepare_file(filename, content): Create a file and write the given content to it. Parameters: filename (str): The name of the file to be created. content (bytes-like object): The content to be written to the file. with open(filename, \'wb\') as f: f.write(content) def perform_mmap_operations(filename): Perform memory-mapped file operations for the given filename and modify specific range of bytes. Parameters: filename (str): The name of the file to be memory mapped and modified. with open(filename, \'r+b\') as f: mm = mmap.mmap(f.fileno(), 0) # Print initial file content print(mm[:]) # Modify the file content using slicing mm[6:12] = b\' world\' # Rewind the file and print updated content mm.seek(0) print(mm[:]) # Close the mmap object mm.close() # Example usage prepare_file(\'example.txt\', b\\"Hello Python!n\\") perform_mmap_operations(\'example.txt\')"},{"question":"# Question on Clustering with K-Means **Context:** You are tasked to cluster data points using the K-Means clustering algorithm. To deepen your understanding, you will first implement the K-Means algorithm from scratch. Afterward, you will use sklearn\'s `KMeans` to cluster the same data and compare the results. **Implementation Requirements:** 1. **Implement K-Means from Scratch:** - You need to implement the K-Means algorithm, including: - Initialization of centroids - Assignment of data points to the nearest centroid - Update of centroids based on the mean of assigned points - Iteration until convergence or a maximum number of iterations is reached - Your implementation should handle both random initialization and K-Means++ initialization. 2. **Validation and Comparison:** - Use sklearn\'s `KMeans` with the same data and compare the results with your implementation. - Use visualizations to depict clustering results. 3. **Evaluation Metrics:** - Use the Silhouette Score and Calinski-Harabasz Index to evaluate and compare both implementations. **Input:** - A dataset of two-dimensional points, stored in a CSV file with columns \\"x\\" and \\"y\\". - The number of clusters `k`. **Output:** - Clustering labels for each data point from both implementations. - Plots showing the clustering results from both implementations. - Values of Silhouette Score and Calinski-Harabasz Index for both implementations. **Constraints:** - Do not use the sklearn\'s `KMeans` implementation for the custom K-Means. - Use matplotlib for visualizations. - Ensure your code runs efficiently for a reasonable number of data points (up to thousands of points). # Example: Assume you have the following data points in a file `data.csv`: ``` x,y 1.0,2.0 2.0,3.0 3.0,4.0 8.0,5.0 9.0,7.0 10.0,6.0 ``` **Steps:** 1. Implement the K-Means clustering function from scratch. 2. Load the data from the CSV file. 3. Use your implementation to find clusters. 4. Use sklearnâ€™s `KMeans` to find clusters with the same data. 5. Plot the resulting clusters for both methods. 6. Compute and print the Silhouette Score and Calinski-Harabasz Index for both implementations. # Submission: Provide the following in your submission: 1. A Python script implementing the K-Means from scratch. 2. A Jupyter notebook showcasing the entire workflow including data loading, clustering, visualizations, and evaluation metrics. 3. Cluster plots comparing your implementation with sklearn\'s implementation. 4. Evaluation metric scores for both implementations. **Note:** Ensure that your code is well-documented and includes inline comments explaining each step of the algorithm.","solution":"import numpy as np from sklearn.metrics import silhouette_score, calinski_harabasz_score import matplotlib.pyplot as plt from sklearn.datasets import make_blobs import pandas as pd def initialize_centroids(X, k, init=\'random\'): Initialize centroids for K-Means clustering. Parameters: - X: np.ndarray, data points - k: int, number of clusters - init: str, \'random\' or \'kmeans++\' Returns: - centroids: np.ndarray, initialized centroids if init == \'random\': indices = np.random.choice(X.shape[0], k, replace=False) centroids = X[indices] elif init == \'kmeans++\': centroids = [X[np.random.randint(X.shape[0])]] for _ in range(1, k): dist_sq = np.array([min([np.inner(c-x, c-x) for c in centroids]) for x in X]) probs = dist_sq / dist_sq.sum() cumulative_probs = probs.cumsum() r = np.random.rand() next_centroid = X[np.where(cumulative_probs >= r)[0][0]] centroids.append(next_centroid) centroids = np.array(centroids) return centroids def assign_clusters(X, centroids): Assign data points to the nearest centroid. Parameters: - X: np.ndarray, data points - centroids: np.ndarray, current centroids Returns: - labels: np.ndarray, assigned cluster labels distances = np.array([np.linalg.norm(X - centroid, axis=1) for centroid in centroids]) labels = np.argmin(distances, axis=0) return labels def compute_centroids(X, labels, k): Update centroids based on mean of assigned points. Parameters: - X: np.ndarray, data points - labels: np.ndarray, cluster labels - k: int, number of clusters Returns: - centroids: np.ndarray, updated centroids centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)]) return centroids def kmeans(X, k, max_iters=100, tol=1e-4, init=\'random\'): K-Means clustering algorithm. Parameters: - X: np.ndarray, data points - k: int, number of clusters - max_iters: int, maximum number of iterations - tol: float, tolerance to declare convergence - init: str, \'random\' or \'kmeans++\' Returns: - centroids: np.ndarray, final centroids - labels: np.ndarray, cluster labels centroids = initialize_centroids(X, k, init=init) for _ in range(max_iters): old_centroids = centroids labels = assign_clusters(X, centroids) centroids = compute_centroids(X, labels, k) if np.all(np.linalg.norm(centroids - old_centroids, axis=1) < tol): break return centroids, labels def plot_clusters(X, labels, centroids, title=\'Clusters\'): Plot clustered data points and centroids. Parameters: - X: np.ndarray, data points - labels: np.ndarray, cluster labels - centroids: np.ndarray, cluster centroids - title: str, plot title plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap=\'viridis\') plt.scatter(centroids[:, 0], centroids[:, 1], c=\'red\', s=200, alpha=0.5) plt.title(title) plt.show() # Example Usage if __name__ == \\"__main__\\": # Generate synthetic data X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0) # Custom KMeans k = 4 centroids, labels = kmeans(X, k, init=\'kmeans++\') # Plot Custom KMeans results plot_clusters(X, labels, centroids, title=\'Custom K-Means\') # Sklearn KMeans from sklearn.cluster import KMeans kmeans_sklearn = KMeans(n_clusters=k, init=\'k-means++\') kmeans_sklearn.fit(X) # Plot Sklearn KMeans results plot_clusters(X, kmeans_sklearn.labels_, kmeans_sklearn.cluster_centers_, title=\'Sklearn K-Means\') # Evaluation Metrics score_custom_silh = silhouette_score(X, labels) score_custom_ch = calinski_harabasz_score(X, labels) score_sklearn_silh = silhouette_score(X, kmeans_sklearn.labels_) score_sklearn_ch = calinski_harabasz_score(X, kmeans_sklearn.labels_) print(f\\"Custom K-Means Silhouette Score: {score_custom_silh}\\") print(f\\"Custom K-Means Calinski-Harabasz Index: {score_custom_ch}\\") print(f\\"Sklearn K-Means Silhouette Score: {score_sklearn_silh}\\") print(f\\"Sklearn K-Means Calinski-Harabasz Index: {score_sklearn_ch}\\")"},{"question":"# Question You are provided with a dataset containing numerical information that needs to be visualized in a heatmap. Your task is to write a Python function using the seaborn library to generate a heatmap with a specific color palette. The color palette should be created as follows: 1. The palette should be a sequential gradient from light gray to a user-specified color. 2. The color should be provided as a hex code. 3. The palette should contain exactly 10 distinct colors. 4. The palette should be converted to a continuous colormap for the heatmap. Input Format: - A 2D list or numpy array `data` representing the numerical data. - A string `color_hex` representing the hex code of the target color. Output Format: - The function should display the heatmap using seaborn. Constraints: - The input data will always be a valid 2D list or numpy array. - The hex code will always be in a valid string format. Example: ```python import numpy as np data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) color_hex = \\"#79C\\" create_heatmap(data, color_hex) ``` Expected Output: - A heatmap visualization with a gradient color palette ranging from light gray to the specified color `#79C`. Function Signature: ```python def create_heatmap(data, color_hex): # Your code here ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np from matplotlib.colors import LinearSegmentedColormap def create_heatmap(data, color_hex): Create and display a heatmap with a color palette ranging from light gray to the specified color. Parameters: data (2D list or numpy array): Numerical data for the heatmap. color_hex (str): Hex code of the target color. # Create a list of 10 colors from light gray to the specified color palette = [ \\"#D3D3D3\\" ] + [ color_hex ] # Light gray and the specified color cmap = LinearSegmentedColormap.from_list(\\"custom_palette\\", palette, N=10) # Plot the heatmap sns.heatmap(data, cmap=cmap) plt.show()"},{"question":"**Question: Creating and Customizing Faceted Plots with Seaborn** You are given the `penguins` dataset. Your task is to write a Python function that generates a customized faceted plot using Seaborn\'s object-oriented interface. Specifically, you need to facet the plot by the species and sex of the penguins, and customize the axes sharing and appearance of the plot. **Requirements:** 1. **Load the Penguins dataset:** - Use `seaborn`\'s `load_dataset` function to load the `penguins` dataset. 2. **Create a faceted plot:** - Use the `so.Plot` class to create a plot with the following characteristics: - The x-axis should represent `bill_length_mm`. - The y-axis should represent `bill_depth_mm`. - Facet the plot by columns based on `species` and by rows based on `sex`. 3. **Customize axes sharing:** - Do not share the x-axis or y-axis between the facets. 4. **Add elements to the plot:** - Use dots to represent the data points. 5. **Display the plot:** - Ensure the plot is displayed in the output. **Function Signature:** ```python def create_custom_faceted_plot(): pass ``` **Expected Output:** The function should produce a faceted plot with independent x and y axes for each subplot, displaying dots representing the `bill_length_mm` and `bill_depth_mm` of penguins, faceted by species and sex. **Constraints:** - You are required to use the seaborn `objects` interface. - Your solution should use no more than 20 lines of code, excluding import statements. **Additional Information:** - Ensure the plot is well-labeled and visually clear. - Handle any potential exceptions that may occur during the plotting process. ```python # Example function implementation (this will not be provided to students) import seaborn.objects as so from seaborn import load_dataset def create_custom_faceted_plot(): penguins = load_dataset(\\"penguins\\") p = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .facet(col=\\"species\\", row=\\"sex\\") .add(so.Dots()) ) p.share(x=False, y=False) p.show() # To test the function create_custom_faceted_plot() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def create_custom_faceted_plot(): Creates a faceted plot of the penguins dataset with specific customizations. penguins = load_dataset(\\"penguins\\") plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .facet(col=\\"species\\", row=\\"sex\\") .add(so.Dots()) ) plot.share(x=False, y=False) plot.show()"},{"question":"# Image Type Detection and Extension Objective You are given an interface to determine the type of an image file using the `imghdr` module in Python. Your task is to write a function that extends this capability to recognize a new image format, namely \'ico\' (icon files), in addition to the existing formats. Problem Statement 1. **Function 1**: Implement a function called `detect_image_format(file_or_bytes: Union[str, bytes]) -> str` which: - Accepts a file path as a string or a byte stream as bytes. - Returns the detected image format as a string. If the format is unrecognized, return `\\"unknown\\"`. 2. **Function 2**: Implement a function called `add_ico_format_recognition() -> None` which: - Extends the `imghdr` module to recognize \'ico\' files. - An icon file can be identified by its \'ico\' signature located in the first two bytes (0:2), which are equivalent to `b\'x00x00\'`. Input and Output - Input: `file_or_bytes` - It can be either a file path (string) or a byte stream (bytes). - Example file paths: `\'path/to/image.jpg\'`, `\'path/to/icon.ico\'` - Example byte streams: `b\'x89PNGrnx1an\'` - Output: The function `detect_image_format` should return the image format as a string. - Example output: `\'jpeg\'`, `\'png\'`, `\'ico\'`, or `\'unknown\'` Constraints - You should handle the cases where the file does not exist or is unreadable gracefully (e.g., by returning `\\"unknown\\"`). - Your solution should only add new test for \'ico\' files without modifying `imghdr` module\'s original tests for other formats. Example Usage ```python # Example usage of your functions # First, add ico format recognition add_ico_format_recognition() # Then, detect image formats print(detect_image_format(\\"example.jpg\\")) # Output: \'jpeg\' print(detect_image_format(b\'x89PNGrnx1an\')) # Output: \'png\' print(detect_image_format(\\"icon.ico\\")) # Output: \'ico\' if the file exists and is valid print(detect_image_format(b\'x00x00\')) # Output: \'ico\' print(detect_image_format(\\"nonexistentfile.png\\")) # Output: \'unknown\' ``` Notes - Make sure to show how you are extending the `imghdr` module to recognize the new format. - Include necessary imports and handle possible exceptions as needed.","solution":"import imghdr from typing import Union def detect_image_format(file_or_bytes: Union[str, bytes]) -> str: Detect the image format from the given file path or byte stream. Args: - file_or_bytes (Union[str, bytes]): File path or byte stream. Returns: - str: Detected image format, or \\"unknown\\" if unrecognized. if isinstance(file_or_bytes, str): try: with open(file_or_bytes, \'rb\') as f: file_header = f.read(32) except (FileNotFoundError, IOError): return \\"unknown\\" elif isinstance(file_or_bytes, bytes): file_header = file_or_bytes[:32] else: return \\"unknown\\" format = imghdr.what(None, h=file_header) return format if format else \\"unknown\\" def add_ico_format_recognition() -> None: Extends the imghdr module to recognize \'ico\' files. An icon file can be identified by its \'ico\' signature located in the first two bytes (0:2), which are equivalent to b\'x00x00\'. Returns: - None def detect_ico(header, file): if header[:2] == b\'x00x00\': return \'ico\' return None imghdr.tests.append(detect_ico)"},{"question":"# Question: ConfigParser Advanced Usage You are provided with a configuration file named `settings.ini` which contains application settings. Your task is to write a Python script using the `configparser` module to perform the following tasks: 1. **Load the Configuration File**: - Read the given `settings.ini` file. - Ensure that the file exists and handle the case where the file might be missing. 2. **Modify Configuration**: - Add a new section called `database`. - In the `database` section, add the following settings: ```ini host = localhost port = 5432 user = admin password = admin123 ``` - Ensure that the `password` setting is written only if it doesn\'t already exist. 3. **Retrieve and Print Settings**: - Print all settings under the `database` section, in the format: `key = value`. - Retrieve and print the `CompressionLevel` setting from the `[DEFAULT]` section. - Retrieve and print the `ForwardX11` setting from the `topsecret.server.example` section, using the appropriate type conversion method. # Input and Output - **Input**: The `settings.ini` file which may look like: ```ini [DEFAULT] ServerAliveInterval = 45 Compression = yes CompressionLevel = 9 ForwardX11 = yes [forge.example] User = hg [topsecret.server.example] Port = 50022 ForwardX11 = no ``` - **Output**: The script should print output similar to: ``` [database] host = localhost port = 5432 user = admin password = admin123 CompressionLevel in DEFAULT section: 9 ForwardX11 in topsecret.server.example section: False ``` # Constraints - Use the appropriate methods from the `configparser` module. - Ensure that the script handles missing files gracefully. - Use type conversion methods provided by `configparser` for boolean and integer values. # Performance Requirements - The script should handle configuration files with up to 100 sections and 1000 settings efficiently. # Example Starting Point: Here is a starting point for your script: ```python import configparser import os def main(): config = configparser.ConfigParser() config_file_path = \'settings.ini\' # 1. Load the configuration file if os.path.exists(config_file_path): config.read(config_file_path) else: print(f\\"The file {config_file_path} does not exist.\\") return # 2. Modify the configuration to add a [database] section if not config.has_section(\'database\'): config.add_section(\'database\') if not config.has_option(\'database\', \'password\'): config.set(\'database\', \'password\', \'admin123\') config.set(\'database\', \'host\', \'localhost\') config.set(\'database\', \'port\', \'5432\') config.set(\'database\', \'user\', \'admin\') # 3. Retrieve and print settings print(\\"[database]\\") for key, value in config.items(\'database\'): print(f\\"{key} = {value}\\") compression_level = config.get(\'DEFAULT\', \'CompressionLevel\') print(f\\"CompressionLevel in DEFAULT section: {compression_level}\\") forward_x11 = config.getboolean(\'topsecret.server.example\', \'ForwardX11\') print(f\\"ForwardX11 in topsecret.server.example section: {forward_x11}\\") if __name__ == \\"__main__\\": main() ``` Complete the `main` function to fulfill all tasks outlined in the problem statement.","solution":"import configparser import os def main(): config = configparser.ConfigParser() config_file_path = \'settings.ini\' # 1. Load the configuration file if os.path.exists(config_file_path): config.read(config_file_path) else: print(f\\"The file {config_file_path} does not exist.\\") return # 2. Modify the configuration to add a [database] section if not config.has_section(\'database\'): config.add_section(\'database\') if not config.has_option(\'database\', \'password\'): config.set(\'database\', \'password\', \'admin123\') config.set(\'database\', \'host\', \'localhost\') config.set(\'database\', \'port\', \'5432\') config.set(\'database\', \'user\', \'admin\') # Save the changes to the file with open(config_file_path, \'w\') as configfile: config.write(configfile) # 3. Retrieve and print settings print(\\"[database]\\") for key, value in config.items(\'database\'): print(f\\"{key} = {value}\\") compression_level = config.get(\'DEFAULT\', \'CompressionLevel\') print(f\\"CompressionLevel in DEFAULT section: {compression_level}\\") forward_x11 = config.getboolean(\'topsecret.server.example\', \'ForwardX11\') print(f\\"ForwardX11 in topsecret.server.example section: {forward_x11}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Time Delta Operations with Panda DataFrames Background You will be provided with a DataFrame containing time-logged events. The DataFrame captures start and end times for various tasks in a project. Your task is to compute various statistics and manipulate these time data using pandas\' `Timedelta` functionality. DataFrame The DataFrame `df_tasks` you will use has the following columns: - **task_id**: Unique identifier for each task. - **start_time**: The start time of the task (as a string in the format \'YYYY-MM-DD HH:MM:SS\'). - **end_time**: The end time of the task (as a string in the format \'YYYY-MM-DD HH:MM:SS\'). Function Definition Write a function `compute_task_statistics` that accepts a DataFrame `df_tasks` and performs the following operations: 1. **Compute Duration**: Create a new column `duration` in the DataFrame that contains the duration of each task as `Timedelta`. 2. **Filter Tasks**: Create a new DataFrame `short_tasks` that includes only those tasks whose duration is less than 2 hours. 3. **Calculate Averages**: Compute the average duration of tasks in the original DataFrame and the new filtered DataFrame. 4. **Resample Duration**: Resample the average daily task duration and store it in a new Series `daily_avg_duration`. 5. **Component Extraction**: Extract components (days, seconds) of the average duration of tasks from both DataFrames. Constraints - You should not change the original DataFrame. - If there are any missing values (`NaT`), ignore them during computation. Function Signature ```python def compute_task_statistics(df_tasks: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Timedelta, pd.Timedelta, pd.Series, Dict[str, int], Dict[str, int]]: pass ``` Expected Input ```python import pandas as pd # Example input DataFrame data = { \\"task_id\\": [1, 2, 3, 4], \\"start_time\\": [\\"2023-08-01 08:00:00\\", \\"2023-08-01 09:30:00\\", \\"2023-08-01 10:00:00\\", \\"2023-08-01 15:00:00\\"], \\"end_time\\": [\\"2023-08-01 10:00:00\\", \\"2023-08-01 10:30:00\\", \\"2023-08-01 14:00:00\\", \\"2023-08-01 16:00:00\\"], } df_tasks = pd.DataFrame(data) ``` Expected Output ```python # Given the above input, the output should be something like: short_tasks: DataFrame with tasks which have a duration less than 2 hours average_duration_original: The average duration of the tasks in the original DataFrame. average_duration_short: The average duration of the short tasks. daily_avg_duration: A Series with average daily durations. components_original: A dictionary with components of the average duration from the original DataFrame. components_short: A dictionary with components of the average duration from the short tasks DataFrame. # Output structure ( short_tasks, average_duration_original, average_duration_short, daily_avg_duration, components_original, components_short ) ``` Notes - Ensure you are using pandas\' `Timedelta` functionality extensively as described in the provided documentation. - Your solution will be judged based on correctness, readability, and efficiency.","solution":"import pandas as pd from typing import Tuple, Dict def compute_task_statistics(df_tasks: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Timedelta, pd.Timedelta, pd.Series, Dict[str, int], Dict[str, int]]: # Ensure start_time and end_time are in datetime format df_tasks[\'start_time\'] = pd.to_datetime(df_tasks[\'start_time\']) df_tasks[\'end_time\'] = pd.to_datetime(df_tasks[\'end_time\']) # Compute duration as a new column df_tasks[\'duration\'] = df_tasks[\'end_time\'] - df_tasks[\'start_time\'] # Filter tasks with duration less than 2 hours (7200 seconds) short_tasks = df_tasks[df_tasks[\'duration\'] < pd.Timedelta(hours=2)].copy() # Calculate average duration avg_duration_original = df_tasks[\'duration\'].mean() avg_duration_short = short_tasks[\'duration\'].mean() # Resample duration to calculate average daily duration daily_duration = df_tasks.set_index(\'start_time\').resample(\'D\')[\'duration\'].mean() daily_avg_duration = daily_duration.mean() # Extract components of average duration components_original = { \'days\': avg_duration_original.days, \'seconds\': avg_duration_original.seconds } components_short = { \'days\': avg_duration_short.days, \'seconds\': avg_duration_short.seconds } return short_tasks, avg_duration_original, avg_duration_short, daily_avg_duration, components_original, components_short"},{"question":"You are given a piece of Python code that is intended to sort a large list of integers. However, it runs significantly slower than expected, and it occasionally crashes due to a segmentation fault. Use the `faulthandler` and `cProfile` modules to analyze and optimize the code. # Provided Code ```python import random def generate_large_list(n): return [random.randint(1, 1000000) for _ in range(n)] def sort_large_list(lst): # Intentionally inefficient sort implementation for i in range(len(lst)): for j in range(i + 1, len(lst)): if lst[i] > lst[j]: lst[i], lst[j] = lst[j], lst[i] return lst if __name__ == \\"__main__\\": large_list = generate_large_list(100000) sorted_list = sort_large_list(large_list) print(\\"List sorted successfully!\\") ``` # Tasks 1. **Identify Crashes**: Use the `faulthandler` module to identify why the code sometimes crashes. Modify the code to write a traceback to a file when a segmentation fault occurs. 2. **Profiling**: Use the `cProfile` module to profile the `sort_large_list` function. Identify the bottlenecks in the code. 3. **Optimize**: Modify the `sort_large_list` function to use a more efficient sorting algorithm. Ensure that the optimized version handles the large list correctly without crashing. 4. **Verify**: Write test cases to verify that your optimized sorting function works correctly. # Expected Input and Output - **Input**: - A list of integers (length up to 100,000). - **Output**: - A sorted list of integers. - Traceback file in case of crashes. - Profiling report that identifies bottlenecks. # Constraints - The code should handle lists up to 100,000 integers without crashing. - The optimized sorting algorithm should be efficient enough to sort the list within a reasonable time frame. # Performance Requirements - The new sorting algorithm should have a better time complexity compared to the original nested loop implementation. # Implementation 1. **Fault Handler**: ```python import faulthandler if __name__ == \\"__main__\\": faulthandler.enable(file=open(\\"traceback.txt\\", \\"w\\")) large_list = generate_large_list(100000) sorted_list = sort_large_list(large_list) print(\\"List sorted successfully!\\") ``` 2. **Profiling**: ```python import cProfile if __name__ == \\"__main__\\": faulthandler.enable(file=open(\\"traceback.txt\\", \\"w\\")) large_list = generate_large_list(100000) profiler = cProfile.Profile() profiler.enable() sorted_list = sort_large_list(large_list) profiler.disable() profiler.print_stats() print(\\"List sorted successfully!\\") ``` 3. **Optimized Sort**: ```python def optimized_sort_large_list(lst): return sorted(lst) if __name__ == \\"__main__\\": faulthandler.enable(file=open(\\"traceback.txt\\", \\"w\\")) large_list = generate_large_list(100000) profiler = cProfile.Profile() profiler.enable() sorted_list = optimized_sort_large_list(large_list) profiler.disable() profiler.print_stats() print(\\"List sorted successfully!\\") ``` 4. **Verification**: ```python def test_sorting_function(): test_cases = [ ([5, 3, 1, 4, 2], [1, 2, 3, 4, 5]), ([1], [1]), ([], []), ([2, 2, 2, 2], [2, 2, 2, 2]), ] for i, (input_list, expected_output) in enumerate(test_cases): assert optimized_sort_large_list(input_list.copy()) == expected_output, f\\"Test case {i} failed!\\" print(\\"All test cases passed!\\") if __name__ == \\"__main__\\": faulthandler.enable(file=open(\\"traceback.txt\\", \\"w\\")) test_sorting_function() large_list = generate_large_list(100000) profiler = cProfile.Profile() profiler.enable() sorted_list = optimized_sort_large_list(large_list) profiler.disable() profiler.print_stats() print(\\"List sorted successfully!\\") ```","solution":"import random import faulthandler import cProfile def generate_large_list(n): return [random.randint(1, 1000000) for _ in range(n)] def sort_large_list(lst): # Intentionally inefficient sort implementation for i in range(len(lst)): for j in range(i + 1, len(lst)): if lst[i] > lst[j]: lst[i], lst[j] = lst[j], lst[i] return lst def optimized_sort_large_list(lst): return sorted(lst) if __name__ == \\"__main__\\": faulthandler.enable(file=open(\\"traceback.txt\\", \\"w\\")) large_list = generate_large_list(100000) profiler = cProfile.Profile() profiler.enable() sorted_list = optimized_sort_large_list(large_list) profiler.disable() profiler.dump_stats(\\"profiling_stats.prof\\") print(\\"List sorted successfully!\\")"},{"question":"**Python Programming Question** **Objective:** Demonstrate understanding and application of Python\'s \\"sysconfig,\\" \\"dataclasses,\\" and \\"contextlib\\" modules. **Problem Statement:** Your task is to design a Python program that reads system configuration, manages data using data classes, and ensures proper resource management with context managers. **Requirements:** 1. Implement a function `get_python_build_info()` that uses the `sysconfig` module to return a dictionary with the current Python version, platform, and the build date. 2. Create a data class `BuildInfo` that will hold: - Python version, - Platform, - Build date. Ensure the data class is immutable (frozen). 3. Implement a context manager using `contextlib` that will: - On entering, print \\"Entering context.\\" - On exiting, print \\"Exiting context.\\" - Properly handle any exception raised within the context by printing \\"An exception occurred: \\" followed by the exception message. 4. Use the context manager to create an instance of `BuildInfo` using data retrieved from the `get_python_build_info()` function and print it. Here\'s a function signature to get you started: ```python from dataclasses import dataclass from contextlib import contextmanager import sysconfig def get_python_build_info() -> dict: # Your code here pass @dataclass(frozen=True) class BuildInfo: # Your code here pass @contextmanager def custom_context_manager(): # Your code here pass if __name__ == \\"__main__\\": # Your code here pass ``` **Constraints:** - You need to ensure the `BuildInfo` class is frozen (immutable). - The context manager should handle exceptions gracefully and always print the \\"Entering context\\" and \\"Exiting context\\" messages appropriately, regardless of whether an exception occurs. - You should utilize the `sysconfig` module to retrieve the required configuration details. **Example Output:** ``` Entering context. BuildInfo(python_version=\'3.10.0\', platform=\'darwin\', build_date=\'Oct 22 2021\') Exiting context. ``` If an exception occurs within the context, the output should include: ``` Entering context. An exception occurred: <exception_message> Exiting context. ``` **Note:** Replace the example data (e.g., \'3.10.0\', \'darwin\', \'Oct 22 2021\') with actual values from your system. **Evaluation:** - Correctness of the implementation. - Proper use of the `sysconfig`, `dataclasses`, and `contextlib` modules. - Clean and readable code. - Comprehensive handling of exceptions within the context manager.","solution":"from dataclasses import dataclass from contextlib import contextmanager import sysconfig def get_python_build_info() -> dict: Retrieves current Python build information. Returns: A dictionary containing Python version, platform, and build date. python_version = sysconfig.get_python_version() platform = sysconfig.get_platform() build_date = sysconfig.get_config_var(\'BUILD_TAG\') return { \'python_version\': python_version, \'platform\': platform, \'build_date\': build_date } @dataclass(frozen=True) class BuildInfo: python_version: str platform: str build_date: str @contextmanager def custom_context_manager(): A custom context manager that prints messages on entering and exiting the context, and handles exceptions gracefully. print(\\"Entering context.\\") try: yield except Exception as e: print(f\\"An exception occurred: {e}\\") finally: print(\\"Exiting context.\\") if __name__ == \\"__main__\\": build_info_dict = get_python_build_info() with custom_context_manager(): build_info = BuildInfo(**build_info_dict) print(build_info)"},{"question":"You are working on a Python project that involves updating an old codebase. This codebase uses several deprecated methods and you need to ensure that all deprecations are handled properly without overwhelming the standard output with warning messages during execution. You also need to ensure the warnings are properly captured and tested. Tasks 1. **Custom Deprecation Handling**: Create a function `custom_warn(message: str, category: Type[Warning], stacklevel: int = 1) -> None` that issues a warning using the `warnings.warn()` method. The usage of this function should adjust the stack level as specified by the `stacklevel` parameter. 2. **Context Manager for Suppressing Warnings**: Write a context manager class `SuppressWarnings` that will suppress specific categories of warnings within its context. The context manager should accept a list of warning categories to suppress. 3. **Testing Warnings**: Write a function `test_warnings()` that: - Uses the `catch_warnings` context manager to test that a function correctly raises a specified warning. - Tests the `custom_warn` function to ensure it raises a `DeprecationWarning` with the message \\"This method is deprecated.\\" - Ensure that any other type of warning raised within the test is allowed to propagate normally. Constraints - Do not use the `warnings.simplefilter(\\"ignore\\")` method directly within the `test_warnings` function, except within the context manager for specific tests. - Only use methods and attributes available in the `warnings` module as documented. Example Usage ```python def old_method(): custom_warn(\\"This method is deprecated.\\", DeprecationWarning) with SuppressWarnings([DeprecationWarning]): old_method() # This should suppress the DeprecationWarning test_warnings() # This should test that the DeprecationWarning is raised correctly ``` Implementation Details - **`custom_warn` Function**: - Parameters: - `message`: The warning message to be displayed. - `category`: The category of the warning. - `stacklevel`: The stack level at which to report the warning. - **`SuppressWarnings` Context Manager**: - Parameters: - `categories`: A list of warning categories to suppress. - **`test_warnings` Function**: - No parameters. - Should raise an assertion error if the `custom_warn` does not perform as expected. Submission Requirements - Implement and test the `custom_warn` function. - Implement and test the `SuppressWarnings` context manager. - Implement the `test_warnings` function with assertions to confirm correct warning behavior. Good luck!","solution":"import warnings from typing import Type, List def custom_warn(message: str, category: Type[Warning], stacklevel: int = 1) -> None: Issues a warning with the specified message, category, and stacklevel. warnings.warn(message, category, stacklevel=stacklevel) class SuppressWarnings: Context manager to suppress specific categories of warnings. def __init__(self, categories: List[Type[Warning]]): self.categories = categories def __enter__(self): for category in self.categories: warnings.filterwarnings(\\"ignore\\", category=category) def __exit__(self, exc_type, exc_val, exc_tb): warnings.resetwarnings() def test_warnings(): Tests the custom_warn function and SuppressWarnings context manager to ensure warnings are handled correctly. with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") custom_warn(\\"This method is deprecated.\\", DeprecationWarning, stacklevel=2) assert len(w) == 1 and issubclass(w[-1].category, DeprecationWarning) assert str(w[-1].message) == \\"This method is deprecated.\\" with warnings.catch_warnings(record=True) as w: with SuppressWarnings([DeprecationWarning]): custom_warn(\\"This method is deprecated.\\", DeprecationWarning) assert len(w) == 0 # Ensure that other types of warnings are not suppressed with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") custom_warn(\\"This is a user warning.\\", UserWarning, stacklevel=1) assert len(w) == 1 and issubclass(w[-1].category, UserWarning) assert str(w[-1].message) == \\"This is a user warning.\\""},{"question":"You are given two classes `torch.finfo` and `torch.iinfo` that provide numerical properties of PyTorch\'s floating-point and integer data types, respectively. Your task is to implement a Python function using PyTorch that takes a list of PyTorch data types and returns a dictionary containing the numerical properties of these data types. Specifically, your function should output the following properties for each data type: - For floating-point types (`torch.float32`, `torch.float64`, `torch.float16`, `torch.bfloat16`): - `bits` - `eps` - `max` - `min` - `tiny` - `resolution` - For integer types (`torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, `torch.int64`): - `bits` - `max` - `min` # Input: - `dtype_list`: List of PyTorch data types (Both floating-point and integer types) # Output: - A dictionary where the keys are the data types and the values are dictionaries containing the numerical properties specified above. # Example: ```python import torch def get_numerical_properties(dtype_list): pass dtype_list = [torch.float32, torch.int32, torch.float64, torch.uint8] print(get_numerical_properties(dtype_list)) ``` Output: ```python { torch.float32: { \'bits\': 32, \'eps\': 1.1920928955078125e-07, \'max\': 3.4028234663852886e+38, \'min\': -3.4028234663852886e+38, \'tiny\': 1.1754943508222875e-38, \'resolution\': 1e-07 }, torch.int32: { \'bits\': 32, \'max\': 2147483647, \'min\': -2147483648 }, torch.float64: { \'bits\': 64, \'eps\': 2.220446049250313e-16, \'max\': 1.7976931348623157e+308, \'min\': -1.7976931348623157e+308, \'tiny\': 2.2250738585072014e-308, \'resolution\': 1e-15 }, torch.uint8: { \'bits\': 8, \'max\': 255, \'min\': 0 } } ``` # Constraints: 1. The input list will contain valid PyTorch data types only. 2. Your solution should handle both floating-point and integer data types correctly. 3. You should not use any external libraries except PyTorch. # Notes: - Pay special attention to the properties that each data type should have, as listed above. - Ensure that your function correctly differentiates between floating-point and integer types and fetches the appropriate properties from `torch.finfo` and `torch.iinfo` respectively.","solution":"import torch def get_numerical_properties(dtype_list): result = {} for dtype in dtype_list: if dtype.is_floating_point: # Get properties for floating-point types finfo = torch.finfo(dtype) result[dtype] = { \'bits\': finfo.bits, \'eps\': finfo.eps, \'max\': finfo.max, \'min\': finfo.min, \'tiny\': finfo.tiny, \'resolution\': finfo.resolution } else: # Get properties for integer types iinfo = torch.iinfo(dtype) result[dtype] = { \'bits\': iinfo.bits, \'max\': iinfo.max, \'min\': iinfo.min } return result"},{"question":"Coding Assessment Question # Objective Design a command-line utility using Python\'s `optparse` module that processes user inputs for a simple file utility program. The utility should be able to perform multiple actions such as listing files in a directory, displaying file information, and optionally filtering files based on file extension. # Question Write a Python script using the `optparse` module that implements the following functionality: 1. **List files**: - Add an option to specify a directory (`-d` or `--directory`). - If the directory is specified, list all files in the directory. - If not specified, list files in the current working directory. 2. **Filter by extension**: - Add an option to filter files by a given extension (`-e` or `--extension`). - If specified, only list files that match the given extension. 3. **Display help and usage**: - Generate a usage message and help guide for the command-line utility. # Input - The script should handle command-line inputs using the specified options. # Output - The script should print the list of files based on the specified options. - If the help option is provided, it should display the help message and exit. # Constraints - Do not use the `argparse` module. - The script must handle errors gracefully, such as invalid directories or nonexistent extensions. # Example Usage Assume the script is named `file_utility.py`: 1. To list all files in the current directory: ```sh python file_utility.py ``` 2. To list all files in a specified directory: ```sh python file_utility.py -d /path/to/directory ``` 3. To list all `.txt` files in the current directory: ```sh python file_utility.py -e .txt ``` 4. To list all `.py` files in a specified directory: ```sh python file_utility.py -d /path/to/directory -e .py ``` # Example Output For the command `python file_utility.py -d /path/to/directory -e .py`, the output might be: ``` file1.py script.py module.py ``` # Implementation Notes - Use the `optparse` module to handle the command-line arguments. - Ensure the script handles the specified options and combines them correctly. - Provide meaningful error messages for invalid inputs. # Submission Submit the Python script implementing the above functionality. The script should be well-documented and adhere to best practices for using the `optparse` module.","solution":"import os from optparse import OptionParser def list_files(directory, extension): Lists files in the directory with the given extension. try: directory_files = os.listdir(directory) except FileNotFoundError: print(f\\"Error: The directory \'{directory}\' does not exist.\\") return files_to_list = [] for file in directory_files: if extension: if file.endswith(extension): files_to_list.append(file) else: files_to_list.append(file) for file in files_to_list: print(file) def main(): usage = \\"usage: %prog [options]\\" parser = OptionParser(usage=usage) parser.add_option(\\"-d\\", \\"--directory\\", dest=\\"directory\\", help=\\"specify directory to list files\\", metavar=\\"DIR\\") parser.add_option(\\"-e\\", \\"--extension\\", dest=\\"extension\\", help=\\"filter files by extension\\", metavar=\\"EXT\\") (options, args) = parser.parse_args() directory = options.directory if options.directory else os.getcwd() extension = options.extension list_files(directory, extension) if __name__ == \\"__main__\\": main()"},{"question":"# Python Asyncio: Web Scraper Task with Concurrent Requests Description You\'re required to implement a simplified web scraper using Python\'s `asyncio` library. The goal is to fetch multiple web pages concurrently, process their content, and store results. You will: 1. Fetch content from multiple URLs concurrently. 2. Extract and count occurrences of a specific keyword within each page\'s content. 3. Handle potential timeouts and cancellations gracefully. 4. Ensure clean handling and collection of results even in the presence of errors. Requirements - Implement the function `fetch_and_process(urls, keyword)` that accepts a list of URLs and a keyword. - Use `asyncio` to fetch contents from the given URLs concurrently. - Extract and count the occurrences of the given keyword in the content of each URL. - Handle timeouts: if fetching content from a URL takes longer than 2 seconds, skip it and move on. - Handle cancellations: ensure any in-progress tasks can be cancelled gracefully. - Collect the results in the format: `{url1: count1, url2: count2, ...}` where `count` is the occurrence of the keyword in the content fetched from the respective URL. - Use appropriate exception handling to manage network errors or other unforeseen issues. Input - `urls`: List of strings, where each string is a URL (up to 10 URLs). - `keyword`: String, the keyword to count within the fetched web page content. Output - Dictionary where keys are the URLs and values are the counts of keyword occurrences. Constraints - You can assume the URLs provided are well-formed and valid. - Maximum of 10 URLs. - Implement proper concurrency handling and make sure to test your solution with asynchronous test cases. # Example ```python import asyncio import aiohttp async def fetch_and_process(urls, keyword): results = {} async def fetch(url): async with aiohttp.ClientSession() as session: try: async with session.get(url, timeout=2) as response: content = await response.text() count = content.lower().count(keyword.lower()) return url, count except asyncio.TimeoutError: print(f\\"Timeout fetching {url}\\") return url, 0 except Exception as e: print(f\\"Error fetching {url}: {str(e)}\\") return url, 0 tasks = [asyncio.create_task(fetch(url)) for url in urls] for task in asyncio.as_completed(tasks): url, count = await task results[url] = count return results # Example usage: urls = [ \\"https://example1.com\\", \\"https://example2.com\\", # up to 10 URLs ] keyword = \\"example\\" result = asyncio.run(fetch_and_process(urls, keyword)) print(result) ``` # Note - Ensure to import necessary modules such as `aiohttp`. - Test the function with various scenarios including valid URLs, URLs leading to timeouts, and handling of cancellation. - This implementation should demonstrate a solid grasp of asyncio for managing concurrent network requests, error handling, and result collection.","solution":"import asyncio import aiohttp async def fetch_and_process(urls, keyword): results = {} async def fetch(url): async with aiohttp.ClientSession() as session: try: async with session.get(url, timeout=2) as response: content = await response.text() count = content.lower().count(keyword.lower()) return url, count except asyncio.TimeoutError: print(f\\"Timeout fetching {url}\\") return url, 0 except Exception as e: print(f\\"Error fetching {url}: {str(e)}\\") return url, 0 tasks = [asyncio.create_task(fetch(url)) for url in urls] for task in asyncio.as_completed(tasks): url, count = await task results[url] = count return results"},{"question":"Objective The purpose of this task is to test your understanding of various Python built-in types, particularly numeric types, boolean operations, and sequence manipulations. You are required to implement specific functionalities that involve these concepts comprehensively. Problem Statement You need to implement a function `process_data(data, start, end)` that takes in the following parameters: - `data`: A list of integers - `start`: An integer - `end`: An integer The function should perform the following actions: 1. **Segment the Data**: Return a segment of the list `data` from index `start` to `end` (inclusive of `start` and exclusive of `end`). 2. **Compute Statistics**: - Calculate and return the sum, product, and average of the integers in the specified segment. - Return the count of elements in the specified segment. 3. **Boolean Analysis**: - Check if all elements in the specified segment are non-zero (using `all()`). - Check if any element in the specified segment is zero (using `any()`). 4. **Transform Data**: - Return a list where each integer in the specified segment is squared. Function Signature ```python def process_data(data: list[int], start: int, end: int) -> dict: pass ``` Expected Output The function should return a dictionary with the following keys and values: - `\'segment\'`: List of integers in the specified segment. - `\'sum\'`: The sum of the integers in the segment. - `\'product\'`: The product of the integers in the segment. - `\'average\'`: The average of the integers in the segment (float). - `\'count\'`: The count of integers in the segment. - `\'all_non_zero\'`: Boolean result of checking if all integers in the segment are non-zero. - `\'any_zero\'`: Boolean result of checking if any integer in the segment is zero. - `\'squared\'`: List of squared values of integers in the segment. Constraints - The `start` index will always be less than or equal to the `end` index. - The `data` list will be non-empty and contain at least one integer. - The integers in the `data` list can be negative, zero, or positive. Example ```python data = [1, 2, 3, 4, 5, 0, 6] start = 2 end = 6 result = process_data(data, start, end) print(result) ``` Output: ```python { \'segment\': [3, 4, 5, 0], \'sum\': 12, \'product\': 0, \'average\': 3.0, \'count\': 4, \'all_non_zero\': False, \'any_zero\': True, \'squared\': [9, 16, 25, 0] } ``` Notes - Carefully handle edge cases like empty lists and segments with only zero values.","solution":"def process_data(data, start, end): Processes data to return segment information, statistics, and boolean analysis. :param data: List of integers :param start: Start index (inclusive) :param end: End index (exclusive) :return: Dictionary containing segment details and statistics segment = data[start:end] # extract the segment segment_sum = sum(segment) # Calculate product product = 1 for number in segment: product *= number count = len(segment) average = segment_sum / count if count != 0 else 0 all_non_zero = all(segment) any_zero = any(num == 0 for num in segment) squared = [num ** 2 for num in segment] return { \'segment\': segment, \'sum\': segment_sum, \'product\': product, \'average\': average, \'count\': count, \'all_non_zero\': all_non_zero, \'any_zero\': any_zero, \'squared\': squared, }"},{"question":"# Distributed Matrix Multiplication in PyTorch Objective: Write a Python script using PyTorch\'s `torch.distributed` package to perform a distributed matrix multiplication. The task involves all processes contributing to the multiplication of two large matrices in a distributed manner. Details: 1. **Initialization**: - Initialize the distributed environment using TCP initialization. - Assume a world size of 4 (i.e., 4 processes). 2. **Matrix Setup**: - Each process should create a part of the matrices `A` and `B`. - All processes should combine their parts of `A` and `B` using the `all_gather` collective operation. 3. **Matrix Multiplication**: - After gathering, each process should independently perform matrix multiplication on the gathered matrices. - Use `all_reduce` to sum up the partial results from all processes to get the final matrix `C`. 4. **Debugging**: - Use `torch.distributed.monitored_barrier` to ensure that all processes complete their tasks successfully. 5. **Cleanup**: - Clean up resources by destroying the process group. Constraints: - Each process will create matrix parts of size `(N/2, N)` and `(N, N/2)`, where `N` is the total matrix size. Ensure `N` is divisible by 2. - Assume the rank and world size will be provided via environment variables `RANK` and `WORLD_SIZE`. Performance Requirements: - Ensure the script handles synchronization properly. - Focus on efficient use of collective operations to minimize communication overhead. Input: - The total matrix size `N` (an integer). Output: - Print the final resulting matrix `C` from the distributed matrix multiplication. ```python import os import torch import torch.distributed as dist import torch.multiprocessing as mp def distributed_matrix_multiplication(rank, world_size, N): # Initialize the process group dist.init_process_group(backend=\'gloo\', init_method=\'tcp://127.0.0.1:29500\', rank=rank, world_size=world_size) # Create part of matrices A and B A_part = torch.randn(N // 2, N) B_part = torch.randn(N, N // 2) # Gather parts of matrices A and B A_parts = [torch.zeros_like(A_part) for _ in range(world_size)] B_parts = [torch.zeros_like(B_part) for _ in range(world_size)] dist.all_gather(A_parts, A_part) dist.all_gather(B_parts, B_part) # Concatenate parts to form full matrices A and B A = torch.cat(A_parts, dim=0) B = torch.cat(B_parts, dim=1) # Perform matrix multiplication C_partial = torch.matmul(A, B) # Reduce partial results to get the final matrix C dist.all_reduce(C_partial, op=dist.ReduceOp.SUM) if rank == 0: print(\\"Resulting matrix C:\\") print(C_partial) # Ensure all processes complete dist.monitored_barrier() # Clean up dist.destroy_process_group() def main(): N = 8 # Example matrix size world_size = 4 # Number of processes os.environ[\'WORLD_SIZE\'] = str(world_size) mp.spawn(distributed_matrix_multiplication, args=(world_size, N), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main() ``` Note: - Ensure you have `torch`, `torchvision`, and other necessary packages installed. - Run the script using a multi-processing launch utility (e.g., `python -m torch.distributed.launch --nproc_per_node=4 script_name.py`).","solution":"import os import torch import torch.distributed as dist import torch.multiprocessing as mp def distributed_matrix_multiplication(rank, world_size, N): # Initialize the process group dist.init_process_group(backend=\'gloo\', init_method=\'tcp://127.0.0.1:29500\', rank=rank, world_size=world_size) # Create part of matrices A and B A_part = torch.randn(N // 2, N) B_part = torch.randn(N, N // 2) # Gather parts of matrices A and B A_parts = [torch.zeros_like(A_part) for _ in range(world_size)] B_parts = [torch.zeros_like(B_part) for _ in range(world_size)] dist.all_gather(A_parts, A_part) dist.all_gather(B_parts, B_part) # Concatenate parts to form full matrices A and B A = torch.cat(A_parts, dim=0) B = torch.cat(B_parts, dim=1) # Perform matrix multiplication C_partial = torch.matmul(A, B) # Reduce partial results to get the final matrix C dist.all_reduce(C_partial, op=dist.ReduceOp.SUM) if rank == 0: print(\\"Resulting matrix C:\\") print(C_partial) # Ensure all processes complete dist.monitored_barrier() # Clean up dist.destroy_process_group() def main(): N = 8 # Example matrix size world_size = 4 # Number of processes os.environ[\'WORLD_SIZE\'] = str(world_size) mp.spawn(distributed_matrix_multiplication, args=(world_size, N), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main()"},{"question":"Matrix Multiplication Using Shared Memory Objective: Write a Python program that uses the `multiprocessing.shared_memory` module to perform matrix multiplication using shared memory to store the matrices. Details: - Create two shared memory blocks to store the input matrices (Matrix A and Matrix B). - Create a shared memory block to store the result matrix (Matrix C). - Use multiple processes to compute the matrix multiplication in parallel. - Ensure proper lifecycle management of the shared memory blocks and processes. Input: - Two matrices A and B, where the number of columns in A is equal to the number of rows in B. Output: - The resultant matrix C, which is the product of A and B. Constraints: - The matrix sizes will not exceed 50x50. - All elements in the matrices are integers. - Ensure that matrix multiplication is performed efficiently using parallel processing. Requirements: 1. Define a function `matrix_multiplication(A, B)` that performs the following steps: - Allocates shared memory blocks for matrices A, B, and C. - Copies matrices A and B into the shared memory blocks. - Spawns multiple processes to compute the product matrix C in parallel. - Returns the resultant matrix C. 2. Each process should compute a part of the resultant matrix C and write its results back to the shared memory block. 3. Ensure proper cleanup of shared memory blocks after computation. Example: ```python from multiprocessing import shared_memory, Process import numpy as np def matrix_multiplication(A, B): # Your implementation here # Example matrices A = np.array([[1, 2], [3, 4]]) B = np.array([[5, 6], [7, 8]]) # Perform matrix multiplication result = matrix_multiplication(A, B) # Output the result matrix print(result) # Expected output: [[19, 22], [43, 50]] ``` **Note:** - You may use NumPy arrays for matrix operations. - Pay attention to synchronization to prevent race conditions. - Ensure all processes terminate gracefully.","solution":"from multiprocessing import Process, shared_memory import numpy as np def worker(A_name, B_name, C_name, a_shape, b_shape, start_row, end_row): existing_shm_A = shared_memory.SharedMemory(name=A_name) existing_shm_B = shared_memory.SharedMemory(name=B_name) existing_shm_C = shared_memory.SharedMemory(name=C_name) A = np.ndarray(a_shape, dtype=np.int32, buffer=existing_shm_A.buf) B = np.ndarray(b_shape, dtype=np.int32, buffer=existing_shm_B.buf) C = np.ndarray((a_shape[0], b_shape[1]), dtype=np.int32, buffer=existing_shm_C.buf) for i in range(start_row, end_row): for j in range(b_shape[1]): C[i, j] = np.dot(A[i, :], B[:, j]) existing_shm_A.close() existing_shm_B.close() existing_shm_C.close() def matrix_multiplication(A, B): A = np.array(A, dtype=np.int32) B = np.array(B, dtype=np.int32) a_shape = A.shape b_shape = B.shape assert a_shape[1] == b_shape[0], \\"Number of columns in A must be equal to number of rows in B\\" shm_A = shared_memory.SharedMemory(create=True, size=A.nbytes) shm_B = shared_memory.SharedMemory(create=True, size=B.nbytes) shm_C = shared_memory.SharedMemory(create=True, size=a_shape[0] * b_shape[1] * np.dtype(np.int32).itemsize) A_shared = np.ndarray(a_shape, dtype=np.int32, buffer=shm_A.buf) B_shared = np.ndarray(b_shape, dtype=np.int32, buffer=shm_B.buf) C_shared = np.ndarray((a_shape[0], b_shape[1]), dtype=np.int32, buffer=shm_C.buf) np.copyto(A_shared, A) np.copyto(B_shared, B) num_processes = 4 rows_per_process = a_shape[0] // num_processes processes = [] for i in range(num_processes): start_row = i * rows_per_process end_row = (i + 1) * rows_per_process if i != num_processes - 1 else a_shape[0] p = Process(target=worker, args=(shm_A.name, shm_B.name, shm_C.name, a_shape, b_shape, start_row, end_row)) processes.append(p) p.start() for p in processes: p.join() result = np.array(C_shared) shm_A.close() shm_A.unlink() shm_B.close() shm_B.unlink() shm_C.close() shm_C.unlink() return result.tolist()"},{"question":"# Objective Your task is to implement a set of functions that mimic some of the `PyLongObject` related operations in Python. This will demonstrate your understanding of Python\'s integer capabilities and error handling. # Problem Statement Implement the following functions in Python: 1. `from_long(v: int) -> int`: - Converts a normal integer to Python\'s built-in integer. - This function simply returns the parameter `v`. - **Input:** An integer `v`. - **Output:** The same integer `v`. 2. `from_unsigned_long(v: int) -> int`: - Converts an unsigned integer (non-negative) to Python\'s built-in integer. - This function should raise a `ValueError` if the input integer `v` is negative. - **Input:** An unsigned integer `v`. - **Output:** The same integer `v`, if it is non-negative. Raise `ValueError` if `v` is negative. 3. `from_double(v: float) -> int`: - Converts the integer part of a floating-point number `v` to a Python integer. - **Input:** A floating-point number `v`. - **Output:** The integer part of `v`. 4. `from_string(s: str, base: int = 10) -> int`: - Converts a string `s` representing a number in a given base to a Python integer. - The function should handle bases from 2 to 36. - If the base is 0, the function should interpret the string based on common integer literals (0x for hexadecimal, 0o for octal, etc.). - The function should raise a `ValueError` if the string is not a valid number in the given base. - **Input:** - A string `s` representing the number. - An integer `base` (optional, default is 10). - **Output:** The integer represented by the string in the given base. # Constraints - Do not use any external libraries or built-in functions that directly perform these conversions. - You may assume that input values will be in a reasonable range to prevent overflow in typical use cases. - The `from_string` function should handle leading spaces and validate the input correctly. # Example ```python print(from_long(123)) # Output: 123 print(from_unsigned_long(123)) # Output: 123 print(from_unsigned_long(-123)) # Output: Raise ValueError print(from_double(123.45)) # Output: 123 print(from_string(\\"123\\", 10)) # Output: 123 print(from_string(\\"7B\\", 16)) # Output: 123 print(from_string(\\"017\\", 0)) # Output: 15 (interpreted as octal) print(from_string(\\"0x1f\\", 0)) # Output: 31 (interpreted as hexadecimal) ``` Implement these functions with proper error handling and validation where necessary.","solution":"def from_long(v: int) -> int: Converts a normal integer to Python\'s built-in integer. Simply returns the parameter v. return v def from_unsigned_long(v: int) -> int: Converts an unsigned integer (non-negative) to Python\'s built-in integer. Raises a ValueError if the input integer v is negative. if v < 0: raise ValueError(\\"negative value not allowed for unsigned long\\") return v def from_double(v: float) -> int: Converts the integer part of a floating-point number v to a Python integer. return int(v) def from_string(s: str, base: int = 10) -> int: Converts a string s representing a number in a given base to a Python integer. Should handle bases from 2 to 36 and base 0 for common integer literals. Raises a ValueError if the string is not a valid number in the given base. s = s.strip() # Handle leading and trailing spaces try: return int(s, base) except ValueError: raise ValueError(f\\"invalid literal for int() with base {base}: {s}\\")"},{"question":"Advanced Python: Module Execution Using `runpy` # Objective In this question, you will demonstrate your understanding of how to use the `runpy` module to execute Python modules dynamically. You will implement a function that utilizes `runpy.run_module` and `runpy.run_path` to execute a Python module given its module name and a filesystem path respectively. # Problem Statement You must implement a function `execute_module` that takes three arguments: 1. `mod_name` (str): The absolute module name to be executed using `runpy.run_module`. 2. `path_name` (str): The filesystem location of the Python code to be executed using `runpy.run_path`. 3. `use_path` (bool): A flag that determines which function to use for execution. If `True`, use `runpy.run_path`; otherwise, use `runpy.run_module`. The function should execute the appropriate module or script and return the resulting module globals dictionary. You should also handle the pre-population of a custom global variable `custom_var` with the value `\\"pre-populated\\"` before executing the code. # Function Signature ```python def execute_module(mod_name: str, path_name: str, use_path: bool) -> dict: pass ``` # Input - `mod_name` (str): An absolute module name, e.g., `\\"my_module\\"`. - `path_name` (str): A filesystem path to a Python script or module, e.g., `\\"/path/to/my_script.py\\"`. - `use_path` (bool): A flag to choose between `run_module` and `run_path`. # Output - A dictionary containing the global variables of the executed module/script. Ensure that `custom_var` is included in the globals. # Constraints - You may assume the module names and paths given are valid and accessible. - The execution of the module must reflect in the current process environment. - You do not need to handle exceptions; assume valid inputs are provided. # Example Usage ```python # Example module and path (assuming they exist and are correct) mod_name = \\"my_package.my_module\\" path_name = \\"/path/to/my_package/my_script.py\\" use_path_flag = True result_globals = execute_module(mod_name, path_name, use_path_flag) print(result_globals[\'custom_var\']) # Output should be \\"pre-populated\\" ``` # Notes - Review the `runpy` module documentation to understand the implications of using `run_module` and `run_path`. - Consider the special global variables set by `runpy` to ensure the correct execution environment.","solution":"import runpy def execute_module(mod_name: str, path_name: str, use_path: bool) -> dict: custom_globals = { \'custom_var\': \'pre-populated\' } if use_path: result_globals = runpy.run_path(path_name, init_globals=custom_globals) else: result_globals = runpy.run_module(mod_name, init_globals=custom_globals) return result_globals"},{"question":"**Question: Implementing a Custom URL Opener with Advanced Features** **Objective:** Demonstrate your understanding of the `urllib.request` module by creating a custom URL opener that incorporates proxy settings, handles basic HTTP authentication, processes redirections conditionally, and manages SSL contexts. **Requirements:** 1. **Function Name:** `custom_url_opener` 2. **Parameters:** - `url` (string): The target URL to be opened. - `method` (string): The HTTP method to use (GET, POST, or PUT). - `data` (bytes, optional): Data to send in the request body (default is `None`). - `headers` (dictionary, optional): Headers to include in the request (default is an empty dictionary). - `use_proxy` (boolean, optional): Flag to indicate whether to use a proxy (default is `True`). - `proxy` (dictionary, optional): Proxy settings for the request, mapping protocols to proxy URLs (default is an empty dictionary). - `auth` (tuple, optional): A tuple containing username and password for basic authentication (default is `None`). - `context` (ssl.SSLContext, optional): SSL context for handling HTTPS requests (default is `None`). 3. **Returns:** - A tuple containing the response status code, reason phrase, and response body as a string. 4. **Constraints:** - Handle exceptions like `URLError` and `HTTPError` gracefully, returning appropriate error messages. - Support for `http`, `https`, and `ftp` URLs. - Conditional redirection: Automatically follow redirects for status codes 301, 302, and 303, but not for 307. **Example Usage:** ```python url = \\"http://www.example.com/api/resource\\" method = \\"POST\\" data = b\'{\\"key\\": \\"value\\"}\' headers = { \\"Content-Type\\": \\"application/json\\", \\"User-Agent\\": \\"CustomAgent/1.0\\" } proxy = { \\"http\\": \\"http://proxy.example.com:8080\\", \\"https\\": \\"https://proxy.example.com:8080\\" } auth = (\\"username\\", \\"password\\") response_code, response_reason, response_body = custom_url_opener( url=url, method=method, data=data, headers=headers, use_proxy=True, proxy=proxy, auth=auth ) print(f\\"Response Code: {response_code}\\") print(f\\"Response Reason: {response_reason}\\") print(f\\"Response Body: {response_body}\\") ``` **Additional Information:** - Use `urlopen`, `Request`, and relevant handlers from the `urllib.request` module. - Implement proper handling for proxy settings and authentication using appropriate handlers like `ProxyHandler` and `HTTPBasicAuthHandler`. - Ensure secure handling of SSL contexts if provided. **Guidance:** The function `custom_url_opener` should be implemented to handle the specified complexities, ensuring a clear understanding of how to work with the `urllib.request` module\'s features.","solution":"import urllib.request import urllib.error import ssl import base64 def custom_url_opener(url, method=\\"GET\\", data=None, headers={}, use_proxy=True, proxy={}, auth=None, context=None): Open a URL using advanced features like proxy settings, basic authentication, redirection handling, and SSL context management. Parameters: - url (string): The target URL to be opened. - method (string): The HTTP method to use (GET, POST, or PUT). - data (bytes, optional): Data to send in the request body (default is None). - headers (dictionary, optional): Headers to include in the request (default is an empty dictionary). - use_proxy (boolean, optional): Flag to indicate whether to use a proxy (default is True). - proxy (dictionary, optional): Proxy settings for the request, mapping protocols to proxy URLs (default is an empty dictionary). - auth (tuple, optional): A tuple containing username and password for basic authentication (default is None). - context (ssl.SSLContext, optional): SSL context for handling HTTPS requests (default is None). Returns: - A tuple containing the response status code, reason phrase, and response body as a string. try: # Create request object req = urllib.request.Request(url, data=data, headers=headers, method=method) # Set up proxy handler if use_proxy is True and proxy is provided if use_proxy and proxy: proxy_handler = urllib.request.ProxyHandler(proxy) opener = urllib.request.build_opener(proxy_handler) else: opener = urllib.request.build_opener() # Add basic auth credentials if provided if auth: username, password = auth auth_string = f\\"{username}:{password}\\" base64_bytes = base64.b64encode(auth_string.encode(\'utf-8\')) auth_header_value = f\\"Basic {base64_bytes.decode(\'utf-8\')}\\" req.add_header(\\"Authorization\\", auth_header_value) # Manage SSL context if context: opener = urllib.request.build_opener(urllib.request.HTTPSHandler(context=context)) # Override default redirect handlers conditionally class NoRedirectHandler(urllib.request.HTTPRedirectHandler): def redirect_request(self, req, fp, code, msg, headers, newurl): if code in [301, 302, 303]: return super().redirect_request(req, fp, code, msg, headers, newurl) else: return None opener = urllib.request.build_opener(NoRedirectHandler(), *opener.handlers) response = opener.open(req) # Return status code, reason, and response body return response.status, response.reason, response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return e.code, e.reason, e.read().decode(\'utf-8\') except urllib.error.URLError as e: return None, str(e.reason), \'\'"},{"question":"# Advanced Coding Assessment Question: Custom C Extension Type in Python Objective: To test your deep understanding of creating custom Python objects, managing their attributes, and implementing various object protocols using C extension definitions. This challenge will involve implementing specific methods and managing memory for custom Python objects. Task: Design and implement a custom Python C extension type `MyCustomType` that models a numerical data point with specific attributes and methods. Requirements: 1. **Attributes:** - `value`: A floating-point number. - `name`: A string to describe the data point. 2. **Methods:** - Implement a constructor to initialize the object with `value` and `name`. - Implement `tp_repr` to return a string in the format: `\\"DataPoint(name=<name>, value=<value>)\\"`. - Implement comparison operators (`tp_richcompare`) to compare `value` attributes (`<`, `<=`, `==`, `!=`, `>`, `>=`). 3. **Memory Management:** - Override `tp_dealloc` to manage memory clean-up. 4. **Attribute Management:** - Implement getattr and setattr to manage `value` and `name`. - Ensure getting and setting attributes conform to correct types (float for `value` and string for `name`). 5. **Additional Methods:** - Provide a method to increment the `value` attribute by a given increment. Constraints: - The `value` should be a non-negative float always. - Raise appropriate exceptions for invalid operations or values properly. Example: ```python # Example Usage in Python import myextension # Create an instance data_point = myextension.MyCustomType(name=\\"Temperature\\", value=25.5) # Representation print(repr(data_point)) # DataPoint(name=Temperature, value=25.5) # Attribute Access print(data_point.value) # 25.5 print(data_point.name) # Temperature # Attribute Mutation data_point.value = 30.0 data_point.name = \\"Pressure\\" # Invalid Attribute Mutation should raise TypeError try: data_point.value = \\"invalid\\" except TypeError as e: print(e) # TypeError: value attribute must be a float # Comparison data_point1 = myextension.MyCustomType(name=\\"Humidity\\", value=45.0) print(data_point > data_point1) # Assuming value comparison - False # Increment Method data_point.increment_value(5.0) print(data_point.value) # 35.0 ``` Submission Format: Submit a Python file with a source code for the C extension module (`myextension.c`), including make or compile instructions, and a `setup.py` if necessary, to test the code within Python effectively. **Hints:** - You can refer to implementing `PyTypeObject` and managing memory using the `tp_alloc` and `tp_free` functions. - Utilize `PyType_Ready()` for setting methods and attributes appropriately. - Use `PyErr_SetString` to handle error situations effectively.","solution":"class MyCustomType: def __init__(self, name, value): if not isinstance(name, str): raise TypeError(\\"name attribute must be a string\\") if not isinstance(value, (int, float)) or value < 0: raise ValueError(\\"value attribute must be a non-negative number\\") self._name = name self._value = float(value) @property def name(self): return self._name @name.setter def name(self, name): if not isinstance(name, str): raise TypeError(\\"name attribute must be a string\\") self._name = name @property def value(self): return self._value @value.setter def value(self, value): if not isinstance(value, (int, float)) or value < 0: raise ValueError(\\"value attribute must be a non-negative number\\") self._value = float(value) def __repr__(self): return f\\"DataPoint(name={self._name}, value={self._value})\\" def __eq__(self, other): return self._value == other._value def __lt__(self, other): return self._value < other._value def __le__(self, other): return self._value <= other._value def __gt__(self, other): return self._value > other._value def __ge__(self, other): return self._value >= other._value def increment_value(self, increment): if not isinstance(increment, (int, float)) or increment < 0: raise ValueError(\\"increment must be a non-negative number\\") self._value += float(increment)"},{"question":"# **Python Coding Assessment Question: HTTP Request Handler** Objective Demonstrate your understanding of the `urllib.request` module in Python by implementing a function that performs an HTTP GET request with custom headers, handles potential errors, and returns information about the request and response. Task Implement a function `fetch_url_info(url: str, headers: dict = None) -> dict` that: 1. Performs an HTTP GET request to the given URL. 2. Allows the user to pass custom headers. 3. Handles `URLError` and `HTTPError` exceptions. 4. Returns a dictionary with the following keys: - `status_code`: the HTTP status code of the response. - `headers`: a dictionary of the response headers. - `url`: the final URL after any redirects. - `content`: the first 500 characters of the response content (as a string). Input - `url` (str): The URL to request. - `headers` (dict, optional): A dictionary of custom HTTP headers to pass with the request. Default is `None`. Output - A dictionary containing: - `status_code` (int): The HTTP status code of the response. - `headers` (dict): The response headers. - `url` (str): The final URL after any redirects. - `content` (str): The first 500 characters of the response content. Constraints - Do not use any libraries other than `urllib.request` and `urllib.error`. Examples ```python # Example 1 url = \\"http://www.example.com\\" headers = {\\"User-Agent\\": \\"Mozilla/5.0\\"} result = fetch_url_info(url, headers) print(result) # Output format (actual content may vary, example provided): # { # \'status_code\': 200, # \'headers\': {\'Content-Type\': \'text/html; charset=UTF-8\', ...}, # \'url\': \'http://www.example.com\', # \'content\': \'<!doctype html>... (might be longer string)\' # } # Example 2 (handling error) url = \\"http://www.nonexistentwebsite1234.com\\" result = fetch_url_info(url) print(result) # Output format (message): # { # \'status_code\': None, # \'headers\': {}, # \'url\': None, # \'content\': \'URLError: <specific error message>\' # } ``` Implementation Notes - Use a try-except block to handle `URLError` and `HTTPError`. - If an error occurs, return the error message in the `content` field, and set `status_code` and `url` to `None`. - Use the `info()` method from the response to get the headers. - Use the `geturl()` method from the response to get the final URL after any redirects. - The content should be read and sliced to the first 500 characters.","solution":"import urllib.request import urllib.error def fetch_url_info(url: str, headers: dict = None) -> dict: Performs an HTTP GET request to the given URL, optionally using custom headers. Args: url (str): The URL to request. headers (dict, optional): Custom headers to use for the request. Returns: dict: A dictionary containing status code, headers, final URL, and truncated response content. if headers is None: headers = {} req = urllib.request.Request(url, headers=headers) response_info = { \\"status_code\\": None, \\"headers\\": {}, \\"url\\": None, \\"content\\": \\"\\" } try: with urllib.request.urlopen(req) as response: status_code = response.getcode() response_headers = dict(response.info()) final_url = response.geturl() content = response.read(500).decode(\'utf-8\', errors=\'ignore\') response_info[\'status_code\'] = status_code response_info[\'headers\'] = response_headers response_info[\'url\'] = final_url response_info[\'content\'] = content except urllib.error.HTTPError as e: response_info[\'content\'] = f\'HTTPError: {e.reason}\' except urllib.error.URLError as e: response_info[\'content\'] = f\'URLError: {e.reason}\' return response_info"},{"question":"# Question: Customizing Seaborn Plots You are given a dataset with daily temperatures recorded in two cities over a month. Your task is to visualize this data using Seaborn and customize your plots to make them visually appealing and suitable for a professional presentation. Dataset ```python import pandas as pd # Sample Data data = { \\"Day\\": range(1, 31), \\"City_A\\": [32, 35, 30, 28, 33, 31, 29, 34, 36, 35, 32, 30, 31, 29, 28, 32, 34, 33, 32, 30, 31, 29, 28, 32, 34, 33, 32, 31, 30, 29], \\"City_B\\": [22, 25, 23, 21, 24, 26, 23, 22, 24, 23, 21, 22, 24, 25, 23, 22, 24, 23, 21, 24, 25, 23, 22, 24, 22, 23, 25, 24, 23, 21] } temperature_df = pd.DataFrame(data) ``` Requirements 1. **Plot the Data:** - Create a line plot showing the daily temperatures for both City A and City B. 2. **Apply Seaborn Themes:** - Use the `darkgrid` style for the plot background. 3. **Customization:** - Remove the top and right spines from the plot. - Set the context to `talk` to make the plot suitable for presentations. - Temporarily set the style to `whitegrid` only for the City A temperature line. 4. **Enhance Readability:** - Increase the font size of the plot title to 20. - Change the line width of both lines to 2.5. - Set different colors for each cityâ€™s line. 5. **Display Plot:** - Ensure that the plot includes a legend and labels for both axes. - The plot should have a title: \\"Daily Temperatures in City A and City B\\". Constraints - Use the Seaborn library for all styling and customization. Expected Output A well-customized Seaborn plot that meets the requirements outlined above. Code Template ```python import seaborn as sns import matplotlib.pyplot as plt # Assume `temperature_df` is already loaded as given # Your implementation here # Create the plot plt.figure(figsize=(10, 6)) # Temporarily set style for City A with sns.axes_style(\\"whitegrid\\"): sns.lineplot(x=\\"Day\\", y=\\"City_A\\", data=temperature_df, label=\\"City A\\", linewidth=2.5) # Set theme and other customizations for the rest of the plot sns.set_theme(style=\\"darkgrid\\") sns.lineplot(x=\\"Day\\", y=\\"City_B\\", data=temperature_df, label=\\"City B\\", linewidth=2.5, color=\\"red\\") # Remove spines sns.despine() # Set the context sns.set_context(\\"talk\\") # Increase title font size plt.title(\\"Daily Temperatures in City A and City B\\", fontsize=20) plt.xlabel(\\"Day\\") plt.ylabel(\\"Temperature (Â°C)\\") plt.legend() # Show the plot plt.show() ``` Sample Output (Your plot should look similar to what is described under \\"Expected Output\\".)","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Sample Data data = { \\"Day\\": range(1, 31), \\"City_A\\": [32, 35, 30, 28, 33, 31, 29, 34, 36, 35, 32, 30, 31, 29, 28, 32, 34, 33, 32, 30, 31, 29, 28, 32, 34, 33, 32, 31, 30, 29], \\"City_B\\": [22, 25, 23, 21, 24, 26, 23, 22, 24, 23, 21, 22, 24, 25, 23, 22, 24, 23, 21, 24, 25, 23, 22, 24, 22, 23, 25, 24, 23, 21] } temperature_df = pd.DataFrame(data) # Create the plot plt.figure(figsize=(10, 6)) # Temporarily set style for City A with sns.axes_style(\\"whitegrid\\"): sns.lineplot(x=\\"Day\\", y=\\"City_A\\", data=temperature_df, label=\\"City A\\", linewidth=2.5, color=\\"blue\\") # Set theme and other customizations for the rest of the plot sns.set_theme(style=\\"darkgrid\\") sns.lineplot(x=\\"Day\\", y=\\"City_B\\", data=temperature_df, label=\\"City B\\", linewidth=2.5, color=\\"red\\") # Remove spines sns.despine() # Set the context sns.set_context(\\"talk\\") # Increase title font size plt.title(\\"Daily Temperatures in City A and City B\\", fontsize=20) plt.xlabel(\\"Day\\") plt.ylabel(\\"Temperature (Â°C)\\") plt.legend() # Show the plot plt.show()"},{"question":"# Advanced Python Coding Assessment Question Background Python\'s `dataclasses` module provides a decorator and functions for automatically adding special methods to user-defined classes. These methods include `__init__`, `__repr__`, `__eq__`, and others. Context management allows code to set up a runtime context and ensure proper cleanup, typically using the `with` statement. In this task, you will create a class that utilizes both `dataclasses` for simple and immutable data structures and `contextlib` to manage resources efficiently. Task 1. Define a `@dataclass` named `Resource` with the following attributes: - `name: str` - `value: int`, which should be immutable (i.e., it cannot be changed once an instance is created) 2. Implement a context manager class named `ResourceHandler` that: - Initializes with a list of `Resource` objects. - On entering the context, prints \\"Entering resource context\\". - On exiting the context: - Prints \\"Exiting resource context\\". - Ensures all resources\' values are reset to 0. 3. Demonstrate the use of the `ResourceHandler` context manager. Create a list of resources, use the context manager to manage these resources, and show the resources before and after the context manager block. Constraints - Use `dataclasses` for defining the `Resource` class. - Ensure `Resource.value` is immutable after creation. - Use `contextlib` or appropriate context management techniques for the `ResourceHandler`. Example ```python from contextlib import contextmanager from dataclasses import dataclass from typing import List @dataclass(frozen=True) class Resource: name: str value: int class ResourceHandler: def __init__(self, resources: List[Resource]): self.resources = resources def __enter__(self): print(\\"Entering resource context\\") return self.resources def __exit__(self, exc_type, exc_value, traceback): print(\\"Exiting resource context\\") for resource in self.resources: resource.value = 0 # This should reset the value of each resource return False # Creating a list of resources resources = [Resource(name=\\"resource1\\", value=10), Resource(name=\\"resource2\\", value=20)] # Using the context manager print(\\"Resources before context:\\") for resource in resources: print(resource) with ResourceHandler(resources) as managed_resources: print(\\"Resources inside context:\\") for resource in managed_resources: print(resource) print(\\"Resources after context:\\") for resource in resources: print(resource) ``` Note: The output example should ensure the immutability constraint conflicts are appropriately resolved, such as using a supposed way to reset values or providing a different cleanup strategy if immutability enforces stricter restrictions. Expected Output: ```python Resources before context: Resource(name=\'resource1\', value=10) Resource(name=\'resource2\', value=20) Entering resource context Resources inside context: Resource(name=\'resource1\', value=10) Resource(name=\'resource2\', value=20) Exiting resource context Resources after context: Resource(name=\'resource1\', value=0) Resource(name=\'resource2\', value=0) ``` Hint: To handle the immutability of the `Resource` class effectively, consider the design choices around modifying or cloning the `Resource` instances inside the `ResourceHandler`.","solution":"from dataclasses import dataclass, replace from typing import List from contextlib import AbstractContextManager @dataclass(frozen=True) class Resource: name: str value: int class ResourceHandler(AbstractContextManager): def __init__(self, resources: List[Resource]): self.resources = resources def __enter__(self): print(\\"Entering resource context\\") return self.resources def __exit__(self, exc_type, exc_value, traceback): print(\\"Exiting resource context\\") # Create a new list of resources with value reset to 0 self.resources[:] = [replace(resource, value=0) for resource in self.resources] return False # Demonstration of the solution resources = [Resource(name=\\"resource1\\", value=10), Resource(name=\\"resource2\\", value=20)] print(\\"Resources before context:\\") for resource in resources: print(resource) with ResourceHandler(resources) as managed_resources: print(\\"Resources inside context:\\") for resource in managed_resources: print(resource) print(\\"Resources after context:\\") for resource in resources: print(resource)"},{"question":"XML Pull Parsing with Partial DOM Expansion Problem Statement You are required to implement a function `filter_expensive_items(xml_string: str, price_threshold: int) -> str` that filters and returns items in an XML string where the price exceeds a specific threshold. The function should parse the XML content using the `xml.dom.pulldom` module, expand the relevant nodes to ensure all child elements are included, and return the filtered XML items as a single string. Function Signature ```python def filter_expensive_items(xml_string: str, price_threshold: int) -> str: pass ``` Parameters - `xml_string`: A string containing XML data. - `price_threshold`: An integer representing the price threshold for filtering items. Returns - A string that contains the filtered XML items, each ensuring that all child elements are included. Example ```python xml_content = \'\'\' <sales> <item price=\\"30\\"> <name>Item 1</name> <description>Low-cost item</description> </item> <item price=\\"75\\"> <name>Item 2</name> <description>Expensive item</description> </item> <item price=\\"120\\"> <name>Item 3</name> <description>Luxury item</description> </item> </sales> \'\'\' output = filter_expensive_items(xml_content, 50) print(output) ``` **Expected Output:** ```xml <item price=\\"75\\"> <name>Item 2</name> <description>Expensive item</description> </item> <item price=\\"120\\"> <name>Item 3</name> <description>Luxury item</description> </item> ``` Constraints - Assume all prices in the `price` attribute are valid integers. - The XML structure is guaranteed to have items with a `price` attribute and no nested `item` elements inside other `item` elements. - The function should not use traditional DOM or SAX parsers directly but should leverage `xml.dom.pulldom`. # Implementation Notes 1. Use the `pulldom.parseString()` or `pulldom.parse()` function to parse the provided XML string. 2. Loop through the events and nodes, and identify `START_ELEMENT` events for `item`. 3. Use the `expandNode()` method to ensure the full element (with children) is expanded. 4. Check the `price` attribute, and if it exceeds the `price_threshold`, add the expanded XML string of the element to the result. 5. Return the concatenated XML strings of filtered items.","solution":"from xml.dom import pulldom def filter_expensive_items(xml_string: str, price_threshold: int) -> str: Filters items from the XML string where the price exceeds the given threshold. Parameters: - xml_string: A string containing the XML data. - price_threshold: An integer representing the price threshold. Returns: - A string containing the filtered XML items. doc = pulldom.parseString(xml_string) result = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \\"item\\": price = int(node.getAttribute(\\"price\\")) if price > price_threshold: doc.expandNode(node) result.append(node.toxml()) return \'\'.join(result)"},{"question":"Write a function `process_numbers` that takes in a list of integers and performs the following operations: 1. If the length of the list is zero, it returns \\"Empty List\\". 2. If the length of the list is odd, it calculates the sum of all integers in the list and checks if it is positive. If it is, it adds the result to the list and returns the updated list. Use an `assert` statement to ensure the result is positive before adding. 3. If the length of the list is even, it removes the first occurrence of the smallest number in the list and returns the updated list. 4. Ensure the function uses an annotated variable to store the sum of the list when the list length is odd. 5. Ensure that your code handles the possible exceptions that might occur during the operations. # Example ```python process_numbers([1, 2, 3]) # Output: [1, 2, 3, 6] process_numbers([4, 1, 3, 2]) # Output: [4, 3, 2] ``` # Constraints - The input list can contain both positive and negative integers. - The input list will have a maximum length of 1000. # Function Signature ```python def process_numbers(numbers: list) -> list: pass ```","solution":"from typing import List, Union def process_numbers(numbers: List[int]) -> Union[List[int], str]: Processes a list of integers according to the specified conditions: 1. If empty, returns \\"Empty List\\". 2. If length is odd, calculates the sum and adds it if positive. 3. If length is even, removes the first occurrence of the smallest number. try: if len(numbers) == 0: return \\"Empty List\\" if len(numbers) % 2 != 0: total: int = sum(numbers) assert total > 0, \\"Sum is not positive\\" numbers.append(total) else: smallest_number = min(numbers) numbers.remove(smallest_number) return numbers except (TypeError, ValueError, AssertionError) as e: return f\\"Error: {str(e)}\\""},{"question":"Data Classes and Context Managers in Python **Objective**: Demonstrate your ability to use Python\'s `dataclasses` and `contextlib` modules by designing a class that manages a resource (e.g., a file handle) and logs information related to its operations. Problem Statement You are tasked with implementing a class `ResourceHandler` that manages a file handle using the `with` statement (context manager) and logs the operations performed during its lifecycle. Use Python\'s `dataclass` for the class definition and `contextlib` to create a context manager for file handling. The `ResourceHandler` should meet the following requirements: 1. **Initialization**: Upon initialization, it should accept a file path and an optional mode (default is \'r\', read mode). 2. **Logging**: Implement logging where each operation (opening, reading, writing, closing the file) is logged to a list attribute named `operations_log`. 3. **File Operations**: Provide methods to read from and write to the file. 4. **Context Management**: Use `contextlib` to ensure that the file resource is properly managed when entering and exiting the `with` block. 5. **Test Cases**: Provide test cases that demonstrate the usage of the `ResourceHandler` for reading from and writing to a file. Expected Input and Output Formats **Input**: - File path: A string representing the path to the file. - Mode: An optional string indicating the file mode (default is \'r\'). **Output**: - Operations log: A list of strings where each entry records a performed operation. Constraints or Limitations - Ensure that the file is properly closed after operations, even if an exception occurs. - Avoid external libraries; use only Python standard libraries. Example Usage ```python # Example usage of ResourceHandler file_path = \'example.txt\' # Writing to a file with ResourceHandler(file_path, \'w\') as handler: handler.write(\'Hello, World!\') # Reading from a file with ResourceHandler(file_path, \'r\') as handler: content = handler.read() # Checking operation log handler = ResourceHandler(file_path, \'r\') with handler: handler.read() print(handler.operations_log) # Should log operations like opening, reading, and closing the file. ``` Implementation Details 1. Use `dataclasses` to define the `ResourceHandler` class. 2. Implement context management methods `__enter__` and `__exit__` using `contextlib`. 3. Ensure all file operations are logged correctly. Your task is to complete the implementation of the `ResourceHandler` class to fulfill the requirements mentioned above.","solution":"from dataclasses import dataclass, field from contextlib import contextmanager @dataclass class ResourceHandler: file_path: str mode: str = \'r\' operations_log: list = field(default_factory=list) file_handle: object = field(init=False, default=None) @contextmanager def open(self): try: self.operations_log.append(f\\"Opening file {self.file_path} in {self.mode} mode\\") self.file_handle = open(self.file_path, self.mode) yield self.file_handle finally: if self.file_handle: self.operations_log.append(\\"Closing file\\") self.file_handle.close() self.file_handle = None def read(self): with self.open() as file: self.operations_log.append(\\"Reading from file\\") return file.read() def write(self, content): with self.open() as file: self.operations_log.append(\\"Writing to file\\") file.write(content)"},{"question":"**Question:** Write a Python function named `compare_future_features` that takes two feature names as input and returns a string indicating which feature became mandatory earlier. If both features became mandatory in the same version, the function should return a string indicating that they were finalized at the same time. Assume the input feature names are always valid and are present in the `__future__.py` file. # Function Signature ```python def compare_future_features(feature1: str, feature2: str) -> str: ``` # Input - `feature1`: A string representing the name of the first feature. - `feature2`: A string representing the name of the second feature. # Output - A string that is one of the following: - `\\"Feature <feature1> became mandatory earlier than <feature2>\\"` - `\\"Feature <feature2> became mandatory earlier than <feature1>\\"` - `\\"Both features became mandatory at the same time\\"` # Constraints 1. Both `feature1` and `feature2` are guaranteed to be valid feature names present in the `__future__` module. # Example ```python # Assuming the following data from __future__: # nested_scopes = _Feature((2, 1, 0, \\"beta\\", 1), (2, 2, 0, \\"final\\", 0), 0x1000) # generators = _Feature((2, 2, 0, \\"alpha\\", 1), (2, 3, 0, \\"final\\", 0), 0x2000) print(compare_future_features(\\"nested_scopes\\", \\"generators\\")) # Output: \\"Feature nested_scopes became mandatory earlier than generators\\" print(compare_future_features(\\"generators\\", \\"nested_scopes\\")) # Output: \\"Feature nested_scopes became mandatory earlier than generators\\" print(compare_future_features(\\"nested_scopes\\", \\"nested_scopes\\")) # Output: \\"Both features became mandatory at the same time\\" ``` # Notes 1. To compare the `MandatoryRelease` tuples of the features, consider them as version numbers, thus `(2, 2, 0, \\"final\\", 0)` is earlier than `(2, 3, 0, \\"final\\", 0)`.","solution":"from __future__ import nested_scopes, generators def compare_future_features(feature1: str, feature2: str) -> str: Compares the mandatory release versions of two __future__ features. Args: feature1 (str): The name of the first feature. feature2 (str): The name of the second feature. Returns: str: A string indicating which feature became mandatory earlier. Or a string indicating that both features became mandatory at the same time. # Mapping feature names to their corresponding __future__ objects feature_map = { \\"nested_scopes\\": nested_scopes, \\"generators\\": generators, } mandatory_release1 = feature_map[feature1].mandatory mandatory_release2 = feature_map[feature2].mandatory if mandatory_release1 < mandatory_release2: return f\\"Feature {feature1} became mandatory earlier than {feature2}\\" elif mandatory_release1 > mandatory_release2: return f\\"Feature {feature2} became mandatory earlier than {feature1}\\" else: return \\"Both features became mandatory at the same time\\""},{"question":"**Question: Advanced Logging Configuration in Python** Python provides powerful logging capabilities through the \\"logging\\" library. This library can be configured using dictionaries with the `logging.config.dictConfig` function or using files with the `logging.config.fileConfig` function. In this task, you will implement a function that configures logging according to specific requirements given in a dictionary format. **Requirements:** 1. **Function Name:** `configure_logging` 2. **Input:** - A dictionary `config` that specifies logging configuration. The format will adhere to the schema described in the documentation. 3. **Output:** - The function does not return anything. - The function should configure logging as specified in the given dictionary. **Constraints:** - You are allowed to use standard libraries such as `logging`, `logging.config`, and `configparser`. - Use of any other external libraries is prohibited. **Common Errors Handling:** - If the configuration dictionary is invalid, the function should handle exceptions gracefully and print a meaningful error message describing the issue. **Performance Considerations:** - The function should efficiently process the configuration dictionary and should not introduce significant delay. **Sample Input:** ```python sample_config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'simple\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'simple\', \'stream\': \'ext://sys.stdout\' } }, \'loggers\': { \'\': { # root logger \'level\': \'DEBUG\', \'handlers\': [\'console\'] }, \'my_module\': { \'level\': \'INFO\', \'handlers\': [\'console\'], \'propagate\': False } } } ``` **Example Usage:** ```python def configure_logging(config): import logging.config try: logging.config.dictConfig(config) except (ValueError, TypeError, AttributeError, ImportError) as e: print(f\\"Failed to configure logging: {e}\\") # Usage example configure_logging(sample_config) import logging # Get a logger logger = logging.getLogger(\'my_module\') logger.info(\\"This is an info message\\") logger.debug(\\"This is a debug message\\") # This won\'t be printed because the level is set to INFO # Root logger will print DEBUG messages root_logger = logging.getLogger() root_logger.debug(\\"This will be printed\\") ``` **Explanation:** 1. The configuration dictionary specifies a custom formatter (`simple`), a console handler (`console`), and two loggers: the root logger and `my_module`. 2. `configure_logging` should setup logging as described in the dictionary. 3. The function ensures that logging is configured correctly or prints an error message if something goes wrong. Implement the function `configure_logging` based on the above requirements.","solution":"def configure_logging(config): Configures logging according to the specified configuration dictionary. Parameters: - config (dict): A dictionary containing the logging configuration. The function sets up logging as specified in the given dictionary using logging.config.dictConfig. import logging.config try: logging.config.dictConfig(config) except (ValueError, TypeError, AttributeError, ImportError, KeyError) as e: print(f\\"Failed to configure logging: {e}\\")"},{"question":"# Advanced Debugging with `pdb` You are given the task of implementing a Python script that exhibits faulty behavior. Your job is to identify and fix the issue using the `pdb` module\'s debugging capabilities, effectively demonstrating your understanding of `pdb`. 1. **Faulty Script**: Consider the following Python script which contains an intentional bug. ```python def calculate_factorial(n): if n == 0: return 1 else: return n * calculate_factorial(n - 1) def main(): try: result = calculate_factorial(5) print(\\"Factorial computed:\\", result) except Exception as e: import pdb; pdb.post_mortem() print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": main() ``` 2. **Objective**: You need to: - Use the `pdb` module to identify the problem in the script. - Set appropriate breakpoints and step through the code to understand where and why the error occurs. - Fix the bug in the script. 3. **Requirements**: - Utilize at least three different commands provided by `pdb` to explore and debug the script. - Provide a brief explanation of how you used each command and what you observed. - Present the corrected version of the script. # Input and Output Format - **Initial Input**: The provided faulty script. - **Final Output**: The corrected script and a brief explanation of the debugging process. # Constraints: - You are not allowed to simply print debug statements or modify the script without using `pdb`. - Show your debugging session\'s steps using `pdb` commands. # Example Workflow 1. Start the script. 2. Use `pdb.pm()` to enter post-mortem debugging if an exception occurs. 3. Use commands like `where`, `up`, `down`, `list`, `print`, or others to analyze the error. 4. Fix the identified issue. ```plaintext Usage of pdb commands: 1. \\"list\\" to see the source code around the current line. 2. \\"where\\" to print a stack trace. 3. \\"print n\\" to check the values of variables. Corrected Script: ... ``` **Note**: Make sure your corrected script performs as expected without errors, properly calculating the factorial of 5.","solution":"def calculate_factorial(n): if n < 0: raise ValueError(\\"Input should be a non-negative integer\\") if n == 0: return 1 else: return n * calculate_factorial(n - 1) def main(): try: result = calculate_factorial(5) print(\\"Factorial computed:\\", result) except Exception as e: import pdb; pdb.post_mortem() print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Question: Functional Data Processing in Python You are given a list of tuples representing records of employees in a company. Each tuple contains the following information about an employee: - `name` (string): The employee\'s name. - `age` (int): The employee\'s age. - `department` (string): The department in which the employee works. - `salary` (float): The employee\'s salary. Your task is to implement a series of functions that process these records using functional programming techniques. You should demonstrate your understanding of iterators, generators, and functions from the `itertools` and `functools` modules. Requirements 1. **Function `filter_by_department(records, dept)`** - **Input**: A list of tuples (`records`), and a string (`dept`) representing the department. - **Output**: A filtered list of tuples containing only those employees who work in the specified department. - **Constraints**: None. - **Example**: ```python records = [ (\'Alice\', 30, \'HR\', 50000.0), (\'Bob\', 24, \'Engineering\', 60000.0), (\'Charlie\', 29, \'HR\', 55000.0) ] dept = \'HR\' filter_by_department(records, dept) # Output: [(\'Alice\', 30, \'HR\', 50000.0), (\'Charlie\', 29, \'HR\', 55000.0)] ``` 2. **Function `average_salary_by_department(records)`** - **Input**: A list of tuples (`records`). - **Output**: A dictionary where the keys are department names, and the values are the average salaries of employees in those departments. - **Constraints**: None. - **Example**: ```python records = [ (\'Alice\', 30, \'HR\', 50000.0), (\'Bob\', 24, \'Engineering\', 60000.0), (\'Charlie\', 29, \'HR\', 55000.0) ] average_salary_by_department(records) # Output: {\'HR\': 52500.0, \'Engineering\': 60000.0} ``` 3. **Function `employee_name_and_age_generator(records)`** - **Input**: A list of tuples (`records`). - **Output**: A generator that yields the name and age of each employee. - **Constraints**: None. - **Example**: ```python records = [ (\'Alice\', 30, \'HR\', 50000.0), (\'Bob\', 24, \'Engineering\', 60000.0), (\'Charlie\', 29, \'HR\', 55000.0) ] gen = employee_name_and_age_generator(records) list(gen) # Output: [(\'Alice\', 30), (\'Bob\', 24), (\'Charlie\', 29)] ``` 4. **Function `sort_employees_by_age(records)`** - **Input**: A list of tuples (`records`). - **Output**: A list of tuples sorted by the age of the employees in ascending order. - **Constraints**: None. - **Example**: ```python records = [ (\'Alice\', 30, \'HR\', 50000.0), (\'Bob\', 24, \'Engineering\', 60000.0), (\'Charlie\', 29, \'HR\', 55000.0) ] sort_employees_by_age(records) # Output: [(\'Bob\', 24, \'Engineering\', 60000.0), (\'Charlie\', 29, \'HR\', 55000.0), (\'Alice\', 30, \'HR\', 50000.0)] ``` Constraints - You must use functional programming techniques where applicable, such as lambda functions, `map()`, `filter()`, `reduce()`, list comprehensions, generator expressions, `itertools` functions, and `functools` functions. - Avoid using loops (e.g., `for` loops) except when iterating through generator objects. - Do not use mutable state or implement these functions in an imperative style. Performance Requirements - The solution should work efficiently for up to 10,000 records. Implement these functions in a Python script or Jupyter Notebook cell, and ensure that they work correctly with the sample inputs provided in the examples.","solution":"from itertools import groupby from functools import reduce def filter_by_department(records, dept): Returns a filtered list of employees who work in the specified department. return list(filter(lambda record: record[2] == dept, records)) def average_salary_by_department(records): Returns a dictionary where keys are department names and values are average salaries of employees in those departments. # Group records by department records_sorted_by_dept = sorted(records, key=lambda record: record[2]) grouped_by_dept = groupby(records_sorted_by_dept, key=lambda record: record[2]) avg_salary_dict = {} for dept, group in grouped_by_dept: group_list = list(group) total_salary = reduce(lambda acc, record: acc + record[3], group_list, 0) average_salary = total_salary / len(group_list) avg_salary_dict[dept] = average_salary return avg_salary_dict def employee_name_and_age_generator(records): A generator that yields the name and age of each employee. return ((record[0], record[1]) for record in records) def sort_employees_by_age(records): Returns a list of employees sorted by age in ascending order. return sorted(records, key=lambda record: record[1])"},{"question":"# Python and C Interoperability: Writing a Python function to reload a module Objective Write a Python function that uses the `ctypes` library to reload a given module. The function should leverage the `PyImport_ReloadModule` function from the Python/C API as documented above. Requirements 1. **Input**: - `module_name` (string): The name of the module you want to reload. 2. **Output**: - Return the new reference to the reloaded module if successful. - If the module cannot be reloaded, return `None`. 3. **Constraints**: - You must use the `ctypes` library to call `PyImport_ReloadModule`. - Handle exceptions and errors gracefully, ensuring no crashes or uncaught exceptions. You can assume Python 3.10 environment with standard libraries installed. Instructions 1. Import the `ctypes` library. 2. Define the function `reload_module(module_name: str) -> object`. 3. Use `ctypes` to load the Python library (`libpython3.10.so`, `libpython3.10.dylib`, or `python310.dll` depending on your OS). 4. Set up `PyImport_ReloadModule` to accept a module object and return a new reference. 5. Implement the reloading logic using `ctypes`. 6. Test your function with both standard libraries (e.g., `math`) and custom user-defined modules. Example ```python from ctypes import py_object, pythonapi, c_char_p def reload_module(module_name: str) -> object: try: # Load a given module by name module = __import__(module_name) # Define PyImport_ReloadModule from the ctypes python API pythonapi.PyImport_ReloadModule.restype = py_object pythonapi.PyImport_ReloadModule.argtypes = [py_object] # Reload the module reloaded_module = pythonapi.PyImport_ReloadModule(module) return reloaded_module except Exception as e: print(f\\"Error reloading module: {e}\\") return None # Example usage: reloaded = reload_module(\\"math\\") if reloaded: print(f\\"Module reloaded: {reloaded.__name__}\\") else: print(\\"Failed to reload module.\\") ``` In this function, you first load the module using Pythonâ€™s built-in `__import__` function, and then use `ctypes` to access the `PyImport_ReloadModule` C API function to reload the module. Handle any exceptions and return the reloaded module or `None` if an error occurs. This task evaluates your understanding of how Python can interface with C libraries and handle dynamic importation and reloading of modules, highlighting advanced use of Python and C interoperability.","solution":"from ctypes import py_object, pythonapi, c_char_p def reload_module(module_name: str) -> object: try: # Load a given module by name module = __import__(module_name) # Define PyImport_ReloadModule from the ctypes python API pythonapi.PyImport_ReloadModule.restype = py_object pythonapi.PyImport_ReloadModule.argtypes = [py_object] # Reload the module reloaded_module = pythonapi.PyImport_ReloadModule(module) return reloaded_module except Exception as e: print(f\\"Error reloading module: {e}\\") return None # Example usage: reloaded = reload_module(\\"math\\") if reloaded: print(f\\"Module reloaded: {reloaded.__name__}\\") else: print(\\"Failed to reload module.\\")"},{"question":"**Objective**: Assess the understanding and application of Python\'s `tarfile` module. Problem Statement You are a data engineer tasked with managing archives of data files. Using the `tarfile` module, you need to create a utility function that handles specific tarfile operations. Your function should: 1. Open a tar archive and list its members. 2. Extract all `.txt` files from the archive to a specified directory. 3. Create a new tar archive containing only the extracted `.txt` files from step 2. Function Signature ```python def extract_and_rearchive(txt_archive_path: str, extract_path: str, new_archive_path: str) -> List[str]: Extracts all .txt files from the given txt_archive_path and re-archives them into a new tarfile. Parameters: - txt_archive_path: str: The path to the original tar archive containing the data files. - extract_path: str: The path to the directory where .txt files should be extracted. - new_archive_path: str: The path where the new tar archive will be created. Returns: - List[str]: A list of extracted .txt filenames. ``` Requirements 1. **Extract the \'.txt\' files**: While extracting, only extract files with a `.txt` extension. 2. **Create a new tar archive**: The new archive should contain only the extracted `.txt` files. 3. **Error Handling**: Handle cases where the provided tar archive cannot be read or does not exist. 4. **Listing members**: Print a list of all members in the original archive for verification. Constraints - Assume the archive paths and directories provided are valid paths. - Ensure that the code works efficiently for tar archives up to 2GB in size. - The filenames of extracted `.txt` files should be returned in the order they were found in the original archive. Example Usage ```python archive_path = \\"data_files.tar.gz\\" extract_directory = \\"./extracted_txt_files\\" new_archive = \\"txt_files_archive.tar\\" extracted_files = extract_and_rearchive(archive_path, extract_directory, new_archive) print(\\"Extracted .txt files:\\", extracted_files) ``` Performance Requirements - The function should handle tar archives with up to 1000 members efficiently. - Avoid loading the entire archive into memory; process files in a streaming manner if possible. # Evaluation Criteria - **Correctness**: The function should accurately extract and re-archive `.txt` files as specified. - **Efficiency**: The function should be optimized to handle large tar archives without excessive memory usage. - **Error Handling**: The function should handle all possible errors gracefully and provide meaningful error messages. - **Code Quality**: The code should be well-organized, modular, and follow Python best practices.","solution":"import tarfile from typing import List import os def extract_and_rearchive(txt_archive_path: str, extract_path: str, new_archive_path: str) -> List[str]: Extracts all .txt files from the given txt_archive_path and re-archives them into a new tarfile. Parameters: - txt_archive_path: str: The path to the original tar archive containing the data files. - extract_path: str: The path to the directory where .txt files should be extracted. - new_archive_path: str: The path where the new tar archive will be created. Returns: - List[str]: A list of extracted .txt filenames. extracted_txt_files = [] try: with tarfile.open(txt_archive_path, \\"r:*\\") as archive: members = archive.getmembers() # Print all members in the original archive print(\\"Members in original archive:\\") for member in members: print(member.name) # Extract only .txt files with tarfile.open(txt_archive_path, \\"r:*\\") as archive: for member in members: if member.isfile() and member.name.endswith(\'.txt\'): archive.extract(member, path=extract_path) extracted_txt_files.append(member.name) # Create a new archive with the extracted .txt files with tarfile.open(new_archive_path, \\"w\\") as new_archive: for file_name in extracted_txt_files: file_path = os.path.join(extract_path, file_name) new_archive.add(file_path, arcname=os.path.basename(file_path)) except (tarfile.TarError, FileNotFoundError) as e: print(f\\"Error handling tar archive: {e}\\") return extracted_txt_files"},{"question":"You are tasked to design and implement a Python program that calculates the determinants of multiple 2x2 matrices using multiple processes to expedite the computation. The goal is to demonstrate proficiency in creating and managing processes, sharing data between them, and coordinating their activities. # Requirements: 1. **Function Implementation**: - `compute_determinant(matrix: List[List[int]]) -> int`: This function takes in a 2x2 matrix and returns its determinant. The determinant of a 2x2 matrix `[a, b], [c, d]` is calculated as `ad - bc`. - `worker(input_queue: multiprocessing.Queue, output_queue: multiprocessing.Queue)`: This function will be the target of each process. It will fetch matrices from the `input_queue`, compute their determinants using `compute_determinant`, and put the results into the `output_queue`. 2. **Main Process**: - Read a list of 2x2 matrices and distribute the work among multiple processes. - Use `multiprocessing.Queue` to feed tasks to worker processes and to collect results from them. - Use a fixed number of worker processes, for example, 4. 3. **Program Flow**: - Create two queues: `input_queue` for tasks and `output_queue` for results. - Create and start worker processes. - Place the matrices into `input_queue`. - Retrieve and print determinants from `output_queue`. 4. **Constraints**: - The number of worker processes should be fixed. - The input list will contain at least 1 and at most 100 matrices. Implement the above requirements in Python. # Input Format: - A list of `n` 2x2 matrices. Each matrix is represented by a list of two lists, where each inner list contains two integers. # Output Format: - Print the determinants of the matrices in the order they were provided. # Example: ```python if __name__ == \'__main__\': matrices = [[[1, 2], [3, 4]], [[2, 0], [1, 2]], [[0, 1], [1, 0]]] calculate_determinants(matrices) ``` Output: ``` -2 4 -1 ``` # Implementation Details: - Ensure proper use of the `multiprocessing` module functions and classes. - Explicitly handle process termination and joining to avoid any leftovers. - Use `multiprocessing.Queue` for communication between the main process and worker processes. # Evaluation Criteria: - Correctness: The program should correctly compute the determinants. - Efficiency: The program should efficiently manage multiple processes, queues, and data flow. - Code clarity and organization: The solution should be well-structured and readable, following best practices for multiprocessing.","solution":"import multiprocessing from typing import List def compute_determinant(matrix: List[List[int]]) -> int: Computes the determinant of a 2x2 matrix. Parameters: matrix (List[List[int]]): A 2x2 matrix represented as a list of two lists, each containing two integers. Returns: int: The determinant of the matrix. a, b = matrix[0] c, d = matrix[1] return a * d - b * c def worker(input_queue: multiprocessing.Queue, output_queue: multiprocessing.Queue): Worker function to compute determinants of matrices fetched from input_queue. Parameters: input_queue (multiprocessing.Queue): Queue from which matrices are fetched for processing. output_queue (multiprocessing.Queue): Queue where computed determinants are placed. while True: matrix = input_queue.get() if matrix is None: # None is the signal to stop the worker break determinant = compute_determinant(matrix) output_queue.put(determinant) def calculate_determinants(matrices: List[List[List[int]]]): Calculates and prints the determinants of the given list of 2x2 matrices using multiple processes. Parameters: matrices (List[List[List[int]]]): A list of 2x2 matrices. input_queue = multiprocessing.Queue() output_queue = multiprocessing.Queue() num_workers = 4 processes = [] # Start worker processes for _ in range(num_workers): p = multiprocessing.Process(target=worker, args=(input_queue, output_queue)) p.start() processes.append(p) # Put matrices in the input queue for matrix in matrices: input_queue.put(matrix) # Signal the workers to stop for _ in range(num_workers): input_queue.put(None) # Print results as they become available for _ in matrices: determinant = output_queue.get() print(determinant) # Close the processes for p in processes: p.join() if __name__ == \'__main__\': matrices = [[[1, 2], [3, 4]], [[2, 0], [1, 2]], [[0, 1], [1, 0]]] calculate_determinants(matrices)"},{"question":"You are required to write a Python script that interacts with an SQLite database to manage a simple library system. The library system should be able to perform the following operations: 1. **Create and initialize the database:** - Create a table named `books` with the following columns: - `id` (INTEGER PRIMARY KEY), - `title` (TEXT), - `author` (TEXT), - `year` (INTEGER), - `available` (BOOLEAN). 2. **Add a new book to the library:** - Write a function `add_book(title: str, author: str, year: int, available: bool) -> None` that inserts a new book into the `books` table. 3. **List all books:** - Write a function `list_books() -> list` that retrieves all entries in the `books` table and returns a list of dictionaries, where each dictionary represents a book. 4. **Search for a book by title:** - Write a function `search_books_by_title(title: str) -> list` that searches for books by title (case-insensitive) and returns a list of dictionaries representing the matching books. 5. **Update the availability of a book:** - Write a function `update_book_availability(book_id: int, available: bool) -> None` that updates the `available` status for a given book ID. 6. **Remove a book from the library:** - Write a function `remove_book(book_id: int) -> None` that deletes a book from the `books` table by its ID. # Input and Output Formats 1. Function: `add_book` - Input: `title (str)`, `author (str)`, `year (int)`, `available (bool)` - Output: None 2. Function: `list_books` - Input: None - Output: List of dictionaries, where each dictionary contains: `id (int)`, `title (str)`, `author (str)`, `year (int)`, `available (bool)` 3. Function: `search_books_by_title` - Input: `title (str)` - Output: List of dictionaries as described in `list_books` 4. Function: `update_book_availability` - Input: `book_id (int)`, `available (bool)` - Output: None 5. Function: `remove_book` - Input: `book_id (int)` - Output: None # Constraints - Assume the library can hold a maximum of 1000 books. - You should handle possible exceptions that can arise during database operations gracefully. # Performance Requirements - Your solutions should aim to be efficient in terms of database transactions and handling large volumes of data. # Additional Notes - Use meaningful variable names and write clean, readable code. - Include comments where necessary to explain your logic. - Ensure that the database connection is properly closed after operations to prevent data corruption or leaks.","solution":"import sqlite3 def create_database(): Create and initialize the database with a table named \'books\'. conn = sqlite3.connect(\'library.db\') cursor = conn.cursor() cursor.execute(\'\'\'CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY, title TEXT, author TEXT, year INTEGER, available BOOLEAN)\'\'\') conn.commit() conn.close() def add_book(title, author, year, available): Add a new book to the library. Args: title (str): The title of the book. author (str): The author of the book. year (int): The year the book was published. available (bool): The availability status of the book. conn = sqlite3.connect(\'library.db\') cursor = conn.cursor() cursor.execute(\\"INSERT INTO books (title, author, year, available) VALUES (?, ?, ?, ?)\\", (title, author, year, available)) conn.commit() conn.close() def list_books(): List all books in the library. Returns: list: A list of dictionaries, each representing a book. conn = sqlite3.connect(\'library.db\') cursor = conn.cursor() cursor.execute(\\"SELECT * FROM books\\") rows = cursor.fetchall() conn.close() books = [] for row in rows: book = {\'id\': row[0], \'title\': row[1], \'author\': row[2], \'year\': row[3], \'available\': row[4]} books.append(book) return books def search_books_by_title(title): Search for books by title. Args: title (str): The title of the book. Returns: list: A list of dictionaries representing the matching books. conn = sqlite3.connect(\'library.db\') cursor = conn.cursor() cursor.execute(\\"SELECT * FROM books WHERE LOWER(title) LIKE ?\\", (\'%\' + title.lower() + \'%\',)) rows = cursor.fetchall() conn.close() books = [] for row in rows: book = {\'id\': row[0], \'title\': row[1], \'author\': row[2], \'year\': row[3], \'available\': row[4]} books.append(book) return books def update_book_availability(book_id, available): Update the availability of a book. Args: book_id (int): The ID of the book. available (bool): The new availability status of the book. conn = sqlite3.connect(\'library.db\') cursor = conn.cursor() cursor.execute(\\"UPDATE books SET available = ? WHERE id = ?\\", (available, book_id)) conn.commit() conn.close() def remove_book(book_id): Remove a book from the library. Args: book_id (int): The ID of the book. conn = sqlite3.connect(\'library.db\') cursor = conn.cursor() cursor.execute(\\"DELETE FROM books WHERE id = ?\\", (book_id,)) conn.commit() conn.close()"},{"question":"# Question: Configure PyTorch Environment with Custom Environment Variables You are tasked with creating a script that configures a PyTorch environment based on specific CUDA and PyTorch environment variables. The script should be able to read an input configuration (provided as a dictionary) that specifies these environment variables and set them accordingly in the PyTorch environment. **Input Format:** A dictionary where: - Keys are the environment variables names (strings). - Values are the environment variable values (strings or integers). **Output Format:** There is no direct output, but your script should set the environment variables correctly. To verify correctness, print out each environment variable and its value after setting them. **Constraints:** - Assume the provided dictionary will only contain valid environment variables mentioned in the documentation. - Handle both string and integer values appropriately. **Example:** ```python env_config = { \\"PYTORCH_NO_CUDA_MEMORY_CACHING\\": 1, \\"CUDA_VISIBLE_DEVICES\\": \\"0,1\\", \\"PYTORCH_CUDA_ALLOC_CONF\\": \\":4096:2\\" } ``` **Your task:** 1. Implement a function `configure_pytorch_env(env_config: dict) -> None` that sets the environment variables as per the input dictionary. 2. Ensure that the configuration is reflected in the environment by printing out each variable and its value after setting them. ```python import os def configure_pytorch_env(env_config: dict) -> None: Configures the PyTorch environment with the given environment variables. Args: - env_config (dict): A dictionary with environment variables and their values. for var, value in env_config.items(): os.environ[var] = str(value) # Print to verify the setting print(f\\"{var} set to {os.environ[var]}\\") # Example usage env_config = { \\"PYTORCH_NO_CUDA_MEMORY_CACHING\\": 1, \\"CUDA_VISIBLE_DEVICES\\": \\"0,1\\", \\"PYTORCH_CUDA_ALLOC_CONF\\": \\":4096:2\\" } configure_pytorch_env(env_config) ``` Ensure your implementation is robust and handles different types of environment variable values as specified.","solution":"import os def configure_pytorch_env(env_config: dict) -> None: Configures the PyTorch environment with the given environment variables. Args: - env_config (dict): A dictionary with environment variables and their values. for var, value in env_config.items(): os.environ[var] = str(value) # Print to verify the setting print(f\\"{var} set to {os.environ[var]}\\")"},{"question":"**Coding Assessment Question: Custom Logging Setup** **Objective:** Design a custom logging setup that demonstrates your understanding of loggers, handlers, formatters, and filters in the Python `logging` module. **Task:** Implement a Python script that sets up a custom logging system with the following requirements: 1. Create a root logger and set its logging level to `DEBUG`. 2. Add a `StreamHandler` to the root logger to output log messages to the console. 3. Add a `FileHandler` to the root logger to output log messages to a file named `app.log`. 4. Set up a custom formatter for the console handler to display the log level, logger name, and message. 5. Set up a different formatter for the file handler that includes the timestamp, log level, logger name, function name, line number, and message. 6. Create a custom filter that only allows log messages from a specific logger (`\'customLogger\'`) to be output to the console. 7. Create a logger named `\'customLogger\'` and configure it to use the root logger\'s handlers. 8. Log messages at different levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) using both the root logger and `\'customLogger\'` to demonstrate the filtering and formatting setup. **Input and Output:** - No specific input format is required. - The script should generate log messages and output them to the console and `app.log` file according to the defined setup. **Constraints:** - The script should not create any additional files or directories other than `app.log`. - Ensure that the log messages are appropriately formatted and filtered according to the specified requirements. **Performance Requirements:** - The script should execute efficiently without unnecessary delays, even with multiple log messages being generated. **Example:** ```python import logging def setup_logging(): # Step 1: Create a root logger root_logger = logging.getLogger() root_logger.setLevel(logging.DEBUG) # Step 2: Add a StreamHandler to the root logger console_handler = logging.StreamHandler() # Step 3: Add a FileHandler to the root logger file_handler = logging.FileHandler(\'app.log\') # Step 4: Set up a custom formatter for the console handler console_formatter = logging.Formatter(\'%(levelname)s - %(name)s - %(message)s\') console_handler.setFormatter(console_formatter) # Step 5: Set up a different formatter for the file handler file_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(name)s - %(funcName)s - %(lineno)d - %(message)s\') file_handler.setFormatter(file_formatter) # Adding handlers to the root logger root_logger.addHandler(console_handler) root_logger.addHandler(file_handler) # Step 6: Create a custom filter class CustomFilter(logging.Filter): def filter(self, record): return record.name == \'customLogger\' # Add the custom filter to the console handler console_handler.addFilter(CustomFilter()) # Step 7: Create a logger named \'customLogger\' custom_logger = logging.getLogger(\'customLogger\') # Step 8: Log messages root_logger.debug(\'This is a debug message from root logger\') root_logger.info(\'This is an info message from root logger\') custom_logger.debug(\'This is a debug message from custom logger\') custom_logger.info(\'This is an info message from custom logger\') # Run the setup_logging to demonstrate the functionality setup_logging() ``` Ensure that your script correctly implements the setup as described and test it to verify the output in the console and `app.log` file.","solution":"import logging def setup_logging(): # Step 1: Create a root logger and set its logging level to DEBUG root_logger = logging.getLogger() root_logger.setLevel(logging.DEBUG) # Step 2: Add a StreamHandler to the root logger console_handler = logging.StreamHandler() # Step 3: Add a FileHandler to the root logger file_handler = logging.FileHandler(\'app.log\') # Step 4: Set up a custom formatter for the console handler console_formatter = logging.Formatter(\'%(levelname)s - %(name)s - %(message)s\') console_handler.setFormatter(console_formatter) # Step 5: Set up a different formatter for the file handler file_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(name)s - %(funcName)s - %(lineno)d - %(message)s\') file_handler.setFormatter(file_formatter) # Adding handlers to the root logger root_logger.addHandler(console_handler) root_logger.addHandler(file_handler) # Step 6: Create a custom filter that only allows log messages from \'customLogger\' to be output to the console class CustomFilter(logging.Filter): def filter(self, record): return record.name == \'customLogger\' # Add the custom filter to the console handler console_handler.addFilter(CustomFilter()) # Step 7: Create a logger named \'customLogger\' and configure it to use the root logger\'s handlers custom_logger = logging.getLogger(\'customLogger\') # Step 8: Log messages at different levels using both root logger and \'customLogger\' root_logger.debug(\'This is a debug message from root logger\') root_logger.info(\'This is an info message from root logger\') root_logger.warning(\'This is a warning message from root logger\') root_logger.error(\'This is an error message from root logger\') root_logger.critical(\'This is a critical message from root logger\') custom_logger.debug(\'This is a debug message from custom logger\') custom_logger.info(\'This is an info message from custom logger\') custom_logger.warning(\'This is a warning message from custom logger\') custom_logger.error(\'This is an error message from custom logger\') custom_logger.critical(\'This is a critical message from custom logger\') # Run the setup_logging to demonstrate the functionality setup_logging()"},{"question":"# Question: Implement a Utility Function Using `datetime` **Objective:** Implement a function that processes a list of events with their start and end times, and determines the total duration of overlapping events. Each event in the list is represented as a dictionary with \\"start\\" and \\"end\\" keys, which are strings in the format \\"YYYY-MM-DD HH:MM:SS\\". If two or more events overlap, the overlapping duration should not be double-counted. **Function Signature:** ```python def total_overlapping_duration(events: List[Dict[str, str]]) -> timedelta: ``` **Input:** - `events`: A list of dictionaries. Each dictionary contains two keys: - `start`: A string representing the event\'s start time, formatted as \\"YYYY-MM-DD HH:MM:SS\\". - `end`: A string representing the event\'s end time, formatted as \\"YYYY-MM-DD HH:MM:SS\\". **Output:** - Returns a `timedelta` object representing the total duration of the overlapping time across all events. **Constraints:** - You can assume all event times are in the same time zone and are valid. - The list can contain up to 1000 events. **Example:** ```python events = [ {\\"start\\": \\"2023-01-01 10:00:00\\", \\"end\\": \\"2023-01-01 12:00:00\\"}, {\\"start\\": \\"2023-01-01 11:30:00\\", \\"end\\": \\"2023-01-01 13:00:00\\"}, {\\"start\\": \\"2023-01-01 12:30:00\\", \\"end\\": \\"2023-01-01 14:00:00\\"}, ] print(total_overlapping_duration(events)) # Output: timedelta(hours=4, minutes=0, seconds=0) ``` **Implementation Steps:** 1. Parse the `start` and `end` times of each event into `datetime` objects. 2. Sort the events based on their start times. 3. Iterate through the sorted events and track the overlapping periods. 4. Calculate the total overlapping duration and return it as a `timedelta` object. **Performance Considerations:** - Ensure that the solution efficiently handles the overlapping calculation, possibly using a sweep line algorithm and merging events iteratively.","solution":"from typing import List, Dict from datetime import datetime, timedelta def total_overlapping_duration(events: List[Dict[str, str]]) -> timedelta: # Convert event times to datetime objects parsed_events = [(datetime.strptime(e[\'start\'], \'%Y-%m-%d %H:%M:%S\'), datetime.strptime(e[\'end\'], \'%Y-%m-%d %H:%M:%S\')) for e in events] # Sort events by start times parsed_events.sort() # Initialize variables total_duration = timedelta() current_start, current_end = parsed_events[0] for start, end in parsed_events[1:]: if start <= current_end: current_end = max(current_end, end) else: total_duration += current_end - current_start current_start, current_end = start, end total_duration += current_end - current_start return total_duration"},{"question":"# Question: Multi-Precision Model Training with Checkpointing In this assessment, you will demonstrate your understanding of PyTorch and its support for Intel GPUs by implementing a training pipeline for a convolutional neural network (CNN) on the CIFAR-10 dataset. Your implementation should utilize Automatic Mixed Precision (AMP) to optimize performance and ensure model checkpointing during training. **Requirements:** 1. **Data Loading:** - Load the CIFAR-10 dataset with appropriate data transformations. - Use a batch size of 128. 2. **Model Creation:** - Define a CNN model suitable for training on the CIFAR-10 dataset. - Ensure the model is compatible with Intel GPUs by sending it to the \\"xpu\\" device. 3. **Training Configuration:** - Use Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.001 and momentum of 0.9. - Implement loss calculation using `CrossEntropyLoss`. 4. **AMP Training Loop:** - Implement an AMP training loop, enabling mixed precision training by using `torch.autocast`. - Utilize `torch.amp.GradScaler` to scale gradients. 5. **Checkpointing:** - Checkpoint the model and optimizer states every 100 iterations. - Save the checkpoint to a specified file path. 6. **Logging:** - Print the loss every 10 iterations to monitor training progress. **Input Constraints:** - Assume the system has Intel GPU support enabled, and necessary drivers are properly installed. **Expected Output:** - A function `train_cnn_with_amp` that performs the entire training process as described. - A file named `model_checkpoint.pth` containing the model and optimizer state dictionaries. **Function Signature:** ```python def train_cnn_with_amp(data_path: str, checkpoint_path: str) -> None: pass ``` **Additional Guidelines:** - Ensure your code runs efficiently on Intel GPUs. - Use appropriate error handling and comments for clarity. # Example ```python def train_cnn_with_amp(data_path: str, checkpoint_path: str) -> None: import torch import torchvision import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.fc1 = nn.Linear(32 * 6 * 6, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.max_pool2d(x, 2, 2) x = torch.relu(self.conv2(x)) x = torch.max_pool2d(x, 2, 2) x = x.view(-1, 32 * 6 * 6) x = torch.relu(self.fc1(x)) x = self.fc2(x) return x transform = torchvision.transforms.Compose([ torchvision.transforms.Resize((32, 32)), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]) train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True, transform=transform, download=True) train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True) model = SimpleCNN().to(\\"xpu\\") criterion = nn.CrossEntropyLoss().to(\\"xpu\\") optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) scaler = torch.amp.GradScaler() model.train() for epoch in range(5): # Number of epochs can be adjusted for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(\\"xpu\\"), target.to(\\"xpu\\") optimizer.zero_grad() with torch.autocast(device_type=\\"xpu\\", dtype=torch.float16): output = model(data) loss = criterion(output, target) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() if (batch_idx + 1) % 10 == 0: print(f\\"Epoch [{epoch+1}], Batch [{batch_idx+1}], Loss: {loss.item():.4f}\\") if (batch_idx + 1) % 100 == 0: torch.save({ \'model_state_dict\': model.state_dict(), \'optimizer_state_dict\': optimizer.state_dict(), }, checkpoint_path) train_cnn_with_amp(data_path=\\"datasets/cifar10\\", checkpoint_path=\\"model_checkpoint.pth\\") ``` In this function, you will download CIFAR-10 from `data_path`, train the model on Intel GPUs for 5 epochs, and save checkpoints every 100 iterations to `checkpoint_path`.","solution":"def train_cnn_with_amp(data_path: str, checkpoint_path: str) -> None: import torch import torchvision import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.fc1 = nn.Linear(32 * 6 * 6, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = torch.relu(self.conv1(x)) x = torch.max_pool2d(x, 2, 2) x = torch.relu(self.conv2(x)) x = torch.max_pool2d(x, 2, 2) x = x.view(-1, 32 * 6 * 6) x = torch.relu(self.fc1(x)) x = self.fc2(x) return x transform = torchvision.transforms.Compose([ torchvision.transforms.Resize((32, 32)), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]) train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True, transform=transform, download=True) train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True) device = torch.device(\'xpu\' if torch.xpu.is_available() else \'cpu\') model = SimpleCNN().to(device) criterion = nn.CrossEntropyLoss().to(device) optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) scaler = torch.amp.GradScaler() model.train() for epoch in range(5): # Number of epochs can be adjusted for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() with torch.autocast(device_type=device.type, dtype=torch.float16): output = model(data) loss = criterion(output, target) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() if (batch_idx + 1) % 10 == 0: print(f\\"Epoch [{epoch+1}], Batch [{batch_idx+1}], Loss: {loss.item():.4f}\\") if (batch_idx + 1) % 100 == 0: torch.save({ \'model_state_dict\': model.state_dict(), \'optimizer_state_dict\': optimizer.state_dict(), }, checkpoint_path)"},{"question":"**Question: Advanced System Interaction with `sys` Module** You are tasked with creating a Python script named `system_interaction.py` that demonstrates your comprehension of the `sys` module by solving the following problem: # Problem Statement 1. **Command-Line Arguments:** - The script should accept an unknown number of integer arguments from the command line. - Calculate the sum of these integers and print it. 2. **Handling Large Recursion:** - Implement a recursive function `factorial(n)` that computes the factorial of a given non-negative integer `n`. - Set the recursion limit to be large enough to compute the factorial of an integer up to 1000, and recover if the limit is too low. 3. **Exception Logging:** - Add an exception hook to log unhandled exceptions to a file named `error_log.txt` in the following format: ```plaintext Exception Type: <type> Exception Value: <value> Traceback: <traceback> ``` - Ensure that the exception hook retrieves the exception details correctly. # Requirements: 1. **Function Definitions:** Implement the following functions in the script: - `cli_sum()` - Reads command-line arguments, calculates their sum, and prints it. - `factorial(n)` - Recursively calculates the factorial of `n`. - `set_recursion_limit()` - Sets an appropriate recursion limit. - `exception_logger(type, value, traceback)` - Logs exceptions to `error_log.txt`. 2. **Script Execution:** - On script execution, call `cli_sum()` to handle the command-line arguments. - Prompt the user to input an integer for which the factorial will be computed. Handle input exceptions appropriately. - Call `set_recursion_limit()` before computing the factorial. - Call the `factorial(n)` function and print the result. 3. **Constraints and Limitations:** - The script should handle the case where no command-line arguments are provided. - If the recursion limit is reached while computing the factorial, catch the `RecursionError` and print a user-friendly message. # Example Usage: ```bash python system_interaction.py 10 20 30 40 ``` Output: ```plaintext Sum of command-line arguments: 100 Enter an integer to compute its factorial: 5 Factorial of 5 is 120 ``` # Notes: - The script should be executable directly from the command line. - Ensure proper handling of all edge cases and include appropriate docstrings and comments.","solution":"import sys import traceback def cli_sum(): Reads command-line arguments, calculates their sum, and prints it. args = sys.argv[1:] if not args: print(\\"No command-line arguments provided.\\") return try: numbers = [int(arg) for arg in args] total = sum(numbers) print(f\\"Sum of command-line arguments: {total}\\") except ValueError: print(\\"Please provide valid integer arguments.\\") def factorial(n): Recursively calculates the factorial of a non-negative integer n. if n == 0: return 1 else: return n * factorial(n-1) def set_recursion_limit(): Sets an appropriate recursion limit. sys.setrecursionlimit(2000) def exception_logger(type, value, tb): Logs exceptions to \'error_log.txt\'. with open(\\"error_log.txt\\", \\"a\\") as f: f.write(f\\"Exception Type: {type.__name__}n\\") f.write(f\\"Exception Value: {value}n\\") f.write(f\\"Traceback: {\'\'.join(traceback.format_tb(tb))}nn\\") if __name__ == \\"__main__\\": sys.excepthook = exception_logger # Handle command-line arguments cli_sum() # Prompt user for a number to calculate factorial try: n = int(input(\\"Enter an integer to compute its factorial: \\")) set_recursion_limit() try: result = factorial(n) print(f\\"Factorial of {n} is {result}\\") except RecursionError: print(\\"Recursion limit reached while computing factorial. Try with a smaller number.\\") except ValueError: print(\\"Please enter a valid integer.\\")"},{"question":"**Question: Creating and Managing Executable Python ZIP Archives with `zipapp`** **Objective:** Design and implement a Python script that demonstrates an understanding of the `zipapp` module by creating, modifying, and executing Python ZIP archives. **Task:** You are required to write a Python script that: 1. **Creates** an executable ZIP archive from a specified directory containing Python code. 2. **Modifies** the created ZIP archive to update its interpreter. 3. **Executes** the modified ZIP archive to display the output. **Requirements:** 1. Your script should create an archive from a specified source directory (`myapp`) which contains a Python script with a `main` function. 2. Ensure the archive includes a `__main__.py` file that designates the point of entry for execution. 3. Add an appropriate shebang line to specify the interpreter using the `/usr/bin/env python3` command. 4. Your script should modify the created archive to use a different interpreter (`/usr/bin/python3`). 5. Verify that the modified archive executes correctly and outputs the expected result. **Constraints:** - You may assume the source directory `myapp` exists and contains valid Python code. - Handle any potential errors gracefully, providing meaningful error messages. **Performance:** - Ensure the script runs efficiently without unnecessary processing or memory overhead. **Input and Output:** - The input will be the path to the source directory (`myapp`). - The output should be the execution result of the Python code contained within the archive. **Example:** Assume the `myapp` directory contains the following structure: ``` myapp/ - myscript.py # Contents of myscript.py def main(): print(\\"Hello from myscript!\\") ``` Your script should: 1. Create an archive `myapp.pyz` from the `myapp` directory. 2. Modify the archive to use `/usr/bin/python3` as the interpreter. 3. Execute the archive and produce the output: `Hello from myscript!` **Submission:** Submit your Python script named `zipapp_example.py`. The script should meet all the requirements and constraints mentioned above.","solution":"import os import zipapp import subprocess def create_archive(source_dir, archive_name=\'myapp.pyz\'): Creates an executable ZIP archive from the specified directory. Args: source_dir (str): The path to the source directory. archive_name (str): The name of the created archive. Returns: str: The path to the created archive. zipapp.create_archive(source_dir, target=archive_name, interpreter=\'/usr/bin/env python3\') return archive_name def modify_archive_interpreter(archive_name, new_interpreter=\'/usr/bin/python3\'): Modifies the interpreter of the ZIP archive. Args: archive_name (str): The path to the archive to be modified. new_interpreter (str): The new interpreter to be set. with open(archive_name, \'rb\') as original_file: original_content = original_file.read() shebang = f\'#!{new_interpreter}n\'.encode() new_content = shebang + original_content with open(archive_name, \'wb\') as modified_file: modified_file.write(new_content) def execute_archive(archive_name): Executes the ZIP archive and returns the output. Args: archive_name (str): The path to the archive to be executed. Returns: str: The output of the execution. completed_process = subprocess.run([\'python3\', archive_name], capture_output=True, text=True) return completed_process.stdout if __name__ == \'__main__\': source_directory = \'myapp\' archive_name = \'myapp.pyz\' if not os.path.exists(source_directory): print(f\\"Source directory \'{source_directory}\' does not exist.\\") else: # Create the archive archive_path = create_archive(source_directory, archive_name) print(f\\"Created archive: {archive_path}\\") # Modify the interpreter of the archive modify_archive_interpreter(archive_path) print(f\\"Modified interpreter of archive: {archive_path}\\") # Execute the archive and print the output output = execute_archive(archive_path) print(\\"Execution Output:\\") print(output)"},{"question":"# Coding Assessment: Binding and Managing Methods in Python Objective: Write Python functions that demonstrate the creation, checking, and retrieval of functions and instances from instance method objects and method objects according to the given syntax, without using any external libraries. Requirements: 1. **Function: `create_instance_method(func)`** - **Input:** A callable object `func`. - **Output:** An instance method object. - Example: ```python def sample_function(): pass instance_method = create_instance_method(sample_function) ``` 2. **Function: `check_instance_method(obj)`** - **Input:** An object `obj`. - **Output:** Return `True` if the object is an instance method, else `False`. - Example: ```python is_instance_method = check_instance_method(instance_method) # Should return True. ``` 3. **Function: `get_instance_method_function(instance_method)`** - **Input:** An instance method object. - **Output:** The original callable object (function) bound to the instance method object. - Example: ```python func = get_instance_method_function(instance_method) # Should return sample_function. ``` 4. **Function: `create_method(func, instance)`** - **Input:** A callable object `func` and an instance `instance`. - **Output:** A bound method object. - Example: ```python class Sample: pass instance = Sample() bound_method = create_method(sample_function, instance) ``` 5. **Function: `check_method(obj)`** - **Input:** An object `obj`. - **Output:** Return `True` if the object is a method, else `False`. - Example: ```python is_method = check_method(bound_method) # Should return True. ``` 6. **Function: `get_method_function(method)`** - **Input:** A method object. - **Output:** The original callable object (function) bound to the method. - Example: ```python func = get_method_function(bound_method) # Should return sample_function. ``` 7. **Function: `get_method_self(method)`** - **Input:** A method object. - **Output:** The instance bound to the method. - Example: ```python self_instance = get_method_self(bound_method) # Should return instance. ``` Constraints: 1. The callable object `func` will always be a valid function. 2. The instance `instance` will always be a valid class instance. 3. Use Pythonic idioms and proper error handling while implementing the functions. 4. Do not use any external libraries except standard Python libraries. Performance Requirements: 1. The solution should be efficient and balanced between readability and execution speed. 2. All functions should execute in constant or linear time based on the size of the input. ```python # Please implement the functions below: def create_instance_method(func): pass def check_instance_method(obj): pass def get_instance_method_function(instance_method): pass def create_method(func, instance): pass def check_method(obj): pass def get_method_function(method): pass def get_method_self(method): pass ```","solution":"import types def create_instance_method(func): Create an instance method object from a function. class Dummy: pass return types.MethodType(func, Dummy()) def check_instance_method(obj): Check if an object is an instance method. return isinstance(obj, types.MethodType) def get_instance_method_function(instance_method): Retrieve the original function from an instance method object. return instance_method.__func__ def create_method(func, instance): Create a bound method object from a function and an instance. return types.MethodType(func, instance) def check_method(obj): Check if an object is a method. return isinstance(obj, types.MethodType) def get_method_function(method): Retrieve the original function from a method object. return method.__func__ def get_method_self(method): Retrieve the instance to which a method is bound. return method.__self__"},{"question":"# **Python 310 Extension Module: File Management System** You are tasked with extending Python with a C module that introduces a simple file management system. This system will allow users to perform basic file operations such as reading, writing, and appending content to a file. This will help you practice creating and integrating C extensions with Python. You will write a C extension module named `filemgr` with the following functionalities: 1. **Write to a File**: - Function: `filemgr.write(filename, content)` - Description: Creates a new file with the specified name or overwrites the existing file with the provided content. - Returns: Number of characters written. 2. **Read from a File**: - Function: `filemgr.read(filename)` - Description: Reads the contents of the specified file. - Returns: String content of the file. 3. **Append to a File**: - Function: `filemgr.append(filename, content)` - Description: Appends the provided content to the specified file. - Returns: Number of characters appended. C Function Implementations 1. `filemgr_write`: Handles writing content to a file. 2. `filemgr_read`: Handles reading content from a file. 3. `filemgr_append`: Handles appending content to a file. Error Handling - Use appropriate error handling to manage file I/O errors. - Set and return Python exceptions in case of failures (e.g., file not found, permission denied). Method Table and Initialization - Define a method table for mapping Python function calls to corresponding C functions. - Implement the moduleâ€™s initialization function to register these methods. Constraints - Assume text mode operations. - Maximum filename length is 255 characters. - Handle empty content gracefully. Example Usage ```python import filemgr # Writing to a file chars_written = filemgr.write(\'example.txt\', \'Hello, World!\') print(chars_written) # Reading from the file content = filemgr.read(\'example.txt\') print(content) # Appending to the file chars_appended = filemgr.append(\'example.txt\', \'nWelcome to Python Extensions!\') print(chars_appended) # Reading appended content content = filemgr.read(\'example.txt\') print(content) ``` # **Task Requirements** 1. Implement and compile the C functions for the `filemgr` module as specified. 2. Ensure that proper reference counting and error handling mechanisms are in place. 3. Include detailed comments and documentation within your C code for clarity. 4. Provide a brief guide on how to compile and install your extension module. # **Input/Output Specifications** - Input: Filenames and content are strings. - Output: Number of characters (integer) or file content (string). # **Evaluation Criteria** - Correctness: Accurate file operations and proper error handling. - Performance: Efficient file handling without leaks or crashes. - Clarity: Well-commented code and clear usage instructions.","solution":"import os def write(filename, content): Creates a new file with the specified name or overwrites the existing file with the provided content. :param filename: Name of the file to write to. :param content: Content to write into the file. :return: Number of characters written. with open(filename, \'w\') as file: num_chars_written = file.write(content) return num_chars_written def read(filename): Reads the contents of the specified file. :param filename: Name of the file to read from. :return: String content of the file. if not os.path.exists(filename): raise FileNotFoundError(f\\"The file {filename} does not exist.\\") with open(filename, \'r\') as file: content = file.read() return content def append(filename, content): Appends the provided content to the specified file. :param filename: Name of the file to append to. :param content: Content to append to the file. :return: Number of characters appended. with open(filename, \'a\') as file: num_chars_appended = file.write(content) return num_chars_appended"},{"question":"Create a function `custom_plot` that utilizes seaborn to generate a customized plot based on user-provided parameters. This function should demonstrate your comprehension of seaborn\'s capabilities in controlling figure aesthetics as detailed in the documentation. # Function Signature ```python def custom_plot( data: np.ndarray, plot_type: str, theme: str = \\"darkgrid\\", despine: bool = False, context: str = \\"notebook\\", font_scale: float = 1.0, rc_params: dict = None, custom_style_params: dict = None ) -> None: pass ``` # Parameters: - `data`: A 2D NumPy array where each column represents a different data series. - `plot_type`: A string specifying the type of plot to generate. It can be \\"boxplot\\" or \\"violinplot\\". - `theme`: (Optional) A string specifying the seaborn theme to apply. Default is \\"darkgrid\\". - `despine`: (Optional) A boolean indicating whether to remove the top and right spines. Default is False. - `context`: (Optional) A string specifying the context for scaling plot elements. Default is \\"notebook\\". - `font_scale`: (Optional) A float value to scale font elements. Default is 1.0. - `rc_params`: (Optional) A dictionary of additional parameters to pass to `sns.set` for overriding default settings. - `custom_style_params`: (Optional) A dictionary of parameters to override or customize the seaborn theme style. # Output: The function should render the specified plot using the seaborn library, applying the provided customization options. # Example Usage: 1. Generate a boxplot with default settings: ```python custom_plot(data=my_data, plot_type=\\"boxplot\\") ``` 2. Generate a violin plot with a \\"whitegrid\\" theme, removing spines, and scaling context to \\"paper\\": ```python custom_plot(data=my_data, plot_type=\\"violinplot\\", theme=\\"whitegrid\\", despine=True, context=\\"paper\\") ``` 3. Generate a boxplot with custom font scaling and additional `rc` parameters: ```python custom_plot(data=my_data, plot_type=\\"boxplot\\", font_scale=1.5, rc_params={\\"lines.linewidth\\": 2.5}) ``` The function should effectively demonstrate the use of multiple seaborn functionalities including setting themes, custom styles, removing spines, scaling plot elements, and using additional `rc` parameters to customize the overall plot appearance.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def custom_plot( data: np.ndarray, plot_type: str, theme: str = \\"darkgrid\\", despine: bool = False, context: str = \\"notebook\\", font_scale: float = 1.0, rc_params: dict = None, custom_style_params: dict = None ) -> None: Generates a seaborn plot based on user-provided parameters. Parameters: - data: A 2D NumPy array where each column represents a different data series. - plot_type: A string specifying the type of plot to generate. It can be \\"boxplot\\" or \\"violinplot\\". - theme: (Optional) A string specifying the seaborn theme to apply. Default is \\"darkgrid\\". - despine: (Optional) A boolean indicating whether to remove the top and right spines. Default is False. - context: (Optional) A string specifying the context for scaling plot elements. Default is \\"notebook\\". - font_scale: (Optional) A float value to scale font elements. Default is 1.0. - rc_params: (Optional) A dictionary of additional parameters to pass to `sns.set` for overriding default settings. - custom_style_params: (Optional) A dictionary of parameters to override or customize the seaborn theme style. # Apply seaborn theme if rc_params is None: rc_params = {} sns.set_theme(style=theme, context=context, font_scale=font_scale, rc=rc_params) # Apply custom style parameters if provided if custom_style_params: sns.set_style(custom_style_params) # Create the plot plt.figure(figsize=(10, 6)) if plot_type == \\"boxplot\\": sns.boxplot(data=data) elif plot_type == \\"violinplot\\": sns.violinplot(data=data) else: raise ValueError(f\\"Unsupported plot type: {plot_type}\\") # Despine if required if despine: sns.despine() plt.show()"}]'),z={name:"App",components:{PoemCard:S},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},q={class:"search-container"},D={class:"card-container"},R={key:0,class:"empty-state"},F=["disabled"],O={key:0},N={key:1};function L(i,e,l,m,s,o){const h=_("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"ðŸ¤”prompts chatðŸ§ ")])],-1)),t("div",q,[e[3]||(e[3]=t("span",{class:"search-icon"},"ðŸ”",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>s.searchQuery=r),placeholder:"Search..."},null,512),[[y,s.searchQuery]]),s.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>s.searchQuery="")}," âœ• ")):d("",!0)]),t("div",D,[(a(!0),n(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),n("div",R,' No results found for "'+u(s.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:s.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[s.isLoading?(a(),n("span",N,"Loading...")):(a(),n("span",O,"See more"))],8,F)):d("",!0)])}const j=p(z,[["render",L],["__scopeId","data-v-294da29a"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/41.md","filePath":"chatai/41.md"}'),M={name:"chatai/41.md"},H=Object.assign(M,{setup(i){return(e,l)=>(a(),n("div",null,[x(j)]))}});export{Y as __pageData,H as default};
