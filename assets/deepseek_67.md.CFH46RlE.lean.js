import{_ as p,o as a,c as s,a as t,m as u,t as c,C as g,M as _,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(o,e,l,h,i,n){return a(),s("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-bd7ba6c6"]]),z=JSON.parse('[{"question":"Coding Assessment Question # Objective To assess students\' understanding of the `xml.dom` module in Python, specifically their ability to create and manipulate XML documents using DOM methods and properties. # Problem Statement You are given an task to create and manipulate an XML document representing a simple book catalog. Each book entry should include a title, author, year of publication, and price. The structure of the XML document should look like this: ```xml <catalog> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> <price>10.99</price> </book> <!-- More book entries --> </catalog> ``` # Requirements 1. **Function Definition**: - Create a function `create_xml_catalog(books)` that takes a list of dictionaries, each representing a book, and returns a DOM `Document` object for the book catalog. - Each book dictionary will have the keys: `\\"title\\"`, `\\"author\\"`, `\\"year\\"`, and `\\"price\\"`. 2. **Function Implementation**: - Use the `xml.dom` module to create a DOM document. - Add a root element `<catalog>`. - For each book in the input list, create a `<book>` element with child elements `<title>`, `<author>`, `<year>`, and `<price>`. - Use appropriate DOM methods to append these elements to their respective parent nodes. 3. **Input Format**: - `books`: A list of dictionaries, where each dictionary has the following structure: ```python { \\"title\\": \\"string\\", \\"author\\": \\"string\\", \\"year\\": \\"integer\\", \\"price\\": \\"float\\" } ``` 4. **Output Format**: - A DOM `Document` object representing the XML structure. 5. **Constraints**: - Each book dictionary will always contain the keys: `\\"title\\"`, `\\"author\\"`, `\\"year\\"`, and `\\"price\\"`. # Example Input ```python books = [ { \\"title\\": \\"To Kill a Mockingbird\\", \\"author\\": \\"Harper Lee\\", \\"year\\": 1960, \\"price\\": 7.99 }, { \\"title\\": \\"1984\\", \\"author\\": \\"George Orwell\\", \\"year\\": 1949, \\"price\\": 8.99 } ] ``` Output (DOM Document) The output should be a DOM `Document` object which represents the following XML structure: ```xml <catalog> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <year>1960</year> <price>7.99</price> </book> <book> <title>1984</title> <author>George Orwell</author> <year>1949</year> <price>8.99</price> </book> </catalog> ``` Function Signature ```python def create_xml_catalog(books): pass ``` # Evaluation Criteria - Correct use of `xml.dom` methods to create and manipulate the XML document. - Proper structure and hierarchy of XML elements. - Handling of various node types and attributes as per the DOM specification. - Code readability and proper commenting.","solution":"from xml.dom.minidom import Document def create_xml_catalog(books): Creates an XML catalog from a list of books. Args: books (list of dict): List of dictionaries, where each dictionary represents a book with keys \'title\', \'author\', \'year\', and \'price\'. Returns: Document: A DOM Document object representing the XML catalog. # Create a new DOM Document doc = Document() # Create the root element <catalog> catalog = doc.createElement(\'catalog\') doc.appendChild(catalog) # Iterate over the list of books for book in books: # Create a <book> element book_element = doc.createElement(\'book\') # Create and append <title> element title_element = doc.createElement(\'title\') title_text = doc.createTextNode(book[\'title\']) title_element.appendChild(title_text) book_element.appendChild(title_element) # Create and append <author> element author_element = doc.createElement(\'author\') author_text = doc.createTextNode(book[\'author\']) author_element.appendChild(author_text) book_element.appendChild(author_element) # Create and append <year> element year_element = doc.createElement(\'year\') year_text = doc.createTextNode(str(book[\'year\'])) year_element.appendChild(year_text) book_element.appendChild(year_element) # Create and append <price> element price_element = doc.createElement(\'price\') price_text = doc.createTextNode(str(book[\'price\'])) price_element.appendChild(price_text) book_element.appendChild(price_element) # Append the <book> element to <catalog> catalog.appendChild(book_element) return doc"},{"question":"Objective: Demonstrate your understanding of Seaborn for data visualization by creating a sophisticated plot from the `diamonds` dataset. Your task involves loading the dataset, manipulating the data, and creating a customized plot using seaborn. Problem Statement: Write a Python function `create_custom_plot()` that loads the `diamonds` dataset from seaborn, and creates a customized visualization. Your function should: 1. Load the `diamonds` dataset using seaborn\'s `load_dataset` function. 2. Create a plot that displays `carat` on the x-axis and `price` on the y-axis. 3. Utilize logarithmic scaling for both the x and y axes. 4. Display two subplots in a single figure: - Subplot 1: A scatter plot of `carat` vs `price` with points jittered along the x-axis. - Subplot 2: A dot plot of `carat` vs `price` with the following customizations: - Display dots at the 10th, 50th, and 90th percentiles of price for each unique `carat` value. - Different colors for points at each percentile level. The function should not return anything but should save the generated plot as a PNG image named `custom_diamond_plot.png`. Expected Function Signature: ```python def create_custom_plot(): pass ``` Constraints: - The function should handle the dataset efficiently, without any noticeable lag for a dataset of this size. - Use appropriate labeling for the axes and title for the figure to convey the information clearly. Example: ```python create_custom_plot() # This should create a PNG file named `custom_diamond_plot.png` in the working directory. ``` Notes: - Ensure you import necessary libraries within the function. - Pay attention to the aesthetics of the plots to ensure they are clear and informative.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd def create_custom_plot(): # Load the diamonds dataset diamonds = sns.load_dataset(\'diamonds\') # Initialize the figure and axes fig, axs = plt.subplots(1, 2, figsize=(14, 7)) # Scatter plot with jitter on x-axis sns.scatterplot(x=\'carat\', y=\'price\', data=diamonds, ax=axs[0], s=10) axs[0].set_xscale(\'log\') axs[0].set_yscale(\'log\') axs[0].set_title(\'Scatter plot with X-axis jitter\') axs[0].set_xlabel(\'Carat (log scale)\') axs[0].set_ylabel(\'Price (log scale)\') # Dot plot showing percentiles percentiles = diamonds.groupby(\'carat\')[\'price\'].quantile([0.1, 0.5, 0.9]).unstack() percentiles = percentiles.reset_index() percentiles = percentiles.melt(id_vars=\'carat\', value_vars=[0.1, 0.5, 0.9], var_name=\'percentile\', value_name=\'price\') # Custom dot plot sns.scatterplot(x=\'carat\', y=\'price\', hue=\'percentile\', data=percentiles, ax=axs[1], s=100) axs[1].set_xscale(\'log\') axs[1].set_yscale(\'log\') axs[1].set_title(\'Dot plot with percentiles\') axs[1].set_xlabel(\'Carat (log scale)\') axs[1].set_ylabel(\'Price (log scale)\') # Save the plot as a PNG file plt.tight_layout() plt.savefig(\'custom_diamond_plot.png\') plt.close()"},{"question":"Objective Demonstrate your understanding of the `sklearn.datasets` package by loading a dataset, manipulating it, and performing a basic machine learning task. Problem Statement You are required to: 1. Load the `iris` dataset using the `sklearn.datasets` package. 2. Split the dataset into a training set (80%) and a test set (20%). 3. Train a k-nearest neighbors (KNN) classifier with `n_neighbors=3` on the training set. 4. Evaluate the classifier on the test set and report the accuracy. Function Signature ```python def knn_iris_classifier(n_neighbors: int = 3) -> float: Load the Iris dataset, split it into training and test sets, train a KNN classifier and evaluate its accuracy on the test set. Parameters: - n_neighbors (int): The number of neighbors to use for the KNN classifier. Default is 3. Returns: - float: The accuracy of the classifier on the test set. ``` Detailed Steps 1. **Load the Dataset:** - Use `datasets.load_iris()` to load the Iris dataset. - Retrieve the data and target from the Bunch object. 2. **Split the Dataset:** - Use `train_test_split` from `sklearn.model_selection` to split the dataset into training and test sets. - Use `test_size=0.2` to ensure 20% of the data is used for testing. 3. **Train the KNN Classifier:** - Import `KNeighborsClassifier` from `sklearn.neighbors`. - Create an instance of `KNeighborsClassifier` with `n_neighbors=3`. - Fit the classifier on the training set. 4. **Evaluate the Classifier:** - Use the `score` method of the classifier on the test set to compute the accuracy. Example ```python accuracy = knn_iris_classifier() print(f\\"Accuracy: {accuracy:.2f}\\") ``` This should print: ``` Accuracy: 0.97 ``` (Note: The accuracy value may vary slightly due to the random split.) Constraints - You must use `sklearn.datasets`, `train_test_split`, and `KNeighborsClassifier` as specified. - Ensure the function signature matches exactly as specified.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier def knn_iris_classifier(n_neighbors: int = 3) -> float: Load the Iris dataset, split it into training and test sets, train a KNN classifier and evaluate its accuracy on the test set. Parameters: - n_neighbors (int): The number of neighbors to use for the KNN classifier. Default is 3. Returns: - float: The accuracy of the classifier on the test set. # Load the Iris dataset iris_dataset = load_iris() X, y = iris_dataset.data, iris_dataset.target # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train a KNN classifier knn = KNeighborsClassifier(n_neighbors=n_neighbors) knn.fit(X_train, y_train) # Evaluate the classifier accuracy = knn.score(X_test, y_test) return accuracy"},{"question":"**Coding Assessment Question** # Problem Statement You are given a dataset containing information about tips received by waitstaff in a restaurant. Your task is to create a comprehensive visualization using seaborn that demonstrates the relationships between different variables, with proper customizations and semantic mappings. # Required Features 1. Load the \\"tips\\" dataset using seaborn. 2. Create a scatter plot showing the relationship between the total bill and the tip. 3. Map the `hue` to the `day` of the week and the `style` to the time of day (`Lunch`, `Dinner`). 4. Use a different palette for the `hue` and set specific markers for `style`. 5. Control the size of the markers based on the size of the dining party (`size` column). 6. Ensure that all unique values appear in the legend. 7. Customize the marker sizes to be in the range of (20, 200). 8. Add a meaningful title to the plot and customize the appearance (e.g., marker color, size, etc.). # Input - No user input is required. Use the seaborn \\"tips\\" dataset provided in the seaborn library. # Output - Display the scatter plot with the required customizations. # Constraints - Use seaborn for the plotting functions. - Ensure that the plot is clear and well-labeled. # Example Here\'s an example of how the final plot should be implemented: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the scatter plot with customizations sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", palette=\\"deep\\", sizes=(20, 200), legend=\\"full\\" ) # Set plot title and customize appearance plt.title(\\"Scatter Plot of Tips vs Total Bill with Customizations\\") plt.show() ``` Ensure that your solution meets all the requirements specified. # Submission Submit your implementation in a Python script file.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_tips_scatter_plot(): Generates a scatter plot for the tips dataset showing the relationship between total bill and tip, with customizations on hue, style, size and other properties. # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the scatter plot with customizations scatter_plot = sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", palette=\\"deep\\", sizes=(20, 200), legend=\\"full\\" ) # Set plot title and customize appearance plt.title(\\"Scatter Plot of Tips vs Total Bill with Customizations\\") plt.show() return scatter_plot"},{"question":"# Advanced Python Initialization and Thread Management Write a Python script that demonstrates the following functionalities: 1. **Initialize and Finalize the Python Interpreter**: - Use the appropriate Python API functions to initialize and finalize the Python interpreter. 2. **Global Configuration**: - Set a few global configuration variables like `Py_DebugFlag` and `Py_OptimizeFlag` to specific values before initializing the interpreter. 3. **Thread State and GIL Management**: - Create a function that performs a long-running computation (e.g., computing a large Fibonacci sequence or prime numbers). - Ensure this function can release the GIL during its computation to allow other Python threads to run. 4. **Sub-interpreter Usage**: - Create a sub-interpreter and demonstrate its independence (e.g., by changing `sys.path` in the sub-interpreter and showing it doesn\'t affect the main interpreter). # Input and Output Formats **Input**: - None. The script should initialize the interpreter, perform operations, and finalize the interpreter all within the script execution. **Output**: - Print statements demonstrating the different stages (initialization, configuration setting, thread running, sub-interpreter creation, and finalization). # Constraints and Requirements: - Ensure to handle all exceptions and errors gracefully. - Use the provided macros and functions effectively. - The script must be compatible with Python 3.10. # Performance Requirements: - The thread management part should demonstrate proper releasing of the GIL to avoid blocking other Python threads. # Example Output: ``` Initializing Python Interpreter... Setting up global configurations... Py_DebugFlag set to 1 Py_OptimizeFlag set to 2 Creating threads and running long computation... Releasing GIL during computation... Computation done. Sub-interpreter created and isolated. Finalizing Python interpreter... ```","solution":"import sys import threading from ctypes import pythonapi, py_object, c_int import _xxsubinterpreters as subinterpreters # Global configuration for the Python interpreter def initialize_python(): print(\\"Initializing Python Interpreter...\\") pythonapi.Py_Initialize() def finalize_python(): print(\\"Finalizing Python Interpreter...\\") pythonapi.Py_Finalize() def set_global_configs(): print(\\"Setting up global configurations...\\") # Setting Py_DebugFlag Py_DebugFlag = c_int.in_dll(pythonapi, \\"Py_DebugFlag\\") Py_DebugFlag.value = 1 print(f\\"Py_DebugFlag set to {Py_DebugFlag.value}\\") # Setting Py_OptimizeFlag Py_OptimizeFlag = c_int.in_dll(pythonapi, \\"Py_OptimizeFlag\\") Py_OptimizeFlag.value = 2 print(f\\"Py_OptimizeFlag set to {Py_OptimizeFlag.value}\\") def long_computation(): print(\\"Starting long computation...\\") a, b = 0, 1 for _ in range(1000000): a, b = b, a + b # Release the GIL to allow other threads to run pythonapi.PyEval_SaveThread() pythonapi.PyEval_RestoreThread(pythonapi.PyGILState_Ensure()) def run_threads(): print(\\"Creating threads and running long computation...\\") threads = [] for i in range(4): t = threading.Thread(target=long_computation) t.start() threads.append(t) for t in threads: t.join() print(\\"Computation done.\\") def create_subinterpreter(): print(\\"Creating sub-interpreter...\\") subinterpreter_id = subinterpreters.create() def subinterp_task(): subinterpreters.run_string(subinterpreter_id, import sys sys.path.append(\'/tmp\') print(f\\"sys.path in sub-interpreter: {sys.path}\\") ) print(\\"Running task in sub-interpreter...\\") t = threading.Thread(target=subinterp_task) t.start() t.join() if __name__ == \\"__main__\\": initialize_python() set_global_configs() run_threads() create_subinterpreter() finalize_python()"},{"question":"# Problem: Advanced File Operations in Python You are tasked with developing a Python utility to manage file backups. The utility should achieve the following: 1. **Copy File with Metadata**: - Copy a file from a source path to a destination path. - Preserve metadata (using `shutil.copy2`). 2. **Backup Directory**: - Recursively copy the entire content of a source directory to a destination directory. - Handle special files and exceptions appropriately. - Preserve symlinks without following them. 3. **Create Archive**: - Create a compressed archive (format: `gztar`) of a given directory and save it to a specified file path. - The archive should include only certain file types; use a callable that filters filenames based on given patterns. 4. **Move Backup to Safe Location**: - Move the newly created archive to another directory (ensuring it overwrites any existing file with the same name). 5. **Clean Up**: - Delete the original directory tree after successfully creating the backup. Implement a function `perform_backup` with the following signature: ```python def perform_backup(src_file: str, dest_file: str, src_dir: str, backup_dir: str, archive_name: str, final_dest_dir: str, patterns: list): pass ``` **Input:** - `src_file` (str): Path to the file to be copied with metadata. - `dest_file` (str): Destination path for the copied file. - `src_dir` (str): Source directory to be backed up. - `backup_dir` (str): Destination directory for the backup. - `archive_name` (str): Name of the archive file to be created (without format extension). - `final_dest_dir` (str): Directory where the final archive should be moved. - `patterns` (list): List of file patterns to include in the archive (e.g., `[\'*.txt\', \'*.md\']`). **Output:** - None **Constraints:** - Handle all exceptions such as file read/write errors or permission issues. - Ensure that symlinks in the source directory are copied as symlinks. - The final archive should be in the `gztar` format. **Performance Requirements:** - Ensure efficient file handling. - Utilize platform-specific optimizations where applicable. # Example Usage ```python perform_backup( \'example.txt\', \'backup/example.txt\', \'project/\', \'project_backup/\', \'project_archive\', \'final_backup/\', [\'*.py\', \'*.md\'] ) ``` In this example, the function performs the following: - Copies `example.txt` to `backup/example.txt` preserving metadata. - Recursively copies the entire `project/` directory to `project_backup/`. - Creates a `project_archive.tar.gz` file containing only `.py` and `.md` files from `project_backup/`. - Moves `project_archive.tar.gz` to `final_backup/`, overwriting any existing file. - Deletes the original `project/` directory after successful archiving. **Note:** Ensure you include necessary import statements and handle any required path manipulations.","solution":"import shutil import os import fnmatch import tarfile import glob def perform_backup(src_file, dest_file, src_dir, backup_dir, archive_name, final_dest_dir, patterns): def include_patterns(*patterns): Return a callable that checks if a filename matches any of the given patterns. def _filter(filename): return any(fnmatch.fnmatch(filename, pat) for pat in patterns) return _filter try: # Copy src_file to dest_file preserving metadata shutil.copy2(src_file, dest_file) # Recursively copy src_dir to backup_dir preserving symlinks and handling exceptions shutil.copytree(src_dir, backup_dir, symlinks=True, ignore=shutil.ignore_patterns(\'*.pyc\', \'*~\')) # Create a compressed archive of backup_dir archive_path = os.path.join(final_dest_dir, f\\"{archive_name}.tar.gz\\") with tarfile.open(archive_path, \\"w:gz\\") as tar: for root, _, files in os.walk(backup_dir): for file in files: filepath = os.path.join(root, file) if include_patterns(*patterns)(file): tar.add(filepath, arcname=os.path.relpath(filepath, start=backup_dir)) # Move the archive to the final destination directory shutil.move(archive_path, os.path.join(final_dest_dir, os.path.basename(archive_path))) # Delete the original src_dir shutil.rmtree(src_dir) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"You work for an organization that frequently generates complex nested data structures (e.g., dictionaries and lists). To improve readability, you decide to create a custom pretty-printer for these data structures, utilizing the `pprint` module of Python. **Objective:** Implement a function `custom_pprint(data, indent=2, width=80, depth=None, compact=False, sort_dicts=True, underscores=False)` that utilizes the `PrettyPrinter` class from the `pprint` module to pretty-print a given data structure according to the provided formatting parameters. **Function Signature:** ```python def custom_pprint(data, indent=2, width=80, depth=None, compact=False, sort_dicts=True, underscores=False): pass ``` **Inputs:** - `data` (any): The data structure to be pretty-printed. - `indent` (int): The number of spaces to use for indentation. Default is 2. - `width` (int): The maximum number of characters per line. Default is 80. - `depth` (int or None): The number of nesting levels to print. Default is `None` (no constraint). - `compact` (bool): Whether to use a compact formatting. Default is `False`. - `sort_dicts` (bool): Whether to sort dictionary keys before printing. Default is `True`. - `underscores` (bool): Whether to format integers with underscores as thousands separator. Default is `False`. **Outputs:** - The function does not return anything; it should directly print the formatted representation of the data structure. **Constraints:** - The function must leverage the `pprint.PrettyPrinter` class to perform the pretty-printing. - The function should handle various types of data structures, including lists, dictionaries, and nested combinations of these. - The function should be capable of handling recursive data structures. **Example:** ```python data = { \\"name\\": \\"Alice\\", \\"details\\": { \\"age\\": 30, \\"address\\": { \\"city\\": \\"Wonderland\\", \\"zipcode\\": 12345 } }, \\"interests\\": [\\"coding\\", \\"reading\\", \\"chess\\"] } custom_pprint(data, indent=4, width=50, depth=2, compact=True, sort_dicts=False, underscores=True) ``` Expected output: ``` { \'name\': \'Alice\', \'details\': { \'age\': 30, \'address\': { ... } }, \'interests\': [\'coding\', \'reading\', \'chess\'] } ``` In this example, indentation level is 4, maximum width is 50 characters per line, depth is limited to 2 nesting levels, compact format is used, dictionary keys are not sorted, and integer formatting uses underscores. **Note:** Ensure that the printed output adheres to the constraints set by the parameters.","solution":"import pprint def custom_pprint(data, indent=2, width=80, depth=None, compact=False, sort_dicts=True, underscores=False): Pretty-print the given data structure according to the provided formatting parameters. Parameters: - data: The data structure to be pretty-printed. - indent: The number of spaces to use for indentation. - width: The maximum number of characters per line. - depth: The number of nesting levels to print. - compact: Whether to use a compact formatting. - sort_dicts: Whether to sort dictionary keys before printing. - underscores: Whether to format integers with underscores as thousands separator. Returns: - None # Customizing the PrettyPrinter class CustomPrettyPrinter(pprint.PrettyPrinter): def format(self, object, context, maxlevels, level): if isinstance(object, int) and underscores: return (\\"{:_}\\".format(object), True, False) return super().format(object, context, maxlevels, level) printer = CustomPrettyPrinter(indent=indent, width=width, depth=depth, compact=compact, sort_dicts=sort_dicts) printer.pprint(data)"},{"question":"# Asyncio Event Loop and Network Operations Problem Statement: You are provided with a task to build an asynchronous TCP echo server using Python\'s `asyncio` library. The server should accept incoming client connections, read messages sent by the clients, and echo the same messages back to them. Additionally, the server should have the following functionalities: 1. Log the incoming connections and messages. 2. Handle multiple client connections concurrently. 3. Ensure clean shutdown of the server on receiving a termination signal (`SIGINT` or `SIGTERM`). Implement the following functions: 1. `handle_client(reader, writer)`: A coroutine that handles communication with a single client. It reads data from the client, logs the received message, sends the same message back to the client, and closes the connection when the client disconnects. 2. `start_echo_server(host, port)`: A coroutine that starts the echo server on the specified host and port. It should use `asyncio.start_server` to create the server and handle client connections using the `handle_client` coroutine. 3. `main()`: The main coroutine that: - Sets up the event loop, - Registers signal handlers for `SIGINT` and `SIGTERM` to shut down the server cleanly, - Starts the echo server, - Keeps the server running until a termination signal is received. Input/Output Format: - The server should be started on `localhost` and port `8888`. - The server should print logs for incoming connections and messages in the format: ``` Connection from {address} Received {data} from {address} ``` - On receiving a termination signal, the server should print \\"Shutting down server...\\" and terminate gracefully. Example Usage: ```python if __name__ == \'__main__\': import asyncio asyncio.run(main()) ``` Constraints: - You should use the `asyncio` library for all asynchronous operations. - Ensure the server can handle multiple clients concurrently. - Properly handle exceptions and ensure the event loop is closed cleanly. Implementation: ```python import asyncio import signal import functools async def handle_client(reader, writer): address = writer.get_extra_info(\'peername\') print(f\\"Connection from {address}\\") try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {address}\\") writer.write(data) await writer.drain() except asyncio.CancelledError: pass finally: print(f\\"Closing connection with {address}\\") writer.close() await writer.wait_closed() async def start_echo_server(host, port): server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() def ask_exit(signame, loop): print(f\\"got signal {signame}: exit\\") loop.stop() async def main(): loop = asyncio.get_running_loop() for signame in {\'SIGINT\', \'SIGTERM\'}: loop.add_signal_handler(getattr(signal, signame), functools.partial(ask_exit, signame, loop)) print(f\\"Starting echo server on {host}:{port}\\") await start_echo_server(\'localhost\', 8888) print(\\"Shutting down server...\\") if __name__ == \'__main__\': asyncio.run(main()) ```","solution":"import asyncio import signal import functools async def handle_client(reader, writer): address = writer.get_extra_info(\'peername\') print(f\\"Connection from {address}\\") try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {address}\\") writer.write(data) await writer.drain() except asyncio.CancelledError: pass finally: print(f\\"Closing connection with {address}\\") writer.close() await writer.wait_closed() async def start_echo_server(host, port): server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() def ask_exit(signame, loop): print(f\\"got signal {signame}: exit\\") loop.stop() async def main(): loop = asyncio.get_running_loop() for signame in {\'SIGINT\', \'SIGTERM\'}: loop.add_signal_handler(getattr(signal, signame), functools.partial(ask_exit, signame, loop)) host, port = \'localhost\', 8888 print(f\\"Starting echo server on {host}:{port}\\") await start_echo_server(host, port) print(\\"Shutting down server...\\") if __name__ == \'__main__\': asyncio.run(main())"},{"question":"# Custom Import Mechanism in Python **Objective:** Demonstrate understanding of Python\'s import system by implementing a custom import mechanism through the `importlib` module. **Problem Statement:** Python\'s default import system searches for modules in specific locations such as the file system directories listed in `sys.path`. In this task, you will create a custom import mechanism to enable module importing from a specific directory not included in `sys.path`. You are required to: 1. Create a custom importer that can import Python files (.py) from a specified directory. 2. Implement a `find_module` method for this importer that mimics the standard Python import system but only searches the given directory. 3. Implement a test script that demonstrates the usage of this custom importer. **Function Definition:** ```python import importlib.util import sys import os class CustomImporter: def __init__(self, directory): self.directory = directory def find_spec(self, fullname, path=None, target=None): Attempts to locate the specification for the module named \'fullname\' in the specified directory. Parameters: - fullname (str): The fully qualified name of the module to import. - path (Optional[str]): The path to search for submodules within. By default, None. - target (Optional[module]): If find_spec is called as part of reloading a module, the target argument will be the existing module object. By default, None. Returns: - ModuleSpec: The specification of the module to import. None, if the module could not be found. # YOUR CODE HERE def custom_import(name, directory): Imports the module named \'name\' from the specified \'directory\'. Parameters: - name (str): Name of the module to import. - directory (str): Directory from which to import the module. This directory is not in sys.path by default. Returns: - module: The imported module. importer = CustomImporter(directory) sys.meta_path.insert(0, importer) try: module = importlib.import_module(name) finally: sys.meta_path.remove(importer) return module # Example of how to use: if __name__ == \\"__main__\\": directory = \\"path/to/custom/modules\\" module_name = \\"my_custom_module\\" # Import the module using our custom importer mod = custom_import(module_name, directory) # Demonstrate that the module has been imported print(mod) ``` **Constraints:** 1. The custom importer should only attempt to import modules from the specified directory. 2. Mimic Python’s standard import system behavior as closely as possible. **Notes:** - You may presume the directory and the module to be imported exist in the environment where your code is run. - The `find_spec` method should identify and return a spec object for the specified module. - This problem requires knowledge of `importlib`, Python’s import system, and how to manipulate `sys.meta_path`. **Evaluation Criteria:** - Correct implementation of the `CustomImporter` class. - Proper handling of the module spec in the `find_spec` method. - Successful importing of modules using the `custom_import` function. - Demonstration of custom importer usage via the script.","solution":"import importlib.util import sys import os class CustomImporter: def __init__(self, directory): self.directory = directory def find_spec(self, fullname, path=None, target=None): Attempts to locate the specification for the module named \'fullname\' in the specified directory. Parameters: - fullname (str): The fully qualified name of the module to import. - path (Optional[str]): The path to search for submodules within. By default, None. - target (Optional[module]): If find_spec is called as part of reloading a module, the target argument will be the existing module object. By default, None. Returns: - ModuleSpec: The specification of the module to import. None, if the module could not be found. file_path = os.path.join(self.directory, f\'{fullname}.py\') if os.path.isfile(file_path): spec = importlib.util.spec_from_file_location(fullname, file_path) return spec return None def custom_import(name, directory): Imports the module named \'name\' from the specified \'directory\'. Parameters: - name (str): Name of the module to import. - directory (str): Directory from which to import the module. This directory is not in sys.path by default. Returns: - module: The imported module. importer = CustomImporter(directory) sys.meta_path.insert(0, importer) try: module = importlib.import_module(name) finally: sys.meta_path.remove(importer) return module # Example of how to use in script context: # if __name__ == \\"__main__\\": # directory = \\"path/to/custom/modules\\" # module_name = \\"my_custom_module\\" # # # Import the module using our custom importer # mod = custom_import(module_name, directory) # # # Demonstrate that the module has been imported # print(mod)"},{"question":"# Question: Advanced Sequence Manipulation in Python (PyCAPI-310) In this question, you will be required to write a Python function that interacts with sequence objects using low-level Python C API functions (simulated through high-level Python functions). You are not directly using the Python C API, but you need to implement a class `AdvancedSequence` that provides functionalities equivalent to the following operations discussed in the documentation: 1. **Check Sequence Protocol:** Function `is_sequence(obj)` checks if an object implements the sequence protocol. 2. **Size of Sequence:** Function `sequence_size(obj)` returns the number of items in the sequence. 3. **Concatenate Sequences:** Function `concatenate_sequences(seq1, seq2)` returns the concatenation of two sequences. 4. **Repeat Sequence:** Function `repeat_sequence(seq, count)` returns the sequence repeated `count` times. 5. **Get Item:** Function `get_item(seq, index)` retrieves an item from the sequence by index. 6. **Get Slice:** Function `get_slice(seq, start, end)` retrieves a slice from the sequence. 7. **Set Item:** Function `set_item(seq, index, value)` sets an item in the sequence by index. 8. **Delete Item:** Function `delete_item(seq, index)` deletes an item from the sequence by index. 9. **Count Item:** Function `count_item(seq, value)` counts the occurrences of a value in the sequence. 10. **Check Containment:** Function `contains(seq, value)` checks if a value is in the sequence. 11. **Find Index:** Function `find_index(seq, value)` finds the first index of a value in the sequence. 12. **Convert to List:** Function `to_list(seq)` converts the sequence to a list. 13. **Convert to Tuple:** Function `to_tuple(seq)` converts the sequence to a tuple. Implement the class `AdvancedSequence` with static methods for each of these functionalities. Assume the input sequences are Python list or tuples. # Constraints - The input sequences will always be valid non-mutable (tuple) or mutable (list). - Input indices and slice ranges are always valid for the given sequences. - `count` in `repeat_sequence` will always be a non-negative integer. - Target values for search and count functions are always present in the sequence. # Example ```python class AdvancedSequence: @staticmethod def is_sequence(obj): # Implement this method @staticmethod def sequence_size(obj): # Implement this method @staticmethod def concatenate_sequences(seq1, seq2): # Implement this method @staticmethod def repeat_sequence(seq, count): # Implement this method @staticmethod def get_item(seq, index): # Implement this method @staticmethod def get_slice(seq, start, end): # Implement this method @staticmethod def set_item(seq, index, value): # Implement this method @staticmethod def delete_item(seq, index): # Implement this method @staticmethod def count_item(seq, value): # Implement this method @staticmethod def contains(seq, value): # Implement this method @staticmethod def find_index(seq, value): # Implement this method @staticmethod def to_list(seq): # Implement this method @staticmethod def to_tuple(seq): # Implement this method ``` - **`AdvancedSequence.is_sequence([1, 2, 3])`** should return `True`. - **`AdvancedSequence.is_sequence(42)`** should return `False`. - **`AdvancedSequence.sequence_size([1, 2, 3, 4])`** should return `4`. - **`AdvancedSequence.concatenate_sequences([1, 2], [3, 4])`** should return `[1, 2, 3, 4]`. - **`AdvancedSequence.repeat_sequence([1, 2], 3)`** should return `[1, 2, 1, 2, 1, 2]`. - **`AdvancedSequence.get_item([1, 2, 3, 4], 2)`** should return `3`. - **`AdvancedSequence.get_slice([1, 2, 3, 4], 1, 3)`** should return `[2, 3]`. - **`AdvancedSequence.set_item(seq=[1, 2, 3, 4], index=2, value=10)`** should set `seq` to `[1, 2, 10, 4]`. - **`AdvancedSequence.delete_item(seq=[1, 2, 3, 4], index=2)`** should set `seq` to `[1, 2, 4]`. - **`AdvancedSequence.count_item([1, 2, 2, 3, 2], 2)`** should return `3`. - **`AdvancedSequence.contains([1, 2, 3, 4], 3)`** should return `True`. - **`AdvancedSequence.find_index([1, 2, 3, 4], 3)`** should return `2`. - **`AdvancedSequence.to_list((1, 2, 3))`** should return `[1, 2, 3]`. - **`AdvancedSequence.to_tuple([1, 2, 3])`** should return `(1, 2, 3)`. Your implementation should handle the above functionalities and test cases correctly. # Notes - Implementations should leverage Python\'s built-in capabilities to efficiently mimic the described C API functionalities. - Ensure your implementations handle edge cases and constraints as described.","solution":"class AdvancedSequence: @staticmethod def is_sequence(obj): Returns True if the object implements the sequence protocol. return isinstance(obj, (list, tuple)) @staticmethod def sequence_size(obj): Returns the number of items in the sequence. return len(obj) @staticmethod def concatenate_sequences(seq1, seq2): Returns the concatenation of two sequences. return seq1 + seq2 @staticmethod def repeat_sequence(seq, count): Returns the sequence repeated `count` times. return seq * count @staticmethod def get_item(seq, index): Retrieves an item from the sequence by index. return seq[index] @staticmethod def get_slice(seq, start, end): Retrieves a slice from the sequence. return seq[start:end] @staticmethod def set_item(seq, index, value): Sets an item in the sequence by index. Modifies the sequence in place. seq[index] = value @staticmethod def delete_item(seq, index): Deletes an item from the sequence by index. Modifies the sequence in place. del seq[index] @staticmethod def count_item(seq, value): Counts the occurrences of a value in the sequence. return seq.count(value) @staticmethod def contains(seq, value): Checks if a value is in the sequence. return value in seq @staticmethod def find_index(seq, value): Finds the first index of a value in the sequence. return seq.index(value) @staticmethod def to_list(seq): Converts the sequence to a list. return list(seq) @staticmethod def to_tuple(seq): Converts the sequence to a tuple. return tuple(seq)"},{"question":"Objective: Demonstrate your comprehension of Python\'s `collections`, `datetime`, and `enum` modules by implementing a function that organizes daily activities based on priority and their respective times. Problem Statement: You are tasked with organizing a list of daily activities for personal productivity. Each activity has a name, a priority level, and a scheduled time. Implement a function that, given a list of activities, sorts and organizes them by priority and time, and then returns a summary of the schedule. Requirements: 1. **Input:** - A list of tuples, each containing: - An activity name (str) - A priority level (enum, defined as `HIGH`, `MEDIUM`, `LOW`) - A scheduled time (str, format \\"HH:MM\\") - Example: ```python activities = [ (\\"Work on project\\", Priority.HIGH, \\"09:00\\"), (\\"Gym\\", Priority.MEDIUM, \\"07:00\\"), (\\"Breakfast\\", Priority.LOW, \\"08:00\\") ] ``` 2. **Output:** - A dictionary where the keys are priority levels (`HIGH`, `MEDIUM`, `LOW`), and the values are lists of activities sorted by their scheduled times. - Example: ```python { Priority.HIGH: [(\\"Work on project\\", \\"09:00\\")], Priority.MEDIUM: [(\\"Gym\\", \\"07:00\\")], Priority.LOW: [(\\"Breakfast\\", \\"08:00\\")] } ``` 3. **Constraints:** - Activities within the same priority level should be sorted by their scheduled time. - The function should efficiently handle up to 1000 activities. - The scheduled time is guaranteed to be in the correct format \\"HH:MM\\". Performance Requirements: - The solution should run in O(n log n) time complexity, where n is the number of activities. - The space complexity should be O(n). Additional Information: - You can assume the `Priority` enum is defined as follows: ```python from enum import Enum class Priority(Enum): HIGH = 1 MEDIUM = 2 LOW = 3 ``` Function Signature: ```python def organize_activities(activities: list) -> dict: pass ``` Example Usage: ```python activities = [ (\\"Work on project\\", Priority.HIGH, \\"09:00\\"), (\\"Gym\\", Priority.MEDIUM, \\"07:00\\"), (\\"Breakfast\\", Priority.LOW, \\"08:00\\") ] result = organize_activities(activities) print(result) # Expected Output: # { # Priority.HIGH: [(\\"Work on project\\", \\"09:00\\")], # Priority.MEDIUM: [(\\"Gym\\", \\"07:00\\")], # Priority.LOW: [(\\"Breakfast\\", \\"08:00\\")] # } ``` Notes: - Use the `datetime` module to handle and compare times. - Use appropriate data structures from the `collections` module to maintain order and efficiency.","solution":"from enum import Enum from datetime import datetime from collections import defaultdict class Priority(Enum): HIGH = 1 MEDIUM = 2 LOW = 3 def organize_activities(activities: list) -> dict: schedule = defaultdict(list) for activity in activities: name, priority, time_str = activity time_obj = datetime.strptime(time_str, \\"%H:%M\\") schedule[priority].append((name, time_obj)) for priority in schedule: schedule[priority].sort(key=lambda x: x[1]) schedule[priority] = [(name, time_obj.strftime(\\"%H:%M\\")) for name, time_obj in schedule[priority]] return schedule"},{"question":"**Objective**: Implement a custom Python class that mimics some attribute and item management behaviors demonstrated by the `python310` API functions. # Task You are required to implement a Python class `CustomObject` that supports the following operations: Attributes: - `has_attr(attr_name: str) -> bool`: Returns `True` if the object has an attribute named `attr_name`, `False` otherwise. - `get_attr(attr_name: str) -> Any`: Returns the value of the attribute named `attr_name`. If the attribute does not exist, raise an `AttributeError`. - `set_attr(attr_name: str, value: Any) -> None`: Sets the value of the attribute named `attr_name`. - `del_attr(attr_name: str) -> None`: Deletes the attribute named `attr_name`. If the attribute does not exist, raise an `AttributeError`. Items: - `get_item(key: Any) -> Any`: Returns the value corresponding to `key`. If the key does not exist, raise a `KeyError`. - `set_item(key: Any, value: Any) -> None`: Sets the value for `key`. - `del_item(key: Any) -> None`: Deletes the key-value pair for `key`. If the key does not exist, raise a `KeyError`. # Constraints - You must use Python\'s built-in functionalities without relying on external libraries. - The operations must handle errors appropriately, by raising the correct exceptions when necessary. # Example Usage: ```python obj = CustomObject() # Attribute operations assert not obj.has_attr(\'name\') obj.set_attr(\'name\', \'Python310\') assert obj.has_attr(\'name\') assert obj.get_attr(\'name\') == \'Python310\' obj.del_attr(\'name\') try: obj.get_attr(\'name\') except AttributeError: print(\\"AttributeError raised as expected\\") # Item operations assert not obj.get_item(\'key1\') obj.set_item(\'key1\', \'value1\') assert obj.get_item(\'key1\') == \'value1\' obj.del_item(\'key1\') try: obj.get_item(\'key1\') except KeyError: print(\\"KeyError raised as expected\\") ``` # Implementation ```python class CustomObject: def __init__(self): self._attributes = {} self._items = {} def has_attr(self, attr_name: str) -> bool: return attr_name in self._attributes def get_attr(self, attr_name: str) -> Any: if attr_name in self._attributes: return self._attributes[attr_name] else: raise AttributeError(f\\"\'{type(self).__name__}\' object has no attribute \'{attr_name}\'\\") def set_attr(self, attr_name: str, value: Any) -> None: self._attributes[attr_name] = value def del_attr(self, attr_name: str) -> None: if attr_name in self._attributes: del self._attributes[attr_name] else: raise AttributeError(f\\"\'{type(self).__name__}\' object has no attribute \'{attr_name}\'\\") def get_item(self, key: Any) -> Any: if key in self._items: return self._items[key] else: raise KeyError(f\\"\'{type(self).__name__}\' object has no key \'{key}\'\\") def set_item(self, key: Any, value: Any) -> None: self._items[key] = value def del_item(self, key: Any) -> None: if key in self._items: del self._items[key] else: raise KeyError(f\\"\'{type(self).__name__}\' object has no key \'{key}\'\\") ``` # Notes: - Your implementation should pass the example usage without modifications. - Ensure proper exception handling and error messages as indicated.","solution":"class CustomObject: def __init__(self): self._attributes = {} self._items = {} def has_attr(self, attr_name: str) -> bool: return attr_name in self._attributes def get_attr(self, attr_name: str): if attr_name in self._attributes: return self._attributes[attr_name] else: raise AttributeError(f\\"\'{type(self).__name__}\' object has no attribute \'{attr_name}\'\\") def set_attr(self, attr_name: str, value: any) -> None: self._attributes[attr_name] = value def del_attr(self, attr_name: str) -> None: if attr_name in self._attributes: del self._attributes[attr_name] else: raise AttributeError(f\\"\'{type(self).__name__}\' object has no attribute \'{attr_name}\'\\") def get_item(self, key: any) -> any: if key in self._items: return self._items[key] else: raise KeyError(f\\"\'{type(self).__name__}\' object has no key \'{key}\'\\") def set_item(self, key: any, value: any) -> None: self._items[key] = value def del_item(self, key: any) -> None: if key in self._items: del self._items[key] else: raise KeyError(f\\"\'{type(self).__name__}\' object has no key \'{key}\'\\")"},{"question":"# Question You are required to implement a function that generates and processes UUIDs using the Python `uuid` module. The task involves creating different types of UUIDs and extracting specific information from them. Function Signature ```python def uuid_operations(): This function should: 1. Generate a UUID using uuid1() and return its string form. 2. Generate a UUID using uuid3() with uuid.NAMESPACE_DNS and \'example.com\', and return its integer form. 3. Generate a random UUID using uuid4() and return its 16-byte string. 4. Generate a UUID using uuid5() with uuid.NAMESPACE_URL and \'https://example.com\', and return its URN form. 5. Instantiate a UUID from the string \\"12345678-1234-5678-1234-567812345678\\" and return its tuple of fields. Return: A tuple containing the results of the above operations in the following order: 1. UUID1 string form (str) 2. UUID3 integer form (int) 3. UUID4 16-byte string (bytes) 4. UUID5 URN form (str) 5. Tuple of fields from the provided UUID string (tuple) pass ``` Example ```python result = uuid_operations() # Example (sample outputs, actual values will differ as UUIDs are unique): # result = ( # \'a8098c1a-f86e-11da-bd1a-00112444be1e\', # 92888743541407693881154426429039322158, # b\'x8f#-x07gxc1x1bZjx93ixe7Pxdfxa4x81\', # \'urn:uuid:886313e1-3b8a-5372-9b90-0c9aee199e5d\', # (305419896, 4660, 22136, 18, 52, 94689280573676) # ) ``` Constraints - You must use the functions provided by the `uuid` module to generate and manipulate the UUIDs. - The UUIDs generated by the function may differ each time you run the function due to their unique nature, except for UUID3 and UUID5, which are deterministic based on their inputs. - Make sure to handle any exceptions that might occur during UUID creation and processing. Notes - Carefully read the documentation for the `uuid` module to understand how to work with UUIDs. - This task assesses your ability to use built-in modules and generate unique identifiers as well as understand their properties.","solution":"import uuid def uuid_operations(): This function should: 1. Generate a UUID using uuid1() and return its string form. 2. Generate a UUID using uuid3() with uuid.NAMESPACE_DNS and \'example.com\', and return its integer form. 3. Generate a random UUID using uuid4() and return its 16-byte string. 4. Generate a UUID using uuid5() with uuid.NAMESPACE_URL and \'https://example.com\', and return its URN form. 5. Instantiate a UUID from the string \\"12345678-1234-5678-1234-567812345678\\" and return its tuple of fields. Return: A tuple containing the results of the above operations in the following order: 1. UUID1 string form (str) 2. UUID3 integer form (int) 3. UUID4 16-byte string (bytes) 4. UUID5 URN form (str) 5. Tuple of fields from the provided UUID string (tuple) # Step 1: Generate a UUID using uuid1() and return its string form. uuid1_str = str(uuid.uuid1()) # Step 2: Generate a UUID using uuid3() with uuid.NAMESPACE_DNS and \'example.com\', and return its integer form. uuid3_int = int(uuid.uuid3(uuid.NAMESPACE_DNS, \'example.com\')) # Step 3: Generate a random UUID using uuid4() and return its 16-byte string. uuid4_bytes = uuid.uuid4().bytes # Step 4: Generate a UUID using uuid5() with uuid.NAMESPACE_URL and \'https://example.com\', and return its URN form. uuid5_urn = uuid.uuid5(uuid.NAMESPACE_URL, \'https://example.com\').urn # Step 5: Instantiate a UUID from the string \\"12345678-1234-5678-1234-567812345678\\" and return its tuple of fields. uuid_str = \\"12345678-1234-5678-1234-567812345678\\" uuid_fields = uuid.UUID(uuid_str).fields # Returning the results as a tuple. return (uuid1_str, uuid3_int, uuid4_bytes, uuid5_urn, uuid_fields)"},{"question":"Using Seaborn for Data Visualization **Objective:** Create visualizations using the Seaborn library based on the provided dataset. **Instructions:** You will be using the `tips` dataset provided by Seaborn. Your task is to visualize data on meal tips given by various customers at different days of the week. 1. **Dataset Loading:** - Load the `tips` dataset from Seaborn. 2. **Plot 1: Total Tips per Day** - Use the Seaborn objects interface to create a bar plot showing the total number of tips given each day. - Count the total tips and display them as a bar plot with `day` on the x-axis. 3. **Plot 2: Total Tips per Day Grouped by Gender** - Modify the previous plot to also group data by the sex of the customer. - Differentiate groups using different colors or shades. 4. **Plot 3: Total Tips per Table Size** - Create a bar plot to show the total number of tips given based on the size of the table (number of people). 5. **Plot 4: Total Tips where Size is a Y-variable** - Create a bar plot that shows the total count of tips given; this time, use table size as the y-axis variable. **Coding Requirements:** - Use `seaborn.objects` for visualizations. - Ensure your code is clear and includes necessary comments. **Expected Input/Output:** 1. **Input:** None. You will load the dataset within the code. 2. **Output:** Four different plots as described above. **Example Skeleton Code:** ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset tips = load_dataset(\\"tips\\") # Plot 1: Total Tips per Day plot1 = so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()) plot1.show() # Plot 2: Total Tips per Day Grouped by Gender plot2 = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot2.show() # Plot 3: Total Tips per Table Size plot3 = so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()) plot3.show() # Plot 4: Total Tips where Size is a Y-variable plot4 = so.Plot(tips, y=\\"size\\").add(so.Bar(), so.Count()) plot4.show() ``` Ensure that your plots are correctly displaying the counts based on various criteria. Comment on any insights derived from the visualizations.","solution":"import seaborn.objects as so from seaborn import load_dataset def load_tips_dataset(): Loads the tips dataset from Seaborn. return load_dataset(\\"tips\\") def plot_total_tips_per_day(tips): Creates a bar plot showing the total number of tips given each day. plot = so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()) plot.show() def plot_total_tips_per_day_grouped_by_gender(tips): Creates a bar plot showing the total number of tips given each day grouped by gender. plot = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot.show() def plot_total_tips_per_table_size(tips): Creates a bar plot showing the total number of tips given based on the size of the table. plot = so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()) plot.show() def plot_total_tips_with_size_as_y(tips): Creates a bar plot showing the total count of tips given; this time, with table size as the y-axis variable. plot = so.Plot(tips, y=\\"size\\").add(so.Bar(), so.Count()) plot.show()"},{"question":"# Question: Create a script that reads a dataset and uses seaborn\'s `PairGrid` to generate a customized grid of plots demonstrating pairwise relationships in the dataset. The script should fulfill the following requirements: 1. **Load Dataset:** - Load the \\"penguins\\" dataset using seaborn\'s `load_dataset` function. 2. **Customize PairGrid:** - Create an instance of `PairGrid` using the penguins dataset with the following variable settings: - Use `body_mass_g`, `bill_length_mm`, `flipper_length_mm` as the variables. - Apply `species` as the hue. 3. **Map Functions:** - On the diagonal, use a histogram to display the distribution of each variable. - On the off-diagonal, use a scatter plot to show pairwise relationships. 4. **Enhancements:** - Make sure to add a legend to the plot. - Customize the scatter plot to vary the size of markers based on the `sex` of the penguins. # Expected Input and Output: - **Input:** - No user input is required. The script operates on the pre-loaded \\"penguins\\" dataset from seaborn. - **Output:** - A graphical output displaying a grid of plots with customized properties as specified in the requirements. # Constraints: - Assume seaborn and matplotlib are pre-installed and available for import. - Aim for efficiency and readability in your code. # Performance Requirements: - The plot generation should complete in a reasonable timeframe for a dataset of the size of the \\"penguins\\" dataset. # Example Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Verify data loading and inspect print(penguins.head()) # Define variables to use variables = [\\"body_mass_g\\", \\"bill_length_mm\\", \\"flipper_length_mm\\"] # Create PairGrid instance g = sns.PairGrid(data=penguins, hue=\\"species\\", vars=variables) # Custom mapping g.map_diag(sns.histplot) g.map_offdiag(sns.scatterplot, size=penguins[\\"sex\\"]) g.add_legend() # Display the plot plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_pairgrid_plot(): # Load the \\"penguins\\" dataset penguins = sns.load_dataset(\\"penguins\\") # Define variables to use variables = [\\"body_mass_g\\", \\"bill_length_mm\\", \\"flipper_length_mm\\"] # Create PairGrid instance g = sns.PairGrid(data=penguins, hue=\\"species\\", vars=variables) # Custom mapping g.map_diag(sns.histplot) g.map_offdiag(sns.scatterplot, size=penguins[\\"sex\\"], sizes=(10, 100)) # Add legend to the plot g.add_legend() # Display the plot plt.show() # Call the function to create the plot (not required for unit testing, only for interactive execution) # create_pairgrid_plot()"},{"question":"**Question: Analyzing and Visualizing Titanic Data with Seaborn** You are given the Titanic dataset, which you can load directly using Seaborn. Your task is to create a visualization that provides insights into the age distribution of passengers based on their class and survival status. Follow these steps to achieve this: 1. Set the theme to \\"whitegrid\\". 2. Load the Titanic dataset using `sns.load_dataset(\\"titanic\\")`. 3. Create a vertical boxplot where: - The x-axis represents the passenger class. - The y-axis represents the passenger age. - Use hue to differentiate between passengers who survived and those who did not. 4. Customize the boxplot further by: - Adding notches to the boxes. - Removing the caps from the boxes. - Customizing the appearance of outliers by using \\"x\\" markers. - Changing the face color of the boxes to a semi-transparent shade of blue (use RGBA `(0.3, 0.5, 0.7, 0.5)`). - Setting the color and linewidth of the median lines to red and 2, respectively. 5. Add a vertical line at age 25 using a dashed line style. **Constraints:** - The visualization should be clear and self-explanatory. - Ensure that all customizations are applied correctly as described. - The plot should be displayed as output. **Output:** - A boxplot with the described customizations. Use the template below to accomplish this task: ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Set the theme sns.set_theme(style=\\"whitegrid\\") # Step 2: Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Step 3: Create the boxplot ax = sns.boxplot( data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", notch=True, showcaps=False, flierprops={\\"marker\\": \\"x\\"}, boxprops={\\"facecolor\\": (0.3, 0.5, 0.7, 0.5)}, medianprops={\\"color\\": \\"r\\", \\"linewidth\\": 2} ) # Step 5: Add vertical line at age 25 ax.axhline(25, color=\\"0.3\\", linestyle=\\"--\\") # Step 6: Display the plot plt.show() ``` Complete the template and generate the visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_titanic_data(): Creates and displays a boxplot showing age distribution of Titanic passengers based on their class and survival status, following the specified customizations. # Step 1: Set the theme sns.set_theme(style=\\"whitegrid\\") # Step 2: Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Step 3: Create the boxplot ax = sns.boxplot( data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", notch=True, showcaps=False, flierprops={\\"marker\\": \\"x\\"}, boxprops={\\"facecolor\\": (0.3, 0.5, 0.7, 0.5)}, medianprops={\\"color\\": \\"r\\", \\"linewidth\\": 2} ) # Step 5: Add vertical line at age 25 ax.axhline(25, color=\\"0.3\\", linestyle=\\"--\\") # Step 6: Display the plot plt.show()"},{"question":"Objective Assess your understanding of WSGI concepts and your ability to create a WSGI-compliant web application using the `wsgiref` library. Problem Statement You are tasked with creating a WSGI-compliant web application that serves a simple TODO list. Your application should support the following functionalities: - Displaying the current TODO list items. - Adding a new item to the TODO list. # Requirements: 1. Implement a WSGI application that serves HTML pages for viewing and adding TODO items. 2. The application should have two routes: - `GET /` - Displays the current TODO list items. - `POST /add` - Adds a new item to the TODO list. 3. Use `wsgiref.simple_server` to serve your application on port 8000. 4. Ensure that your application is WSGI-compliant as per **PEP 3333**. Input and Output Formats: - The `GET /` route should return an HTML page with a form to add a new TODO item and a list of current items. - The `POST /add` route should accept form data containing a single `item` field and add this to the TODO list. Constraints: - You cannot use any external web frameworks (e.g., Flask, Django). Only the standard library and `wsgiref` utilities are allowed. - The TODO list should not be persisted between server restarts. It only needs to be stored in memory. Performance Requirements: - The server must handle up to 10 simultaneous connections without crashing or hanging. # Example Usage: ```python # Starting the server python todo_app.py Serving on port 8000... # Accessing the TODO list # Navigate to http://localhost:8000 in your web browser ``` # Implementation Tips: - Use the `wsgiref.util` functions for URL handling and manipulating environment variables. - Use the `wsgiref.headers.Headers` class to manage response headers. - Ensure your response types and status codes are compliant with WSGI specifications. Submission: Provide a single Python file named `todo_app.py` containing your WSGI application and server start-up code. # Example Code Outline: ```python from wsgiref.simple_server import make_server from wsgiref.util import setup_testing_defaults from wsgiref.headers import Headers TODO_LIST = [] def todo_app(environ, start_response): setup_testing_defaults(environ) path = environ[\'PATH_INFO\'] method = environ[\'REQUEST_METHOD\'] if path == \'/\' and method == \'GET\': # TODO: Implement logic to display TODO items pass elif path == \'/add\' and method == \'POST\': # TODO: Implement logic to add a new TODO item pass else: # Handle unknown routes status = \'404 Not Found\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) return [b\\"Not found\\"] # Start the WSGI server with make_server(\'\', 8000, todo_app) as server: print(\\"Serving on port 8000...\\") server.serve_forever() ```","solution":"from wsgiref.simple_server import make_server from wsgiref.util import setup_testing_defaults from wsgiref.headers import Headers import urllib.parse TODO_LIST = [] def todo_app(environ, start_response): setup_testing_defaults(environ) path = environ[\'PATH_INFO\'] method = environ[\'REQUEST_METHOD\'] if path == \'/\' and method == \'GET\': status = \'200 OK\' headers = [(\'Content-type\', \'text/html; charset=utf-8\')] start_response(status, headers) response_body = \'<h1>TODO List</h1>\' response_body += \'<ul>\' for item in TODO_LIST: response_body += f\'<li>{item}</li>\' response_body += \'</ul>\' response_body += \'\'\' <form action=\\"/add\\" method=\\"post\\"> <input type=\\"text\\" name=\\"item\\" /> <input type=\\"submit\\" value=\\"Add\\" /> </form> \'\'\' return [response_body.encode(\'utf-8\')] elif path == \'/add\' and method == \'POST\': try: size = int(environ.get(\'CONTENT_LENGTH\', 0)) except ValueError: size = 0 post_data = environ[\'wsgi.input\'].read(size) params = urllib.parse.parse_qs(post_data.decode(\'utf-8\')) item = params.get(\'item\', [\'\'])[0] if item: TODO_LIST.append(item) status = \'303 See Other\' headers = [(\'Location\', \'/\')] start_response(status, headers) return [b\'\'] else: status = \'404 Not Found\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) return [b\\"Not found\\"] if __name__ == \'__main__\': with make_server(\'\', 8000, todo_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"# Unicode String Encoder-Decoder Problem Statement: You are to implement a set of functions that leverages Python\'s built-in codecs for encoding and decoding Unicode strings. Specifically, you\'ll need to complete the following tasks: 1. Create a function to encode a given Unicode string to UTF-8. 2. Create a function to decode a given UTF-8 encoded string back to a Unicode string. 3. Create a function that takes a Unicode string and returns a dictionary with various properties of the string, such as length, whether it contains only alphanumeric characters, and the number of whitespace characters. Detailed Specs: 1. **Function `encode_to_utf8`**: - **Input**: `unicode_str` (Type: str) - a Unicode string. - **Output**: A UTF-8 encoded bytes object. 2. **Function `decode_from_utf8`**: - **Input**: `utf8_bytes` (Type: bytes) - a UTF-8 encoded bytes object. - **Output**: A decoded Unicode string. 3. **Function `unicode_string_properties`**: - **Input**: `unicode_str` (Type: str) - a Unicode string. - **Output**: A dictionary with the following keys: - `length`: Length of the string (in code points). - `is_alphanumeric`: Boolean indicating if the string is alphanumeric. - `whitespace_count`: Number of whitespace characters in the string. Function Signatures: ```python def encode_to_utf8(unicode_str: str) -> bytes: pass def decode_from_utf8(utf8_bytes: bytes) -> str: pass def unicode_string_properties(unicode_str: str) -> dict: pass ``` Constraints: - You are not allowed to use any external libraries other than the built-in `str` and `bytes` methods and the `unicodedata` module. - Handle edge cases, such as an empty string or non-printable characters appropriately. Example Usage: ```python # Example for encoding to UTF-8 print(encode_to_utf8(\\"hello\\")) # Output: b\'hello\' # Example for decoding from UTF-8 print(decode_from_utf8(b\'hello\')) # Output: \'hello\' # Example for Unicode string properties print(unicode_string_properties(\\"hello 123\\")) # Output: {\'length\': 9, \'is_alphanumeric\': False, \'whitespace_count\': 1} ``` Note: You are to implement the logic for each function with consideration for performance where applicable.","solution":"def encode_to_utf8(unicode_str: str) -> bytes: Encode a given Unicode string to UTF-8 bytes. Parameters: unicode_str (str): The Unicode string to encode. Returns: bytes: The encoded UTF-8 bytes object. return unicode_str.encode(\'utf-8\') def decode_from_utf8(utf8_bytes: bytes) -> str: Decode a given UTF-8 encoded bytes object back to a Unicode string. Parameters: utf8_bytes (bytes): The UTF-8 encoded bytes object to decode. Returns: str: The decoded Unicode string. return utf8_bytes.decode(\'utf-8\') def unicode_string_properties(unicode_str: str) -> dict: Return a dictionary with various properties of the given Unicode string. Parameters: unicode_str (str): The Unicode string. Returns: dict: A dictionary with the following properties: - length: Length of the string (in code points). - is_alphanumeric: Boolean indicating if the string is alphanumeric. - whitespace_count: Number of whitespace characters in the string. properties = { \'length\': len(unicode_str), \'is_alphanumeric\': unicode_str.isalnum(), \'whitespace_count\': sum(1 for char in unicode_str if char.isspace()) } return properties"},{"question":"You have been provided with a dataset consisting of features of various animals and their corresponding labels, which represent categories like \'mammal\', \'bird\', \'reptile\', etc. Your task is to implement the following using the `scikit-learn` package\'s `neighbors` module: Task Requirements: 1. **Unsupervised Nearest Neighbors Search**: - Implement a function `find_nearest_neighbors` that takes in a dataset `X` and returns the indices and distances of the three nearest neighbors for each point. 2. **Supervised k-Nearest Neighbors Classification**: - Implement a function `knn_classification` to classify each point in a new dataset `X_new` based on the k-nearest neighbors classifier trained on the provided dataset `(X, y)`. 3. **Neighborhood Components Analysis (NCA) for Dimensionality Reduction**: - Implement a function `nca_dimensionality_reduction` that performs supervised dimensionality reduction on the dataset `(X, y)` using NCA and then returns the transformed dataset. **Input:** - `X`: A 2D numpy array of shape `(n_samples, n_features)` representing the input data. - `y`: A 1D numpy array of shape `(n_samples,)` representing the labels. - `X_new`: A 2D numpy array of shape `(m_samples, n_features)` representing new input data to classify. - `n_neighbors`: An integer representing the number of neighbors to consider in the k-neighbors algorithms. **Output:** - For `find_nearest_neighbors(X, n_neighbors)`: Returns a tuple `(indices, distances)`, where: - `indices`: A 2D numpy array of shape `(n_samples, n_neighbors)` containing indices of the nearest neighbors. - `distances`: A 2D numpy array of shape `(n_samples, n_neighbors)` containing distances to the nearest neighbors. - For `knn_classification(X, y, X_new, n_neighbors)`: Returns a numpy array of shape `(m_samples,)` containing the predicted labels for `X_new`. - For `nca_dimensionality_reduction(X, y, n_components)`: Returns a 2D numpy array of shape `(n_samples, n_components)` containing the dimension-reduced data. **Constraints:** - Avoid looping through each point manually; rely solely on the methods provided by `sklearn.neighbors`. - Ensure you use an appropriate distance metric like Euclidean distance for the nearest neighbors search and classification algorithms. Function Templates: ```python import numpy as np from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier from sklearn.neighbors import NeighborhoodComponentsAnalysis from sklearn.pipeline import Pipeline def find_nearest_neighbors(X, n_neighbors=3): Finds the nearest neighbors for each point in the dataset. Parameters: X (numpy.ndarray): The input data of shape (n_samples, n_features). n_neighbors (int): The number of nearest neighbors to find. Returns: tuple: (indices, distances) indices (numpy.ndarray): Indices of the nearest neighbors with shape (n_samples, n_neighbors). distances (numpy.ndarray): Distances to the nearest neighbors with shape (n_samples, n_neighbors). # YOUR CODE HERE def knn_classification(X, y, X_new, n_neighbors=3): Classifies new data points based on k-nearest neighbors. Parameters: X (numpy.ndarray): The training input data of shape (n_samples, n_features). y (numpy.ndarray): The training labels of shape (n_samples,). X_new (numpy.ndarray): The new data to classify of shape (m_samples, n_features). n_neighbors (int): The number of neighbors to use for classification. Returns: numpy.ndarray: The predicted labels for X_new of shape (m_samples,). # YOUR CODE HERE def nca_dimensionality_reduction(X, y, n_components=2): Reduces data to a lower dimensionality using Neighborhood Components Analysis (NCA). Parameters: X (numpy.ndarray): The input data of shape (n_samples, n_features). y (numpy.ndarray): The corresponding labels of shape (n_samples,). n_components (int): The number of dimensions to reduce to. Returns: numpy.ndarray: The dimension-reduced data of shape (n_samples, n_components). # YOUR CODE HERE ``` Make sure to test your code with appropriate inputs to ensure correctness and performance. **Examples:** ```python X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) y = np.array([0, 0, 0, 1, 1, 1]) X_new = np.array([[-0.8, -1], [0.9, 1]]) indices, distances = find_nearest_neighbors(X, 2) print(indices) print(distances) predictions = knn_classification(X, y, X_new, 3) print(predictions) X_reduced = nca_dimensionality_reduction(X, y, 2) print(X_reduced) ``` **Note**: You need to install scikit-learn (`sklearn`) if you haven\'t already. You can install it using `pip install scikit-learn`.","solution":"import numpy as np from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier from sklearn.neighbors import NeighborhoodComponentsAnalysis from sklearn.pipeline import Pipeline def find_nearest_neighbors(X, n_neighbors=3): Finds the nearest neighbors for each point in the dataset. Parameters: X (numpy.ndarray): The input data of shape (n_samples, n_features). n_neighbors (int): The number of nearest neighbors to find. Returns: tuple: (indices, distances) indices (numpy.ndarray): Indices of the nearest neighbors with shape (n_samples, n_neighbors). distances (numpy.ndarray): Distances to the nearest neighbors with shape (n_samples, n_neighbors). nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\'auto\').fit(X) distances, indices = nbrs.kneighbors(X) return indices, distances def knn_classification(X, y, X_new, n_neighbors=3): Classifies new data points based on k-nearest neighbors. Parameters: X (numpy.ndarray): The training input data of shape (n_samples, n_features). y (numpy.ndarray): The training labels of shape (n_samples,). X_new (numpy.ndarray): The new data to classify of shape (m_samples, n_features). n_neighbors (int): The number of neighbors to use for classification. Returns: numpy.ndarray: The predicted labels for X_new of shape (m_samples,). knn = KNeighborsClassifier(n_neighbors=n_neighbors) knn.fit(X, y) return knn.predict(X_new) def nca_dimensionality_reduction(X, y, n_components=2): Reduces data to a lower dimensionality using Neighborhood Components Analysis (NCA). Parameters: X (numpy.ndarray): The input data of shape (n_samples, n_features). y (numpy.ndarray): The corresponding labels of shape (n_samples,). n_components (int): The number of dimensions to reduce to. Returns: numpy.ndarray: The dimension-reduced data of shape (n_samples, n_components). nca = NeighborhoodComponentsAnalysis(n_components=n_components, random_state=42) nca.fit(X, y) return nca.transform(X)"},{"question":"# Problem: Implementing a Custom Gradient Descent Optimizer In this task, you are required to implement a custom gradient descent optimizer using PyTorch. Your optimizer will be used for training a single-layer neural network on a sample dataset. # Specifications 1. **Dataset**: - Create a dataset with `N` data points where each data point has a single feature and one label (i.e., ( y = 2x + 3 ) where ( x ) is a random sample). - Set `N` to be 1000. 2. **Model**: - Initialize a single-layer neural network with one node (a linear model with a weight and a bias). 3. **Loss Function**: - Use Mean Squared Error (MSE) as the loss function. 4. **Custom Optimizer**: - Implement a custom gradient descent optimizer class, `CustomSGD`, with the following methods: - `__init__(self, params, lr=0.01)`: Initialize with model parameters and learning rate. - `step(self)`: Perform one optimization step. 5. **Training Loop**: - Train the model for a given number of epochs. - Print the loss every 100 epochs. # Expected Input and Output The provided function will be: ```python def custom_gradient_descent(): # Initialize data, model, loss function, and optimizer # Training loop # Return final model parameters (weight and bias) ``` - **Output**: - Final values of the model parameters (weight and bias) after training. # Constraints 1. You are allowed to use only the functions from the provided PyTorch documentation. 2. Ensure that the optimizer updates the parameters based on the gradients calculated from the loss function. # Example Output After running your function, you might get an output similar to: ```python Final weight: 1.998453 Final bias: 3.001032 ``` # Implementation Notes 1. **Dataset Creation**: - Use `torch.randn` to create random samples for ( x ). - Compute ( y ) using the given linear relation and add some noise to simulate real-world data. 2. **Model Initialization**: - Use `torch.nn.Parameter` to define the weight and bias as trainable parameters. 3. **Loss Computation**: - Calculate the MSE between the predictions and actual ( y ). 4. **Optimizer Implementation**: - Ensure gradients are computed and parameters updated correctly without using built-in optimizers like `torch.optim.SGD`. **Here is the function signature:** ```python def custom_gradient_descent(): # Your implementation here pass ``` Make sure to adhere to the constraints and test your function thoroughly.","solution":"import torch class CustomSGD: def __init__(self, params, lr=0.01): self.params = list(params) self.lr = lr def step(self): with torch.no_grad(): for param in self.params: if param.grad is not None: param -= self.lr * param.grad param.grad.zero_() def custom_gradient_descent(): # Create dataset N = 1000 x = torch.randn(N, 1) y = 2 * x + 3 + 0.1 * torch.randn(N, 1) # Adding noise to the data # Initialize model weight = torch.nn.Parameter(torch.randn(1, 1)) bias = torch.nn.Parameter(torch.randn(1)) # Define the model prediction def model(x): return x @ weight + bias # Loss function (Mean Squared Error) def mse_loss(pred, true): return ((pred - true) ** 2).mean() # Custom optimizer optimizer = CustomSGD([weight, bias], lr=0.01) # Training loop epochs = 1000 for epoch in range(epochs): y_pred = model(x) loss = mse_loss(y_pred, y) # Compute gradients loss.backward() # Perform optimization step optimizer.step() # Print loss every 100 epochs if (epoch + 1) % 100 == 0: print(f\'Epoch {epoch + 1}, Loss: {loss.item()}\') return weight.item(), bias.item()"},{"question":"# Question: Advanced Color Palettes with Seaborn You are tasked with creating an advanced data visualization using Seaborn, leveraging its capabilities to handle color palettes from matplotlib colormaps. Your task is to implement a function `visualize_color_palettes(data, continuous_colormap, discrete_colormap, num_colors)`, which will: 1. Set up a Seaborn theme to use for all the plots. 2. Create a continuous color palette from the provided `continuous_colormap`. 3. Create a discrete color palette from the provided `discrete_colormap` with the specified number of colors (`num_colors`). 4. Generate and display the following types of plots using Seaborn: - A heatmap of the data using the continuous color palette. - A bar plot of the sum across columns of the data using the discrete color palette. **Function Signature:** ```python def visualize_color_palettes(data, continuous_colormap: str, discrete_colormap: str, num_colors: int): pass ``` # Input: - `data` (pandas.DataFrame): A dataframe containing numerical data. - `continuous_colormap` (str): Name of the continuous colormap to use. - `discrete_colormap` (str): Name of the discrete colormap to use. - `num_colors` (int): Number of colors for the discrete colormap. # Output: - The function should display two plots: - A heatmap of the data using the specified continuous colormap. - A bar plot of the sum of each column using the specified discrete colormap. # Constraints: - Ensure that the colormaps used are from valid matplotlib colormap names. - Handle cases where the number of requested discrete colors is beyond the maximum available in the colormap. - Implement proper exception handling for cases like invalid colormap names or data input issues. # Example: ```python import pandas as pd # Example data data = pd.DataFrame({ \\"A\\": [1, 2, 3], \\"B\\": [4, 5, 6], \\"C\\": [7, 8, 9] }) visualize_color_palettes(data, \\"viridis\\", \\"Set2\\", 3) ``` # Notes: - You\'ll need to import and use pandas, seaborn, and matplotlib for this task. - Refer to the Seaborn documentation for additional customization options for the plots.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap def visualize_color_palettes(data, continuous_colormap: str, discrete_colormap: str, num_colors: int): Generates a heatmap and a bar plot using specified colormaps. Parameters: data (pandas.DataFrame): A dataframe containing numerical data. continuous_colormap (str): Name of the continuous colormap to use. discrete_colormap (str): Name of the discrete colormap to use. num_colors (int): Number of colors for the discrete colormap. try: # Set up seaborn theme sns.set_theme() # Create a continuous color palette continuous_palette = sns.color_palette(continuous_colormap, as_cmap=True) # Create a discrete color palette with the specified number of colors discrete_palette = sns.color_palette(discrete_colormap, num_colors) # Generate the heatmap using the continuous color palette plt.figure(figsize=(10, 8)) sns.heatmap(data, cmap=continuous_palette, annot=True, fmt=\\"g\\") plt.title(\'Heatmap with Continuous Colormap\') plt.show() # Generate the bar plot using the discrete color palette for the column sums plt.figure(figsize=(10, 8)) column_sums = data.sum() sns.barplot(x=column_sums.index, y=column_sums.values, palette=discrete_palette) plt.title(\'Bar Plot with Discrete Colormap\') plt.show() except ValueError as e: print(f\\"Error: {e}\\") except TypeError as e: print(f\\"Error: {e}\\")"},{"question":"# Question: You are tasked with handling a high-performance computing scenario where tensors need to be distributed and operated across multiple devices. The goal is to create a distributed tensor, perform an operation, and gather the results. Task: 1. Create a `DTensor` from a given local tensor with the model\'s weights sharded across devices. 2. Perform a basic operation on the `DTensor`. 3. Gather the distributed tensor back to a single device and verify the results. Requirements: 1. Implement a function `create_and_operate_dtensor()` that accepts the following parameters: - `local_tensor` (torch.Tensor): The local tensor containing model weights. - `device_mesh_shape` (tuple): Shape of the device mesh, e.g., (2, 2) for a 2x2 grid of devices. - `shard_dim` (int): The dimension along which to shard the tensor. - `operation` (str): The operation to perform, e.g., \'add\', \'multiply\'. 2. The function should perform the following steps: 1. Initialize a `DeviceMesh` with the specified shape. 2. Create a `DTensor` from the local tensor, sharded along the specified dimension. 3. Perform the specified operation (addition or multiplication) on the `DTensor`. For simplicity, you can use a scalar value (e.g., add 1 to each element, multiply each element by 2). 4. Gather the resulting `DTensor` back to a single device and return it as a local tensor. 3. Ensure proper logging is enabled to debug any issues during the distributed operations. Example Usage: ```python import torch def create_and_operate_dtensor(local_tensor, device_mesh_shape, shard_dim, operation): # Your implementation here pass # Example parameters local_tensor = torch.randn(8, 8) # Example local tensor device_mesh_shape = (2, 2) # 2x2 grid of devices shard_dim = 0 # Shard along the 0th dimension operation = \'add\' # Operation to perform # Call the function result_tensor = create_and_operate_dtensor(local_tensor, device_mesh_shape, shard_dim, operation) print(result_tensor) ``` Constraints: - Assume `torch.distributed` has been properly initialized and all devices are available for use. - Handle potential errors gracefully and ensure the function works under the SPMD model. - Provide appropriate debug logs to trace any issues in the distributed logic. Performance: - Ensure the operations are efficient and leverage the distributed nature of the `DTensor`.","solution":"import torch import torch.distributed as dist def create_and_operate_dtensor(local_tensor, device_mesh_shape, shard_dim, operation): Create a distributed tensor, perform an operation, and gather the results. Parameters: - local_tensor (torch.Tensor): The local tensor containing model weights. - device_mesh_shape (tuple): Shape of the device mesh. - shard_dim (int): The dimension along which to shard the tensor. - operation (str): The operation to perform (\'add\' or \'multiply\'). Returns: - torch.Tensor: The gathered tensor on a single device. # Initialize device mesh (simulating with processes) num_processes = device_mesh_shape[0] * device_mesh_shape[1] devices = [torch.device(f\'cuda:{i % torch.cuda.device_count()}\') for i in range(num_processes)] # Assuming tensor can be evenly divided across devices shard_size = local_tensor.size(shard_dim) // num_processes shards = local_tensor.chunk(num_processes, dim=shard_dim) results = [] for i in range(num_processes): shard = shards[i].to(devices[i]) if operation == \'add\': shard += 1 elif operation == \'multiply\': shard *= 2 else: raise ValueError(f\\"Unsupported operation: {operation}\\") results.append(shard.cpu()) # Gathering the result to a single device result_tensor = torch.cat(results, dim=shard_dim) return result_tensor"},{"question":"You are provided with a DataFrame containing sales data for a fictional retail store. The DataFrame `sales_df` has the following columns: `Date`, `Product`, `Category`, `Quantity Sold`, and `Revenue`. Your task is to perform various operations related to indexing using pandas and complete the following function implementations. **Function 1: check_unique_index** Write a function `check_unique_index(df)` that takes the DataFrame `sales_df` as input and returns whether the index of the DataFrame is unique or not. - **Input**: A pandas DataFrame `df`. - **Output**: A boolean value indicating whether the index is unique. ```python def check_unique_index(df): pass ``` **Function 2: groupby_category_indexed** Write a function `groupby_category_indexed(df)` that groups the DataFrame by the `Category` column and returns a DataFrame whose rows are indexed by `Category` and columns are the sum of `Quantity Sold` and `Revenue` for each category. - **Input**: A pandas DataFrame `df`. - **Output**: A pandas DataFrame indexed by `Category` with columns `Total Quantity Sold` and `Total Revenue`. ```python def groupby_category_indexed(df): pass ``` **Function 3: multiindex_manipulation** Write a function `multiindex_manipulation(df)` that sets a MultiIndex using `Category` and `Product`, then sorts the DataFrame first by `Category` and then by `Product`, and finally returns the modified DataFrame. - **Input**: A pandas DataFrame `df`. - **Output**: A pandas DataFrame with a MultiIndex (Category, Product) sorted by `Category` and `Product`. ```python def multiindex_manipulation(df): pass ``` **Function 4: reset_and_set_index** Write a function `reset_and_set_index(df)` that resets the index of the DataFrame and then sets `Date` as the new index, making sure that the original index is retained as a column in the DataFrame. Return the newly indexed DataFrame. - **Input**: A pandas DataFrame `df`. - **Output**: A pandas DataFrame with `Date` as the index while retaining the original index as a column. ```python def reset_and_set_index(df): pass ``` # Constraints: 1. Assume the input DataFrame `sales_df` has the columns `Date`, `Product`, `Category`, `Quantity Sold`, and `Revenue`. 2. Ensure your solution handles large DataFrames efficiently. 3. The `Date` column in the DataFrame is already in a datetime format. Test these functions using appropriate test cases to verify their accuracy.","solution":"import pandas as pd def check_unique_index(df): Returns whether the index of the DataFrame is unique or not. return df.index.is_unique def groupby_category_indexed(df): Groups the DataFrame by \'Category\' and returns a DataFrame indexed by \'Category\' with the sum of \'Quantity Sold\' and \'Revenue\' for each category. grouped_df = df.groupby(\'Category\').agg({ \'Quantity Sold\': \'sum\', \'Revenue\': \'sum\' }).rename(columns={\'Quantity Sold\': \'Total Quantity Sold\', \'Revenue\': \'Total Revenue\'}) return grouped_df def multiindex_manipulation(df): Sets a MultiIndex using \'Category\' and \'Product\', sorts by \'Category\' and then \'Product\', and returns the modified DataFrame. df = df.set_index([\'Category\', \'Product\']) df = df.sort_index(level=[\'Category\', \'Product\']) return df def reset_and_set_index(df): Resets the index and then sets \'Date\' as the new index, retaining the original index as a column. df_reset = df.reset_index() df_reset = df_reset.set_index(\'Date\') return df_reset"},{"question":"Objective: To test the candidate\'s ability to use the `pickletools` module for disassembling and optimizing pickled data in Python. Problem Description: You are given a function that pickles a dictionary for storage or transmission. It is required to analyze and optimize this pickled data for efficiency. Write a Python function `analyze_and_optimize_pickle(data, annotate=False)` that: 1. Accepts a dictionary `data` and a boolean `annotate`. 2. Pickles the dictionary. 3. Disassembles the pickled data. 4. Optimizes the pickled data. 5. Returns a tuple containing: - The disassembled output (as a list of strings, each string representing one line of disassembly). - The optimized pickle string (as bytes). Input: - `data`: A dictionary that needs to be pickled. - `annotate`: A boolean flag to determine whether to annotate the disassembly output. (default is `False`) Output: - A tuple containing: - The disassembled output as a list of strings. - The optimized pickle string as bytes. Example Input: ```python data = {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"city\\": \\"Wonderland\\"} annotate = True ``` Example Output: ```python ( [ \\" 0: x80 PROTO 4\\", \\" 2: x95 FRAME 47\\", \\" 11: x85 TUPLE1\\", \\" 12: Q BINPUT 1\\", \\" 14: X BINUNICODE \'name\'\\", \\" 25: u0050 PUT 0\\", \\" 27: X BINUNICODE \'Alice\'\\", \\" 38: Q BINPUT 2\\", \\" ...\\" ], b\'x80x04x95.x00x00x00x00x00x00x00}x94(x8cx04namex94x8cx05Alicex94x8cx03agex94Kx1ex8cx04cityx94x8cnWonderlandx94u.\' ) ``` Constraints: - You should ensure that the disassembly output is readable and meaningful. - Use the functions provided by the `pickletools` module effectively. - The implementation should handle the basic operations without errors. # Notes: - You might want to import the necessary modules (`pickle` and `pickletools`) to accomplish this task. - Consider the performance and clarity of your code. - Include inline comments to make your implementation understandable.","solution":"import pickle import pickletools def analyze_and_optimize_pickle(data, annotate=False): Pickles a dictionary, disassembles and optimizes the pickled data. Parameters: data (dict): The dictionary to be pickled. annotate (bool): A flag to determine whether to annotate the disassembly output. Returns: tuple: (disassembled output as a list of strings, optimized pickle string as bytes) # Step 1: Pickle the dictionary pickled_data = pickle.dumps(data) # Step 2: Disassemble the pickled data disassembled_output = [] disassembly = pickletools.genops(pickled_data) for opcode, arg, position in disassembly: line = f\\"{position:4d}: {opcode.code} {opcode.name}\\" if arg is not None: line += f\\" {arg}\\" disassembled_output.append(line) # Step 3: Optimize the pickled data optimized_pickle = pickletools.optimize(pickled_data) return disassembled_output, optimized_pickle"},{"question":"Objective: Your task is to implement a robust Python module that can handle multiple operations and exceptions from a given set of tasks. You are required to demonstrate your understanding of advanced exception handling and recursion control in Python. Requirements: 1. **Function Implementation**: Implement a function `process_tasks(tasks: List[Dict[str, Any]]) -> Tuple[List[Any], List[str]]` that processes a list of tasks. Each task is a dictionary that specifies an operation (`\\"operation\\"`) and a list of arguments (`\\"args\\"`). The function should return a tuple: - A list of results of successfully completed tasks. - A list of error messages for the tasks that fail. 2. **Supported Operations**: - `\\"divide\\"`: Divide the first argument by the second. - `\\"sqrt\\"`: Calculate the square root of the argument. - `\\"concat\\"`: Concatenate the string arguments. - Any other operation should raise a `NotImplementedError`. 3. **Exception Handling**: - Handle `ZeroDivisionError`, `ValueError`, `TypeError`, and `NotImplementedError` specifically. - For any other unexpected exceptions, record a generic error message. - Use custom error messages that specify the type of error and the operation involved. 4. **Recursion Control**: Implement a function `factorial(n: int) -> int` that calculates the factorial of a number. This function should: - Raise a `RecursionError` with a custom message if the recursion limit is reached. - Return the correct factorial value otherwise. 5. **Warnings**: Implement a warning system for deprecated operations: - If a task contains a deprecated operation `\\"add\\"`, issue a `DeprecationWarning` with a message `\\"The \'add\' operation is deprecated, use \'sum\' instead.\\"` - Adjust the `process_tasks` function to transform `\\"add\\"` operations into `\\"sum\\"` and calculate the sum of the arguments. Input and Output Formats: - **Input**: A list of dictionaries, where each dictionary has an `\\"operation\\"` key (str) and an `\\"args\\"` key (list). - **Output**: A tuple containing a list of results and a list of error messages. Constraints: - Performance is not a primary concern, but your code should handle typical exceptions gracefully. - You must use Python’s built-in exception handling mechanisms and warnings where appropriate. Example: ```python tasks = [ {\\"operation\\": \\"divide\\", \\"args\\": [10, 2]}, {\\"operation\\": \\"sqrt\\", \\"args\\": [16]}, {\\"operation\\": \\"concat\\", \\"args\\": [\\"Hello\\", \\" \\", \\"world\\"]}, {\\"operation\\": \\"add\\", \\"args\\": [1, 2, 3]}, {\\"operation\\": \\"unsupported_op\\", \\"args\\": []}, ] results, errors = process_tasks(tasks) print(\\"Results:\\", results) print(\\"Errors:\\", errors) ``` **Output:** ```plaintext Results: [5.0, 4.0, \'Hello world\', 6] Errors: [\\"NotImplementedError: Unsupported operation \'unsupported_op\'\\"] ``` Important: - Edge cases should be tested, such as division by zero, negative values for square root, incorrect types of arguments, etc. - Factorial function should work correctly and handle recursion errors appropriately.","solution":"import math import warnings def divide(a, b): try: return a / b except ZeroDivisionError as e: raise ZeroDivisionError(f\\"ZeroDivisionError: Cannot divide {a} by zero.\\") def sqrt(x): try: if x < 0: raise ValueError(f\\"ValueError: Cannot calculate square root of negative number {x}.\\") return math.sqrt(x) except ValueError as e: raise ValueError(e) def concat(*args): try: return \'\'.join(args) except TypeError as e: raise TypeError(f\\"TypeError: Concatenation requires string arguments.\\") def sum_args(*args): try: return sum(args) except TypeError as e: raise TypeError(f\\"TypeError: Sum requires numeric arguments.\\") def factorial(n): try: if n < 0: raise ValueError(f\\"ValueError: Factorial is not defined for negative numbers.\\") elif n == 0: return 1 return n * factorial(n - 1) except RecursionError as e: raise RecursionError(\\"RecursionError: Maximum recursion depth reached.\\") def process_tasks(tasks): results = [] errors = [] for task in tasks: operation = task.get(\'operation\') args = task.get(\'args\', []) try: if operation == \\"divide\\": result = divide(*args) elif operation == \\"sqrt\\": result = sqrt(*args) elif operation == \\"concat\\": result = concat(*args) elif operation == \\"add\\": warnings.warn(\\"The \'add\' operation is deprecated, use \'sum\' instead.\\", DeprecationWarning) result = sum_args(*args) else: raise NotImplementedError(f\\"NotImplementedError: Unsupported operation \'{operation}\'\\") results.append(result) except (ZeroDivisionError, ValueError, TypeError, NotImplementedError) as e: errors.append(str(e)) except Exception as e: errors.append(f\\"Exception: {e}\\") return results, errors"},{"question":"# Seaborn Coding Assessment Question **Objective:** Assess your understanding of data visualization using Seaborn\'s `seaborn.objects` interface. **Instructions:** 1. **Data Loading and Preprocessing:** - Load the dataset `brain_networks` from Seaborn. - Preprocess the data so that it contains the average values of networks for each `hemi` and `timepoint` up to `timepoint < 100`. 2. **Plot Configuration:** - Create a paired scatter plot using `so.Plot` with `x` values `[5, 8, 12, 15]`. - Use `y` values `[6, 13, 16]`. 3. **Customization:** - Add paths to the plot with the following customizations: - Set `linewidth` to `1`. - Set `alpha` to `0.8`. - Color the paths based on the `hemi` variable. # Constraints: - Use only the `seaborn.objects` interface to create the plots. - Follow the preprocessing steps detailed in the documentation. # Performance Requirements: - Ensure the code runs efficiently on a standard desktop with moderate data size. # Expected Input and Output: - **Input:** None (the dataset is pre-defined in `seaborn`). - **Output:** A seaborn plot with the specified configurations. **Example:** ```python import seaborn.objects as so from seaborn import load_dataset # Load and preprocess the data networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) # Customize the paths p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") ``` This example demonstrates the expected solution format. Ensure your solution adheres to these guidelines.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load and preprocess the data networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair( x=[5, 8, 12, 15], y=[6, 13, 16], ) .layout(size=(8, 5)) .share(x=True, y=True) ) # Customize the paths p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\")"},{"question":"Imagine you are working with an application that requires configuration settings to be read from an INI file. Using the `configparser` module, you need to write a function to read and validate the contents of this configuration file. # Task Write a Python function, `parse_and_validate_config(file_path: str) -> dict`, that reads a configuration file in INI format, validates the settings, and returns a dictionary with the configuration values. # Input - `file_path`: A string representing the path to the INI configuration file. # Output - A dictionary containing the parsed and validated configuration settings. # Constraints 1. The INI file must contain the following sections and options: - Section: `Database` - `host` (string, non-empty) - `port` (integer, between 1024 and 65535 inclusive) - `username` (string) - `password` (string) - Section: `Server` - `host` (string, non-empty) - `port` (integer, between 1024 and 65535 inclusive) - `use_ssl` (boolean) 2. If any required section or option is missing, or if any validation check fails, raise a `ValueError` with an appropriate message. # Example Assume the following INI file content at `config.ini`: ```ini [Database] host = db.example.com port = 5432 username = admin password = secret [Server] host = server.example.com port = 8080 use_ssl = yes ``` Calling `parse_and_validate_config(\'config.ini\')` should return: ```python { \'Database\': { \'host\': \'db.example.com\', \'port\': 5432, \'username\': \'admin\', \'password\': \'secret\' }, \'Server\': { \'host\': \'server.example.com\', \'port\': 8080, \'use_ssl\': True } } ``` # Notes - Use the `configparser` module for parsing the INI file. - Use appropriate methods to validate the data types and ranges of the configuration options. - Handle the boolean conversion for `use_ssl` appropriately (`yes`, `true`, `1` should be considered as `True`; `no`, `false`, `0` as `False`). Good luck!","solution":"import configparser def parse_and_validate_config(file_path: str) -> dict: # Initialize the parser and read the file config = configparser.ConfigParser() config.read(file_path) # Validate and retrieve the Database section if \'Database\' not in config: raise ValueError(\\"Missing \'Database\' section in the configuration file.\\") if not config[\'Database\'].get(\'host\'): raise ValueError(\\"Missing or empty \'host\' in \'Database\' section.\\") try: database_port = int(config[\'Database\'].get(\'port\')) except (TypeError, ValueError): raise ValueError(\\"Invalid \'port\' value in \'Database\' section. It must be an integer.\\") if not 1024 <= database_port <= 65535: raise ValueError(\\"Invalid \'port\' value in \'Database\' section. It must be between 1024 and 65535.\\") database_username = config[\'Database\'].get(\'username\') database_password = config[\'Database\'].get(\'password\') if database_username is None: raise ValueError(\\"Missing \'username\' in \'Database\' section.\\") if database_password is None: raise ValueError(\\"Missing \'password\' in \'Database\' section.\\") # Validate and retrieve the Server section if \'Server\' not in config: raise ValueError(\\"Missing \'Server\' section in the configuration file.\\") if not config[\'Server\'].get(\'host\'): raise ValueError(\\"Missing or empty \'host\' in \'Server\' section.\\") try: server_port = int(config[\'Server\'].get(\'port\')) except (TypeError, ValueError): raise ValueError(\\"Invalid \'port\' value in \'Server\' section. It must be an integer.\\") if not 1024 <= server_port <= 65535: raise ValueError(\\"Invalid \'port\' value in \'Server\' section. It must be between 1024 and 65535.\\") use_ssl_str = config[\'Server\'].get(\'use_ssl\', \'\').lower() if use_ssl_str in [\'yes\', \'true\', \'1\']: use_ssl = True elif use_ssl_str in [\'no\', \'false\', \'0\']: use_ssl = False else: raise ValueError(\\"Invalid \'use_ssl\' value in \'Server\' section. It must be a boolean (\'yes\', \'no\', \'true\', \'false\', \'1\', \'0\').\\") # Construct the resulting dictionary result = { \'Database\': { \'host\': config[\'Database\'][\'host\'], \'port\': database_port, \'username\': database_username, \'password\': database_password }, \'Server\': { \'host\': config[\'Server\'][\'host\'], \'port\': server_port, \'use_ssl\': use_ssl } } return result"},{"question":"# Async Subprocess Manager Write a Python function using the asyncio module that runs multiple shell commands concurrently. The function should support passing commands, capturing their outputs, and returning the results asynchronously. Implement the function: ```python import asyncio async def run_commands(commands): Run multiple shell commands concurrently using asyncio. Args: commands: List of strings, each representing a shell command. Returns: A dictionary where: - Keys are the commands. - Values are dictionaries with the keys: * \'returncode\': int, the return code of the command. * \'stdout\': str, the standard output of the command. * \'stderr\': str, the standard error of the command. Constraints: - All commands should run concurrently. - Capture both stdout and stderr for each command. - Ensure none of the command outputs exceed 1MB in size. - Handle potential errors such as command not found, permission denied, etc. Example Usage: result = asyncio.run(run_commands([\'ls /\', \'sleep 1\', \'echo \\"hello world\\"\'])) print(result) results = {} async def run(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() return { \'returncode\': proc.returncode, \'stdout\': stdout.decode() if stdout else \'\', \'stderr\': stderr.decode() if stderr else \'\' } tasks = [run(cmd) for cmd in commands] responses = await asyncio.gather(*tasks, return_exceptions=True) for cmd, response in zip(commands, responses): if isinstance(response, dict): results[cmd] = response else: results[cmd] = { \'returncode\': None, \'stdout\': \'\', \'stderr\': str(response) } return results # Example usage if __name__ == \\"__main__\\": commands = [\'ls /\', \'sleep 1\', \'echo \\"hello world\\"\'] result = asyncio.run(run_commands(commands)) print(result) ``` * **Input:** - `commands`: A list of shell command strings to be run concurrently. * **Output:** - A dictionary with details of each command\'s execution, including standard output, standard error, and the return code. **Constraints & Requirements:** - Ensure the commands run concurrently. - Capture and return both stdout and stderr streams. - Limit captured output size to 1MB to prevent excessive memory use. - Handle errors gracefully ensuring the function doesn\'t crash. * **Performance Considerations:** - The function should be able to run up to 10 commands concurrently without noticeable delay. - Commands should be lightweight enough to complete within a reasonable time frame (within 2-3 seconds). For long-running commands, this might need adjustments. **Example usage:** ```python asyncio.run(run_commands([\'ls /\', \'sleep 1\', \'echo \\"hello world\\"\'])) ```","solution":"import asyncio async def run_commands(commands): Run multiple shell commands concurrently using asyncio. Args: commands: List of strings, each representing a shell command. Returns: A dictionary where: - Keys are the commands. - Values are dictionaries with the keys: * \'returncode\': int, the return code of the command. * \'stdout\': str, the standard output of the command. * \'stderr\': str, the standard error of the command. Constraints: - All commands should run concurrently. - Capture both stdout and stderr for each command. - Ensure none of the command outputs exceed 1MB in size. - Handle potential errors such as command not found, permission denied, etc. results = {} async def run(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() return { \'returncode\': proc.returncode, \'stdout\': stdout.decode() if stdout else \'\', \'stderr\': stderr.decode() if stderr else \'\' } tasks = [run(cmd) for cmd in commands] responses = await asyncio.gather(*tasks, return_exceptions=True) for cmd, response in zip(commands, responses): if isinstance(response, dict): results[cmd] = response else: results[cmd] = { \'returncode\': None, \'stdout\': \'\', \'stderr\': str(response) } return results # Example usage if __name__ == \\"__main__\\": commands = [\'ls /\', \'sleep 1\', \'echo \\"hello world\\"\'] result = asyncio.run(run_commands(commands)) print(result)"},{"question":"**Title: Garbage Collection Cycle Detection in Python** **Problem Statement:** You are tasked with analyzing and managing reference cycles within a Python program using the `gc` module. A reference cycle occurs when a group of objects reference each other, creating a loop that prevents them from being garbage collected even though there are no external references to the cycle. To assist in detecting and debugging such cycles, implement a function that monitors garbage collection and provides information about detected cycles. **Objective:** Implement a function `detect_cycles(thresholds, debug_flags)` that performs the following tasks: 1. Sets specific thresholds for garbage collection. 2. Enables debugging with specified flags. 3. Runs garbage collection explicitly. 4. Returns details of objects involved in detected reference cycles. # Function Signature: ```python def detect_cycles(thresholds: tuple, debug_flags: int) -> dict: ``` # Input: 1. `thresholds` (tuple): A tuple of three integers `(threshold0, threshold1, threshold2)` specifying the garbage collection thresholds. 2. `debug_flags` (int): An integer representing the debugging flags (can be a combination of `gc.DEBUG_STATS`, `gc.DEBUG_COLLECTABLE`, `gc.DEBUG_UNCOLLECTABLE`, `gc.DEBUG_SAVEALL`, `gc.DEBUG_LEAK`). # Output: - Returns a dictionary with the following keys: - `collected` (int): The number of objects successfully collected. - `uncollectable` (int): The number of objects identified as uncollectable. - `garbage` (list): A list of uncollectable objects tracked by `gc.garbage`. # Example: ```python import gc # Example usage of the detect_cycles function thresholds = (700, 10, 10) debug_flags = gc.DEBUG_LEAK result = detect_cycles(thresholds, debug_flags) print(result) ``` # Constraints: 1. Ensure that the thresholds are non-negative integers. 2. The function should run a full collection (generation = 2). # Implementation Notes: - Utilize `gc.set_threshold(threshold0, threshold1, threshold2)` to set the thresholds. - Use `gc.set_debug(flags)` to set debugging options. - Perform garbage collection with `gc.collect()`. - Access `gc.garbage` to obtain uncollectable objects. - Enable and disable automatic garbage collection using `gc.enable()` and `gc.disable()` appropriately. Write a well-documented and optimized solution to demonstrate your understanding of the `gc` module and memory management in Python.","solution":"import gc def detect_cycles(thresholds: tuple, debug_flags: int) -> dict: Sets specific thresholds for garbage collection, enables debugging with specified flags, runs garbage collection explicitly, and returns details of objects involved in detected reference cycles. Args: thresholds (tuple): A tuple of three integers (threshold0, threshold1, threshold2) specifying the garbage collection thresholds. debug_flags (int): An integer representing the debugging flags. Returns: dict: A dictionary with the details of garbage collection process including: - \'collected\': The number of objects successfully collected. - \'uncollectable\': The number of objects identified as uncollectable. - \'garbage\': A list of uncollectable objects tracked by gc.garbage. # Ensure the thresholds are non-negative integers assert all(isinstance(t, int) and t >= 0 for t in thresholds), \\"Thresholds must be non-negative integers.\\" # Setting garbage collection thresholds gc.set_threshold(*thresholds) # Enabling debugging gc.set_debug(debug_flags) # Explicitly run garbage collection collected = gc.collect(generation=2) # Retrieve the number of uncollectable objects and the list of garbage uncollectable = len(gc.garbage) garbage = gc.garbage.copy() # Disable debugging output gc.set_debug(0) # Return the details of the garbage collection process return { \'collected\': collected, \'uncollectable\': uncollectable, \'garbage\': garbage }"},{"question":"Using the `seaborn.objects` module, create a function `custom_plot(data)` that takes a pandas DataFrame `data` and generates a customized plot layout with the following specifications: 1. The figure size should be set to 6x6 inches. 2. Create a 2x2 grid of subplots. The subplots should use the first and second columns of the DataFrame for faceting along the rows and the third and fourth columns for faceting along the columns. 3. Use the \\"constrained\\" layout engine to manage the spacing between the subplots. 4. Adjust the extent of the plot so that it occupies the bottom 80% of the figure. The function should return the Plot object. Function Signature ```python def custom_plot(data: pd.DataFrame) -> so.Plot: ``` Input - `data`: A pandas DataFrame with at least 4 columns. Output - Returns a `seaborn.objects.Plot` object configured according to the specifications. Constraints 1. Assume the DataFrame `data` has at least 4 columns and a sufficient number of rows to create meaningful plots. 2. You cannot use any other plotting library to achieve the task. # Example ```python import pandas as pd import seaborn.objects as so # Sample DataFrame data = pd.DataFrame({ \'A\': range(10), \'B\': range(10, 20), \'X\': range(20, 30), \'Y\': range(30, 40) }) plot = custom_plot(data) plot.show() ``` # Notes - If the DataFrame `data` has more than 4 columns, only the first four columns should be considered for creating the subplot structure.","solution":"import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def custom_plot(data: pd.DataFrame) -> so.Plot: Generates a customized plot layout using seaborn.objects. Parameters: data (pd.DataFrame): A pandas DataFrame with at least 4 columns. Returns: plot (so.Plot): Configured seaborn plot object. if data.shape[1] < 4: raise ValueError(\\"DataFrame must have at least 4 columns.\\") # Extract columns for faceting row1, row2 = data.columns[0], data.columns[1] col1, col2 = data.columns[2], data.columns[3] # Create the plot p = so.Plot(data) # Define the layout fig, axs = plt.subplots(2, 2, figsize=(6, 6), constrained_layout=True) fig.subplots_adjust(bottom=0.2) # Extend plot to bottom 80% of the figure # Return the Plot object return p"},{"question":"You are required to design a Python function that processes customer orders for an e-commerce platform. Your function will: 1. Filter out invalid orders based on specific criteria. 2. Summarize the valid orders by product category. 3. Provide a flexible parameter setting for handling varying orders using default and keyword arguments. # Detailed Requirements 1. **Function Signature**: ```python def process_orders(orders: list, min_value: int = 10, discount: float = 0.10, *, verbose=False) -> dict: ``` 2. **Input**: - `orders`: A list of dictionaries where each dictionary represents an order. Each order has the following keys: - `\'order_id\'`: a unique identifier for the order (integer) - `\'category\'`: the product category (string) - `\'value\'`: the order\'s value (float) - `min_value`: An integer (default `10`). Orders with a `value` less than this will be considered invalid. - `discount`: A float (default `0.10`). A discount that needs to be applied to all valid orders. - `verbose`: A keyword-only boolean argument (default `False`). When `True`, print detailed messages about the process. 3. **Output**: - Return a dictionary summarizing the valid orders. The keys should be the product categories, and the values should be the total discounted order value for each category. # Constraints: - Each order dictionary is guaranteed to have valid keys but not necessarily valid values. - The function should utilize for loops for iterating through orders and if statements to filter invalid ones. - Avoid modifying the original orders list. - Proper use of default and keyword arguments is expected. # Example: ```python orders = [ {\'order_id\': 1, \'category\': \'books\', \'value\': 12.99}, {\'order_id\': 2, \'category\': \'electronics\', \'value\': 9.99}, {\'order_id\': 3, \'category\': \'books\', \'value\': 22.50}, {\'order_id\': 4, \'category\': \'toys\', \'value\': 15.00}, ] result = process_orders(orders, min_value=10, discount=0.10, verbose=True) print(result) ``` **Expected Output**: ``` Processing order ID: 1 Processing order ID: 3 Processing order ID: 4 { \'books\': (12.99 * 0.90) + (22.50 * 0.90), \'toys\': 15.00 * 0.90 } ``` # Note: Ensure to properly document your function with docstrings and adhere to proper coding style as per PEP 8.","solution":"def process_orders(orders: list, min_value: int = 10, discount: float = 0.10, *, verbose=False) -> dict: Processes customer orders by filtering and summarizing valid orders by category. Parameters: - orders (list): List of dictionaries where each dictionary represents an order. - min_value (int): Minimum value for an order to be considered valid. Default is 10. - discount (float): Discount to be applied to all valid orders. Default is 0.10. - verbose (bool): If True, print detailed process messages. Default is False. Returns: - dict: A dictionary summarizing the total discounted order value by product category. summary = {} for order in orders: order_id = order.get(\'order_id\') category = order.get(\'category\') value = order.get(\'value\') if value is None or category is None or order_id is None: continue if value >= min_value: discounted_value = value * (1 - discount) if category in summary: summary[category] += discounted_value else: summary[category] = discounted_value if verbose: print(f\\"Processing order ID: {order_id}\\") return summary"},{"question":"# Naive Bayes Classifier Comparison You are given a dataset representing a collection of text documents, and your task is to classify these documents into different categories. You are required to use different Naive Bayes classifiers provided by `scikit-learn` and compare their performance to determine the best model for this classification task. Dataset The dataset is a CSV file (`text_data.csv`) with the following columns: - `text`: The text content of the document. - `label`: The category label for the document. Task 1. **Data Preprocessing**: - Load the dataset and split it into training and test sets. - Convert the text data into numerical features using both `CountVectorizer` and `TfidfVectorizer`. - For `BernoulliNB`, binarize the features. 2. **Model Implementation**: - Implement and train the following Naive Bayes classifiers: - `GaussianNB` - `MultinomialNB` - `ComplementNB` - `BernoulliNB` 3. **Model Evaluation**: - Evaluate the performance of each classifier using appropriate metrics (e.g., accuracy, precision, recall). - Compare the performance of the classifiers to determine which one performs the best. 4. **Bonus**: - Explain which Naive Bayes classifier is most suitable for this text classification problem and why. Constraints and Performance Requirements - You should use the `scikit-learn` library for model implementation. - Ensure that your code is efficient and runs within a reasonable time frame. Expected Input and Output - **Input**: The path to the CSV file (`text_data.csv`). - **Output**: Performance metrics for each Naive Bayes classifier and a conclusion on which classifier is the best. Example Code ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, precision_score, recall_score def load_data(file_path): # Load the dataset df = pd.read_csv(file_path) return df def preprocess_data(df): # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(df[\'text\'], df[\'label\'], test_size=0.2, random_state=42) # Feature extraction count_vectorizer = CountVectorizer() tfidf_vectorizer = TfidfVectorizer() X_train_count = count_vectorizer.fit_transform(X_train) X_test_count = count_vectorizer.transform(X_test) X_train_tfidf = tfidf_vectorizer.fit_transform(X_train) X_test_tfidf = tfidf_vectorizer.transform(X_test) return X_train_count, X_test_count, X_train_tfidf, X_test_tfidf, y_train, y_test def evaluate_model(clf, X_train, X_test, y_train, y_test): # Train the model clf.fit(X_train.toarray(), y_train) # Predict y_pred = clf.predict(X_test.toarray()) # Metrics accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') return accuracy, precision, recall if __name__ == \\"__main__\\": # Load data df = load_data(\'text_data.csv\') # Preprocess data X_train_count, X_test_count, X_train_tfidf, X_test_tfidf, y_train, y_test = preprocess_data(df) # Initialize models gnb = GaussianNB() mnb = MultinomialNB() cnb = ComplementNB() bnb = BernoulliNB() # Evaluate GaussianNB accuracy, precision, recall = evaluate_model(gnb, X_train_count, X_test_count, y_train, y_test) print(f\\"GaussianNB - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\\") # Evaluate MultinomialNB accuracy, precision, recall = evaluate_model(mnb, X_train_count, X_test_count, y_train, y_test) print(f\\"MultinomialNB - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\\") # Evaluate ComplementNB accuracy, precision, recall = evaluate_model(cnb, X_train_count, X_test_count, y_train, y_test) print(f\\"ComplementNB - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\\") # Evaluate BernoulliNB X_train_binarized = (X_train_count > 0).astype(int) X_test_binarized = (X_test_count > 0).astype(int) accuracy, precision, recall = evaluate_model(bnb, X_train_binarized, X_test_binarized, y_train, y_test) print(f\\"BernoulliNB - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\\") # Draw conclusions on the best classifier based on evaluations # (Include this in your report) ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, precision_score, recall_score def load_data(file_path): df = pd.read_csv(file_path) return df def preprocess_data(df): X_train, X_test, y_train, y_test = train_test_split(df[\'text\'], df[\'label\'], test_size=0.2, random_state=42) count_vectorizer = CountVectorizer() tfidf_vectorizer = TfidfVectorizer() X_train_count = count_vectorizer.fit_transform(X_train) X_test_count = count_vectorizer.transform(X_test) X_train_tfidf = tfidf_vectorizer.fit_transform(X_train) X_test_tfidf = tfidf_vectorizer.transform(X_test) return X_train_count, X_test_count, X_train_tfidf, X_test_tfidf, y_train, y_test def evaluate_model(clf, X_train, X_test, y_train, y_test): clf.fit(X_train.toarray(), y_train) y_pred = clf.predict(X_test.toarray()) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') return accuracy, precision, recall if __name__ == \\"__main__\\": df = load_data(\'text_data.csv\') X_train_count, X_test_count, X_train_tfidf, X_test_tfidf, y_train, y_test = preprocess_data(df) gnb = GaussianNB() mnb = MultinomialNB() cnb = ComplementNB() bnb = BernoulliNB() accuracy, precision, recall = evaluate_model(gnb, X_train_count, X_test_count, y_train, y_test) print(f\\"GaussianNB - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\\") accuracy, precision, recall = evaluate_model(mnb, X_train_count, X_test_count, y_train, y_test) print(f\\"MultinomialNB - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\\") accuracy, precision, recall = evaluate_model(cnb, X_train_count, X_test_count, y_train, y_test) print(f\\"ComplementNB - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\\") X_train_binarized = (X_train_count > 0).astype(int) X_test_binarized = (X_test_count > 0).astype(int) accuracy, precision, recall = evaluate_model(bnb, X_train_binarized, X_test_binarized, y_train, y_test) print(f\\"BernoulliNB - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}\\")"},{"question":"# XML Document Processing and Manipulation You are tasked with writing a Python function that processes an XML document representing a collection of books. The XML document has a structure where each book contains a title, an author, a genre, and a price. Here is an example of the XML structure: ```xml <catalog> <book id=\\"bk101\\"> <title>Python Programming</title> <author>John Doe</author> <genre>Programming</genre> <price>29.99</price> </book> <book id=\\"bk102\\"> <title>Advanced Data Analysis</title> <author>Jane Smith</author> <genre>Data Science</genre> <price>39.95</price> </book> <!-- more book elements --> </catalog> ``` Your task is to implement the function `process_books(xml_data: str) -> dict` that performs the following operations: 1. **Parse the XML data** and build the XML tree. 2. **Extract the total number of books** in the catalog. 3. **Find the most expensive book** in the catalog and retrieve its title. 4. **Create a dictionary** where the keys are genres and the values are lists of titles of books that belong to those genres. The function should return a dictionary with this information structured as follows: ```python { \\"total_books\\": total_books, # integer \\"most_expensive_book_title\\": most_expensive_book_title, # string \\"books_by_genre\\": { \\"genre1\\": [list of titles], \\"genre2\\": [list of titles], # more genres and their book titles } } ``` **Input:** - `xml_data` (str): A string containing the XML data. **Output:** - (dict): A dictionary containing the total number of books, the title of the most expensive book, and a dictionary of books categorized by genre. Here are the constraints: - All prices are positive floating-point numbers. - The XML structure is guaranteed to be well-formed and follows the described format. - The input `xml_data` string will be non-empty. **Example:** ```python xml_string = \'\'\' <catalog> <book id=\\"bk101\\"> <title>Python Programming</title> <author>John Doe</author> <genre>Programming</genre> <price>29.99</price> </book> <book id=\\"bk102\\"> <title>Advanced Data Analysis</title> <author>Jane Smith</author> <genre>Data Science</genre> <price>39.95</price> </book> </catalog> \'\'\' result = process_books(xml_string) # Expected output: # { # \\"total_books\\": 2, # \\"most_expensive_book_title\\": \\"Advanced Data Analysis\\", # \\"books_by_genre\\": { # \\"Programming\\": [\\"Python Programming\\"], # \\"Data Science\\": [\\"Advanced Data Analysis\\"] # } # } ``` Your implementation should use the `xml.etree.ElementTree` module for parsing and manipulating the XML data. Ensure that your code is efficient and handles the parsing and extraction tasks correctly.","solution":"import xml.etree.ElementTree as ET def process_books(xml_data: str) -> dict: Process the given XML data and return information about books. Args: - xml_data (str): A string containing the XML data. Returns: - dict: A dictionary containing the total number of books, the most expensive book title, and a dictionary of books categorized by genre. tree = ET.ElementTree(ET.fromstring(xml_data)) root = tree.getroot() total_books = 0 max_price = 0 most_expensive_book_title = \\"\\" books_by_genre = {} for book in root.findall(\'book\'): total_books += 1 title = book.find(\'title\').text genre = book.find(\'genre\').text price = float(book.find(\'price\').text) # Update most expensive book information if price > max_price: max_price = price most_expensive_book_title = title # Add book title to the genre list in the dictionary if genre not in books_by_genre: books_by_genre[genre] = [] books_by_genre[genre].append(title) return { \\"total_books\\": total_books, \\"most_expensive_book_title\\": most_expensive_book_title, \\"books_by_genre\\": books_by_genre }"},{"question":"**Coding Assessment Question** # Objective This coding question aims to assess your understanding of data visualization using the Seaborn `objects` interface. You will be required to create a series of plots to visualize different aspects of a dataset. # Problem Statement You are given a dataset called `fmri` containing functional magnetic resonance imaging (fMRI) data from a neuroscience experiment. Your task is to create a series of plots to visualize this data using the Seaborn `objects` interface. # Dataset Details The `fmri` dataset consists of the following columns: - `subject`: Identifier for each subject involved in the experiment. - `timepoint`: Time points at which measurements were taken. - `event`: Type of event (e.g., stimulus or control) during which measurements were recorded. - `region`: Brain region where the measurement was taken. - `signal`: The signal measured at each `timepoint`. # Tasks 1. **Load the Dataset**: Load the `fmri` dataset using `seaborn.load_dataset`. 2. **Line Plot**: Create a line plot to show the `signal` over `timepoint` for a specific `region` (\'parietal\') and `event` (\'stim\'). Group the lines by `subject`. 3. **Adding Error Bands**: Modify the plot to add an error band representing the standard error of the `signal` over `timepoint` for the specified `region` and `event`. 4. **Styling Lines**: Further enhance the plot by distinguishing lines for different `events` (e.g., different linestyles or colors) and using error bands to represent the standard error. 5. **Adding Markers**: Add markers to the line plots to indicate the sampled data points. # Function Signature ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_fmri_data(): # Load the fmri dataset fmri = load_dataset(\\"fmri\\") # Filter specific region and event fmri_filtered = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") # Create a base plot object p = so.Plot(fmri_filtered, \\"timepoint\\", \\"signal\\") # Task 2: Line plot for specific region and event, grouped by subject line_plot = p.add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") # Show the plot for Task 2 plt.figure() line_plot.show() # Task 3: Add error bands error_plot = p.add(so.Line(), so.Agg()).add(so.Band(), so.Est(), group=\\"event\\") # Show the plot for Task 3 plt.figure() error_plot.show() # Task 4: Enhance with different styles for lines based on event styled_plot = p.add(so.Line(), so.Agg()).add(so.Band(), so.Est(), group=\\"event\\").add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) # Show the plot for Task 4 plt.figure() styled_plot.show() # Please make sure to show the generated plots (for example, by calling the function and showing the plots in a Jupyter Notebook). ``` # Constraints - You should use the `seaborn.objects` interface provided in Seaborn for all plotting tasks. - Ensure that the plots are clear and well-labeled. - Handle any potential exceptions that may arise from the dataset operations or plotting. # Evaluation Your implementation will be evaluated on: - Correctness: Following the problem statement and accurately implementing the required tasks. - Usage of Seaborn `objects` interface: Demonstrating understanding and appropriate usage of the Seaborn `objects` interface. - Visualization quality: Producing clear and informative visualizations. - Code quality: Writing clean, readable, and well-documented code.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_fmri_data(): Plots fMRI data from the parietal region during stim events with various enhancements including error bands, different styles based on event, and markers indicating sampled data points. # Load the fmri dataset fmri = load_dataset(\\"fmri\\") # Filter specific region and event fmri_filtered = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") # Create a base plot object p = so.Plot(fmri_filtered, \\"timepoint\\", \\"signal\\") # Task 2: Line plot for specific region and event, grouped by subject line_plot = p.add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") # Show the plot for Task 2 plt.figure() line_plot.show() # Task 3: Add error bands error_plot = p.add(so.Line(), so.Agg()).add(so.Band(), so.Est(), group=\\"event\\") # Show the plot for Task 3 plt.figure() error_plot.show() # Task 4: Enhance with different styles for lines based on event styled_plot = p.add(so.Line(), so.Agg()).add(so.Band(), so.Est(), group=\\"event\\").add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) # Show the plot for Task 4 plt.figure() styled_plot.show()"},{"question":"**PyTorch IR Implementation Challenge** **Objective:** Implement a function using the PyTorch library that performs a series of operations compatible with Core Aten IR and Prims IR principles. **Problem Statement:** You are required to implement a function `perform_ir_operations` that takes two input tensors and performs a series of operations using Core Aten IR principles. Specifically, the function should: 1. Ensure both tensors are of the same shape through broadcasting. 2. Perform an element-wise addition of the two tensors. 3. Convert the resultant tensor to a specified data type. 4. Return the final tensor. **Constraints:** - You are not allowed to use in-place operations. - The function should handle tensors of different shapes by appropriately broadcasting them. - The function should ensure the resultant tensor is explicitly type-promoted to a given data type. **Function Signature:** ```python import torch def perform_ir_operations(tensor1: torch.Tensor, tensor2: torch.Tensor, dtype: torch.dtype) -> torch.Tensor: Perform operations using Core Aten IR principles. Args: tensor1: First input tensor. tensor2: Second input tensor. dtype: The desired dtype for the resultant tensor. Returns: A tensor that is the result of element-wise addition of the input tensors, broadcasted to the same shape and converted to the specified dtype. pass ``` **Example:** ```python # Example input tensors t1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) t2 = torch.tensor([1.0, 2.0]) # Desired output dtype output_dtype = torch.float64 # Calling the function result = perform_ir_operations(t1, t2, output_dtype) # Expected result after broadcasting, addition and type conversion # tensor([[2.0, 4.0], [4.0, 6.0]], dtype=torch.float64) print(result) ``` **Notes:** - You might find operations such as `torch.broadcast_tensors`, `torch.add`, and tensor type conversion functionalities (`tensor.to()` or `tensor.type()`) useful in implementing the solution. - Pay attention to the shape compatibility between the tensors and ensure the final tensor respects the specified data type.","solution":"import torch def perform_ir_operations(tensor1: torch.Tensor, tensor2: torch.Tensor, dtype: torch.dtype) -> torch.Tensor: Perform operations using Core Aten IR principles. Args: tensor1: First input tensor. tensor2: Second input tensor. dtype: The desired dtype for the resultant tensor. Returns: A tensor that is the result of element-wise addition of the input tensors, broadcasted to the same shape and converted to the specified dtype. # Broadcast the tensors to the same shape broadcasted_tensors = torch.broadcast_tensors(tensor1, tensor2) # Element-wise addition of the broadcasted tensors result_tensor = broadcasted_tensors[0] + broadcasted_tensors[1] # Convert the resultant tensor to the specified dtype result_tensor = result_tensor.to(dtype) return result_tensor"},{"question":"Question: Implementing a Cache with Weak References # Background You are tasked with designing a caching system that should automatically discard objects when they are no longer strongly referenced elsewhere in the application. This is to ensure that the cache does not unnecessarily prevent the garbage collection of objects that are no longer needed. # Objectives Implement a `WeakCache` class that uses weak references to store cache items. The class should support the following functionalities: 1. **Storing items with a key**: Items should be stored in the cache with a unique key. If an item with the same key already exists, it should be replaced. 2. **Retrieving items by key**: The cache should allow retrieval of items using their keys. If the item corresponding to a key has been garbage collected, the key should be considered invalid. 3. **Deleting items by key**: It should be possible to explicitly remove items from the cache using their keys. 4. **Automatic cleanup**: The cache should automatically remove entries for items that are garbage collected. # Implementation Details - Use the `weakref.WeakValueDictionary` class to manage the cache. - Implement the `WeakCache` class with the following methods: - `__init__()`: Initializes an empty cache. - `__setitem__(key, value)`: Stores the `value` in the cache with the specified `key`. - `__getitem__(key)`: Retrieves the `value` associated with the `key`. Raises `KeyError` if the key is not found or the item has been garbage collected. - `__delitem__(key)`: Deletes the entry associated with the `key`. # Example Usage ```python cache = WeakCache() cache[\'image1\'] = large_image_object_1 # Storing a large image object cache[\'image2\'] = large_image_object_2 # Storing another large image object # Accessing stored images image1 = cache[\'image1\'] image2 = cache[\'image2\'] # Deleting explicitely del cache[\'image1\'] # If the only strong reference to `large_image_object_2` was in the cache, it may now be garbage collected ``` # Constraints - The keys will be unique strings. - The values will be Python objects that can be weakly referenced. - Aim for efficient performance, utilizing weak references to ensure memory management is optimized. # Submission Implement the `WeakCache` class based on the specifications above and submit your code. ```python import weakref class WeakCache: def __init__(self): self.cache = weakref.WeakValueDictionary() def __setitem__(self, key, value): self.cache[key] = value def __getitem__(self, key): value = self.cache[key] if value is None: raise KeyError(f\\"Key {key} has been garbage collected\\") return value def __delitem__(self, key): if key in self.cache: del self.cache[key] else: raise KeyError(f\\"Key {key} not found in cache\\") ``` Implement these methods and ensure your solution handles automatic garbage collection appropriately.","solution":"import weakref class WeakCache: def __init__(self): self.cache = weakref.WeakValueDictionary() def __setitem__(self, key, value): self.cache[key] = value def __getitem__(self, key): try: return self.cache[key] except KeyError: raise KeyError(f\\"Key {key} not found in cache or has been garbage collected\\") def __delitem__(self, key): try: del self.cache[key] except KeyError: raise KeyError(f\\"Key {key} not found in cache\\")"},{"question":"You are tasked with creating a Python program that reads a list of integers from a file, processes the data, and handles any potential errors gracefully. The goal of this exercise is to assess your understanding of exception handling, including custom exceptions, clean-up actions, and the use of context managers. Task: Write a function `process_integers(file_path: str) -> List[int]` that: 1. Opens and reads a file containing a list of integers, one integer per line. 2. Processes each line to convert the string to an integer. 3. Handles any potential errors gracefully: - If the file does not exist, raise a custom `FileNotFoundCustomError`. - If a line in the file cannot be converted to an integer, skip that line and log an error message. 4. Ensures the file is closed after processing. 5. Returns a list of successfully processed integers. Requirements: - Define a custom exception `FileNotFoundCustomError` which should inherit from `Exception`. - Use a context manager (`with` statement) to handle file opening and closing. - Log any lines that cannot be converted to an integer using a simple print statement indicating the line number and content. - Include unit tests for your function to validate its correctness and handle various edge cases. Input: - A string `file_path` which is the path to the file containing integers. Output: - A list of integers successfully read and processed from the file. Constraints: - The file might not exist. - The file might contain non-integer lines. - The file might be very large, so you should avoid loading the entire file into memory at once. Example: Suppose the file `numbers.txt` contains the following lines: ``` 23 42 invalid 78 90 ``` Here is the code for defining the function `process_integers`: ```python from typing import List class FileNotFoundCustomError(Exception): pass def process_integers(file_path: str) -> List[int]: integers = [] try: with open(file_path, \'r\') as file: for line_no, line in enumerate(file, start=1): try: integers.append(int(line.strip())) except ValueError: print(f\\"Error on line {line_no}: cannot convert {line.strip()} to integer.\\") except FileNotFoundError: raise FileNotFoundCustomError(f\\"The file {file_path} does not exist.\\") return integers # Example usage: # integers = process_integers(\'numbers.txt\') # print(integers) # Output: [23, 42, 78, 90] ``` Unit Tests: Develop unit tests to cover the edge cases: 1. File does not exist. 2. File contains non-integer lines. 3. File is empty. 4. File contains only valid integers. Write the `process_integers` function and the unit tests to validate your implementation.","solution":"from typing import List class FileNotFoundCustomError(Exception): pass def process_integers(file_path: str) -> List[int]: integers = [] try: with open(file_path, \'r\') as file: for line_no, line in enumerate(file, start=1): try: integers.append(int(line.strip())) except ValueError: print(f\\"Error on line {line_no}: cannot convert {line.strip()} to integer.\\") except FileNotFoundError: raise FileNotFoundCustomError(f\\"The file {file_path} does not exist.\\") return integers"},{"question":"**Problem Statement: Nested Configuration Management using ChainMap and Counter** You are tasked to manage configuration settings in a software environment where configurations can be nested and can be overridden at different levels (e.g., user, project, system defaults). For this exercise, you\'ll use the `ChainMap` to handle these nested configurations. Additionally, you will use `Counter` to keep track of how many configurations have been overridden or added at each level. # Requirements: 1. **Initialization**: - Your function should initialize with three configuration dictionaries: `user_config`, `project_config`, and `default_config`. - Use a `ChainMap` to aggregate these configurations in such a way that the highest priority is given to `user_config`, then `project_config`, and finally `default_config`. 2. **Add/Update Configuration**: - Implement a method `add_config(level: str, key: str, value)` to add or update a configuration. `level` will specify which configuration (`user`, `project`, or `default`) to update. - If `level` is `user`, update `user_config`. - If `level` is `project`, update `project_config`. - If `level` is `default`, update `default_config`. - Track how many configurations have been added or updated at each level using a `Counter`. 3. **Retrieve Configuration**: - Implement a method `get_config(key: str)` to retrieve the value of a configuration based on the highest priority `ChainMap`. 4. **Override Count**: - Implement a method `get_override_counts()` that returns a dictionary indicating how many configurations have been added or updated at each level (`user`, `project`, `default`). # Input/Output: - **Initialization**: ```python manager = ConfigManager(user_config, project_config, default_config) ``` - **Add Configuration**: ```python manager.add_config(\'user\', \'theme\', \'dark\') manager.add_config(\'project\', \'resolution\', \'1080p\') manager.add_config(\'default\', \'language\', \'EN\') ``` - **Retrieve Configuration**: ```python value = manager.get_config(\'theme\') ``` - **Override Count**: ```python counts = manager.get_override_counts() ``` # Constraints: - You can assume keys and values are strings. - Configurations only exist in the form of dictionaries. - When adding/updating configurations, overrides are allowed. # Example: ```python user_config = {\'theme\': \'light\'} project_config = {\'resolution\': \'720p\'} default_config = {\'language\': \'EN\', \'theme\': \'default\'} manager = ConfigManager(user_config, project_config, default_config) manager.add_config(\'user\', \'theme\', \'dark\') manager.add_config(\'project\', \'resolution\', \'1080p\') print(manager.get_config(\'theme\')) # Output: \'dark\' print(manager.get_config(\'resolution\')) # Output: \'1080p\' print(manager.get_config(\'language\')) # Output: \'EN\' print(manager.get_override_counts()) # Output: {\'user\': 1, \'project\': 1, \'default\': 0} ``` # Implementation: Implement the `ConfigManager` class with required methods.","solution":"from collections import ChainMap, Counter class ConfigManager: def __init__(self, user_config, project_config, default_config): self.user_config = user_config self.project_config = project_config self.default_config = default_config self.chain = ChainMap(self.user_config, self.project_config, self.default_config) self.override_counter = Counter() def add_config(self, level: str, key: str, value): if level == \'user\': self.user_config[key] = value elif level == \'project\': self.project_config[key] = value elif level == \'default\': self.default_config[key] = value else: raise ValueError(\\"Invalid level. Must be one of \'user\', \'project\', or \'default\'.\\") self.override_counter[level] += 1 # re-initialize ChainMap to reflect updated configurations self.chain = ChainMap(self.user_config, self.project_config, self.default_config) def get_config(self, key: str): return self.chain.get(key) def get_override_counts(self): return dict(self.override_counter)"},{"question":"Objective: To assess the understanding of the Python `dataclasses` module, focusing on advanced features such as post-init processing, handling mutable default values, and frozen instances. Problem Statement: You are tasked with designing a data class to manage orders in an e-commerce platform. Each order consists of one or more `OrderItem`s. You will need to implement the following classes: 1. **OrderItem**: Represents an item in an order. 2. **Order**: Represents an entire order consisting of multiple `OrderItem`s. Requirements: 1. **OrderItem Class**: - Fields: - `product_name` (type: `str`) - `unit_price` (type: `float`) - `quantity` (type: `int`, default: `1`) 2. **Order Class**: - Fields: - `order_id` (type: `str`) - `items` (type: `List[OrderItem]`, with a default factory that initializes an empty list) - `status` (type: `str`, default: `\\"Pending\\"`, keyword-only field) - Methods: - `__post_init__(self)`: - Ensures that the `order_id` follows the pattern \\"ORD-\\", followed by a numeric sequence (e.g., \\"ORD-1234\\"). - Raises a `ValueError` if the pattern is not met. - `add_item(self, item: OrderItem)`: Adds an `OrderItem` to the order. - `total_cost(self) -> float`: Returns the total cost of the order by summing the total cost of all items. 3. **Frozen Instances**: - The `Order` class should be immutable after creation (i.e., any attempt to modify its fields should raise a `FrozenInstanceError`). Input and Output: - **Input**: The classes and methods should handle their own inputs. Instantiation and method calls will be done in the test cases. - **Output**: The methods should return the required values or ensure the correct state of the objects. Constraints: - All field values must meet their specified types. - The `order_id` must follow the specified pattern for validation. - Operations should comply with the immutability of the `Order` class once instantiated. Example Usage: ```python from dataclasses import dataclass, field # Define your classes here # Example usage of the classes item1 = OrderItem(product_name=\\"Laptop\\", unit_price=1200.00) item2 = OrderItem(product_name=\\"Mouse\\", unit_price=50.00, quantity=2) order = Order(order_id=\\"ORD-1001\\", items=[item1], status=\\"Pending\\") order.add_item(item2) print(order.total_cost()) # Expected Output: 1300.00 print(order) # Expected Output: Order(order_id=\'ORD-1001\', items=[OrderItem(product_name=\'Laptop\', unit_price=1200.0, quantity=1), OrderItem(product_name=\'Mouse\', unit_price=50.0, quantity=2)], status=\'Pending\') # Attempting to modify the order should raise an error try: order.status = \\"Shipped\\" except FrozenInstanceError: print(\\"Cannot modify a frozen Order instance!\\") ``` Your task: 1. Implement the `OrderItem` and `Order` classes as described. 2. Ensure all specified requirements are met. 3. Write appropriate test cases to validate the functionality of your implementation.","solution":"from dataclasses import dataclass, field, FrozenInstanceError from typing import List import re @dataclass class OrderItem: product_name: str unit_price: float quantity: int = 1 def total_cost(self) -> float: return self.unit_price * self.quantity @dataclass(frozen=True) class Order: order_id: str items: List[OrderItem] = field(default_factory=list) status: str = field(default=\\"Pending\\", kw_only=True) def __post_init__(self): if not re.match(r\'^ORD-d+\', self.order_id): raise ValueError(\\"order_id must follow the pattern \'ORD-\' followed by numbers.\\") def add_item(self, item: OrderItem): # Since the class is frozen, we can\'t directly modify items. Use object mutation workaround. object.__setattr__(self, \'items\', self.items + [item]) def total_cost(self) -> float: return sum(item.total_cost() for item in self.items)"},{"question":"You are provided with a deprecated Python module `pipes` which allows you to create and manage shell pipeline sequences. Although deprecated, it remains insightful to understand its usage. Problem Statement Using the `pipes` module, your task is to create a pipeline that performs the following operations in sequence on a provided input text: 1. Converts all lowercase letters to uppercase. 2. Reverses the order of the characters. 3. Replaces spaces with underscores. Instructions 1. Define a function `create_pipeline(input_text: str, output_filename: str) -> None` that performs the above steps. 2. Use the `pipes.Template` class to create and manage the pipeline. 3. The function should write the final output to a file specified by `output_filename`. Constraints - Assume the provided input text will not exceed 10,000 characters. - You must use the `pipes` module as specified, leveraging the template and commands appropriately. Example ```python import pipes def create_pipeline(input_text: str, output_filename: str) -> None: # Create a pipeline template t = pipes.Template() # Append commands to the pipeline t.append(\'tr a-z A-Z\', \'--\') # Convert lowercase to uppercase t.append(\'rev\', \'--\') # Reverse characters t.append(\'tr \\" \\" \\"_\\"\', \'--\') # Replace spaces with underscores # Write input text to pipeline and output to file with t.open(output_filename, \'w\') as f: f.write(input_text) # Example usage input_text = \\"hello world\\" output_filename = \'outputfile\' create_pipeline(input_text, output_filename) # The content of \'outputfile\' should be: DLROW_OLLEH with open(output_filename, \'r\') as f: print(f.read()) # Output: DLROW_OLLEH ``` Verification After implementing the function, ensure the `outputfile` contains the expected transformed text according to the sequence of operations defined. **Note**: Even though the `pipes` module is deprecated, this exercise aims to solidify your understanding of creating and manipulating shell pipelines programmatically.","solution":"import pipes def create_pipeline(input_text: str, output_filename: str) -> None: This function processes the input_text through a pipeline that: 1. Converts all lowercase letters to uppercase. 2. Reverses the order of the characters. 3. Replaces spaces with underscores. The final output is written to the file specified by output_filename. # Create a pipeline template t = pipes.Template() # Append commands to the pipeline t.append(\'tr a-z A-Z\', \'--\') # Convert lowercase to uppercase t.append(\'rev\', \'--\') # Reverse characters t.append(\'tr \\" \\" \\"_\\"\', \'--\') # Replace spaces with underscores # Write input text to pipeline and output to file with t.open(output_filename, \'w\') as f: f.write(input_text)"},{"question":"Objective: Demonstrate your understanding of the Python \\"builtins\\" module by implementing a custom class that enhances file reading functionality. Problem Statement: You are required to implement a class called `LineNumberFile` that wraps around the built-in `open` function. This class should enable reading a file\'s contents while adding line numbers to each line. Use the `builtins.open` function to ensure compatibility with the built-in file handling. 1. Implement the `LineNumberFile` class with the following methods: - `__init__(self, path: str)`: Opens a file at the specified path for reading. - `readline(self) -> str`: Reads a single line from the file, prefixed by its line number. - `read(self) -> str`: Reads all lines from the file, each prefixed by its line number. - `__enter__(self)` and `__exit__(self, exc_type, exc_val, exc_tb)`: To support context management (i.e., using the `with` statement). Requirements: - Do not use any third-party libraries. - Use the `builtins.open` function to open files. - The line numbers should start from 1. - Ensure proper handling of the file resource (i.e., the file should be properly closed after operations). Example: ```python # Given the following content in \'example.txt\': # Hello World # This is a test file. # Python is fun! ln_file = LineNumberFile(\'example.txt\') # Using readline(): print(ln_file.readline()) # Output: \'1: Hello Worldn\' print(ln_file.readline()) # Output: \'2: This is a test file.n\' # Using read(): ln_file = LineNumberFile(\'example.txt\') print(ln_file.read()) # Output: # \'1: Hello Worldn\' # \'2: This is a test file.n\' # \'3: Python is fun!n\' # Using with statement: with LineNumberFile(\'example.txt\') as ln_file: print(ln_file.read()) # Output: # \'1: Hello Worldn\' # \'2: This is a test file.n\' # \'3: Python is fun!n\' ``` Constraints: - You can assume that the file exists at the given path and that you have read permissions. - Handle any potential exceptions that might occur during file operations (e.g., file not found, read errors). Performance Requirements: - The implemented methods should have linear time complexity concerning the file size being read.","solution":"import builtins class LineNumberFile: def __init__(self, path: str): self.path = path self.file = builtins.open(path, \'r\') self.line_number = 0 def readline(self) -> str: line = self.file.readline() if line: self.line_number += 1 return f\\"{self.line_number}: {line}\\" else: return line def read(self) -> str: lines = [] for line in self.file: self.line_number += 1 lines.append(f\\"{self.line_number}: {line}\\") return \'\'.join(lines) def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): self.file.close() # Example content of example.txt for testing # Hello World # This is a test file. # Python is fun! # Usage Example: # ln_file = LineNumberFile(\'example.txt\') # print(ln_file.readline()) # Output: \'1: Hello Worldn\' # print(ln_file.readline()) # Output: \'2: This is a test file.n\' # ln_file = LineNumberFile(\'example.txt\') # print(ln_file.read()) # Output: # \'1: Hello Worldn\' # \'2: This is a test file.n\' # \'3: Python is fun!n\' # with LineNumberFile(\'example.txt\') as ln_file: # print(ln_file.read()) # # Output: # # \'1: Hello Worldn\' # # \'2: This is a test file.n\' # # \'3: Python is fun!n\'"},{"question":"Objectives: - Create, manipulate, and verify tensor views in PyTorch. - Understand and handle contiguous and non-contiguous tensors. Problem Statement: You are provided with a 3-dimensional tensor `tensor` of size `(4, 3, 2)` with random values. Implement a function `tensor_view_operations` that performs the following operations: 1. Create a view of this tensor such that it has the shape `(2, 6, 2)`. 2. Modify the element at position `(0, 0, 0)` of the view by setting it to -1. 3. Verify that the same element in the original tensor also reflects this change. 4. Check the contiguity of the new view tensor. 5. If the tensor is non-contiguous, convert it to a contiguous tensor and return the new shape. 6. Ultimately, return a tuple with: - the original tensor after modification, - whether the new view tensor was contiguous, - the shape of the contiguous tensor (if conversion was needed), - the id\'s of the storage of original and view tensors (to confirm same underlying data). Input: - A tensor of size `(4, 3, 2)` with random values, generated within the function. Output: - A tuple containing: - The modified original tensor. - A boolean indicating whether the new view tensor was contiguous. - The shape of the tensor after making it contiguous (if applicable). - The storage IDs of the original and view tensors to confirm shared data. Constraints: - Do not manually change the values of tensors except for the specified element. - Do not use additional functions or libraries for tensor manipulation outside of those provided by PyTorch. Example Usage: ```python import torch def tensor_view_operations(): # Step 1: Create original tensor tensor = torch.rand(4, 3, 2) # Step 2: Create a view tensor of shape (2, 6, 2) view_tensor = tensor.view(2, 6, 2) # Step 3: Modify the element at (0, 0, 0) of the view tensor view_tensor[0, 0, 0] = -1 # Verify change is reflected in original tensor assert tensor[0, 0, 0] == -1 # Step 4: Check if the view tensor is contiguous is_contiguous = view_tensor.is_contiguous() # Step 5: Make the tensor contiguous if needed if not is_contiguous: view_tensor_contiguous = view_tensor.contiguous() shape_contiguous_tensor = view_tensor_contiguous.shape else: shape_contiguous_tensor = view_tensor.shape storage_original = tensor.storage().data_ptr() storage_view = view_tensor.storage().data_ptr() # Step 6: Return the required tuple return tensor, is_contiguous, shape_contiguous_tensor, storage_original, storage_view # Example call to the function result = tensor_view_operations() print(result) # Output as described in the question ``` Note: - Assume the latest version of PyTorch is used. - Treat this as a standalone function without requiring external inputs.","solution":"import torch def tensor_view_operations(): # Step 1: Create original tensor tensor = torch.rand(4, 3, 2) # Step 2: Create a view tensor of shape (2, 6, 2) view_tensor = tensor.view(2, 6, 2) # Step 3: Modify the element at (0, 0, 0) of the view tensor view_tensor[0, 0, 0] = -1 # Verify change is reflected in original tensor assert tensor[0, 0, 0] == -1 # Step 4: Check if the view tensor is contiguous is_contiguous = view_tensor.is_contiguous() # Step 5: Make the tensor contiguous if needed if not is_contiguous: view_tensor_contiguous = view_tensor.contiguous() shape_contiguous_tensor = view_tensor_contiguous.shape else: shape_contiguous_tensor = view_tensor.shape storage_original = tensor.storage().data_ptr() storage_view = view_tensor.storage().data_ptr() # Step 6: Return the required tuple return tensor, is_contiguous, shape_contiguous_tensor, storage_original, storage_view"},{"question":"# Asyncio Synchronization Primitives **Problem Statement:** You are tasked with implementing an asynchronous resource manager using asyncio synchronization primitives. The resource manager controls access to a limited number of resources (`R`). Each task requests a resource, performs some work, and then releases it. The resource manager must ensure that no more than `R` tasks can hold the resource at the same time. Your implementation should provide the following functionalities: 1. `request_resource`: An asynchronous method for tasks to request a resource. 2. `release_resource`: An asynchronous method for tasks to release a resource. 3. Proper use of the `Semaphore` primitive to manage access to the resources. **Specifications:** - Implement the class `ResourceManager` with the following methods: - `__init__(self, resources: int)`: Initialize the resource manager with a given number of resources `R`. - `async def request_resource(self) -> None`: Method for a task to request a resource. - `async def release_resource(self) -> None`: Method for a task to release a resource. - **Input:** - `resources` (int): The number of available resources (1 ≤ `resources` ≤ 1000). - **Output:** N/A - However, you should ensure that your implementation properly handles resource requests and releases without exceeding the available resources. **Example Usage:** ```python import asyncio async def task(resource_manager, id): print(f\\"Task {id} requesting resource\\") await resource_manager.request_resource() print(f\\"Task {id} acquired resource\\") await asyncio.sleep(1) # Simulate work print(f\\"Task {id} releasing resource\\") await resource_manager.release_resource() async def main(): resource_manager = ResourceManager(3) tasks = [task(resource_manager, i) for i in range(5)] await asyncio.gather(*tasks) asyncio.run(main()) ``` In this example, ensure that only three tasks can hold the resource simultaneously. Tasks should wait if all resources are currently held and proceed only when a resource is released. **Constraints:** - Ensure no more than the prescribed number of resources are held by tasks simultaneously. - You should use the asyncio `Semaphore` to manage resource allocation. **Notes:** - Properly handle asynchronous behavior to ensure fair resource distribution. - Avoid common pitfalls like deadlocks and resource starvation.","solution":"import asyncio class ResourceManager: def __init__(self, resources: int): self._semaphore = asyncio.Semaphore(resources) async def request_resource(self) -> None: await self._semaphore.acquire() async def release_resource(self) -> None: self._semaphore.release()"},{"question":"# File and Directory Processing with Python Modules **Objective:** Write a Python function that takes a directory path as input and performs the following operations: 1. Lists all files in the directory and its subdirectories that match a specific pattern. 2. Compares files in pairs and identifies pairs of identical files based on their content. 3. Copies all non-duplicate files to a new directory, maintaining the folder structure. **Requirements:** 1. You must use the `pathlib`, `filecmp`, and `shutil` modules in your implementation. 2. The pattern for matching files should be `*.txt`. 3. Ensure the performance is optimized for a large number of files and subdirectories. **Function Signature:** ```python def process_files_and_directories(input_dir: str, output_dir: str) -> None: Processes files in the specified input directory and copies non-duplicate files to the output directory. Args: - input_dir (str): The path to the input directory. - output_dir (str): The path to the output directory. Returns: None ``` **Input:** - `input_dir`: A string representing the path of the input directory. - `output_dir`: A string representing the path of the output directory. **Output:** - This function should not return any value. It should create a new directory structure in `output_dir` containing only non-duplicate files from `input_dir`. **Constraints:** - The `input_dir` and `output_dir` paths will be valid directories with appropriate read/write permissions. - The file pattern to match is fixed as `*.txt`. - The solution should be efficient to handle large file systems with potentially thousands of files. **Example Usage:** ```python input_dir = \\"/path/to/input\\" output_dir = \\"/path/to/output\\" process_files_and_directories(input_dir, output_dir) ``` Upon executing the above function, the `output_dir` should contain all non-duplicate `.txt` files from the `input_dir` while preserving the directory structure.","solution":"from pathlib import Path import filecmp import shutil def process_files_and_directories(input_dir: str, output_dir: str) -> None: Processes files in the specified input directory and copies non-duplicate files to the output directory. Args: - input_dir (str): The path to the input directory. - output_dir (str): The path to the output directory. Returns: None input_path = Path(input_dir) output_path = Path(output_dir) # Step 1: List all .txt files in the directory and its subdirectories txt_files = list(input_path.glob(\'**/*.txt\')) # Step 2: Identify duplicate files based on content seen_files = {} duplicates = set() for file in txt_files: for seen_file in seen_files: if filecmp.cmp(file, seen_file, shallow=False): duplicates.add(file) break else: seen_files[file] = None # Step 3: Copy all non-duplicate files to a new directory, maintaining the folder structure for file in txt_files: if file not in duplicates: relative_path = file.relative_to(input_path) destination = output_path / relative_path destination.parent.mkdir(parents=True, exist_ok=True) shutil.copy2(file, destination)"},{"question":"**Programming Assessment Question:** # Function Overloading Utility In this task, you are required to implement a utility function to handle polymorphic operations on built-in types using Python’s built-in functions. You need to create a class `PolyOperations` that allows overloading for arithmetic and boolean operations on different types such as integers, floats, strings, and lists. # Specifications 1. **Class Definition:** - Define a class `PolyOperations` with methods to handle polymorphic operations. - Define methods for the following operations: - `add` to handle addition between two instances. - `multiply` to handle multiplication between two instances. - `logical_and` to handle logical AND operation between two instances. - `length_compare` to compare the lengths of two instances. 2. **Method Details:** - `add(self, other)`: Method to handle addition. - **Input:** - `self` and `other`: Can be int, float, string, or list. - **Output:** - If both are numbers, return their sum. - If both are strings or both are lists, concatenate them. - Otherwise, raise a `TypeError`. - `multiply(self, other)`: Method to handle multiplication. - **Input:** - `self` and `other`: Can be int, float, string, or list. - **Output:** - If both are numbers, return their product. - If one is a number and the other is a string or list, repeat the string or list the number of times specified. - Otherwise, raise a `TypeError`. - `logical_and(self, other)`: Method to handle logical AND operation. - **Input:** - `self` and `other`: Can be any type. - **Output:** - Use the `bool()` function to get the boolean equivalent and return the logical AND. - `length_compare(self, other)`: Method to compare lengths. - **Input:** - `self` and `other`: Can be string or list. - **Output:** - Return a string \'equal\', \'greater\', or \'less\' based on comparison of lengths. - If the types do not match the required types, raise a `TypeError`. # Constraints * You should utilize built-in functions where appropriate to handle operations and type-checking. * The methods should handle various edge cases such as negative numbers, empty strings/lists, and non-supported types. # Example ```python class PolyOperations: def __init__(self, value): self.value = value def add(self, other): # Implement method here def multiply(self, other): # Implement method here def logical_and(self, other): # Implement method here def length_compare(self, other): # Implement method here # Example usage: obj1 = PolyOperations(3) obj2 = PolyOperations(5) print(obj1.add(obj2)) # Output: 8 obj3 = PolyOperations(\\"Hello\\") obj4 = PolyOperations(\\"World\\") print(obj3.add(obj4)) # Output: \\"HelloWorld\\" obj5 = PolyOperations([1, 2, 3]) obj6 = PolyOperations(2) print(obj5.multiply(obj6)) # Output: [1, 2, 3, 1, 2, 3] obj7 = PolyOperations(True) obj8 = PolyOperations(False) print(obj7.logical_and(obj8)) # Output: False obj9 = PolyOperations(\\"test\\") obj10 = PolyOperations(\\"exam\\") print(obj9.length_compare(obj10)) # Output: \\"equal\\" ``` Implement the `PolyOperations` class and its methods as specified above and test with the examples provided.","solution":"class PolyOperations: def __init__(self, value): self.value = value def add(self, other): if isinstance(self.value, (int, float)) and isinstance(other.value, (int, float)): return self.value + other.value elif isinstance(self.value, str) and isinstance(other.value, str): return self.value + other.value elif isinstance(self.value, list) and isinstance(other.value, list): return self.value + other.value else: raise TypeError(\\"Unsupported types for addition\\") def multiply(self, other): if isinstance(self.value, (int, float)) and isinstance(other.value, (int, float)): return self.value * other.value elif (isinstance(self.value, (int, float)) and isinstance(other.value, (str, list))) or (isinstance(self.value, (str, list)) and isinstance(other.value, (int, float))): return self.value * other.value else: raise TypeError(\\"Unsupported types for multiplication\\") def logical_and(self, other): return bool(self.value) and bool(other.value) def length_compare(self, other): if isinstance(self.value, (str, list)) and isinstance(other.value, (str, list)): if len(self.value) == len(other.value): return \\"equal\\" elif len(self.value) > len(other.value): return \\"greater\\" else: return \\"less\\" else: raise TypeError(\\"Unsupported types for length comparison\\")"},{"question":"# Coding Task: Implementing a Localized Message Translator Description: Your task is to implement a class named `LocalizedTranslator` that supports internationalization and localization using the `gettext` module. This class should allow dynamic switching between languages and provide methods to retrieve translated messages for both singular and plural forms. You are required to: - Read \\".mo\\" files from a specified locale directory. - Provide methods for translating singular and plural messages. - Allow switching of languages dynamically at runtime. Requirements: 1. **Initializing with Locale Directory** - The class should initialize with a base locale directory to read \\".mo\\" files for various languages. 2. **Language Switching** - Implement a method to switch the language, which will load the appropriate \\".mo\\" file for the specified language. 3. **Translating Messages** - Implement methods to get the translations for singular and plural messages: - `get_translation(message: str) -> str`: Translates a singular message. - `get_plural_translation(singular: str, plural: str, n: int) -> str`: Translates a plural message based on the count `n`. 4. **Performance Requirement** - Switching languages should be efficient and not involve reloading already loaded translations. Input/Output: - **Input:** - Initialization with: ```python translator = LocalizedTranslator(locale_dir: str) ``` - `locale_dir`: The base directory where all locale-specific subdirectories and \\".mo\\" files are stored. - Method to switch languages: ```python translator.switch_language(language: str) ``` - `language`: The language code (e.g., \'en\', \'fr\', \'de\'). - Method to get singular translation: ```python translator.get_translation(message: str) -> str ``` - `message`: The message to be translated. - Method to get plural translation: ```python translator.get_plural_translation(singular: str, plural: str, n: int) -> str ``` - `singular`: Singular form of the message. - `plural`: Plural form of the message. - `n`: The count to determine the form of the message. - **Output:** - The appropriate localized string based on the current language and message. Constraints: - Assume \\".mo\\" files are correctly formatted and located in the directories specified by the language codes. - Handle cases where the translation for a given message does not exist by returning the original message. - Efficiently manage memory and avoid redundant loading of translation files. Example Usage: ```python # Assuming directory structure: # locale/ # ├── en/LC_MESSAGES/messages.mo # ├── fr/LC_MESSAGES/messages.mo # ├── de/LC_MESSAGES/messages.mo translator = LocalizedTranslator(\'path/to/locale\') translator.switch_language(\'en\') print(translator.get_translation(\'Welcome\')) # Output: \'Welcome\' translator.switch_language(\'fr\') print(translator.get_translation(\'Welcome\')) # Output: \'Bienvenue\' print(translator.get_plural_translation(\'There is one apple\', \'There are %d apples\', 5)) # Output: \'Il y a 5 pommes\' ``` Implement the `LocalizedTranslator` class in Python to fulfill the above requirements and example usage.","solution":"import gettext import os class LocalizedTranslator: def __init__(self, locale_dir): self.locale_dir = locale_dir self.translations = {} self.current_language = None def switch_language(self, language): if language == self.current_language: return # Language is already set, do nothing if language not in self.translations: # Load the .mo file for the language try: lang_translation = gettext.translation( \'messages\', localedir=self.locale_dir, languages=[language] ) self.translations[language] = lang_translation except FileNotFoundError: # If the .mo file does not exist, use null translation self.translations[language] = gettext.NullTranslations() self.current_language = language self.translations[language].install() def get_translation(self, message): return gettext.gettext(message) def get_plural_translation(self, singular, plural, n): return gettext.ngettext(singular, plural, n)"},{"question":"Objective Design and implement a Python function that processes a list of dictionaries representing students\' scores in various subjects and returns key statistics about the scores. Problem Statement You are provided a list of dictionaries where each dictionary contains student data with keys `\\"name\\"`, `\\"subject\\"`, and `\\"score\\"`. Your task is to write a function `calculate_statistics` that accepts this list and returns a dictionary containing: 1. The names of the top-performing student(s) in each subject. 2. The average score of the students per subject. 3. The number of students scoring above a given threshold for each subject. Function Signature ```python def calculate_statistics(student_scores: list, threshold: int) -> dict: pass ``` Input - `student_scores`: A list of dictionaries. Each dictionary contains: - `\\"name\\"`: a string representing the name of the student. - `\\"subject\\"`: a string representing the subject. - `\\"score\\"`: an integer representing the score. Example: ```python [ {\\"name\\": \\"Alice\\", \\"subject\\": \\"Math\\", \\"score\\": 95}, {\\"name\\": \\"Bob\\", \\"subject\\": \\"Math\\", \\"score\\": 85}, {\\"name\\": \\"Charlie\\", \\"subject\\": \\"History\\", \\"score\\": 90}, {\\"name\\": \\"David\\", \\"subject\\": \\"History\\", \\"score\\": 85}, {\\"name\\": \\"Alice\\", \\"subject\\": \\"History\\", \\"score\\": 92} ] ``` - `threshold`: An integer representing the score threshold. Output - A dictionary containing the following keys and their corresponding values: - `\\"top_performers\\"`: A dictionary where each key is a subject and each value is a list of names of the top-performing student(s) in that subject. - `\\"average_scores\\"`: A dictionary where each key is a subject and each value is the average score of the students in that subject. - `\\"above_threshold\\"`: A dictionary where each key is a subject and each value is the number of students scoring above the given threshold in that subject. Example: ```python { \\"top_performers\\": { \\"Math\\": [\\"Alice\\"], \\"History\\": [\\"Alice\\"] }, \\"average_scores\\": { \\"Math\\": 90.0, \\"History\\": 89.0 }, \\"above_threshold\\": { \\"Math\\": 1, \\"History\\": 1 } } ``` Constraints and Limitations - You may assume that the `student_scores` list is non-empty and well-formed. - Scores will be integers between 0 and 100. - The threshold score will be an integer between 0 and 100. - Performance: The solution should efficiently handle up to 1000 student score entries. Requirements - Utilize control flow tools demonstrated in the provided documentation. - Use appropriate function constructs and methodologies discussed. - Follow PEP 8 coding style guidelines. Example Usage ```python student_scores = [ {\\"name\\": \\"Alice\\", \\"subject\\": \\"Math\\", \\"score\\": 95}, {\\"name\\": \\"Bob\\", \\"subject\\": \\"Math\\", \\"score\\": 85}, {\\"name\\": \\"Charlie\\", \\"subject\\": \\"History\\", \\"score\\": 90}, {\\"name\\": \\"David\\", \\"subject\\": \\"History\\", \\"score\\": 85}, {\\"name\\": \\"Alice\\", \\"subject\\": \\"History\\", \\"score\\": 92} ] threshold = 90 print(calculate_statistics(student_scores, threshold)) # Output should be # { # \\"top_performers\\": { # \\"Math\\": [\\"Alice\\"], # \\"History\\": [\\"Alice\\"] # }, # \\"average_scores\\": { # \\"Math\\": 90.0, # \\"History\\": 89.0 # }, # \\"above_threshold\\": { # \\"Math\\": 1, # \\"History\\": 1 # } # } ```","solution":"def calculate_statistics(student_scores, threshold): subject_stats = {} for student in student_scores: name, subject, score = student[\'name\'], student[\'subject\'], student[\'score\'] if subject not in subject_stats: subject_stats[subject] = { \'top_performers\': [name], \'top_score\': score, \'total_score\': score, \'num_students\': 1, \'above_threshold\': 1 if score > threshold else 0 } else: if score > subject_stats[subject][\'top_score\']: subject_stats[subject][\'top_performers\'] = [name] subject_stats[subject][\'top_score\'] = score elif score == subject_stats[subject][\'top_score\']: subject_stats[subject][\'top_performers\'].append(name) subject_stats[subject][\'total_score\'] += score subject_stats[subject][\'num_students\'] += 1 if score > threshold: subject_stats[subject][\'above_threshold\'] += 1 result = { \'top_performers\': {}, \'average_scores\': {}, \'above_threshold\': {} } for subject, stats in subject_stats.items(): result[\'top_performers\'][subject] = stats[\'top_performers\'] result[\'average_scores\'][subject] = stats[\'total_score\'] / stats[\'num_students\'] result[\'above_threshold\'][subject] = stats[\'above_threshold\'] return result"},{"question":"# Coding Assessment: Model Performance Analysis using Validation and Learning Curves You are given a dataset, and your task is to evaluate the performance of an estimator (Support Vector Classifier) using both validation and learning curves. Your objective is to determine if the model is overfitting, underfitting, or performing well. Requirements: 1. **Load Data**: - Use the `load_wine` dataset from scikit-learn. 2. **Validation Curve**: - Use `validation_curve` to plot the influence of the hyperparameter `C` for an SVC with a linear kernel. - Use a range of values for `C` from `10^-3` to `10^3`. - Plot both training and validation scores. 3. **Learning Curve**: - Use `learning_curve` to plot the learning curve of the SVC with `C` value set to 1. - Use `train_sizes` to vary from `10%` to `100%` of the dataset. - Plot both training and validation scores. 4. **Visualization**: - Use `ValidationCurveDisplay` and `LearningCurveDisplay` from `sklearn.model_selection` to visualize the results. 5. **Analysis**: - Analyze the plots and briefly describe if the model is overfitting, underfitting, or performing well based on the curves generated. Input Format: ```python None ``` Output Format: - Display two plots: 1. Validation Curve showing training and validation scores against different values of `C`. 2. Learning Curve showing training and validation scores against varying training dataset sizes. - Provide a brief conclusion analysis of the model\'s performance. Constraints: - Use `shuffle` with a `random_state` of 42 before generating the learning and validation curves to ensure reproducibility. Example Code Outline: ```python import numpy as np from sklearn.datasets import load_wine from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve, ValidationCurveDisplay, LearningCurveDisplay from sklearn.utils import shuffle import matplotlib.pyplot as plt # Load and shuffle the data X, y = load_wine(return_X_y=True) X, y = shuffle(X, y, random_state=42) # Validation Curve param_range = np.logspace(-3, 3, 10) train_scores, valid_scores = validation_curve(SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range) # Validation Curve Display ValidationCurveDisplay.from_estimator(SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range) plt.show() # Learning Curve train_sizes = np.linspace(0.1, 1.0, 10) train_sizes, train_scores, valid_scores = learning_curve(SVC(kernel=\\"linear\\", C=1), X, y, train_sizes=train_sizes) # Learning Curve Display LearningCurveDisplay.from_estimator(SVC(kernel=\\"linear\\", C=1), X, y, train_sizes=train_sizes) plt.show() # Analyze results # Provide your analysis here based on the generated plots. ``` Notes: - Ensure correct labeling and titles for all plots for clarity. - Your analysis should consider the behavior of the training and validation scores across the different ranges of `C` and training sizes.","solution":"import numpy as np from sklearn.datasets import load_wine from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve, ValidationCurveDisplay, LearningCurveDisplay from sklearn.utils import shuffle import matplotlib.pyplot as plt def model_performance_analysis(): # Load and shuffle the data X, y = load_wine(return_X_y=True) X, y = shuffle(X, y, random_state=42) # Validation Curve param_range = np.logspace(-3, 3, 10) train_scores, valid_scores = validation_curve(SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5) # Validation Curve Display plt.figure() plt.title(\\"Validation Curve with SVC\\") plt.xlabel(\\"C\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) lw = 2 plt.semilogx(param_range, np.mean(train_scores, axis=1), label=\\"Training score\\", color=\\"darkorange\\", lw=lw) plt.semilogx(param_range, np.mean(valid_scores, axis=1), label=\\"Cross-validation score\\", color=\\"navy\\", lw=lw) plt.legend(loc=\\"best\\") plt.grid() plt.show() # Learning Curve train_sizes = np.linspace(0.1, 1.0, 10) train_sizes, train_scores, valid_scores = learning_curve(SVC(kernel=\\"linear\\", C=1), X, y, train_sizes=train_sizes, cv=5) # Learning Curve Display plt.figure() plt.title(\\"Learning Curve with SVC\\") plt.xlabel(\\"Training Examples\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) lw = 2 plt.plot(train_sizes, np.mean(train_scores, axis=1), \'o-\', color=\\"darkorange\\", label=\\"Training score\\") plt.plot(train_sizes, np.mean(valid_scores, axis=1), \'o-\', color=\\"navy\\", label=\\"Cross-validation score\\") plt.legend(loc=\\"best\\") plt.grid() plt.show() # Analysis: # Based on the validation curve, if training and validation scores both increase and converge to a high score (close to 1) # over the range of C values, the model is performing well. If training scores are high but validation scores drop or stay # significantly lower, the model might be overfitting. If both scores are low, it might be underfitting. # # Based on the learning curve, if both training and validation scores converge to a high score with increasing training examples, # the model is performing well. If there is a sizable gap between the two, the model could be overfitting. If both scores are low, # the model might be underfitting. # Note: Actual analysis would be based on generated plots."},{"question":"You are given a list of colors in RGB format. Your task is to convert each color to the HSV format and then back to the RGB format. Write a function `convert_colors` that takes a list of RGB color tuples and returns a list of converted RGB color tuples after converting them to HSV and back to RGB. Implementing this function will test your comprehension of using the `colorsys` module for bidirectional color space conversions. Function Signature ```python def convert_colors(rgb_colors: List[Tuple[float, float, float]]) -> List[Tuple[float, float, float]]: pass ``` Input - `rgb_colors` (List[Tuple[float, float, float]]): A list of tuples where each tuple contains three float values representing the red, green, and blue components of a color. Each component is between 0 and 1. Output - A list of tuples where each tuple contains three float values representing the red, green, and blue components of the converted color. Each component should be between 0 and 1. Example ```python # Input rgb_colors = [(0.2, 0.4, 0.4), (0.8, 0.0, 0.4)] # Output [(0.2, 0.4, 0.4), (0.8, 0.0, 0.4)] ``` Constraints - The length of `rgb_colors` will be between 1 and 10^3. - Each float value in the RGB tuples will be between 0 and 1. Notes 1. You should use the `colorsys.rgb_to_hsv` and `colorsys.hsv_to_rgb` functions for the conversions. 2. Ensure that the final RGB values are accurate and within the specified range (0 to 1). 3. Document your code properly to explain the steps performed. # Hint Use numpy or other libraries if needed to handle the transformations effectively, but ideally, try to solve this using native Python functions provided by the `colorsys` module. Implement the function below: ```python from typing import List, Tuple import colorsys def convert_colors(rgb_colors: List[Tuple[float, float, float]]) -> List[Tuple[float, float, float]]: result = [] for rgb in rgb_colors: hsv = colorsys.rgb_to_hsv(*rgb) rgb_converted = colorsys.hsv_to_rgb(*hsv) result.append(rgb_converted) return result ```","solution":"from typing import List, Tuple import colorsys def convert_colors(rgb_colors: List[Tuple[float, float, float]]) -> List[Tuple[float, float, float]]: result = [] for rgb in rgb_colors: # Convert RGB to HSV hsv = colorsys.rgb_to_hsv(*rgb) # Convert HSV back to RGB rgb_converted = colorsys.hsv_to_rgb(*hsv) result.append(rgb_converted) return result"},{"question":"Objective: Your task is to demonstrate your understanding of the `seaborn.objects` module by creating a multi-layered plot with various transformations and customizations. Problem Statement: Given a dataset \\"tips\\" which contains information about the tips received in a restaurant, you are required to produce a plot with the following specifications: 1. Plot tips given (`tip`) against the total bill (`total_bill`). 2. The plot should include points for each data instance. 3. Overlay a linear fit showing the relationship between the `total_bill` and `tip`. 4. Use different colors for points based on the day of the week (`day`). 5. Facet this plot by the time of day (`time`). Dataset: Use the seaborn\'s built-in dataset `tips`. Implementation: 1. Load the dataset using `tips = seaborn.load_dataset(\\"tips\\")`. 2. Create a plot using `seaborn.objects`. 3. Add points to the plot. 4. Overlay a linear fit. 5. Apply different colors to points based on `day`. 6. Facet the plot by `time` so there are separate panels for lunch and dinner. Code Template: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"day\\") .add(so.Dot(), so.Dodge()) .add(so.Line(), so.PolyFit()) .facet(col=\\"time\\") ) # Display the plot plot.show() ``` Expected Output: The plot should show scatter points representing the tip amounts against the total bill, colored based on the day of the week, with a linear regression line overlaid on each facet representing lunch and dinner times. Make sure your plot meets all the specified requirements.","solution":"import seaborn as sns import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"day\\") .add(so.Dot(), so.Dodge()) .add(so.Line(), so.PolyFit()) .facet(col=\\"time\\") ) # Show the plot plot.show()"},{"question":"**Advanced Python Assessment Question: Unix Group Database Analysis** You are tasked with writing a Python function that utilizes the `grp` module to gather and organize information about Unix groups. Your function should be able to identify groups with more than a specified number of members and list such group names along with their member counts. # Function Signature ```python def get_groups_with_min_members(min_members: int) -> List[Tuple[str, int]]: pass ``` # Input - `min_members` (int): The minimum number of members required for a group to be included in the output list. # Output - A list of tuples, where each tuple contains: - A group name (string) - The count of members in that group (int) # Requirements 1. Use the `grp.getgrall()` method to retrieve all group entries. 2. Identify groups with a member count greater than or equal to `min_members`. 3. Return a list of tuples sorted alphabetically by group name, where each tuple consists of the group name and the number of members in that group. # Constraints - Assume `min_members` is always a non-negative integer. - Handle potential `TypeError` exceptions that may arise due to incorrect data types passed to the `grp` methods. # Example ```python # Example group entries retrieved might look like: # [ # grp.struct_group(gr_name=\'group1\', gr_passwd=\'x\', gr_gid=1001, gr_mem=[\'user1\', \'user2\']), # grp.struct_group(gr_name=\'group2\', gr_passwd=\'x\', gr_gid=1002, gr_mem=[\'user3\']), # grp.struct_group(gr_name=\'group3\', gr_passwd=\'x\', gr_gid=1003, gr_mem=[\'user4\', \'user5\', \'user6\']) # ] # For min_members = 2, the function should return: # [(\'group1\', 2), (\'group3\', 3)] assert get_groups_with_min_members(2) == [(\'group1\', 2), (\'group3\', 3)] ``` # Note - Do not use any additional imports other than the `grp` module. - Ensure the code adheres to Python best practices.","solution":"import grp from typing import List, Tuple def get_groups_with_min_members(min_members: int) -> List[Tuple[str, int]]: Returns a list of group names along with their member counts for groups that have at least `min_members` members. :param min_members: The minimum number of members required :return: A list of tuples (group_name, member_count) sorted alphabetically by group name. try: all_groups = grp.getgrall() except TypeError as e: raise TypeError(f\\"An error occurred when trying to retrieve group entries: {e}\\") groups_with_required_members = [ (group.gr_name, len(group.gr_mem)) for group in all_groups if len(group.gr_mem) >= min_members ] return sorted(groups_with_required_members, key=lambda x: x[0])"},{"question":"**Objective**: Assess students\' ability to work with the `zlib` module for compression and decompression tasks. **Problem Statement**: Write a Python function `compress_and_check_integrity` that takes a list of strings, compresses them using the `zlib` module, and then verifies the integrity of the compressed data by decompressing it and ensuring the decompressed data matches the original input. The function should handle compression level customization, use checksums to verify data integrity, and properly handle any compression or decompression errors. **Function Signature**: ```python def compress_and_check_integrity(data_list: List[str], compression_level: int = -1) -> bool: pass ``` **Input**: - `data_list`: A list of strings to be compressed. Example: `[\\"hello world\\", \\"foo bar\\", \\"compression test\\"]` - `compression_level`: An integer from 0 to 9 or -1, where -1 represents the default compression level. **Output**: - Returns a boolean value: - `True` if the decompressed data matches the original data. - `False` if there is a mismatch or if an error occurred. **Constraints**: - The function should use Adler-32 checksums to verify data integrity. - The function should catch and appropriately handle `zlib.error` exceptions. - You should manage large data sets efficiently, considering memory usage. # Example ```python # Example usage: data_list = [\\"hello world\\", \\"foo bar\\", \\"compression test\\"] compression_level = 5 assert compress_and_check_integrity(data_list, compression_level) == True # This should return True if data integrity is maintained. ``` # Notes: 1. Use `zlib.compress` and `zlib.decompress` for compression and decompression. 2. Utilize `zlib.adler32` to compute checksums before and after compression. 3. Handle all exceptional cases where compression or decompression might fail. **Performance Requirements**: - The function should efficiently handle lists with up to 10,000 strings. - Each string can be up to 1,024 characters long. **Hints**: 1. You can concatenate all strings in the list into a single string before compression to simplify the process, but ensure to decompress it back correctly. 2. Ensure to compute checksums both before and after compressing and decompressing to validate data integrity.","solution":"import zlib from typing import List def compress_and_check_integrity(data_list: List[str], compression_level: int = -1) -> bool: try: # Step 1: Concatenate all strings into a single string original_data = \'\'.join(data_list) # Step 2: Calculate the checksum of the original data original_checksum = zlib.adler32(original_data.encode(\'utf-8\')) # Step 3: Compress the concatenated string compressed_data = zlib.compress(original_data.encode(\'utf-8\'), level=compression_level) # Step 4: Decompress the data decompressed_data = zlib.decompress(compressed_data) # Step 5: Calculate the checksum of the decompressed data decompressed_checksum = zlib.adler32(decompressed_data) # Step 6: Verify if the original data matches the decompressed data return original_checksum == decompressed_checksum and original_data.encode(\'utf-8\') == decompressed_data except zlib.error: # Handle any compression or decompression errors return False"},{"question":"Coding Assessment Question # Objective The objective of this task is to assess your understanding of validation curves and learning curves using the scikit-learn library. You will need to use these tools to analyze a machine learning model\'s performance with varying hyperparameters and training dataset sizes. # Problem Statement You are given a dataset and you need to train a Support Vector Machine (SVM) classifier. The task involves three steps: 1. Plot the validation curve for the `C` parameter of the SVM. 2. Plot the learning curve to understand the model\'s performance with varying sizes of the training data. 3. Analyze and interpret the resulting plots to determine if your model is overfitting, underfitting, or well-fitted. # Instructions 1. **Load the Dataset**: - Load the Iris dataset using the `load_iris` function from `sklearn.datasets`. 2. **Plot Validation Curve**: - Use the `validation_curve` function from `sklearn.model_selection`. - Hyperparameter: `C` with a range of `np.logspace(-7, 3, 10)`. - Model: SVM with a linear kernel (`SVC(kernel=\\"linear\\")`). - Split the dataset into training and validation sets. - Plot training and validation scores against the values of `C`. 3. **Plot Learning Curve**: - Use the `learning_curve` function from `sklearn.model_selection`. - Model: SVM with a linear kernel (`SVC(kernel=\\"linear\\")`). - Training sizes: `[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]`. - Cross-validation: 5-fold CV. - Plot training scores and validation scores against the training sizes. 4. **Interpret the Results**: - Determine if the model is overfitting, underfitting, or well-fitted based on the plots. - Explain your reasoning. # Function Signature Your function should have the following signature: ```python import numpy as np from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve import matplotlib.pyplot as plt def analyze_svm_performance(): # Your code here pass ``` # Constraints - You must use the `validation_curve` and `learning_curve` functions from `sklearn.model_selection`. - You should use the SVM model with a linear kernel (`SVC(kernel=\\"linear\\")`). # Expected Output - Two plots: one for the validation curve and one for the learning curve. - A textual interpretation of the plots to explain if the model is overfitting, underfitting, or well-fitted. # Example Plot The expected validation curve plot may look like this (values are illustrative): ``` C (log scale) | |--- --- --- Training Score | |--- --- --- --- | |--- --- --- | - --- | --- Valid Score +----------------------------------------- 10^-7 ... 1 ... 10^3 ``` The expected learning curve plot may have training and validation scores converging as follows: ``` Training Size | || O Training Score|| O O || O O ++-------------------------- 10 ... 50 ... 100 Training Size ``` Interpret these plots in a markdown cell or a docstring within the function analyzing `overfitting`, `underfitting`, and `well-fitted` model scenarios.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve import matplotlib.pyplot as plt def analyze_svm_performance(): # Load iris dataset data = load_iris() X, y = data.data, data.target # SVM classifier with linear kernel model = SVC(kernel=\\"linear\\") # Validation curve for C parameter param_range = np.logspace(-7, 3, 10) train_scores, valid_scores = validation_curve( model, X, y, param_name=\\"C\\", param_range=param_range, cv=5, scoring=\\"accuracy\\" ) # Plot validation curve plt.figure(figsize=(8, 6)) plt.plot(param_range, np.mean(train_scores, axis=1), label=\\"Training score\\", color=\\"darkorange\\") plt.plot(param_range, np.mean(valid_scores, axis=1), label=\\"Validation score\\", color=\\"navy\\") plt.xscale(\'log\') plt.xlabel(\\"Parameter C\\") plt.ylabel(\\"Accuracy\\") plt.title(\\"Validation Curve for SVM\\") plt.legend(loc=\\"best\\") plt.grid() plt.show() # Learning curve for SVM classifier train_sizes, train_scores_lc, valid_scores_lc = learning_curve( model, X, y, train_sizes=np.linspace(0.1, 1.0, 10), cv=5, scoring=\\"accuracy\\" ) # Plot learning curve plt.figure(figsize=(8, 6)) plt.plot(train_sizes, np.mean(train_scores_lc, axis=1), label=\\"Training score\\", color=\\"darkorange\\") plt.plot(train_sizes, np.mean(valid_scores_lc, axis=1), label=\\"Validation score\\", color=\\"navy\\") plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Accuracy\\") plt.title(\\"Learning Curve for SVM\\") plt.legend(loc=\\"best\\") plt.grid() plt.show() # Interpretation of results print(\\"Validation Curve Analysis:\\") print(\\"If the training score is high but the validation score is low for large C values, it indicates overfitting.\\") print(\\"If both scores are low for small C values, the model might be underfitting.\\") print(\\"If both scores are high and close to each other, the model is well-fitted.\\") print(\\"nLearning Curve Analysis:\\") print(\\"If the training score is high but the validation score is low for all training sizes, it indicates overfitting.\\") print(\\"If both scores are low for all training sizes, the model might be underfitting.\\") print(\\"If both scores converge to a high value with increasing training size, the model is likely well-fitted.\\") # Run the analysis function analyze_svm_performance()"},{"question":"# Problem Description You are tasked with creating a Python function that fetches the content of a given webpage, extracts all hyperlinks from the page, and returns them as a list. # Function Signature ```python def fetch_and_extract_links(url: str) -> list: ``` # Input - `url` (str): A string representing the URL of the webpage to fetch. # Output - (list): A list of strings where each string is a hyperlink extracted from the given webpage. # Constraints and Requirements 1. Utilize the `urllib.request` module to fetch the content of the webpage. 2. Use the `urllib.parse` module to parse the URLs. 3. Ensure your function handles exceptions appropriately, possibly by returning an empty list if the webpage cannot be fetched. 4. The function should ignore non-HTTP/HTTPS links such as `mailto:`. # Example ```python assert fetch_and_extract_links(\\"https://example.com\\") == [\\"https://example.com/link1\\", \\"https://example.com/link2\\"] ``` # Notes - Assume the URL passed to the function is well-formed. - Use appropriate libraries or methods to parse HTML content and extract the hyperlinks. # Hints - You may find `BeautifulSoup` from the `bs4` package helpful for parsing HTML. However, it is not mandatory, and students can use regular expressions or other parsing methods. Good luck!","solution":"import urllib.request import urllib.parse from bs4 import BeautifulSoup def fetch_and_extract_links(url: str) -> list: Fetches content from the given URL and extracts all hyperlinks. Parameters: url (str): The URL of the webpage to fetch. Returns: list: A list of hyperlinks found on the webpage. try: response = urllib.request.urlopen(url) web_content = response.read() soup = BeautifulSoup(web_content, \'html.parser\') links = [] for a_tag in soup.find_all(\'a\', href=True): href = a_tag[\'href\'] parsed_href = urllib.parse.urljoin(url, href) if parsed_href.startswith(\'http://\') or parsed_href.startswith(\'https://\'): links.append(parsed_href) return links except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"# Advanced Coding Assessment Question --- **Title: Implement a Custom Symbolic Shape Operation in Dynamo** **Objective:** In this exercise, you will extend the capabilities of Dynamo by implementing a custom symbolic shape operation. Your task is to add functionality to Dynamo to trace and handle a custom operation on the shapes of tensors, specifically an operation to compute the product of tensor dimensions dynamically. **Background:** TorchDynamo, a component of `torch.compile`, traces Python functions and generates FX graphs representing the operations executed on tensors. It handles dynamic shapes by tracing them symbolically when their values change between function calls. This dynamic tracing allows for the creation of generalized models. **Task:** 1. Implement a function `dynamic_shape_product` that takes a tensor and returns the product of its dimensions (shape). This function should work dynamically, meaning it should adapt if the shape of the tensor changes between function calls. 2. Modify Dynamo to trace the `dynamic_shape_product` function symbolically, ensuring it can handle changes in tensor shapes without recompiling the entire function. You will need to: - Define the `dynamic_shape_product` operation. - Extend Dynamo\'s `VariableTracker` and related classes to handle this new operation. - Ensure proper guards are created and checked for reusing the graph when the shape changes. **Implementation Steps:** 1. Define the `dynamic_shape_product` function. 2. Extend the appropriate classes in Dynamo to support the new operation. 3. Ensure the operation is traced symbolically and guards are added correctly. **Code Scaffold:** ```python import torch from torch._dynamo.variables import VariableTracker, TensorVariable, VariableBuilder from torch._dynamo.guards import check_shape # Step 1: Implement the dynamic_shape_product function @torch.compile def dynamic_shape_product(tensor): product = 1 for dim in tensor.shape: product *= dim return product x = torch.randn(200, 300) print(dynamic_shape_product(x)) # Step 2: Extend Dynamo to handle the new operation # (You will be implementing this section) class ShapeProductVariable(VariableTracker): def __init__(self, tensor_var): super().__init__() self.tensor_var = tensor_var def reconstruct(self, codegen): # Implement reconstruction logic to handle symbolic shape product pass class VariableBuilderExtended(VariableBuilder): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) def _wrap(self, value): if isinstance(value, torch.Tensor): return TensorVariable(value) # Add support for ShapeProductVariable return super()._wrap(value) # Step 3: Ensure proper guards are created and checked def check_shape_product_guard(tensor): # Implement guard checking logic for the shape product pass # Usage example to test your implementation x = torch.randn(100, 200) result1 = dynamic_shape_product(x) x = torch.randn(150, 250) result2 = dynamic_shape_product(x) # Should trace the shape dynamically print(result1) print(result2) ``` **Constraints:** - You are allowed to modify only sections marked with comments. - Ensure your implementation handles dynamic changes in tensor shapes efficiently. **Performance Requirements:** - The solution should handle changes in tensor shapes without unnecessary recompilations. - The implementation should be efficient and maintain the expected speed-up from using `torch.compile`. **Submission:** Submit your modified code file containing the `dynamic_shape_product` implementation and the necessary changes to Dynamo. ---","solution":"import torch def dynamic_shape_product(tensor): Returns the product of the tensor dimensions (shape). product = 1 for dim in tensor.shape: product *= dim return product x = torch.randn(200, 300) print(dynamic_shape_product(x))"},{"question":"# Question: Analyzing Sales Data with Pandas You are provided with a CSV file containing sales data from a retail store. Your task is to analyze this data using pandas and implement specific functions to answer various business questions. The sales data CSV file has the following columns: - `Date`: Date of the sales transaction (string in `YYYY-MM-DD` format) - `Store`: Store identifier (string) - `Product`: Product identifier (string) - `Quantity`: Number of units sold (integer) - `Revenue`: Total revenue from the sale (float) Your task is to implement the following functions to perform analytical operations on the data: 1. `load_data(file_path: str) -> pd.DataFrame`: This function takes a file path for the CSV file and returns a pandas DataFrame containing the sales data. 2. `total_sales_by_store(df: pd.DataFrame) -> pd.DataFrame`: This function takes the sales data DataFrame and returns a new DataFrame with the total quantity of items sold and total revenue for each store. 3. `highest_grossing_product(df: pd.DataFrame, n: int) -> pd.DataFrame`: This function takes the sales data DataFrame and an integer `n`, and returns a DataFrame with the top `n` products that generated the highest total revenue. 4. `monthly_sales_trend(df: pd.DataFrame, store_id: str) -> pd.DataFrame`: This function takes the sales data DataFrame and a store identifier (string), and returns a DataFrame showing the total revenue for each month for the specified store. 5. `postprocess_data(df: pd.DataFrame) -> pd.DataFrame`: This function applies the following transformations to the sales data DataFrame and returns the processed DataFrame: - Replace any missing values in `Quantity` with 0. - Replace any missing values in `Revenue` with 0.0. - Convert the `Date` column to a pandas datetime object. **Constraints:** - Assume the input CSV file is correctly formatted. - You may use any standard pandas methods for data manipulation. - Ensure your code handles large datasets efficiently. # Example Usage: ```python import pandas as pd # Load the data df = load_data(\\"sales_data.csv\\") # Get total sales by store total_sales_df = total_sales_by_store(df) print(total_sales_df) # Get the highest grossing products top_products_df = highest_grossing_product(df, 5) print(top_products_df) # Get monthly sales trend for a specific store monthly_trend_df = monthly_sales_trend(df, \\"Store_A\\") print(monthly_trend_df) # Post-process the sales data processed_df = postprocess_data(df) print(processed_df) ``` # Expected Outputs: 1. `total_sales_by_store(df)` ``` Store Total_Quantity Total_Revenue 0 Store_A 12345 23456.78 1 Store_B 67890 34567.89 ... ``` 2. `highest_grossing_product(df, 5)` ``` Product Total_Revenue 0 Product_1 54321.45 1 Product_2 43210.32 ... 4 Product_5 12345.67 ``` 3. `monthly_sales_trend(df, \\"Store_A\\")` ``` Month Total_Revenue 0 2023-01 1234.00 1 2023-02 2345.00 ... 11 2023-12 6789.00 ``` 4. `postprocess_data(df)` ``` Date Store Product Quantity Revenue 0 2023-01-01 Store_A Product_1 10 123.45 1 2023-01-01 Store_B Product_2 0 0.00 ... ``` Implement all the required functions and ensure to test them with example data.","solution":"import pandas as pd def load_data(file_path: str) -> pd.DataFrame: Loads the sales data from the given CSV file into a pandas DataFrame. Args: - file_path (str): The file path of the CSV file containing the sales data. Returns: - pd.DataFrame: A DataFrame containing the sales data. return pd.read_csv(file_path) def total_sales_by_store(df: pd.DataFrame) -> pd.DataFrame: Calculates the total quantity of items sold and total revenue for each store. Args: - df (pd.DataFrame): The sales data DataFrame. Returns: - pd.DataFrame: A DataFrame with the total quantity of items sold and total revenue for each store. result = df.groupby(\'Store\').agg(Total_Quantity=(\'Quantity\', \'sum\'), Total_Revenue=(\'Revenue\', \'sum\')).reset_index() return result def highest_grossing_product(df: pd.DataFrame, n: int) -> pd.DataFrame: Finds the top `n` products that generated the highest total revenue. Args: - df (pd.DataFrame): The sales data DataFrame. - n (int): The number of top products to return. Returns: - pd.DataFrame: A DataFrame with the top `n` products that generated the highest total revenue. result = df.groupby(\'Product\').agg(Total_Revenue=(\'Revenue\', \'sum\')).reset_index() result = result.sort_values(by=\'Total_Revenue\', ascending=False).head(n) return result def monthly_sales_trend(df: pd.DataFrame, store_id: str) -> pd.DataFrame: Shows the total revenue for each month for the specified store. Args: - df (pd.DataFrame): The sales data DataFrame. - store_id (str): The store identifier. Returns: - pd.DataFrame: A DataFrame showing the total revenue for each month for the specified store. df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') store_df = df[df[\'Store\'] == store_id] result = store_df.groupby(\'Month\').agg(Total_Revenue=(\'Revenue\', \'sum\')).reset_index() result[\'Month\'] = result[\'Month\'].astype(str) return result def postprocess_data(df: pd.DataFrame) -> pd.DataFrame: Applies the following transformations to the sales data DataFrame: - Replace missing values in `Quantity` with 0. - Replace missing values in `Revenue` with 0.0. - Converts the `Date` column to a pandas datetime object. Args: - df (pd.DataFrame): The sales data DataFrame. Returns: - pd.DataFrame: The processed DataFrame. df[\'Quantity\'] = df[\'Quantity\'].fillna(0) df[\'Revenue\'] = df[\'Revenue\'].fillna(0.0) df[\'Date\'] = pd.to_datetime(df[\'Date\']) return df"},{"question":"# Assessment Question **Objective:** Create boxplots using the seaborn library to visualize different aspects of a dataset. You are expected to utilize various seaborn features and matplotlib parameters to customize the plots. **Dataset:** You will use the built-in Titanic dataset from seaborn. Load the dataset using the following code: ```python import seaborn as sns sns.set_theme(style=\\"whitegrid\\") titanic = sns.load_dataset(\\"titanic\\") ``` **Task:** 1. **Horizontal Boxplot of Fare:** - Create a single horizontal boxplot of the `fare` column from the Titanic dataset. 2. **Boxplot Grouped by Class:** - Create a vertical boxplot to visualize the distribution of `age` within each `class`. 3. **Nested Grouping Boxplot:** - Create a vertical boxplot with nested grouping by `class` and coloring by `sex`. 4. **Customized Boxplot:** - Create a boxplot of `fare` grouped by `class`, where: - The boxes are wider. - The lines are shown as line art. - The median line should be red with a linewidth of 2. - Use a fill color with an opacity of 50% for the boxes. - Mark the outliers with a diamond shape (`\\"D\\"`). - Hint: To achieve these customizations, you may need to use seaborn and underlying matplotlib parameters. **Requirements:** - Your code should be clean and include comments explaining each step. - You should ensure any customization parameter from matplotlib is correctly applied. **Sample Input:** _N/A (The data is loaded directly using seaborn\'s `load_dataset` function)_ **Sample Output:** There is no textual output. The function will display the required boxplots. Each plot should be accurately labeled and display according to the given specifications. **Grading Criteria:** - Correct loading and usage of the dataset. - Accurate creation of the boxplots with the specified groupings. - Appropriate application of custom seaborn and matplotlib parameters. - Clear and well-commented code. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt # Set the theme for the plots sns.set_theme(style=\\"whitegrid\\") # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") def create_horizontal_boxplot_fare(): Creates a horizontal boxplot for the \'fare\' column from the Titanic dataset. plt.figure(figsize=(10, 6)) sns.boxplot(x=\'fare\', data=titanic) plt.title(\'Horizontal Boxplot of Fare\') plt.show() def create_boxplot_age_by_class(): Creates a vertical boxplot to visualize the distribution of \'age\' within each \'class\'. plt.figure(figsize=(10, 6)) sns.boxplot(x=\'class\', y=\'age\', data=titanic) plt.title(\'Boxplot of Age by Class\') plt.show() def create_nested_grouping_boxplot(): Creates a vertical boxplot with nested grouping by \'class\' and coloring by \'sex\'. plt.figure(figsize=(10, 6)) sns.boxplot(x=\'class\', y=\'age\', hue=\'sex\', data=titanic) plt.title(\'Nested Grouping Boxplot by Class and Sex\') plt.show() def create_customized_boxplot(): Creates a customized boxplot for \'fare\' grouped by \'class\' with specific aesthetic customizations. plt.figure(figsize=(10, 6)) # Create the boxplot with custom parameters sns.boxplot(x=\'class\', y=\'fare\', data=titanic, linewidth=1.5, width=0.5, fliersize=5, boxprops=dict(facecolor=(0.1, 0.2, 0.5, 0.5)), medianprops=dict(color=\'red\', linewidth=2), flierprops=dict(marker=\'D\', color=\'green\', markersize=5)) plt.title(\'Customized Boxplot of Fare grouped by Class\') plt.show()"},{"question":"Advanced Date Offsets with Pandas You are given a list of dates and a custom schedule that certain dates should follow based on business rules. Implement a function that adjusts the given dates according to the closest subsequent custom business date from a reference schedule. # Function Signature ```python import pandas as pd def adjust_to_business_dates(dates: list, business_days: pd.tseries.offsets.CustomBusinessDay) -> list: Adjusts a list of dates to the closest subsequent business dates according to given custom business day rules. Parameters: dates (list): A list of date strings in \'YYYY-MM-DD\' format that need to be adjusted. business_days (pd.tseries.offsets.CustomBusinessDay): A pandas CustomBusinessDay object defining the custom business days. Returns: list: A list of adjusted dates in \'YYYY-MM-DD\' format according to the custom business days. pass ``` # Input - `dates`: A list of date strings in the format \'YYYY-MM-DD\'. Example: [\\"2023-01-01\\", \\"2023-01-20\\", \\"2023-02-10\\"] - `business_days`: A pandas `CustomBusinessDay` object that defines the custom schedule for business days, including holidays and weekmask. # Output - Returns a list of date strings in \'YYYY-MM-DD\' format after adjustment. # Example Usage ```python from pandas.tseries.offsets import CustomBusinessDay # Custom business days: Monday to Friday, with custom holidays holidays = [\\"2023-01-02\\", \\"2023-02-20\\"] business_days = CustomBusinessDay(holidays=holidays) dates = [\\"2023-01-01\\", \\"2023-01-20\\", \\"2023-02-10\\", \\"2023-02-19\\"] adjusted_dates = adjust_to_business_dates(dates, business_days) print(adjusted_dates) # Expected output: [\\"2023-01-03\\", \\"2023-01-23\\", \\"2023-02-13\\", \\"2023-02-21\\"] ``` # Constraints 1. The input dates will be valid date strings in \'YYYY-MM-DD\' format. 2. The `business_days` object will be a valid `CustomBusinessDay` object. 3. The function must handle edge cases such as: - Dates that fall exactly on a holiday. - Dates that fall on weekends. - Sequential processing should ensure that output dates follow the custom business schedule accurately. # Notes - You may use the properties and methods related to `CustomBusinessDay` and other pandas date offset functionalities to help you implement the function. - Consider performance if the input list is very large.","solution":"import pandas as pd def adjust_to_business_dates(dates: list, business_days: pd.tseries.offsets.CustomBusinessDay) -> list: Adjusts a list of dates to the closest subsequent business dates according to given custom business day rules. Parameters: dates (list): A list of date strings in \'YYYY-MM-DD\' format that need to be adjusted. business_days (pd.tseries.offsets.CustomBusinessDay): A pandas CustomBusinessDay object defining the custom business days. Returns: list: A list of adjusted dates in \'YYYY-MM-DD\' format according to the custom business days. adjusted_dates = [] for date in dates: original_date = pd.Timestamp(date) if original_date.dayofweek < 5 and original_date not in business_days.holidays: # If the date is already a business day, use it as is adjusted_dates.append(date) else: # Find the next business day next_business_day = original_date + business_days adjusted_dates.append(next_business_day.strftime(\'%Y-%m-%d\')) return adjusted_dates"},{"question":"**Question:** You are given a dataset with features and multiple target variables, and you are required to implement a multioutput classification model using scikit-learn. The dataset consists of samples that have multiple target properties to be predicted, where each target can take more than two possible class labels. # Requirements: 1. Load and preprocess the dataset. 2. Implement a multioutput classification model using `MultiOutputClassifier`. 3. Fit the model to the training data and make predictions on the test data. 4. Evaluate the performance of your model using appropriate evaluation metrics. # Input: - A CSV file containing the dataset. The file path will be provided as input. The dataset has the following format: - The first `n` columns are the features. - The next `m` columns are the target variables. # Output: - The accuracy score of the multioutput classification model on the test dataset. # Constraints: - Use `RandomForestClassifier` as the base estimator for the `MultiOutputClassifier`. # Example: Assume the dataset is stored in a file named `dataset.csv`. You can use the following steps as reference: 1. Load the dataset using Pandas: ```python import pandas as pd dataset = pd.read_csv(\'dataset.csv\') ``` 2. Split the dataset into features (X) and targets (Y): ```python X = dataset.iloc[:, :n].values Y = dataset.iloc[:, n:].values ``` 3. Split the dataset into training and testing sets: ```python from sklearn.model_selection import train_test_split X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) ``` 4. Implement and fit the multioutput classification model: ```python from sklearn.multioutput import MultiOutputClassifier from sklearn.ensemble import RandomForestClassifier base_estimator = RandomForestClassifier(random_state=42) multi_target_classifier = MultiOutputClassifier(base_estimator, n_jobs=-1) multi_target_classifier.fit(X_train, Y_train) ``` 5. Make predictions and evaluate the model: ```python from sklearn.metrics import accuracy_score Y_pred = multi_target_classifier.predict(X_test) score = accuracy_score(Y_test, Y_pred) print(f\'Accuracy Score: {score:.4f}\') ``` # Submission: Submit a Python script or Jupyter notebook that accomplishes the above tasks, ensures correct input and output formats, and includes comments explaining the logic of your code.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.multioutput import MultiOutputClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def multioutput_classification(file_path): Loads the dataset, splits into features and targets, trains a multioutput classifier, and evaluates its performance. Args: file_path (str): The path to the CSV file containing the dataset. Returns: float: The accuracy score of the multioutput classification model on the test dataset. # Load the dataset dataset = pd.read_csv(file_path) # Determine the number of features and targets n = len(dataset.columns) - 3 # Assuming the first n columns are features and the next m (here 3) are targets m = 3 # Split the dataset into features (X) and targets (Y) X = dataset.iloc[:, :n].values Y = dataset.iloc[:, n:].values # Split the dataset into training and testing sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Implement and fit the multioutput classification model base_estimator = RandomForestClassifier(random_state=42) multi_target_classifier = MultiOutputClassifier(base_estimator, n_jobs=-1) multi_target_classifier.fit(X_train, Y_train) # Make predictions and evaluate the model Y_pred = multi_target_classifier.predict(X_test) score = accuracy_score(Y_test.flatten(), Y_pred.flatten()) return score"},{"question":"Objective: You are required to demonstrate your understanding of seaborn, particularly its `so.Plot` class and the `Dot` mark. You will create a visual representation of the \\"tips\\" dataset, implementing various customizations and statistical elements. Problem Statement: You are given the `tips` dataset. Your task is to create a faceted scatter plot with the following requirements: 1. Use `total_bill` as the x-axis and `tip` as the y-axis. 2. Color the dots based on the `sex` column. 3. Create facets for each `day` of the week and wrap them in a 2x2 grid. 4. Add edge colors to the dots to make them more distinguishable. 5. Adjust the point size to 6 for better visualization. 6. Include error bars showing the standard error of the mean for the tips on each day. Your final visualization should be clear, with all points and error bars effectively communicated. Input: - You don\'t need any input from the user. The dataset (`tips`) is loaded for you. Output: - Your code should generate a seaborn plot meeting the described requirements. Constraints: - You must use seaborn\'s `so.Plot` class and the `Dot` mark to create the plot. - Faceting, edge color customization, and error bars must be implemented as described. Performance Requirements: - The plot should efficiently handle the size of the `tips` dataset. ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") .facet(\\"day\\", wrap=2) .add(so.Dot(pointsize=6, edgecolor=\\"w\\")) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ).show() ``` **NOTE:** The above code snippet showcases a template. You must write the function that puts this all together and generates the plot.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_faceted_scatter_plot(): # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") .facet(\\"day\\", wrap=2) .add(so.Dot(pointsize=6, edgecolor=\\"w\\")) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) # Display the plot plot.show() # Running the function to generate the plot create_faceted_scatter_plot()"},{"question":"**Problem Statement** You are provided with a dataset containing sales information of a retail store in a hierarchical format. Each sale is recorded with the store location, the type of product sold, and the date of sale. Additionally, the sales amount is recorded for each entry. The dataset is provided as a pandas DataFrame with a MultiIndex (hierarchical index) consisting of \'Location\', \'Product\', and \'Date\'. Your task is to write a Python function using pandas that performs the following operations: 1. **Create a MultiIndex DataFrame:** Given three lists, `locations`, `products`, and `dates`, create a MultiIndex DataFrame `df` with random sales amounts for each combination of (Location, Product, Date). The DataFrame should have the MultiIndex on the rows and a single column \'Sales\'. 2. **Aggregate Sales Data:** Create a function `aggregate_sales` that takes the DataFrame `df` as input and returns a DataFrame aggregated at the \'Location\' level, showing the total sales for each location. 3. **Filter by Date:** Create another function `filter_by_date` that takes the DataFrame `df` and a date range (start_date, end_date) as input and returns a DataFrame filtered to only include sales within the specified date range. 4. **Get Sales for a Specific Product:** Implement a function `get_product_sales` that takes the DataFrame `df` and a product name as input and returns a DataFrame showing the sales for that specific product, indexed by \'Location\' and \'Date\'. 5. **Sort and Extract Data:** Implement a function `sort_and_extract` that sorts the DataFrame `df` by \'Date\' within each (Location, Product) group and extracts the top N sales entries, regardless of location or product. The function should take `df` and an integer `N` as input and return the top N entries (by sales amount). **Input:** 1. A DataFrame `df` with a MultiIndex (Location, Product, Date) and a \'Sales\' column. 2. For `filter_by_date`: A start date `start_date` and an end date `end_date` in \'YYYY-MM-DD\' format as strings. 3. For `get_product_sales`: A product name `product_name` as a string. 4. For `sort_and_extract`: An integer `N`. **Output:** 1. For `aggregate_sales`: A DataFrame with total sales aggregated by \'Location\'. 2. For `filter_by_date`: A DataFrame filtered by the specified date range. 3. For `get_product_sales`: A DataFrame showing sales for the specified product. 4. For `sort_and_extract`: A DataFrame with the top N sales entries. ```python import pandas as pd import numpy as np def create_sales_dataframe(locations, products, dates): Creates a MultiIndex DataFrame with random sales amounts. index = pd.MultiIndex.from_product([locations, products, dates], names=[\\"Location\\", \\"Product\\", \\"Date\\"]) sales = np.random.randint(100, 1000, size=len(index)) df = pd.DataFrame(sales, index=index, columns=[\\"Sales\\"]) return df def aggregate_sales(df): Aggregates total sales by Location. return df.groupby(level=\\"Location\\").sum() def filter_by_date(df, start_date, end_date): Filters the DataFrame by the specified date range. return df.loc[(slice(None), slice(None), slice(start_date, end_date)), :] def get_product_sales(df, product_name): Gets sales data for the specified product. return df.xs(product_name, level=\\"Product\\") def sort_and_extract(df, N): Sorts the DataFrame by \'Date\' within each (Location, Product) group and extracts the top N sales entries. sorted_df = df.sort_values(by=\\"Date\\") return sorted_df.nlargest(N, columns=\\"Sales\\") # Example Usage: locations = [\\"New York\\", \\"Los Angeles\\", \\"Chicago\\"] products = [\\"Electronics\\", \\"Clothing\\", \\"Groceries\\"] dates = pd.date_range(\\"2023-01-01\\", periods=5).strftime(\'%Y-%m-%d\').tolist() df = create_sales_dataframe(locations, products, dates) print(\\"Original DataFrame:\\") print(df) agg_sales = aggregate_sales(df) print(\\"Aggregated Sales by Location:\\") print(agg_sales) filtered_df = filter_by_date(df, \\"2023-01-02\\", \\"2023-01-04\\") print(\\"Filtered by Date (2023-01-02 to 2023-01-04):\\") print(filtered_df) product_sales = get_product_sales(df, \\"Clothing\\") print(\\"Sales for \'Clothing\':\\") print(product_sales) top_sales = sort_and_extract(df, 5) print(\\"Top 5 Sales Entries:\\") print(top_sales) ``` **Constraints:** - Perform these operations efficiently to handle large datasets. - Ensure the hierarchical indexing is maintained throughout these transformations. - `dates` are in \'YYYY-MM-DD\' format. - Assume that the input DataFrame `df` and other parameters are provided correctly.","solution":"import pandas as pd import numpy as np def create_sales_dataframe(locations, products, dates): Creates a MultiIndex DataFrame with random sales amounts. index = pd.MultiIndex.from_product([locations, products, dates], names=[\\"Location\\", \\"Product\\", \\"Date\\"]) sales = np.random.randint(100, 1000, size=len(index)) df = pd.DataFrame(sales, index=index, columns=[\\"Sales\\"]) return df def aggregate_sales(df): Aggregates total sales by Location. return df.groupby(level=\\"Location\\").sum() def filter_by_date(df, start_date, end_date): Filters the DataFrame by the specified date range. return df.loc[(slice(None), slice(None), slice(start_date, end_date)), :] def get_product_sales(df, product_name): Gets sales data for the specified product. return df.xs(product_name, level=\\"Product\\") def sort_and_extract(df, N): Sorts the DataFrame by \'Date\' within each (Location, Product) group and extracts the top N sales entries. sorted_df = df.sort_values(by=\\"Sales\\", ascending=False) return sorted_df.head(N)"},{"question":"- Advanced Regular Expressions in Python Objective Write a Python function that extracts specific pieces of information from structured text using regular expressions. The function should demonstrate an understanding of character classes, metacharacters, group capturing, lookaheads, and substitutions. Problem Statement You are given a log file containing multiple entries. Each entry provides information about a user login event in the following format: ``` [DATE: YYYY-MM-DD] [TIME: HH:MM:SS] - User: USER_NAME logged in from IP: XXX.XXX.XXX.XXX ``` Write a function `extract_log_details(log_text)` that, given a string `log_text` containing multiple log entries, returns a list of dictionaries. Each dictionary should contain the parsed details of one log entry: 1. `date`: The date of the login. 2. `time`: The time of the login. 3. `user`: The username. 4. `ip`: The IP address. Function Signature ```python def extract_log_details(log_text: str) -> list[dict]: pass ``` Input - `log_text` (str): A single string containing multiple log entries, each on a new line. Output - `list[dict]`: A list of dictionaries, each containing the extracted details (`date`, `time`, `user`, `ip`) from each log entry. Constraints - You must use regular expressions (`re` module) to parse the log entries. - You must handle any possible whitespace variations around the timestamps and usernames. - Assume the log entries are well-formed as shown in the example below. Example ```python log_text = [DATE: 2023-01-15] [TIME: 08:45:22] - User: alice123 logged in from IP: 192.168.0.1 [DATE: 2023-01-15] [TIME: 12:22:30] - User: bob_the_builder logged in from IP: 172.16.254.2 [DATE: 2023-01-16] [TIME: 18:50:15] - User: charlie logged in from IP: 10.0.0.5 expected_output = [ {\\"date\\": \\"2023-01-15\\", \\"time\\": \\"08:45:22\\", \\"user\\": \\"alice123\\", \\"ip\\": \\"192.168.0.1\\"}, {\\"date\\": \\"2023-01-15\\", \\"time\\": \\"12:22:30\\", \\"user\\": \\"bob_the_builder\\", \\"ip\\": \\"172.16.254.2\\"}, {\\"date\\": \\"2023-01-16\\", \\"time\\": \\"18:50:15\\", \\"user\\": \\"charlie\\", \\"ip\\": \\"10.0.0.5\\"}, ] ``` Your function should return the `expected_output` for the given example `log_text`.","solution":"import re def extract_log_details(log_text: str) -> list[dict]: Extracts log details from the given log text. pattern = re.compile(r\\"[DATE:s*(d{4}-d{2}-d{2})]s*[TIME:s*(d{2}:d{2}:d{2})]s*-s*User:s*(w+)s*loggeds*ins*froms*IP:s*([d.]+)\\") matches = pattern.findall(log_text) log_details = [] for match in matches: date, time, user, ip = match log_details.append({ \\"date\\": date, \\"time\\": time, \\"user\\": user, \\"ip\\": ip }) return log_details"},{"question":"# Advanced Python Object Manipulation **Objective**: Implement a function that processes various types of Python objects and demonstrates type safety along with manipulating complex structures. **Task**: Write a function `process_python_objects(input_data: Any) -> Dict[str, Any]`. This function takes an input which can be one of the following types: `int`, `float`, `complex`, `dict`, `list`, `tuple`, `set`, `str`, `bytes`. The function should return a dictionary summarizing information processed from the input data. **Details and Requirements**: 1. **Input Data**: - The `input_data` parameter can be of any type mentioned above. - You should first check the type of the input data. 2. **Output Data**: - The output should be a dictionary with the following keys: - `\'type\'`: the type of the input data - `\'details\'`: a summary of the data based on its type. 3. Summary Requirements based on Type: - For `int`, `float`, `complex`: Return their mathematical properties like whether it\'s positive, negative, or zero (and real/imaginary parts for complex). - For `dict`: Return the count of keys and the types of the keys and values. - For `list`, `tuple`, `set`: Return the length and the types of elements. - For `str`, `bytes`: Return the length and whether it contains alphabetical characters, digits, or special characters. **Example**: ```python def process_python_objects(input_data: Any) -> Dict[str, Any]: # Your code here # Example test cases print(process_python_objects(5)) # Output: {\'type\': \'int\', \'details\': {\'is_positive\': True, \'is_negative\': False, \'is_zero\': False}} print(process_python_objects(-2.3)) # Output: {\'type\': \'float\', \'details\': {\'is_positive\': False, \'is_negative\': True, \'is_zero\': False}} print(process_python_objects(complex(3, -4))) # Output: {\'type\': \'complex\', \'details\': {\'real\': 3, \'imag\': -4, \'is_positive_real\': True, \'is_negative_real\': False, \'is_zero_real\': False, \'is_positive_imag\': False, \'is_negative_imag\': True, \'is_zero_imag\': False}} print(process_python_objects({\\"key\\": \\"value\\", 1: 2})) # Output: {\'type\': \'dict\', \'details\': {\'key_count\': 2, \'key_types\': [<class \'str\'>, <class \'int\'>], \'value_types\': [<class \'str\'>, <class \'int\'>]}} print(process_python_objects([1, 2, 3])) # Output: {\'type\': \'list\', \'details\': {\'length\': 3, \'element_types\': [<class \'int\'>]}} print(process_python_objects(\\"hello!\\")) # Output: {\'type\': \'str\', \'details\': {\'length\': 6, \'has_alpha\': True, \'has_digits\': False, \'has_special\': True}} print(process_python_objects(b\'hello123\')) # Output: {\'type\': \'bytes\', \'details\': {\'length\': 8, \'has_alpha\': True, \'has_digits\': True, \'has_special\': False}} ``` **Constraints**: - You can assume the input will be valid as per the specified types. - Performance should be efficient with a maximum time complexity of O(n), where n is the size of the input data.","solution":"from typing import Any, Dict def process_python_objects(input_data: Any) -> Dict[str, Any]: result = { \'type\': type(input_data).__name__, \'details\': {} } if isinstance(input_data, (int, float)): result[\'details\'] = { \'is_positive\': input_data > 0, \'is_negative\': input_data < 0, \'is_zero\': input_data == 0 } elif isinstance(input_data, complex): result[\'details\'] = { \'real\': input_data.real, \'imag\': input_data.imag, \'is_positive_real\': input_data.real > 0, \'is_negative_real\': input_data.real < 0, \'is_zero_real\': input_data.real == 0, \'is_positive_imag\': input_data.imag > 0, \'is_negative_imag\': input_data.imag < 0, \'is_zero_imag\': input_data.imag == 0 } elif isinstance(input_data, dict): result[\'details\'] = { \'key_count\': len(input_data), \'key_types\': list(set(type(k) for k in input_data.keys())), \'value_types\': list(set(type(v) for v in input_data.values())) } elif isinstance(input_data, (list, tuple, set)): result[\'details\'] = { \'length\': len(input_data), \'element_types\': list(set(type(el) for el in input_data)) } elif isinstance(input_data, str): result[\'details\'] = { \'length\': len(input_data), \'has_alpha\': any(c.isalpha() for c in input_data), \'has_digits\': any(c.isdigit() for c in input_data), \'has_special\': any(not c.isalnum() for c in input_data) } elif isinstance(input_data, bytes): result[\'details\'] = { \'length\': len(input_data), \'has_alpha\': any(c.isalpha() for c in input_data.decode(\'utf-8\', \'ignore\')), \'has_digits\': any(c.isdigit() for c in input_data.decode(\'utf-8\', \'ignore\')), \'has_special\': any(not c.isalnum() for c in input_data.decode(\'utf-8\', \'ignore\')) } return result"},{"question":"# Problem: Compress and Decompress Data with `zlib` You are required to implement two functions: one for compressing data and another for decompressing data using the `zlib` module in Python. 1. **compress_data(data: bytes, level: int) -> bytes** This function should compress the given data using the specified compression level. - **Parameters:** - `data` (bytes): The input data to be compressed. - `level` (int): The compression level, which is an integer from 0 to 9. `1` is the fastest and least compressed, whereas `9` is the slowest and most compressed. `0` is no compression. - **Returns:** - Compressed data as a bytes object. 2. **decompress_data(data: bytes) -> bytes** This function should decompress the given data. - **Parameters:** - `data` (bytes): The compressed input data to be decompressed. - **Returns:** - Decompressed data as a bytes object. # Example ```python import zlib def compress_data(data: bytes, level: int) -> bytes: pass # Your implementation here def decompress_data(data: bytes) -> bytes: pass # Your implementation here # Example usage: original_data = b\\"Hello, world!\\" * 1000 # Data to compress # Compressing data compressed_data = compress_data(original_data, level=9) print(f\\"Compressed data size: {len(compressed_data)} bytes\\") # Decompressing data decompressed_data = decompress_data(compressed_data) print(f\\"Decompressed data size: {len(decompressed_data)} bytes\\") print(f\\"Data matches: {original_data == decompressed_data}\\") ``` # Constraints - The input data for both functions will be non-empty bytes objects. - The `level` parameter in `compress_data` will always be an integer between 0 and 9 (inclusive). - The functions should handle errors gracefully and raise a `ValueError` if decompression fails due to incorrect or corrupted input data. # Notes - You may find the `zlib.compress` and `zlib.decompress` functions useful for implementation. - Ensure that the output from `decompress_data` matches the original input to `compress_data` for valid data.","solution":"import zlib def compress_data(data: bytes, level: int) -> bytes: Compress the given data using the specified compression level. Parameters: data (bytes): The input data to be compressed. level (int): The compression level, which is an integer from 0 to 9. Returns: bytes: Compressed data. if not isinstance(data, bytes): raise TypeError(\\"Data must be a bytes object\\") if not isinstance(level, int) or level < 0 or level > 9: raise ValueError(\\"Level must be an integer between 0 and 9\\") return zlib.compress(data, level) def decompress_data(data: bytes) -> bytes: Decompress the given data. Parameters: data (bytes): The compressed input data to be decompressed. Returns: bytes: Decompressed data. if not isinstance(data, bytes): raise TypeError(\\"Data must be a bytes object\\") try: return zlib.decompress(data) except zlib.error as e: raise ValueError(\\"Failed to decompress data\\") from e"},{"question":"# Seaborn Advanced Plotting Task You are provided with two datasets, `fmri` (a dataset of functional Magnetic Resonance Imaging results) and `seaice` (a dataset tracking the extent of sea ice over time). Objective 1. **Data Preparation**: - Filter the `fmri` dataset to include only the \\"parietal\\" region. - For the `seaice` dataset, transform the `Date` to include the day of the year as `Day` and the year as `Year`. Filter the dataset to include data from the year 1980 onwards and transform `Year` into a string. Reshape the dataset so that the `index` is `Day`, and the columns are `Year` with values being `Extent`. Finally, retain only the columns for the years 1980 and 2019. 2. **Plotting**: - Create a band plot that fills between the 1980 and 2019 values for sea ice extent over the course of the year. Customize the plot to include a semi-transparent band with edges. - Combine the filtered `fmri` dataset to create a line plot of `signal` over `timepoint` for different `events`, with an error band around each line to show variability. Input - The function does not take any parameters. It should include the necessary steps to: - Load the datasets. - Perform the described filtering and transformations. - Create and display the plots. Output - Two visualizations: 1. A band plot of the `seaice` dataset comparing the extents of 1980 and 2019. 2. A line plot of the `fmri` dataset with error bands, showing `signal` over `timepoint` across different `events`. Use the Seaborn objects interface (`seaborn.objects`) to create these plots. Ensure your code follows good practices in data manipulation and visualization. Example Code Template ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_plots(): # Load and prepare the fmri dataset fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") # Load and prepare the seaice dataset seaice = ( load_dataset(\\"seaice\\") .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) # Create the seaice band plot p = so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") p.add(so.Band(alpha=.5, edgewidth=2)) plt.show() # Create the fmri line plot with error bands ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) .show() ) # Call the function to generate and display the plots create_plots() ``` Ensure the produced plots clearly demonstrate the differences and trends in the datasets while using advanced Seaborn functionalities.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd import matplotlib.pyplot as plt def create_plots(): # Load and prepare the fmri dataset fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") # Load and prepare the seaice dataset seaice = load_dataset(\\"seaice\\") seaice[\'Date\'] = pd.to_datetime(seaice[\'Date\']) seaice = ( seaice.assign( Day=lambda x: x[\\"Date\\"].dt.dayofyear, Year=lambda x: x[\\"Date\\"].dt.year ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") ) seaice_filtered = seaice.filter([\\"1980\\", \\"2019\\"]).dropna().reset_index() # Create the seaice band plot seaice_plot = so.Plot(seaice_filtered, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") seaice_plot.add(so.Band(alpha=.5, edgewidth=2)) seaice_plot.show() # Create the fmri line plot with error bands fmri_plot = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) fmri_plot.show() # Call the function to generate and display the plots create_plots()"},{"question":"# Objective Implement a custom logging system using Python\'s built-in `logging` module to capture log messages of varying severity levels and direct them to different destinations with specific formatting requirements. # Problem Statement You are required to create a custom logging setup that meets the following requirements: 1. **Logger Creation**: - Create a logger named `application_logger`. 2. **Handlers**: - Create two handlers: 1. A `StreamHandler` that logs messages with severity `INFO` and above to the console. 2. A `FileHandler` that logs messages with severity `DEBUG` and above to a file named `app.log`. 3. **Formatters**: - Define two different formatters: 1. A simple one for the console that only outputs the message. 2. A detailed one for the file that includes the timestamp, logger name, log level, and the message. 4. **Filter**: - Create a custom filter that allows only messages containing the word \\"critical\\". 5. **Implementation**: - Attach the handlers and the filter to `application_logger` appropriately. - Log messages of varying severity (DEBUG, INFO, WARNING, ERROR, CRITICAL) to test your logging configuration. # Input - No direct input, but you are expected to hard-code the log messages as per the requirements in your script. # Output - Log messages should appear on the console for severity levels `INFO` and above. - Log messages should be written to `app.log` for severity levels `DEBUG` and above. - Only messages containing the word \\"critical\\" should pass through the filter. # Example Usage ```python import logging # --- Your implementation goes here --- if __name__ == \'__main__\': # Test logging configuration application_logger.debug(\'This is a debug message\') application_logger.info(\'This is an info message\') application_logger.warning(\'This is a warning message\') application_logger.error(\'This is an error message\') application_logger.critical(\'This is a critical message\') ``` # Constraints - You must only use the `logging` module from the Python standard library. - Use appropriate logging levels and ensure that the filter and formatting are accurately applied. # Note Ensure you handle file creation and any related exceptions appropriately. The log file should be created in the current working directory.","solution":"import logging # Create a logger application_logger = logging.getLogger(\'application_logger\') application_logger.setLevel(logging.DEBUG) # Create handlers console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.DEBUG) # Create formatters console_formatter = logging.Formatter(\'%(message)s\') file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') # Add formatters to handlers console_handler.setFormatter(console_formatter) file_handler.setFormatter(file_formatter) # Custom filter to allow only messages with \\"critical\\" class CriticalFilter(logging.Filter): def filter(self, record): return \'critical\' in record.msg.lower() # Apply filter to the logger application_logger.addFilter(CriticalFilter()) # Add handlers to the logger application_logger.addHandler(console_handler) application_logger.addHandler(file_handler) # Example log messages to test the configuration application_logger.debug(\'This is a debug message with critical info\') application_logger.info(\'This is an info message with critical info\') application_logger.warning(\'This is a warning message containing critical info\') application_logger.error(\'This message does not contain the keyword\') application_logger.critical(\'This is a critical severity message\') if __name__ == \\"__main__\\": # some example logs to test application_logger.debug(\'Debug log for testing critical filter\') application_logger.info(\'Info log mentioning critical\') application_logger.warning(\'A critical warning!!\') application_logger.error(\'Non-critical error log\') application_logger.critical(\'Critically important log message\')"},{"question":"Coding Assessment Question # Objective Create a visualization using seaborn that demonstrates your understanding of rug plots and their customization options. The goal is to show multiple dimensions of the dataset and provide a clear readable plot. # Task Given the seaborn `tips` dataset: 1. Create a scatter plot showing the relationship between the `total_bill` and `tip`. 2. Add a rug plot on both axes (`x` and `y`) to the scatter plot. 3. Use the `hue` parameter to differentiate the data points by `time` (lunch/dinner). 4. Customize the rug plot on the x-axis to: - Have a height of 0.1. - Be placed outside the axes without clipping. 5. Customize the rug plot on the y-axis not to use any additional parameters. 6. Save the final plot to a file named `custom_scatter_rug_plot.png`. # Input and Output Format - **Input**: No input required from the user; the dataset is loaded within the function. - **Output**: A file named `custom_scatter_rug_plot.png` should be created in the working directory, containing the generated plot. # Constraints - Make sure the plot is clear and readable. - Make effective use of Seaborn customization functionalities as described above. # Function Signature ```python def create_custom_scatter_rug_plot(): # Your implementation here # Example Usage create_custom_scatter_rug_plot() ``` # Notes - Hint: Look into the `sns.rugplot` documentation for details on the parameters `height` and `clip_on`. - Ensure that the rug plots for x and y axes are clearly visible and effectively convey the data distribution along each axis.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_scatter_rug_plot(): # Load the tips dataset tips = sns.load_dataset(\'tips\') # Create the scatter plot scatter_plot = sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'time\') # Add rug plot on the x-axis with customizations sns.rugplot(data=tips, x=\'total_bill\', height=0.1, clip_on=False, ax=scatter_plot) # Add rug plot on the y-axis without additional parameters sns.rugplot(data=tips, y=\'tip\', ax=scatter_plot) # Customize the visual style of the plot (optional) scatter_plot.set_title(\'Total Bill vs Tip with Rug Plots for Time of Day\') scatter_plot.set_xlabel(\'Total Bill\') scatter_plot.set_ylabel(\'Tip\') # Save the plot to a file plt.savefig(\'custom_scatter_rug_plot.png\') plt.close()"},{"question":"# Advanced File Locking with fcntl Module **Objective:** Demonstrate the usage of Python\'s `fcntl` module for file descriptor control and locking mechanisms. **Problem Statement:** You are tasked with implementing a function `apply_file_lock(filepath: str, lock_type: str) -> str` that locks a given file using the `fcntl` module. The function should: 1. Acquire either a shared (`LOCK_SH`) or exclusive lock (`LOCK_EX`) on the file specified by `filepath`. 2. Release the lock after a short delay (`time.sleep(2)`). 3. Return a status message after acquiring and releasing the lock. # Function Signature ```python import fcntl import os import time def apply_file_lock(filepath: str, lock_type: str) -> str: # Your code goes here ``` # Input - `filepath` (str): The path to the file that needs to be locked. - `lock_type` (str): The type of lock to acquire, either \\"shared\\" or \\"exclusive\\". # Output - Return a message indicating the successful acquisition and release of the lock in the format: \\"Successfully acquired and released [LOCK_TYPE] lock on [FILEPATH].\\" # Constraints - The file should be valid and accessible for reading and writing. - The function should handle errors gracefully and return a meaningful error message if the lock cannot be acquired. - `lock_type` should strictly be either \\"shared\\" or \\"exclusive\\". Any other value should raise a `ValueError`. # Example Usage ```python print(apply_file_lock(\\"/tmp/testfile.txt\\", \\"shared\\")) # Output: \\"Successfully acquired and released shared lock on /tmp/testfile.txt.\\" print(apply_file_lock(\\"/tmp/testfile.txt\\", \\"exclusive\\")) # Output: \\"Successfully acquired and released exclusive lock on /tmp/testfile.txt.\\" ``` # Additional Notes - Use `fcntl.lockf()` with the appropriate command to acquire and release the lock. - `LOCK_SH` should be used for a shared lock, and `LOCK_EX` for an exclusive lock. Ensure correct usage by mapping the `lock_type` string to the corresponding `fcntl` constants. - Handle any potential `OSError` exceptions that may arise from locking operations.","solution":"import fcntl import os import time def apply_file_lock(filepath: str, lock_type: str) -> str: try: # Open the file with open(filepath, \'r+\') as file: # Determine the lock type if lock_type == \\"shared\\": lock_command = fcntl.LOCK_SH elif lock_type == \\"exclusive\\": lock_command = fcntl.LOCK_EX else: raise ValueError(\\"lock_type must be \'shared\' or \'exclusive\'\\") # Apply the lock fcntl.lockf(file, lock_command) # Simulate a delay time.sleep(2) # Release the lock fcntl.lockf(file, fcntl.LOCK_UN) return f\\"Successfully acquired and released {lock_type} lock on {filepath}.\\" except OSError as e: return f\\"Error locking file: {e}\\" except Exception as e: return f\\"An unexpected error occurred: {e}\\""},{"question":"Objective: Use the scikit-learn package to load one of the toy datasets, preprocess the data, and build a simple classification model. Problem Statement: Write a function `classify_wine()` that performs the following tasks: 1. Load the wine dataset using `load_wine()` function from `sklearn.datasets`. 2. Preprocess the dataset: - Split the dataset into features (`X`) and target (`y`). - Standardize the features (mean=0 and variance=1). - Split the dataset into training and testing sets (70% training, 30% testing). 3. Train a logistic regression classifier on the training set. 4. Evaluate the model on the testing set and return the accuracy score. Implementation Details: - Use `train_test_split` from `sklearn.model_selection` for splitting the dataset. - Use `StandardScaler` from `sklearn.preprocessing` for standardizing the features. - Use `LogisticRegression` from `sklearn.linear_model` for the classifier. - Use `accuracy_score` from `sklearn.metrics` to evaluate the model. Function Signature: ```python def classify_wine() -> float: pass ``` Constraints: - Do not modify the random state in any splitting or training function to ensure reproducibility. - Do not change the structure of the function signature. Example: ```python accuracy = classify_wine() print(f\\"Model accuracy: {accuracy}\\") ``` Expected output is a floating-point number representing the accuracy of the model on the testing set, e.g., `0.95`.","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def classify_wine() -> float: # Load the wine dataset data = load_wine() X = data.data y = data.target # Standardize the features (mean=0 and var=1) scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the dataset into training and testing sets (70% training, 30% testing) X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=0) # Train a logistic regression classifier on the training set classifier = LogisticRegression(max_iter=1000) classifier.fit(X_train, y_train) # Evaluate the model on the testing set y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"Objective: Create a Python function that reads multiple text files, processes each line by reversing the words, and writes the modified content back to the same files using the `fileinput` module. Demonstrate the usage of both the context manager and in-place modification features of the `fileinput` module. Requirements: 1. **Input**: - A list of filenames (strings) to be processed. 2. **Processing**: - For each line in each file, reverse the order of the words. - Write the processed lines back to the corresponding files. 3. **Output**: - The function should return a list of filenames that were successfully processed. 4. **Constraints**: - Assume each line contains only printable ASCII characters. - No file should be larger than 1 MB. 5. **Function Prototype**: ```python def reverse_words_in_files(filenames: list[str]) -> list[str]: pass ``` 6. **Performance**: - Ensure the function can handle reading and writing of files efficiently. - Handle any potential I/O errors gracefully. Example: Suppose `file1.txt` contains: ``` hello world python programming ``` and `file2.txt` contains: ``` file input module line by line ``` After running `reverse_words_in_files([\\"file1.txt\\", \\"file2.txt\\"])`, `file1.txt` should be: ``` world hello programming python ``` and `file2.txt` should be: ``` input file module line by line ``` Notes: - Use the `fileinput.input()` function for reading and applying the `inplace=True` option for in-place file modification. - Utilize string manipulation to reverse the order of words in each line. - Ensure that appropriate error handling is implemented for file operations.","solution":"import fileinput def reverse_words_in_files(filenames: list[str]) -> list[str]: This function takes a list of filenames, processes each line by reversing the order of words, and writes the modified content back to the same files. Args: filenames (list[str]): A list of filenames to be processed. Returns: list[str]: A list of filenames that were successfully processed. successfully_processed_files = [] for filename in filenames: try: with fileinput.input(files=(filename,), inplace=True, backup=\'.bak\') as f: for line in f: reversed_line = \' \'.join(line.strip().split()[::-1]) print(reversed_line) successfully_processed_files.append(filename) except Exception as e: print(f\\"Error processing file {filename}: {e}\\") return successfully_processed_files"},{"question":"**Question: Implement and Utilize Asyncio Queues for a Task Scheduler** # Problem Statement You are to design a task scheduling system that concurrently processes tasks with varying priorities. Use the `asyncio.PriorityQueue` to schedule tasks based on their priorities. Each task must be associated with a priority level, where a lower number indicates a higher priority. Implement a function `schedule_tasks(tasks: List[Tuple[int, str]]) -> None` that: 1. Accepts a list of tuples. Each tuple consists of an integer priority and a string representing the task description. 2. Uses an `asyncio.PriorityQueue` to manage the tasks. 3. Processes tasks concurrently using a specific number of worker coroutines. 4. Outputs the task descriptions in order of their processing completion. # Requirements - The function must process tasks using at least 3 workers concurrently. - Use the `asyncio.PriorityQueue` to ensure tasks are processed in the correct priority order. - Each worker should process tasks by \\"completing\\" the task, simulated by `asyncio.sleep()`, with the duration being proportional to the priority (i.e., `await asyncio.sleep(priority * 0.1)`). - Once all tasks are completed, print every task\'s description in the order they were processed. # Input - `tasks`: List of tuples with each tuple containing: - An integer `priority`: Lower numbers indicate higher priority. - A string `description`: Description of the task. # Output - Print the task description of each processed task in the order they were completed. # Example ```python import asyncio async def schedule_tasks(tasks): # Your implementation here tasks = [(1, \'Clean the house\'), (3, \'Buy groceries\'), (2, \'Pay bills\'), (0, \'Emergency meeting\')] asyncio.run(schedule_tasks(tasks)) ``` **Expected Output:** The output order will vary due to the concurrent nature and the sleep simulation but must respect the priorities with tasks executed sooner for higher priority: ``` Emergency meeting Clean the house Pay bills Buy groceries ``` # Constraints - There will be at least one task in the input list. - Each task\'s priority is a non-negative integer. **Note:** It\'s important to correctly manage the lifecycle of the worker tasks and ensure they gracefully handle the completion of all tasks.","solution":"import asyncio from typing import List, Tuple async def worker(name: str, queue: asyncio.PriorityQueue, output_list: List[str]): while True: priority, task = await queue.get() if priority is None: # Break out of the loop if we encounter the stop signal break await asyncio.sleep(priority * 0.1) output_list.append(task) print(task) # Output the task description queue.task_done() async def schedule_tasks(tasks: List[Tuple[int, str]]) -> None: queue = asyncio.PriorityQueue() output_list = [] # Add tasks to the priority queue for task in tasks: await queue.put(task) # Create worker tasks to process the queue concurrently workers = [] for i in range(3): # At least 3 workers worker_task = asyncio.create_task(worker(f\'worker-{i}\', queue, output_list)) workers.append(worker_task) # Wait until the queue is fully processed await queue.join() # Stop the workers for worker_task in workers: await queue.put((None, None)) await asyncio.gather(*workers)"},{"question":"# **Asynchronous File Reader with Platform-Specific Handling** **Objective:** Write an asynchronous function `read_file_contents(file_path: str) -> str` that reads the contents of a file asynchronously and returns the content as a string. The implementation should handle platform-specific differences mentioned in the provided documentation, particularly focusing on the limitations and differences on Windows and macOS platforms. **Requirements:** 1. **Input:** - `file_path` (str): The path to the file to be read. 2. **Output:** - Returns the content of the file as a string. 3. **Constraints:** - The function should work correctly on both Windows and macOS. - On Windows, use \\"ProactorEventLoop\\" if available; on macOS, ensure compatibility with both modern versions and versions <= 10.8. - If on Windows and the \\"SelectorEventLoop\\" is used, ensure the async read handles the limitations regarding pipe file descriptors. 4. **Performance Requirements:** - The function should handle large files efficiently. - Ensure minimal blocking of the event loop. **Hints:** - Use `sys.platform` to determine the operating system. - Consider using `try-except` blocks to handle platform-specific method availability. - Utilize the async features of `aiofiles` or similar libraries for efficient file reading. **Example:** ```python import asyncio async def read_file_contents(file_path: str) -> str: # Your implementation goes here pass # Usage example: # content = asyncio.run(read_file_contents(\'/path/to/file.txt\')) # print(content) ``` # **Important Note:** You will need to install any necessary library to support asynchronous file operations (e.g., `aiofiles`). --- Your task is to complete the function `read_file_contents` with the above specifications. Make sure to handle platform-specific differences effectively. **Evaluation Criteria:** - Correctness of the implementation. - Effective usage of async/await and event loops. - Handling of platform-specific differences. - Code efficiency and readability.","solution":"import asyncio import sys async def read_file_contents(file_path: str) -> str: import aiofiles # Select the appropriate event loop policy based on the platform if sys.platform.startswith(\'win\'): try: loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) except AttributeError: loop = asyncio.SelectorEventLoop() asyncio.set_event_loop(loop) elif sys.platform.startswith(\'darwin\'): # macOS specific handling can be more centered on compatibility # For instance, modern python should handle the event loop natively # Older versions of macOS should be handled automatically by asyncio pass # No specific handling required currently else: loop = asyncio.get_event_loop() async with aiofiles.open(file_path, \'r\') as file: contents = await file.read() return contents"},{"question":"**Objective**: Develop a function that utilizes PyTorch\'s `torch.mps` module to perform basic operations involving MPS device management, random number generation, and memory management. Problem Statement You are required to implement a class `MPSManager` that effectively manages an MPS device by utilizing the functionalities provided in the `torch.mps` module. Your class should handle device initialization, random number generator state management, and memory management efficiently. Additionally, the class should perform profiling of a sample operation execution on the MPS device. Requirements 1. **Class Definition**: Create a class named `MPSManager`. 2. **Methods**: - `__init__(self)`: Initialize the MPS device. Ensure that at least one MPS device is available, otherwise raise an appropriate exception. - `set_seed(self, seed: int)`: Set a seed for the random number generator on the MPS device. - `get_rng_state(self) -> torch.Tensor`: Return the current state of the random number generator on the MPS device. - `empty_cache(self)`: Release all the unoccupied cached memory on the MPS device. - `profile_operation(self, operation: Callable[[], None]) -> dict`: Profile the given operation execution on the MPS device. The `operation` is a callable taking no arguments and performing a certain task on the MPS device. This method should start profiling, execute the operation, stop profiling, and return profiling data as a dictionary. Input and Output Formats - `__init__(self)`: No input parameters. Raises an exception if no MPS device is available. - `set_seed(self, seed: int)`: Takes an integer `seed` as input. No return value. - `get_rng_state(self) -> torch.Tensor`: Returns a tensor representing the random number generator\'s state. - `empty_cache(self)`: No input parameters. No return value. - `profile_operation(self, operation: Callable[[], None]) -> dict`: Takes a callable `operation` as input. Returns a dictionary containing profiling details. Constraints - You must ensure the methods handle the appropriate PyTorch `torch.mps` functionalities for their operations. - The profiling details should include at least the keys `start_time` and `end_time`, representing the start and end time of the operation; and `duration`, representing the time taken by the operation. - Assume the operations provided in `profile_operation` are valid and appropriately designed to work on the MPS device. ```python import torch import time class MPSManager: def __init__(self): if torch.mps.device_count() < 1: raise RuntimeError(\\"No MPS device found\\") self.device = torch.device(\\"mps\\") def set_seed(self, seed: int): torch.mps.manual_seed(seed) def get_rng_state(self) -> torch.Tensor: return torch.mps.get_rng_state() def empty_cache(self): torch.mps.empty_cache() def profile_operation(self, operation: Callable[[], None]) -> dict: profiling_data = {} torch.mps.profiler.start() start_time = time.time() operation() end_time = time.time() torch.mps.profiler.stop() profiling_data[\'start_time\'] = start_time profiling_data[\'end_time\'] = end_time profiling_data[\'duration\'] = end_time - start_time return profiling_data # Example Usage: # manager = MPSManager() # manager.set_seed(42) # state = manager.get_rng_state() # manager.empty_cache() # profile_data = manager.profile_operation(lambda: torch.zeros((1000, 1000), device=\'mps\')) ``` Performance Requirements - Ensure that memory is properly managed by releasing unoccupied cache regularly. - Profiling should accurately capture the operation\'s execution time and other relevant details.","solution":"import torch import time from typing import Callable class MPSManager: def __init__(self): if torch.mps.device_count() < 1: raise RuntimeError(\\"No MPS device found\\") self.device = torch.device(\\"mps\\") def set_seed(self, seed: int): torch.manual_seed(seed) def get_rng_state(self) -> torch.Tensor: return torch.mps.get_rng_state() def empty_cache(self): torch.mps.empty_cache() def profile_operation(self, operation: Callable[[], None]) -> dict: profiling_data = {} start_time = time.time() operation() end_time = time.time() profiling_data[\'start_time\'] = start_time profiling_data[\'end_time\'] = end_time profiling_data[\'duration\'] = end_time - start_time return profiling_data # Example usage: # manager = MPSManager() # manager.set_seed(42) # state = manager.get_rng_state() # manager.empty_cache() # profile_data = manager.profile_operation(lambda: torch.zeros((1000, 1000), device=\'mps\'))"},{"question":"# Task You are required to create a custom HTTP server using Python\'s `http.server` module. Implement a subclass of `BaseHTTPRequestHandler` to handle the following types of requests: 1. **GET Request**: The server should respond to a GET request with a simple HTML page that lists the files in a specified directory. If the path points to a file, it should respond with the file\'s content. 2. **POST Request**: The server should save any content from a POST request to a specified file in the directory. # Requirements 1. Implement the `MyHTTPRequestHandler` class, which should inherit from `BaseHTTPRequestHandler`. 2. For a **GET request**: - If the path is a directory, respond with an HTML page listing all files in that directory. - If the path is a file, respond with the contents of the file. 3. For a **POST request**: - Save the data received in the request body to a file in the directory specified by the request path. # Input and Output GET Request Example - Input: `GET /my-directory/` - Output: An HTML page listing all files in `my-directory`. POST Request Example - Input: `POST /my-directory/upload.txt` with body `Hello World!` - Output: File `upload.txt` is created in `my-directory` containing `Hello World!`. # Constraints - The implementation should handle basic errors such as \\"File Not Found\\" or \\"Method Not Allowed\\". - The directory should be specified as an instance variable. # Performance Requirements - The implementation should handle concurrent requests efficiently. # Example Usage ```python import http.server import socketserver class MyHTTPRequestHandler(http.server.BaseHTTPRequestHandler): directory = \'/path/to/your/directory\' def do_GET(self): # Implement GET request handling here def do_POST(self): # Implement POST request handling here PORT = 8000 Handler = MyHTTPRequestHandler with socketserver.TCPServer((\\"\\", PORT), Handler) as httpd: print(\\"serving at port\\", PORT) httpd.serve_forever() ``` **Note**: Replace `\'/path/to/your/directory\'` with the actual path you want to serve. # Submission Submit a Python script that: - Defines the `MyHTTPRequestHandler` class with the specified functionality. - Demonstrates the server running and handling GET and POST requests according to the examples.","solution":"import http.server import os from urllib.parse import unquote class MyHTTPRequestHandler(http.server.BaseHTTPRequestHandler): directory = os.path.expanduser(\\"~/my_test_directory\\") def do_GET(self): path = unquote(self.path.strip(\\"/\\")) full_path = os.path.join(self.directory, path) if os.path.isdir(full_path): self.send_response(200) self.send_header(\'Content-type\', \'text/html\') self.end_headers() self.wfile.write(b\\"<html><body>\\") self.wfile.write(b\\"<h1>Directory listing</h1>\\") self.wfile.write(b\\"<ul>\\") for filename in os.listdir(full_path): self.wfile.write(f\\"<li><a href=\'{filename}\'>{filename}</a></li>\\".encode()) self.wfile.write(b\\"</ul>\\") self.wfile.write(b\\"</body></html>\\") elif os.path.isfile(full_path): self.send_response(200) self.send_header(\'Content-Type\', \'text/plain\') self.end_headers() with open(full_path, \'rb\') as file: self.wfile.write(file.read()) else: self.send_error(404, \\"File not found\\") def do_POST(self): path = unquote(self.path.strip(\\"/\\")) full_path = os.path.join(self.directory, path) if not os.path.exists(os.path.dirname(full_path)): os.makedirs(os.path.dirname(full_path)) content_length = int(self.headers[\'Content-Length\']) post_data = self.rfile.read(content_length) try: with open(full_path, \'wb\') as file: file.write(post_data) self.send_response(201) self.end_headers() except Exception as e: self.send_error(500, f\\"Could not write to file: {e}\\") if __name__ == \\"__main__\\": import socketserver PORT = 8000 Handler = MyHTTPRequestHandler with socketserver.TCPServer((\\"\\", PORT), Handler) as httpd: print(\\"serving at port\\", PORT) httpd.serve_forever()"},{"question":"Coding Assessment Question # Objective You are required to demonstrate your understanding of PyTorch\'s Generic Join Context Manager for handling distributed training with uneven input sizes. You will implement a distributed training loop using PyTorch, ensuring correct synchronization and completion using the `Join`, `Joinable`, and `JoinHook` classes. # Task 1. Implement a PyTorch training loop for a simple neural network (e.g., a fully connected network) for distributed training. 2. Your training function should handle inputs of uneven sizes across the distributed workers. 3. Use the `Join`, `Joinable`, and `JoinHook` classes to ensure proper synchronization and handling of uneven inputs. # Requirements 1. **Model Design:** - Design a simple fully connected neural network using `torch.nn.Module`. 2. **Distributed Training Setup:** - Initialize a distributed process group. - Implement a training loop using `torch.distributed` functionalities. - Use the `Join`, `Joinable`, and `JoinHook` classes to manage uneven input sizes. 3. **Input and Output Formats:** - The training function should take the input arguments `rank`, `world_size`, and `num_epochs`. - Your function should print loss values for each epoch for each process. 4. **Constraints:** - The input data for each process should differ in size to simulate uneven inputs. 5. **Performance Requirements:** - Ensure that the training loop is efficient and correctly synchronized. # Example Function Signature ```python import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.distributed.algorithms import Join, Joinable, JoinHook # Define your neural network model class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Implement your training function def distributed_training(rank, world_size, num_epochs): # Initialize process group dist.init_process_group(\'gloo\', rank=rank, world_size=world_size) # Create a model and move it to the appropriate device model = SimpleNet().to(rank) # Define loss and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Create uneven input data for each process input_data = torch.randn((rank + 1) * 10, 10).to(rank) target_data = torch.randn((rank + 1) * 10, 1).to(rank) # Implement distributed training loop model.train() for epoch in range(num_epochs): # Zero the parameter gradients optimizer.zero_grad() # Forward pass outputs = model(input_data) loss = criterion(outputs, target_data) # Backward pass and optimization loss.backward() optimizer.step() # Print the loss print(f\'Rank {rank}, Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\') # Ensure proper synchronization and handling of uneven inputs with Join context with Join([model], rank): pass # Cleanup dist.destroy_process_group() # Example usage (to be run in a distributed manner with appropriate launch script) if __name__ == \\"__main__\\": import argparse parser = argparse.ArgumentParser() parser.add_argument(\\"--rank\\", type=int) parser.add_argument(\\"--world_size\\", type=int) parser.add_argument(\\"--num_epochs\\", type=int, default=10) args = parser.parse_args() distributed_training(args.rank, args.world_size, args.num_epochs) ```","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.distributed.algorithms.join import Join, Joinable # Define your neural network model class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Function to set up the process group for distributed training def setup(rank, world_size): dist.init_process_group(\'gloo\', rank=rank, world_size=world_size) # Function to clean up the process group def cleanup(): dist.destroy_process_group() # Implement your training function def distributed_training(rank, world_size, num_epochs, backend=\'gloo\'): # Initialize process group setup(rank, world_size) # Create a model and move it to the appropriate device model = SimpleNet().to(rank) # Define loss and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Create uneven input data for each process input_data = torch.randn((rank + 1) * 10, 10).to(rank) target_data = torch.randn((rank + 1) * 10, 1).to(rank) # Implement distributed training loop model.train() for epoch in range(num_epochs): # Zero the parameter gradients optimizer.zero_grad() # Forward pass outputs = model(input_data) loss = criterion(outputs, target_data) # Backward pass and optimization loss.backward() optimizer.step() # Print the loss print(f\'Rank {rank}, Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\') # Ensure proper synchronization and handling of uneven inputs with Join context with Join([model], rank): pass # Cleanup cleanup()"},{"question":"# Python-C Extension: Custom Math Operations Module In this task, you will create a Python extension module using C. The module, `mymath`, will provide custom math operations unavailable in the standard Python library. The operations will include advanced calculations that utilize underlying C library functions for performance efficiency. Specifications: 1. **C File (`mymathmodule.c`)**: - Implement the functions `add`, `subtract`, `multiply`, and `divide`. - Ensure these functions can be called from Python. The functions will take two integers as input and return the result as an integer. - Follow Python\'s API for error handling. For example, `divide` should raise a `ZeroDivisionError` if the second argument is zero. - Properly manage memory and reference counts. 2. **Method Table and Initialization**: - Provide a method table listing the above functions. - Implement the initialization function to create the module object and add these functions to it. 3. **Python Code to Test**: - Write a small Python script to test your module, importing `mymath` and calling each of the operations with different arguments. Function Definitions in C: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> // Add two integers static PyObject *mymath_add(PyObject *self, PyObject *args) { int a, b; if (!PyArg_ParseTuple(args, \\"ii\\", &a, &b)) return NULL; return PyLong_FromLong(a + b); } // Subtract second integer from first static PyObject *mymath_subtract(PyObject *self, PyObject *args) { int a, b; if (!PyArg_ParseTuple(args, \\"ii\\", &a, &b)) return NULL; return PyLong_FromLong(a - b); } // Multiply two integers static PyObject *mymath_multiply(PyObject *self, PyObject *args) { int a, b; if (!PyArg_ParseTuple(args, \\"ii\\", &a, &b)) return NULL; return PyLong_FromLong(a * b); } // Divide first integer by second static PyObject *mymath_divide(PyObject *self, PyObject *args) { int a, b; if (!PyArg_ParseTuple(args, \\"ii\\", &a, &b)) return NULL; if (b == 0) { PyErr_SetString(PyExc_ZeroDivisionError, \\"division by zero\\"); return NULL; } return PyLong_FromLong(a / b); } // Method table static PyMethodDef MyMathMethods[] = { {\\"add\\", mymath_add, METH_VARARGS, \\"Add two integers\\"}, {\\"subtract\\", mymath_subtract, METH_VARARGS, \\"Subtract two integers\\"}, {\\"multiply\\", mymath_multiply, METH_VARARGS, \\"Multiply two integers\\"}, {\\"divide\\", mymath_divide, METH_VARARGS, \\"Divide two integers\\"}, {NULL, NULL, 0, NULL} }; // Module initialization static struct PyModuleDef mymathmodule = { PyModuleDef_HEAD_INIT, \\"mymath\\", \\"Custom Math Module\\", -1, MyMathMethods }; PyMODINIT_FUNC PyInit_mymath(void) { return PyModule_Create(&mymathmodule); } ``` Python Test Script: ```python import mymath print(\\"Addition:\\", mymath.add(10, 5)) print(\\"Subtraction:\\", mymath.subtract(10, 5)) print(\\"Multiplication:\\", mymath.multiply(10, 5)) try: print(\\"Division:\\", mymath.divide(10, 0)) except ZeroDivisionError as e: print(e) print(\\"Division:\\", mymath.divide(10, 5)) ``` # Constraints: - Ensure that your functions handle errors gracefully and set exceptions correctly using the provided Python API functions (`PyErr_SetString`, etc.). - Manage reference counts appropriately to avoid memory leaks or the usage of freed memory. - The module should be named `mymath` and follow the structure and practices outlined in the provided documentation. You should submit: 1. The `mymathmodule.c` file containing the C code. 2. A Python script (`test_mymath.py`) that tests all functions in the module. Good luck!","solution":"def add(a, b): Returns the sum of a and b. return a + b def subtract(a, b): Returns the difference when b is subtracted from a. return a - b def multiply(a, b): Returns the product of a and b. return a * b def divide(a, b): Returns the quotient of a divided by b. Raises a ZeroDivisionError if b is zero. if b == 0: raise ZeroDivisionError(\\"division by zero\\") return a // b # Using integer division to be consistent with C implementation"},{"question":"# Python 3.10 Coding Assessment Question Objective Implement a complex number processing utility using built-in Python 3.10 functions. Detailed Description You are required to write several functions to process complex numbers and collections of these numbers. Specifically, you will implement a utility that performs the following tasks: 1. **Compute Magnitudes:** Given a list of complex numbers, return their magnitudes sorted in ascending order. 2. **Unique Angles Dictionary:** For a given list of complex numbers, return a dictionary where the keys are unique angles (in degrees) from the complex numbers (rounded to one decimal place), and the values are lists of original complex numbers that correspond to each angle. 3. **Filtered Magnitudes:** Filter out complex numbers whose magnitude is below a certain threshold and return their magnitudes as a list. Function Specifications 1. **compute_magnitudes(complex_numbers: List[complex]) -> List[float]** - Takes a list of complex numbers and returns a list of their magnitudes sorted in ascending order. 2. **unique_angles_dict(complex_numbers: List[complex]) -> Dict[float, List[complex]]** - Takes a list of complex numbers and returns a dictionary. Keys are unique angles (in degrees, rounded to one decimal place) obtained using `cmath.phase`. Values are lists of complex numbers corresponding to each angle. 3. **filter_below_threshold(complex_numbers: List[complex], threshold: float) -> List[float]** - Takes a list of complex numbers and a threshold value and returns a list of magnitudes of numbers whose magnitude is greater than or equal to the threshold. Input and Output Formats - **Input:** - A list of complex numbers (`complex_numbers`). - A floating-point threshold (`threshold`) for the third function. - **Output:** - For the first function, a sorted list of magnitudes as floats. - For the second function, a dictionary as described. - For the third function, a list of magnitudes above the threshold as floats. Constraints - Use only the built-in functions listed in the provided documentation. - Magnitudes and angles should be computed using built-in capabilities. - Ensure your solution handles potential edge cases (e.g., empty list input). Examples ```python # Example Input complex_numbers = [3 + 4j, 1 - 1j, -1 + 0j] threshold = 2.0 # Example Output for compute_magnitudes [1.0, 5.0, 5.0] # Example Output for unique_angles_dict { 135.0: [(1-1j)], 180.0: [(-1+0j)], 53.1: [(3+4j)] } # Example Output for filter_below_threshold [5.0] ``` Implement the required functions with the given specifications.","solution":"import math import cmath from typing import List, Dict def compute_magnitudes(complex_numbers: List[complex]) -> List[float]: Returns the magnitudes of the given list of complex numbers sorted in ascending order. magnitudes = [abs(c) for c in complex_numbers] return sorted(magnitudes) def unique_angles_dict(complex_numbers: List[complex]) -> Dict[float, List[complex]]: Returns a dictionary where keys are unique angles (in degrees), and values are lists of complex numbers corresponding to each angle. angle_dict = {} for c in complex_numbers: angle = math.degrees(cmath.phase(c)) angle = round(angle, 1) if angle not in angle_dict: angle_dict[angle] = [] angle_dict[angle].append(c) return angle_dict def filter_below_threshold(complex_numbers: List[complex], threshold: float) -> List[float]: Filters out complex numbers whose magnitude is below the specified threshold and returns a list of the remaining magnitudes. return [abs(c) for c in complex_numbers if abs(c) >= threshold]"},{"question":"# Python Coding Assessment Question Question Title: Implementing a Custom Context Manager for Resource Management Objective: To test the student\'s understanding of compound statements, context management, and error handling in Python. Problem Statement: You are required to design a custom context manager that can be used to manage the lifecycle of a resource. Specifically, this context manager will open a file, allow operations to be performed on it, and ensure that the file is properly closed when done, regardless of whether an error occurred during the operations. The context manager should: - Open a file when entering the context. - Allow operations to be performed on the file within the context. - Close the file when exiting the context, even if an exception is raised. - Suppress any IOError that might occur during the closing of the file but propagate any other exceptions. Task: 1. Implement a class `FileManager` that acts as a context manager. 2. The `FileManager` should have methods `__enter__()` and `__exit__()` to handle entering and exiting the context. 3. In the `__enter__()` method, open the specified file in write mode and return the file object. 4. In the `__exit__()` method, ensure the file is closed properly. If an `IOError` occurs during closing, it should be suppressed. Any other exceptions should not be suppressed. Signature: ```python class FileManager: def __init__(self, filename: str): # Initialize with the given filename pass def __enter__(self): # Open the file and return the file object pass def __exit__(self, exc_type, exc_value, traceback): # Ensure the file is closed and handle exceptions during closing pass ``` Input: - A string `filename` representing the name of the file to manage. Output: - The `FileManager` does not return any output, but it must handle the file as specified. Example Usage: ```python with FileManager(\'example.txt\') as f: f.write(\'Hello, world!\') # After the with block, the file should be closed automatically. ``` Constraints: - Do not use any external libraries for file handling. - Handle exceptions as specified, ensuring the program does not crash when `IOError` occurs during file closing. Notes: - Pay special attention to resource management and exception handling as discussed in the provided documentation.","solution":"class FileManager: def __init__(self, filename: str): self.filename = filename self.file = None def __enter__(self): self.file = open(self.filename, \'w\') return self.file def __exit__(self, exc_type, exc_value, traceback): try: if self.file: self.file.close() except IOError: # Suppress IOError pass # Propagate any other exceptions return exc_type is IOError"},{"question":"**Objective:** Implement a function that simulates importing multiple modules as if by using the `PyImport_ImportModule` or similar functions from the Python C API in a Python-only context. This exercise tests your understanding of Python\'s import system, reference handling, and dynamic module importing. **Problem Statement:** You are required to implement a function `import_multiple_modules(module_names: List[str]) -> Dict[str, Optional[ModuleType]]` in Python. This function will: 1. Accept a list of module names (strings) to be imported. 2. Attempt to import each module. 3. Return a dictionary where the keys are the module names and the values are the modules themselves if imported successfully, or `None` if the module could not be imported. **Function Signature:** ```python from typing import List, Dict, Optional from types import ModuleType def import_multiple_modules(module_names: List[str]) -> Dict[str, Optional[ModuleType]]: pass ``` **Requirements:** 1. Use the built-in `importlib` module to perform the imports. 2. Handle any exceptions that occur during the import operation and mark the module\'s value as `None` in the result dictionary. 3. Make sure that the function is efficient and correctly handles the import and error checking. 4. Do not use the `exec` function or similar dynamic code execution functions. **Constraints:** - The input list may contain between 1 and 100 module names. - Module names are valid Python identifiers (no need to handle malformed names). **Example:** ```python import sys import types import importlib def import_multiple_modules(module_names): result = {} for name in module_names: try: result[name] = importlib.import_module(name) except ModuleNotFoundError: result[name] = None return result # Example usage: module_list = [\\"os\\", \\"sys\\", \\"nonexistentmodule\\"] result_dict = import_multiple_modules(module_list) print(result_dict) # Expected output: # {\'os\': <module \'os\' from \'...\'>, \'sys\': <module \'sys\' (built-in)>, \'nonexistentmodule\': None} ``` **Note:** - You should not rely on specific paths or versions of Python libraries in your implementation. Assumptions about the system\'s environment should be minimized. - Include appropriate error handling and comments in your implementation to demonstrate your understanding of potential edge cases and failure modes.","solution":"from typing import List, Dict, Optional from types import ModuleType import importlib def import_multiple_modules(module_names: List[str]) -> Dict[str, Optional[ModuleType]]: Accepts a list of module names, attempts to import each, and returns a dictionary with the module or None if import fails. Args: module_names : List[str] - List of module names to import. Returns: Dict[str, Optional[ModuleType]] - Dictionary with module names as keys and module or None as values. result = {} for name in module_names: try: result[name] = importlib.import_module(name) except ModuleNotFoundError: result[name] = None return result"},{"question":"Implement a Quoted-Printable Encoder and Decoder You are required to implement a small utility that makes use of the `quopri` module to encode and decode files in quoted-printable format. Function 1: `encode_file(input_file: str, output_file: str, quotetabs: bool) -> None` - **Objective**: Encode the contents of the input file and write the encoded data to the output file. - **Parameters**: - `input_file`: A string representing the name of the input file (in binary mode). - `output_file`: A string representing the name of the output file (in binary mode). - `quotetabs`: A boolean flag that decides whether to encode embedded spaces and tabs. - **Return Value**: None - **Behavior**: - Open the `input_file` in binary read mode. - Open the `output_file` in binary write mode. - Use the `quopri.encode` function to perform the encoding. Function 2: `decode_file(input_file: str, output_file: str) -> None` - **Objective**: Decode the contents of the input file and write the decoded data to the output file. - **Parameters**: - `input_file`: A string representing the name of the input file (in binary mode). - `output_file`: A string representing the name of the output file (in binary mode). - **Return Value**: None - **Behavior**: - Open the `input_file` in binary read mode. - Open the `output_file` in binary write mode. - Use the `quopri.decode` function to perform the decoding. Constraints - `input_file` and `output_file` will always be valid file paths. - You can assume the files are small enough to fit into memory. Example Usage ```python # Assume \'input.txt\' contains binary data that needs encoding encode_file(\'input.txt\', \'encoded_output.txt\', quotetabs=True) # Assume \'encoded_output.txt\' contains quoted-printable encoded data decode_file(\'encoded_output.txt\', \'decoded_output.txt\') ``` # Performance Requirements - The implementation should be efficient with respect to file handling, ensuring that files are properly opened and closed. # Note - To test your implementation, create some files with binary data (contents are up to you), and encode/decode them using your functions.","solution":"import quopri def encode_file(input_file: str, output_file: str, quotetabs: bool) -> None: Encode the contents of the input file and write the encoded data to the output file. :param input_file: A string representing the name of the input file (in binary mode). :param output_file: A string representing the name of the output file (in binary mode). :param quotetabs: A boolean flag that decides whether to encode embedded spaces and tabs. :return: None with open(input_file, \'rb\') as f_in: with open(output_file, \'wb\') as f_out: quopri.encode(f_in, f_out, quotetabs) def decode_file(input_file: str, output_file: str) -> None: Decode the contents of the input file and write the decoded data to the output file. :param input_file: A string representing the name of the input file (in binary mode). :param output_file: A string representing the name of the output file (in binary mode). :return: None with open(input_file, \'rb\') as f_in: with open(output_file, \'wb\') as f_out: quopri.decode(f_in, f_out)"},{"question":"# Custom SquarePlus Activation Function in PyTorch Context Activation functions play a crucial role in neural networks by introducing non-linearity. While there are numerous built-in activations like ReLU, Sigmoid, etc., sometimes custom activations are necessary for specific tasks. In this exercise, you\'re required to implement a custom activation function called `SquarePlus` in PyTorch. The `SquarePlus` function is defined by: [ text{SquarePlus}(x) = (x + sqrt{x^2 + 1}) / 2 ] and its gradient is: [ text{SquarePlus}\'(x) = (1 + frac{x}{sqrt{x^2 + 1}}) / 2 ] Task 1. **Implement the Custom Function**: Implement the `SquarePlusFunction` by subclassing `torch.autograd.Function`. Your class should define both `forward` and `backward` methods. 2. **Wrap it in a Module**: Create a custom `SquarePlus` module by subclassing `torch.nn.Module`. The forward pass of this module should use your custom `SquarePlusFunction`. 3. **Validate with Gradient Check**: Validate the implementation using `torch.autograd.gradcheck` to ensure the gradients are computed correctly. Requirements - Implement the `SquarePlusFunction` class with the necessary `forward` and `backward` methods. - Create a `SquarePlus` module that uses the `SquarePlusFunction`. - Perform gradient checking using given input tensors suitable for gradient calculation. Input Your implementation should handle the following: 1. An N-dimensional input tensor for the `SquarePlusFunction`. 2. Standard module inputs for the `SquarePlus` class, ensuring it can handle batched inputs. Output Your output should include: 1. The computed forward pass of the `SquarePlus` activation function. 2. The gradients passed back during the backward pass. Example Code Structure ```python import torch from torch.autograd import Function from torch.nn import Module import torch.nn.functional as F from torch.autograd import gradcheck class SquarePlusFunction(Function): @staticmethod def forward(ctx, input): # Your implementation here @staticmethod def backward(ctx, grad_output): # Your implementation here class SquarePlus(Module): def forward(self, input): return SquarePlusFunction.apply(input) # Example usage and gradient check input = torch.randn(5, 5, dtype=torch.double, requires_grad=True) square_plus_module = SquarePlus() print(\\"Output:\\", square_plus_module(input)) # Gradient check input_for_check = (torch.randn(5, 5, dtype=torch.double, requires_grad=True),) grad_test = gradcheck(SquarePlusFunction.apply, input_for_check, eps=1e-6, atol=1e-4) print(\\"Gradient check passed:\\", grad_test) ``` Constraints - Ensure that the forward and backward passes are implemented without any in-place operations on tensors. - The `backward` function should properly handle non-differentiable inputs by returning `None` as needed.","solution":"import torch from torch.autograd import Function from torch.nn import Module from torch.autograd import gradcheck class SquarePlusFunction(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) output = (input + torch.sqrt(input**2 + 1)) / 2 return output @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * (1 + input / torch.sqrt(input**2 + 1)) / 2 return grad_input class SquarePlus(Module): def forward(self, input): return SquarePlusFunction.apply(input) # Example usage and gradient check input = torch.randn(5, 5, dtype=torch.double, requires_grad=True) square_plus_module = SquarePlus() output = square_plus_module(input) print(\\"Output:\\", output) # Gradient check input_for_check = (torch.randn(5, 5, dtype=torch.double, requires_grad=True),) grad_test = gradcheck(SquarePlusFunction.apply, input_for_check, eps=1e-6, atol=1e-4) print(\\"Gradient check passed:\\", grad_test)"},{"question":"# Multi-threaded Counter with Synchronization You are required to write a Python program using the `_thread` module that demonstrates the following functionality: 1. **Counter Class**: - Implement a `Counter` class that contains: - A private integer variable `count`. - A lock for thread synchronization. - Methods `increment` and `decrement` to safely adjust the `count` in a thread-safe manner. - A method `get_count` to return the current value of `count`. 2. **Worker Function**: - A worker function `adjust_counter(counter, increments)` that accepts a `Counter` instance and a number of increments to perform. The function should: - Increment the `counter` by 1 for each iteration if `increments` is positive. - Decrement the `counter` by 1 for each iteration if `increments` is negative. 3. **Multi-threaded Execution**: - In the `main` function, create an instance of the `Counter` class. - Start `n` threads to increment the counter by `m` (where `n` and `m` are specified by the user). - Start `k` threads to decrement the counter by `p` (where `k` and `p` are specified by the user). 4. **Result**: - Ensure all threads complete their execution. - Print the final value of the counter. **Constraints**: - You may assume reasonable bounds for the values of `n`, `m`, `k`, and `p` (e.g., `1 <= n, m, k, p <= 1000`). **Input**: - Four integers `n`, `m`, `k`, and `p` representing the number of incrementing threads, increments per thread, decrementing threads, and decrements per thread respectively. **Output**: - The final counter value after all threads have finished their work. # Example: ```python import _thread import time class Counter: def __init__(self): self.count = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: self.count += 1 def decrement(self): with self.lock: self.count -= 1 def get_count(self): with self.lock: return self.count def adjust_counter(counter, increments): if increments > 0: for _ in range(increments): counter.increment() else: for _ in range(-increments): counter.decrement() def main(n, m, k, p): counter = Counter() threads = [] for _ in range(n): _thread.start_new_thread(adjust_counter, (counter, m)) for _ in range(k): _thread.start_new_thread(adjust_counter, (counter, -p)) # Wait for threads to complete time.sleep(1) # Adjust sleep time based on your needs print(\\"Final Counter Value:\\", counter.get_count()) if __name__ == \\"__main__\\": n, m, k, p = 5, 10, 3, 7 # Example values main(n, m, k, p) ``` **Important Note**: In real-world scenarios, thread management would involve using more advanced synchronization mechanisms such as joining threads or using higher-level threading modules like `threading` which handle such scenarios more gracefully. This example is kept simple to focus on demonstrating the basics of using the `_thread` module.","solution":"import _thread import time class Counter: def __init__(self): self.count = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: self.count += 1 def decrement(self): with self.lock: self.count -= 1 def get_count(self): with self.lock: return self.count def adjust_counter(counter, increments): if increments > 0: for _ in range(increments): counter.increment() else: for _ in range(-increments): counter.decrement() def main(n, m, k, p): counter = Counter() for _ in range(n): _thread.start_new_thread(adjust_counter, (counter, m)) for _ in range(k): _thread.start_new_thread(adjust_counter, (counter, -p)) # Wait for threads to complete time.sleep(1) # Adjust sleep time based on your needs return counter.get_count()"},{"question":"**Question: Exception Handling and Name Resolution in Functions** In this assignment, you are required to create a function `process_data` that processes a list of integers. The function should demonstrate the principles of name binding, scope resolution, and exception handling. # Requirements: 1. **Input**: - `data`: A list of integers. 2. **Processing**: - The function should iterate over each integer in the list. - If an integer is less than `0`, it should raise a `ValueError` with the message `\\"Negative value encountered: <value>\\"`. - The function should maintain a count of how many times each integer has been processed. Use a dictionary where the keys are integers and the values are their count. - You should have a nested function `increment_count` inside `process_data` that updates the count. Use the `nonlocal` keyword in it to modify the enclosing function’s variable. 3. **Output**: - Return the dictionary with counts of each integer processed. 4. **Constraints and Limitations**: - You should use exception handling to ensure that even if a `ValueError` is raised, the function continues to process the remaining integers. - Modify the builtins `print` function to write the error message to a list `error_log` instead of the console. # Example: ```python data = [1, 2, 3, -1, 2, 3, 4] output = process_data(data) print(output) # Expected Output: {1: 1, 2: 2, 3: 2, 4: 1} print(error_log) # Expected to contain: [\\"Negative value encountered: -1\\"] ``` # Implementation: ```python # Write your implementation of process_data here. ``` This function `process_data` should illustrate your understanding of name resolution rules, proper use of scopes (including local and nonlocal), and effective exception handling. Ensure to handle logs in a way that shows flexibility with modifying builtins.","solution":"def process_data(data): Processes a list of integers and returns a dictionary with the counts of each integer. It should handle negative values by raising exceptions and continue processing. count_dict = {} error_log = [] def increment_count(value): nonlocal count_dict if value in count_dict: count_dict[value] += 1 else: count_dict[value] = 1 # Override the print function to write to error_log import builtins original_print = builtins.print def mock_print(*args, **kwargs): message = \' \'.join(str(arg) for arg in args) error_log.append(message) builtins.print = mock_print try: for num in data: try: if num < 0: raise ValueError(f\\"Negative value encountered: {num}\\") increment_count(num) except ValueError as e: print(e) finally: # Restore the original print function builtins.print = original_print return count_dict, error_log"},{"question":"**Objective**: Demonstrate understanding and practical use of the `fnmatch` module for file manipulation based on Unix shell-style patterns. **Problem Statement**: You are given a directory path and a wildcard pattern. Your task is to write a Python function that searches through all the files in the specified directory and its subdirectories (recursively) and returns a list of all files that match the given pattern. The comparison should be case-insensitive. **Function Signature**: ```python def find_matching_files(directory: str, pattern: str) -> list: Search for files matching the given pattern in the specified directory and its subdirectories. Parameters: - directory (str): The path of the directory to search. - pattern (str): The Unix shell-style wildcard pattern to match filenames against. Returns: - list: A list with the paths of all matching files. ``` **Input**: - `directory`: A string representing the path of the directory to search. - `pattern`: A Unix shell-style wildcard pattern (e.g., `*.txt`, `data_??.csv`, `[!a]*.py`). **Output**: - Returns a list of strings representing the paths of the matching files. **Constraints**: - The function should handle large directories efficiently. - It should be case-insensitive for pattern matching. **Example**: ```python # Assume the following file structure in directory \\"/test\\": # /test/ # ├── file1.txt # ├── File2.TXT # ├── subdir/ # │ ├── file3.txt # │ └── Note.doc # └── image.JPG # Calling the function with pattern \\"*.txt\\" should return: result = find_matching_files(\\"/test\\", \\"*.txt\\") # Possible output: [\'/test/file1.txt\', \'/test/File2.TXT\', \'/test/subdir/file3.txt\'] ``` **Hint**: - You might find the functions `os.listdir()`, `os.path.join()`, and `os.walk()` useful for navigating directories. - Remember to use `fnmatch.fnmatch()` for pattern matching. Good luck, and happy coding!","solution":"import os import fnmatch def find_matching_files(directory: str, pattern: str) -> list: Search for files matching the given pattern in the specified directory and its subdirectories. Parameters: - directory (str): The path of the directory to search. - pattern (str): The Unix shell-style wildcard pattern to match filenames against. Returns: - list: A list with the paths of all matching files. matching_files = [] # Walk through directory and subdirectories for root, _, files in os.walk(directory): for filename in files: if fnmatch.fnmatchcase(filename.lower(), pattern.lower()): matching_files.append(os.path.join(root, filename)) return matching_files"},{"question":"# Multi-threaded File Processing with Thread Pool and Synchronization **Objective:** Implement a multi-threaded file processing system where multiple threads read data from different files, process the data, and store the results in a shared resource. Use synchronization primitives to ensure thread-safe operations. **Problem Statement:** You are given a list of file names, each file containing numerical data. The task is to create a system that uses multiple threads to: 1. Read data from each file concurrently. 2. Process the data (e.g., calculate the sum of numbers in each file). 3. Store the results in a shared dictionary, where the key is the file name and the value is the calculated sum. Implement the following functions: 1. `read_and_process_file(file_name: str, results_dict: dict, lock: threading.Lock) -> None`: - Read the file and calculate the sum of the numbers in the file. - Use the provided lock to ensure that updates to `results_dict` are thread-safe. - Store the result in `results_dict` where the key is `file_name`. 2. `process_files_concurrently(file_names: List[str]) -> Dict[str, int]`: - Create a thread pool to process files concurrently. - Ensure that a maximum of 5 threads are processing files at any given time using a threading `Semaphore`. - Utilize a `threading.Lock` to protect access to the shared results dictionary. - Return the final results dictionary containing the file names and their respective sums. **Input:** - `file_names`: A list of strings where each string is the path to a file containing numerical data. **Output:** - A dictionary where keys are file names, and values are the sums of the numbers in the respective files. **Constraints:** - Each file contains integers, one per line. - Handle file read errors gracefully. - Ensure all threads complete execution before returning the results. ```python import threading from typing import List, Dict def read_and_process_file(file_name: str, results_dict: Dict[str, int], lock: threading.Lock) -> None: This function reads the file, processes the numbers, and updates the shared dictionary. # Your implementation here def process_files_concurrently(file_names: List[str]) -> Dict[str, int]: This function processes multiple files concurrently and returns the result dictionary. # Your implementation here # Example usage: file_names = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] print(process_files_concurrently(file_names)) ``` **Note:** - You will need to provide implementation for these functions. - Ensure that your implementation is thread-safe and handles file operations correctly. - The example usage demonstrates how to call your function, but the actual files need to contain numerical data for testing.","solution":"import threading from typing import List, Dict import os def read_and_process_file(file_name: str, results_dict: Dict[str, int], lock: threading.Lock) -> None: This function reads the file, processes the numbers, and updates the shared dictionary. try: with open(file_name, \'r\') as file: total = sum(int(line.strip()) for line in file) except Exception as e: total = 0 with lock: results_dict[file_name] = total def process_files_concurrently(file_names: List[str]) -> Dict[str, int]: This function processes multiple files concurrently and returns the result dictionary. results_dict = {} lock = threading.Lock() semaphore = threading.Semaphore(5) def worker(file_name): with semaphore: read_and_process_file(file_name, results_dict, lock) threads = [] for file_name in file_names: thread = threading.Thread(target=worker, args=(file_name,)) threads.append(thread) thread.start() for thread in threads: thread.join() return results_dict"},{"question":"# PyTorch Special Functions: Gamma Distribution Transformation **Objective**: Use functions from the `torch.special` module to transform a given set of inputs through a series of mathematical transformations involving gamma-related functions. **Task Description**: Implement a function `gamma_transformations` that takes a 1-dimensional PyTorch tensor as input and performs the following operations: 1. Calculate the logarithm of the gamma function for each element using `gammaln`. 2. Compute the digamma function (the logarithmic derivative of the gamma function) for each element using `digamma`. 3. Compute the generalized gamma function (incomplete gamma function) for each element using `gammainc`. 4. Sum up the results of these operations element-wise. # Function Signature: ```python import torch def gamma_transformations(input_tensor: torch.Tensor) -> torch.Tensor: # Your code here ``` # Input: - `input_tensor`: A 1-dimensional PyTorch tensor of size `n` (1 <= n <= 1000), where each element is a positive real number between 0.01 and 100. # Output: - Returns a 1-dimensional PyTorch tensor of size `n` containing the result of the given transformations. # Example: ```python input_tensor = torch.tensor([0.1, 1.0, 2.5, 5.0]) output = gamma_transformations(input_tensor) print(output) # Example output: tensor([value1, value2, value3, value4]) ``` # Constraints: 1. Use only functions from the `torch.special` module for the transformations. 2. Ensure numerical stability by handling potential edge cases where the functions might produce extreme values or NaNs, particularly around the lower or upper limits of the input domain. This task assesses the student’s ability to work with special mathematical functions in PyTorch, handle tensor operations, and ensure computational integrity under varying input conditions.","solution":"import torch def gamma_transformations(input_tensor: torch.Tensor) -> torch.Tensor: Applies a series of gamma-related transformations to the input tensor. Steps: 1. Compute the logarithm of the gamma function for each element using gammaln. 2. Compute the digamma function for each element. 3. Compute the generalized gamma function for each element using gammainc. 4. Sum up the results from the previous steps element-wise. Args: input_tensor (torch.Tensor): 1-dimensional tensor of positive real numbers. Returns: torch.Tensor: 1-dimensional tensor containing the transformed results. import torch.special # Compute the logarithm of the gamma function gammaln_values = torch.special.gammaln(input_tensor) # Compute the digamma function digamma_values = torch.special.digamma(input_tensor) # Compute the generalized gamma function (lower incomplete gamma function) # We use input_tensor as both arguments to gammainc for simplicity here. gammainc_values = torch.special.gammainc(input_tensor, input_tensor) # Sum up the results element-wise result = gammaln_values + digamma_values + gammainc_values return result"},{"question":"# PyLongObject Manipulation and Conversion You are tasked with writing a Python function that demonstrates the creation and conversion of PyLongObjects using various provided functions. Function Specification Write a Python function `manipulate_pylong_objects` that performs the following tasks: 1. **Create PyLongObjects**: - Create a PyLongObject from a `long` integer (e.g., 123456789). - Create a PyLongObject from an `unsigned long` integer (e.g., 3000000000). - Create a PyLongObject from a string containing a valid integer (e.g., \\"4294967295\\"). - Create a PyLongObject from a float (e.g., 12345.6789). 2. **Convert PyLongObjects**: - Convert the PyLongObject created from the `long` integer into a `long`. - Convert the PyLongObject created from the `unsigned long` integer into an `unsigned long`. - Convert the PyLongObject created from the string into a `long long`. - Convert the PyLongObject created from the float into a `double`. 3. **Handle Conversion Errors**: - If any conversion fails due to overflow or type error, catch the exception and return an appropriate error message. 4. **Return Results**: - Return a dictionary with the results of the conversions with keys: `\\"long_from_long\\"`, `\\"unsigned_long_from_unsigned_long\\"`, `\\"long_long_from_string\\"`, and `\\"double_from_float\\"`. Input: - No input is required for the function. The function will work with internally defined values. Output: - A dictionary containing the converted values as described above. Example: ```python { \\"long_from_long\\": 123456789, \\"unsigned_long_from_unsigned_long\\": 3000000000, \\"long_long_from_string\\": 4294967295, \\"double_from_float\\": 12345.0 } ``` Constraints: - Make sure to handle error and overflow conditions appropriately. - Use the functions described in the provided documentation for creating and converting PyLongObjects. **Your task:** Implement the `manipulate_pylong_objects` function according to the specification above.","solution":"def manipulate_pylong_objects(): results = {} try: # Create PyLongObjects long_obj = int(123456789) unsigned_long_obj = int(3000000000) string_obj = int(\\"4294967295\\") float_obj = float(12345.6789) # Convert PyLongObjects long_from_long = long_obj unsigned_long_from_unsigned_long = unsigned_long_obj # Python handles big integers natively long_long_from_string = string_obj double_from_float = float(int(float_obj)) results[\\"long_from_long\\"] = long_from_long results[\\"unsigned_long_from_unsigned_long\\"] = unsigned_long_from_unsigned_long results[\\"long_long_from_string\\"] = long_long_from_string results[\\"double_from_float\\"] = double_from_float except (OverflowError, TypeError) as e: return {\\"error\\": str(e)} return results"},{"question":"**Objective**: Create functions to utilize `zlib` for compressing and decompressing data efficiently. This will test your understanding of compression/decompression mechanisms, handling stream-based data, and error management in Python. **Problem Statement**: You are given a list of strings `data_list` that need to be compressed and subsequently decompressed. Implement two functions: 1. `compress_data_list(data_list, level=-1)`: - **Input**: - `data_list`: A list of strings to be compressed. - `level`: An optional integer between `0` to `9` or `-1`, representing the compression level. Default is `-1`. - **Output**: - A list of compressed byte objects corresponding to the strings in `data_list`. 2. `decompress_data_list(compressed_list)`: - **Input**: - `compressed_list`: A list of compressed byte objects. - **Output**: - A list of decompressed strings corresponding to the byte objects in `compressed_list`. **Constraints**: - Each string in `data_list` contains printable ASCII characters and has a maximum length of 10,000 characters. - The list `data_list` can have up to 1,000 strings. - The functions should handle any compression/decompression errors gracefully using exception handling. **Example**: ```python data_list = [\\"The quick brown fox jumps over the lazy dog.\\"] * 5 # Compress the data list compressed_list = compress_data_list(data_list, level=6) print(compressed_list) # Decompress the data list decompressed_list = decompress_data_list(compressed_list) print(decompressed_list) ``` The `decompressed_list` should match the original `data_list`. **Notes**: - Use the `zlib.compress` function for compression and `zlib.decompress` function for decompression. - Handle the default compression level and provide efficient data processing techniques. This problem will test your ability to work with compression libraries, handle a list of data efficiently, and ensure integrity through decompression.","solution":"import zlib def compress_data_list(data_list, level=-1): Compresses a list of strings using zlib. :param data_list: List of strings to compress :param level: Compression level (default -1) :return: List of compressed byte objects if not (isinstance(level, int) and -1 <= level <= 9): raise ValueError(\\"Compression level must be an integer between -1 and 9.\\") compressed_list = [] for data in data_list: try: compressed_data = zlib.compress(data.encode(\'utf-8\'), level) compressed_list.append(compressed_data) except Exception as e: print(f\\"Error compressing data: {e}\\") return compressed_list def decompress_data_list(compressed_list): Decompresses a list of compressed byte objects using zlib. :param compressed_list: List of compressed byte objects :return: List of decompressed strings decompressed_list = [] for compressed_data in compressed_list: try: decompressed_data = zlib.decompress(compressed_data).decode(\'utf-8\') decompressed_list.append(decompressed_data) except Exception as e: print(f\\"Error decompressing data: {e}\\") return decompressed_list"},{"question":"# Python Coding Assessment: File System Traversal and Permission Modification **Objective**: Use the `stat` module to analyze and update file permissions within a directory. Problem Statement You are required to write a Python function `secure_files(directory: str) -> List[str]` that recursively traverses the given directory and performs the following tasks: 1. **Identify and list all files with write permissions enabled for others (i.e., the world/anyone).** 2. **For each identified file:** - Print the file name. - Change the file permissions to remove the write permission for others. The function should return a list containing the relative paths of all files whose permissions were modified. Function Signature ```python def secure_files(directory: str) -> List[str]: ``` Input - `directory` (str): The path of the directory to be traversed. Output - Returns a list of strings, where each string is the relative path of a file whose permissions were modified. Constraints - Assume you have read and write permissions for the directory and its contents. - You should use the `os`, `stat`, and `sys` modules as needed. - Use the functions provided by the `stat` module to check file modes and permissions. Example ```python # Suppose the directory structure is as follows: # test_dir/ # ├── file1.txt (permissions: -rw-r--r--) # ├── file2.txt (permissions: -rw-rw-rw-) # ├── subdir/ # │ ├── file3.txt (permissions: -rw-rw-rw-) # │ ├── file4.txt (permissions: -rw-r--r--) secure_files(\'test_dir\') # Expected output: # [\'file2.txt\', \'subdir/file3.txt\'] # And the permissions of file2.txt and subdir/file3.txt will be changed to remove others\' write permission. ``` Note - You can use `os.walk()` for directory traversal. - Use `os.chmod()` to modify file permissions. - Use functions from the `stat` module to check and set file permissions.","solution":"import os import stat from typing import List def secure_files(directory: str) -> List[str]: modified_files = [] for root, dirs, files in os.walk(directory): for file in files: file_path = os.path.join(root, file) file_stat = os.stat(file_path) if file_stat.st_mode & stat.S_IWOTH: print(file_path) os.chmod(file_path, file_stat.st_mode & ~stat.S_IWOTH) modified_files.append(os.path.relpath(file_path, directory)) return modified_files"},{"question":"**Objective:** Write a Python function `execute_module_or_path` that determines whether to execute a Python module by its name or by a specified path, using the `runpy` module. Use the appropriate `runpy` function based on the input provided. The function should also handle and demonstrate the following requirements: - Pre-populating the module\'s globals dictionary with a given dictionary. - Configuring the `__name__` of the executed code. - Altering the `sys` module when needed. **Function Signature:** ```python def execute_module_or_path(identifier: str, globals_dict: dict, run_by_path: bool, run_name: str = None) -> dict: pass ``` **Parameters:** - `identifier` (str): - If `run_by_path` is `False`, this should be the module\'s name (e.g., `\'some_module\'`). - If `run_by_path` is `True`, this should be the path to the file or sys.path entry (e.g., `\'/path/to/module.py\'`). - `globals_dict` (dict): A dictionary used to pre-populate the global variables for the module\'s execution. - `run_by_path` (bool): - If `True`, execute the code at the specified filesystem location using `runpy.run_path`. - If `False`, execute the code of the specified module using `runpy.run_module`. - `run_name` (str, optional): The name to assign to `__name__` during execution. Defaults to `None`. **Returns:** - `dict`: The resulting globals dictionary after the module or path has been executed. **Constraints and Notes:** - The `identifier` must be a valid module name or a valid path depending on the `run_by_path` flag. - Ensure the proper use of the `runpy` functions: `runpy.run_module` and `runpy.run_path`. - Handle any exceptions that may be raised during the execution of the module or path. - Alter the `sys` module only if necessary and ensure any alterations are properly restored to their original state. **Example Usage:** ```python def sample_usage(): # Example with module name globals_dict = {\'example_var\': 42} result = execute_module_or_path(\'example_module\', globals_dict, run_by_path=False) print(result) # Example with file path globals_dict = {\'example_var\': 42} result = execute_module_or_path(\'/path/to/example_module.py\', globals_dict, run_by_path=True) print(result) ``` **Notes:** - You will need to create the `example_module` for testing the above examples or replace the names/paths with actual modules and paths available in your environment. - Ensure that your implementation cleans up any changes it makes to the `sys` module after execution.","solution":"import runpy import sys def execute_module_or_path(identifier: str, globals_dict: dict, run_by_path: bool, run_name: str = None) -> dict: if run_name is not None: globals_dict[\'__name__\'] = run_name original_sys_path = list(sys.path) try: if run_by_path: result_globals = runpy.run_path(identifier, init_globals=globals_dict) else: result_globals = runpy.run_module(identifier, init_globals=globals_dict) except Exception as e: raise RuntimeError(f\\"Failed to execute module or path: {e}\\") finally: sys.path = original_sys_path return result_globals"},{"question":"# Task: Write a function `create_penguins_plot` that generates a dot plot using the Seaborn Objects Interface. The plot should visualize the `body_mass_g` on the y-axis for different `species` on the x-axis with some jitter applied to the dots. The function should take three parameters: 1. `width`: A float controlling the amount of jitter relative to the spacing between the marks along the orientation axis. 2. `jitter_x`: A float controlling the jitter in data units along the x-axis. 3. `jitter_y`: A float controlling the jitter in data units along the y-axis. Your function should: 1. Load the `penguins` dataset from seaborn. 2. Create a `so.Plot` object with the `species` on the x-axis and `body_mass_g` on the y-axis. 3. Apply jitter to both x and y using the specified `width`, `jitter_x`, and `jitter_y` parameters. 4. Return the plot object. Input: - `width`: float - `jitter_x`: float - `jitter_y`: float Output: - A seaborn plot object with the specified jitter applied. Constraint: - Ensure the function handles empty data gracefully without errors. - Adequate performance for datasets up to 10,000 entries should be maintained. # Example Usage: ```python plot = create_penguins_plot(0.3, 100, 5) plot.show() ``` ```python plot = create_penguins_plot(0.5, 50, 10) plot.show() ``` ```python # No jitter on the orientation axis, jitter applied only to x-axis and y-axis plot = create_penguins_plot(0, 150, 20) plot.show() ``` # Note: - Ensure you have seaborn installed and updated to the latest version which supports the Objects Interface. - Reference: The `seaborn.objects` and `seaborn.load_dataset` modules.","solution":"import seaborn as sns from seaborn import objects as so def create_penguins_plot(width, jitter_x, jitter_y): Generates a dot plot with jitter using the Seaborn Objects Interface. Parameters: - width (float): Amount of jitter relative to the spacing between the marks along the orientation axis. - jitter_x (float): Amount of jitter in data units along the x-axis. - jitter_y (float): Amount of jitter in data units along the y-axis. Returns: - so.Plot object with the specified jitter applied # Load the penguins dataset from seaborn penguins = sns.load_dataset(\\"penguins\\") # Create the plot object p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\").add(so.Dot(), so.Jitter(width=width, x=jitter_x, y=jitter_y)) return p"},{"question":"# Seaborn Data Visualization Assessment Objective Your task is to demonstrate your understanding of seaborn\'s data visualization capabilities by creating various plots using both long-form and wide-form data formats. Task Description 1. **Data Preparation**: - Load the built-in seaborn dataset \\"flights\\". - Convert this dataset into a wide-form format by pivoting. 2. **Plot Creation**: - Create a line plot using the long-form \\"flights\\" data to visualize the number of passengers per year, colored by month. - Create the same line plot using the wide-form \\"flights\\" data. 3. **Advanced Data Manipulation and Plotting**: - Load the built-in seaborn dataset \\"anagrams\\". - Transform the dataset into a long-form format suitable for seaborn plotting. - Create a point plot to visualize the average score of memory performance as a function of both attention and number of solutions. Requirements - **Data Transformation Functions**: - Implement functions to pivot data, melt data, or manipulate data as needed. - **Plotting Functions**: - Implement functions to create the specified plots. Input and Output Formats 1. **Function 1: Data Preparation** ```python def prepare_data(): Returns: flights_long (pd.DataFrame): The long-form \'flights\' dataset. flights_wide (pd.DataFrame): The wide-form \'flights\' dataset. ``` 2. **Function 2: Long-form Plot** ```python def long_form_plot(flights_long): Args: flights_long (pd.DataFrame): The long-form \'flights\' dataset. Returns: None: Displays the plot inline. ``` 3. **Function 3: Wide-form Plot** ```python def wide_form_plot(flights_wide): Args: flights_wide (pd.DataFrame): The wide-form \'flights\' dataset. Returns: None: Displays the plot inline. ``` 4. **Function 4: Advanced Plot** ```python def advanced_plot(): Returns: None: Displays the plot inline. ``` Constraints - Use only seaborn, pandas, numpy, and matplotlib libraries. - Ensure the plots are clear, labeled, and well-presented. Example Here\'s a simple example to illustrate how you might structure your functions and plots. ```python import seaborn as sns import pandas as pd def prepare_data(): flights = sns.load_dataset(\\"flights\\") flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") return flights, flights_wide def long_form_plot(flights_long): sns.relplot(data=flights_long, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") def wide_form_plot(flights_wide): sns.relplot(data=flights_wide, kind=\\"line\\") def advanced_plot(): anagrams = sns.load_dataset(\\"anagrams\\") anagrams_long = anagrams.melt(id_vars=[\\"subidr\\", \\"attnr\\"], var_name=\\"solutions\\", value_name=\\"score\\") sns.catplot(data=anagrams_long, x=\\"solutions\\", y=\\"score\\", hue=\\"attnr\\", kind=\\"point\\") ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def prepare_data(): Load and transform the \'flights\' dataset into long-form and wide-form formats. Returns: flights_long (pd.DataFrame): The long-form \'flights\' dataset. flights_wide (pd.DataFrame): The wide-form \'flights\' dataset. flights = sns.load_dataset(\\"flights\\") flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") return flights, flights_wide def long_form_plot(flights_long): Create a line plot using the long-form \'flights\' dataset. Args: flights_long (pd.DataFrame): The long-form \'flights\' dataset. plt.figure(figsize=(10, 6)) sns.lineplot(data=flights_long, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\") plt.title(\\"Number of Passengers Per Year Colored by Month\\") plt.show() def wide_form_plot(flights_wide): Create a line plot using the wide-form \'flights\' dataset. Args: flights_wide (pd.DataFrame): The wide-form \'flights\' dataset. flights_wide.plot(figsize=(10, 6)) plt.title(\\"Number of Passengers Per Year Colored by Month\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Passengers\\") plt.legend(title=\\"Month\\") plt.show() def advanced_plot(): Create a point plot to visualize the average score of memory performance as a function of both attention and number of solutions. anagrams = sns.load_dataset(\\"anagrams\\") anagrams_long = anagrams.melt(id_vars=[\\"subidr\\", \\"attnr\\"], var_name=\\"solutions\\", value_name=\\"score\\") plt.figure(figsize=(10, 6)) sns.pointplot(data=anagrams_long, x=\\"solutions\\", y=\\"score\\", hue=\\"attnr\\", dodge=True) plt.title(\\"Average Score of Memory Performance by Attention and Number of Solutions\\") plt.show()"},{"question":"Custom PyTorch DataLoader with Multi-Process and Memory Pinning You are tasked with implementing a PyTorch data pipeline for training a machine learning model. You will need to create a custom dataset class, a custom collate function, and set up a DataLoader that supports multi-process data loading and memory pinning for efficient GPU usage. The dataset comprises tuples of images (3D numpy arrays) and labels (integers), where images are represented as 3-channel (RGB) tensors. Requirements 1. Implement a `CustomDataset` class that inherits from `torch.utils.data.Dataset`. This class should load images from a provided directory and the corresponding labels from a JSON file. Each image is a numpy array with shape `(3, H, W)`. 2. Implement a custom collate function `custom_collate_fn` that pads each image in a batch to match the maximum height and width in the batch, and stacks them into a single tensor. Additionally, convert the list of labels into a single tensor. 3. Create a DataLoader using the custom dataset and collate function, enabling multi-process data loading with 4 worker processes, and memory pinning to accelerate data transfer to the GPU. 4. Ensure your implementation handles the following constraints: - The dataset directory contains images stored as `.png` files. - The JSON file provides labels in the format: `{\\"image_filename\\": label}`. - The DataLoader should use a batch size of 8. - Automatically shuffle the data at the start of each epoch. Input Format - A directory path `data_dir` containing images as `.png` files. - A JSON file `labels.json` located in the same directory, mapping image filenames to integer labels. Output Format - N/A (ensure the custom DataLoader works correctly and efficiently). Example Code ```python import json import os from PIL import Image import numpy as np import torch from torch.utils.data import Dataset, DataLoader class CustomDataset(Dataset): def __init__(self, data_dir): self.data_dir = data_dir with open(os.path.join(data_dir, \'labels.json\'), \'r\') as f: self.labels = json.load(f) self.image_filenames = list(self.labels.keys()) def __len__(self): return len(self.image_filenames) def __getitem__(self, idx): image_filename = self.image_filenames[idx] image = np.array(Image.open(os.path.join(self.data_dir, image_filename))) label = self.labels[image_filename] return torch.tensor(image).permute(2, 0, 1), label # Convert HWC to CHW def custom_collate_fn(batch): images, labels = zip(*batch) max_height = max(image.size(1) for image in images) max_width = max(image.size(2) for image in images) padded_images = torch.zeros(len(images), 3, max_height, max_width) for i, image in enumerate(images): _, h, w = image.size() padded_images[i, :, :h, :w] = image # Padding each image return padded_images, torch.tensor(labels) # Create DataLoader data_dir = \'path_to_data_directory\' # replace with your data directory dataset = CustomDataset(data_dir) dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn, num_workers=4, pin_memory=True) # Test dataloader for images, labels in dataloader: print(images.shape) # Should print a tensor of shape (batch_size, 3, max_height, max_width) print(labels.shape) # Should print a tensor of shape (batch_size,) ``` Ensure your implementation: - Loads data without memory errors. - Efficiently handles data loading with multiple workers. - Properly pads variable-sized images and batches them. - Utilizes memory pinning for faster GPU transfers.","solution":"import json import os from PIL import Image import numpy as np import torch from torch.utils.data import Dataset, DataLoader class CustomDataset(Dataset): def __init__(self, data_dir): Args: data_dir (str): Directory path where the images (.png) and labels.json are stored. self.data_dir = data_dir with open(os.path.join(data_dir, \'labels.json\'), \'r\') as f: self.labels = json.load(f) self.image_filenames = list(self.labels.keys()) def __len__(self): return len(self.image_filenames) def __getitem__(self, idx): image_filename = self.image_filenames[idx] image = np.array(Image.open(os.path.join(self.data_dir, image_filename))) label = self.labels[image_filename] # Convert image to tensor and change dimensions from HWC to CHW image_tensor = torch.tensor(image).permute(2, 0, 1) return image_tensor, label def custom_collate_fn(batch): images, labels = zip(*batch) max_height = max(image.size(1) for image in images) max_width = max(image.size(2) for image in images) padded_images = torch.zeros(len(images), 3, max_height, max_width) for i, image in enumerate(images): _, h, w = image.size() padded_images[i, :, :h, :w] = image return padded_images, torch.tensor(labels) # Function to create a DataLoader def create_data_loader(data_dir, batch_size=8, num_workers=4, pin_memory=True): dataset = CustomDataset(data_dir) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, num_workers=num_workers, pin_memory=pin_memory) return dataloader"},{"question":"Objective: To assess your understanding of PyTorch, including tensor operations, mathematical functions, and gradient computation, please complete the following task. Problem Statement: You are tasked with implementing a function that prepares data for a neural network, processes it by applying various tensor operations, and calculates the gradient of a predefined loss function with respect to the parameters. Task: Implement a function `prepare_and_process_data(tensor_data, target_tensor)` that performs the following operations: 1. **Data Preparation**: - `tensor_data` and `target_tensor` are 2D tensors of the same shape, representing batch input data and corresponding targets. - Normalize the `tensor_data` to have zero mean and unit variance. - Convert `target_tensor` to a one-hot encoded tensor. 2. **Model Definition**: - Create a simple linear model with a single layer using `torch.nn.Linear`. - Initialize the model\'s weights and biases using a normal distribution. 3. **Forward Pass**: - Perform a forward pass by passing the normalized `tensor_data` through the linear model. 4. **Loss Computation**: - Compute the mean squared error loss between the model\'s outputs and the `target_tensor`. 5. **Gradient Computation**: - Compute the gradient of the loss with respect to the model\'s parameters. 6. **Return**: - Return the normalized input data, model\'s output, computed loss, and gradients of the loss w.r.t the model\'s parameters. Function Signature: ```python import torch def prepare_and_process_data(tensor_data: torch.Tensor, target_tensor: torch.Tensor): # Data Preparation: Normalize tensor data # Model Definition: Initialize a linear model # Forward Pass: Pass through the model # Loss Computation: Calculate MSE loss # Gradient Computation: Calculate gradients # Return: normalized data, model output, loss, gradients pass ``` Constraints: - `tensor_data` and `target_tensor` are of shape (batch_size, num_features) where batch_size > 1 and num_features > 1. - Do not use external libraries other than PyTorch. - Ensure the function runs efficiently with a maximum time complexity of O(n * m) where n is the batch size and m is the number of features. Example: ```python batch_size = 4 num_features = 3 tensor_data = torch.randn(batch_size, num_features) target_tensor = torch.randint(0, 2, (batch_size, num_features), dtype=torch.float32) normalized_data, model_output, loss, gradient = prepare_and_process_data(tensor_data, target_tensor) print(\\"Normalized Data:\\", normalized_data) print(\\"Model Output:\\", model_output) print(\\"Loss:\\", loss) print(\\"Gradient:\\", gradient) ```","solution":"import torch def prepare_and_process_data(tensor_data: torch.Tensor, target_tensor: torch.Tensor): # Normalize tensor_data to have zero mean and unit variance mean = tensor_data.mean(dim=0, keepdim=True) std = tensor_data.std(dim=0, keepdim=True) normalized_data = (tensor_data - mean) / std # Convert target_tensor to one-hot encoded tensor target_tensor_one_hot = torch.nn.functional.one_hot(target_tensor.argmax(dim=1), num_classes=target_tensor.size(1)).to(torch.float32) # Create a simple linear model with a single layer input_dim = tensor_data.size(1) model = torch.nn.Linear(input_dim, input_dim) torch.nn.init.normal_(model.weight) torch.nn.init.normal_(model.bias) # Perform a forward pass model_output = model(normalized_data) # Compute mean squared error loss criterion = torch.nn.MSELoss() loss = criterion(model_output, target_tensor_one_hot) # Compute gradient of the loss with respect to the model\'s parameters model.zero_grad() loss.backward() gradients = { \\"weight\\": model.weight.grad, \\"bias\\": model.bias.grad } return normalized_data, model_output, loss.item(), gradients"},{"question":"Objective Write a Python function called `titanic_plot_analysis` that utilizes the seaborn library to produce multiple customized plots from the Titanic dataset. This function should demonstrate your ability to create and customize complex visualizations using seaborn. Function Signature ```python def titanic_plot_analysis() -> None: pass ``` Requirements 1. **Load the Titanic Dataset**: Use the `sns.load_dataset` function to load the Titanic dataset. 2. **Create a Violin Plot**: - Show the distribution of age (`x`) across different classes (`y`). - Color the plot by gender (`hue=\'sex\'`). - Customize the violin plot by adjusting `bw_adjust` to `0.5`, setting `cut` to `0`, and enabling `split=True`. 3. **Create a Box Plot**: - Display a box plot of age (`x`) against the classes (`y`). - Add a hue distinction for gender (`hue=\'sex\'`). - Ensure box colors have an alpha transparency value of 0.6. 4. **Layer a Swarm Plot**: - On top of the box plot from step 3, overlay a swarm plot showing individual ages grouped by class. - Set the size parameter of swarm points to `3`. 5. **Create a Grid of Bar Plots**: - Use `col=\'sex\'` to create separate subplots for male and female passengers. - Each subplot should display the average survival rate (`y=\'survived\'`) for each class (`x=\'class\'`). - Customize the figsize with `height=4` and `aspect=0.6`. 6. **Customize the Grid**: - Set the y-axis label to `\\"Survival Rate\\"`. - Rename the x-axis tick labels to `[\\"Men\\", \\"Women\\", \\"Children\\"]`. - Set the titles of each subplot to include both the column name and variable. - Define the y-axis limit from 0 to 1. - Remove the spine on the left side of the plots. Expectations - The function should not return any value. - The function should create and display all specified plots when executed. - Ensure proper use of seaborn documentation to utilize functions and parameters effectively. Example Usage ```python titanic_plot_analysis() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def titanic_plot_analysis() -> None: # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create a Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot( x=\\"age\\", y=\\"class\\", hue=\\"sex\\", data=titanic, bw=0.5, cut=0, split=True ) plt.title(\'Violin Plot of Age Distribution by Class and Gender\') plt.show() # Create a Box Plot plt.figure(figsize=(10, 6)) sns.boxplot( x=\\"age\\", y=\\"class\\", hue=\\"sex\\", data=titanic, palette=\\"Set3\\", boxprops=dict(alpha=0.6) ) # Layer a Swarm Plot sns.swarmplot( x=\\"age\\", y=\\"class\\", data=titanic, color=\\"0.25\\", size=3 ) plt.title(\'Box Plot with Swarm Plot Overlay of Age Distribution by Class and Gender\') plt.show() # Create a Grid of Bar Plots g = sns.catplot( x=\\"class\\", y=\\"survived\\", col=\\"sex\\", data=titanic, kind=\\"bar\\", height=4, aspect=0.6, ci=None ) # Customize the Grid g.set_axis_labels(\\"\\", \\"Survival Rate\\") g.set_xticklabels([\\"Men\\", \\"Women\\", \\"Children\\"]) g.set_titles(col_template=\\"{col_name} {col_var}\\") g.set(ylim=(0, 1)) for ax in g.axes.flat: ax.spines[\'left\'].set_visible(False) plt.show()"},{"question":"Asynchronous Task Scheduler # Objective In this task, you will implement an asynchronous task scheduler using Python\'s `asyncio` module. The scheduler will handle multiple tasks that need to be executed at different intervals. # Problem Statement You need to create a class `TaskScheduler` that provides functionality to schedule and run periodic tasks concurrently. # Requirements 1. **Class Definition**: `TaskScheduler` should be a class with the following methods: - `__init__(self)`: Initializes the scheduler with an empty task list. - `schedule_task(self, interval: int, coro)`: Schedules a coroutine `coro` to run every `interval` seconds. - `start(self)`: Starts the task scheduler and runs all scheduled tasks concurrently. 2. **Task Execution**: Each task should run periodically at the specified interval. The scheduler should stop running if all tasks are completed or canceled externally. 3. **Usage of asyncio**: Implement the scheduler using asyncio\'s high-level APIs, including creating and running coroutines concurrently. # Input and Output - No direct input or output is required from the class methods. The focus is on correctly scheduling and running the tasks. - The tasks themselves can provide print statements for debugging or demonstration purposes. # Constraints - The scheduler should efficiently handle multiple tasks with different intervals. - Ensure that all scheduled tasks run concurrently and periodically. # Example ```python import asyncio class TaskScheduler: def __init__(self): self.tasks = [] def schedule_task(self, interval: int, coro): async def task(): while True: await coro() await asyncio.sleep(interval) self.tasks.append(task) async def start(self): await asyncio.gather(*(task() for task in self.tasks)) # Example task async def print_time(): print(f\'Current time: {asyncio.get_event_loop().time()}\') # Setting up the scheduler scheduler = TaskScheduler() scheduler.schedule_task(2, print_time) # Schedule print_time to run every 2 seconds # Start the scheduler asyncio.run(scheduler.start()) ``` # Explanation In this example: - A `TaskScheduler` object is created. - The `print_time` coroutine is scheduled to run every 2 seconds. - The scheduler is started, which begins concurrent execution of the scheduled tasks. Implement and test your `TaskScheduler` class with various tasks and intervals to demonstrate its functionality.","solution":"import asyncio class TaskScheduler: def __init__(self): self.tasks = [] def schedule_task(self, interval: int, coro): Schedules a coroutine `coro` to run every `interval` seconds. async def task(): while True: await coro() await asyncio.sleep(interval) self.tasks.append(task) async def start(self): Starts the task scheduler and runs all scheduled tasks concurrently. await asyncio.gather(*(task() for task in self.tasks)) # Example async task async def print_hello(): print(f\'Hello! The time is {asyncio.get_event_loop().time()}\') # Setting up the scheduler scheduler = TaskScheduler() scheduler.schedule_task(2, print_hello) # Schedule print_hello to run every 2 seconds # To run the scheduler manually: # asyncio.run(scheduler.start())"},{"question":"Objective Implement a solution that demonstrates your understanding of Principal Component Analysis (PCA) using scikit-learn. You are required to perform dimensionality reduction on a dataset and visualize the results. Problem Statement You are given a dataset containing features of various observations. Your task is to reduce the dimensionality of this dataset and visualize the results in a 2D plot using PCA. Additionally, you will need to calculate and return the explained variance ratio of the first two principal components. Dataset For this exercise, use the `iris` dataset, which is available via `sklearn.datasets`. Function Signature ```python def perform_pca_and_plot(): This function should: 1. Load the iris dataset. 2. Perform PCA to reduce the dimensionality to 2 components. 3. Create a 2D scatter plot of the results, coloring the points based on their class. 4. Return the explained variance ratio of the first two components. pass ``` Requirements 1. **Library to Use:** `scikit-learn` for PCA and `matplotlib` for plotting. 2. **Input and Output Formats:** - No input parameters are required for the function. - The function should return a tuple containing the explained variance ratio of the first two components as a numpy array. 3. **Plot Specifications:** - The scatter plot should have the x-axis labeled as \\"Principal Component 1\\" and the y-axis labeled as \\"Principal Component 2\\". - Color the points based on their class labels. 4. **Performance Constraints:** - Ensure the plot is generated using matplotlib. - The function should execute within a reasonable time for the provided dataset. Example The function call `perform_pca_and_plot()` should return the explained variance ratio of the first two components, and display a 2D scatter plot of the iris dataset reduced to two principal components. ```python import numpy as np # Example output (The actual values may vary) explained_variance_ratio = np.array([0.92461872, 0.05306648]) ``` Hint You can refer to the documentation of `sklearn.decomposition.PCA` and `sklearn.datasets.load_iris` for more details on implementing PCA and loading the dataset.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.decomposition import PCA from sklearn.datasets import load_iris def perform_pca_and_plot(): # Load the iris dataset iris = load_iris() X = iris.data y = iris.target target_names = iris.target_names # Perform PCA to reduce the dimensionality to 2 components pca = PCA(n_components=2) X_r = pca.fit_transform(X) # Create a 2D scatter plot of the results, coloring the points based on their class plt.figure() colors = [\'navy\', \'turquoise\', \'darkorange\'] lw = 2 for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'PCA of IRIS dataset\') plt.xlabel(\\"Principal Component 1\\") plt.ylabel(\\"Principal Component 2\\") plt.grid() # Show the plot plt.show() # Return the explained variance ratio of the first two components return pca.explained_variance_ratio_"},{"question":"<|Analysis Begin|> The provided documentation gives a brief overview of various pandas functionalities through a series of example code snippets. The examples span across varied topics such as conditional operations on dataframes, multi-indexing, groupby operations, arithmetic with pandas, slicing, creating new columns, handling missing data, timeseries, merging dataframes, and IO operations involving CSVs, SQL, and other file formats. Based on this documentation, we can design a question that challenges students to implement several key pandas operations. Considering the depth and breadth of the examples, they can be utilized to design a question that ensures the student\'s comprehensive understanding of pandas\' conditional operations, groupby mechanics, merging functionalities, and handling missing data. <|Analysis End|> <|Question Begin|> # Pandas Data Manipulation and Analysis Task You are given a CSV file named `sales_data.csv` containing sales records with the following columns: - `sales_id`: Unique identifier for each sales transaction. - `product_id`: Identifier for the product sold. - `store_id`: Identifier for the store where the sale occurred. - `date`: The date of the sale (formatted as \'YYYY-MM-DD\'). - `quantity`: Number of units sold. - `revenue`: Total revenue from the sale. Using this data, your tasks are as follows: 1. **Load the Data:** Load the `sales_data.csv` into a pandas DataFrame. 2. **Data Cleaning:** - Check for and handle any missing values in the `quantity` and `revenue` columns by replacing them with the mean value of the respective column. - Ensure the `date` column is in datetime format. 3. **Feature Engineering:** - Create a new column `month` that derives the month from the `date` column. - Create a new column `high_revenue` that indicates with a boolean value (True/False) whether the revenue of a transaction is higher than the median revenue of all transactions. 4. **Data Analysis:** - Group the data by `store_id` and `month`, and calculate the total `quantity` of products sold and the total `revenue` for each group. - Find the `store_id` and `month` combination where the highest total `quantity` of products was sold. - For each `store_id`, find the month with the highest average `revenue` per transaction. 5. **Data Merging:** - Assume you have another CSV file named `store_info.csv` with columns `store_id`, `store_name`, and `region`. Merge this DataFrame with your sales data to include `store_name` and `region` for each sale. Implement a Python function following the format below: ```python import pandas as pd def analyze_sales_data(sales_file: str, store_file: str): # Load the sales data sales_df = pd.read_csv(sales_file) # Data Cleaning # Process the missing values, converting date column to datetime, etc. # Feature Engineering # Add \'month\', \'high_revenue\' columns # Data Analysis # Grouping, aggregations, and required analysis # Data Merging # Merge with store info # Return or print necessary outputs # Note: Ensure the implementation returns or prints the requested results return result # Example usage: # result = analyze_sales_data(\\"sales_data.csv\\", \\"store_info.csv\\") # print(result) ``` Make sure your solution handles edge cases and follows good coding practices.","solution":"import pandas as pd def analyze_sales_data(sales_file: str, store_file: str): # Load the sales data sales_df = pd.read_csv(sales_file) # Data Cleaning sales_df[\'quantity\'].fillna(sales_df[\'quantity\'].mean(), inplace=True) sales_df[\'revenue\'].fillna(sales_df[\'revenue\'].mean(), inplace=True) sales_df[\'date\'] = pd.to_datetime(sales_df[\'date\']) # Feature Engineering sales_df[\'month\'] = sales_df[\'date\'].dt.month median_revenue = sales_df[\'revenue\'].median() sales_df[\'high_revenue\'] = sales_df[\'revenue\'] > median_revenue # Data Analysis group = sales_df.groupby([\'store_id\', \'month\']).agg( total_quantity=(\'quantity\', \'sum\'), total_revenue=(\'revenue\', \'sum\') ).reset_index() highest_quantity_store_month = group.loc[group[\'total_quantity\'].idxmax()] avg_revenue_per_transaction = sales_df.groupby([\'store_id\', \'month\'])[\'revenue\'].mean().reset_index() highest_avg_revenue_per_store = avg_revenue_per_transaction.loc[ avg_revenue_per_transaction.groupby(\'store_id\')[\'revenue\'].idxmax() ] # Data Merging store_info_df = pd.read_csv(store_file) merged_df = pd.merge(sales_df, store_info_df, on=\'store_id\', how=\'left\') # Returning the results as a tuple return (group, highest_quantity_store_month, highest_avg_revenue_per_store, merged_df) # Example usage: # result = analyze_sales_data(\\"sales_data.csv\\", \\"store_info.csv\\") # print(result)"},{"question":"# Python Development Mode Assessment Objective: To assess the student\'s ability to use Python Development Mode to debug and resolve issues in a Python script. Problem Statement: You are provided with a Python script that reads a text file and processes its contents. The script has potential issues that can be identified using Python Development Mode. Your task is to: 1. Run the script with Python Development Mode enabled. 2. Identify and fix any issues reported by the Development Mode. 3. Write a report summarizing the initial issues, how you identified them, and the steps taken to fix them. Here is the provided script: ```python import sys import os def main(): if len(sys.argv) < 2: print(\\"Usage: python script.py <file_name>\\") sys.exit(1) file_name = sys.argv[1] fp = open(file_name) firstline = fp.readline() print(firstline.rstrip()) os.close(fp.fileno()) # The file is closed implicitly if __name__ == \\"__main__\\": main() ``` Instructions: 1. Save the script as `script.py`. 2. Create a text file named `example.txt` with some content to use as input for the script. 3. Enable Python Development Mode and run the script using the command: ```sh python3 -X dev script.py example.txt ``` 4. Observe any warnings or errors displayed and take note of them. 5. Modify the script to fix the identified issues. 6. Execute the modified script in Python Development Mode again to ensure all issues are resolved. 7. Write a report detailing: - The original issues identified. - How you identified these issues. - Changes made to the script to resolve these issues. Submission Requirements: 1. The modified `script.py` file with all issues resolved. 2. The text file `example.txt` used for testing. 3. A report (in PDF or markdown format) summarizing the issues, identification process, and fixes applied. Constraints: - Do not remove or bypass the usage of Python Development Mode; it must be enabled to identify and fix issues in your script. - Ensure that the modified script adheres to best practices for resource handling (e.g., proper file closing). Evaluation: - Correct identification of issues using Python Development Mode. - Effectiveness and correctness of the fixes applied. - Clarity and thoroughness of the report documenting the debugging process.","solution":"import sys def main(): if len(sys.argv) < 2: print(\\"Usage: python script.py <file_name>\\") sys.exit(1) file_name = sys.argv[1] try: with open(file_name) as fp: firstline = fp.readline() print(firstline.rstrip()) except FileNotFoundError: print(f\\"Error: The file \'{file_name}\' does not exist.\\") sys.exit(1) if __name__ == \\"__main__\\": main()"},{"question":"Objective Your task is to demonstrate proficiency in using seaborn\'s `relplot` function for visualizing data. Instructions 1. **Load Data:** - Load the \\"tips\\" dataset using `seaborn.load_dataset`. 2. **Create a Multifaceted Scatter Plot:** - Use `seaborn.relplot` to create a scatter plot that shows the relationship between the \'total_bill\' and \'tip\' variables. - Color the points by \'day\' and style them by \'time\'. - Facet the plot into columns based on \'time\' and rows based on \'sex\'. - Set the `height` of each facet to 5 and the aspect ratio to 0.8. 3. **Enhance the Plot:** - Add a horizontal line at y = 0 with gray color and dashed style across all facets. - Set the x-axis label as \\"Total Bill\\" and the y-axis label as \\"Tip\\". - Title each facet with \\"Time: {col_name} | Sex: {row_name}\\" where `{col_name}` and `{row_name}` are the respective levels of \'time\' and \'sex\'. 4. **Save the Plot:** - Save the resulting plot as `faceted_tips_plot.png`. Expected Function Signature ```python def create_faceted_tips_plot(): pass ``` Notes - Ensure you import the necessary libraries (`seaborn`, `matplotlib.pyplot`). - Your function should not take any parameters. - Handle any required settings (e.g., theme setting) within the function. Example ```python def create_faceted_tips_plot(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"ticks\\") # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the scatter plot with facets g = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", col=\\"time\\", row=\\"sex\\", height=5, aspect=0.8 ) # Enhance the plot g.map(plt.axhline, y=0, color=\\".7\\", dashes=(2, 1), zorder=0) g.set_axis_labels(\\"Total Bill\\", \\"Tip\\") g.set_titles(\\"Time: {col_name} | Sex: {row_name}\\") g.tight_layout(w_pad=0) # Save the plot g.savefig(\\"faceted_tips_plot.png\\") ``` Submission Submit your solution in a Python script or Jupyter Notebook file. Be sure to validate your code with the examples provided.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_faceted_tips_plot(): sns.set_theme(style=\\"ticks\\") # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the scatter plot with facets g = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", col=\\"time\\", row=\\"sex\\", height=5, aspect=0.8 ) # Enhance the plot g.map(plt.axhline, y=0, color=\\".7\\", dashes=(2, 1), zorder=0) g.set_axis_labels(\\"Total Bill\\", \\"Tip\\") g.set_titles(\\"Time: {col_name} | Sex: {row_name}\\") g.tight_layout(w_pad=0) # Save the plot g.savefig(\\"faceted_tips_plot.png\\")"},{"question":"# Question: File Organizer using the \\"glob\\" Module Your task is to implement a function that organizes files in a given directory based on their file types. The function should create subdirectories for each file type and move the files into the corresponding subdirectory. The file type is defined by the file extension (e.g., \\".txt\\", \\".jpg\\"). Function Signature ```python def organize_files(base_dir: str) -> None: Organize files in the specified directory based on their file types. Parameters: - base_dir: str - The path to the directory to organize. Returns: - None ``` Input - `base_dir`: A string representing the path of the directory to organize. The path may be absolute or relative. Output - This function does not return anything. It modifies the directory structure by creating subdirectories and moving files into them. Constraints - Do not organize files inside already existing subdirectories. - Ignore hidden files (those starting with a dot). - If the function encounters any issues (e.g., permission denied), it should skip the problematic file and continue processing the rest. - Use the `glob` module to find files. Example Suppose you have the following files in the directory `/my_dir`: ``` file1.txt file2.txt image1.jpg image2.jpg document.pdf ``` After calling `organize_files(\\"/my_dir\\")`, the structure should become: ``` /my_dir /txt file1.txt file2.txt /jpg image1.jpg image2.jpg /pdf document.pdf ``` Notes - Use the `glob.glob` function to find files in the given directory. - Create subdirectories named after the file extensions without the leading dot (e.g., \\"txt\\" for \\".txt\\"). - You may use the `os` module for directory and file operations. Hint Consider using `glob.glob()` with a pattern to fetch all files in the base directory and `os.makedirs()` to create subdirectories. Use `os.path.splitext()` to extract file extensions.","solution":"import os import glob import shutil def organize_files(base_dir: str) -> None: Organize files in the specified directory based on their file types. Parameters: - base_dir: str - The path to the directory to organize. Returns: - None if not os.path.isdir(base_dir): raise ValueError(f\\"Invalid directory: {base_dir}\\") files = glob.glob(os.path.join(base_dir, \'*\')) for file in files: if os.path.isfile(file) and not os.path.basename(file).startswith(\'.\'): ext = os.path.splitext(file)[1][1:] if ext: dest_dir = os.path.join(base_dir, ext) os.makedirs(dest_dir, exist_ok=True) shutil.move(file, os.path.join(dest_dir, os.path.basename(file)))"},{"question":"# Advanced Python Coding Challenge Problem Statement You are to implement a custom container class in Python that models a simplified version of a list, but with additional constraints and functionalities. Class Definition: Define a class `CustomList` that supports the following functionalities: 1. **Initialization**: - The class should be initialized with a list of integers. - Example: `c = CustomList([1, 2, 3, 4, 5])` 2. **Custom Addition**: - Implement a method `add(self, value: int) -> None` that adds a given integer value to each element in the list. - Example: If the list is `[1, 2, 3]` and you call `add(2)`, the list should now be `[3, 4, 5]`. 3. **Custom Multiplication**: - Implement a method `multiply(self, value: int) -> None` that multiplies each element in the list by a given integer value. - Example: If the list is `[1, 2, 3]` and you call `multiply(3)`, the list should now be `[3, 6, 9]`. 4. **Find Max**: - Implement a method `find_max(self) -> int` that returns the maximum value in the list. 5. **Iterator Protocol**: - Implement the iterator protocol so that elements in `CustomList` can be iterated using a for-loop. - Example: `for item in c: print(item)` should print each element in the list. Input and Output Formats: - **Input**: You don\'t need to handle input directly. This will be managed through the class initialization and method calls. - **Output**: The methods themselves will handle internal state changes and should not return outputs except `find_max` which returns an integer. Constraints: - The list at initialization will contain at most 1000 elements. - Each integer element in the list will be in the range -10^6 to 10^6. - The value input to `add` and `multiply` will also be an integer in the range -10^6 to 10^6. Example Usage: ```python c = CustomList([1, 2, 3, 4, 5]) c.add(2) print(c) # Prints: CustomList([3, 4, 5, 6, 7]) c.multiply(3) print(c) # Prints: CustomList([9, 12, 15, 18, 21]) print(c.find_max()) # Prints: 21 for item in c: print(item) # Prints: # 9 # 12 # 15 # 18 # 21 ``` Additional Requirements: - Ensure the class and its methods are well-documented with docstrings. - The class should have an appropriate `__str__` method to display the list elements. - Handle potential edge cases like empty initialization list gracefully.","solution":"class CustomList: def __init__(self, elements): Initializes the CustomList with a list of integers. self.elements = elements if elements is not None else [] def add(self, value): Adds the specified value to each element in the list. self.elements = [x + value for x in self.elements] def multiply(self, value): Multiplies each element in the list by the specified value. self.elements = [x * value for x in self.elements] def find_max(self): Returns the maximum value in the list. return max(self.elements) if self.elements else None def __iter__(self): Returns an iterator to allow for looping through the list. return iter(self.elements) def __str__(self): Returns a string representation of the CustomList. return f\\"CustomList({self.elements})\\""},{"question":"**Coding Assessment Question: Implementing and Testing a Custom Backward Function using PyTorch Prims IR** # Objective The goal of this task is to assess your understanding of PyTorch primitives and their application in defining custom operations and gradients. You will implement a custom backward function using PyTorch\'s Prims IR and validate it with a simple forward function. # Problem Statement You need to implement a custom backward function for a simple element-wise operation using the PyTorch Prims IR. Specifically, you will create forward and backward functions for the element-wise squared operation, `f(x) = x^2`. You will then test these functions using PyTorch\'s autograd functionality to ensure that they work correctly. # Requirements 1. **Forward Function**: - Define a forward function using Prims IR that performs the element-wise squared operation on a tensor. 2. **Backward Function**: - Define a custom backward function using Prims IR for computing the gradient of the forward operation. 3. **Testing**: - Test the forward and backward implementations using PyTorch\'s autograd by defining a simple test case. # Input and Output Specifications - **Input**: - A PyTorch tensor `x` of shape `(N, )`, where `N` is the number of elements. The tensor will require gradients. - **Output**: - The result of the forward operation as a PyTorch tensor. - The computed gradients for the tensor `x`. # Constraints 1. You must use the `prims` module for defining the forward and backward operations. 2. You must validate your implementation using PyTorch autograd and ensure that your custom backward function computes the gradients correctly. # Performance Requirements The implementation should be efficient and leverage the broadcasting and type promotion facilities provided by the Prims IR. Design your solution to handle tensors of varying sizes efficiently. # Function Signature ```python import torch from torch import prims def custom_forward(x): Computes the element-wise squared operation using Prims IR. Args: x (torch.Tensor): Input tensor requiring gradients. Returns: torch.Tensor: The squared tensor. # Implement the forward operation using Prims IR def custom_backward(grad_output, x): Computes the gradient for the squared operation using Prims IR. Args: grad_output (torch.Tensor): Tensor containing the gradient of the loss w.r.t the output. x (torch.Tensor): Input tensor. Returns: torch.Tensor: The gradient of the loss w.r.t the input tensor x. # Implement the backward operation using Prims IR def test_custom_functions(): Tests the custom forward and backward functions for correctness using PyTorch autograd. # Define a sample input tensor x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # Perform the custom forward operation y = custom_forward(x) # Perform a backward pass y.sum().backward() # Check if gradients match the expected values print(x.grad) # Example usage test_custom_functions() ``` **Note**: Make sure to implement the `custom_forward` and `custom_backward` functions correctly and ensure that the `test_custom_functions` method verifies the correctness of your backward function.","solution":"import torch from torch.autograd import Function class CustomSquared(Function): @staticmethod def forward(ctx, x): ctx.save_for_backward(x) return x ** 2 @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors grad_input = 2 * x * grad_output return grad_input def custom_forward(x): return CustomSquared.apply(x)"},{"question":"Coding Assessment Question # Problem Statement: In financial applications, precise decimal representation and arithmetic are crucial to avoid rounding issues that can lead to significant financial discrepancies. The standard floating-point arithmetic in Python is not suitable for such applications due to representation errors. Instead, Python provides the `decimal` module which can handle such exact decimal arithmetic. Your task is to implement a function using the `decimal` module to handle financial transactions. The function should perform the following operations: 1. **Initialize an account balance** to 0.0 using a decimal. 2. **Process a list of transactions** where each transaction is a tuple consisting of a string (\\"D\\" for deposit or \\"W\\" for withdrawal) and a decimal value. For each transaction, update the balance accordingly. 3. **Return the final balance** accurately using the decimal representation. # Input Format: - An integer `n` representing the number of transactions. - A list of `n` transactions where each transaction is a tuple of the form (operation:str, amount:decimal). # Output Format: - A decimal value representing the final account balance after processing all transactions. # Constraints: - The number of transactions `n` will not exceed 10^6. - The amount in each transaction will be a non-negative decimal value with up to 2 decimal places. - Ensure that your implementation uses the `decimal` module for all computations to maintain precision. # Example: ```python from decimal import Decimal def calculate_balance(n, transactions): # Your code here # Example Input: n = 3 transactions = [(\\"D\\", Decimal(\'100.50\')), (\\"W\\", Decimal(\'20.25\')), (\\"D\\", Decimal(\'40.75\'))] # Function Call print(calculate_balance(n, transactions)) # Expected Output: Decimal(\'121.00\') ``` # Notes: - Use the `decimal.Decimal` class for all transactions and balance computations. - Handle edge cases such as no transactions gracefully. - Ensure the function is efficient and can handle the upper limit of transactions within a reasonable time.","solution":"from decimal import Decimal def calculate_balance(n, transactions): Calculate the final balance after processing a list of transactions. Parameters: n (int): Number of transactions transactions (list of tuple): List where each tuple contains a string (\\"D\\" for deposit or \\"W\\" for withdrawal) and a decimal value representing the amount. Returns: Decimal: Final balance as a decimal value balance = Decimal(\'0.0\') for operation, amount in transactions: if operation == \\"D\\": balance += amount elif operation == \\"W\\": balance -= amount return balance"},{"question":"**Objective**: Demonstrate your ability to use functions from the `python310` package to serialize and deserialize data. **Problem Statement**: You are required to write two Python functions to serialize and deserialize data. The first function will serialize a list of mixed Python objects (integers, strings, floats, etc.) to a file, and the second function will read from the file and reconstruct the original list. You will use the `python310` package for marshalling. **Function 1: `serialize_objects(objects: list, filename: str, version: int) -> None`** * **Input:** * `objects` - A list of Python objects to be serialized. The list can contain integers, strings, floats, and other serializable Python objects. * `filename` - The name of the file to which the objects should be serialized. * `version` - The marshalling version to use (0, 1, or 2). * **Output:** * None. The objects should be written to the specified file. * **Constraints:** * The file should be opened in binary mode. * Proper error handling should be implemented to handle potential issues during serialization. **Function 2: `deserialize_objects(filename: str) -> list`** * **Input:** * `filename` - The name of the file from which the objects should be deserialized. * **Output:** * A list of Python objects that were read from the file. * **Constraints:** * The file should be opened in binary mode. * Proper error handling should be implemented to handle potential issues during deserialization. **Additional Notes:** 1. Numeric values should be serialized considering the least significant byte comes first. 2. On errors with file operations or marshalling functions, proper Python exceptions should be raised or handled gracefully. **Example Test Case:** ```python objects = [123, \\"hello\\", 45.67, {\\"key\\": \\"value\\"}] filename = \\"test.marshal\\" version = 2 # Serialize the objects serialize_objects(objects, filename, version) # Deserialize the objects result = deserialize_objects(filename) assert result == objects, \\"The deserialized objects do not match the original objects\\" ``` **Requirements:** 1. Implement the `serialize_objects` function that serializes the given list of objects to a file using the specified marshalling version. 2. Implement the `deserialize_objects` function that reads the list of objects from the file and reconstructs them. 3. Ensure your code handles errors gracefully and opens files in binary mode as required. You will be evaluated on the correctness, performance, and robustness of your implementation.","solution":"import marshal def serialize_objects(objects, filename, version): Serializes a list of Python objects to a file using the specified version of marshalling. Args: objects (list): The list of objects to serialize. filename (str): The name of the file to which the objects are serialized. version (int): The marshalling version to use (0, 1, or 2). Returns: None try: with open(filename, \'wb\') as file: marshal.dump(objects, file, version) except Exception as e: print(f\\"An error occurred while serializing objects: {e}\\") def deserialize_objects(filename): Deserializes a list of Python objects from a file. Args: filename (str): The name of the file from which the objects are deserialized. Returns: list: The list of deserialized objects. try: with open(filename, \'rb\') as file: return marshal.load(file) except Exception as e: print(f\\"An error occurred while deserializing objects: {e}\\") return None"},{"question":"# Question: Implementing a Structured Logging System You are tasked with implementing a structured logging system using the Unix \\"syslog\\" library in Python. This logging system should be capable of initializing the logger with specified options, logging messages at different priority levels, and managing the logging lifecycle. Requirements: 1. **Function `initialize_logger(ident:str, logoption:int, facility:int)`:** - Initializes the logger with the given identifier (`ident`), log option (`logoption`), and facility (`facility`). 2. **Function `log_message(priority:int, message:str)`:** - Logs a message with the specified priority level (`priority`). 3. **Function `set_priority_mask(mask:int)`:** - Sets the logging priority mask using the given mask. 4. **Function `close_logger()`:** - Closes and resets the logger. Constraints: - Follow the priority and facility constants as given in the documentation (`LOG_EMERG`, `LOG_ALERT`, `LOG_CRIT`, etc.). - Only use syslog functions (`syslog()`, `openlog()`, `closelog()`, `setlogmask()`) for interactions with the system logger. Example Usage: ```python initialize_logger(ident=\\"MyLogger\\", logoption=syslog.LOG_PID, facility=syslog.LOG_MAIL) log_message(priority=syslog.LOG_INFO, message=\\"Info message\\") log_message(priority=syslog.LOG_ERR, message=\\"Error message\\") set_priority_mask(mask=syslog.LOG_UPTO(syslog.LOG_WARNING)) log_message(priority=syslog.LOG_DEBUG, message=\\"Debug message\\") # This should not appear due to priority mask close_logger() ``` Expected Behavior: - The \\"MyLogger\\" identifier should be prepended to messages. - The process ID should be included in each log message. - Messages should be logged to the mail facility. - Messages with a priority lower than or equal to `LOG_WARNING` should be logged, while those like `LOG_DEBUG` (lower priority) should not. Implement the following functions: ```python def initialize_logger(ident: str, logoption: int, facility: int): pass def log_message(priority: int, message: str): pass def set_priority_mask(mask: int): pass def close_logger(): pass ``` Ensure your implementation follows the required functionality and correctly demonstrates the usage of the syslog module as documented.","solution":"import syslog def initialize_logger(ident: str, logoption: int, facility: int): Initializes the syslog logger with the specified identifier, log option, and facility. syslog.openlog(ident=ident, logoption=logoption, facility=facility) def log_message(priority: int, message: str): Logs a message with the specified priority level. syslog.syslog(priority, message) def set_priority_mask(mask: int): Sets the logging priority mask. syslog.setlogmask(mask) def close_logger(): Closes the syslog logger. syslog.closelog()"},{"question":"# Question You are given a directory structure and a requirement to find the top `n` largest files within it, but you need to ignore certain file types based on their extensions. Using the `pathlib` module, write a Python function that performs the following tasks: 1. Accepts a directory path (as a string), a list of file extensions to ignore (as a list of strings), and an integer `n`. 2. Recursively searches the directory and subdirectories to list all files, except those with extensions specified in the ignore list. 3. Finds the top `n` largest files based on their size and returns their paths along with their sizes. Function Signature ```python from pathlib import Path from typing import List, Tuple def find_largest_files(directory: str, ignore_exts: List[str], n: int) -> List[Tuple[Path, int]]: ``` Input - `directory` (str): A string representing the path to the directory to search. - `ignore_exts` (List[str]): A list of file extensions to ignore (e.g., `[\'.tmp\', \'.log\']`). - `n` (int): The number of largest files to return. Output - List[Tuple[Path, int]]: A list of tuples, where each tuple contains a `Path` object representing the file path and an integer representing its size (in bytes). The list should be sorted by file size in descending order and contain up to `n` files. Constraints 1. If fewer than `n` files are found, return all found files. 2. The function should handle non-existent directories gracefully by returning an empty list. 3. Handle permission errors gracefully by skipping the files or directories that cause such errors. # Example ```python from pathlib import Path from typing import List, Tuple def find_largest_files(directory: str, ignore_exts: List[str], n: int) -> List[Tuple[Path, int]]: directory_path = Path(directory) if not directory_path.exists() or not directory_path.is_dir(): return [] files = [] for file in directory_path.rglob(\'*\'): if file.suffix not in ignore_exts and file.is_file(): try: size = file.stat().st_size files.append((file, size)) except PermissionError: continue files.sort(key=lambda x: x[1], reverse=True) return files[:n] # Example usage: directory = \'/my_directory\' ignore_exts = [\'.tmp\', \'.log\'] n = 5 print(find_largest_files(directory, ignore_exts, n)) ``` # Explanation In this example: - The function is called with the path to a directory, an ignore list of file extensions (`[\'.tmp\', \'.log\']`), and the number of files to return (`n=5`). - The function searches all files recursively in the given directory, filters out the files with specified extensions, and lists the sizes of the remaining files. - It returns the `n` largest files from the list, sorted in descending order of file size.","solution":"from pathlib import Path from typing import List, Tuple def find_largest_files(directory: str, ignore_exts: List[str], n: int) -> List[Tuple[Path, int]]: directory_path = Path(directory) if not directory_path.exists() or not directory_path.is_dir(): return [] files = [] for file in directory_path.rglob(\'*\'): if file.suffix not in ignore_exts and file.is_file(): try: size = file.stat().st_size files.append((file, size)) except PermissionError: continue files.sort(key=lambda x: x[1], reverse=True) return files[:n]"},{"question":"# Question: Understanding and Implementing PyTorch Utility Functions You are required to demonstrate your understanding of fundamental PyTorch operations and utility functions by implementing a set of functions analogous to those in the `torch.ao.ns.fx.utils` module. These functions will compute: - Signal to Quantization Noise Ratio (SQNR) - Normalized L2 error - Cosine similarity Requirements 1. **Signal to Quantization Noise Ratio (SQNR)**: - **Function**: `compute_sqnr(x: torch.Tensor, y: torch.Tensor) -> float` - **Description**: Compute the Signal to Quantization Noise Ratio between two tensors, `x` and `y`. - **Expected Output**: A single float value representing the SQNR. 2. **Normalized L2 error**: - **Function**: `compute_normalized_l2_error(x: torch.Tensor, y: torch.Tensor) -> float` - **Description**: Compute the normalized L2 error between tensors `x` and `y`, i.e., the L2 (Euclidean) distance divided by the norm of `x`. - **Expected Output**: A single float value representing the normalized L2 error. 3. **Cosine Similarity**: - **Function**: `compute_cosine_similarity(x: torch.Tensor, y: torch.Tensor) -> float` - **Description**: Compute the cosine similarity between two tensors `x` and `y`. - **Expected Output**: A single float value representing the cosine similarity. Input and Output Formats - **Input**: Two input tensors will be provided to each function. The tensors `x` and `y` will always have the same shape and will contain floating-point values. For simplicity, you can assume that the tensors will be 1-dimensional vectors. - **Output**: A single floating-point value, specific to each function\'s requirement. Constraints - Use PyTorch operations and functionalities to implement the utility functions. - The implementation should be efficient and adhere to best practices in PyTorch programming. Example: ```python import torch def compute_sqnr(x: torch.Tensor, y: torch.Tensor) -> float: # Your implementation here pass def compute_normalized_l2_error(x: torch.Tensor, y: torch.Tensor) -> float: # Your implementation here pass def compute_cosine_similarity(x: torch.Tensor, y: torch.Tensor) -> float: # Your implementation here pass # Example Usage x = torch.tensor([1.0, 2.0, 3.0]) y = torch.tensor([1.0, 2.1, 2.9]) assert isinstance(compute_sqnr(x, y), float) assert isinstance(compute_normalized_l2_error(x, y), float) assert isinstance(compute_cosine_similarity(x, y), float) ``` Provide the implementation for these functions and test their correctness using some sample data.","solution":"import torch def compute_sqnr(x: torch.Tensor, y: torch.Tensor) -> float: Compute the Signal to Quantization Noise Ratio (SQNR) between two tensors. Parameters: x (torch.Tensor): The signal tensor. y (torch.Tensor): The quantized signal tensor. Returns: float: The computed SQNR. signal_power = torch.sum(x ** 2) noise_power = torch.sum((x - y) ** 2) sqnr = 10 * torch.log10(signal_power / noise_power) return sqnr.item() def compute_normalized_l2_error(x: torch.Tensor, y: torch.Tensor) -> float: Compute the normalized L2 error between two tensors. Parameters: x (torch.Tensor): The original tensor. y (torch.Tensor): The approximated tensor. Returns: float: The normalized L2 error. l2_error = torch.norm(x - y) norm_x = torch.norm(x) normalized_l2_error = l2_error / norm_x return normalized_l2_error.item() def compute_cosine_similarity(x: torch.Tensor, y: torch.Tensor) -> float: Compute the cosine similarity between two tensors. Parameters: x (torch.Tensor): The first tensor. y (torch.Tensor): The second tensor. Returns: float: The cosine similarity. cos_sim = torch.nn.functional.cosine_similarity(x, y, dim=0) return cos_sim.item()"},{"question":"You are tasked with writing a function, `extract_text_nodes`, using the `xml.dom` module in Python. This function should parse an XML string and return all the text contained within the text nodes of the document, concatenated together. If an XML element has child elements, only the text directly contained within the element should be concatenated, not the text within its child elements. Here\'s the outline of what you need to do: 1. Parse the provided XML string into a DOM document. 2. Traverse the elements in the DOM. 3. Extract and concatenate the text within each text node, ignoring text in child elements. Implement the following function: ```python from xml.dom.minidom import parseString def extract_text_nodes(xml_string: str) -> str: Given a string containing XML data, parse it into a DOM document and extract all the text directly contained within text nodes. Args: xml_string (str): A string containing XML data Returns: str: All text directly within text nodes concatenated together pass ``` # Input: - `xml_string`: A string representing an XML document. # Output: - A string containing the concatenated text directly from text nodes. # Constraints: - You may not use any external libraries except for `xml.dom.minidom`. - The provided XML will always be well-formed. # Example: ```python xml_string = <?xml version=\\"1.0\\"?> <root> <greeting>Hello</greeting> <greeting>World</greeting> <nested> <element>Ignore this text</element> But not this text </nested> </root> assert extract_text_nodes(xml_string) == \\"HelloWorldBut not this text\\" ``` # Explanation: The function should correctly parse the XML content and concatenate text from text nodes while ignoring text within child elements\' nodes. The result for the given example should be \\"HelloWorldBut not this text\\".","solution":"from xml.dom.minidom import parseString def extract_text_nodes(xml_string: str) -> str: Given a string containing XML data, parse it into a DOM document and extract all the text directly contained within text nodes. Args: xml_string (str): A string containing XML data Returns: str: All text directly within text nodes concatenated together dom = parseString(xml_string) result = [] def traverse(node): for child in node.childNodes: if child.nodeType == child.TEXT_NODE: result.append(child.nodeValue.strip()) elif child.nodeType == child.ELEMENT_NODE: # Recursively traverse element nodes to further inspect their children for subchild in child.childNodes: if subchild.nodeType == subchild.TEXT_NODE: result.append(subchild.nodeValue.strip()) traverse(dom.documentElement) # Start traversing from the root element return \\"\\".join(result)"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of generating and serving documentation using Python\'s `pydoc` module. You will write a Python script that generates HTML documentation for a specified module and serves it through a local HTTP server. # Problem Statement You are required to implement a function called `generate_and_serve_docs`. This function takes a single argument `module_name` as a string, which is the name of an existing Python module, generates its HTML documentation using the `pydoc` module, and serves this documentation using an HTTP server. Additionally, the function should start the server and open a web browser to view the generated documentation. # Requirements 1. **Input**: - `module_name` (str): The name of the Python module for which the documentation should be generated. 2. **Functionality**: - Use the `pydoc` module to generate HTML documentation for the specified module. - Save the HTML documentation to a file named `module_name.html`. - Start an HTTP server using `pydoc` to serve the generated HTML file. - Open the web browser automatically to the generated documentation page. 3. **Constraints**: - Assume that the provided `module_name` is a valid and importable module in the Python environment. - The server should run on `localhost` and an arbitrary unused port. # Example Suppose you want to generate and serve documentation for the `math` module. Your function call will look like this: ```python generate_and_serve_docs(\\"math\\") ``` # Expected Output - An HTML file named `math.html` should be created, containing the documentation for the `math` module. - An HTTP server should start, and your default web browser should open to display the documentation. # Implementation ```python import pydoc import webbrowser from http.server import HTTPServer, SimpleHTTPRequestHandler import os def generate_and_serve_docs(module_name): # Generate HTML documentation pydoc.writedoc(module_name) # Rename generated HTML file from <module>.html to <module_name>.html os.rename(module_name + \\".html\\", module_name + \\".generated.html\\") # Define a simple HTTP server class CustomHTTPRequestHandler(SimpleHTTPRequestHandler): def log_message(self, format, *args): return handler = CustomHTTPRequestHandler httpd = HTTPServer((\'localhost\', 0), handler) port = httpd.server_port # Open the web browser with the local server address url = f\\"http://localhost:{port}/{module_name}.generated.html\\" webbrowser.open(url) # Serve the documentation print(f\\"Serving {module_name} documentation at {url}\\") httpd.serve_forever() # Example usage # generate_and_serve_docs(\\"math\\") ``` # Notes - Ensure proper handling and cleanup, such as removing the generated HTML file after serving the documentation. - For testing, you may need to adjust the example module name to one that is available in your environment.","solution":"import pydoc import webbrowser from http.server import HTTPServer, SimpleHTTPRequestHandler import os def generate_and_serve_docs(module_name): # Generate HTML documentation pydoc.writedoc(module_name) # Rename generated HTML file to avoid name conflicts original_html_file = f\\"{module_name}.html\\" generated_html_file = f\\"{module_name}.generated.html\\" os.rename(original_html_file, generated_html_file) # Define a simple HTTP server class CustomHTTPRequestHandler(SimpleHTTPRequestHandler): def log_message(self, format, *args): return handler = CustomHTTPRequestHandler # Bind to an arbitrary unused port httpd = HTTPServer((\'localhost\', 0), handler) port = httpd.server_port # Open the web browser with the local server address url = f\\"http://localhost:{port}/{generated_html_file}\\" webbrowser.open(url) # Serve the documentation print(f\\"Serving {module_name} documentation at {url}\\") httpd.serve_forever()"},{"question":"You have been tasked with implementing a function that will manage a list of student records in a university. Each student record consists of the student\'s name and their GPA. The list should be maintained in a sorted order based on the GPAs. You should also provide utilities to find students with GPAs within particular ranges efficiently. Your task is to implement the `StudentRecords` class with the following methods: 1. `add_record(self, name: str, gpa: float) -> None`: Adds a new student record while maintaining the list sorted by GPA. 2. `find_students_in_gpa_range(self, low: float, high: float) -> List[str]`: Finds all students with GPAs in the range `[low, high]` inclusive and returns their names sorted by GPA. # Constraints - Each name is a string with no spaces. - The GPA is a float and lies between 0.0 to 4.0 inclusive. - The list of records will have at most 1000 entries. - The function should efficiently manage and search the records. # Example Usage ```python records = StudentRecords() records.add_record(\\"Alice\\", 3.5) records.add_record(\\"Bob\\", 3.7) records.add_record(\\"Charlie\\", 3.1) records.add_record(\\"Diana\\", 3.8) assert records.find_students_in_gpa_range(3.0, 3.5) == [\\"Charlie\\", \\"Alice\\"] assert records.find_students_in_gpa_range(3.5, 4.0) == [\\"Alice\\", \\"Bob\\", \\"Diana\\"] assert records.find_students_in_gpa_range(2.0, 3.0) == [] records.add_record(\\"Eve\\", 3.2) assert records.find_students_in_gpa_range(3.0, 3.5) == [\\"Charlie\\", \\"Eve\\", \\"Alice\\"] ``` # Implementation You should use the `bisect` module to handle the sorted insertion and searching efficiently.","solution":"from bisect import bisect_left, bisect_right from typing import List class StudentRecords: def __init__(self): self.records = [] def add_record(self, name: str, gpa: float) -> None: # Maintain pair as (gpa, name) for sorted ordering record = (gpa, name) idx = bisect_left(self.records, record) self.records.insert(idx, record) def find_students_in_gpa_range(self, low: float, high: float) -> List[str]: low_record = (low, \\"\\") high_record = (high, \\"xff\\") # Use the highest possible character as a boundary low_idx = bisect_left(self.records, low_record) high_idx = bisect_right(self.records, high_record) return [name for gpa, name in self.records[low_idx:high_idx]]"},{"question":"# Coding Assessment Objective To assess your understanding of integrating seaborn\'s `rugplot` with other plots, customizing plot aesthetics, and working with larger datasets. Problem Statement You are provided with two datasets: `tips` and `diamonds`. Your task is to create a composite plot for each dataset using seaborn and matplotlib. The plot should include: 1. A scatter plot. 2. A rug plot along both axes. 3. Custom aesthetics such as hue, height, and transparency. Detailed Requirements 1. **Tips Dataset Visualization:** - Load the `tips` dataset using `seaborn.load_dataset()`. - Create a scatter plot with `total_bill` on the x-axis and `tip` on the y-axis. - Add a rug plot along both axes (`total_bill` on x-axis and `tip` on y-axis). - Use hue mapping for the `day` variable. - Adjust the height of the rug to 0.1. 2. **Diamonds Dataset Visualization:** - Load the `diamonds` dataset using `seaborn.load_dataset()`. - Create a scatter plot with `carat` on the x-axis and `price` on the y-axis. - Add a rug plot along both axes (`carat` on x-axis and `price` on y-axis). - Use hue mapping for the `color` variable. - Adjust the height of the rug to 0.05. - Ensure the rug lines are thin (line width of 1) and use a high transparency (alpha of 0.3). Constraints - Your code should be efficient and clear. - Use appropriate seaborn and matplotlib methods to achieve the desired plot aesthetics. - Ensure the plots are correctly labeled and visually informative. Input None (Datasets are loaded within the function). Output The function should display two plots as per the detailed requirements (`plt.show()` statements). Function Signature ```python def visualize_datasets(): import seaborn as sns import matplotlib.pyplot as plt # Load datasets tips = sns.load_dataset(\\"tips\\") diamonds = sns.load_dataset(\\"diamonds\\") # Tips Dataset Visualization plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\") sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", height=0.1) plt.title(\\"Tips Dataset Visualization\\") # Diamonds Dataset Visualization plt.figure(figsize=(10, 6)) sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", hue=\\"color\\") sns.rugplot(data=diamonds, x=\\"carat\\", y=\\"price\\", height=0.05, lw=1, alpha=0.3) plt.title(\\"Diamonds Dataset Visualization\\") # Display plots plt.show() ```","solution":"def visualize_datasets(): import seaborn as sns import matplotlib.pyplot as plt # Load datasets tips = sns.load_dataset(\\"tips\\") diamonds = sns.load_dataset(\\"diamonds\\") # Tips Dataset Visualization plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\") sns.rugplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", height=0.1) plt.title(\\"Tips Dataset Visualization\\") # Diamonds Dataset Visualization plt.figure(figsize=(10, 6)) sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", hue=\\"color\\") sns.rugplot(data=diamonds, x=\\"carat\\", y=\\"price\\", height=0.05, lw=1, alpha=0.3) plt.title(\\"Diamonds Dataset Visualization\\") # Display plots plt.show()"},{"question":"# Python Initialization Challenge **Objective:** Complete a Python function that sets up a custom Python environment for a specific application. The goal of this task is to solidify your understanding of Python initialization using the `PyConfig` and `PyPreConfig` structures. **Problem Statement:** You are required to write a function in Python that simulates the initialization flow using mockup structures similar to those described in the documentation. The function should prepare a Python environment with specific configurations and handle possible exceptions. **Requirements:** 1. **Function Name:** ```python def init_custom_python(program_name: str, module_paths: list, utf8_mode: int, memory_allocator: int) -> str: ``` 2. **Function Arguments:** - `program_name`: A string that represents the program name to be used for initialization. - `module_paths`: A list of strings where each string is a path to custom modules that should be added to `sys.path`. - `utf8_mode`: An integer that sets the UTF-8 mode (0 or 1). - `memory_allocator`: An integer that sets the memory allocator mode (values 0 to 6 as described in `PyPreConfig.allocator`). 3. **Behavior:** - The function should create mock-up configurations mimicking `PyConfig` and `PyPreConfig` structures. - It should simulate the setting of the program name, module search paths, and UTF-8 mode. - The function should simulate initialization using the provided configurations. - It should handle any mock-up `PyStatus` exceptions gracefully and return appropriate status messages. 4. **Output:** - Return a string message \\"Initialization Successful\\" if the configuration and initialization steps are completed successfully. - Return an error message if any of the mock-up `PyStatus` exceptions are raised during the process. 5. **Constraints:** - Assume `module_paths` contains valid, non-NULL strings. - `utf8_mode` and `memory_allocator` should only take valid integer values within their defined ranges. **Example:** ```python result = init_custom_python(\\"my_program\\", [\\"/path/to/module1\\", \\"/path/to/module2\\"], 1, 5) print(result) # Should print \\"Initialization Successful\\" or an appropriate error message. ``` **Hint:** You can assume the presence of helper functions that mimic the functions found in the documentation, such as `PyConfig_SetBytesString()`, `PyWideStringList_Append()`, and checks for `PyStatus_Exception()`. **Note:** This task requires careful handling of the input parameters to simulate initialization accurately and error-free.","solution":"def init_custom_python(program_name: str, module_paths: list, utf8_mode: int, memory_allocator: int) -> str: Simulates the initialization of a custom Python environment. Args: - program_name (str): The program name to be used for initialization. - module_paths (list): List of strings representing paths to custom modules. - utf8_mode (int): Integer setting for UTF-8 mode (0 or 1). - memory_allocator (int): Integer setting for memory allocator mode (values 0 to 6). Returns: - str: \\"Initialization Successful\\" if the initialization is successful, otherwise an error message. try: # Mock-up PyConfig and PyPreConfig structures PyConfig = { \'program_name\': program_name, \'module_search_paths\': module_paths, \'utf8_mode\': utf8_mode } PyPreConfig = { \'allocator\': memory_allocator } # Simulate setting the program name PyConfig[\'program_name\'] = program_name # Simulate setting the module search paths PyConfig[\'module_search_paths\'] = module_paths # Pretend to append each path # Simulate setting UTF-8 mode PyConfig[\'utf8_mode\'] = utf8_mode # Simulate setting memory allocator PyPreConfig[\'allocator\'] = memory_allocator # Simulate initialization check for exceptions (mock-up) if utf8_mode not in [0, 1]: raise ValueError(\\"Invalid UTF-8 mode value\\") if memory_allocator not in range(0, 7): raise ValueError(\\"Invalid memory allocator value\\") return \\"Initialization Successful\\" except Exception as e: return f\\"Initialization Failed: {str(e)}\\""},{"question":"Suppose you are working on a scientific computing problem where reproducibility and performance both are critical. In this context, we want to ensure that uninitialized memory in tensors is properly managed for reproducibility without significantly impacting the performance. Implement a function `create_deterministic_tensor` that: 1. Efficiently initializes an uninitialized tensor, filling it with a specified value based on the datatype. 2. Makes sure to use deterministic algorithms by utilizing `torch.use_deterministic_algorithms()`. 3. Allows toggling the `fill_uninitialized_memory` setting based on the need for performance or complete reproducibility. # Function Signature ```python def create_deterministic_tensor( shape: tuple, dtype: torch.dtype, fill_value: float, fill_uninitialized_memory: bool = True ) -> torch.Tensor: Initializes a tensor of the given shape and dtype, ensuring determinism in operations and optionally filling uninitialized memory with a specified fill_value. Parameters: - shape (tuple): The shape of the tensor to be created. - dtype (torch.dtype): The desired data type of the tensor. - fill_value (float): The value to fill the uninitialized memory with. - fill_uninitialized_memory (bool): Whether uninitialized memory should be filled with the specified fill_value (defaults to True). Returns: - torch.Tensor: A tensor of the specified shape and dtype. ``` # Example Usage ```python shape = (3, 3) dtype = torch.float32 fill_value = 1.0 tensor = create_deterministic_tensor(shape, dtype, fill_value, fill_uninitialized_memory=True) print(tensor) ``` # Constraints - The function should support basic PyTorch datatypes like `torch.float32`, `torch.int32`, etc. - Handle edge cases gracefully (e.g., zero-size tensors). - Do not use any random number generation. # Performance Requirements - Ensure the code is performant and does not introduce unnecessary overhead when the `fill_uninitialized_memory` is set to `False`. # Additional Information - You can refer to the `torch.utils.deterministic` module documentation for managing deterministic behavior in PyTorch. - Utilize PyTorch functionalities under `torch.empty`, `torch.fill_`, and `torch.use_deterministic_algorithms()` wherever necessary.","solution":"import torch def create_deterministic_tensor(shape: tuple, dtype: torch.dtype, fill_value: float, fill_uninitialized_memory: bool = True) -> torch.Tensor: Initializes a tensor of the given shape and dtype, ensuring determinism in operations and optionally filling uninitialized memory with a specified fill_value. Parameters: - shape (tuple): The shape of the tensor to be created. - dtype (torch.dtype): The desired data type of the tensor. - fill_value (float): The value to fill the uninitialized memory with. - fill_uninitialized_memory (bool): Whether uninitialized memory should be filled with the specified fill_value (defaults to True). Returns: - torch.Tensor: A tensor of the specified shape and dtype. torch.use_deterministic_algorithms(True) tensor = torch.empty(shape, dtype=dtype) if fill_uninitialized_memory: tensor.fill_(fill_value) return tensor"},{"question":"# PyTorch Tensor Operations Assessment Objective: The objective of this exercise is to test your understanding of creating and manipulating tensors using PyTorch. Problem Statement: You are tasked to write functions that perform several operations on tensors and to compute specific results. Below are the specifications for the tasks: 1. **Create Tensor**: - Write a function `create_tensor` that takes in a Python list of lists and converts it to a 2-dimensional torch tensor of type `torch.float32`. ```python def create_tensor(data: list) -> torch.Tensor: Create a 2D Tensor from a list of lists. Args: data (list): A 2D list containing numeric values. Returns: torch.Tensor: A tensor with dtype torch.float32. pass ``` 2. **Tensor Operations**: - Write a function `tensor_operations` that accepts a tensor and performs the following operations on it: 1. Compute the element-wise square of the tensor. 2. Sum all elements of the squared tensor. 3. Compute the gradient of the sum with respect to the original tensor. 4. Return the squared tensor, the sum, and the gradient. ```python def tensor_operations(tensor: torch.Tensor) -> tuple: Perform operations on a tensor. Args: tensor (torch.Tensor): A tensor on which operations will be performed. Returns: tuple: A tuple containing the squared tensor, the sum of elements, and gradient. pass ``` 3. **Tensor Indexing and Slicing**: - Write a function `tensor_indexing` that takes in a tensor and returns: 1. The element located at the index (1, 1). 2. A tensor where the element at index (0, 1) has been set to `10`. 3. A tensor representing the first row. ```python def tensor_indexing(tensor: torch.Tensor) -> tuple: Index and slice the tensor. Args: tensor (torch.Tensor): A 2D tensor. Returns: tuple: - Element at index (1, 1). - Tensor with modified value at index (0, 1). - First row of the tensor. pass ``` 4. **Tensor Properties**: - Write a function `tensor_properties` that accepts a tensor and returns its shape, the number of elements, its data type, and whether it requires a gradient. ```python def tensor_properties(tensor: torch.Tensor) -> dict: Return properties of the tensor. Args: tensor (torch.Tensor): A tensor whose properties will be returned. Returns: dict: A dictionary containing the tensor\'s shape, number of elements, dtype, and requires_grad status. pass ``` Constraints: - You can assume the input tensor for `tensor_operations`, `tensor_indexing`, and `tensor_properties` will always be a non-empty 2D tensor. - The function `create_tensor` should handle any 2D list containing numeric values (integers or floats). Example: ```python # Example inputs and outputs: data = [[1, 2, 3], [4, 5, 6]] tensor = create_tensor(data) # tensor(tensor([[ 1., 2., 3.], # [ 4., 5., 6.]], dtype=torch.float32)) sq_tensor, sum_elements, grad = tensor_operations(tensor) # sq_tensor => tensor([[ 1., 4., 9.], # [ 16., 25., 36.]], dtype=torch.float32) # sum_elements => tensor(91., dtype=torch.float32) # grad => tensor([[ 2., 4., 6.], # [ 8., 10., 12.]]) elem, modified_tensor, first_row = tensor_indexing(tensor) # elem => tensor(5., dtype=torch.float32) # modified_tensor => tensor([[ 1., 10., 3.], # [ 4., 5., 6.]], dtype=torch.float32) # first_row => tensor([ 1., 2., 3.], dtype=torch.float32) props = tensor_properties(tensor) # props => {\'shape\': (2, 3), \'num_elements\': 6, \'dtype\': torch.float32, \'requires_grad\': False} ``` Make sure to test your functions thoroughly to verify their correctness.","solution":"import torch def create_tensor(data: list) -> torch.Tensor: Create a 2D Tensor from a list of lists. Args: data (list): A 2D list containing numeric values. Returns: torch.Tensor: A tensor with dtype torch.float32. return torch.tensor(data, dtype=torch.float32) def tensor_operations(tensor: torch.Tensor) -> tuple: Perform operations on a tensor. Args: tensor (torch.Tensor): A tensor on which operations will be performed. Returns: tuple: A tuple containing the squared tensor, the sum of elements, and gradient. tensor.requires_grad_(True) squared_tensor = tensor ** 2 sum_elements = squared_tensor.sum() sum_elements.backward() gradient = tensor.grad return squared_tensor, sum_elements, gradient def tensor_indexing(tensor: torch.Tensor) -> tuple: Index and slice the tensor. Args: tensor (torch.Tensor): A 2D tensor. Returns: tuple: - Element at index (1, 1). - Tensor with modified value at index (0, 1). - First row of the tensor. element = tensor[1, 1].item() modified_tensor = tensor.clone() modified_tensor[0, 1] = 10 first_row = tensor[0, :] return element, modified_tensor, first_row def tensor_properties(tensor: torch.Tensor) -> dict: Return properties of the tensor. Args: tensor (torch.Tensor): A tensor whose properties will be returned. Returns: dict: A dictionary containing the tensor\'s shape, number of elements, dtype, and requires_grad status. return { \'shape\': tuple(tensor.shape), \'num_elements\': tensor.numel(), \'dtype\': tensor.dtype, \'requires_grad\': tensor.requires_grad }"},{"question":"**Coding Assessment Question:** **Objective:** Your task is to create a Python script that extracts specific members from a given tar archive based on defined criteria, using proper extraction filters to ensure security. **Scenario:** You are provided with a tar archive that contains various files and directories. You need to fulfill the following tasks: 1. Extract all Python files from the provided tar archive to a specified directory. 2. Ensure that the extraction process adheres to security measures by using the data extraction filter. **Requirements:** 1. Implement a function `extract_python_files` with the following signature: ```python def extract_python_files(tar_path: str, extract_dir: str) -> None: Extracts all Python files from the given tar archive to the specified directory using the \'data\' extraction filter. Parameters: tar_path (str): The path to the tar archive. extract_dir (str): The directory to extract the files to. ``` 2. The function should: - Use the `tarfile` module to handle the tar archive. - Identify and extract only the `.py` files from the archive. - Ensure that extraction is done using the \'data\' filter to mitigate any potential security risks. - Create `extract_dir` if it does not exist. 3. Implement proper error handling to manage scenarios such as: - Invalid tar file. - Extraction errors. **Constraints:** - The tar archive file may be compressed using any supported compression (gzip, bz2, lzma). - You must not use any external libraries other than the standard Python library. **Example Usage:** ```python extract_python_files(\'sample.tar.gz\', \'/path/to/extract/\') ``` This function call should extract all Python files from `sample.tar.gz` to the directory `/path/to/extract/`. **Notes:** - Ensure that you use the provided `tarfile` module\'s functionality for extraction with the specified filter. **Submission:** Submit the `extract_python_files` function implementation. Ensure that your code is well-documented and follows Python best practices.","solution":"import tarfile import os def extract_python_files(tar_path: str, extract_dir: str) -> None: Extracts all Python files from the given tar archive to the specified directory using the \'data\' extraction filter. Parameters: tar_path (str): The path to the tar archive. extract_dir (str): The directory to extract the files to. try: if not os.path.exists(extract_dir): os.makedirs(extract_dir) with tarfile.open(tar_path, \'r\') as tar: for member in tar.getmembers(): if member.isfile() and member.name.endswith(\'.py\'): member_path = os.path.join(extract_dir, member.name) member_dir = os.path.dirname(member_path) if not os.path.exists(member_dir): os.makedirs(member_dir) tar.extract(member, path=extract_dir, filter=\'data\') except (tarfile.TarError, IOError) as e: print(f\\"Error extracting files from {tar_path}: {e}\\")"},{"question":"# Custom Pickling with `copyreg` Given the following class definition of a `Point` and a custom class `Circle`, you are required to define custom pickling functions and register them using Python\'s `copyreg` module. ```python class Point: def __init__(self, x, y): self.x = x self.y = y class Circle: def __init__(self, radius, center): self.radius = radius self.center = center ``` **Objective:** 1. Define a custom pickling function `pickle_point` for the `Point` class that returns a tuple containing the class and a tuple of the `x` and `y` attributes. 2. Define a custom pickling function `pickle_circle` for the `Circle` class that returns a tuple containing the class and a tuple of the `radius` attribute and the `Point` object representing the `center`. 3. Register these custom pickling functions using the `copyreg.pickle` function. 4. Demonstrate the use of these registered pickling functions by: - Creating an instance of `Circle`. - Copying the instance using `copy.copy`. - Pickling and then unpickling the instance using the `pickle` module. **Constraints:** - You must use the `copyreg` module to register the custom pickling functions. - Do not modify the class definitions provided. **Expected Input/Output:** Your solution should demonstrate the following: 1. Successful registering of the custom pickling functions. 2. Correct copying of the `Circle` instance, with the custom pickling function being utilized (with output illustrating the pickling process). 3. Correct pickling and unpickling of the `Circle` instance, ensuring the state of the instance is maintained. ```python # Example usage circle = Circle(5, Point(2, 3)) import copy copied_circle = copy.copy(circle) # Should use the custom pickling function for Circle import pickle serialized_circle = pickle.dumps(circle) # Should use the custom pickling function for Circle unserialized_circle = pickle.loads(serialized_circle) # Should correctly reconstruct the Circle instance # Ensure the copied and unserialized instances are correct assert copied_circle.radius == circle.radius assert copied_circle.center.x == circle.center.x assert copied_circle.center.y == circle.center.y assert unserialized_circle.radius == circle.radius assert unserialized_circle.center.x == circle.center.x assert unserialized_circle.center.y == circle.center.y ``` Implement the required functionality and demonstrate the use of the `copyreg` module as specified.","solution":"import copyreg import pickle import copy class Point: def __init__(self, x, y): self.x = x self.y = y class Circle: def __init__(self, radius, center): self.radius = radius self.center = center def pickle_point(point): return Point, (point.x, point.y) def pickle_circle(circle): return Circle, (circle.radius, circle.center) # Register the custom pickling functions copyreg.pickle(Point, pickle_point) copyreg.pickle(Circle, pickle_circle) # Example usage circle = Circle(5, Point(2, 3)) copied_circle = copy.copy(circle) # Should use the custom pickling function for Circle serialized_circle = pickle.dumps(circle) # Should use the custom pickling function for Circle unserialized_circle = pickle.loads(serialized_circle) # Should correctly reconstruct the Circle instance # Output for demonstration purposes print(f\\"Original Circle: Radius = {circle.radius}, Center = ({circle.center.x}, {circle.center.y})\\") print(f\\"Copied Circle: Radius = {copied_circle.radius}, Center = ({copied_circle.center.x}, {copied_circle.center.y})\\") print(f\\"Unserialized Circle: Radius = {unserialized_circle.radius}, Center = ({unserialized_circle.center.x}, {unserialized_circle.center.y})\\")"},{"question":"**Problem Statement**: You are tasked with writing a robust Python function that attempts to read a file and handle various potential errors effectively using the `errno` module. **Function Signature**: ```python def read_file_with_error_handling(file_path: str) -> str: pass ``` **Description**: 1. Implement a function `read_file_with_error_handling` that takes the path to a file as input (`file_path`). 2. The function should attempt to read the content of the file specified by `file_path`. 3. If the file is read successfully, the function should return the content of the file as a string. 4. If an error occurs during the file reading, your function should handle it, and: - Return a string with the format: \\"Error [error_code]: [error_message]\\", where `[error_code]` is the numeric value of the error, and `[error_message]` is the corresponding human-readable error message obtained using `os.strerror()`. 5. The function should use the `errno` module and appropriately handle exceptions like `FileNotFoundError`, `PermissionError`, `IsADirectoryError`, and any other potential errors documented in the `errno` module. **Example**: ```python # Assume the file \'non_existent.txt\' does not exist print(read_file_with_error_handling(\'non_existent.txt\')) # Output should be \\"Error 2: No such file or directory\\" # Assume the file \'restricted_access.txt\' exists, but you don\'t have permission to read it print(read_file_with_error_handling(\'restricted_access.txt\')) # Output should be \\"Error 13: Permission denied\\" ``` **Constraints**: - Do not use any libraries/modules other than `os` and `errno`. - Ensure your function has appropriate error handling for all cases mentioned.","solution":"import os import errno def read_file_with_error_handling(file_path: str) -> str: try: with open(file_path, \'r\') as file: return file.read() except FileNotFoundError: return f\\"Error {errno.ENOENT}: {os.strerror(errno.ENOENT)}\\" except PermissionError: return f\\"Error {errno.EACCES}: {os.strerror(errno.EACCES)}\\" except IsADirectoryError: return f\\"Error {errno.EISDIR}: {os.strerror(errno.EISDIR)}\\" except Exception as e: error_code = e.errno if hasattr(e, \'errno\') else errno.EIO return f\\"Error {error_code}: {os.strerror(error_code)}\\""},{"question":"# Custom Event Loop Policy and Child Watcher Implementation You are tasked with creating a custom event loop policy for a hypothetical operating system called \\"MyOS\\". This operating system requires a unique event loop handling mechanism and specific child watcher requirements. Requirements: 1. **Custom Event Loop Policy**: Create a custom event loop policy named `MyOSEventLoopPolicy` that subclasses `DefaultEventLoopPolicy`. - Override the `new_event_loop()` method to print \\"Creating new event loop for MyOS\\" whenever a new event loop is being created. - Override the `get_event_loop()` method to print \\"Getting event loop for MyOS\\" whenever the current event loop is requested. 2. **Custom Child Watcher**: Implement a custom child watcher named `MyOSChildWatcher` that should: - Inherit from `AbstractChildWatcher`. - Print \\"Adding child handler for PID: <pid>\\" when `add_child_handler` is called. - Print \\"Removing child handler for PID: <pid>\\" when `remove_child_handler` is called. - Print \\"Attaching watcher to event loop\\" when `attach_loop` is called. - Print \\"Watcher is active\\" when `is_active` is called, and return `True`. - Print \\"Closing watcher\\" when `close` is called. 3. **Integration**: - Set your custom event loop policy (`MyOSEventLoopPolicy`) as the current event loop policy. - Set your custom child watcher (`MyOSChildWatcher`) as the current child watcher. Input and Output: - **Input**: The solution should take no inputs from the user. - **Output**: The output should be the print statements as described in the requirements. Constraints: - You must use Python 3.10 or later. - Ensure that all functionalities specified are implemented. Hints: 1. Subclass `DefaultEventLoopPolicy` to create `MyOSEventLoopPolicy`. 2. Implement all methods of `AbstractChildWatcher` in your `MyOSChildWatcher`. 3. Use `asyncio.set_event_loop_policy` and `asyncio.set_child_watcher` to set your custom implementations. # Solution Template: Here is a starting template for your solution. You need to fill in the details according to the requirements. ```python import asyncio class MyOSEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def new_event_loop(self): print(\\"Creating new event loop for MyOS\\") return super().new_event_loop() def get_event_loop(self): print(\\"Getting event loop for MyOS\\") return super().get_event_loop() class MyOSChildWatcher(asyncio.AbstractChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Adding child handler for PID: {pid}\\") def remove_child_handler(self, pid): print(f\\"Removing child handler for PID: {pid}\\") return True def attach_loop(self, loop): print(\\"Attaching watcher to event loop\\") def is_active(self): print(\\"Watcher is active\\") return True def close(self): print(\\"Closing watcher\\") # Set custom event loop policy asyncio.set_event_loop_policy(MyOSEventLoopPolicy()) # Set custom child watcher watcher = MyOSChildWatcher() asyncio.set_child_watcher(watcher) # Code to demonstrate the custom policy and watcher loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.sleep(1)) watcher.add_child_handler(1234, lambda pid, returncode: None) watcher.remove_child_handler(1234) watcher.attach_loop(loop) print(watcher.is_active()) watcher.close() ``` Make sure to thoroughly test your implementation to ensure it meets the specified requirements.","solution":"import asyncio class MyOSEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def new_event_loop(self): print(\\"Creating new event loop for MyOS\\") return super().new_event_loop() def get_event_loop(self): print(\\"Getting event loop for MyOS\\") return super().get_event_loop() class MyOSChildWatcher(asyncio.AbstractChildWatcher): def add_child_handler(self, pid, callback, *args): print(f\\"Adding child handler for PID: {pid}\\") def remove_child_handler(self, pid): print(f\\"Removing child handler for PID: {pid}\\") return True def attach_loop(self, loop): print(\\"Attaching watcher to event loop\\") def is_active(self): print(\\"Watcher is active\\") return True def close(self): print(\\"Closing watcher\\") # Set custom event loop policy asyncio.set_event_loop_policy(MyOSEventLoopPolicy()) # Set custom child watcher watcher = MyOSChildWatcher() asyncio.set_child_watcher(watcher) # Code to demonstrate the custom policy and watcher loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.sleep(1)) watcher.add_child_handler(1234, lambda pid, returncode: None) watcher.remove_child_handler(1234) watcher.attach_loop(loop) print(watcher.is_active()) watcher.close()"},{"question":"**Python Coding Assessment Question** **Objective:** Write a Python script that utilizes the `distutils` package to automate the process of creating a source distribution for a given Python project. The script should handle the following tasks: 1. Generate a `setup.py` file if it does not already exist. 2. Create a source distribution of the project, including all specified files and metadata. 3. Handle additional metadata such as author, version, and description. 4. Verify that the generated source distribution is properly structured and includes all necessary data. **Input:** - `project_dir` (str): The path to the root directory of the Python project. - `metadata` (dict): A dictionary containing the necessary metadata for the project. Keys include \'name\', \'version\', \'author\', \'author_email\', \'description\', and other optional metadata fields (keys and values are strings). **Output:** - The function should create a source distribution in the specified project directory. - The script should print out the structure of the generated distribution and ensure that there are no missing files or metadata. **Constraints:** - Assume the `project_dir` contains at least one Python module (.py file). - You cannot use external libraries other than `distutils` for the purpose of this task. - The function should handle edge cases, such as missing metadata fields or an empty project directory. - The function should work with both Python 3.10 and later versions that still support distutils. **Example:** ```python import os from distutils.core import setup from distutils.archive_util import make_archive def create_source_distribution(project_dir, metadata): # Step 1: Generate a setup.py file if it does not already exist setup_path = os.path.join(project_dir, \'setup.py\') if not os.path.exists(setup_path): with open(setup_path, \'w\') as f: f.write(f from distutils.core import setup setup( name=\'{metadata.get(\'name\', \'default_name\')}\', version=\'{metadata.get(\'version\', \'0.1\')}\', author=\'{metadata.get(\'author\', \'unknown\')}\', author_email=\'{metadata.get(\'author_email\', \'unknown@example.com\')}\', description=\'{metadata.get(\'description\', \'No description\')}\', packages=[], ) ) # Step 2: Create source distribution make_archive(f\\"{metadata.get(\'name\', \'default_name\')}-{metadata.get(\'version\', \'0.1\')}\\", \'gztar\', root_dir=project_dir) # Step 3: Verify the generated distribution structure generated_distribution = f\\"{metadata.get(\'name\', \'default_name\')}-{metadata.get(\'version\', \'0.1\')}.tar.gz\\" if os.path.exists(generated_distribution): print(f\\"Source distribution created successfully at {generated_distribution}\\") else: print(\\"Error: Source distribution was not created.\\") # Step 4: Print structure for verification (optional) import tarfile with tarfile.open(generated_distribution, \'r:gz\') as tar: tar.list() # Example usage metadata_example = { \'name\': \'sample_project\', \'version\': \'1.0\', \'author\': \'John Doe\', \'author_email\': \'john.doe@example.com\', \'description\': \'A sample python project\' } create_source_distribution(\'/path/to/sample_project\', metadata_example) ``` **Explanation:** - The `create_source_distribution` function first checks for the presence of `setup.py` and generates one if absent. - It then creates a source distribution archive file in the project directory. - The function verifies the final structure by printing out the contents of the generated archive. - The example shows how to use the function with sample metadata to create a distribution for a sample project.","solution":"import os from distutils.core import setup from distutils.dir_util import copy_tree from distutils.archive_util import make_archive def create_source_distribution(project_dir, metadata): # Step 1: Generate a setup.py file if it does not already exist setup_path = os.path.join(project_dir, \'setup.py\') if not os.path.exists(setup_path): with open(setup_path, \'w\') as f: f.write(f from distutils.core import setup setup( name=\'{metadata.get(\'name\', \'default_name\')}\', version=\'{metadata.get(\'version\', \'0.1\')}\', author=\'{metadata.get(\'author\', \'unknown\')}\', author_email=\'{metadata.get(\'author_email\', \'unknown@example.com\')}\', description=\'{metadata.get(\'description\', \'No description\')}\', packages=[\'\'], ) ) # Step 2: Create source distribution dist_dir = os.path.join(project_dir, \'dist\') if not os.path.exists(dist_dir): os.makedirs(dist_dir) copy_tree(project_dir, dist_dir) archive_name = f\\"{metadata.get(\'name\', \'default_name\')}-{metadata.get(\'version\', \'0.1\')}\\" make_archive(os.path.join(project_dir, archive_name), \'gztar\', root_dir=dist_dir) # Step 3: Verify the generated distribution structure generated_distribution = os.path.join(project_dir, f\\"{archive_name}.tar.gz\\") if os.path.exists(generated_distribution): print(f\\"Source distribution created successfully at {generated_distribution}\\") else: print(\\"Error: Source distribution was not created.\\") # Step 4: Print structure for verification (optional) import tarfile with tarfile.open(generated_distribution, \'r:gz\') as tar: tar.list()"},{"question":"# Signal Handling and Multithreading in Python This challenge involves creating a Python program that handles Unix signals in a multi-threaded environment. You will need to demonstrate your understanding of signal handling, custom handlers, and the interaction between signals and threads. Task 1. Write a program that spawns a new thread which takes a long time to execute (e.g., a sleep for 30 seconds). 2. Set a signal handler for `SIGINT` that: - Prints a message (\\"SIGINT received in main thread\\") - Does not terminate the program immediately. 3. When `SIGINT` is received in the main thread, the signal handler should set a flag that signals the child thread to terminate gracefully. 4. The child thread should frequently check for this termination flag and if checked, should exit cleanly. 5. Ensure that the main thread waits for the child thread to terminate before exiting itself. Constraints & Requirements - Use the `signal` module to set the signal handler. - Use the `threading` module to manage threads. - The signal handler should only set a flag, and the actual termination logic should be handled in the child thread. - The child thread should not terminate abruptly; it should clean up resources and then exit. Input and Output - There are no direct inputs to the program. The program should handle the signal sent by someone pressing `Ctrl + C`. - The expected output is a print statement confirming receipt of `SIGINT`, followed by clean termination messages from both main and child threads. Example Usage Here is an example snippet to illustrate what the signal handler might look like: ```python import signal import threading import time termination_flag = False def signal_handler(sig, frame): global termination_flag print(\\"SIGINT received in main thread\\") termination_flag = True def long_running_task(): global termination_flag print(\\"Child thread started, running long task...\\") for i in range(30): if termination_flag: print(\\"Termination flag set, exiting child thread...\\") break time.sleep(1) print(\\"Child thread terminating gracefully\\") if __name__ == \\"__main__\\": signal.signal(signal.SIGINT, signal_handler) thread = threading.Thread(target=long_running_task) thread.start() thread.join() print(\\"Main thread exiting\\") ``` Notes - Extensive use of `signal` and `threading` functions is necessary. - The program should be robust against multiple signals. Evaluation Your program will be evaluated based on: - Correctness: Does it handle signals as described? - Graceful termination: Does the child thread clean up and exit when the flag is set? - Code clarity: Is your code readable and well-commented?","solution":"import signal import threading import time # Global termination flag termination_flag = False def signal_handler(sig, frame): global termination_flag print(\\"SIGINT received in main thread\\") termination_flag = True def long_running_task(): global termination_flag print(\\"Child thread started, running long task...\\") for i in range(30): if termination_flag: print(\\"Termination flag set, exiting child thread...\\") break time.sleep(1) print(\\"Child thread terminating gracefully\\") if __name__ == \\"__main__\\": # Register signal handler signal.signal(signal.SIGINT, signal_handler) # Start the long running thread thread = threading.Thread(target=long_running_task) thread.start() # Wait for the child thread to complete thread.join() print(\\"Main thread exiting\\")"},{"question":"**Coding Assessment Question** You have been provided with a Python source code file (`code.py`) that contains several function definitions, classes, and variable declarations. Your task is to write a program that analyzes this source code using the `symtable` module and extracts specific information about the symbols defined in the code. # Objectives 1. Generate the top-level `SymbolTable` from the source code in `code.py`. 2. Print a summary of all the symbols found in the code, along with their properties. # Requirements 1. Implement a function `analyze_code(file_path: str) -> None` that takes the file path of the Python source code as input. 2. The function should print the following information for each symbol: - Symbol name - Is it a parameter? - Is it a local variable? - Is it a global variable? - Is it referenced? - Is it imported? - Is it nonlocal? - Is it annotated? - Is it free? - Is it assigned? - Does it introduce a namespace? # Input - A file path to a Python source code file (`code.py`). # Output - Print to the console the extracted information for each symbol in the format shown below (one symbol per line): ``` Symbol Name: <name> Is Parameter: <True/False> Is Local: <True/False> Is Global: <True/False> Is Referenced: <True/False> Is Imported: <True/False> Is Nonlocal: <True/False> Is Annotated: <True/False> Is Free: <True/False> Is Assigned: <True/False> Introduces Namespace: <True/False> ``` # Constraints - The provided Python source code file (`code.py`) will be well-formatted and syntactically correct. - Your solution should handle cases where symbols may belong to nested functions or classes. # Example Scenario Given a file `code.py` with the following content: ```python def foo(x, y): z = x + y return z class Bar: def baz(self, value): return value ``` The expected output might be: ``` Symbol Name: foo Is Parameter: False Is Local: False Is Global: True Is Referenced: False Is Imported: False Is Nonlocal: False Is Annotated: False Is Free: False Is Assigned: False Introduces Namespace: True Symbol Name: x Is Parameter: True Is Local: True Is Global: False Is Referenced: False Is Imported: False Is Nonlocal: False Is Annotated: False Is Free: False Is Assigned: False Introduces Namespace: False Symbol Name: y Is Parameter: True Is Local: True Is Global: False Is Referenced: False Is Imported: False Is Nonlocal: False Is Annotated: False Is Free: False Is Assigned: False Introduces Namespace: False Symbol Name: z Is Parameter: False Is Local: True Is Global: False Is Referenced: False Is Imported: False Is Nonlocal: False Is Annotated: False Is Free: False Is Assigned: True Introduces Namespace: False Symbol Name: Bar Is Parameter: False Is Local: False Is Global: True Is Referenced: False Is Imported: False Is Nonlocal: False Is Annotated: False Is Free: False Is Assigned: False Introduces Namespace: True Symbol Name: baz Is Parameter: False Is Local: False Is Global: False Is Referenced: False Is Imported: False Is Nonlocal: False Is Annotated: False Is Free: False Is Assigned: False Introduces Namespace: False Symbol Name: value Is Parameter: True Is Local: True Is Global: False Is Referenced: False Is Imported: False Is Nonlocal: False Is Annotated: False Is Free: False Is Assigned: False Introduces Namespace: False ``` Use the `symtable` module to achieve this analysis.","solution":"import symtable def analyze_code(file_path: str) -> None: with open(file_path, \'r\') as file: source_code = file.read() top_level_symtable = symtable.symtable(source_code, file_path, \'exec\') def print_symbol_info(sym): print(f\\"Symbol Name: {sym.get_name()}\\") print(f\\"Is Parameter: {sym.is_parameter()}\\") print(f\\"Is Local: {sym.is_local()}\\") print(f\\"Is Global: {sym.is_global()}\\") print(f\\"Is Referenced: {sym.is_referenced()}\\") print(f\\"Is Imported: {sym.is_imported()}\\") print(f\\"Is Nonlocal: {sym.is_nonlocal()}\\") print(f\\"Is Annotated: {sym.is_annotated()}\\") print(f\\"Is Free: {sym.is_free()}\\") print(f\\"Is Assigned: {sym.is_assigned()}\\") print(f\\"Introduces Namespace: {sym.is_namespace()}\\") print() def analyze_symtable(st): for sym in st.get_symbols(): print_symbol_info(sym) for child_st in st.get_children(): analyze_symtable(child_st) analyze_symtable(top_level_symtable)"},{"question":"**Question: Implement a Custom Task Scheduler** You are required to create a task scheduler using Python\'s `sched` module to manage a series of tasks. Implement the following functions to demonstrate your understanding of the `sched` module: 1. **Function: `display_event()`** ```python def display_event(message: str): This function should print the current time and the provided message. Args: message (str): Message to be displayed. ``` 2. **Function: `schedule_tasks(scheduler: sched.scheduler, tasks: list)`** ```python def schedule_tasks(scheduler: sched.scheduler, tasks: list): Schedules multiple tasks using the provided scheduler. Args: scheduler (sched.scheduler): The scheduler instance to be used for scheduling tasks. tasks (list): A list of tuples where each tuple contains: - a delay or absolute time (int or float), - priority (int), - the message (str) to be displayed, - whether the time is absolute (boolean). Example: tasks = [(time.time() + 5, 1, \\"Task 1\\", True), (10, 2, \\"Task 2\\", False)] schedule_tasks(scheduler, tasks) ``` **Instructions:** 1. Implement the `display_event` function, which prints the current time (in seconds) and a given message. 2. Implement the `schedule_tasks` function, which adds tasks to the scheduler. Each task is represented as a tuple: - If the last element of the tuple is `True`, schedule the task using an absolute time (`enterabs`); otherwise, use a delay (`enter`). 3. Use the priority value provided in the task tuple to prioritize tasks when scheduling. 4. Demonstrate the working of your scheduler by: - Creating an instance of the scheduler. - Scheduling at least three tasks with varying times and priorities. - Running the scheduler to execute the tasks. **Constraints:** - Time can be simulated or real (using `time.sleep`). - Ensure that tasks are scheduled accurately and executed according to their priority and time. **Example Execution:** ```python import sched import time # Sample display_event function def display_event(message: str): print(f\\"{time.time()}: {message}\\") # Sample schedule_tasks function def schedule_tasks(scheduler, tasks): for task in tasks: time_value, priority, message, is_absolute = task if is_absolute: scheduler.enterabs(time_value, priority, display_event, argument=(message,)) else: scheduler.enter(time_value, priority, display_event, argument=(message,)) # Create scheduler instance scheduler = sched.scheduler(time.time, time.sleep) # Define tasks tasks = [ (time.time() + 5, 1, \\"Absolute Task 1\\", True), (3, 2, \\"Relative Task 2\\", False), (7, 1, \\"Relative Task 3\\", False), ] # Schedule tasks schedule_tasks(scheduler, tasks) # Run the scheduler scheduler.run() ``` This example should help you understand how to implement and test your custom task scheduler.","solution":"import sched import time def display_event(message: str): This function prints the current time and the provided message. Args: message (str): Message to be displayed. print(f\\"{time.time()}: {message}\\") def schedule_tasks(scheduler: sched.scheduler, tasks: list): Schedules multiple tasks using the provided scheduler. Args: scheduler (sched.scheduler): The scheduler instance to be used for scheduling tasks. tasks (list): A list of tuples where each tuple contains: - a delay or absolute time (int or float), - priority (int), - the message (str) to be displayed, - whether the time is absolute (boolean). Example: tasks = [(time.time() + 5, 1, \\"Task 1\\", True), (10, 2, \\"Task 2\\", False)] schedule_tasks(scheduler, tasks) for task in tasks: time_value, priority, message, is_absolute = task if is_absolute: scheduler.enterabs(time_value, priority, display_event, argument=(message,)) else: scheduler.enter(time_value, priority, display_event, argument=(message,))"},{"question":"# Large Integer Conversion and Manipulation **Objective:** Implement functions that convert various data types to Python integers and handle error scenarios based on the provided specifications of the `PyLongObject` as documented. # Task: 1. Write a function `convert_to_py_integer(value)` that converts a given input to a Python integer (`PyLongObject`). The function should accept inputs of various types (string, float, long, etc.) and convert them appropriately. 2. Implement a function `handle_integer_overflow(value)` that accepts a Python integer (`PyLongObject`) and ensures it is within the `LONG_MIN` and `LONG_MAX` range. If the value exceeds these limits, it should raise an `OverflowError`. # Specifications: `convert_to_py_integer(value)` - **Input:** - A single argument `value` which can be of type `string`, `float`, `long`, or any other type that can be converted to a Python integer. - **Output:** - Returns a Python integer (`PyLongObject`). - **Constraints:** - For string inputs, ensure the string can be converted to an integer. If not, raise a `ValueError`. - For float inputs, extract the integer part and convert it accordingly. - For other types, attempt the conversion if possible, or raise a `TypeError` if the conversion cannot be made. `handle_integer_overflow(value)` - **Input:** - A single argument `value` which is a Python integer (`PyLongObject`). - **Output:** - Returns the same `value` if it is within the `LONG_MIN` and `LONG_MAX` range. - Raises an `OverflowError` if `value` exceeds the allowed range. - **Constraints:** - Assume `LONG_MIN = -9223372036854775808` and `LONG_MAX = 9223372036854775807`. # Example Usage: ```python try: py_int = convert_to_py_integer(\\"12345\\") result = handle_integer_overflow(py_int) print(result) except (ValueError, TypeError, OverflowError) as e: print(f\\"Error: {e}\\") ``` # Notes: - Ensure proper error handling and validate input types rigorously. - Raise appropriate exceptions as per the problem description. - Consider edge cases such as extremely large numbers and invalid string formats.","solution":"def convert_to_py_integer(value): Converts a given input to a Python integer (PyLongObject). Args: value: The input value to be converted, which can be a string, float, or other type. Returns: int: The converted Python integer value. Raises: ValueError: If the string input cannot be converted to an integer. TypeError: If the input type is not supported for conversion. if isinstance(value, int): return value elif isinstance(value, str): try: return int(value) except ValueError: raise ValueError(\\"String input cannot be converted to an integer.\\") elif isinstance(value, float): return int(value) else: raise TypeError(\\"Unsupported type for conversion to integer.\\") def handle_integer_overflow(value): Ensures the given integer value is within the range of LONG_MIN and LONG_MAX. Args: value: The input Python integer (PyLongObject). Returns: int: The same value if within range. Raises: OverflowError: If the value is outside the range of LONG_MIN and LONG_MAX. LONG_MIN = -9223372036854775808 LONG_MAX = 9223372036854775807 if not isinstance(value, int): raise TypeError(\\"Input value must be an integer.\\") if value < LONG_MIN or value > LONG_MAX: raise OverflowError(\\"Integer value out of LONG_MIN and LONG_MAX range.\\") return value"},{"question":"Question: You are required to create a Python function `plot_styled_barplots` that generates two barplots using Seaborn. The function should display: 1. A bar plot with the default Seaborn style. 2. A bar plot with the \\"darkgrid\\" style, applied temporarily using a context manager. **Input:** - Two lists of numbers: `x` and `y`, representing the x and y values of the bar plots respectively. **Output:** - The function should display the two bar plots as described above. **Constraints:** - The lengths of `x` and `y` will be equal and at most 20. - The values in `x` and `y` will be integers. **Function Signature:** ```python def plot_styled_barplots(x: list, y: list) -> None: pass ``` **Example:** ```python x = [1, 2, 3, 4, 5] y = [5, 7, 3, 8, 6] plot_styled_barplots(x, y) ``` **Expected Output:** - Two bar plots will be displayed. The first plot will use the default Seaborn style, and the second plot will be displayed with the \\"darkgrid\\" style applied temporarily. **Notes:** - Use `sns.axes_style()` to switch styles. - Use the context manager functionality of Seaborn to handle the temporary style change. Additional Information You should ensure that your function contains the following: 1. Import statement for Seaborn. 2. Absolutely no return statement; the function should only produce the plots as side-effects. 3. Proper labels and titles for each plot to differentiate them. Use the provided guidelines and requirements to implement the function. You should verify that your implementation adheres to the requirements and constraints.","solution":"import matplotlib.pyplot as plt import seaborn as sns def plot_styled_barplots(x: list, y: list) -> None: Generates two barplots using Seaborn. One with the default Seaborn style and another with the \\"darkgrid\\" style applied temporarily. # First plot: Default Seaborn style plt.figure(figsize=(10, 4)) plt.subplot(1, 2, 1) sns.barplot(x=x, y=y).set_title(\'Default Style\') # Second plot: \\"darkgrid\\" style applied temporarily using a context manager with sns.axes_style(\\"darkgrid\\"): plt.subplot(1, 2, 2) sns.barplot(x=x, y=y).set_title(\'Darkgrid Style\') # Display the plots plt.tight_layout() plt.show()"},{"question":"You are tasked with implementing a multi-threaded simulation using the Python `queue` module. This simulation should model a simplified restaurant order processing system, where orders are placed by customers and processed by kitchen staff. # Requirements: 1. Implement a class `Restaurant`, which has: - A `PriorityQueue` to manage incoming orders. Each order should be a tuple in the form `(priority, order_id, item)`, where `priority` is an integer indicating the importance of the order (1 being the highest priority), `order_id` is a unique identifier for the order, and `item` is a description of the food item. - Methods `place_order(self, order_id, priority, item)` and `process_order(self)`. The `place_order` method should add a new order to the priority queue. The `process_order` method should retrieve and process the highest priority order from the queue. 2. Implement a method `start_processing(self, num_threads)` in the `Restaurant` class that starts a specified number of worker threads. Each thread should continuously call `process_order` to handle orders. 3. Ensure graceful shutdown of the simulation by defining a method `shutdown(self)`, which ensures that all threads complete any ongoing work before exiting. # Input Format: - You do not need to handle any input reading. Focus on defining the `Restaurant` class and its methods. # Output Format: - Your code should not print anything. Focus on the correct implementation of the class methods and ensure thread-safety. # Example: ```python import threading import queue class Restaurant: def __init__(self): self.order_queue = queue.PriorityQueue() self.lock = threading.Lock() self.threads = [] def place_order(self, order_id, priority, item): # Add order to the priority queue self.order_queue.put((priority, order_id, item)) def process_order(self): while True: try: # Retrieve the highest priority order priority, order_id, item = self.order_queue.get(block=False) # Simulate processing order print(f\\"Processing order {order_id}: {item}\\") # Mark the order as processed self.order_queue.task_done() except queue.Empty: break # Exit when no more orders are left def start_processing(self, num_threads): # Start worker threads for _ in range(num_threads): thread = threading.Thread(target=self.process_order) thread.start() self.threads.append(thread) def shutdown(self): for thread in self.threads: thread.join() # Example usage: # restaurant = Restaurant() # restaurant.place_order(1, 2, \\"Burger\\") # restaurant.place_order(2, 1, \\"Pizza\\") # restaurant.place_order(3, 3, \\"Salad\\") # restaurant.start_processing(2) # restaurant.shutdown() ``` # Constraints: - **Performance**: Ensure that methods are thread-safe and handle concurrent access correctly. - **Limitations**: Assume a maximum of 10 worker threads for this simulation. Implement the `Restaurant` class based on the requirements and example above.","solution":"import threading import queue class Restaurant: def __init__(self): self.order_queue = queue.PriorityQueue() self.lock = threading.Lock() self.threads = [] self.stop_event = threading.Event() def place_order(self, order_id, priority, item): with self.lock: self.order_queue.put((priority, order_id, item)) def process_order(self): while not self.stop_event.is_set() or not self.order_queue.empty(): try: priority, order_id, item = self.order_queue.get(timeout=1) # Here would be the processing logic self.order_queue.task_done() except queue.Empty: continue def start_processing(self, num_threads): self.stop_event.clear() for _ in range(num_threads): thread = threading.Thread(target=self.process_order) thread.start() self.threads.append(thread) def shutdown(self): self.stop_event.set() for thread in self.threads: thread.join() self.threads = []"},{"question":"**Question: Secure Password Storage and Verification using `crypt` Module** You are required to develop a secure user registration and login system that leverages hashing for password security using the deprecated Python `crypt` module. Your task is to implement two core functions: `register_user` and `verify_user`. # Function Details: 1. `register_user(username: str, password: str) -> dict` - **Input**: - `username`: The username of the user being registered. - `password`: The plain-text password provided by the user. - **Output**: Returns a dictionary with the following keys: - `\'username\'`: The username of the registered user. - `\'hashed_password\'`: The hashed password using the strongest available method. - `\'salt\'`: The salt used for hashing the password. - **Constraints**: - The password should be hashed using the strongest available method in the `crypt` module. - The salt should be generated using the `crypt.mksalt` method. 2. `verify_user(username: str, plain_password: str, stored_hash: dict) -> bool` - **Input**: - `username`: The username of the user attempting to log in. - `plain_password`: The plain-text password provided by the user. - `stored_hash`: The dictionary containing the stored `\'username\'`, `\'hashed_password\'`, and `\'salt\'`. - **Output**: Returns `True` if the password is valid; otherwise, returns `False`. - **Constraints**: - The password verification should use the same method and salt as during registration. # Performance Requirements - Ensure that the password verification process is constant-time to prevent timing attacks. - Handle edge cases where the registered and provided usernames do not match. # Example: ```python # Example usage user_data = register_user(\\"alice\\", \\"my_secure_password\\") # user_data would be something like: # {\'username\': \'alice\', \'hashed_password\': \'<hashed_value>\', \'salt\': \'<salt_value>\'} is_valid = verify_user(\\"alice\\", \\"my_secure_password\\", user_data) # is_valid should be True is_valid_wrong = verify_user(\\"alice\\", \\"wrong_password\\", user_data) # is_valid_wrong should be False ``` Implement the functions `register_user` and `verify_user` to meet the above requirements.","solution":"import crypt def register_user(username, password): Registers a new user with hashed password. Parameters: username (str): The username of the user. password (str): The plain-text password of the user. Returns: dict: A dictionary containing the username, hashed password, and salt. salt = crypt.mksalt(crypt.METHOD_SHA512) hashed_password = crypt.crypt(password, salt) return { \'username\': username, \'hashed_password\': hashed_password, \'salt\': salt } def verify_user(username, plain_password, stored_hash): Verifies a user\'s password against the stored hash. Parameters: username (str): The username of the user. plain_password (str): The plain-text password provided by the user. stored_hash (dict): A dictionary containing the stored username, hashed password, and salt. Returns: bool: True if the provided password is correct, False otherwise. if username != stored_hash[\'username\']: return False expected_hash = crypt.crypt(plain_password, stored_hash[\'salt\']) return expected_hash == stored_hash[\'hashed_password\']"},{"question":"**Objective:** Implement a function `extract_attachments` that extracts attachment filenames and payloads from a multipart email message. **Function Signature:** ```python def extract_attachments(message: email.message.Message) -> dict: pass ``` **Description:** Write a function `extract_attachments` that takes an instance of `email.message.Message` representing an email message and extracts all attachments. The function should return a dictionary where the keys are the filenames of the attachments and the values are their corresponding payloads. # Requirements: 1. The function should handle both single-part and multi-part messages. 2. If an attachment does not have a filename, it should be skipped. 3. The payloads of the attachments should be in their decoded form (consider `Content-Transfer-Encoding`). 4. The function should be recursive to handle nested multi-part messages. 5. If the message has no attachments, the function should return an empty dictionary. # Input: - `message`: An instance of `email.message.Message` # Output: - Returns a dictionary with filenames as keys and decoded payloads as values. # Constraints: - Assume the input message object always follows the RFC standards for email messages. - You can use any standard Python library but should avoid non-standard libraries that are not part of the `email` module or similar. # Example: Consider the following usage example: ```python import email from email.message import Message # Sample MIME email creation (simplified) msg = Message() msg.set_boundary(\\"boundary\\") msg.set_payload([ email.message.Message(_payload=\\"This is the main body text.\\"), email.message.Message(_payload=(\\"Sample Attachment Content.\\").encode(\'utf-8\')) ]) msg.get_payload()[1].add_header(\\"Content-Disposition\\", \\"attachment\\", filename=\\"sample.txt\\") # Extracting attachments result = extract_attachments(msg) print(result) # Output: {\'sample.txt\': b\'Sample Attachment Content.\'} ``` # Important: - Remember to include docstrings and comments in your code for better clarity. # Performance: - The function should handle typical email sizes efficiently, but there are no specific performance constraints.","solution":"import email from email.message import Message from typing import Dict def extract_attachments(message: email.message.Message) -> Dict[str, bytes]: Extracts attachment filenames and payloads from a multipart email message. Args: message (email.message.Message): An email message object. Returns: dict: A dictionary where the keys are the filenames of the attachments and the values are their corresponding payloads. attachments = {} if message.is_multipart(): for part in message.get_payload(): attachments.update(extract_attachments(part)) else: content_disposition = message.get(\\"Content-Disposition\\", None) if content_disposition and \\"attachment\\" in content_disposition: filename = message.get_filename() if filename: payload = message.get_payload(decode=True) attachments[filename] = payload return attachments"},{"question":"**Objective:** The task aims to evaluate your understanding of seaborn, specifically the `so.Plot` interface, and your ability to apply various scales and transformations to enhance visual data representation. **Task:** Using the seaborn package, create a series of visualizations based on the provided `mpg` dataset. Your visualizations should demonstrate the following concepts: 1. **Scatter Plot with Logarithmic Scale:** - Create a scatter plot with `horsepower` on the x-axis and `weight` on the y-axis. - Apply a logarithmic scale to the y-axis. - Map the `origin` to the color of the points using a qualitative palette. 2. **Bar Plot with Nominal Scale:** - Create a bar plot to show the count of cars with different number of `cylinders`. - Ensure that the `cylinders` variable is treated as a nominal scale. 3. **Customized Line Plot:** - Create a line plot with `model_year` on the x-axis and `mpg` (miles per gallon) on the y-axis. - Apply a sqrt transformation to the y-axis. - Customize the color scale to reflect the `cylinders` using a continuous scale from \\"#b2dfdb\\" to \\"#00796b\\". - Add a polynomial fit line of order 2 to the plot. **Constraints:** - Use the seaborn `so.Plot` interface for all visualizations. - Make sure your code is well-documented and functions correctly within a Jupyter Notebook. **Input Format:** - The `mpg` dataset can be loaded using `mpg = sns.load_dataset(\\"mpg\\")`. - Only include cars with `cylinders` in `[4, 6, 8]` to match the query in the documentation provided. **Output:** - Three correctly rendered plots according to the specifications outlined above. **Performance:** - Ensure plots are generated efficiently and accurately reflect the transformations and scales applied. ```python import seaborn.objects as so from seaborn import load_dataset # Load and filter dataset mpg = load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\") # 1. Scatter Plot with Logarithmic Scale p1 = (so.Plot(mpg, x=\\"horsepower\\", y=\\"weight\\", color=\\"origin\\") .add(so.Dots()) .scale(y=\\"log\\", color=\\"dark\\")) p1.show() # 2. Bar Plot with Nominal Scale p2 = (so.Plot(mpg, x=\\"cylinders\\") .add(so.Bar(), so.Hist()) .scale(x=so.Nominal())) p2.show() # 3. Customized Line Plot p3 = (so.Plot(mpg, x=\\"model_year\\", y=\\"mpg\\", color=\\"cylinders\\") .add(so.Line()) .scale(y=so.Continuous(trans=\\"sqrt\\"), color=so.Continuous([\\"#b2dfdb\\", \\"#00796b\\"])) .add(so.Line(marker=\\"o\\"), so.PolyFit(order=2))) p3.show() ``` Ensure you understand the concepts and approach them step-by-step to create the required visualizations accurately.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load and filter dataset mpg = load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\") # 1. Scatter Plot with Logarithmic Scale def scatter_plot_log_scale(): p1 = (so.Plot(mpg, x=\\"horsepower\\", y=\\"weight\\", color=\\"origin\\") .add(so.Dots()) .scale(y=\\"log\\", color=\\"dark\\")) return p1 # 2. Bar Plot with Nominal Scale def bar_plot_nominal_scale(): p2 = (so.Plot(mpg, x=\\"cylinders\\") .add(so.Bar(), so.Hist()) .scale(x=so.Nominal())) return p2 # 3. Customized Line Plot def customized_line_plot(): p3 = (so.Plot(mpg, x=\\"model_year\\", y=\\"mpg\\", color=\\"cylinders\\") .add(so.Line()) .scale(y=so.Continuous(trans=\\"sqrt\\"), color=so.Continuous([\\"#b2dfdb\\", \\"#00796b\\"])) .add(so.Line(marker=\\"o\\"), so.PolyFit(order=2))) return p3"},{"question":"Objective: Write a Python function that utilizes the `sysconfig` module to verify the installation paths and configuration details for your Python environment. Problem Statement: Implement a function `verify_python_setup()` that performs the following tasks: 1. Retrieves and returns the default installation scheme used on the current platform. 2. Retrieves a list of all installation path names supported by `sysconfig`. 3. Retrieves the actual path for each path name under the default scheme. 4. Retrieves and prints five important configuration variables: `\'AR\'`, `\'CXX\'`, `\'Py_ENABLE_SHARED\'`, `\'LIBDIR\'`, and `\'LIBPL\'`. Input: None Output: - Print the default installation scheme. - Print all the installation path names. - Print the paths for each installation path name. - Print the values of the above-specified configuration variables (one line per variable). Function Signature: ```python def verify_python_setup(): pass ``` Example Output: ``` Default installation scheme: posix_prefix Installation path names: [\'stdlib\', \'platstdlib\', \'purelib\', \'platlib\', \'include\', \'platinclude\', \'scripts\', \'data\'] Paths: stdlib: /usr/local/lib/python3.10 ... Configuration Variables: AR: ar CXX: g++ Py_ENABLE_SHARED: 0 LIBDIR: /usr/local/lib LIBPL: /usr/local/lib/python3.10/config-3.10-x86_64-linux-gnu ``` Constraints: - Your function should handle different platforms appropriately using the details provided by `sysconfig`. - Ensure all paths and variables retrieved exist; otherwise, handle missing data gracefully. - Do not use any hard-coded values; rely entirely on `sysconfig`.","solution":"import sysconfig def verify_python_setup(): # Get the default installation scheme default_scheme = sysconfig.get_default_scheme() print(f\\"Default installation scheme: {default_scheme}\\") # Get the list of all installation path names path_names = sysconfig.get_path_names() print(f\\"Installation path names: {path_names}\\") # Get the actual paths for each path name under the default scheme print(\\"Paths:\\") for path_name in path_names: path = sysconfig.get_path(path_name, scheme=default_scheme) print(f\\" {path_name}: {path}\\") # Get the configuration variables config_vars = [\'AR\', \'CXX\', \'Py_ENABLE_SHARED\', \'LIBDIR\', \'LIBPL\'] print(\\"Configuration Variables:\\") for var in config_vars: value = sysconfig.get_config_var(var) print(f\\" {var}: {value}\\")"},{"question":"Implementing and Evaluating Naive Bayes Classifiers Objective Your task is to demonstrate your understanding of Naive Bayes classifiers available in the scikit-learn library, specifically focusing on `GaussianNB`, `MultinomialNB`, and `ComplementNB`. Problem Description You are provided with a dataset containing information about various types of data which will be used to build a classification model. The dataset is split into training and testing sets. 1. **Load the dataset**: Obtain the dataset using any realistic dataset function from scikit-learn, such as `load_iris()`, `load_digits()`, etc. 2. **Preprocess the data**: Depending on the dataset, you may need to preprocess the features to fit the assumptions of each Naive Bayes classifier. 3. **Train and evaluate models**: - Implement three classifiers: `GaussianNB`, `MultinomialNB`, and `ComplementNB`. - Train each classifier on the training set. - Evaluate the performance of each classifier on the testing set using appropriate metrics like accuracy, precision, recall, and F1-score. 4. **Compare the results**: Provide a comparison of the results obtained from the different classifiers. Constraints - Use the `train_test_split` function from `sklearn.model_selection` to split the dataset. - You must use `scikit-learn` for implementing the classifiers. - Ensure that any necessary preprocessing steps are appropriately handled for each classifier (e.g., data scaling for `GaussianNB`). Input and Output Formats - **Input**: None. The function should not take any inputs; it should load the dataset internally. - **Output**: Print the evaluation metrics for each classifier and a final comparison statement summarizing the results. Example Assume you are working with the Iris dataset: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def naive_bayes_classification(): # Load dataset data = load_iris() X, y = data.data, data.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0) # Initialize classifiers classifiers = { \\"GaussianNB\\": GaussianNB(), \\"MultinomialNB\\": MultinomialNB(), \\"ComplementNB\\": ComplementNB() } # Train and evaluate each classifier for name, clf in classifiers.items(): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) print(f\\"Results for {name}:\\") print(f\\"Accuracy: {accuracy_score(y_test, y_pred)}\\") print(f\\"Precision: {precision_score(y_test, y_pred, average=\'weighted\')}\\") print(f\\"Recall: {recall_score(y_test, y_pred, average=\'weighted\')}\\") print(f\\"F1 Score: {f1_score(y_test, y_pred, average=\'weighted\')}\\") print(\\"n\\") # Final comparison statement print(\\"Compare the above results to determine which classifier performed the best for this dataset.\\") # Call the function to execute the classification naive_bayes_classification() ``` Your task is to implement and run the function `naive_bayes_classification()` as described above.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def naive_bayes_classification(): # Load dataset data = load_iris() X, y = data.data, data.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0) # Initialize classifiers classifiers = { \\"GaussianNB\\": GaussianNB(), \\"MultinomialNB\\": MultinomialNB(), \\"ComplementNB\\": ComplementNB() } results = {} # Train and evaluate each classifier for name, clf in classifiers.items(): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) results[name] = { \\"accuracy\\": accuracy_score(y_test, y_pred), \\"precision\\": precision_score(y_test, y_pred, average=\'weighted\'), \\"recall\\": recall_score(y_test, y_pred, average=\'weighted\'), \\"f1_score\\": f1_score(y_test, y_pred, average=\'weighted\') } print(f\\"Results for {name}:\\") print(f\\"Accuracy: {results[name][\'accuracy\']}\\") print(f\\"Precision: {results[name][\'precision\']}\\") print(f\\"Recall: {results[name][\'recall\']}\\") print(f\\"F1 Score: {results[name][\'f1_score\']}\\") print(\\"n\\") # Final comparison statement print(\\"Compare the above results to determine which classifier performed the best for this dataset.\\") return results # Call the function to execute the classification naive_bayes_classification()"},{"question":"**Embedding a Python Interpreter in C** You are tasked with creating a C program that embeds a Python interpreter. Your C program must read a text file containing Python code, execute it using the embedded interpreter, and handle any potential exceptions. # Requirements: 1. **Input File**: Assume the Python code is provided in a file named `script.py`. 2. **Execution**: The C program should execute the Python code contained in the file. 3. **Output**: The C program should print any output produced by the Python script to the console. 4. **Exception Handling**: If the Python script raises an exception, the C program should handle it and print a relevant error message. # Constraints: - You must use functions provided in the Python 3.10 API documentation, such as `PyRun_SimpleFileExFlags`. - Ensure that the program correctly initializes and finalizes the Python interpreter. # Input: - The input is always the contents of `script.py` file. You do not need to handle dynamic input within the Python script. # Output: - The output should be the result of executing the Python code, printed to the console. - If an exception occurs, print an error message in the format: `Error: <exception message>`. # Additional Notes: - Ensure proper error checking and resource management. - Use `Py_Initialize()` and `Py_Finalize()` to manage the Python interpreter lifecycle. Here\'s the skeleton of the C program: ```c #include <Python.h> #include <stdio.h> int main(int argc, char *argv[]) { // Initialize the Python interpreter Py_Initialize(); // Open the Python script file FILE *fp = fopen(\\"script.py\\", \\"r\\"); if (fp == NULL) { fprintf(stderr, \\"Error: Could not open script.pyn\\"); return 1; } // Run the Python script int result = PyRun_SimpleFileExFlags(fp, \\"script.py\\", 0, NULL); // Check for exceptions if (result != 0) { // Handle the exception PyObject *exc_type, *exc_value, *exc_traceback; PyErr_Fetch(&exc_type, &exc_value, &exc_traceback); PyErr_NormalizeException(&exc_type, &exc_value, &exc_traceback); const char *exc_msg = PyUnicode_AsUTF8(PyObject_Str(exc_value)); fprintf(stderr, \\"Error: %sn\\", exc_msg); PyErr_Clear(); } // Close the file fclose(fp); // Finalize the Python interpreter Py_Finalize(); return result; } ``` Implement this C program to embed the Python interpreter, ensuring all specified requirements are met.","solution":"# There is no Python solution required for this task since it involves writing a C program."},{"question":"**Objective:** Demonstrate proficiency in creating custom object types and implementing related functionalities such as attribute access, method definitions, and memory management in Python. **Problem Statement:** You are tasked with designing a custom object type called `CustomList` that mimics basic behaviors of a Python list but includes additional features for enhanced usability. Your implementation should include methods for adding, removing, and accessing elements, as well as maintaining the order of elements. You should also include a custom method to reverse the list and manage memory allocation for the object on the heap. **Requirements:** 1. **Initialization**: The `CustomList` should be initialized with an optional list of elements. 2. **Adding Elements**: Implement a method `add_element(self, element)` that adds an element to the end of the `CustomList`. 3. **Removing Elements**: Implement a method `remove_element(self, element)` that removes the first occurrence of the specified element from the `CustomList`. 4. **Accessing Elements**: Implement a method `get_element(self, index)` that returns the element at the given index. 5. **Reversing the List**: Implement a method `reverse_list(self)` that reverses the order of elements in the `CustomList`. 6. **Memory Management**: Ensure your `CustomList` efficiently handles memory allocation and deallocation on the heap. **Input Format:** - Initialization with an optional list of elements: `custom_list = CustomList([1, 2, 3, 4])` - Method calls: `custom_list.add_element(5)`, `custom_list.remove_element(2)`, `custom_list.get_element(1)`, `custom_list.reverse_list()` **Output Format:** - The methods `add_element`, `remove_element`, and `reverse_list` will directly modify the `CustomList` object. - The method `get_element` will return the element at the specified index. **Constraints:** - You should handle edge cases such as removing an element not present in the list, or accessing an index out of bounds by raising appropriate exceptions. - The implementation should follow best practices for memory management and performance. ```python class CustomList: def __init__(self, elements=None): Initialize the CustomList with optional elements. pass def add_element(self, element): Add an element to the end of the CustomList. pass def remove_element(self, element): Remove the first occurrence of the specified element from the CustomList. pass def get_element(self, index): Return the element at the given index. pass def reverse_list(self): Reverse the order of elements in the CustomList. pass # Example usage: custom_list = CustomList([1, 2, 3, 4]) custom_list.add_element(5) custom_list.remove_element(2) element = custom_list.get_element(1) # Should return 1 (after removal of 2) custom_list.reverse_list() # Should reverse the list ``` Make sure to include test cases to validate your implementation, checking for both standard scenarios and edge cases.","solution":"class CustomList: def __init__(self, elements=None): Initialize the CustomList with optional elements. self.elements = elements if elements is not None else [] def add_element(self, element): Add an element to the end of the CustomList. self.elements.append(element) def remove_element(self, element): Remove the first occurrence of the specified element from the CustomList. if element in self.elements: self.elements.remove(element) else: raise ValueError(f\'Element {element} not found in CustomList.\') def get_element(self, index): Return the element at the given index. if index < 0 or index >= len(self.elements): raise IndexError(f\'Index {index} out of bounds.\') return self.elements[index] def reverse_list(self): Reverse the order of elements in the CustomList. self.elements.reverse()"},{"question":"Unicode String Handling in Python Objective Your task is to implement a function `process_unicode_strings` that receives a list of Unicode strings and returns a dictionary with the following information for each string: 1. The normalized form of the string using NFC (Normalization Form C). 2. A list of tuples containing each character\'s: - Unicode character - Code point in hexadecimal - Unicode category (e.g., \'Ll\' for lowercase letter, \'Nd\' for decimal number) - Unicode name if available, otherwise return \'UNKNOWN\' Additionally, implement encoding and decoding of each string to/from UTF-8 encoded bytes. Function Signature ```python def process_unicode_strings(strings: list) -> dict: pass ``` Input - `strings` (list of str): A list of Unicode strings. Output - (dict): A dictionary where keys are the original strings and values are dictionaries with the following keys: - \'normalized\': The NFC normalized form of the original string. - \'details\': A list of tuples, each tuple containing the following for each character in the string: - Character itself (str) - Code point in hexadecimal (str starting with \'U+\') - Unicode category (str) - Unicode name (str) - \'encoded\': The UTF-8 encoded bytes of the original string - \'decoded\': The string obtained by decoding the UTF-8 bytes back to Unicode Example ```python # Input strings = [\\"café\\", \\"à la carte\\", \\"Gürzenichstraße\\"] # Output { \\"café\\": { \\"normalized\\": \\"café\\", \\"details\\": [ (\'c\', \'U+0063\', \'Ll\', \'LATIN SMALL LETTER C\'), (\'a\', \'U+0061\', \'Ll\', \'LATIN SMALL LETTER A\'), (\'f\', \'U+0066\', \'Ll\', \'LATIN SMALL LETTER F\'), (\'é\', \'U+00E9\', \'Ll\', \'LATIN SMALL LETTER E WITH ACUTE\') ], \\"encoded\\": b\'cafxc3xa9\', \\"decoded\\": \\"café\\" }, \\"à la carte\\": { \\"normalized\\": \\"à la carte\\", \\"details\\": [ (\'à\', \'U+00E0\', \'Ll\', \'LATIN SMALL LETTER A WITH GRAVE\'), (\' \', \'U+0020\', \'Zs\', \'SPACE\'), (\'l\', \'U+006C\', \'Ll\', \'LATIN SMALL LETTER L\'), (\'a\', \'U+0061\', \'Ll\', \'LATIN SMALL LETTER A\'), (\' \', \'U+0020\', \'Zs\', \'SPACE\'), (\'c\', \'U+0063\', \'Ll\', \'LATIN SMALL LETTER C\'), (\'a\', \'U+0061\', \'Ll\', \'LATIN SMALL LETTER A\'), (\'r\', \'U+0072\', \'Ll\', \'LATIN SMALL LETTER R\'), (\'t\', \'U+0074\', \'Ll\', \'LATIN SMALL LETTER T\'), (\'e\', \'U+0065\', \'Ll\', \'LATIN SMALL LETTER E\') ], \\"encoded\\": b\'xc3xa0 la carte\', \\"decoded\\": \\"à la carte\\" }, \\"Gürzenichstraße\\": { \\"normalized\\": \\"Gürzenichstraße\\", \\"details\\": [ (\'G\', \'U+0047\', \'Lu\', \'LATIN CAPITAL LETTER G\'), (\'ü\', \'U+00FC\', \'Ll\', \'LATIN SMALL LETTER U WITH DIAERESIS\'), (\'r\', \'U+0072\', \'Ll\', \'LATIN SMALL LETTER R\'), (\'z\', \'U+007A\', \'Ll\', \'LATIN SMALL LETTER Z\'), (\'e\', \'U+0065\', \'Ll\', \'LATIN SMALL LETTER E\'), (\'n\', \'U+006E\', \'Ll\', \'LATIN SMALL LETTER N\'), (\'i\', \'U+0069\', \'Ll\', \'LATIN SMALL LETTER I\'), (\'c\', \'U+0063\', \'Ll\', \'LATIN SMALL LETTER C\'), (\'h\', \'U+0068\', \'Ll\', \'LATIN SMALL LETTER H\'), (\'s\', \'U+0073\', \'Ll\', \'LATIN SMALL LETTER S\'), (\'t\', \'U+0074\', \'Ll\', \'LATIN SMALL LETTER T\'), (\'r\', \'U+0072\', \'Ll\', \'LATIN SMALL LETTER R\'), (\'a\', \'U+0061\', \'Ll\', \'LATIN SMALL LETTER A\'), (\'ß\', \'U+00DF\', \'Ll\', \'LATIN SMALL LETTER SHARP S\'), (\'e\', \'U+0065\', \'Ll\', \'LATIN SMALL LETTER E\') ], \\"encoded\\": b\'Gxc3xbcrzenichstraxc3x9fe\', \\"decoded\\": \\"Gürzenichstraße\\" } } ``` Constraints - The list of strings will have a size `n` such that `1 <= n <= 100`. - Each string will have a length `m` such that `1 <= m <= 1000`. - Assume all input strings are valid Unicode. Notes - Make sure to handle and encode/decode Unicode strings properly. - Pay attention to normalization and character properties using `unicodedata`. Good luck!","solution":"import unicodedata def process_unicode_strings(strings: list) -> dict: result = {} for s in strings: normalized = unicodedata.normalize(\'NFC\', s) details = [] for char in s: code_point = f\\"U+{ord(char):04X}\\" category = unicodedata.category(char) name = unicodedata.name(char, \'UNKNOWN\') details.append((char, code_point, category, name)) encoded = s.encode(\'utf-8\') decoded = encoded.decode(\'utf-8\') result[s] = { \'normalized\': normalized, \'details\': details, \'encoded\': encoded, \'decoded\': decoded } return result"},{"question":"**Objective:** You are required to demonstrate your understanding of loading real-world datasets using scikit-learn, preprocessing the data, and implementing a machine learning model. **Task:** 1. Load the \\"Olivetti faces\\" dataset (which consists of various face images) using scikit-learn. 2. Preprocess the data: - Normalize the pixel values to a range between 0 and 1. 3. Implement a simple machine learning model to perform classification on this dataset. 4. Evaluate the model performance using appropriate metrics (e.g., accuracy). **Requirements:** - Your code should load the dataset using `datasets.fetch_olivetti_faces()`. - Normalize the dataset\'s feature values. - Implement a Support Vector Machine (SVM) classifier. - Split the data into training and test sets. - Train the SVM model on the training set and evaluate it on the test set. - Print the accuracy of your model. **Input and Output Formats:** - The input consists of no explicit input other than the import statements. - The output should be the accuracy value as a floating-point number. **Constraints and Limitations:** - The model should be implemented using the `sklearn.svm.SVC` class. - You should use an 80-20 train-test split for evaluation. ```python # Sample Code Template from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score def olivetti_faces_classification(): # Load the dataset data = datasets.fetch_olivetti_faces() X, y = data.data, data.target # Normalize the data scaler = StandardScaler() X = scaler.fit_transform(X) # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement the SVM classifier svm_clf = SVC() svm_clf.fit(X_train, y_train) # Predict on the test set y_pred = svm_clf.predict(X_test) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) # Print the accuracy print(f\'Accuracy: {accuracy:.4f}\') # Call the function to execute the classification olivetti_faces_classification() ``` **Expected Output:** ``` Accuracy: [value between 0 and 1] ``` **Note:** Feel free to explore different kernel functions for the SVM classifier to see if you can improve the accuracy.","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score def olivetti_faces_classification(): # Load the dataset data = datasets.fetch_olivetti_faces() X, y = data.data, data.target # Normalize the data scaler = StandardScaler() X = scaler.fit_transform(X) # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement the SVM classifier svm_clf = SVC(kernel=\'linear\', random_state=42) svm_clf.fit(X_train, y_train) # Predict on the test set y_pred = svm_clf.predict(X_test) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) # Return the accuracy return accuracy # Call the function to execute the classification and print the accuracy accuracy = olivetti_faces_classification() print(f\'Accuracy: {accuracy:.4f}\')"},{"question":"Objective: Design a function `fetch_and_validate_data` that establishes an HTTP connection to a given server, sends a request to a specified URL, and processes the response. The function should handle various edge cases using appropriate exceptions and ensure the response meets certain criteria before returning data. Specification: 1. **Function Signature**: ```python def fetch_and_validate_data(host: str, port: int, method: str, url: str, expected_status_code: int, timeout: int = 10) -> str: Fetches data from a given URL using the specified HTTP method and validates the response status. Parameters: - host (str): The server host. - port (int): The server port. - method (str): HTTP method (\'GET\', \'POST\', etc.). - url (str): The URL path on the server. - expected_status_code (int): The expected HTTP status code for validation. - timeout (int): The optional timeout for the connection in seconds (default is 10). Returns: - str: The response body if the status code matches the expected value. Raises: - `http.client.InvalidURL`: If the URL is invalid. - `http.client.HTTPException`: For any HTTP-related errors. - `ValueError`: If the response status code does not match the expected status code. ``` 2. **Input/Output**: - **Input**: - `host` and `port`: The server to connect to (e.g., `\'www.python.org\'` and `80`). - `method`: HTTP method (e.g., `\'GET\'`). - `url`: The server URL path (e.g., `\'/index.html\'`). - `expected_status_code`: Expected HTTP status code (e.g., `200`). - `timeout`: Optional timeout (default `10` seconds). - **Output**: - The response body as a string if the status code matches the expected value. - **Exceptions**: - Raises `http.client.InvalidURL` if the URL is malformed. - Raises `http.client.HTTPException` for other HTTP errors. - Raises `ValueError` if the response status code does not match the expected status code. 3. **Constraints**: - You must use the `http.client` library for handling the connection and request. - Implement proper error handling using the exceptions provided in the `http.client` module. Example: ```python try: response = fetch_and_validate_data(\'www.python.org\', 80, \'GET\', \'/\', 200) print(\\"Response body:\\", response) except http.client.InvalidURL: print(\\"Invalid URL provided.\\") except http.client.HTTPException as e: print(f\\"HTTP error occurred: {e}\\") except ValueError as ve: print(f\\"Validation error: {ve}\\") ``` **Note**: The function `fetch_and_validate_data` should handle the HTTP connection lifecycle, including opening, making a request, reading the response, and closing the connection. Proper handling of exceptions is crucial to ensure robustness.","solution":"import http.client from typing import Optional def fetch_and_validate_data(host: str, port: int, method: str, url: str, expected_status_code: int, timeout: int = 10) -> str: Fetches data from a given URL using the specified HTTP method and validates the response status. Parameters: - host (str): The server host. - port (int): The server port. - method (str): HTTP method (\'GET\', \'POST\', etc.). - url (str): The URL path on the server. - expected_status_code (int): The expected HTTP status code for validation. - timeout (int): The optional timeout for the connection in seconds (default is 10). Returns: - str: The response body if the status code matches the expected value. Raises: - http.client.InvalidURL: If the URL is invalid. - http.client.HTTPException: For any HTTP-related errors. - ValueError: If the response status code does not match the expected status code. try: connection = http.client.HTTPConnection(host, port, timeout=timeout) connection.request(method, url) response = connection.getresponse() if response.status != expected_status_code: raise ValueError(f\\"Expected status {expected_status_code}, but got {response.status}\\") data = response.read().decode() return data except http.client.InvalidURL: raise http.client.InvalidURL(\\"The URL provided is invalid.\\") except http.client.HTTPException as e: raise http.client.HTTPException(f\\"HTTP error occurred: {e}\\") finally: if connection: connection.close()"},{"question":"# Python Coding Assessment Question Objective Your task is to utilize the `mailcap` module to read mailcap file information and determine the appropriate command for handling files of different MIME types. Problem Statement Write a Python function `handle_mime_files(mime_types_with_filenames: dict) -> dict` that takes a dictionary with MIME types as keys and filenames as values. The function should return a dictionary where each key is a MIME type from the input, and each value is a 2-tuple containing: 1. The command line to be executed. 2. The mailcap entry for that MIME type. Input - `mime_types_with_filenames`: A dictionary where: - Key: A string representing the MIME type (e.g., `\'video/mpeg\'`). - Value: A string representing the filename associated with this MIME type (e.g., `\'example.mpeg\'`). Output - A dictionary where each key is a MIME type from the input, and each value is a 2-tuple: - First element: A string containing the command line to execute. - Second element: The mailcap entry dictionary for the MIME type. Example ```python import mailcap # Example usage of the function \'handle_mime_files\' mime_files = { \'video/mpeg\': \'example.mpeg\', \'image/jpeg\': \'photo.jpg\' } print(handle_mime_files(mime_files)) ``` Expected output format: ```python { \'video/mpeg\': (\'xmpeg example.mpeg\', {\'view\': \'xmpeg %s\'}), \'image/jpeg\': (\'xv photo.jpg\', {\'view\': \'xv %s\'}) } ``` Constraints 1. If `findmatch` does not find a match or rejects a filename due to security reasons, the value for that MIME type should be `(None, None)`. 2. Assume that mailcap files may be located at standard paths and user-specific paths as documented. Notes - Use `mailcap.getcaps()` to gather mailcap information. - Use `mailcap.findmatch()` to determine the correct command for each MIME type. - The solution should handle improper characters in filenames by strictly following the `findmatch` function\'s security rules. # Function Signature ```python def handle_mime_files(mime_types_with_filenames: dict) -> dict: pass ```","solution":"import mailcap def handle_mime_files(mime_types_with_filenames: dict) -> dict: caps = mailcap.getcaps() result = {} for mime_type, filename in mime_types_with_filenames.items(): command, entry = mailcap.findmatch(caps, mime_type, filename, \\"view\\") if command is None or entry is None: result[mime_type] = (None, None) else: result[mime_type] = (command, entry) return result"},{"question":"Problem Statement: You are tasked with creating a Python script that reads a text file, processes its contents, and prints the processed contents to the console. Your script should be able to handle different file encodings specified using special comments within the file. Requirements: 1. **Input**: - The script should accept a single argument which is the path to the text file. - The text file may specify its encoding using a special comment (`# -*- coding: encoding -*-`). If no encoding is specified, default to `UTF-8`. 2. **Output**: - The processed content should be printed to the console. 3. **Constraints**: - You must handle errors gracefully. For instance, if the specified encoding is not valid or the file does not exist, the script should print an appropriate error message. 4. **Performance**: - The script should handle files up to 10MB efficiently. Example of File Content: ``` # -*- coding: cp1252 -*- Hello, World! ``` Usage: ```sh python3.10 script.py /path/to/textfile.txt ``` Implementation Details: 1. Your script should read the first two lines of the file to check for the encoding declaration. 2. If an encoding declaration is found, read the file using the specified encoding. If no declaration is found, use `UTF-8`. 3. Print the contents of the file to the console. Additional Hints: - Use the `sys.argv` list to retrieve command-line arguments. - To handle different encodings, you can use the `open` function with the `encoding` parameter. - Use exception handling to catch and handle any errors related to file operations and encoding issues. Implement the function `process_file` which takes the file path as an argument and outputs the processed content. Sample Implementation: ```python import sys def process_file(file_path): encoding = \'UTF-8\' # Default encoding try: with open(file_path, \'r\', encoding=encoding) as f: first_line = f.readline().strip() second_line = f.readline().strip() # Look for encoding declaration if first_line.startswith(\'# -*- coding:\') and first_line.endswith(\'-*-\'): encoding = first_line.split(\':\')[1].strip().strip(\'-*-\') elif second_line.startswith(\'# -*- coding:\') and second_line.endswith(\'-*-\'): encoding = second_line.split(\':\')[1].strip().strip(\'-*-\') with open(file_path, \'r\', encoding=encoding) as f: contents = f.read() print(contents) except FileNotFoundError: print(f\\"Error: The file {file_path} does not exist.\\") except LookupError: print(f\\"Error: The encoding {encoding} is not recognized.\\") except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python3.10 script.py /path/to/textfile.txt\\") else: process_file(sys.argv[1]) ``` Question: 1. Implement the `process_file` function according to the requirements. 2. Handle any errors that may occur due to file operations or invalid encodings gracefully.","solution":"import sys import re def process_file(file_path): encoding = \'UTF-8\' # Default encoding try: with open(file_path, \'r\', encoding=encoding) as f: first_line = f.readline().strip() second_line = f.readline().strip() # Look for encoding declaration encoding_declaration = None for line in [first_line, second_line]: match = re.match(r\'#s*-*- coding: (S+) -*-\', line) if match: encoding_declaration = match.group(1) break if encoding_declaration: encoding = encoding_declaration with open(file_path, \'r\', encoding=encoding) as f: contents = f.read() print(contents) except FileNotFoundError: print(f\\"Error: The file {file_path} does not exist.\\") except LookupError: print(f\\"Error: The encoding {encoding} is not recognized.\\") except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python3.10 script.py /path/to/textfile.txt\\") else: process_file(sys.argv[1])"},{"question":"Coding Assessment Question # Objective Implement a Python function that uses the `concurrent.futures` module to perform concurrent execution of multiple tasks. The function will process a list of integers, where each integer will be squared. The task involves ensuring that the results are collected and returned correctly and efficiently. # Function Signature ```python def process_numbers_concurrently(numbers: list, use_threads: bool = True) -> list: ``` # Input - `numbers` (list): A list of integers to be squared. - `use_threads` (bool): A boolean flag indicating whether to use `ThreadPoolExecutor` (`True`) or `ProcessPoolExecutor` (`False`). Default is `True`. # Output - A list of integers where each element is the square of the corresponding element in the input `numbers` list. # Constraints - The function must use `concurrent.futures.ThreadPoolExecutor` if `use_threads` is `True`. - The function must use `concurrent.futures.ProcessPoolExecutor` if `use_threads` is `False`. - The order of the results in the output list should match the order of the input numbers. - The input list `numbers` will have at most 10,000 integers. - Each integer in the `numbers` list will be between -1,000 and 1,000. # Example ```python # Example usage numbers = [1, 2, 3, 4, 5] result = process_numbers_concurrently(numbers, use_threads=True) print(result) # Output: [1, 4, 9, 16, 25] ``` # Notes - Make sure to handle exceptions that might occur during the execution of tasks. - Ensure that the function can handle both small and large lists of numbers efficiently. - Use appropriate methods to manage the future objects and collect the results.","solution":"import concurrent.futures def square_number(n): return n * n def process_numbers_concurrently(numbers: list, use_threads: bool = True) -> list: Processes a list of integers, where each integer is squared. Uses either ThreadPoolExecutor or ProcessPoolExecutor from concurrent.futures. Args: - numbers: list of integers to be squared. - use_threads: boolean flag indicating whether to use ThreadPoolExecutor (True) or ProcessPoolExecutor (False). Default is True. Returns: - A list of integers where each element is the square of the corresponding element in the input numbers list. if use_threads: with concurrent.futures.ThreadPoolExecutor() as executor: results = list(executor.map(square_number, numbers)) else: with concurrent.futures.ProcessPoolExecutor() as executor: results = list(executor.map(square_number, numbers)) return results"},{"question":"Objective Create a customized plot using `seaborn.objects.Plot` class to visualize the relationship between two variables from a dataset, adding specific customizations to the plot. This will assess understanding of seaborn\'s advanced plotting features and customization options. Problem Statement Using the `healthexp` dataset provided by seaborn, you are required to: 1. Load the `healthexp` dataset. 2. Create a plot to show the trajectory of health expenditure (`Spending_USD`) and life expectancy (`Life_Expectancy`) for each country. 3. Customize the plot by adding a `Path` mark with: - A circular (`\\"o\\"`) marker. - Marker size of 4. - A line width of 1. - A marker fill color of \'white\'. 4. Color the trajectories by country. Constraints - Use seaborn\'s `objects.Plot` class. - Ensure that the plot is clear and readable with appropriate customization. - Your implementation should be contained in a function `create_customized_plot()` which shows the plot once called. Input and Output - Input: None (Dataset is to be loaded within the function) - Output: A displayed plot Complete the function `create_customized_plot()` to achieve the desired plot. ```python import seaborn.objects as so from seaborn import load_dataset def create_customized_plot(): # Load the dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create the plot p = so.Plot(healthexp, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", color=\\"Country\\") # Add path with customization p.add(so.Path(marker=\\"o\\", pointsize=4, linewidth=1, fillcolor=\\"w\\")) # Show the plot p.show() # Call the function to display the plot create_customized_plot() ``` Make sure to test your function to verify it generates the correct plot. Evaluation Criteria - Correct implementation of the function. - Proper use of seaborn\'s `Plot` class and `Path` mark. - Correct customizations applied to the plot. - The resulting plot should be clear and correctly formatted as per the requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_customized_plot(): # Load the dataset healthexp = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) # Create the plot p = so.Plot(healthexp, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", color=\\"Country\\") # Add path with customization p.add(so.Path(marker=\\"o\\", pointsize=4, linewidth=1, fillcolor=\\"w\\")) # Show the plot p.show()"},{"question":"**Objective**: Implement a function that reads a Python source file, tokenizes its content, identifies all function definitions, and then reconstructs a new source file with line numbers indicated as comments at the start of each line. **Task**: 1. **Implement the function `transform_source_with_line_numbers(input_filename: str, output_filename: str) -> None`:** - Open and read a Python source file specified by `input_filename`. - Tokenize the content using the `tokenize` module. - For each line in the original source: - Add a comment with the line number at the beginning of the line. - Save the modified content into a new file specified by `output_filename`. **Details**: - Use the `tokenize.open()` function to handle the source file reading. - Use the `tokenize.tokenize()` function to generate tokens. - Use the `tokenize.untokenize()` function to reconstruct the modified source code. - Ensure that the indentation and code structure remain unchanged except for the added comments. **Constraints**: - The input Python source file is guaranteed to be syntactically correct. - The line comments with line numbers should be of the form `# Line X`, where `X` is the 1-based line number. **Example**: Given a Python source file `example.py` with the following content: ```python def greet(name): print(f\\"Hello, {name}!\\") greet(\\"Alice\\") ``` The function call `transform_source_with_line_numbers(\'example.py\', \'modified_example.py\')` should result in `modified_example.py` with the following content: ```python # Line 1 def greet(name): # Line 2 print(f\\"Hello, {name}!\\") # Line 3 greet(\\"Alice\\") ``` **Input/Output**: - `input_filename` (str): The path to the input Python source file. - `output_filename` (str): The path to the output modified Python source file with line numbers. **Performance Requirements**: - The solution should process any valid Python file within reasonable limits of size and complexity (e.g., common usage scenarios). You are required to demonstrate this functionality by writing the complete function `transform_source_with_line_numbers` and providing a valid example using a sample input file.","solution":"import tokenize def transform_source_with_line_numbers(input_filename: str, output_filename: str) -> None: Reads a Python source file, tokenizes its content, identifies all function definitions, and reconstructs a new source file with line numbers indicated as comments at the start of each line. Args: input_filename (str): The path to the input Python source file. output_filename (str): The path to the output modified Python source file with line numbers. with tokenize.open(input_filename) as f: original_lines = f.readlines() with open(output_filename, \'w\') as f: for i, line in enumerate(original_lines, 1): f.write(f\'# Line {i}n\') f.write(line)"},{"question":"Objective: To assess the student\'s understanding of Python\'s `json` module, particularly in serialization, deserialization, and implementing custom encoders and decoders. Problem Statement: You are required to implement a class `CustomJSONHandler` that handles serialization and deserialization of specific Python objects to and from JSON strings. 1. Implement a custom JSON encoder `MyEncoder` that can handle the serialization of the following custom object: - **CustomObject**: A class that represents a custom object with the following attributes: - `name` (string) - `value` (integer) - `timestamp` (datetime object) 2. Implement a custom JSON decoder that can deserialize JSON strings into `CustomObject` instances. 3. Implement the class `CustomJSONHandler` that has the following methods: - `serialize_custom_object(custom_obj: CustomObject) -> str`: Serialize an instance of `CustomObject` into a JSON string using `MyEncoder`. - `deserialize_custom_object(json_str: str) -> CustomObject`: Deserialize a JSON string into an instance of `CustomObject` using the custom decoder. Specifications: - The `CustomObject` class should be implemented with the specified attributes. - The **datetime** attribute should be serialized in ISO 8601 format. - Ensure that the custom encoder and decoder can correctly handle the datetime attribute. Constraints: - Do not use any external libraries other than the `json` and `datetime` modules. - The solution should handle typical edge cases such as invalid JSON strings gracefully. Example: ```python from datetime import datetime import json class CustomObject: def __init__(self, name, value, timestamp): self.name = name self.value = value self.timestamp = timestamp class MyEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, CustomObject): return {\\"name\\": obj.name, \\"value\\": obj.value, \\"timestamp\\": obj.timestamp.isoformat()} if isinstance(obj, datetime): return obj.isoformat() return json.JSONEncoder.default(self, obj) def custom_decoder(dct): if \'name\' in dct and \'value\' in dct and \'timestamp\' in dct: from datetime import datetime return CustomObject(dct[\'name\'], dct[\'value\'], datetime.fromisoformat(dct[\'timestamp\'])) return dct class CustomJSONHandler: @staticmethod def serialize_custom_object(custom_obj: CustomObject) -> str: return json.dumps(custom_obj, cls=MyEncoder) @staticmethod def deserialize_custom_object(json_str: str) -> CustomObject: return json.loads(json_str, object_hook=custom_decoder) # Example Usage obj = CustomObject(\\"example\\", 100, datetime.now()) json_str = CustomJSONHandler.serialize_custom_object(obj) print(json_str) deserialized_obj = CustomJSONHandler.deserialize_custom_object(json_str) print(deserialized_obj.name, deserialized_obj.value, deserialized_obj.timestamp) ``` Evaluation: - **Correctness**: The solution must correctly serialize and deserialize instances of `CustomObject`. - **Code Quality**: The solution should follow good coding practices including readability, proper naming conventions, and error handling. - **Complexity**: The solution should be efficient and handle typical edge cases.","solution":"from datetime import datetime import json class CustomObject: def __init__(self, name, value, timestamp): self.name = name self.value = value self.timestamp = timestamp class MyEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, CustomObject): return { \\"name\\": obj.name, \\"value\\": obj.value, \\"timestamp\\": obj.timestamp.isoformat() } return super().default(obj) def custom_decoder(dct): if \'name\' in dct and \'value\' in dct and \'timestamp\' in dct: return CustomObject( dct[\'name\'], dct[\'value\'], datetime.fromisoformat(dct[\'timestamp\']) ) return dct class CustomJSONHandler: @staticmethod def serialize_custom_object(custom_obj: CustomObject) -> str: return json.dumps(custom_obj, cls=MyEncoder) @staticmethod def deserialize_custom_object(json_str: str) -> CustomObject: return json.loads(json_str, object_hook=custom_decoder)"},{"question":"# Custom Codec Implementation and Utilization **Objective:** You are tasked with implementing a custom codec and using it to encode and decode text, demonstrating your understanding of the codec-related functions in Python. **Instructions:** 1. **Implement a Custom Codec:** - Write a function `register_custom_codec` that registers a new codec search function. - This custom codec should: - Reverse the text when encoding. - Reverse the text back to its original form when decoding. 2. **Utilize the Custom Codec:** - Write a function `encode_text` that takes a string and encodes it using the custom codec. - Write a function `decode_text` that takes an encoded string and decodes it back to the original string using the custom codec. **Constraints:** - You must define and register the custom codec within your solution. - The custom codec should be registered under the name \\"reverse_codec\\". **Input/Output Format:** - `register_custom_codec()` should register the codec without any return value. - `encode_text(text: str) -> str` should take a plain text string and return an encoded string. - `decode_text(encoded_text: str) -> str` should take an encoded string and return the original text. **Example:** ```python # Register the custom codec register_custom_codec() # Encode text original_text = \\"hello world\\" encoded_text = encode_text(original_text) print(encoded_text) # Output should be \\"dlrow olleh\\" # Decode text decoded_text = decode_text(encoded_text) print(decoded_text) # Output should be \\"hello world\\" ``` Write your implementation below: ```python def register_custom_codec(): # Implementation for registering the custom codec pass def encode_text(text: str) -> str: # Implementation for encoding text using the custom codec pass def decode_text(encoded_text: str) -> str: # Implementation for decoding text using the custom codec pass # Example usage register_custom_codec() original_text = \\"hello world\\" encoded_text = encode_text(original_text) print(encoded_text) # Should print \\"dlrow olleh\\" decoded_text = decode_text(encoded_text) print(decoded_text) # Should print \\"hello world\\" ```","solution":"import codecs class ReverseCodec(codecs.Codec): def encode(self, input, errors=\'strict\'): return input[::-1], len(input) def decode(self, input, errors=\'strict\'): return input[::-1], len(input) def reverse_search_function(encoding): if encoding == \'reverse_codec\': return codecs.CodecInfo( name=\'reverse_codec\', encode=ReverseCodec().encode, decode=ReverseCodec().decode ) return None def register_custom_codec(): codecs.register(reverse_search_function) def encode_text(text: str) -> str: return codecs.encode(text, \'reverse_codec\') def decode_text(encoded_text: str) -> str: return codecs.decode(encoded_text, \'reverse_codec\') # Example usage register_custom_codec() original_text = \\"hello world\\" encoded_text = encode_text(original_text) print(encoded_text) # Should print \\"dlrow olleh\\" decoded_text = decode_text(encoded_text) print(decoded_text) # Should print \\"hello world\\""},{"question":"**Objective**: Assess understanding of seaborn for creating complex, customized visualizations with statistical and categorical data. **Question**: You are provided with the \\"penguins\\" dataset, which contains measurements for penguins from different species. Your task is to create a comprehensive visualization using seaborn that displays the relationships between `flipper_length_mm`, `bill_length_mm`, and `body_mass_g`, categorized by `species`. **Steps**: 1. **Load the Dataset**: - Import seaborn and pandas. - Load the \\"penguins\\" dataset using `sns.load_dataset(\\"penguins\\")`. 2. **Create a Visualization**: - Create a scatter plot using `sns.relplot`. - Map `flipper_length_mm` to the x-axis and `bill_length_mm` to the y-axis. - Use `hue` to differentiate by `species` and `size` to represent `body_mass_g`. - Split the data into subplots by `species` using the `col` parameter. 3. **Customize the Plot**: - Set a `darkgrid` theme using `sns.set_theme`. - Customize the plot such that: - Add appropriate axis labels with units for clarity. - Set the plot title to \\"Penguin Measurements\\". - Add a legend titled \\"Body mass (g)\\". - Set the figure size to 12x8 inches. - Ensure that the axes margins provide enough space for the data points. 4. **Enhance the Visualization**: - Plot the regression line for each scatter plot using `sns.lmplot`. - Customize the style and color of the regression lines for readability. ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Step 1: Load the Dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Create a Visualization sns.set_theme(style=\\"darkgrid\\") plot = sns.relplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", size=\\"body_mass_g\\", col=\\"species\\" ) # Step 3: Customize the Plot plot.set_axis_labels(\\"Flipper Length (mm)\\", \\"Bill Length (mm)\\", labelpad=10) plot.fig.suptitle(\\"Penguin Measurements\\", y=1.02) plot.figure.set_size_inches(12, 8) plot.set_titles(\\"{col_name} species\\") plot.legend.set_title(\\"Body mass (g)\\") # Step 4: Enhance the Visualization sns.lmplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", col=\\"species\\", scatter=False, line_kws={\\"linestyle\\":\\"--\\"} ) plt.show() ``` **Constraints**: - Ensure the plot is clear and readable. - Handle any missing data appropriately. - Use matplotlib where necessary to refine the plot beyond seaborn\'s capabilities. **Expected Output**: A multi-faceted visualization showing scatter plots with regression lines for each penguin species, detailing the relationships between flipper length, bill length, and body mass, all clearly labeled and well-formatted.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_penguin_visualization(): # Step 1: Load the Dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Create a Visualization sns.set_theme(style=\\"darkgrid\\") plot = sns.relplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", size=\\"body_mass_g\\", col=\\"species\\" ) # Step 3: Customize the Plot plot.set_axis_labels(\\"Flipper Length (mm)\\", \\"Bill Length (mm)\\", labelpad=10) plot.fig.suptitle(\\"Penguin Measurements\\", y=1.02) plot.figure.set_size_inches(12, 8) plot.set_titles(\\"{col_name} species\\") plot.legend.set_title(\\"Body mass (g)\\") # Step 4: Enhance the Visualization sns.lmplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", col=\\"species\\", scatter=False, line_kws={\\"linestyle\\":\\"--\\"} ) plt.show() # Example run if __name__ == \'__main__\': create_penguin_visualization()"},{"question":"# POP3 Email Client Task You are required to implement a simple email client using the `poplib` module to interact with a POP3 email server. The client should be able to connect to the server securely using SSL, authenticate the user, list emails in the mailbox, and retrieve the content of a specific email. If there are any errors during these operations, they should be handled gracefully, and an appropriate message should be displayed to the user. Requirements: 1. **Connection:** - Establish a connection to the POP3 email server using SSL. - Use the `POP3_SSL` class for the connection. 2. **Authentication:** - Authenticate the user using the provided username and password. 3. **List Emails:** - List all the emails in the mailbox with their unique IDs and sizes. 4. **Retrieve Email Content:** - Retrieve and print the content of a specific email given its unique ID. 5. **Error Handling:** - Handle and report errors such as connection issues, invalid credentials, and other possible `poplib.error_proto` exceptions. 6. **Performance:** - Ensure that the connection times out if it takes too long (use a timeout value of your choice). Input Specification: - The user should provide: - `hostname` (string): The hostname of the POP3 server. - `port` (int): The port number for the POP3 over SSL (default is 995). - `username` (string): The username for the email account. - `password` (string): The password for the email account. - `email_id` (int): The ID of the email to retrieve. Output Specification: - Display the list of emails with their IDs and sizes. - Print the content of the specified email. - Display appropriate error messages for any issues encountered during the process. Example Usage: ```python def main(): hostname = \'pop.example.com\' port = 995 username = \'your_username\' password = \'your_password\' email_id = 1 try: pop_client = EmailClientSSL(hostname, port) pop_client.authenticate(username, password) pop_client.list_emails() pop_client.retrieve_email_content(email_id) except Exception as e: print(\\"An error occurred:\\", e) class EmailClientSSL: def __init__(self, hostname, port): # Initialize the connection with the POP3 server using SSL pass def authenticate(self, username, password): # Authenticate the user with the provided credentials pass def list_emails(self): # List all emails in the mailbox with their IDs and sizes pass def retrieve_email_content(self, email_id): # Retrieve and print the content of the specified email pass if __name__ == \\"__main__\\": main() ``` Implement the `EmailClientSSL` class methods `__init__`, `authenticate`, `list_emails`, and `retrieve_email_content` to meet the requirements specified above.","solution":"import poplib from poplib import error_proto class EmailClientSSL: def __init__(self, hostname, port=995, timeout=10): Initialize the connection with the POP3 server using SSL. self.hostname = hostname self.port = port self.timeout = timeout self.connection = None def connect(self): try: self.connection = poplib.POP3_SSL(self.hostname, self.port, timeout=self.timeout) print(\\"Connected to the server.\\") except (ConnectionRefusedError, error_proto) as e: print(f\\"Failed to connect to the server: {e}\\") return False return True def authenticate(self, username, password): try: self.connection.user(username) self.connection.pass_(password) print(\\"Authenticated successfully.\\") except error_proto as e: print(f\\"Authentication failed: {e}\\") return False return True def list_emails(self): try: email_list = self.connection.list() email_count = len(email_list[1]) print(f\\"Number of messages: {email_count}\\") for email in email_list[1]: print(email) except error_proto as e: print(f\\"Failed to list emails: {e}\\") return False return True def retrieve_email_content(self, email_id): try: response, lines, octets = self.connection.retr(email_id) msg_content = \\"n\\".join(line.decode(\'utf-8\') for line in lines) print(\\"Email content:\\") print(msg_content) except error_proto as e: print(f\\"Failed to retrieve email content: {e}\\") return False return True def disconnect(self): if self.connection: self.connection.quit() print(\\"Disconnected from the server.\\") def main(): hostname = \'pop.example.com\' port = 995 username = \'your_username\' password = \'your_password\' email_id = 1 client = EmailClientSSL(hostname, port) if client.connect(): if client.authenticate(username, password): if client.list_emails(): client.retrieve_email_content(email_id) client.disconnect() if __name__ == \\"__main__\\": main()"},{"question":"Objective: To assess your understanding of Python\'s cryptographic services, specifically the `hashlib` and `hmac` modules, you will implement a secure password storage and verification system. This system should include functionality for hashing passwords using HMAC (Keyed-Hash Message Authentication Code) and verifying if a given password matches the stored hash. Additionally, it should support generating a secure random salt for each password to enhance security. Function Requirements: **Function 1: `generate_salt()`** - **Input:** None - **Output:** A random 16-byte hexadecimal string. - **Description:** This function should use the `secrets` module to generate a secure random salt that will be used in the password hashing process. **Function 2: `hash_password(password: str, salt: str, key: str) -> str`** - **Input:** - `password`: The password string to be hashed. - `salt`: The salt string generated by `generate_salt()`. - `key`: A secret key string used for HMAC. - **Output:** A string containing the hexadecimal HMAC hash of the password combined with the salt. - **Description:** This function should use the `hmac` and `hashlib` modules to generate a secure hash of the password using the provided salt and key. **Function 3: `verify_password(stored_hash: str, password: str, salt: str, key: str) -> bool`** - **Input:** - `stored_hash`: The previously stored HMAC hash. - `password`: The input password string to verify. - `salt`: The salt string used during the original password hash generation. - `key`: The same secret key string used during the original password hash generation. - **Output:** A boolean indicating whether the input password matches the stored hash. - **Description:** This function should verify if the provided password, when hashed with the same salt and key, matches the stored hash. Example Usage: ```python # Example workflow salt = generate_salt() key = \\"super_secret_key\\" # Store a new password stored_hash = hash_password(\\"my_secure_password\\", salt, key) # Verify the password is_valid = verify_password(stored_hash, \\"my_secure_password\\", salt, key) print(is_valid) # Should print: True # Verify an incorrect password is_valid = verify_password(stored_hash, \\"wrong_password\\", salt, key) print(is_valid) # Should print: False ``` Constraints: - Do not use any external libraries outside of the Python Standard Library. - Ensure that the salt and key are managed securely and not hardcoded in a real application. Notes: - Use the `secrets` module for generating the salt. - The `hashlib` and `hmac` modules should be used for hashing and keyed hashing, respectively.","solution":"import hmac import hashlib import secrets def generate_salt(): Generates a random 16-byte hexadecimal salt. Returns: str: A random 16-byte hexadecimal string. return secrets.token_hex(16) def hash_password(password, salt, key): Generates a HMAC hash of the given password using the provided salt and key. Args: password (str): The password to be hashed. salt (str): The salt to be used in hashing. key (str): The secret key to be used for HMAC. Returns: str: The HMAC hash as a hexadecimal string. hashed = hmac.new(key.encode(), (salt + password).encode(), hashlib.sha256) return hashed.hexdigest() def verify_password(stored_hash, password, salt, key): Verifies if the provided password, when hashed with the same salt and key, matches the stored hash. Args: stored_hash (str): The previously stored HMAC hash. password (str): The input password to verify. salt (str): The salt used during the original password hash generation. key (str): The same secret key used during the original password hash generation. Returns: bool: True if the input password matches the stored hash, False otherwise. calculated_hash = hash_password(password, salt, key) return hmac.compare_digest(stored_hash, calculated_hash)"},{"question":"**Question: Implement a Custom Flexible Attention Mechanism with Dynamic Blocks** In this task, you are required to implement a custom attention mechanism using PyTorch. Your task is to create a flexible attention mechanism that leverages dynamic blocks and mask utility functions provided by the `torch.nn.attention.flex_attention` module. # Requirements: 1. **Function Definition**: Implement a function `custom_flexible_attention` that has the following signature: ```python def custom_flexible_attention(query, key, value, block_shape): Params: - query (torch.Tensor): Tensor of shape (batch_size, seq_len, dim). - key (torch.Tensor): Tensor of shape (batch_size, seq_len, dim). - value (torch.Tensor): Tensor of shape (batch_size, seq_len, dim). - block_shape (tuple): A tuple defining the block shape, e.g., (block_height, block_width). Returns: - output (torch.Tensor): Tensor of shape (batch_size, seq_len, dim). ``` 2. **Mask Application**: Use the block mask utilities to dynamically create masks based on the input shapes and apply them to the attention scores. 3. **Attention Computation**: - Compute raw attention scores using the query and key tensors. - Apply the generated masks to the attention scores. - Use the masked attention scores to compute weighted sum over the values. 4. **Constraints**: - Ensure the block_shape dimensions are valid given the input tensor dimensions. - Include error handling for incompatible dimensions. 5. **Testing Function**: Provide a testing code snippet demonstrating the usage of `custom_flexible_attention` with sample inputs. # Example Usage: ```python import torch from torch.nn.attention.flex_attention import create_block_mask # Assuming `custom_flexible_attention` is implemented: batch_size = 2 seq_len = 10 dim = 4 block_shape = (2, 2) query = torch.randn(batch_size, seq_len, dim) key = torch.randn(batch_size, seq_len, dim) value = torch.randn(batch_size, seq_len, dim) output = custom_flexible_attention(query, key, value, block_shape) print(\\"Output shape:\\", output.shape) # Expected: torch.Size([batch_size, seq_len, dim]) ``` **Note**: Use the block mask utility functions as necessary. # Grading Criteria: - Correctness: Function produces the correct output and handles edge cases properly. - Code Quality: Code is clean, well-documented, and follows Python best practices. - Efficiency: Solution is optimized for performance and avoids unnecessary computations.","solution":"import torch import torch.nn.functional as F def create_block_mask(tensor, block_shape): Function to create a dynamic block mask for the attention mechanism. Params: - tensor (torch.Tensor): Tensor to create a block mask for. - block_shape (tuple): Shape of the block (block_height, block_width). Returns: - mask (torch.Tensor): Block mask. batch_size, seq_len, _ = tensor.size() mask = torch.ones((batch_size, seq_len, seq_len), dtype=torch.bool) num_blocks = seq_len // block_shape[0] for i in range(num_blocks): for j in range(num_blocks): mask[:, i*block_shape[0]:(i+1)*block_shape[0], j*block_shape[1]:(j+1)*block_shape[1]] = False return mask def custom_flexible_attention(query, key, value, block_shape): Custom flexible attention mechanism. Params: - query (torch.Tensor): Tensor of shape (batch_size, seq_len, dim). - key (torch.Tensor): Tensor of shape (batch_size, seq_len, dim). - value (torch.Tensor): Tensor of shape (batch_size, seq_len, dim). - block_shape (tuple): A tuple defining the block shape, e.g., (block_height, block_width). Returns: - output (torch.Tensor): Tensor of shape (batch_size, seq_len, dim). # Check dimensions batch_size, seq_len, dim = query.size() assert key.size() == (batch_size, seq_len, dim), \\"Key tensor has mismatched dimensions.\\" assert value.size() == (batch_size, seq_len, dim), \\"Value tensor has mismatched dimensions.\\" # Compute attention scores attention_scores = torch.bmm(query, key.transpose(1, 2)) / dim ** 0.5 # Create block mask mask = create_block_mask(attention_scores, block_shape) # Apply mask to attention scores attention_scores.masked_fill_(mask, float(\'-inf\')) # Apply softmax to get attention probabilities attention_probabilities = F.softmax(attention_scores, dim=-1) # Compute attention output output = torch.bmm(attention_probabilities, value) return output"},{"question":"# Advanced Programming Assessment with \\"unittest\\" Module **Objective**: Design and implement a Python class to manage a virtual library system. This class should support adding and borrowing books. Write unit tests using the `unittest` module to ensure the correctness of the library system. **Task**: Implement a class `Library` with the following functionalities: 1. **Add a Book**: Adds a book to the library with a unique identifier. If the book already exists, do not add it again. 2. **Borrow a Book**: Borrows a book based on its identifier. If the book does not exist or is already borrowed, raise an appropriate exception. Implement unit tests in a separate class inheriting from `unittest.TestCase` to test your `Library` class. **Function Specifications**: Library Class ```python class Library: def __init__(self): Initialize an empty library. pass def add_book(self, book_id: str, title: str): Add a book to the library. :param book_id: Unique identifier for the book. :param title: Title of the book. :raises ValueError: If the book already exists. pass def borrow_book(self, book_id: str) -> str: Borrow a book from the library. :param book_id: Unique identifier for the book. :return: Title of the borrowed book. :raises ValueError: If the book does not exist or is already borrowed. pass ``` Unit Tests Create a test suite using `unittest` to verify the functionality of the `Library` class, ensuring proper handling of edge cases and expected failures. Example test cases: 1. Adding a new book successfully. 2. Adding a book that already exists (expecting an error). 3. Borrowing a book that exists. 4. Borrowing a book that does not exist (expecting an error). 5. Borrowing a book that is already borrowed (expecting an error). The testing class structure should include: ```python import unittest class TestLibrary(unittest.TestCase): def setUp(self): pass def tearDown(self): pass def test_add_book_success(self): pass def test_add_book_duplicate(self): pass def test_borrow_book_success(self): pass def test_borrow_book_nonexistent(self): pass def test_borrow_book_already_borrowed(self): pass if __name__ == \'__main__\': unittest.main() ``` # Constraints and Requirements - Ensure proper management of book identifiers and titles. - Handle exceptions appropriately and validate the exceptions in your tests. - The solution should be efficient and handle a reasonable number of books. # Evaluation Criteria - Correctness and completeness of the implementation. - Thoroughness and robustness of the unit tests. - Proper use of Python’s `unittest` module features for setup, teardown, and assertions.","solution":"class Library: def __init__(self): Initialize an empty library. self.books = {} self.borrowed_books = set() def add_book(self, book_id: str, title: str): Add a book to the library. :param book_id: Unique identifier for the book. :param title: Title of the book. :raises ValueError: If the book already exists. if book_id in self.books: raise ValueError(f\\"Book with ID {book_id} already exists.\\") self.books[book_id] = title def borrow_book(self, book_id: str) -> str: Borrow a book from the library. :param book_id: Unique identifier for the book. :return: Title of the borrowed book. :raises ValueError: If the book does not exist or is already borrowed. if book_id not in self.books: raise ValueError(f\\"Book with ID {book_id} does not exist.\\") if book_id in self.borrowed_books: raise ValueError(f\\"Book with ID {book_id} is already borrowed.\\") self.borrowed_books.add(book_id) return self.books[book_id]"},{"question":"# SSL/TLS Secure Server and Client Communication **Objective:** Create a Python program that demonstrates secure communication between a server and a client using the `ssl` module. This exercise will test your ability to manage SSL/TLS contexts, load certificates, handle socket connections, and manage errors effectively. **Task:** 1. **Create SSL/TLS Contexts:** - Create an SSL/TLS context for the server with the protocol `PROTOCOL_TLS_SERVER`. - Create an SSL/TLS context for the client with the protocol `PROTOCOL_TLS_CLIENT`. 2. **Server Setup:** - The server should load a certificate and private key from provided PEM files (`server_cert.pem` and `server_key.pem`). - The server should also load CA certificates for verification (`ca_cert.pem`). - The server should listen on localhost and a provided port (e.g., 8443). - Use `SSLContext.wrap_socket` method to wrap the server socket. 3. **Client Setup:** - The client should load CA certificates for verification (`ca_cert.pem`). - Use `SSLContext.wrap_socket` method to wrap the client socket. - The client should connect to the server and perform a handshake. 4. **Data Exchange:** - Once connected, the client should send a message (e.g., \\"Hello, Server!\\") to the server. - The server should receive the message, respond with another message (e.g., \\"Hello, Client!\\"), and send it back to the client. - Ensure that both communications are encrypted using SSL/TLS. 5. **Error Handling:** - Implement proper error handling for SSL/TLS errors (e.g., certificate validation errors, handshake failures). **Input:** - Paths to the server\'s certificate and private key (`server_cert.pem`, `server_key.pem`). - Path to the CA certificate (`ca_cert.pem`). - Port number for the server to listen on. **Output:** - The server prints the received message from the client. - The client prints the received message from the server. **Constraints:** - Use Python 3.10. - Ensure that the client and server handle SSL/TLS errors gracefully and terminate the communication properly on error. **Example Usage:** ```python # Paths to certificate files server_cert = \'path/to/server_cert.pem\' server_key = \'path/to/server_key.pem\' ca_cert = \'path/to/ca_cert.pem\' port = 8443 # Run server (in a separate process or thread) run_ssl_server(server_cert, server_key, ca_cert, port) # Run client run_ssl_client(ca_cert, \'localhost\', port) ``` # Functions to Implement ```python import ssl import socket def run_ssl_server(server_cert, server_key, ca_cert, port): Sets up an SSL/TLS server that listens on the given port, accepts client connections, receives a message from the client, and sends a response back. Args: - server_cert (str): Path to the server\'s certificate file. - server_key (str): Path to the server\'s private key file. - ca_cert (str): Path to the CA certificates file. - port (int): The port number to listen on. # Implement server setup and communication def run_ssl_client(ca_cert, hostname, port): Sets up an SSL/TLS client that connects to the given hostname and port, sends a message to the server, and prints the server\'s response. Args: - ca_cert (str): Path to the CA certificates file. - hostname (str): The hostname of the server to connect to. - port (int): The port number to connect to. # Implement client setup and communication ``` **Note:** Ensure the server is running before starting the client for successful communication. Handle all exceptions properly and provide meaningful error messages for debugging.","solution":"import ssl import socket import threading def run_ssl_server(server_cert, server_key, ca_cert, port): context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER) context.load_cert_chain(certfile=server_cert, keyfile=server_key) context.load_verify_locations(cafile=ca_cert) context.verify_mode = ssl.CERT_REQUIRED bindsocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) bindsocket.bind((\'localhost\', port)) bindsocket.listen(5) def handle_client(conn, addr): print(\'Connection from:\', addr) ssl_conn = context.wrap_socket(conn, server_side=True) try: data = ssl_conn.recv(1024) print(\'Received from client:\', data.decode()) ssl_conn.send(b\'Hello, Client!\') except ssl.SSLError as e: print(\'SSL error:\', str(e)) finally: ssl_conn.shutdown(socket.SHUT_RDWR) ssl_conn.close() while True: newsocket, fromaddr = bindsocket.accept() threading.Thread(target=handle_client, args=(newsocket, fromaddr)).start() def run_ssl_client(ca_cert, hostname, port): context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT) context.load_verify_locations(cafile=ca_cert) with socket.create_connection((hostname, port)) as sock: with context.wrap_socket(sock, server_hostname=hostname) as ssock: try: ssock.sendall(b\'Hello, Server!\') data = ssock.recv(1024) print(\'Received from server:\', data.decode()) except ssl.SSLError as e: print(\'SSL error:\', str(e))"},{"question":"# Persistent Data Management with `shelve` You are required to implement several functions for managing user preferences in a persistent storage solution using Python\'s `shelve` module. Your task is to develop a small system that allows users to store, retrieve, modify, and delete their preferences persistently. # Requirements: 1. **Implement a function to initialize the preferences storage:** ```python def init_prefs(filename: str): Initializes the persistent storage using the given filename. Args: filename (str): The base filename for the underlying database. Returns: None ``` 2. **Implement a function to add or update a user preference:** ```python def set_preference(filename: str, user: str, preference: dict): Adds or updates the user preference in the persistent storage. Args: filename (str): The base filename for the underlying database. user (str): The username as the key. preference (dict): A dictionary containing the user\'s preferences. Returns: None ``` 3. **Implement a function to retrieve a user preference:** ```python def get_preference(filename: str, user: str) -> dict: Retrieves the preference for the specified user. Args: filename (str): The base filename for the underlying database. user (str): The username as the key. Returns: dict: The user\'s preferences if they exist, otherwise an empty dictionary. ``` 4. **Implement a function to delete a user preference:** ```python def delete_preference(filename: str, user: str): Deletes the preference for the specified user. Args: filename (str): The base filename for the underlying database. user (str): The username as the key. Returns: None ``` 5. **Implement a function to close the preferences storage:** ```python def close_prefs(filename: str): Closes the persistent storage. Args: filename (str): The base filename for the underlying database. Returns: None ``` # Constraints: - The `filename` will be a string with a valid file path. - The `user` will be a non-empty string. - The `preference` will be a dictionary containing simple serializable key-value pairs (strings, integers, etc.). # Performance Requirements: - The operations should be efficient with respect to on-disk and in-memory modifications, especially considering the behavior of the `writeback` parameter in `shelve`. # Example Usage: ```python # Initialize the preferences storage init_prefs(\'user_prefs\') # Set some user preferences set_preference(\'user_prefs\', \'alice\', {\'theme\': \'dark\', \'notifications\': True}) set_preference(\'user_prefs\', \'bob\', {\'theme\': \'light\', \'notifications\': False}) # Retrieve user preferences print(get_preference(\'user_prefs\', \'alice\')) # Output: {\'theme\': \'dark\', \'notifications\': True} print(get_preference(\'user_prefs\', \'charlie\')) # Output: {} # Delete user preferences delete_preference(\'user_prefs\', \'bob\') # Attempt to retrieve the deleted preference print(get_preference(\'user_prefs\', \'bob\')) # Output: {} # Close preferences storage close_prefs(\'user_prefs\') ``` # Note: Ensure to handle potential exceptions, such as `KeyError` for missing keys, and always close the shelve storage properly to avoid data corruption.","solution":"import shelve def init_prefs(filename: str): Initializes the persistent storage using the given filename. Args: filename (str): The base filename for the underlying database. Returns: None # By opening and immediately closing, the shelve file will be created if it doesn\'t exist. with shelve.open(filename) as db: pass def set_preference(filename: str, user: str, preference: dict): Adds or updates the user preference in the persistent storage. Args: filename (str): The base filename for the underlying database. user (str): The username as the key. preference (dict): A dictionary containing the user\'s preferences. Returns: None with shelve.open(filename, writeback=True) as db: db[user] = preference def get_preference(filename: str, user: str) -> dict: Retrieves the preference for the specified user. Args: filename (str): The base filename for the underlying database. user (str): The username as the key. Returns: dict: The user\'s preferences if they exist, otherwise an empty dictionary. with shelve.open(filename) as db: return dict(db.get(user, {})) def delete_preference(filename: str, user: str): Deletes the preference for the specified user. Args: filename (str): The base filename for the underlying database. user (str): The username as the key. Returns: None with shelve.open(filename, writeback=True) as db: if user in db: del db[user] def close_prefs(filename: str): Closes the persistent storage. Args: filename (str): The base filename for the underlying database. Returns: None # No specific action needed to \'close\' as shelve is used in context manager with shelve.open(filename) as db: pass"},{"question":"**Problem Statement** You are tasked with creating a series of plots to visualize a dataset using seaborn to demonstrate your understanding of different types of color palettes. You will use the seaborn `load_dataset` function to load the `tips` dataset, which contains information about restaurant tips. **Instructions:** 1. Load the `tips` dataset using seaborn. 2. Create three different plots, each utilizing a different type of color palette: qualitative, sequential, and diverging. Use seaborn\'s built-in palette functions demonstrated in the documentation. **Detailed Requirements:** 1. **Qualitative Plot:** - Plot a bar chart showing the mean tip amount grouped by day of the week. - Use a qualitative color palette (use `color_palette` or a specific named palette). - Add a title to the plot: \\"Mean Tip Amount by Day (Qualitative Palette)\\". 2. **Sequential Plot:** - Create a KDE plot for the total bill amount. - Use a sequential color palette. - Add a title to the plot: \\"Total Bill Amount Distribution (Sequential Palette)\\". 3. **Diverging Plot:** - Plot a scatter plot of `total_bill` vs. `tip`, colored by `size` (party size). - Use a diverging color palette. - Add a title to the plot: \\"Total Bill vs Tip by Party Size (Diverging Palette)\\". **Constraints:** - Ensure that the plots are clear and aesthetically pleasing. - Use appropriate function parameters to customize the color palettes as needed. - All plots must include a title and axis labels for clarity. **Input and Output:** - No input data is required from the user; the dataset is to be loaded internally. - The output should be three visualizations displayed inline (if using Jupyter Notebook) or saved as images. **Example Code Structure:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset data = sns.load_dataset(\\"tips\\") # Qualitative Plot plt.figure(figsize=(8, 6)) # [Your code here for qualitative plot] plt.title(\\"Mean Tip Amount by Day (Qualitative Palette)\\") plt.show() # Sequential Plot plt.figure(figsize=(8, 6)) # [Your code here for sequential plot] plt.title(\\"Total Bill Amount Distribution (Sequential Palette)\\") plt.show() # Diverging Plot plt.figure(figsize=(8, 6)) # [Your code here for diverging plot] plt.title(\\"Total Bill vs Tip by Party Size (Diverging Palette)\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset data = sns.load_dataset(\\"tips\\") def plot_qualitative(): plt.figure(figsize=(8, 6)) sns.barplot(x=\'day\', y=\'tip\', data=data, estimator=\'mean\', palette=\'Set1\') plt.title(\\"Mean Tip Amount by Day (Qualitative Palette)\\") plt.xlabel(\\"Day of Week\\") plt.ylabel(\\"Mean Tip Amount\\") plt.show() def plot_sequential(): plt.figure(figsize=(8, 6)) sns.kdeplot(data[\'total_bill\'], shade=True, palette=\'Blues\') plt.title(\\"Total Bill Amount Distribution (Sequential Palette)\\") plt.xlabel(\\"Total Bill Amount\\") plt.ylabel(\\"Density\\") plt.show() def plot_diverging(): plt.figure(figsize=(8, 6)) scatter_plot = sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'size\', palette=\'coolwarm\', data=data) scatter_plot.legend(title=\'Party Size\') plt.title(\\"Total Bill vs Tip by Party Size (Diverging Palette)\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.show()"},{"question":"Objective Implement a Python function that evaluates and processes an expression consisting of mathematical and logical operations provided as a list of tuples. Description You need to implement the function `process_operations(operations: List[Tuple[str, Any]]) -> Any`, where each tuple in the `operations` list represents an operation and its operands. Your task is to evaluate these operations sequentially. The operations are defined with their corresponding `operator` module function names. Parameters - `operations`: A list of tuples, where each tuple\'s first element is a string representing the operator function\'s name (e.g., \'add\', \'mul\', \'and_\', \'eq\') and the subsequent elements are the operands for that operation. The operands can be numbers or the results of previous operations. Requirements 1. Evaluate each operation and its operands in the order they appear in the list. 2. Support binary mathematical operations (`add`, `sub`, `mul`, `truediv`, etc.), comparison operations (`lt`, `le`, `eq`, etc.), logical operations (`and_`, `or_`, `not_`, etc.), and in-place operations (`iadd`, `imul`, etc.). 3. Return the final result of the last operation. Constraints - The operations list will have at least one operation. - The first operation\'s operands will be numerical or Boolean literals. - Ensure correct handling for both integer and floating-point operations. Example ```python from operator import add, mul, sub, truediv, eq, ne, and_, or_ def process_operations(operations): result = None op_map = { \'add\': add, \'sub\': sub, \'mul\': mul, \'truediv\': truediv, \'eq\': eq, \'ne\': ne, \'and_\': and_, \'or_\': or_, } for operation in operations: op_name = operation[0] op_func = op_map[op_name] if len(operation) == 3: result = op_func(operation[1], operation[2]) elif len(operation) == 2: result = op_func(operation[1]) return result # Example usage: operations = [ (\\"add\\", 10, 5), (\\"mul\\", 3), ] # The result should be (10 + 5) * 3 = 45 print(process_operations(operations)) # Output: 45 ``` Note 1. Use the `operator` module\'s functions within your implementation. 2. Handle exceptions and invalid operations gracefully.","solution":"from operator import add, sub, mul, truediv, eq, ne, lt, le, gt, ge, and_, or_, not_, iadd, imul from typing import List, Tuple, Any def process_operations(operations: List[Tuple[str, Any]]) -> Any: result = None op_map = { \'add\': add, \'sub\': sub, \'mul\': mul, \'truediv\': truediv, \'eq\': eq, \'ne\': ne, \'lt\': lt, \'le\': le, \'gt\': gt, \'ge\': ge, \'and_\': and_, \'or_\': or_, \'not_\': not_, \'iadd\': iadd, \'imul\': imul } for operation in operations: op_name = operation[0] op_func = op_map[op_name] operands = operation[1:] if result is None: if len(operands) == 1: result = op_func(operands[0]) else: result = op_func(operands[0], operands[1]) else: result = op_func(result, *operands) return result"},{"question":"Coding Assessment Question # Objective To assess your understanding of Python\'s handling of environment variables using the \\"os\\" module, which provides POSIX interface functionalities. You will implement a function that manipulates environment variables and interacts with the system. # Problem Statement You are required to write a function `process_environment` that performs the following tasks: 1. Retrieves the current value of the `HOME` environment variable and stores it in a variable called `original_home`. 2. Sets a new environment variable `TEST_ENV` to the value \\"Python310\\". 3. Verifies that `TEST_ENV` has been set correctly. 4. Deletes the `TEST_ENV` environment variable. 5. Verifies that `TEST_ENV` has been deleted. # Function Signature ```python def process_environment() -> tuple: # Your code here ``` # Expected Input and Output Formats - **Input:** None. - **Output:** A tuple `(original_home, test_env_before_delete, test_env_after_delete)`, where: - `original_home` (str or None) is the value of the `HOME` environment variable if it exists, otherwise `None`. - `test_env_before_delete` (str or None) is the value of the `TEST_ENV` environment variable before it is deleted, which should be \\"Python310\\". - `test_env_after_delete` (None) is the value of the `TEST_ENV` environment variable after deletion, which should be `None`. # Example ```python output = process_environment() print(output) # Expected Output: # (\'/home/user\', \'Python310\', None) # The exact value of original_home will depend on the system ``` # Constraints - You should use the `os` module for environment variable manipulation. - Ensure that your function works correctly across different operating systems (e.g., Unix, Windows). # Notes - The function should handle cases where the `HOME` environment variable might not be set. - Use the `os.environ` dictionary for environment variable operations.","solution":"import os def process_environment() -> tuple: # Retrieve the current value of the HOME environment variable original_home = os.environ.get(\'HOME\') # Set a new environment variable TEST_ENV to the value \\"Python310\\" os.environ[\'TEST_ENV\'] = \'Python310\' # Verify that TEST_ENV has been set correctly test_env_before_delete = os.environ.get(\'TEST_ENV\') # Delete the TEST_ENV environment variable del os.environ[\'TEST_ENV\'] # Verify that TEST_ENV has been deleted test_env_after_delete = os.environ.get(\'TEST_ENV\') return (original_home, test_env_before_delete, test_env_after_delete)"},{"question":"**Objective**: Implement a custom module importer using the functionalities and abstract base classes provided by the `importlib` package. **Problem Statement**: You are required to create a custom module importer named `CustomPathFinder` that: 1. Can find and load Python modules from a specified directory path. 2. Reload a module if it has been modified. 3. Handle both source (`.py`) and bytecode (`.pyc`) files. Implement the following components: - `CustomPathFinder`: A class that implements `importlib.abc.MetaPathFinder` and `importlib.abc.PathEntryFinder`. It should be able to locate modules within a specified directory and handle both `.py` and `.pyc` files. The class should also have the capability to invalidate its cache when required. - `CustomLoader`: A class that implements `importlib.abc.Loader`, capable of loading and executing both source and bytecode modules. **Requirements**: 1. `CustomPathFinder` should have a method `find_spec()` for finding module specifications. 2. `CustomLoader` should have methods `create_module()`, `exec_module()`, `get_code()`, and `get_source()`. 3. You should provide a function `add_custom_importer(directory)` to add the `CustomPathFinder` to the system\'s `sys.meta_path`. 4. Include a test function `test_custom_importer()` that: - Adds `CustomPathFinder` to `sys.meta_path`. - Imports a module from the specified directory using your custom importer. - Demonstrates reloading the module after modification. # Constraints: - The custom importer should only handle `.py` and `.pyc` files. - Ensure proper exception handling during module import and reload processes. - The solution should be compatible with Python 3.7 and above. # Sample Directory Structure: ``` /path/to/modules/ - example.py - example.pyc ``` # Expected Input and Output Formats: - **Input**: Path to directory containing Python modules. - **Output**: Successfully imported and reloaded module using custom importer. # Function Signatures: ```python import importlib.abc import importlib.util import sys import os class CustomPathFinder(importlib.abc.MetaPathFinder, importlib.abc.PathEntryFinder): def __init__(self, path): # Constructor implementation here def find_spec(self, fullname, path=None, target=None): # Implementation to find module specification def invalidate_caches(self): # Implementation to invalidate caches class CustomLoader(importlib.abc.Loader): def __init__(self, path): # Constructor implementation here def create_module(self, spec): # Implementation to create module def exec_module(self, module): # Implementation to execute module def get_code(self, fullname): # Implementation to get code object def get_source(self, fullname): # Implementation to get source code def add_custom_importer(directory): # Function to add custom importer to sys.meta_path def test_custom_importer(): # Test function to demonstrate the custom importer ``` **Note**: You may use additional helper functions or classes if needed.","solution":"import importlib.abc import importlib.util import os import sys class CustomPathFinder(importlib.abc.MetaPathFinder, importlib.abc.PathEntryFinder): def __init__(self, path): self.path = path self.modules_cache = {} def find_spec(self, fullname, path=None, target=None): module_name = fullname.split(\'.\')[-1] module_path = os.path.join(self.path, f\\"{module_name}.py\\") bytecode_path = os.path.join(self.path, f\\"{module_name}.pyc\\") if os.path.isfile(module_path) or os.path.isfile(bytecode_path): loader = CustomLoader(module_path if os.path.isfile(module_path) else bytecode_path) return importlib.util.spec_from_file_location(fullname, loader.path, loader=loader) return None def invalidate_caches(self): self.modules_cache = {} class CustomLoader(importlib.abc.Loader): def __init__(self, path): self.path = path def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): code = self.get_code(module.__name__) exec(code, module.__dict__) def get_code(self, fullname): source = self.get_source(fullname) return compile(source, self.path, \'exec\') def get_source(self, fullname): if self.path.endswith(\'.py\'): with open(self.path, \'r\') as f: return f.read() elif self.path.endswith(\'.pyc\'): with open(self.path, \'rb\') as f: return importlib.util._unpack_uint32(f.read(4)) raise ImportError(f\\"Unable to load source for {fullname}\\") def add_custom_importer(directory): if not os.path.isdir(directory): raise ValueError(f\\"{directory} is not a directory\\") sys.meta_path.insert(0, CustomPathFinder(directory))"},{"question":"# Set and Frozenset Manipulation in Python Objective Implement a Python class `SetOperations` that encapsulates various operations on sets and frozensets using the low-level C API functions provided by the `python310` package. Your class should demonstrate a comprehensive understanding of set and frozenset functionality as documented. Class Requirements 1. **Class Definition**: ```python class SetOperations: def __init__(self): pass ``` 2. **Methods**: - `create_set(elements: list) -> set`: - Creates a new set from a list of elements. - Returns the created set. - `create_frozenset(elements: list) -> frozenset`: - Creates a new frozenset from a list of elements. - Returns the created frozenset. - `add_to_set(s: set, element) -> None`: - Adds an element to a set. - `remove_from_set(s: set, element) -> None`: - Removes an element from a set. If the element does not exist, it should do nothing. - `clear_set(s: set) -> None`: - Clears all elements from a set. - `set_size(s) -> int`: - Returns the number of elements in a set or frozenset. - `contains_element(s, element) -> bool`: - Checks if the set or frozenset contains an element. Constraints and Requirements 1. You must use the functions specified in the provided documentation to achieve the functionality. 2. Your implementation should handle any exceptions or errors gracefully. 3. The class methods should be efficient and handle large sets effectively. Example Usage ```python # Create an instance of the class set_ops = SetOperations() # Create a new set and frozenset my_set = set_ops.create_set([1, 2, 3]) my_frozenset = set_ops.create_frozenset([\'a\', \'b\', \'c\']) # Add elements to the set set_ops.add_to_set(my_set, 4) set_ops.add_to_set(my_set, 5) # Remove an element from the set set_ops.remove_from_set(my_set, 2) # Clear all elements from the set set_ops.clear_set(my_set) # Get the size of set and frozenset set_size = set_ops.set_size(my_set) frozenset_size = set_ops.set_size(my_frozenset) # Check if an element is in the set or frozenset is_in_set = set_ops.contains_element(my_set, 4) is_in_frozenset = set_ops.contains_element(my_frozenset, \'a\') print(f\\"Set: {my_set}\\") print(f\\"Frozenset: {my_frozenset}\\") print(f\\"Size of set: {set_size}\\") print(f\\"Size of frozenset: {frozenset_size}\\") print(f\\"Is 4 in set: {is_in_set}\\") print(f\\"Is \'a\' in frozenset: {is_in_frozenset}\\") ``` Ensure that your implementation adheres to the documented API functions and correctly manages memory and potential errors.","solution":"class SetOperations: def __init__(self): pass def create_set(self, elements: list) -> set: Creates a new set from a list of elements. Returns the created set. return set(elements) def create_frozenset(self, elements: list) -> frozenset: Creates a new frozenset from a list of elements. Returns the created frozenset. return frozenset(elements) def add_to_set(self, s: set, element) -> None: Adds an element to a set. s.add(element) def remove_from_set(self, s: set, element) -> None: Removes an element from a set. If the element does not exist, it does nothing. s.discard(element) def clear_set(self, s: set) -> None: Clears all elements from a set. s.clear() def set_size(self, s) -> int: Returns the number of elements in a set or frozenset. return len(s) def contains_element(self, s, element) -> bool: Checks if the set or frozenset contains an element. return element in s"},{"question":"**Advanced Command-Line Option Parsing** **Objective:** Assess the student\'s ability to implement and extend command-line option parsing using the deprecated `optparse` module. **Problem Statement:** You are designing a command-line tool named `taskmanager` that helps users manage their daily tasks. The tool must support various operations like adding a new task, listing all tasks, marking a task as complete, and removing tasks. Each task has a unique ID, a description, and a status indicating whether it is complete. Your task is to implement the `taskmanager` command-line tool using the `optparse` module, adhering to the following functionality: 1. **Add Task (`--add`)** - Usage: `taskmanager --add \\"Task Description\\"` - Adds a new task with the given description. Assign a unique ID to the task. 2. **List Tasks (`--list`)** - Usage: `taskmanager --list` - Lists all tasks with their IDs, descriptions, and statuses. 3. **Complete Task (`--complete`)** - Usage: `taskmanager --complete <task_id>` - Marks the task with the given ID as complete. 4. **Remove Task (`--remove`)** - Usage: `taskmanager --remove <task_id>` - Removes the task with the given ID from the list. **Input and Output Formats:** * Inputs are provided via command-line options. * Outputs should be printed to the console in a human-readable format. **Example:** ```sh taskmanager --add \\"Buy groceries\\" Task added with ID: 1 taskmanager --add \\"Read a book\\" Task added with ID: 2 taskmanager --list ID: 1 | Task: Buy groceries | Status: Incomplete ID: 2 | Task: Read a book | Status: Incomplete taskmanager --complete 1 Task ID 1 marked as complete. taskmanager --list ID: 1 | Task: Buy groceries | Status: Complete ID: 2 | Task: Read a book | Status: Incomplete taskmanager --remove 1 Task ID 1 removed from the list. taskmanager --list ID: 2 | Task: Read a book | Status: Incomplete ``` **Implementation Constraints:** - Use the `optparse` module for command-line option parsing. - Ensure that appropriate error handling is in place for invalid inputs. - Maintain tasks in a simple in-memory list for the purpose of this exercise. - Implement the functionality in a single Python script. **Additional Requirements:** 1. **Default Values:** - Ensure a meaningful default behavior if no options are specified. 2. **Help Message:** - Generate a help message that describes the usage of the tool and the available options. 3. **Error Management:** - Handle and display errors gracefully, providing feedback to the user about incorrect option usage. 4. **Extensibility:** - Structure the code to easily allow the addition of new features or options in the future. **Guidelines:** - Make sure to thoroughly test your script using various scenarios. - Documentation comments should be included to explain the logic where necessary. Good luck!","solution":"import sys from optparse import OptionParser, OptionValueError class TaskManager: def __init__(self): self.tasks = [] self.next_id = 1 def add_task(self, description): task = {\'id\': self.next_id, \'description\': description, \'status\': \'Incomplete\'} self.tasks.append(task) self.next_id += 1 return task[\'id\'] def list_tasks(self): return [{\'id\': task[\'id\'], \'description\': task[\'description\'], \'status\': task[\'status\']} for task in self.tasks] def complete_task(self, task_id): for task in self.tasks: if task[\'id\'] == task_id: task[\'status\'] = \'Complete\' return True return False def remove_task(self, task_id): for task in self.tasks: if task[\'id\'] == task_id: self.tasks.remove(task) return True return False def main(): parser = OptionParser() parser.add_option(\\"--add\\", dest=\\"add\\", help=\\"Add a new task with the given description\\") parser.add_option(\\"--list\\", action=\\"store_true\\", dest=\\"list\\", help=\\"List all tasks\\") parser.add_option(\\"--complete\\", dest=\\"complete\\", type=\\"int\\", help=\\"Mark the task with the given ID as complete\\") parser.add_option(\\"--remove\\", dest=\\"remove\\", type=\\"int\\", help=\\"Remove the task with the given ID\\") (options, args) = parser.parse_args() task_manager = TaskManager() if options.add: task_id = task_manager.add_task(options.add) print(f\\"Task added with ID: {task_id}\\") elif options.list: tasks = task_manager.list_tasks() if not tasks: print(\\"No tasks found.\\") for task in tasks: print(f\\"ID: {task[\'id\']} | Task: {task[\'description\']} | Status: {task[\'status\']}\\") elif options.complete is not None: success = task_manager.complete_task(options.complete) if success: print(f\\"Task ID {options.complete} marked as complete.\\") else: print(f\\"Task ID {options.complete} not found.\\") elif options.remove is not None: success = task_manager.remove_task(options.remove) if success: print(f\\"Task ID {options.remove} removed from the list.\\") else: print(f\\"Task ID {options.remove} not found.\\") else: parser.print_help() if __name__ == \\"__main__\\": main()"},{"question":"Objective: Develop a Python program using scikit-learn to handle a dataset that is too large to fit into memory at once. The program should perform the following tasks: 1. Stream data from a file. 2. Extract features using a `HashingVectorizer`. 3. Train an incremental learning classifier (`SGDClassifier`) using mini-batches of data. Data: Assume your data is contained in a text file `large_dataset.txt`, where each line is a document that needs classification. Each document is a string of text with a corresponding label (either 0 or 1), separated by a tab. Requirements: 1. **Input Format:** - The input file `large_dataset.txt` should have each line as `<label>t<text>`. - Example lines: ``` 0 This is a negative example. 1 This is a positive example. ``` 2. **Output:** - Print the accuracy of the classifier after every 10,000 examples have been processed. 3. **Steps:** - Use a generator function to stream data from the file. - Extract text features with `HashingVectorizer`. - Train using `SGDClassifier` incrementally with mini-batches of size 1,000. 4. **Performance Requirements:** - Efficient memory usage, only a small portion of data should be in memory at any time. - Ensure the entire dataset is processed without loading it fully into memory. 5. **Implementation Constraints:** - Use only scikit-learn for machine learning components. - Do not use additional data processing libraries like pandas, numpy, etc., apart from handling the file read operations. Example Code Structure: ```python from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(file_path): with open(file_path, \'r\') as file: for line in file: label, text = line.split(\'t\') yield int(label), text.strip() def main(): # Parameters file_path = \'large_dataset.txt\' batch_size = 1000 report_interval = 10000 # Initialize vectorizer, classifier vectorizer = HashingVectorizer() classifier = SGDClassifier() # Stream data and incrementally fit the model X_batch, y_batch = [], [] total_examples = 0 all_predictions, all_true_labels = [], [] for label, text in stream_data(file_path): X_batch.append(text) y_batch.append(label) # If batch is ready, vectorize and fit if len(X_batch) == batch_size: X_vect = vectorizer.transform(X_batch) classifier.partial_fit(X_vect, y_batch, classes=[0, 1]) # Append predictions for this batch to compute accuracy later predictions = classifier.predict(X_vect) all_predictions.extend(predictions) all_true_labels.extend(y_batch) # Clear the batch X_batch, y_batch = [], [] total_examples += batch_size # Report the accuracy if total_examples % report_interval == 0: accuracy = accuracy_score(all_true_labels, all_predictions) print(f\'Processed {total_examples} examples. Accuracy: {accuracy:.4f}\') all_predictions, all_true_labels = [], [] # Final accuracy report if any remaining examples if X_batch: X_vect = vectorizer.transform(X_batch) classifier.partial_fit(X_vect, y_batch, classes=[0, 1]) predictions = classifier.predict(X_vect) all_predictions.extend(predictions) all_true_labels.extend(y_batch) total_examples += len(y_batch) accuracy = accuracy_score(all_true_labels, all_predictions) print(f\'Processed {total_examples} examples. Accuracy: {accuracy:.4f}\') if __name__ == \\"__main__\\": main() ``` Notes: - The `partial_fit` method allows updating the model incrementally. - The `HashingVectorizer` is used for text feature extraction to handle streaming data efficiently. - Use proper exception handling and logging for a production-grade solution.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_data(file_path): A generator function to stream data from file. :param file_path: str, path to the file :yield: tuple (label, text) with open(file_path, \'r\') as file: for line in file: label, text = line.split(\'t\', 1) yield int(label), text.strip() def main(file_path, batch_size=1000, report_interval=10000): Main function to stream data, vectorize features, train SGD classifier, and report accuracy incrementally. :param file_path: str, path to the input file :param batch_size: int, size of mini-batches for incremental learning :param report_interval: int, number of examples processed before reporting accuracy # Initialize vectorizer and classifier vectorizer = HashingVectorizer() classifier = SGDClassifier() # Stream data and incrementally fit the model X_batch, y_batch = [], [] total_examples = 0 all_predictions, all_true_labels = [], [] for label, text in stream_data(file_path): X_batch.append(text) y_batch.append(label) # If batch is ready, vectorize and fit if len(X_batch) == batch_size: X_vect = vectorizer.transform(X_batch) classifier.partial_fit(X_vect, y_batch, classes=[0, 1]) # Append predictions for this batch to compute accuracy later predictions = classifier.predict(X_vect) all_predictions.extend(predictions) all_true_labels.extend(y_batch) # Clear the batch X_batch, y_batch = [], [] total_examples += batch_size # Report the accuracy if total_examples % report_interval == 0: accuracy = accuracy_score(all_true_labels, all_predictions) print(f\'Processed {total_examples} examples. Accuracy: {accuracy:.4f}\') all_predictions, all_true_labels = [], [] # Final accuracy report if any remaining examples if X_batch: X_vect = vectorizer.transform(X_batch) classifier.partial_fit(X_vect, y_batch, classes=[0, 1]) predictions = classifier.predict(X_vect) all_predictions.extend(predictions) all_true_labels.extend(y_batch) total_examples += len(y_batch) accuracy = accuracy_score(all_true_labels, all_predictions) print(f\'Processed {total_examples} examples. Accuracy: {accuracy:.4f}\') if __name__ == \\"__main__\\": file_path = \'large_dataset.txt\' # Path to your text file main(file_path)"},{"question":"Objective: Write a Python script that processes a given textual file using the `lzma` module to perform a sequence of compression and decompression tasks. Your script should follow these steps: 1. **Compress the File**: - Read the entire contents of a text file (`input.txt`). - Compress the content using custom compression settings: - Use `FORMAT_XZ`. - Use a custom filter chain with `FILTER_DELTA` (distance = 1) and `FILTER_LZMA2` (preset = 6). - Write the compressed data to a new file (`compressed_output.xz`). 2. **Decompress the Data**: - Read the compressed data back from the `compressed_output.xz` file. - Decompress the data using the appropriate settings. - Save the decompressed data to another file (`decompressed_output.txt`). 3. **Validation**: - Ensure that the contents of `decompressed_output.txt` match the original `input.txt`. Constraints: - You should handle exceptions appropriately, ensuring that your script outputs meaningful error messages in case of failures. - The script should work with files of any size. - Ensure that you close all files properly after reading or writing operations to avoid resource leaks. Input and Output Formats: - Your script should work with the exact filenames provided: `input.txt`, `compressed_output.xz`, and `decompressed_output.txt`. - There are no constraints on the contents of `input.txt`. Performance: - The script should efficiently handle the compression and decompression process without excessive memory usage. - It should handle large files gracefully. Example: Given an `input.txt` file with the following content: ``` Hello, this is a test file for LZMA compression. This file contains multiple lines of text. ``` After running your script: - The `compressed_output.xz` should contain the compressed binary data. - The `decompressed_output.txt` should contain the exact same content as the original `input.txt`. Good luck!","solution":"import lzma def process_file(): try: # Step 1: Read the entire contents of the input file with open(\'input.txt\', \'rb\') as input_file: original_data = input_file.read() # Step 2: Compress the contents using custom compression settings compressor = lzma.LZMACompressor(format=lzma.FORMAT_XZ, filters=[ {\'id\': lzma.FILTER_DELTA, \'dist\': 1}, {\'id\': lzma.FILTER_LZMA2, \'preset\': 6} ]) compressed_data = compressor.compress(original_data) compressed_data += compressor.flush() # Step 3: Write the compressed data to a new file with open(\'compressed_output.xz\', \'wb\') as compressed_file: compressed_file.write(compressed_data) # Step 4: Read the compressed data back from the file with open(\'compressed_output.xz\', \'rb\') as compressed_file: compressed_data = compressed_file.read() # Step 5: Decompress the data decompressor = lzma.LZMADecompressor(format=lzma.FORMAT_XZ) decompressed_data = decompressor.decompress(compressed_data) # Step 6: Write the decompressed data to another file with open(\'decompressed_output.txt\', \'wb\') as decompressed_file: decompressed_file.write(decompressed_data) except Exception as e: print(f\\"Error occurred: {e}\\") if __name__ == \\"__main__\\": process_file()"},{"question":"**Objective:** The goal is to assess students\' comprehension of seaborn\'s error bar functionality, including the ability to implement and interpret different types of error bars in data visualizations. **Question:** Using seaborn, you are required to visualize the spread and uncertainty of a given set of data points. Please follow the guidelines below to complete the task: 1. **Data Generation**: - Generate a dataset of 200 data points sampled from a normal distribution with a mean of 5 and a standard deviation of 2. 2. **Visualizations**: - Create a single figure with four subplots (2x2 grid format) displaying the following: - Top-left: A point plot with error bars representing the **standard deviation** of the dataset. - Top-right: A point plot with error bars representing the **standard error** of the dataset. - Bottom-left: A point plot with error bars representing a **percentile interval** (95%) of the dataset. - Bottom-right: A point plot with error bars representing a **confidence interval** (95%) of the dataset using the bootstrap method with a bootstrapping seed of 123 and 5000 iterations. **Requirements & Constraints**: - Use appropriate seaborn functions and parameters to achieve each visualization. - Ensure each subplot has a title reflecting the type of error bar displayed. - Use matplotlib to create the figure and subplots layout. - The y-axis should represent the data points with their error bars, and the x-axis can be a constant dummy variable since only one dataset is visualized. **Input Format**: - No external input required. Data must be generated within the script. **Output Format**: - A single figure with four subplots displaying the different types of error bars. **Example**: ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"darkgrid\\") # Generate data np.random.seed(0) data = np.random.normal(5, 2, 200) # Create 2x2 subplots fig, axs = plt.subplots(2, 2, figsize=(14, 8)) # Top-left: Standard deviation error bars sns.pointplot(x=[0]*len(data), y=data, errorbar=\\"sd\\", ax=axs[0, 0], capsize=.3) axs[0, 0].set_title(\\"Standard Deviation\\") # Top-right: Standard error bars sns.pointplot(x=[0]*len(data), y=data, errorbar=\\"se\\", ax=axs[0, 1], capsize=.3) axs[0, 1].set_title(\\"Standard Error\\") # Bottom-left: Percentile interval error bars sns.pointplot(x=[0]*len(data), y=data, errorbar=(\\"pi\\", 95), ax=axs[1, 0], capsize=.3) axs[1, 0].set_title(\\"Percentile Interval 95%\\") # Bottom-right: Confidence interval error bars sns.pointplot(x=[0]*len(data), y=data, errorbar=(\\"ci\\", 95), ax=axs[1, 1], capsize=.3, n_boot=5000, seed=123) axs[1, 1].set_title(\\"Confidence Interval 95%\\") # Display plot plt.tight_layout() plt.show() ``` Provide your code implementation in a script file or notebook cell.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def create_error_bar_plots(): sns.set_theme(style=\\"darkgrid\\") # Generate data np.random.seed(0) data = np.random.normal(5, 2, 200) # Create 2x2 subplots fig, axs = plt.subplots(2, 2, figsize=(14, 8)) # Top-left: Standard deviation error bars sns.pointplot(x=[0]*len(data), y=data, errorbar=\\"sd\\", ax=axs[0, 0], capsize=.3) axs[0, 0].set_title(\\"Standard Deviation\\") # Top-right: Standard error bars sns.pointplot(x=[0]*len(data), y=data, errorbar=\\"se\\", ax=axs[0, 1], capsize=.3) axs[0, 1].set_title(\\"Standard Error\\") # Bottom-left: Percentile interval error bars sns.pointplot(x=[0]*len(data), y=data, errorbar=(\\"pi\\", 95), ax=axs[1, 0], capsize=.3) axs[1, 0].set_title(\\"Percentile Interval 95%\\") # Bottom-right: Confidence interval error bars sns.pointplot(x=[0]*len(data), y=data, errorbar=(\\"ci\\", 95), ax=axs[1, 1], capsize=.3, n_boot=5000, seed=123) axs[1, 1].set_title(\\"Confidence Interval 95%\\") # Display plot plt.tight_layout() plt.show() # Call the function to create the plots create_error_bar_plots()"},{"question":"File Path Conversion and Process Interactivity Checker In this task, you will implement two main functionalities: 1. **File System Path Conversion**: Implement a function `convert_to_fs_path` that takes a single argument and returns its file system representation. 2. **Process Interactivity Check**: Implement a function `is_interactive_process` that takes a file pointer (simulated via a file name) and returns whether it is deemed to be interactive. Details: 1. **convert_to_fs_path(path: Union[str, bytes, os.PathLike]) -> Union[str, bytes]**: - If the input is a `str` or `bytes` object, return it as is. - If the input implements the `os.PathLike` interface, return the result of calling `path.__fspath__()`. - If the input does not match any of the above conditions, raise a `TypeError`. 2. **is_interactive_process(file_name: str) -> bool**: - Check if the file pointer associated with `file_name` is deemed interactive. This can be simulated by verifying if the file name matches `\'<stdin>\'`, `\'???\'`, or if the standard input/output is associated with a terminal. - Return `True` if the file is interactive, `False` otherwise. Implementation Constraints and Requirements: - **File System Path Conversion** must handle all typical scenarios described and raise appropriate errors where necessary. - **Process Interactivity Checker** should accurately determine interactivity based on standard input conditions. - Performance is not a primary concern, but your implementation should be efficient and avoid unnecessary computations. Example Usage: ```python # Example `os.PathLike` implementation (simulated for this task) class MyPath: def __fspath__(self): return \\"/home/user/example\\" # convert_to_fs_path function print(convert_to_fs_path(\\"some/path\\")) # Should output: \\"some/path\\" print(convert_to_fs_path(b\\"some/bytes/path\\")) # Should output: b\\"some/bytes/path\\" print(convert_to_fs_path(MyPath())) # Should output: \\"/home/user/example\\" # is_interactive_process function print(is_interactive_process(\'<stdin>\')) # Should output: True print(is_interactive_process(\'???\')) # Should output: True print(is_interactive_process(\'file.txt\')) # Should output: False ``` You are to implement both `convert_to_fs_path` and `is_interactive_process` functions with appropriate error handling and functionalities as described above.","solution":"import os def convert_to_fs_path(path): Converts the given path to its file system representation. Parameters: path (Union[str, bytes, os.PathLike]): The path to be converted. Returns: Union[str, bytes]: The file system representation of the path. Raises: TypeError: If the path is not of the supported types. if isinstance(path, (str, bytes)): return path elif hasattr(path, \'__fspath__\'): return path.__fspath__() else: raise TypeError(\\"Unsupported path type\\") def is_interactive_process(file_name): Checks if the file pointer associated with file_name is deemed to be interactive. Parameters: file_name (str): The file name to be checked. Returns: bool: True if the file is interactive, False otherwise. if file_name in (\'<stdin>\', \'???\'): return True # Normally would use os.isatty(), but for simulation we check specific filenames return False"},{"question":"You are given a specific task to construct an email message, parse a raw email message, modify the parsed message, and serialize it back using Python\'s `email` package. Task 1. **Construct an Email Message:** - Create a new email message with the following details: - **From:** `sender@example.com` - **To:** `recipient@example.com` - **Subject:** `Test Email` - **Body:** `This is a test email message.` 2. **Parse an Existing Raw Email Message:** - Given the following raw email message string: ``` raw_email = \\"From: original_sender@example.comnTo: recipient@example.comnSubject: Original SubjectnnThis is the original email body.\\" ``` 3. **Modify the Parsed Email Message:** - Change the **From** field to `modified_sender@example.com`. - Append the following text to the existing email body: `nAppended text to the original message.` 4. **Serialize the Modified Email Message:** - Convert the modified email message back into a raw email string. Implementation - Define a function `construct_email()` which creates and returns the email message as specified in task 1. - Define a function `parse_and_modify_email(raw_email)` which: - Parses the given raw email string. - Modifies the \\"From\\" field and appends to the body as specified in task 3. - Returns the modified email as a raw email string. Input and Output **Function: `construct_email()`** - No Input - Output: A string representing the serialized email. **Function: `parse_and_modify_email(raw_email)`** - Input: `raw_email` (string) - Output: Modified raw email string Constraints - You should use the `email` package as described above. - You should define the functions exactly with the given names and signatures. - You should handle all necessary imports within the function. Example ```python def construct_email(): from email.message import EmailMessage msg = EmailMessage() msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' msg[\'Subject\'] = \'Test Email\' msg.set_content(\'This is a test email message.\') return msg.as_string() def parse_and_modify_email(raw_email): from email import message_from_string from email.policy import default msg = message_from_string(raw_email, policy=default) msg[\'From\'] = \'modified_sender@example.com\' msg.set_content(msg.get_content() + \'nAppended text to the original message.\') return msg.as_string() ``` Make sure to follow the function signatures and implement all specified actions.","solution":"def construct_email(): from email.message import EmailMessage msg = EmailMessage() msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' msg[\'Subject\'] = \'Test Email\' msg.set_content(\'This is a test email message.\') return msg.as_string() def parse_and_modify_email(raw_email): from email import message_from_string from email.policy import default msg = message_from_string(raw_email, policy=default) msg.replace_header(\'From\', \'modified_sender@example.com\') new_body = msg.get_body(preferencelist=(\'plain\',)).get_content() + \'nAppended text to the original message.\' msg.set_content(new_body) return msg.as_string()"},{"question":"Problem Statement You are given a dataset containing information about houses in a particular area. The dataset includes various features (e.g., number of bedrooms, size of the house, etc.) and the corresponding house prices. Your task is to implement a function to train various linear models and compare their performances in predicting house prices. Your function should: 1. Load the dataset. 2. Preprocess the data (if necessary). 3. Train the following regression models on the dataset: - Linear Regression - Ridge Regression - Lasso Regression - Elastic Net 4. Evaluate the models using Mean Squared Error (MSE). 5. Return the model performances. # Dataset The dataset will be provided as a CSV file with the following columns: - `num_bedrooms` (int): Number of bedrooms - `num_bathrooms` (int): Number of bathrooms - `area` (int): Size of the house in square feet - `price` (float): Price of the house # Function Signature ```python def compare_linear_models(csv_file_path: str) -> dict: pass ``` # Input - **csv_file_path** (*str*): Path to the CSV file containing the dataset. # Output - **result_dict** (*dict*): A dictionary where keys are the names of the models and values are their corresponding MSE. # Constraints - Use `scikit-learn` for model implementations. - Do not use any high-level functions that perform all tasks automatically; showcase the underlying steps. - Assume the dataset is clean with no missing values. # Example ```python csv_file_path = \\"house_prices.csv\\" result = compare_linear_models(csv_file_path) print(result) ``` This might output: ```python { \'Linear Regression\': 25000000.0, \'Ridge Regression\': 24500000.0, \'Lasso Regression\': 26000000.0, \'Elastic Net\': 25500000.0 } ``` # Additional Information - You may assume that `alpha` and `l1_ratio` parameters for Ridge and Elastic Net are to be chosen using cross-validation. - Use `random_state=42` for reproducibility wherever applicable. - Ensure that your implementation is efficient and properly commented.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.metrics import mean_squared_error def compare_linear_models(csv_file_path: str) -> dict: # Load the dataset df = pd.read_csv(csv_file_path) # Define features and target variable X = df[[\'num_bedrooms\', \'num_bathrooms\', \'area\']] y = df[\'price\'] # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize models models = { \'Linear Regression\': LinearRegression(), \'Ridge Regression\': Ridge(alpha=1.0, random_state=42), \'Lasso Regression\': Lasso(alpha=1.0, random_state=42), \'Elastic Net\': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42) } # Dictionary to store mean squared error for each model model_performances = {} # Train and evaluate each model for name, model in models.items(): # Train the model model.fit(X_train, y_train) # Predict on the test set y_pred = model.predict(X_test) # Calculate the mean squared error mse = mean_squared_error(y_test, y_pred) # Store the performance model_performances[name] = mse return model_performances"},{"question":"# PyTorch Intermediate Programming Assessment You are tasked with designing a PyTorch function that performs a series of tensor operations and verifies the correctness of these operations. Implement the following function: ```python import torch def tensor_operations_and_test(): Performs the following tensor operations and tests: 1. Creates a 2D tensor A of size (5, 5) with random values between -1 and 1. 2. Creates a 2D tensor B of size (5, 5) as the transpose of A. 3. Multiplies tensor A by tensor B to produce tensor C. 4. Creates a tensor D by summing all elements of tensor C. 5. Verifies that the shape of tensor B is indeed the transpose of tensor A. 6. Verifies that the tensor D retains scalar properties even after various operations. The function should conclude without any assertion errors if all operations and tests are successful. # Implement your function here pass ``` # Requirements and Constraints 1. **Random Seed**: For reproducibility, set the random seed to `42` before creating tensor A using `torch.manual_seed(42)`. 2. **Tensor Creation**: Use `torch.rand` or `torch.randn` to create the initial tensor A. Ensure values are within the range -1 and 1. 3. **Verification**: Utilize `torch.testing.assert_allclose` or `torch.testing.assert_close` for comparing tensors and validating equivalent structures. 4. **Scalability**: Ensure that the function can handle different tensor sizes by modifying the initial size (e.g., `(5, 5)` to any other size). # Expected Input and Output Formats - Input: No inputs are expected for the function. - Output: No return value is expected. The function should internally perform assertions to validate the operations. # Performance Requirements The function should execute efficiently for the tensor size provided `(5, 5)`. Ensure that tensor operations are performed using PyTorch\'s optimized functions for best performance. Implementing this function will demonstrate your comprehension of tensor operations, validation, and the use of PyTorch\'s `torch.testing` module for rigorous testing.","solution":"import torch import torch.testing def tensor_operations_and_test(): Performs the following tensor operations and tests: 1. Creates a 2D tensor A of size (5, 5) with random values between -1 and 1. 2. Creates a 2D tensor B of size (5, 5) as the transpose of A. 3. Multiplies tensor A by tensor B to produce tensor C. 4. Creates a tensor D by summing all elements of tensor C. 5. Verifies that the shape of tensor B is indeed the transpose of tensor A. 6. Verifies that the tensor D retains scalar properties even after various operations. The function should conclude without any assertion errors if all operations and tests are successful. # Set random seed for reproducibility torch.manual_seed(42) # Create a 2D tensor A of size (5, 5) with random values between -1 and 1 A = 2 * torch.rand(5, 5) - 1 # Create tensor B as the transpose of A B = A.t() # Multiply tensor A by tensor B to produce tensor C C = torch.matmul(A, B) # Sum all elements of tensor C to create tensor D D = torch.sum(C) # Verify that the shape of tensor B is indeed the transpose of tensor A assert B.shape == (A.shape[1], A.shape[0]) # Verify that tensor D retains scalar properties even after various operations assert D.dim() == 0 # Additional assertions can be done using torch.testing torch.testing.assert_close(B, A.T, rtol=1e-04, atol=1e-04) torch.testing.assert_close(D, C.sum(), rtol=1e-04, atol=1e-04)"},{"question":"**Question: File Analysis Tool** You are tasked with implementing a command-line tool in Python that analyzes text files within a specified directory. Your tool should be able to perform several operations based on the provided command-line arguments. # Requirements 1. **Listing Files:** List all `.txt` files in the specified directory. 2. **Word Count:** Report the number of occurrences of a specified word across all `.txt` files. 3. **Line Count:** Count the total number of lines across all `.txt` files. 4. **File Content Matching:** Find and list all lines in all `.txt` files that match a given regular expression pattern. # Input and Output Formats - **Input:** - Directory path (string): Path to the directory containing text files. - Command-line arguments: - `--list`: List all `.txt` files in the directory. - `--word_count <word>`: Count occurrences of `<word>` in all `.txt` files. - `--line_count`: Report the total number of lines in all `.txt` files. - `--match_lines <pattern>`: List all lines matching the `<pattern>` in all `.txt` files. - **Output:** - Depending on the command, the output should be a list of file names, an integer word/line count, or a list of matching lines. # Constraints - The directory path is guaranteed to exist, and it contains only text files with a `.txt` extension for simplicity. - The commands should handle large files efficiently in terms of time and space complexity. # Example Command Usages 1. **Listing Files:** ```bash python file_analyzer.py /path/to/directory --list ``` **Output:** ``` file1.txt file2.txt ``` 2. **Word Count:** ```bash python file_analyzer.py /path/to/directory --word_count example ``` **Output:** ``` 15 ``` 3. **Line Count:** ```bash python file_analyzer.py /path/to/directory --line_count ``` **Output:** ``` 300 ``` 4. **File Content Matching:** ```bash python file_analyzer.py /path/to/directory --match_lines \\"bexampleb\\" ``` **Output:** ``` line containing example in file1.txt another line with example in file2.txt ``` # Implementation Details Implement the tool in a script named `file_analyzer.py`. Your implementation should: 1. Use the `os` and `glob` modules to list and access files in the directory. 2. Utilize the `argparse` module to parse command-line arguments. 3. Use the `re` module to handle regular expressions. 4. Process files efficiently to handle potentially large files. ```python import os import glob import argparse import re def list_files(directory): files = glob.glob(os.path.join(directory, \'*.txt\')) for file in files: print(os.path.basename(file)) def word_count(directory, word): count = 0 files = glob.glob(os.path.join(directory, \'*.txt\')) for file in files: with open(file, \'r\', encoding=\'utf-8\') as f: for line in f: count += line.count(word) print(count) def line_count(directory): count = 0 files = glob.glob(os.path.join(directory, \'*.txt\')) for file in files: with open(file, \'r\', encoding=\'utf-8\') as f: count += sum(1 for _ in f) print(count) def match_lines(directory, pattern): regex = re.compile(pattern) files = glob.glob(os.path.join(directory, \'*.txt\')) for file in files: with open(file, \'r\', encoding=\'utf-8\') as f: for line in f: if regex.search(line): print(line.strip()) def main(): parser = argparse.ArgumentParser(description=\'File Analysis Tool\') parser.add_argument(\'directory\', type=str, help=\'Path to the directory containing text files\') parser.add_argument(\'--list\', action=\'store_true\', help=\'List all text files in the directory\') parser.add_argument(\'--word_count\', type=str, help=\'Count occurrences of the specified word in all text files\') parser.add_argument(\'--line_count\', action=\'store_true\', help=\'Count the total number of lines in all text files\') parser.add_argument(\'--match_lines\', type=str, help=\'List all lines matching the specified pattern in all text files\') args = parser.parse_args() if args.list: list_files(args.directory) elif args.word_count: word_count(args.directory, args.word_count) elif args.line_count: line_count(args.directory) elif args.match_lines: match_lines(args.directory, args.match_lines) else: parser.print_help() if __name__ == \\"__main__\\": main() ```","solution":"import os import glob import re import argparse def list_files(directory): List all .txt files in the specified directory. files = glob.glob(os.path.join(directory, \'*.txt\')) for file in files: print(os.path.basename(file)) def word_count(directory, word): Count occurrences of the specified word in all .txt files within the directory. count = 0 files = glob.glob(os.path.join(directory, \'*.txt\')) for file in files: with open(file, \'r\', encoding=\'utf-8\') as f: for line in f: count += line.lower().count(word.lower()) print(count) def line_count(directory): Count the total number of lines in all .txt files within the directory. count = 0 files = glob.glob(os.path.join(directory, \'*.txt\')) for file in files: with open(file, \'r\', encoding=\'utf-8\') as f: count += sum(1 for _ in f) print(count) def match_lines(directory, pattern): List all lines matching the specified pattern in all .txt files within the directory. regex = re.compile(pattern) files = glob.glob(os.path.join(directory, \'*.txt\')) for file in files: with open(file, \'r\', encoding=\'utf-8\') as f: for line in f: if regex.search(line): print(line.strip()) def main(): parser = argparse.ArgumentParser(description=\'File Analysis Tool\') parser.add_argument(\'directory\', type=str, help=\'Path to the directory containing text files\') parser.add_argument(\'--list\', action=\'store_true\', help=\'List all text files in the directory\') parser.add_argument(\'--word_count\', type=str, help=\'Count occurrences of the specified word in all text files\') parser.add_argument(\'--line_count\', action=\'store_true\', help=\'Count the total number of lines in all text files\') parser.add_argument(\'--match_lines\', type=str, help=\'List all lines matching the specified pattern in all text files\') args = parser.parse_args() if args.list: list_files(args.directory) elif args.word_count: word_count(args.directory, args.word_count) elif args.line_count: line_count(args.directory) elif args.match_lines: match_lines(args.directory, args.match_lines) else: parser.print_help() if __name__ == \\"__main__\\": main()"},{"question":"Problem Description: You are tasked with creating a script for a command-line tool called `filemanager.py`. This tool should perform various file operations based on the provided command-line arguments. The script should support the following functionalities: 1. **Copy a file**: - Short option: `-c` - Long option: `--copy` - Requires an argument: the source and destination file paths separated by a comma (e.g., `-c source.txt,destination.txt`) 2. **Delete a file**: - Short option: `-d` - Long option: `--delete` - Requires an argument: the file path to be deleted (e.g., `-d file.txt`) 3. **Move a file**: - Short option: `-m` - Long option: `--move` - Requires an argument: the source and destination file paths separated by a comma (e.g., `-m source.txt,destination.txt`) 4. **Print file content**: - Short option: `-p` - Long option: `--print` - Requires an argument: the file path to print (e.g., `-p file.txt`) 5. **Verbosity**: - Short option: `-v` - Long option: `--verbose` - Does not require an argument: When provided, the script prints detailed logs of the operations being performed. Additional Requirements: - The script should handle invalid options gracefully and print an appropriate error message. - Use `sys.argv` to parse the command-line arguments. - Ensure that the script handles multiple options being passed together. - If the operations are invalid (e.g., file paths not provided correctly), output an appropriate error message without executing any operation. Input Format: - Command-line arguments in the format specified. Output Format: - Detailed logs of operations if verbosity is enabled. - Error messages for invalid options or arguments. Constraints: - The `--copy`, `--move` options should validate that the source file exists before performing the operation. - Operations must be done sequentially as they appear in the command line. Example: Command-line input: ``` python filemanager.py -c source.txt,destination.txt --delete file.txt --verbose ``` Expected output (if verbosity is enabled and the source file exists): ``` Copying file from source.txt to destination.txt Deleting file file.txt ``` # Implementation Implement the function `main()` in the `filemanager.py` script to achieve the described functionalities. ```python import getopt import sys import os def main(): # Initialize necessary variables verbose = False operations = [] # Parse command-line arguments try: opts, args = getopt.getopt(sys.argv[1:], \\"c:d:m:p:v\\", [\\"copy=\\", \\"delete=\\", \\"move=\\", \\"print=\\", \\"verbose\\"]) except getopt.GetoptError as err: print(err) sys.exit(2) for opt, arg in opts: if opt in (\\"-v\\", \\"--verbose\\"): verbose = True elif opt in (\\"-c\\", \\"--copy\\"): source, destination = arg.split(\\",\\") operations.append((\\"copy\\", source, destination)) elif opt in (\\"-d\\", \\"--delete\\"): operations.append((\\"delete\\", arg)) elif opt in (\\"-m\\", \\"--move\\"): source, destination = arg.split(\\",\\") operations.append((\\"move\\", source, destination)) elif opt in (\\"-p\\", \\"--print\\"): operations.append((\\"print\\", arg)) else: print(f\\"Unhandled option {opt}\\") sys.exit(2) # Perform the file operations for operation in operations: op_type = operation[0] if op_type == \\"copy\\": source, destination = operation[1], operation[2] if os.path.exists(source): if verbose: print(f\\"Copying file from {source} to {destination}\\") with open(source, \'r\') as src_file: with open(destination, \'w\') as dest_file: dest_file.write(src_file.read()) else: print(f\\"Source file {source} does not exist.\\") sys.exit(2) elif op_type == \\"delete\\": filepath = operation[1] if os.path.exists(filepath): os.remove(filepath) if verbose: print(f\\"Deleting file {filepath}\\") else: print(f\\"File {filepath} does not exist.\\") sys.exit(2) elif op_type == \\"move\\": source, destination = operation[1], operation[2] if os.path.exists(source): if verbose: print(f\\"Moving file from {source} to {destination}\\") os.rename(source, destination) else: print(f\\"Source file {source} does not exist.\\") sys.exit(2) elif op_type == \\"print\\": filepath = operation[1] if os.path.exists(filepath): if verbose: print(f\\"Printing content of {filepath}\\") with open(filepath, \'r\') as file: print(file.read()) else: print(f\\"File {filepath} does not exist.\\") sys.exit(2) else: print(f\\"Unknown operation {op_type}\\") sys.exit(2) if __name__ == \\"__main__\\": main() ```","solution":"import getopt import sys import os def main(): # Initialize necessary variables verbose = False operations = [] # Parse command-line arguments try: opts, args = getopt.getopt(sys.argv[1:], \\"c:d:m:p:v\\", [\\"copy=\\", \\"delete=\\", \\"move=\\", \\"print=\\", \\"verbose\\"]) except getopt.GetoptError as err: print(err) sys.exit(2) for opt, arg in opts: if opt in (\\"-v\\", \\"--verbose\\"): verbose = True elif opt in (\\"-c\\", \\"--copy\\"): source, destination = arg.split(\\",\\") operations.append((\\"copy\\", source, destination)) elif opt in (\\"-d\\", \\"--delete\\"): operations.append((\\"delete\\", arg)) elif opt in (\\"-m\\", \\"--move\\"): source, destination = arg.split(\\",\\") operations.append((\\"move\\", source, destination)) elif opt in (\\"-p\\", \\"--print\\"): operations.append((\\"print\\", arg)) else: print(f\\"Unhandled option {opt}\\") sys.exit(2) # Perform the file operations for operation in operations: op_type = operation[0] if op_type == \\"copy\\": source, destination = operation[1], operation[2] if os.path.exists(source): if verbose: print(f\\"Copying file from {source} to {destination}\\") with open(source, \'r\') as src_file: with open(destination, \'w\') as dest_file: dest_file.write(src_file.read()) else: print(f\\"Source file {source} does not exist.\\") sys.exit(2) elif op_type == \\"delete\\": filepath = operation[1] if os.path.exists(filepath): os.remove(filepath) if verbose: print(f\\"Deleting file {filepath}\\") else: print(f\\"File {filepath} does not exist.\\") sys.exit(2) elif op_type == \\"move\\": source, destination = operation[1], operation[2] if os.path.exists(source): if verbose: print(f\\"Moving file from {source} to {destination}\\") os.rename(source, destination) else: print(f\\"Source file {source} does not exist.\\") sys.exit(2) elif op_type == \\"print\\": filepath = operation[1] if os.path.exists(filepath): if verbose: print(f\\"Printing content of {filepath}\\") with open(filepath, \'r\') as file: print(file.read()) else: print(f\\"File {filepath} does not exist.\\") sys.exit(2) else: print(f\\"Unknown operation {op_type}\\") sys.exit(2) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Seaborn Plotting Assessment **Context:** Seaborn is a powerful Python library for data visualization based on matplotlib. In particular, the `seaborn.objects` module provides a flexible interface to create complex visual representations. In this task, you will work with a dataset using seaborn to create an advanced plot that demonstrates your understanding of various seaborn functionalities. **Dataset Information:** You will use a dataset containing information on stock prices over time (`dowjones`) and a dataset related to an fMRI study (`fmri`). # Task: 1. **Load the Dow Jones Dataset:** Load the `dowjones` dataset from seaborn\'s built-in datasets. This dataset contains two columns: `Date` (the date of the recorded price) and `Price` (the stock price). 2. **Create a Line Plot for Dow Jones:** Create a line plot that shows the fluctuation of the Dow Jones stock price over time. Include markers at each data point to highlight the individual observations. 3. **Load the fMRI Dataset:** Load the `fmri` dataset from seaborn\'s built-in datasets. This dataset includes multiple columns, but you will use `timepoint`, `signal`, `event`, and `region`. 4. **Advanced fMRI Plot:** Create a plot for the `fmri` dataset that includes the following: - Line plots for the `signal` over `timepoint`, grouped by `subject`. - Different colors for each `region`. - Different line styles for each `event`. - Error bands to show variability within each `event`. **Requirements:** - Your code should be modular, with distinct functions for loading data, creating the Dow Jones plot, and creating the fMRI plot. - Handle missing values appropriately, if any exist in the dataset. - Ensure the visualization is clear, well-labeled, and conveys the required information effectively. **Submission:** Provide the complete code for the task within a Jupyter notebook. The notebook should include: - Data loading and exploration - Function definitions - Plot creation and customization - Explanations for each step and any observations # Example: Here is a brief structure to guide you: ```python import seaborn.objects as so from seaborn import load_dataset # Function to load datasets def load_data(): dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") return dowjones, fmri # Function to create Dow Jones plot def plot_dowjones(dowjones): p = so.Plot(dowjones, \\"Date\\", \\"Price\\") p.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\")) p.show() # Function to create advanced fMRI plot def plot_fmri(fmri): p = so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") p.add(so.Line(), so.Agg()) p.add(so.Band(), so.Est(), group=\\"event\\") p.show() # Main function to execute the tasks def main(): dowjones, fmri = load_data() plot_dowjones(dowjones) plot_fmri(fmri) # Run main function if __name__ == \\"__main__\\": main() ``` Ensure your code follows a similar structured flow but elaborates the functionalities as described in the task.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Function to load datasets def load_data(): dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") return dowjones, fmri # Function to create Dow Jones plot def plot_dowjones(dowjones): plt.figure(figsize=(10, 6)) p = so.Plot(dowjones, x=\\"Date\\", y=\\"Price\\") p.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\")) plt.title(\\"Dow Jones Stock Price Over Time\\") plt.xlabel(\\"Date\\") plt.ylabel(\\"Price\\") plt.show() # Function to create advanced fMRI plot def plot_fmri(fmri): plt.figure(figsize=(10, 6)) p = so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") p.add(so.Line(), so.Agg()) p.add(so.Band(), so.Est(), group=\\"event\\") plt.title(\\"fMRI Signal Over Timepoint\\") plt.xlabel(\\"Timepoint\\") plt.ylabel(\\"Signal\\") plt.show() # Main function to execute the tasks def main(): dowjones, fmri = load_data() plot_dowjones(dowjones) plot_fmri(fmri)"},{"question":"# Question: Advanced Data Encoding and Decoding Problem Statement In this task, you are required to implement a set of functions to encode and decode data using multiple encoding schemes provided by the `base64` module. You will need to handle Base64, Base32, and Ascii85 encodings, along with converting the encoded outputs into a specific format. **Your task is to implement the following functions:** 1. `multi_encode(data: bytes, encodings: List[str], options: Dict[str, Any]) -> bytes`: - `data`: A bytes-like object to be encoded. - `encodings`: A list of encoding methods in the order they should be applied. Possible values include `\\"base64\\"`, `\\"base32\\"`, `\\"ascii85\\"`. - `options`: A dictionary containing optional keys that might include: - `\'altchars\'`: Optional alternative characters for Base64 (default to `None`). - `\'foldspaces\'`: Optional boolean for Ascii85 if spaces should be folded (default to `False`). Only relevant for Ascii85 encoding. - `\'adobe\'`: Optional boolean indicating if Adobe\'s framing should be used in Ascii85 (default to `False`). Only relevant for Ascii85 encoding. - Other options relevant based on encoding type. 2. `multi_decode(data: bytes, encodings: List[str], options: Dict[str, Any]) -> bytes`: - `data`: A bytes-like object to be decoded. - `encodings`: A list of encoding methods in the order they should be reversed in the decoding process. Possible values include `\\"base64\\"`, `\\"base32\\"`, `\\"ascii85\\"`. - `options`: A dictionary containing optional keys that might include: - `\'altchars\'`: Optional alternative characters for Base64 (default to `None`). - `\'foldspaces\'`: Optional boolean for Ascii85 if folded spaces should be considered (default to `False`). Only relevant for Ascii85 decoding. - `\'adobe\'`: Optional boolean indicating if Adobe\'s framing was used in Ascii85 (default to `False`). Only relevant for Ascii85 decoding. - Other options relevant based on decoding type. **Your implementation should handle the following constraints:** 1. Ensure that all encodings provided in the list are valid and supported. 2. Apply the provided options wherever applicable. 3. Raise appropriate exceptions for invalid encodings or data. Example ```python data = b\\"hello world\\" encodings = [\\"base64\\", \\"ascii85\\"] options = {\'altchars\': b\'-_\', \'foldspaces\': True, \'adobe\': True} encoded_data = multi_encode(data, encodings, options) decoded_data = multi_decode(encoded_data, encodings, options) print(encoded_data) # Output should be a bytes object representing the multi-encoded data. print(decoded_data) # Should output: b\\"hello world\\" again. ``` Notes: - Make sure to use the appropriate documentation sections for reference on each encoding method. - Consider handling errors gracefully by informing the user of incorrect input or usage. - Your solution should be efficient and error-proof.","solution":"import base64 from typing import List, Dict, Any def multi_encode(data: bytes, encodings: List[str], options: Dict[str, Any]) -> bytes: for encoding in encodings: if encoding == \\"base64\\": altchars = options.get(\'altchars\', None) data = base64.b64encode(data, altchars=altchars) elif encoding == \\"base32\\": data = base64.b32encode(data) elif encoding == \\"ascii85\\": foldspaces = options.get(\'foldspaces\', False) adobe = options.get(\'adobe\', False) data = base64.a85encode(data, foldspaces=foldspaces, adobe=adobe) else: raise ValueError(f\\"Unsupported encoding type: {encoding}\\") return data def multi_decode(data: bytes, encodings: List[str], options: Dict[str, Any]) -> bytes: for encoding in reversed(encodings): if encoding == \\"base64\\": altchars = options.get(\'altchars\', None) data = base64.b64decode(data, altchars=altchars) elif encoding == \\"base32\\": data = base64.b32decode(data) elif encoding == \\"ascii85\\": foldspaces = options.get(\'foldspaces\', False) adobe = options.get(\'adobe\', False) data = base64.a85decode(data, foldspaces=foldspaces, adobe=adobe) else: raise ValueError(f\\"Unsupported encoding type: {encoding}\\") return data"},{"question":"**Problem Statement:** You are required to work with Sun AU audio files using the `sunau` module in Python. The task involves reading an existing AU audio file, extracting some of its properties and data, processing the audio data to change its sample width, and then writing the modified data to a new AU file. **Requirements:** 1. Read an AU file and extract its properties: - Number of channels - Sample width - Frame rate - Number of frames - Compression type 2. Convert the sample width of the audio data to a different value (e.g., from 8-bit to 16-bit). 3. Write the modified audio data to a new AU file with the same properties, except for the changed sample width. **Function Signature:** ```python def process_au_file(input_file: str, output_file: str, target_sampwidth: int) -> None: Read an AU audio file, modify its sample width, and write the result to a new AU file. input_file : str The path to the input AU file. output_file : str The path to the output AU file. target_sampwidth : int The target sample width for the output AU file (in bytes). pass ``` **Input:** - `input_file`: A string representing the file path of the input AU audio file. - `output_file`: A string representing the file path where the output AU audio file will be saved. - `target_sampwidth`: An integer representing the desired sample width for the output AU file (1 for 8-bit, 2 for 16-bit, etc.). **Output:** - The function does not return anything, but it will create an AU file at `output_file` with the modified sample width. **Constraints:** - The input AU file will be a valid AU file. - The `target_sampwidth` will be one of the supported sample widths (1, 2, 3, or 4 bytes). **Example:** ```python process_au_file(\\"input.au\\", \\"output.au\\", 2) ``` In this example, the function reads the input AU file \\"input.au\\", converts its sample width to 16-bit, and writes the result to the output AU file \\"output.au\\". **Notes:** - You will need to handle reading the audio frames, converting their sample width properly, and ensuring the output file has the correct parameters set. - Consider handling different encoding formats supported by the sunau module and converting data accordingly. **Assumptions:** - Input AU file has a valid format and accessible path. - Free to use external libraries like `numpy` for handling byte data manipulation if necessary.","solution":"import sunau import numpy as np def process_au_file(input_file: str, output_file: str, target_sampwidth: int) -> None: Read an AU audio file, modify its sample width, and write the result to a new AU file. input_file : str The path to the input AU file. output_file : str The path to the output AU file. target_sampwidth : int The target sample width for the output AU file (in bytes). # Read the input AU file with sunau.open(input_file, \'rb\') as infile: n_channels = infile.getnchannels() sampwidth = infile.getsampwidth() frame_rate = infile.getframerate() n_frames = infile.getnframes() compression_type = infile.getcomptype() compression_name = infile.getcompname() # Read frames frames = infile.readframes(n_frames) # Convert frames to numpy array for easier manipulation input_format = {1: \'int8\', 2: \'int16\', 3: \'int24\', 4: \'int32\'}[sampwidth] output_format = {1: \'int8\', 2: \'int16\', 3: \'int24\', 4: \'int32\'}[target_sampwidth] # Converting the byte data to numpy arrays audio_data = np.frombuffer(frames, dtype=np.dtype(input_format)) # Reshape the array back to a 2D array with shape (n_frames, n_channels) audio_data = audio_data.reshape(-1, n_channels) # Convert the audio sample width audio_data_converted = audio_data.astype(np.dtype(output_format)) # Writing the output AU file with sunau.open(output_file, \'wb\') as outfile: outfile.setnchannels(n_channels) outfile.setsampwidth(target_sampwidth) outfile.setframerate(frame_rate) outfile.setcomptype(compression_type, compression_name) # Write the converted frames to the output file outfile.writeframes(audio_data_converted.tobytes())"},{"question":"# Time Zone Conversion and Daylight Saving Time Handling **Objective:** Implement a function that converts a given UTC datetime to a specified time zone and accounts for daylight saving transitions. **Function Signature:** ```python from datetime import datetime from typing import Union from zoneinfo import ZoneInfo def convert_utc_to_timezone(utc_dt: datetime, timezone: str) -> Union[datetime, str]: Converts a given UTC datetime to the specified timezone. Parameters: - `utc_dt` (datetime): A datetime object representing the time in UTC. - `timezone` (str): A string representing the target time zone key. Returns: - A datetime object representing the time in the specified time zone. - A string with an error message if the time zone key is invalid or conversion fails. Note: - The input datetime object must be timezone-aware and set to UTC. - Handle daylight saving time transitions correctly. - Use the `ZoneInfo` class from the `zoneinfo` module. - Appropriate error handling for time zone issues should be implemented. pass ``` **Input Format:** - `utc_dt`: A timezone-aware datetime object in UTC. - `timezone`: A string representing a valid IANA time zone key (e.g., \\"America/Los_Angeles\\"). **Output Format:** - Returns a timezone-aware datetime object converted to the specified time zone. - If the time zone key is invalid, returns a string with the error message: `\\"Invalid time zone key\\"`. **Constraints:** - The function should handle daylight saving time transitions without requiring further intervention by the user. - If the conversions involve ambiguous or nonexistent times (e.g., during daylight saving transitions), they should be handled using the `fold` attribute as described in the `zoneinfo` documentation. **Example Usage:** ```python # Example 1: utc_dt = datetime(2020, 11, 1, 8, 0, 0, tzinfo=ZoneInfo(\\"UTC\\")) timezone = \\"America/Los_Angeles\\" print(convert_utc_to_timezone(utc_dt, timezone)) # Expected Output: 2020-11-01 01:00:00-08:00 # Example 2: utc_dt = datetime(2021, 3, 14, 10, 0, 0, tzinfo=ZoneInfo(\\"UTC\\")) timezone = \\"America/New_York\\" print(convert_utc_to_timezone(utc_dt, timezone)) # Expected Output: 2021-03-14 06:00:00-04:00 # Example 3: Invalid time zone utc_dt = datetime(2021, 3, 14, 10, 0, 0, tzinfo=ZoneInfo(\\"UTC\\")) timezone = \\"Invalid/Timezone\\" print(convert_utc_to_timezone(utc_dt, timezone)) # Expected Output: \\"Invalid time zone key\\" ``` The problem assesses the understanding of time zone conversions, use of the `ZoneInfo` class, handling daylight saving transitions, and proper error handling in Python.","solution":"from datetime import datetime from typing import Union from zoneinfo import ZoneInfo, ZoneInfoNotFoundError def convert_utc_to_timezone(utc_dt: datetime, timezone: str) -> Union[datetime, str]: Converts a given UTC datetime to the specified timezone. Parameters: - `utc_dt` (datetime): A datetime object representing the time in UTC. - `timezone` (str): A string representing the target time zone key. Returns: - A datetime object representing the time in the specified time zone. - A string with an error message if the time zone key is invalid or conversion fails. Note: - The input datetime object must be timezone-aware and set to UTC. - Handle daylight saving time transitions correctly. - Use the `ZoneInfo` class from the `zoneinfo` module. - Appropriate error handling for time zone issues should be implemented. if utc_dt.tzinfo is None or utc_dt.tzinfo != ZoneInfo(\\"UTC\\"): return \\"Input datetime must be timezone-aware and set to UTC.\\" try: target_tz = ZoneInfo(timezone) except ZoneInfoNotFoundError: return \\"Invalid time zone key\\" return utc_dt.astimezone(target_tz)"},{"question":"You are tasked with designing an asynchronous pipeline using PyTorch\'s `torch.futures.Future` to process a batch of data in parallel. Your pipeline consists of several stages, each performing a different computation asynchronously. You will need to collect the results from all stages and return the aggregated output. # Function Signature ```python def process_pipeline(data: List[int]) -> List[int]: Given a list of integers, apply a series of asynchronous transformations to the data and return the final aggregated results. The transformations should occur in different stages: 1. Add 1 to each element. 2. Square each element. 3. Take the square root of each element (use torch.sqrt). You should utilize torch.futures.Future for handling asynchronous operations and the utility functions collect_all and wait_all to manage multiple futures. Parameters: data (List[int]): A list of integers. Returns: List[int]: A list of transformed integers after all stages are completed. ``` # Requirements 1. **Stage 1**: Apply the transformation `x + 1` to each element in the list asynchronously using `torch.futures.Future`. 2. **Stage 2**: Once all elements have been transformed in Stage 1, square each element asynchronously. 3. **Stage 3**: Finally, compute the square root of each element asynchronously. 4. Use `torch.futures.Future`, `collect_all`, and `wait_all` to handle the asynchronous operations. 5. The final output should be a list of integers after all transformations are applied. # Constraints - You must handle asynchronous execution using PyTorch\'s `torch.futures.Future`. - Use `collect_all` and `wait_all` to manage multiple futures. # Example ```python data = [1, 2, 3, 4] result = process_pipeline(data) # Expected result after transformations: [1.414, 2, 2.449, 3] (rounded for simplicity) ``` # Note Please ensure you have the necessary PyTorch package installed to run the code. # Hint - You may find it useful to create helper functions to perform each stage of the transformation asynchronously. - Remember to handle the futures appropriately to ensure the correct order of execution and aggregation of results.","solution":"import torch from torch.futures import Future from typing import List def add_one(x: int) -> Future: future = Future() future.set_result(x + 1) return future def square(x: int) -> Future: future = Future() future.set_result(x * x) return future def sqrt(x: int) -> Future: future = Future() future.set_result(torch.sqrt(torch.tensor(x, dtype=torch.float64)).item()) return future def process_pipeline(data: List[int]) -> List[int]: stage1_futures = [add_one(x) for x in data] stage1_results = [future.wait() for future in stage1_futures] stage2_futures = [square(x) for x in stage1_results] stage2_results = [future.wait() for future in stage2_futures] stage3_futures = [sqrt(x) for x in stage2_results] stage3_results = [future.wait() for future in stage3_futures] return stage3_results"},{"question":"# Complex Number Analyzer You are tasked with creating a function called `analyze_complex_numbers` that takes a list of complex numbers and performs a series of operations to extract various mathematical properties and transformations. Specifically, this function should: 1. Convert each complex number from Cartesian (rectangular) to polar coordinates and calculate the modulus and phase. 2. Compute the exponential, logarithmic (natural log), and square root values for each complex number. 3. Calculate the sine, cosine, and tangent of each complex number. The function should return a dictionary where the keys are the original complex numbers and the values are dictionaries containing the results of the above operations. Input: - A list of complex numbers `numbers` where each element is a complex number, e.g., `[complex(1, 2), complex(3, -4), ...]`. Output: - A dictionary with the following structure: ```python { original_complex_number: { \'polar\': (modulus, phase), \'exp\': complex_number, \'log\': complex_number, \'sqrt\': complex_number, \'sin\': complex_number, \'cos\': complex_number, \'tan\': complex_number }, ... } ``` Example: ```python numbers = [complex(1, 1), complex(-1, 1), complex(1, -1), complex(-1, -1)] result = analyze_complex_numbers(numbers) ``` Expected `result`: ```python { (1+1j): { \'polar\': (1.4142135623730951, 0.7853981633974483), \'exp\': (1.4686939399158851+2.2873552871788423j), \'log\': (0.34657359027997264+0.7853981633974483j), \'sqrt\': (1.0986841134678098+0.45508986056222733j), \'sin\': (1.2984575814159773+0.6349639147847361j), \'cos\': (0.8337300251311491-0.9888977057628651j), \'tan\': (0.2717525853195117+1.0839233273386946j) }, ... } ``` # Implementation Notes: - Use the `cmath` module for mathematical operations involving complex numbers. - Ensure that the function handles various cases of complex numbers, including those with negative real or imaginary parts. - Test the function with different complex numbers to verify its correctness. # Constraints: - The input list can contain up to 100 complex numbers. - The values of real and imaginary parts can be as large as floating-point precision allows. **Your implementation should display a solid understanding of complex numbers and their operations using the `cmath` module.** ```python import cmath def analyze_complex_numbers(numbers): result = {} for num in numbers: modulus, phase = cmath.polar(num) result[num] = { \'polar\': (modulus, phase), \'exp\': cmath.exp(num), \'log\': cmath.log(num), \'sqrt\': cmath.sqrt(num), \'sin\': cmath.sin(num), \'cos\': cmath.cos(num), \'tan\': cmath.tan(num) } return result # Example usage numbers = [complex(1, 1), complex(-1, 1), complex(1, -1), complex(-1, -1)] print(analyze_complex_numbers(numbers)) ```","solution":"import cmath def analyze_complex_numbers(numbers): result = {} for num in numbers: modulus, phase = cmath.polar(num) result[num] = { \'polar\': (modulus, phase), \'exp\': cmath.exp(num), \'log\': cmath.log(num), \'sqrt\': cmath.sqrt(num), \'sin\': cmath.sin(num), \'cos\': cmath.cos(num), \'tan\': cmath.tan(num) } return result"},{"question":"In this assessment, you are required to manipulate binary data using the `struct` module. The task is to create a function to pack a sequence of mixed data types into a binary format and then unpack it back to its original data structure. # Instructions Write a function `pack_and_unpack` which: 1. Takes in a format string that describes the layout and content of the binary data. 2. Takes a list of values which need to be packed as per the format string. 3. Packs these values into a binary format using the `struct.pack` method. 4. Then unpacks this binary data back into its original values using the `struct.unpack` method. 5. Returns the unpacked data. # Function Definition ```python def pack_and_unpack(format_str: str, values: list): # Your code here ``` # Inputs - **format_str**: A string which represents the layout and data types of the binary data (e.g., `\'>bhl\'`). - **values**: A list of values that match the format string types (e.g., `[1, 2, 3]` for format `\'>bhl\'`). # Outputs - Returns a list or tuple of unpacked values as per the format string. # Constraints - The format string and values list must be compatible. - The values list must match the length and types specified by the format string. - Ensure to handle errors or exceptions gracefully if the inputs are inconsistent. # Example ```python # Example 1: format_str = \'>bhl\' values = [1, 2, 3] result = pack_and_unpack(format_str, values) # Result: (1, 2, 3) # Example 2: format_str = \'<i4s\' values = [65535, b\'test\'] result = pack_and_unpack(format_str, values) # Result: (65535, b\'test\') ``` # Note - Use the `struct.pack()` function to pack the values into a binary format. - Use the `struct.unpack()` function to unpack the binary data back into the original values. Make sure to test your function with varied inputs to ensure correctness.","solution":"import struct def pack_and_unpack(format_str: str, values: list): Packs the values into binary format according to the format string and then unpacks it back to the original values. :param format_str: A string which represents the layout and data types of the binary data. :param values: A list of values that match the format string types. :return: A tuple of unpacked values as per the format string. packed_data = struct.pack(format_str, *values) unpacked_data = struct.unpack(format_str, packed_data) return unpacked_data"},{"question":"# Custom Distribution Implementation and Utilization Objective You will implement a custom probability distribution by extending the `torch.distributions.Distribution` class and then use this custom distribution to perform some basic tasks. Requirements 1. Define a `CustomDistribution` class that models a simple probability distribution of your choice. This class should extend the `torch.distributions.Distribution` class. 2. Implement the following methods for your distribution: - `__init__(self, *parameters)`: Initialize the distribution parameters. - `sample(self, sample_shape=torch.Size())`: Generate samples from the distribution. - `log_prob(self, value)`: Compute the log-probability of a given value. - `cdf(self, value)`: Compute the cumulative distribution function for a given value. 3. Demonstrate the usage of your custom distribution by: - Sampling 1000 values from the distribution and plotting a histogram. - Calculating the log-probability of a list of values. - Computing the cumulative distribution function for a list of values. Your custom distribution should be unique and non-trivial for this assignment (e.g., a Linear Combination of two common distributions like Normal and Beta). Ensure appropriate handling of edge cases and validation of the input parameters. Input and Output Formats - **Input**: You do not need to handle inputs as a script. Your class will be initialized and methods called as described in the requirements. - **Output**: Your output should consist of: - A plot showing the histogram of 1000 sampled values. - A printed list of log-probabilities for a given list of values. - A printed list of cumulative probabilities (CDF values) for the same list of values. ```python import torch import torch.distributions as dist import matplotlib.pyplot as plt class CustomDistribution(dist.Distribution): def __init__(self, param1, param2): super(CustomDistribution, self).__init__(validate_args=False) self.param1 = torch.tensor(param1) self.param2 = torch.tensor(param2) def sample(self, sample_shape=torch.Size()): raise NotImplementedError(\\"CustomDistribution.sample has not been implemented yet\\") def log_prob(self, value): raise NotImplementedError(\\"CustomDistribution.log_prob has not been implemented yet\\") def cdf(self, value): raise NotImplementedError(\\"CustomDistribution.cdf has not been implemented yet\\") # Example of Usage # Define a Custom Distribution dist = CustomDistribution(param1=..., param2=...) # Sample 1000 values and plot a histogram samples = dist.sample((1000,)) plt.hist(samples.numpy(), bins=50) plt.show() # Compute log-probability and CDF for some values values = torch.tensor([0.1, 0.5, 0.9]) log_probs = dist.log_prob(values) cdf_vals = dist.cdf(values) print(\\"Log-Probabilities: \\", log_probs) print(\\"CDF Values: \\", cdf_vals) ``` In summary, the task involves understanding the essentials of probabilistic modeling and distribution handling in PyTorch, engaging in custom distribution creation, and leveraging the API\'s core functionalities to validate theoretical probabilities.","solution":"import torch import torch.distributions as dist import matplotlib.pyplot as plt # Define a custom distribution class class CustomDistribution(dist.Distribution): def __init__(self, mean1, std1, mean2, std2): super().__init__(validate_args=False) self.normal1 = dist.Normal(mean1, std1) self.normal2 = dist.Normal(mean2, std2) def sample(self, sample_shape=torch.Size()): samples1 = self.normal1.sample(sample_shape) samples2 = self.normal2.sample(sample_shape) return 0.5 * samples1 + 0.5 * samples2 def log_prob(self, value): log_prob1 = self.normal1.log_prob(value) log_prob2 = self.normal2.log_prob(value) return torch.log(0.5 * torch.exp(log_prob1) + 0.5 * torch.exp(log_prob2)) def cdf(self, value): cdf1 = self.normal1.cdf(value) cdf2 = self.normal2.cdf(value) return 0.5 * cdf1 + 0.5 * cdf2 # Example of Usage # Define a Custom Distribution mean1, std1 = 0.0, 1.0 mean2, std2 = 3.0, 1.0 custom_dist = CustomDistribution(mean1, std1, mean2, std2) # Sample 1000 values and plot a histogram samples = custom_dist.sample((1000,)) plt.hist(samples.numpy(), bins=50) plt.title(\\"Histogram of Sampled Values\\") plt.xlabel(\\"Value\\") plt.ylabel(\\"Frequency\\") plt.show() # Compute log-probability and CDF for some values values = torch.tensor([0.1, 0.5, 0.9]) log_probs = custom_dist.log_prob(values) cdf_vals = custom_dist.cdf(values) print(\\"Log-Probabilities: \\", log_probs) print(\\"CDF Values: \\", cdf_vals)"},{"question":"Objective: To evaluate your understanding of the `secrets` module in Python for generating secure tokens and performing secure operations. Problem Statement: You are tasked with creating a secure password management system. Implement the following functionalities using the `secrets` module: 1. **Password Generator**: Create a function that generates a secure password with specified conditions. 2. **Token Generator**: Create a function that generates a secure URL-safe token. 3. **Password Verification**: Create a function that verifies whether two passwords (or tokens) match using a secure comparison. Function Details: 1. **generate_password(length: int, min_lower: int, min_upper: int, min_digits: int) -> str:** - Generates a password of specified length with at least `min_lower` lowercase letters, `min_upper` uppercase letters, and `min_digits` digits. - Use the `secrets` module to ensure that the password is secure. - Raise a `ValueError` if requirements cannot be met with the given length. 2. **generate_secure_token(nbytes: int = None) -> str:** - Generates a URL-safe token with the specified number of random bytes. - If `nbytes` is not supplied, the default value should be used. 3. **verify_password(password1: str, password2: str) -> bool:** - Compares two passwords using a secure, constant-time comparison. - Returns `True` if passwords match, otherwise `False`. Constraints: - The `length` parameter in `generate_password` should be between 8 and 128. - The `nbytes` parameter in `generate_secure_token` should be between 16 and 64. - You can assume `secrets` and `string` modules are available for import. Example Usage: ```python # 1. Generate a secure password password = generate_password(12, 2, 2, 3) print(password) # Example output: \'aB3dh2JkL4mP\' # 2. Generate a secure token token = generate_secure_token(32) print(token) # Example output: \'gto4grRphqzu_rj3gRnHzgsHDH37L6e7\' # 3. Verify password result = verify_password(\'password123\', \'password123\') print(result) # Example output: True result = verify_password(\'password123\', \'Password123\') print(result) # Example output: False ``` Note: - Ensure that you handle edge cases such as invalid input parameters. - Write efficient and readable code that adheres to best practices.","solution":"import secrets import string import hmac def generate_password(length: int, min_lower: int, min_upper: int, min_digits: int) -> str: Generates a secure password of specified length with the given conditions. if length < 8 or length > 128: raise ValueError(\\"Password length should be between 8 and 128.\\") if min_lower + min_upper + min_digits > length: raise ValueError(\\"Sum of min requirements exceeds password length.\\") lower = [secrets.choice(string.ascii_lowercase) for _ in range(min_lower)] upper = [secrets.choice(string.ascii_uppercase) for _ in range(min_upper)] digits = [secrets.choice(string.digits) for _ in range(min_digits)] all_chars = lower + upper + digits remaining_length = length - len(all_chars) all_chars.extend(secrets.choice(string.ascii_letters + string.digits) for _ in range(remaining_length)) secrets.SystemRandom().shuffle(all_chars) return \'\'.join(all_chars) def generate_secure_token(nbytes: int = None) -> str: Generates a URL-safe secure token. if nbytes is not None and (nbytes < 16 or nbytes > 64): raise ValueError(\\"nbytes should be between 16 and 64.\\") return secrets.token_urlsafe(nbytes) def verify_password(password1: str, password2: str) -> bool: Compares two passwords using a secure, constant-time comparison. return hmac.compare_digest(password1, password2)"},{"question":"**Objective:** Using the Seaborn library, demonstrate your understanding of categorical data visualization by loading a dataset and creating a series of customized plots. **Task:** 1. Load the \'tips\' dataset from Seaborn. 2. Create a strip plot to visualize the distribution of `total_bill` for each `day` of the week, adding jitter for better visibility. 3. Generate a box plot to summarize the distribution of `total_bill` for each `day` of the week, using different colors to distinguish \'smokers\' and \'non-smokers\'. 4. Create a violin plot to compare the distribution of `total_bill` for each `day` of the week, further dividing by `time` (Lunch/Dinner). 5. Make adjustments to the plots using the `hue`, `order`, and `dodge` parameters where applicable. 6. Use facetting to split one of your plots into multiple subplots based on the `sex` variable. 7. Explain briefly what each plot tells you about the distribution and relationships present in the \'tips\' dataset. **Requirements:** - Use the Seaborn library to create the plots. - Ensure that each plot is properly labeled with titles and axis labels. - For facetting, use the `catplot` function with the appropriate `col` or `row` parameter. - Provide a textual explanation (1-2 sentences) for each plot, summarizing the insights you derive from it. **Input:** - No direct input from the user. **Output:** - A series of plots visualized using Seaborn. - Text explanations for each plot. **Dataset:** - Use the \'tips\' dataset from Seaborn. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\'tips\') # Plot 1: Strip plot with jitter sns.catplot(data=tips, x=\'day\', y=\'total_bill\', kind=\'strip\', jitter=True) plt.title(\'Strip Plot of Total Bill by Day\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() # Plot 2: Box plot with hue for smoker sns.catplot(data=tips, x=\'day\', y=\'total_bill\', kind=\'box\', hue=\'smoker\') plt.title(\'Box Plot of Total Bill by Day with Hue for Smoker\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() # Plot 3: Violin plot with hue for time sns.catplot(data=tips, x=\'day\', y=\'total_bill\', kind=\'violin\', hue=\'time\', split=True) plt.title(\'Violin Plot of Total Bill by Day with Hue for Time\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() # Plot 4: Faceted strip plot with hue for sex sns.catplot(data=tips, x=\'day\', y=\'total_bill\', kind=\'strip\', hue=\'sex\', col=\'sex\', jitter=True) plt.suptitle(\'Strip Plot of Total Bill by Day with Hue and Facet for Sex\', y=1.03) plt.show() ``` In your submitted notebook, make sure to include both the code for generating the plots and your written explanations interpreting each plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset from Seaborn tips = sns.load_dataset(\'tips\') # Create a strip plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\'day\', y=\'total_bill\', jitter=True) plt.title(\'Strip Plot of Total Bill by Day\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() # Explanation: The strip plot shows the distribution of total bills for each day of the week. Jittering is added for better visibility. We can observe that Saturday and Sunday have higher total bills on average, suggesting more spending on weekends. # Create a box plot plt.figure(figsize=(10, 6)) sns.boxplot(data=tips, x=\'day\', y=\'total_bill\', hue=\'smoker\') plt.title(\'Box Plot of Total Bill by Day with Smokers\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() # Explanation: The box plot summarizes the distribution of total bills for each day of the week, distinguishing between smokers and non-smokers. Non-smokers seem to have slightly higher bills on weekends, while smokers show a more varied distribution. # Create a violin plot plt.figure(figsize=(10, 6)) sns.violinplot(data=tips, x=\'day\', y=\'total_bill\', hue=\'time\', split=True) plt.title(\'Violin Plot of Total Bill by Day and Time\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') plt.show() # Explanation: The violin plot visualizes the distribution of total bills for each day, further divided by time (Lunch/Dinner). We can see that dinner usually has a wider distribution of higher total bills compared to lunch. # Create a faceted strip plot g = sns.catplot(data=tips, x=\'day\', y=\'total_bill\', kind=\'strip\', hue=\'sex\', col=\'sex\', jitter=True) g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\'Strip Plot of Total Bill by Day and Sex\') # Explanation: The faceted strip plot shows the distribution of total bills by day, separated by sex. We observe similar trends in spending patterns for both male and female customers, with higher bills on weekends."},{"question":"**Question: Creating a Custom Module Loader** In this exercise, you will implement a custom module loader that can import modules from a predefined dictionary of module names to module contents. # Your Task 1. **Define a Module Directory**: Create a dictionary named `MODULE_DIR` that maps module names (strings) to their corresponding code (strings). ```python MODULE_DIR = { \\"module_a\\": \\"def hello(): return \'Hello from module_a!\'\\", \\"module_b\\": \\"def hello(): return \'Hello from module_b!\'\\" } ``` 2. **Create a Custom Loader and Finder**: Create classes `DictLoader` and `DictFinder` to facilitate importing modules from `MODULE_DIR`. - **DictLoader**: - Should inherit from `importlib.abc.Loader`. - Implements the `create_module` and `exec_module` methods. ```python import importlib.abc import types class DictLoader(importlib.abc.Loader): def create_module(self, spec): # Creating an empty module return types.ModuleType(spec.name) def exec_module(self, module): module_code = MODULE_DIR[module.__name__] exec(module_code, module.__dict__) ``` - **DictFinder**: - Should inherit from `importlib.abc.MetaPathFinder`. - Implements the `find_spec` method. ```python import importlib.util class DictFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): if fullname in MODULE_DIR: loader = DictLoader() return importlib.util.spec_from_loader(fullname, loader) return None ``` 3. **Register the Finder**: Register the `DictFinder` instance with `sys.meta_path` so that it participates in the module import process. ```python import sys sys.meta_path.insert(0, DictFinder()) ``` 4. **Test the Custom Loader**: Write a script to test importing `module_a` and `module_b` using your custom loader. ```python import module_a import module_b print(module_a.hello()) # Output: Hello from module_a! print(module_b.hello()) # Output: Hello from module_b! ``` # Constraints - You cannot modify the `sys.path`. - All modules must be imported using the custom loader and finder. # Expected Output When the test script is executed, it should print: ``` Hello from module_a! Hello from module_b! ``` This question tests the understanding of Python\'s import system, creating custom loaders and finders, and programmatic module handling.","solution":"import importlib.abc import importlib.util import sys import types # Define the module directory MODULE_DIR = { \\"module_a\\": \\"def hello(): return \'Hello from module_a!\'\\", \\"module_b\\": \\"def hello(): return \'Hello from module_b!\'\\" } # Create the custom loader class DictLoader(importlib.abc.Loader): def create_module(self, spec): # Create an empty module return types.ModuleType(spec.name) def exec_module(self, module): # Execute the module code in the module\'s namespace module_code = MODULE_DIR[module.__name__] exec(module_code, module.__dict__) # Create the custom finder class DictFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): if fullname in MODULE_DIR: loader = DictLoader() return importlib.util.spec_from_loader(fullname, loader) return None # Register the custom finder sys.meta_path.insert(0, DictFinder())"},{"question":"You are tasked with creating a command-line utility called `fileutil` using the `argparse` module. This utility should support the following functionalities: 1. **Reading a file**: The user can provide a file path, and the utility should read and print its contents to the console. 2. **Copying content between files**: The user can provide a source file and a destination file, and the utility should copy the content from the source to the destination. 3. **Concatenating files**: The user can provide multiple source files and a destination file. The utility should concatenate the contents of the source files into the destination file. 4. **Displaying help**: The utility should support a `--help` flag to show usage information. The utility should be structured to use subcommands for each of the functionalities (reading, copying, and concatenating). Below are the detailed requirements for each subcommand: 1. **read command**: - **Argument**: `file` (positional, required) - the path of the file to read. 2. **copy command**: - **Arguments**: - `src` (positional, required) - the source file path. - `dst` (positional, required) - the destination file path. 3. **concat command**: - **Arguments**: - `src` (positional, required, variadic) - a list of source file paths. - `dst` (positional, required) - the destination file path. # Instructions 1. Implement the `fileutil` utility according to the specifications provided. 2. Ensure that the utility displays appropriate error messages if the arguments are not supplied correctly. 3. Use the `argparse` module to handle the command-line arguments and subcommands. # Example Usage ```bash # Display help python fileutil.py --help # Read a file python fileutil.py read file.txt # Copy content from source.txt to dest.txt python fileutil.py copy source.txt dest.txt # Concatenate file1.txt and file2.txt into combined.txt python fileutil.py concat file1.txt file2.txt combined.txt ``` # Constraints - You must use the `argparse` module to handle command-line arguments. - The file paths provided in the arguments should be valid file paths accessible in the system where the command is run. - Handle any file-related errors (e.g., file not found, permission denied) gracefully by displaying appropriate error messages.","solution":"import argparse import os def read_file(file_path): try: with open(file_path, \'r\') as file: print(file.read()) except FileNotFoundError: print(f\\"Error: File \'{file_path}\' not found.\\") except PermissionError: print(f\\"Error: Permission denied to read \'{file_path}\'.\\") def copy_file(src, dst): try: with open(src, \'r\') as source_file: content = source_file.read() with open(dst, \'w\') as destination_file: destination_file.write(content) print(f\\"Copied content from \'{src}\' to \'{dst}\'.\\") except FileNotFoundError: print(f\\"Error: File \'{src}\' not found.\\") except PermissionError: print(f\\"Error: Permission denied to read/write \'{src}\' or \'{dst}\'.\\") def concat_files(src_files, dst): try: with open(dst, \'w\') as destination_file: for src in src_files: with open(src, \'r\') as source_file: destination_file.write(source_file.read()) print(f\\"Concatenated files {src_files} into \'{dst}\'.\\") except FileNotFoundError as e: print(f\\"Error: File \'{e.filename}\' not found.\\") except PermissionError as e: print(f\\"Error: Permission denied to read/write \'{e.filename}\'.\\") def main(): parser = argparse.ArgumentParser(description=\\"A file utility program.\\") subparsers = parser.add_subparsers(dest=\'command\', required=True) parser_read = subparsers.add_parser(\'read\', help=\'Read a file\') parser_read.add_argument(\'file\', type=str, help=\'The path of the file to read\') parser_copy = subparsers.add_parser(\'copy\', help=\'Copy content between files\') parser_copy.add_argument(\'src\', type=str, help=\'The source file path\') parser_copy.add_argument(\'dst\', type=str, help=\'The destination file path\') parser_concat = subparsers.add_parser(\'concat\', help=\'Concatenate multiple files\') parser_concat.add_argument(\'src\', nargs=\'+\', help=\'A list of source file paths\') parser_concat.add_argument(\'dst\', type=str, help=\'The destination file path\') args = parser.parse_args() if args.command == \'read\': read_file(args.file) elif args.command == \'copy\': copy_file(args.src, args.dst) elif args.command == \'concat\': concat_files(args.src, args.dst) if __name__ == \'__main__\': main()"},{"question":"Persistent To-Do List Manager using `shelve` # Objective Create a persistent to-do list manager using Python\'s `shelve` module. The to-do list should support operations to add, delete, retrieve, and list tasks while persisting the tasks to a file so that they can be retrieved later, even after the program has been terminated and restarted. # Requirements Implement a class `ToDoList` with the following methods: 1. **`__init__(self, filename: str, writeback: bool = False) -> None`**: - Initializes the to-do list manager. - Opens a shelf with the specified file name and writeback option. 2. **`add_task(self, task: str) -> None`**: - Adds a new task to the to-do list. - Each task is a string. 3. **`delete_task(self, task: str) -> bool`**: - Deletes the specified task from the to-do list. - Returns `True` if the task was successfully deleted, `False` if the task was not found. 4. **`get_task(self, task: str) -> str`**: - Retrieves the details of the specified task. - If the task does not exist, raises a `KeyError`. 5. **`list_tasks(self) -> List[str]`**: - Returns a list of all tasks in the to-do list. 6. **`close(self) -> None`**: - Closes the shelf to persist changes and free resources. # Constraints - The task names are unique and case-sensitive. - Efficiently handle memory usage, especially when dealing with a large number of tasks. # Example Usage ```python # Example usage of ToDoList class # Initialize the ToDoList manager todo_list = ToDoList(\'mytasks.db\') # Add tasks todo_list.add_task(\'Finish homework\') todo_list.add_task(\'Read a book\') # List tasks print(todo_list.list_tasks()) # Output: [\'Finish homework\', \'Read a book\'] # Retrieve a task print(todo_list.get_task(\'Finish homework\')) # Output: \'Finish homework\' # Delete a task todo_list.delete_task(\'Read a book\') # List tasks to confirm deletion print(todo_list.list_tasks()) # Output: [\'Finish homework\'] # Ensure to close the shelf when done todo_list.close() ``` # Performance Requirements - Implement efficient task retrieval and deletion mechanisms. - Ensure the application handles large sets of tasks without significant performance degradation. # Notes - Use the `shelve` module to manage the persistence of tasks. - Remember to handle the resources properly by closing the shelf appropriately to persist changes.","solution":"import shelve class ToDoList: def __init__(self, filename: str, writeback: bool = False) -> None: Initializes the to-do list manager. self.filename = filename self.writeback = writeback self.shelf = shelve.open(self.filename, writeback=self.writeback) def add_task(self, task: str) -> None: Adds a new task to the to-do list. self.shelf[task] = task self.shelf.sync() # Ensure data is written to disk immediately def delete_task(self, task: str) -> bool: Deletes the specified task from the to-do list. Returns True if the task was deleted, False if the task was not found. if task in self.shelf: del self.shelf[task] self.shelf.sync() # Ensure data is written to disk immediately return True return False def get_task(self, task: str) -> str: Retrieves the details of the specified task. if task in self.shelf: return self.shelf[task] else: raise KeyError(\'Task not found.\') def list_tasks(self) -> list: Returns a list of all tasks in the to-do list. return list(self.shelf.keys()) def close(self) -> None: Closes the shelf to persist changes and free resources. self.shelf.close()"},{"question":"You are given a set of URLs and corresponding user agents. You need to determine whether the user agents are allowed to access the URLs according to the `robots.txt` file on each website. Additionally, you need to check if a \\"Crawl-delay\\" parameter is specified for each user-agent. If it is, you should return this value; otherwise, return `None`. Instructions: 1. Implement a function `fetch_robots_info(urls: List[str], user_agents: List[str]) -> Dict[str, Dict[str, Union[bool, int, None]]]` that takes a list of URLs and a list of user agents. 2. For each URL and user agent: - Retrieve and parse the `robots.txt` file using the `urllib.robotparser.RobotFileParser` class. - Determine whether the user agent is allowed to fetch the URL. - Check if a \\"Crawl-delay\\" is specified for the user agent in the `robots.txt` file. 3. Return the results in the form of a dictionary where each URL is a key, and the value is another dictionary with user agents as keys. The values for each user agent should be a dictionary with the following keys: - `\\"can_fetch\\"`: a boolean indicating whether the user agent is allowed to fetch the URL. - `\\"crawl_delay\\"`: an integer indicating the \\"Crawl-delay\\" value, or `None` if not specified. Constraints: - Use the `urllib.robotparser.RobotFileParser` class for processing `robots.txt` files. - Assume there will not be more than 100 URLs and user agents in the input lists. Example: ```python from typing import List, Dict, Union import urllib.robotparser def fetch_robots_info(urls: List[str], user_agents: List[str]) -> Dict[str, Dict[str, Union[bool, int, None]]]: results = {} for url in urls: robot_parser = urllib.robotparser.RobotFileParser() robots_txt_url = f\\"{url}/robots.txt\\" robot_parser.set_url(robots_txt_url) robot_parser.read() results[url] = {} for user_agent in user_agents: can_fetch = robot_parser.can_fetch(user_agent, url) crawl_delay = robot_parser.crawl_delay(user_agent) results[url][user_agent] = { \\"can_fetch\\": can_fetch, \\"crawl_delay\\": crawl_delay } return results # Given example input urls = [\\"http://www.example.com\\", \\"http://www.test.com\\"] user_agents = [\\"Mozilla/5.0\\", \\"Googlebot\\"] # Example output would be a nested dictionary with URL and user agent details # Example call: # fetch_robots_info(urls, user_agents) ``` Note: The example code is a partial solution for demonstration. Ensure to provide a proper implementation that handles possible errors (e.g. unable to reach the `robots.txt` file) and edge cases for full credit.","solution":"from typing import List, Dict, Union import urllib.robotparser def fetch_robots_info(urls: List[str], user_agents: List[str]) -> Dict[str, Dict[str, Union[bool, int, None]]]: results = {} for url in urls: # Initialize the robot file parser and set the URL for the robots.txt robot_parser = urllib.robotparser.RobotFileParser() robots_txt_url = f\\"{url.rstrip(\'/\')}/robots.txt\\" robot_parser.set_url(robots_txt_url) robot_parser.read() results[url] = {} for user_agent in user_agents: can_fetch = robot_parser.can_fetch(user_agent, url) crawl_delay = robot_parser.crawl_delay(user_agent) results[url][user_agent] = { \\"can_fetch\\": can_fetch, \\"crawl_delay\\": crawl_delay } return results"},{"question":"Your task is to implement a function `process_data(data: List[Union[int, Tuple[int, ...], Dict[str, int]]]) -> Dict[str, List[int]]`. This function will process a list of integers, tuples, and dictionaries, and it should categorize the results into three lists based on the input types. Function Signature ```python def process_data(data: List[Union[int, Tuple[int, ...], Dict[str, int]]]) -> Dict[str, List[int]]: ``` # Detailed Instructions: 1. **Inputs:** - The input parameter `data` is a list containing elements of three different types: - Integers (e.g., `4`). - Tuples of integers (e.g., `(1, 2, 3)`). - Dictionaries where keys are strings and values are integers (e.g., `{\\"a\\": 1, \\"b\\": 2}`). 2. **Outputs:** - The output should be a dictionary with the following three keys: - `\\"integers\\"`: a list of all integer elements found in the `data`. - `\\"tuples\\"`: a list of all integer elements found within the tuples in the `data`. - `\\"dictionaries\\"`: a list of all integer values found within the dictionaries in the `data`. 3. **Constraints:** - You must use the `match` statement to differentiate between the types of elements in the list. - Your solution should handle large inputs efficiently within a reasonable time frame. 4. **Examples:** ```python data = [1, (2, 3), {\\"a\\": 4, \\"b\\": 5}, 6, (7, 8, 9), {\\"c\\": 10}] result = process_data(data) # Result should be: # { # \\"integers\\": [1, 6], # \\"tuples\\": [2, 3, 7, 8, 9], # \\"dictionaries\\": [4, 5, 10] # } ``` 5. **Additional Notes:** - Ensure to use the `match` statement for pattern matching elements within the input list. - Write clean, readable, and well-documented code adhering to PEP 8 guidelines. - Include appropriate type annotations for better code quality and readability. # Submission: Submit your solution in the form of a Python function keeping the function signature provided above. Additionally, include a brief explanation of your approach, edge cases considered, and tests performed.","solution":"from typing import List, Union, Tuple, Dict def process_data(data: List[Union[int, Tuple[int, ...], Dict[str, int]]]) -> Dict[str, List[int]]: result = {\\"integers\\": [], \\"tuples\\": [], \\"dictionaries\\": []} for item in data: match item: case int(): result[\\"integers\\"].append(item) case tuple(): result[\\"tuples\\"].extend(item) case dict(): result[\\"dictionaries\\"].extend(item.values()) return result"},{"question":"# PyTorch Coding Assessment Question: Tensor Storage Manipulation In this problem, you will explore and manipulate PyTorch tensor storages directly. You are required to perform the following tasks: 1. **Create a Tensor**: Start by creating a 1-dimensional tensor with three elements, all set to 1.0, of type `torch.float32`. 2. **Access Storage**: Retrieve the untyped storage of the tensor. 3. **Clone and Modify the Storage**: Clone the untyped storage and fill it entirely with zeros. 4. **Update the Tensor**: Reflect the zero-filled storage back to the original tensor such that the tensor\'s values become zeros. 5. **Verification**: Verify that the tensor values indeed match the modified storage. Detailed Steps: 1. **Create a Tensor** ```python original_tensor = torch.ones(3, dtype=torch.float32) ``` 2. **Access Storage** ```python untyped_storage = original_tensor.untyped_storage() ``` 3. **Clone and Modify Storage** ```python cloned_storage = untyped_storage.clone() cloned_storage.fill_(0) ``` 4. **Update the Tensor** ```python original_tensor.set_(cloned_storage, storage_offset=original_tensor.storage_offset(), stride=original_tensor.stride(), size=original_tensor.size()) ``` 5. **Verification** ```python assert torch.equal(original_tensor, torch.zeros(3, dtype=torch.float32)), \\"The tensor values were not updated correctly.\\" ``` Constraints: - Do not use tensor-level methods like `.zero_()` to directly set the tensor elements to zero. - Handle the storage manipulation carefully as improper handling has side-effects. - Ensure that the machine running this can execute PyTorch operations without restrictions on storage/memory access. Expected Output: ```python # Final state of original_tensor should be tensor([0., 0., 0.]) print(original_tensor) # Output should be: # tensor([0., 0., 0.]) ``` Implement the complete functionality by following the steps above and ensure that the verification step passes successfully.","solution":"import torch def manipulate_tensor_storage(): Creates a tensor, manipulates its storage and updates it. Returns: original_tensor (torch.Tensor): The modified tensor with all zero values. # Step 1: Create a Tensor original_tensor = torch.ones(3, dtype=torch.float32) # Step 2: Access Storage untyped_storage = original_tensor.untyped_storage() # Step 3: Clone and Modify Storage cloned_storage = untyped_storage.clone() cloned_storage.fill_(0) # Step 4: Update the Tensor original_tensor.set_(cloned_storage, storage_offset=original_tensor.storage_offset(), stride=original_tensor.stride(), size=original_tensor.size()) # Verification Step assert torch.equal(original_tensor, torch.zeros(3, dtype=torch.float32)), \\"The tensor values were not updated correctly.\\" return original_tensor"},{"question":"**Question: Analyzing and Handling Datasets Using scikit-learn** You are required to demonstrate your understanding of working with datasets using `sklearn.datasets`. Your task involves loading a real-world dataset, analyzing its components, and generating a synthetic dataset with similar properties. Follow the steps below to complete the task: 1. **Load the Iris Dataset**: - Import the `load_iris` function from `sklearn.datasets`. - Load the Iris dataset into a variable called `iris`. - The `iris` data should be an instance of the Bunch object. 2. **Analyze the Dataset**: - Extract and print the following components of the Iris dataset: - The feature data (`data` attribute). - The target labels (`target` attribute). - The names of the features (`feature_names` attribute). - The description of the dataset (`DESCR` attribute). 3. **Generate a Synthetic Dataset**: - Using the `make_classification` function from `sklearn.datasets`, generate a synthetic dataset with a similar number of samples and features as the Iris dataset. - The synthetic dataset should have 4 features and 150 samples to match the Iris dataset. - Set the number of informative features to 3, and the number of redundant features to 1. - Save the synthetic feature data into `X_synthetic` and the target labels into `y_synthetic`. 4. **Compare the Datasets**: - Print the shape of both the original Iris dataset (`iris.data`) and the synthetic dataset (`X_synthetic`) to ensure they match. - Comment on any differences you observe between the Iris dataset and the synthetic dataset you generated. **Code Requirements**: - The expected input is no input (the code should run without requiring any user input). - The expected output is printed information as described in the above steps. - Ensure to handle any possible exceptions that may arise during dataset loading or generation. - Use clear and understandable variable names. **Example Output** (partial): ``` Feature data of Iris dataset: [[5.1 3.5 1.4 0.2] [4.9 3.0 1.4 0.2] ... [5.9 3.0 5.1 1.8]] Target labels of Iris dataset: [0 0 0 0 0 0 0 ... 2 2 2 2 2 2 2] Feature names of Iris dataset: [\'sepal length (cm)\', \'sepal width (cm)\', \'petal length (cm)\', \'petal width (cm)\'] Description of Iris dataset: Iris Plants Database ... 150 samples ... ```","solution":"from sklearn.datasets import load_iris, make_classification def load_and_analyze_iris_dataset(): # Load the Iris dataset iris = load_iris() # Extract the components of the Iris dataset iris_data = iris.data iris_target = iris.target iris_feature_names = iris.feature_names iris_descrip = iris.DESCR print(\\"Feature data of Iris dataset:\\") print(iris_data) print(\\"nTarget labels of Iris dataset:\\") print(iris_target) print(\\"nFeature names of Iris dataset:\\") print(iris_feature_names) print(\\"nDescription of Iris dataset:\\") print(iris_descrip) return iris_data, iris_target, iris_feature_names, iris_descrip def generate_synthetic_dataset(): # Generate a synthetic dataset with similar properties to the Iris dataset X_synthetic, y_synthetic = make_classification( n_samples=150, n_features=4, n_informative=3, n_redundant=1, random_state=42 ) print(\\"nSynthetic feature data:\\") print(X_synthetic) print(\\"nSynthetic target labels:\\") print(y_synthetic) return X_synthetic, y_synthetic def compare_datasets(iris_data, X_synthetic): print(\\"nShape of the original Iris dataset:\\", iris_data.shape) print(\\"Shape of the synthetic dataset:\\", X_synthetic.shape) # Main function to encapsulate the process def main(): iris_data, iris_target, iris_feature_names, iris_descrip = load_and_analyze_iris_dataset() X_synthetic, y_synthetic = generate_synthetic_dataset() compare_datasets(iris_data, X_synthetic) # As a note: The main differences are that the synthetic dataset is randomly generated with specific informative and redundant features, while the Iris dataset is actual biological data with specific meaning. main()"},{"question":"**Objective:** To assess your understanding of the ctypes module in Python, specifically focused on loading shared libraries, defining custom data structures, and handling pointers and function calls. **Problem Statement:** You are required to interface with a C library to perform operations on a custom data type representing a 3D point. The C library exports two functions: 1. **load_library**: Loads the shared library. 2. **add_points**: Adds two `POINT3D` structures and returns the result. The C library has the following function prototypes: ```c // Function prototypes in the C library struct POINT3D { double x; double y; double z; }; struct POINT3D add_points(struct POINT3D a, struct POINT3D b); ``` **Task:** 1. Define the required `POINT3D` structure using ctypes in Python. 2. Load the shared library. 3. Implement the `add_points` function in Python to call the corresponding C function. 4. Write a test function that demonstrates adding two `POINT3D` structures and prints the result. **Implementation Details:** 1. **Define the POINT3D Structure:** - Use `ctypes.Structure` to define the structure. - Specify the `_fields_` attribute to map the structure fields. 2. **Load the Shared Library:** - Use `ctypes.CDLL` to load the shared library. - Handle any exceptions that might occur during loading. 3. **Implement the add_points:** - Define the argument and return types for the `add_points` function. - Call the `add_points` function from the shared library. 4. **Test Function:** - Define two `POINT3D` instances. - Use the `add_points` function to add them. - Print the resulting `POINT3D` instance. **Constraints:** - The shared library file is named `libpoint3d.so` (Linux) or `point3d.dll` (Windows). - The environment should have access to this shared library. **Expected Input and Output:** ```python # Define two points point1 = POINT3D(1.0, 2.0, 3.0) point2 = POINT3D(4.0, 5.0, 6.0) # Adding points using the add_points function result = add_points(point1, point2) # Expected Output # Resulting POINT3D: (5.0, 7.0, 9.0) print(f\'Resulting POINT3D: ({result.x}, {result.y}, {result.z})\') ``` **Skeleton Code:** ```python import ctypes # Define the POINT3D structure class POINT3D(ctypes.Structure): _fields_ = [(\\"x\\", ctypes.c_double), (\\"y\\", ctypes.c_double), (\\"z\\", ctypes.c_double)] def load_library(): # Load the shared library try: if ctypes.os.name == \'nt\': return ctypes.CDLL(\'point3d.dll\') else: return ctypes.CDLL(\'./libpoint3d.so\') except OSError as e: print(\\"Could not load the library:\\", e) return None def add_points(lib, a, b): # Define the function prototype for add_points add_points_func = lib.add_points add_points_func.argtypes = [POINT3D, POINT3D] add_points_func.restype = POINT3D # Call the add_points function return add_points_func(a, b) def test_add_points(): # Load the library lib = load_library() if not lib: return # Define two points point1 = POINT3D(1.0, 2.0, 3.0) point2 = POINT3D(4.0, 5.0, 6.0) # Add the points result = add_points(lib, point1, point2) # Print the result print(f\'Resulting POINT3D: ({result.x}, {result.y}, {result.z})\') # Run the test function test_add_points() ```","solution":"import ctypes import os # Define the POINT3D structure class POINT3D(ctypes.Structure): _fields_ = [(\\"x\\", ctypes.c_double), (\\"y\\", ctypes.c_double), (\\"z\\", ctypes.c_double)] def load_library(): # Load the shared library try: if os.name == \'nt\': return ctypes.CDLL(\'point3d.dll\') else: return ctypes.CDLL(\'./libpoint3d.so\') except OSError as e: print(\\"Could not load the library:\\", e) return None def add_points(lib, a, b): # Define the function prototype for add_points add_points_func = lib.add_points add_points_func.argtypes = [POINT3D, POINT3D] add_points_func.restype = POINT3D # Call the add_points function return add_points_func(a, b)"},{"question":"You are provided with a binary file and need to encode and decode it using the Python `uu` module. Your task is to implement two functions: `encode_file_to_uuencoded` and `decode_uuencoded_file`. 1. **Function `encode_file_to_uuencoded`**: - **Input**: - `input_binary_filepath`: A string representing the path of the binary file to be encoded. - `output_uuencoded_filepath`: A string representing the path where the uuencoded file should be saved. - `name`: (Optional) A string to use as the file name in the uuencoded header; if None, it defaults to the name of `input_binary_filepath`. - `mode`: (Optional) An integer representing the file mode (permissions) to be used in the uuencode header; if None, the mode defaults to `0o666`. - `backtick`: (Optional) A boolean. If true, zeros are represented by \'`\' instead of spaces. - **Output**: - None. The function should save the encoded data to `output_uuencoded_filepath`. 2. **Function `decode_uuencoded_file`**: - **Input**: - `input_uuencoded_filepath`: A string representing the path of the uuencoded file to be decoded. - `output_binary_filepath`: A string representing the path where the decoded binary data should be saved. - `mode`: (Optional) An integer for the file mode to use if creating `output_binary_filepath`; defaults to the mode specified in the uuencoded header. - `quiet`: (Optional) A boolean. If true, suppresses warnings printed to stderr. - **Output**: - None. The function should save the decoded data to `output_binary_filepath`. # Constraints - You can assume `input_binary_filepath` and `input_uuencoded_filepath` are valid paths to existing files that the functions can read. - Your code should handle opening and closing of files properly to avoid resource leaks. - Handle any exceptions that may be raised by the `uu.decode` method. # Example Usage ```python def encode_file_to_uuencoded(input_binary_filepath, output_uuencoded_filepath, name=None, mode=None, backtick=False): import uu with open(input_binary_filepath, \'rb\') as in_file, open(output_uuencoded_filepath, \'wb\') as out_file: uu.encode(in_file, out_file, name=name, mode=mode, backtick=backtick) def decode_uuencoded_file(input_uuencoded_filepath, output_binary_filepath, mode=None, quiet=False): import uu with open(input_uuencoded_filepath, \'rb\') as in_file, open(output_binary_filepath, \'wb\') as out_file: try: uu.decode(in_file, out_file, mode=mode, quiet=quiet) except uu.Error as e: print(f\\"Decoding Error: {e}\\") # Usage example encode_file_to_uuencoded(\'example.bin\', \'example.uu\', name=\'example.bin\', mode=0o644, backtick=True) decode_uuencoded_file(\'example.uu\', \'decoded_example.bin\', quiet=True) ```","solution":"def encode_file_to_uuencoded(input_binary_filepath, output_uuencoded_filepath, name=None, mode=None, backtick=False): Encodes a binary file to uuencoded format. Args: - input_binary_filepath (str): Path of the binary file to be encoded. - output_uuencoded_filepath (str): Path where the uuencoded file should be saved. - name (str, optional): Name to use in the uuencoded header. Defaults to the name of input_binary_filepath. - mode (int, optional): File mode to use in the uuencoded header. Defaults to 0o666. - backtick (bool, optional): Use backticks instead of spaces for zeros. Defaults to False. Returns: - None import uu if name is None: name = input_binary_filepath if mode is None: mode = 0o666 with open(input_binary_filepath, \'rb\') as in_file, open(output_uuencoded_filepath, \'wb\') as out_file: uu.encode(in_file, out_file, name=name, mode=mode, backtick=backtick) def decode_uuencoded_file(input_uuencoded_filepath, output_binary_filepath, mode=None, quiet=False): Decodes a uuencoded file to binary format. Args: - input_uuencoded_filepath (str): Path of the uuencoded file to be decoded. - output_binary_filepath (str): Path where the decoded binary data should be saved. - mode (int, optional): File mode to use if creating output_binary_filepath. Defaults to the mode specified in the uuencoded header. - quiet (bool, optional): Suppress warnings printed to stderr. Defaults to False. Returns: - None import uu with open(input_uuencoded_filepath, \'rb\') as in_file, open(output_binary_filepath, \'wb\') as out_file: try: uu.decode(in_file, out_file, mode=mode, quiet=quiet) except uu.Error as e: print(f\\"Decoding Error: {e}\\")"},{"question":"**Coding Assessment Question:** # Objective: Write a Python function that collects and displays detailed information about the installed packages in a Python environment. # Function Definition: ```python def display_installed_packages_info(): This function collects and displays comprehensive information about all installed packages in the current Python environment. The output should be displayed in the following format: Package: <package_name> - Version: <package_version> - Metadata: - <key_1>: <value_1> - <key_2>: <value_2> ... - Entry Points by Group: - <group_1>: - <entry_point_1>: <entry_point_value_1> - <entry_point_2>: <entry_point_value_2> ... - <group_2>: ... - Files: - <file_path_1>: (size: <size_bytes>, hash: <hash_value>) - <file_path_2>: ... ... pass ``` # Expected Input and Output: - **Input:** No input parameters. - **Output:** The function should print detailed information about each installed package in the current Python environment. # Constraints: - Only packages with \'dist-info\' or \'egg-info\' directories should be considered. - Use the functionalities provided by the `importlib.metadata` module to gather package information. - Handle exceptions gracefully while collecting the package metadata. # Instructions: 1. Retrieve the names of all installed packages. 2. For each package, gather its version and metadata. 3. For each package, list its entry points by group. 4. For each package, retrieve the list of files included in the package distribution. 5. Print the collected information in the specified format. # Example: ```python display_installed_packages_info() # Example output structure: # Package: wheel # - Version: 0.32.3 # - Metadata: # - Name: wheel # - Version: 0.32.3 # - Summary: A built package format for Python. # - Author: ... # - Entry Points by Group: # - console_scripts: # - wheel: wheel.cli:main # - Files: # - wheel/__init__.py: (size: 1287, hash: <hash_value>) # - wheel/cli.py: (size: 4890, hash: <hash_value>) # ... ``` **Notes:** - The actual output will depend on the packages installed in the environment. - Ensure to use the `importlib.metadata` library methods like `distributions()`, `entry_points()`, `metadata()`, `version()`, and `files()` to gather information.","solution":"import importlib.metadata import os import hashlib def display_installed_packages_info(): This function collects and displays comprehensive information about all installed packages in the current Python environment. The output should be displayed in the specified format. for dist in importlib.metadata.distributions(): # Package Name package_name = dist.metadata[\'Name\'] print(f\\"Package: {package_name}\\") # Version package_version = dist.version print(f\\"- Version: {package_version}\\") # Metadata print(\\"- Metadata:\\") metadata = dist.metadata for key, value in metadata.items(): print(f\\" - {key}: {value}\\") # Entry Points by Group entry_points = dist.entry_points entry_point_groups = {} for entry in entry_points: if entry.group not in entry_point_groups: entry_point_groups[entry.group] = [] entry_point_groups[entry.group].append((entry.name, entry.value)) print(\\"- Entry Points by Group:\\") for group, entries in entry_point_groups.items(): print(f\\" - {group}:\\") for name, value in entries: print(f\\" - {name}: {value}\\") # Files print(\\"- Files:\\") files = dist.files for file in files: file_path = str(file) try: file_size = os.path.getsize(file_path) file_hash = hashlib.md5(open(file_path, \'rb\').read()).hexdigest() print(f\\" - {file_path}: (size: {file_size}, hash: {file_hash})\\") except Exception as e: print(f\\" - {file_path}: (error: {str(e)})\\") print() # Call the function to display installed package information display_installed_packages_info()"},{"question":"# Question: Concurrent Web Scraper You are tasked with implementing a concurrent web scraper using Python\'s `concurrent.futures` module. The goal is to fetch the contents of multiple web pages concurrently and process the data efficiently. Here is the detailed specification: Function Signature ```python def fetch_urls(urls: List[str], timeout: int = 10) -> Dict[str, Dict[str, Union[str, int]]]: Concurrently fetch the contents of the provided URLs and return a dictionary with the status of each URL. Args: urls: List[str] - A list of URLs to fetch. timeout: int - The timeout value for each URL fetch operation. Returns: Dict[str, Dict[str, Union[str, int]]] - A dictionary where each URL is a key, and the value is another dictionary containing: - \'status\': HTTP status code or \'error\' - \'content_length\': Length of the fetched content or 0 in case of error ``` Description 1. **Inputs** - `urls`: A list of URLs to fetch. - `timeout`: An optional timeout value (in seconds) for each URL fetch operation, defaulting to 10 seconds. 2. **Outputs** - A dictionary where: - Each key is a URL from the input list. - Each value is another dictionary containing: - `\'status\'`: The HTTP status code of the response or `\'error\'` if the request failed. - `\'content_length\'`: The length of the fetched content, or 0 if an error occurred. 3. **Constraints** - Fetch operations must be performed concurrently to optimize performance. - Properly handle exceptions and include meaningful error information. - Ensure all threads are properly shut down after execution. 4. **Performance Requirements** - Use `ThreadPoolExecutor` with a sensible number of worker threads. - Avoid blocking the main thread unnecessarily. Example ```python urls = [ \\"http://www.example.com\\", \\"http://www.nonexistentwebsite.com\\", \\"http://www.python.org\\" ] result = fetch_urls(urls, timeout=5) print(result) ``` **Expected Output:** ```python { \\"http://www.example.com\\": {\\"status\\": 200, \\"content_length\\": 1270}, \\"http://www.nonexistentwebsite.com\\": {\\"status\\": \\"error\\", \\"content_length\\": 0}, \\"http://www.python.org\\": {\\"status\\": 200, \\"content_length\\": 15123} } ``` Hints - Utilize `concurrent.futures.ThreadPoolExecutor` for concurrent requests. - Use `urllib.request.urlopen` for fetching URLs. - Handle exceptions like `urllib.error.URLError` and `concurrent.futures.TimeoutError`. Implement the function `fetch_urls` based on the provided signature and specifications.","solution":"import urllib.request import urllib.error from concurrent.futures import ThreadPoolExecutor, as_completed from typing import List, Dict, Union def fetch_url(url: str, timeout: int) -> Dict[str, Union[str, int]]: Fetches the contents of the URL and returns a dictionary containing status and content length. Args: url: str - The URL to fetch. timeout: int - The timeout value for each URL fetch operation. Returns: Dict[str, Union[str, int]] - A dictionary with \'status\' and \'content_length\' keys. try: with urllib.request.urlopen(url, timeout=timeout) as response: content = response.read() return { \'status\': response.getcode(), \'content_length\': len(content) } except (urllib.error.URLError, urllib.error.HTTPError) as e: return { \'status\': \'error\', \'content_length\': 0 } def fetch_urls(urls: List[str], timeout: int = 10) -> Dict[str, Dict[str, Union[str, int]]]: Concurrently fetch the contents of the provided URLs and return a dictionary with the status of each URL. Args: urls: List[str] - A list of URLs to fetch. timeout: int - The timeout value for each URL fetch operation. Returns: Dict[str, Dict[str, Union[str, int]]] - A dictionary where each URL is a key, and the value is another dictionary containing: - \'status\': HTTP status code or \'error\' - \'content_length\': Length of the fetched content or 0 in case of error results = {} with ThreadPoolExecutor(max_workers=10) as executor: future_to_url = {executor.submit(fetch_url, url, timeout): url for url in urls} for future in as_completed(future_to_url): url = future_to_url[future] try: result = future.result() except Exception as e: result = {\'status\': \'error\', \'content_length\': 0} results[url] = result return results"},{"question":"# Question: Scope and Exception Handling in Python Implement a function `nested_variable_management` that performs the following tasks: 1. Define a nested function structure with at least three levels (global scope, one intermediate function, and one inner-most function). 2. The inner-most function should manipulate a non-local variable defined in the intermediate function. 3. The intermediate function should manipulate a global variable. 4. Properly handle potential exceptions that could arise during manipulation. **Function Signature:** ```python def nested_variable_management(): pass ``` **Detailed Requirements:** - Define a global variable `global_var` initialized to the value 10. - The intermediate function should change the value of `global_var` to 20. - The intermediate function should define a non-local variable `nonlocal_var` initialized to the value 30. - The inner-most function should modify `nonlocal_var` to be 40. - Add exception handling to manage scenarios where the variables may not be properly defined or modified. **Expected Behavior:** - When `nested_variable_management` is called, it should: - Attempt to perform all variable manipulations. - Print the final values of `global_var` and `nonlocal_var`. - Print appropriate messages if any exceptions are raised (e.g., if trying to access an external variable before definition). **Example Function Call:** ```python nested_variable_management() ``` **Expected Output:** ``` global_var: 20 nonlocal_var: 40 ``` **Constraints:** - The function should work with Python 3.10. - Maintain readability and proper usage of comments to explain each section. # Notes: - Focus on carefully implementing the scope rules and proper exception handling mechanisms. - Avoid using additional libraries or constructs not covered in the provided documentation.","solution":"def nested_variable_management(): global global_var global_var = 10 def intermediate_function(): nonlocal_var = 30 # Modify the global variable try: global global_var global_var = 20 except Exception as e: print(f\\"Error modifying global_var: {e}\\") def inner_most_function(): nonlocal nonlocal_var try: nonlocal_var = 40 except Exception as e: print(f\\"Error modifying nonlocal_var: {e}\\") # Call the innermost function to modify the nonlocal variable inner_most_function() print(f\\"nonlocal_var: {nonlocal_var}\\") # Call the intermediate function to make the changes intermediate_function() print(f\\"global_var: {global_var}\\")"},{"question":"# Question: Advanced Data Visualization with Seaborn You are provided with the `tips` dataset from the seaborn library, which contains information about the tips received and other associated details over various days and times. Your task is to create an advanced data visualization using `seaborn` that demonstrates your understanding of multiple features of the seaborn package. Dataset Description The `tips` dataset contains the following columns: - `total_bill`: Total bill amount (numeric). - `tip`: Tip amount (numeric). - `sex`: Gender of the person paying the bill (categorical: \\"Male\\" or \\"Female\\"). - `smoker`: Whether the person is a smoker (categorical: \\"Yes\\" or \\"No\\"). - `day`: Day of the week (categorical: \\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"). - `time`: Time of day (categorical: \\"Lunch\\" or \\"Dinner\\"). - `size`: Size of the dining party (numeric). Task 1. **Scatter Plot with Multiple Dimensions:** Create a scatter plot using the `tips` dataset where: - The x-axis represents the `total_bill`. - The y-axis represents the `tip`. - Points are colored based on the `day` of the week. - Points have different markers based on the `time` of day. - Points vary in size based on the `size` of the dining party. 2. **Facet Grid Combination:** Use `relplot` to create multiple scatter plots, each in a separate subplot, where: - The subplots are arranged by `time` of day (i.e., \\"Lunch\\" and \\"Dinner\\"). - Within each subplot, points are colored based on the `day` of the week. - Markers and other plot properties previously mentioned remain consistent. # Requirements: - Use the `seaborn` library to create the plots. - Customize the color palette appropriately for better visualization. - Ensure all legends and labels are clear and informative. - Create a Jupyter Notebook cell for each step of the task. Input: There is no input required from the user for this task. Simply use the provided `tips` dataset. Output: A set of visualizations including: 1. A scatter plot with multiple dimensions. 2. A facet grid scatter plot combining different plots for \\"Lunch\\" and \\"Dinner\\". ```python # Sample code to get you started (Do not include in the final submission) import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Step 1: Create the scatter plot with multiple dimensions plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\") plt.title(\\"Scatter Plot of Tips\\") plt.show() # Step 2: Create the facet grid scatter plot sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", size=\\"size\\", sizes=(20, 200), palette=\\"deep\\", legend=\\"full\\", kind=\\"scatter\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") def scatter_plot_with_multiple_dimensions(): Creates a scatter plot from the tips dataset where: - x-axis represents total_bill - y-axis represents tip - Points are colored based on the day of the week - Points have different markers based on the time of day - Points vary in size based on the size of the dining party plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", size=\\"size\\", sizes=(20, 200), palette=\\"viridis\\" ) scatter_plot.set_title(\\"Scatter Plot: Total Bill vs Tip by Day, Time, and Party Size\\") plt.legend(bbox_to_anchor=(1.05, 1), loc=2) plt.show() def facet_grid_scatter_plot(): Creates a facet grid scatter plot from the tips dataset where: - x-axis represents total_bill - y-axis represents tip - Each subplot is arranged by time of day (Lunch and Dinner) - Points are colored based on the day of the week - Points vary in size based on the size of the dining party facet_plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", size=\\"size\\", sizes=(20, 200), palette=\\"deep\\", legend=\\"full\\", kind=\\"scatter\\" ) facet_plot.set_titles(\\"{col_name} Time\\") plt.show()"},{"question":"You are required to implement a function `configure_terminal_for_password_input()` that will configure the terminal to disable echo (i.e., hide user input when typing a password) and then reads the entered password from the user. The function should ensure that the terminal configuration is safely restored to its original state regardless of whether an error occurs or not during password entry. Function Signature: ```python def configure_terminal_for_password_input(prompt: str = \\"Enter Password: \\") -> str: pass ``` Input: - `prompt` (str): A string representing the prompt to be displayed to the user. Default value is `\\"Enter Password: \\"`. Output: - Returns the password entered by the user as a string. Additional Requirements: 1. Use the `termios` module to configure the terminal settings. 2. Ensure that the original terminal settings are restored after the password is entered, even if an exception occurs. 3. Use the `termios.tcsetattr()` and `termios.tcgetattr()` functions to manipulate the tty attributes. 4. You can assume the script runs in a UNIX-like environment where `termios` is available. Example Usage: ```python password = configure_terminal_for_password_input(\\"Please type your password: \\") print(\\"Your password is\\", password) ``` In this example, the terminal will not display the characters as the user types the password, and after entering it, it will print the entered password. # Constraints: - The terminal control operations should be robust and handle exceptions gracefully. - The function should ensure that the terminal settings are always reverted to their original state after the password is entered. Implement the `configure_terminal_for_password_input` function ensuring it meets all the given requirements.","solution":"import termios import sys import os def configure_terminal_for_password_input(prompt: str = \\"Enter Password: \\") -> str: Configure the terminal to disable echo for password input, read the password, then restore the terminal to its original settings. Parameters: - prompt (str): The prompt message to display when asking for the password. Returns: - str: The password entered by the user. fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) try: # Turn off the echoing of characters new_settings = termios.tcgetattr(fd) new_settings[3] = new_settings[3] & ~termios.ECHO termios.tcsetattr(fd, termios.TCSADRAIN, new_settings) # Prompt the user for the password password = input(prompt) finally: # Restore the terminal settings termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) return password"},{"question":"# Advanced Asynchronous Execution With `concurrent.futures` You are tasked with designing a multi-threaded job processing system using the `concurrent.futures` module in Python. The system will process a list of tasks and return the results once all tasks are completed. Requirements 1. Create a function `process_task(task_id, duration)` that simulates task processing by sleeping for `duration` seconds and then returning a message indicating completion. 2. Implement a `manage_tasks(tasks)` function which takes a list of tuples, where each tuple consists of a `task_id` and `duration`, and processes them asynchronously using `ThreadPoolExecutor`. 3. Ensure tasks are processed concurrently but limit the maximum number of threads to 4. 4. Return a list of result messages ordered by the completion time of the tasks. Input - A list of tuples `tasks`, where each tuple contains: - `task_id` (int): A unique identifier for the task. - `duration` (int): The number of seconds the task takes to complete. Output - A list of strings, where each string is a result message from `process_task()`. Constraints - Assume that `tasks` will always contain at least one task and at most 100 tasks. - The `duration` for each task will be between 1 and 10 seconds. Example ```python import time from concurrent.futures import ThreadPoolExecutor, as_completed def process_task(task_id, duration): # Simulate task processing time.sleep(duration) return f\\"Task {task_id} completed in {duration} seconds\\" def manage_tasks(tasks): results = [] with ThreadPoolExecutor(max_workers=4) as executor: future_to_task = {executor.submit(process_task, task_id, duration): (task_id, duration) for task_id, duration in tasks} for future in as_completed(future_to_task): result = future.result() results.append(result) return results # Input tasks = [(1, 2), (2, 1), (3, 3), (4, 2)] # Output result = manage_tasks(tasks) for message in result: print(message) ``` Output: ``` Task 2 completed in 1 seconds Task 1 completed in 2 seconds Task 4 completed in 2 seconds Task 3 completed in 3 seconds ``` # Explanation: In this example, the tasks list `[(1, 2), (2, 1), (3, 3), (4, 2)]` indicates four tasks with their respective processing durations. The `manage_tasks` function should manage these tasks concurrently using up to 4 threads, ensuring that the results are gathered in the order they complete. Notes - You must use `ThreadPoolExecutor` to manage threading. - Consider exception handling for unforeseen errors during task processing. - Your solution should handle concurrency efficiently and demonstrate a strong understanding of asynchronous programming in Python.","solution":"import time from concurrent.futures import ThreadPoolExecutor, as_completed def process_task(task_id, duration): Simulates task processing by sleeping for `duration` seconds. Returns a message indicating the task completion. time.sleep(duration) return f\\"Task {task_id} completed in {duration} seconds\\" def manage_tasks(tasks): Manages the tasks concurrently using ThreadPoolExecutor. Args: - tasks: a list of tuples where each tuple contains task_id and duration. Returns: - A list of result messages ordered by the completion time of the tasks. results = [] with ThreadPoolExecutor(max_workers=4) as executor: # Submit all tasks to the executor future_to_task = {executor.submit(process_task, task_id, duration): (task_id, duration) for task_id, duration in tasks} # Collect results as they are completed for future in as_completed(future_to_task): result = future.result() results.append(result) return results"},{"question":"# PyTorch Accelerator Management You are provided with access to a system equipped with accelerator devices (e.g., GPUs). Using the `torch.accelerator` package, you should write a function to manage and utilize these devices effectively. Your task is to implement a function that: 1. Checks if any accelerator is available. 2. If available, retrieves the count of accelerator devices and sets the first device as active. 3. Verifies if the active device has been set correctly. 4. Ensures that the operations on the selected device are synchronized. 5. Returns a tuple with the device count, the index of the currently active device, and a boolean indicating successful synchronization. # Function Signature ```python def manage_accelerator() -> Tuple[int, int, bool]: ``` # Expected Input and Output - **Input:** The function does not take any input parameters. - **Output:** A tuple containing: - Number of accelerator devices available (int). - Index of the currently active device (int). - Boolean indicating if synchronization was successful. # Constraints - You should use functions from the `torch.accelerator` module only. - Assume you cannot change device assignments during runtime. - The code should handle scenarios where no accelerator is available. # Example ```python result = manage_accelerator() print(result) # Example Output: (2, 0, True) if two devices are present and the first is set and synchronized ``` # Notes - Remember to handle cases where no accelerator is available by returning `(0, -1, False)`.","solution":"import torch from typing import Tuple def manage_accelerator() -> Tuple[int, int, bool]: Checks and manages accelerator devices and returns the device count, the index of the active device, and a boolean indicating successful synchronization. if torch.cuda.is_available(): device_count = torch.cuda.device_count() torch.cuda.set_device(0) active_device = torch.cuda.current_device() try: torch.cuda.synchronize() synchronized = True except Exception as e: synchronized = False else: device_count = 0 active_device = -1 synchronized = False return device_count, active_device, synchronized"},{"question":"Your task is to implement a function `custom_import(module_name: str) -> object` that mimics the behavior of the deprecated `imp` module functions `find_module` and `load_module` using the modern `importlib` module. Specifically, the function should locate and load a Python module given its name. # Requirements: 1. The function should first look for the module using the modern `importlib` approach (`importlib.util.find_spec`). 2. Once the module is located, it should use `importlib.util.spec_from_file_location` and `importlib.util.module_from_spec` to load the module. 3. The loaded module should then be returned. # Input: - `module_name` (str): The name of the module to be imported. # Output: - Returns the module object if found and loaded successfully. - Raises `ImportError` if the module cannot be found or loaded. # Example: ```python # Assuming there\'s a module named my_module.py in the current directory mod = custom_import(\\"my_module\\") print(mod) ``` This should print the module object for \\"my_module\\". # Constraints: - You are **not** allowed to use the built-in `__import__` function or the deprecated `imp` module. - Your code should be compatible with Python 3.5 and above. Below is a starting point for your implementation: ```python import importlib.util import sys def custom_import(module_name: str) -> object: # Your implementation here pass # Example usage (uncomment to test): # mod = custom_import(\\"my_module\\") # print(mod) ```","solution":"import importlib.util import sys def custom_import(module_name: str) -> object: Imports a module given its name using the modern importlib approach. spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module"},{"question":"**Objective:** You are required to implement a function `sum_of_evens` that takes an array of signed integers and returns the sum of all even numbers in the array. Make full use of the `array` module and ensure the solution is efficient in terms of both time and space. **Function Signature:** ```python def sum_of_evens(arr: array) -> int: pass ``` **Input:** - `arr`: An array of signed integers, created using the `array` module with type code `\'i\'`. **Output:** - Returns an integer which is the sum of all even numbers in the array. **Constraints:** - The array will contain at most (10^6) integers. - Each element of the array will be a signed integer between (-10^9) and (10^9). # Example ```python from array import array # Example 1 arr = array(\'i\', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) print(sum_of_evens(arr)) # Output: 30 # Example 2 arr = array(\'i\', [-10, 0, 5, 12, 14, -3]) print(sum_of_evens(arr)) # Output: 16 # Example 3 arr = array(\'i\', [1, 3, 5, 7, 9, 11]) print(sum_of_evens(arr)) # Output: 0 ``` # Requirements - You must use the `array` module from Python to handle the input array. - You should not use any other data structures like lists to store your array elements. - Your implementation should be optimized to handle large arrays efficiently in terms of both time and space. Implement the `sum_of_evens` function and ensure it meets the requirements outlined above.","solution":"from array import array def sum_of_evens(arr: array) -> int: Returns the sum of all even numbers in the input array. Parameters: arr (array): An array of signed integers. Returns: int: The sum of all even numbers in the array. return sum(x for x in arr if x % 2 == 0)"},{"question":"Advanced Tensor Operations and Automatic Differentiation in PyTorch **Objective:** Your task is to implement a function using PyTorch that will perform the following steps: 1. Create a 3x3 tensor with values ranging from 1 to 9 (inclusive) of `dtype=torch.float64`. 2. Reshape the tensor into a 1x9 tensor and move it to a CUDA device if available; otherwise, keep it on the CPU. 3. Perform an element-wise square operation on the tensor and then calculate the sum of all the elements. 4. Use automatic differentiation to compute the gradients of the sum with respect to the input tensor. **Function Signature:** ```python import torch def advanced_tensor_operations(): pass ``` **Expected Workflow:** 1. **Tensor Creation and Data Type Specification:** - Create the initial tensor with values from 1 to 9 and ensure it’s of `dtype=torch.float64`. 2. **Reshaping and Device Management:** - Reshape the tensor to a 1x9 tensor. - Check if CUDA is available, and if so, move the tensor to a CUDA device. 3. **Tensor Operations:** - Perform element-wise square of the tensor. - Calculate the sum of all elements. 4. **Automatic Differentiation:** - Set `requires_grad=True` for the initial tensor before operations. - Compute gradients of the sum with respect to the original tensor. **Input:** - None (The function does not take any input) **Output:** - The function should print the following: 1. The reshaped tensor. 2. The sum of the squared elements. 3. The gradients with respect to the original tensor. **Example:** ```python reshaped_tensor: tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]], device=\'cuda:0\', dtype=torch.float64) sum_of_squares: tensor(285., device=\'cuda:0\', dtype=torch.float64) gradients: tensor([[2., 4., 6., 8., 10., 12., 14., 16., 18.]], device=\'cuda:0\', dtype=torch.float64) ``` *Note: Your results should match the indices and values but may differ on the device depending on CUDA availability.* **Constraints:** - Use PyTorch\'s tensor operations and methods exclusively. - Ensure the code handles the device management (CPU/CUDA) appropriately. *Hint:* - Use `tensor.to(device)` for device management. - Use `requires_grad=True` when creating the tensor to enable automatic differentiation. - Use the `.sum()` method to calculate the sum of tensor elements. - Use `.backward()` on the resulting sum tensor to compute gradients.","solution":"import torch def advanced_tensor_operations(): # Step 1: Create a 3x3 tensor with values from 1 to 9 initial_tensor = torch.arange(1, 10, dtype=torch.float64).reshape(3, 3) # Step 2: Reshape the tensor into a 1x9 tensor reshaped_tensor = initial_tensor.reshape(1, 9) # Check for CUDA availability and move the tensor to the appropriate device device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") reshaped_tensor = reshaped_tensor.to(device) # Set requires_grad=True for automatic differentiation reshaped_tensor.requires_grad_(True) # Step 3: Perform element-wise square and calculate the sum of elements squared_tensor = reshaped_tensor ** 2 sum_of_squares = squared_tensor.sum() # Step 4: Compute gradients sum_of_squares.backward() # Print the results print(f\\"reshaped_tensor: {reshaped_tensor}\\") print(f\\"sum_of_squares: {sum_of_squares}\\") print(f\\"gradients: {reshaped_tensor.grad}\\") return reshaped_tensor, sum_of_squares, reshaped_tensor.grad"},{"question":"**Question** You are given the `penguins` dataset, which contains various measurements of penguins\' traits across different species and sexes. Using Seaborn\'s `Plot` object and the `Dash` and `Dots` marks, create a visualization that meets the following criteria: 1. **Plot Setup**: - Load the `penguins` dataset from Seaborn. - Create a `Plot` object with `species` on the x-axis and `body_mass_g` on the y-axis. - Map the `color` of the data points to `sex`. 2. **Primary Mark**: - Add `Dash` marks to represent individual penguin data points. - Make the dashes semi-transparent with `alpha=0.5`. - Set the width of the dashes relative to the `flipper_length_mm`. 3. **Dodge Adjustment**: - Use a dodge adjustment to separate the dashes by sex, avoiding overlap. 4. **Additional Marks**: - Overlay the plot with `Dots` marks to show individual data points. - Apply `Dodge` and `Jitter` adjustments to the dots for better visualization. **Constraints and Requirements**: - The implementation should not produce any warnings or errors. - Ensure the plot is well-labeled and clearly visualizes the differences among species and sexes. - Your implementation should be efficient and make appropriate use of Seaborn\'s capabilities. **Expected Input and Output**: - **Input**: The `penguins` dataset is pre-loaded from Seaborn. - **Output**: A visual plot showing the required marks and adjustments. **Performance Requirements**: - The plot should render within a reasonable time frame when run on a standard modern computer. **Code Template**: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot p = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") # Add Dash marks with required properties p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") # Apply dodge adjustment to the dashes p.add(so.Dash(), so.Dodge()) # Overlay the plot with Dots marks and apply dodge and jitter adjustments p.add(so.Dots(), so.Dodge(), so.Jitter()) # Display the plot <YOUR_PLOT_DISPLAY_CODE_HERE> ``` Complete the code to display the plot.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot p = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") # Add Dash marks with required properties p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") # Apply dodge adjustment to the dashes p.add(so.Dash(alpha=0.5), so.Dodge()) # Overlay the plot with Dots marks and apply dodge and jitter adjustments p.add(so.Dots(), so.Dodge(), so.Jitter()) # Display the plot p.show()"},{"question":"**Question: Implement a Function to Extract and Print All Chunk Names and Sizes** You are provided with the `chunk` module, which supports reading files that use the EA IFF 85 chunk format. You need to implement a function `print_chunk_info(file_path)` that reads a file and prints the names and sizes of all chunks in the file. # Function Signature: ```python def print_chunk_info(file_path: str) -> None: pass ``` # Input: - `file_path` (str): A string representing the path to the file that contains chunked data. # Output: - The function does not return anything. It prints the names and sizes of all chunks in the file in the following format: ``` Chunk Name: {name}, Chunk Size: {size} ``` where `{name}` is the chunk\'s 4-byte ID and `{size}` is the size of the chunk data in bytes. # Constraints: - Assume the file uses the EA IFF 85 chunk format. - Do not close the underlying file after extracting the chunk information. - Handle both aligned and non-aligned chunks as specified in the initial chunk\'s properties. - Consider both big-endian and little-endian byte orders, though default to big-endian if not specified. # Example: Given a file `example.iff` with the following chunk structure: ``` Offset Length Contents 0 4 \'FORM\' 4 4 100 8 *n* Data bytes 108 4 \'DATA\' 112 4 50 116 *n* Data bytes ``` When `print_chunk_info(\'example.iff\')` is called, it should print: ``` Chunk Name: FORM, Chunk Size: 100 Chunk Name: DATA, Chunk Size: 50 ``` # Hints: - Utilize the methods provided by the `chunk.Chunk` class to navigate through the file and extract necessary information. - Pay attention to alignment and endian-ness properties as set by the initial chunk configuration.","solution":"import chunk def print_chunk_info(file_path: str) -> None: Reads an IFF-formatted file and prints the names and sizes of all chunks in the file. :param file_path: The path to the IFF file. try: with open(file_path, \'rb\') as f: # Create a top-level chunk ch = chunk.Chunk(f, bigendian=True, align=True) while True: print(f\\"Chunk Name: {ch.getname().decode(\'ascii\')}, Chunk Size: {ch.getsize()}\\") ch.skip() ch = chunk.Chunk(f, bigendian=True, align=True) except EOFError: # End of file reached pass except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Objective:** Create visualizations using seaborn\'s violinplot to analyze the relationship between age and fare among passengers of different classes from the Titanic dataset. Your plots will involve various levels of customization, highlighting your ability to manipulate seaborn\'s features for effective data representation. **Problem Statement:** Using the Titanic dataset from seaborn\'s built-in datasets, perform the following tasks: 1. **Basic Violin Plot:** - Create a violin plot showing the distribution of passenger ages. 2. **Grouped Violin Plot:** - Create a violin plot showing the distribution of passenger ages, grouped by passenger class (`class`). 3. **Customized Violin Plot:** - Create a bivariate violin plot showing the distribution of ages grouped by class and differentiated by survival status (`alive`). Customize the plot to: - Show split violins. - Use line-art style (no inner fill). 4. **Advanced Customization:** - Create a violin plot showing the relationship between passenger ages and fares. Customize the plot to: - Preserve the native scale of the age (rounded to the nearest decade) on the x-axis and fare on the y-axis. - Modify the `inner` parameter to represent individual observations as `stick`. - Use `bw_adjust=0.5` to reduce the amount of smoothing. - Normalize the width of each violin to represent the number of observations. # Submission Requirements: - Submit a `.py` or Jupyter notebook file containing the Python code implementation for the above tasks. - Use appropriate comments to explain each part of your code. - The plots generated should be saved as images and included in the submission. **Constraints:** - Use seaborn version >= 0.13.0. - Ensure your code runs without errors. **Evaluation Criteria:** 1. **Correctness:** Plots are correctly generated using the specified customizations. 2. **Code Quality:** Code is clean, well-commented, and follows best practices. 3. **Visualization:** Plots are accurately labeled, legible, and visually effective in representing the data.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_basic_violinplot(data): Creates a basic violin plot showing the distribution of passenger ages. plt.figure(figsize=(10, 6)) sns.violinplot(x=data[\'age\']) plt.title(\'Distribution of Passenger Ages\') plt.xlabel(\'Age\') plt.show() def create_grouped_violinplot(data): Creates a grouped violin plot showing the distribution of passenger ages, grouped by passenger class. plt.figure(figsize=(10, 6)) sns.violinplot(x=data[\'pclass\'], y=data[\'age\']) plt.title(\'Age Distribution by Passenger Class\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.show() def create_customized_violinplot(data): Creates a customized bivariate violin plot showing the distribution of ages grouped by class and differentiated by survival status. Customized to show split violins with line-art style. plt.figure(figsize=(10, 6)) sns.violinplot(x=data[\'pclass\'], y=data[\'age\'], hue=data[\'alive\'], split=True, inner=None) plt.title(\'Age Distribution by Class and Survival Status\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.show() def create_advanced_customized_violinplot(data): Creates a violin plot showing the relationship between passenger ages and fares. Customized to preserver native scale, show individual observations as \'stick\', and reduce smoothing with bw_adjust=0.5. # Round ages to the nearest decade for better visualization data[\'age_rounded\'] = np.round(data[\'age\'] / 10) * 10 plt.figure(figsize=(10, 6)) sns.violinplot(x=\'age_rounded\', y=\'fare\', data=data, inner=\'stick\', bw_adjust=0.5, scale=\'count\') plt.title(\'Fare Distribution by Rounded Age\') plt.xlabel(\'Rounded Age (Decade)\') plt.ylabel(\'Fare\') plt.show()"},{"question":"# PyTorch IR Coding Assessment Objective The task is to implement a function that demonstrates the use of Core Aten IR and Prims IR. This function will simulate a basic backend operation that transforms a given input tensor through several intermediate transformations leveraging both Core Aten IR and Prims IR concepts. Problem Statement Implement a function `transform_tensor` that takes an input tensor and performs the following operations: 1. Use Core Aten IR operations to scale the input tensor. 2. Use Prims IR operations to normalize the scaled tensor. 3. Ensure the final tensor is cast to a specified type. Function Signature ```python import torch def transform_tensor(input_tensor: torch.Tensor, scale_factor: float, target_type: torch.dtype) -> torch.Tensor: Transforms the input tensor by scaling it using Core Aten IR operations, then normalizing it using Prims IR operations, and finally, casting it to the specified target type. Parameters: input_tensor (torch.Tensor): The input tensor to transform. scale_factor (float): The factor by which to scale the input tensor. target_type (torch.dtype): The data type to cast the final tensor to. Returns: torch.Tensor: The transformed tensor. pass ``` Guidelines 1. **Scaling using Core Aten IR**: - Multiply the input tensor by the `scale_factor`. 2. **Normalization using Prims IR**: - Implement normalization as `(tensor - mean) / std`, where `mean` and `std` are calculated over the tensor. 3. **Type Casting**: - Finally, cast the tensor to the `target_type`. 4. **Input and Output Constraints**: - The input tensor will always be a 2D tensor (torch.Tensor). - The `scale_factor` will always be a positive float. - The function should return a tensor of the same shape as the input tensor but with the data type specified by `target_type`. Example ```python input_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) scale_factor = 2.0 target_type = torch.float32 transformed_tensor = transform_tensor(input_tensor, scale_factor, target_type) print(transformed_tensor) ``` This example should output a tensor that has been scaled, normalized, and cast to `float32`. Notes - Make sure to use appropriate PyTorch operations to accomplish the required transformations. - The function should be efficient and make good use of PyTorch\'s built-in functions.","solution":"import torch def transform_tensor(input_tensor: torch.Tensor, scale_factor: float, target_type: torch.dtype) -> torch.Tensor: Transforms the input tensor by scaling it using Core Aten IR operations, then normalizing it using Prims IR operations, and finally, casting it to the specified target type. Parameters: input_tensor (torch.Tensor): The input tensor to transform. scale_factor (float): The factor by which to scale the input tensor. target_type (torch.dtype): The data type to cast the final tensor to. Returns: torch.Tensor: The transformed tensor. # Scale the input tensor scaled_tensor = input_tensor * scale_factor # Calculate mean and standard deviation for normalization mean = scaled_tensor.mean() std = scaled_tensor.std() # Normalize the scaled tensor normalized_tensor = (scaled_tensor - mean) / std # Cast to the specified target type final_tensor = normalized_tensor.to(target_type) return final_tensor"},{"question":"# Question: Gaussian Mixture Models with Scikit-Learn You are provided with a dataset `data.csv` containing 2-dimensional data points for clustering analysis. Your task is to use the scikit-learn package `sklearn.mixture` to create Gaussian Mixture Models and perform the following steps: 1. **Import the data**: Load the data from `data.csv` into a pandas DataFrame. 2. **Data Preprocessing**: Perform necessary data preprocessing, if any, on the dataset. 3. **Gaussian Mixture Model Clustering**: - Create a Gaussian Mixture Model (GMM) instance with a specified number of components (`n_components`). - Fit the model to the data. - Predict the cluster for each data point and add the cluster labels as a new column in the DataFrame. 4. **Model Evaluation**: - Compute the Bayesian Information Criterion (BIC) to evaluate the fitted model. - Experiment with different numbers of components and identify the optimal number of components based on the BIC score. 5. **Visualization**: - Plot the data points colored by their assigned cluster labels. - Draw confidence ellipsoids for the Gaussian components (optional but recommended). **Input**: - `data.csv` containing numerical data for 2D points. - Specified number of components for GMM (e.g., `n_components=2`). **Output**: - DataFrame with an additional column for cluster labels. - BIC scores for different numbers of components. - Visualization of clustered data points with optional ellipsoids. **Constraints**: - Use `sklearn.mixture.GaussianMixture` for the GMM model. - Ensure to preprocess data appropriately if needed (e.g., handling missing values, standardizing features). - You are encouraged to explore the deduction process for the optimal number of components using BIC scores. ```python import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture # Step 1: Load the data data = pd.read_csv(\'data.csv\') # Step 2: Data Preprocessing # (Include any necessary preprocessing steps here) # Step 3: Gaussian Mixture Model Clustering def perform_gmm_clustering(data, n_components): # Create GMM instance gmm = GaussianMixture(n_components=n_components, random_state=42) # Fit the model to data gmm.fit(data) # Predict the cluster for each data point data[\'cluster\'] = gmm.predict(data) return data, gmm # Step 4: Model Evaluation def evaluate_gmm_bic(data, max_components=10): bics = [] for n in range(1, max_components + 1): gmm = GaussianMixture(n_components=n, random_state=42) gmm.fit(data) bics.append(gmm.bic(data)) return bics # Step 5: Visualization def plot_clusters(data, gmm): plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=data[\'cluster\'], cmap=\'viridis\') plt.title(\'GMM Clustering\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() # Example usage data, gmm = perform_gmm_clustering(data, n_components=2) print(data) bics = evaluate_gmm_bic(data) print(bics) plot_clusters(data, gmm) ``` Ensure you understand the functionality of the methods used and explore additional ways to improve the clustering performance or visualization.","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture from sklearn.preprocessing import StandardScaler # Step 1: Load the data def load_data(file_path): data = pd.read_csv(file_path) return data # Step 2: Data Preprocessing def preprocess_data(data): # Handle missing values if any data = data.dropna() # Standardize the features scaler = StandardScaler() scaled_data = scaler.fit_transform(data) return scaled_data # Step 3: Gaussian Mixture Model Clustering def perform_gmm_clustering(data, n_components): # Create GMM instance gmm = GaussianMixture(n_components=n_components, random_state=42) # Fit the model to data gmm.fit(data) # Predict the cluster for each data point labels = gmm.predict(data) data_with_labels = pd.DataFrame(data, columns=[\'Feature1\', \'Feature2\']) data_with_labels[\'cluster\'] = labels return data_with_labels, gmm # Step 4: Model Evaluation def evaluate_gmm_bic(data, max_components=10): bics = [] for n in range(1, max_components + 1): gmm = GaussianMixture(n_components=n, random_state=42) gmm.fit(data) bics.append(gmm.bic(data)) return bics # Step 5: Visualization def plot_clusters(data): plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=data[\'cluster\'], cmap=\'viridis\') plt.title(\'GMM Clustering\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() # Example usage if __name__ == \\"__main__\\": data = load_data(\'data.csv\') processed_data = preprocess_data(data) clustered_data, gmm = perform_gmm_clustering(processed_data, n_components=2) print(clustered_data) bics = evaluate_gmm_bic(processed_data) print(bics) plot_clusters(clustered_data)"},{"question":"You are given a Python script file, and your task is to analyze its imported modules using the `modulefinder` module. You must implement a function `analyze_script_imports(script_path: str) -> dict`, which takes the path to the Python script file as its argument and returns a dictionary containing: - `loaded_modules`: A list of the names of modules successfully imported by the script. - `failed_modules`: A list of the names of modules that the script attempted to import but failed. # Function Signature ```python def analyze_script_imports(script_path: str) -> dict: pass ``` # Input - `script_path`: A string representing the file path to the Python script to be analyzed. # Output - A dictionary with two keys: - `loaded_modules`: A list of strings, where each string is the name of a module successfully imported by the script. - `failed_modules`: A list of strings, where each string is the name of a module that the script attempted to import but failed. # Constraints - You may assume the script file at `script_path` exists and is readable. - The script file may import both standard library modules and custom modules. # Example Suppose the file `example.py` has the following contents: ```python import os import sys try: import non_existent_module except ImportError: pass ``` Calling `analyze_script_imports(\'example.py\')` should return: ```python { \'loaded_modules\': [\'os\', \'sys\'], \'failed_modules\': [\'non_existent_module\'] } ``` # Notes - Use the `ModuleFinder` class from the `modulefinder` module to perform the analysis. - You can use the `finder.modules` to get a dictionary of successfully imported modules and `finder.badmodules` for failed imports.","solution":"import modulefinder def analyze_script_imports(script_path: str) -> dict: Analyzes the imports in a given Python script and provides information about successfully loaded modules and modules that failed to load. Args: script_path (str): The path to the Python script file to be analyzed. Returns: dict: A dictionary with `loaded_modules` and `failed_modules`. finder = modulefinder.ModuleFinder() # Run the script through the module finder finder.run_script(script_path) # Successfully loaded modules loaded_modules = list(finder.modules.keys()) # Modules that failed to load failed_modules = list(finder.badmodules.keys()) return { \'loaded_modules\': loaded_modules, \'failed_modules\': failed_modules }"},{"question":"# Pandas Options Configuration Objective Create a Python script that demonstrates the ability to manipulate pandas options to achieve specific tasks. This will test your understanding of pandas\' configuration options and their practical application in data analysis. Task 1. **Describe a specific option**: Write a function `describe_option_example(option_name: str) -> str` that takes an option name as input and returns a string description of the option. 2. **Set and get an option**: Write a function `set_and_get_option(option_name: str, new_value: any) -> any` that sets a pandas option to a new value and then retrieves the current value of that option. 3. **Reset an option**: Write a function `reset_specific_option(option_name: str) -> None` that resets the given option to its default value. 4. **Context management for options**: Write a function `use_temp_option(option_name: str, temp_value: any) -> str` that temporarily sets an option within a context manager and returns a string indicating whether the temporary setting was successfully applied. 5. **Set numeric format for floating numbers**: Write a function `set_floating_format(format_string: str) -> str` that sets the floating-point format to the specified format string and returns a confirmation message. Constraints - Ensure that the function `describe_option_example` returns descriptive information regarding the option when possible. - The function `set_and_get_option` should handle various types of option values (e.g., boolean, integer, string). - Make sure `reset_specific_option` correctly resets the option without affecting other options. - `use_temp_option` should apply the temporary setting only within its context and revert to the previous setting afterward. - Validate the format string in `set_floating_format` to ensure it\'s a proper float format before applying. Function Signatures ```python def describe_option_example(option_name: str) -> str: pass def set_and_get_option(option_name: str, new_value: any) -> any: pass def reset_specific_option(option_name: str) -> None: pass def use_temp_option(option_name: str, temp_value: any) -> str: pass def set_floating_format(format_string: str) -> str: pass ``` Input and Output Formats 1. `describe_option_example` - Input: `option_name` (str) — The name of the pandas option. - Output: A string description of the option. 2. `set_and_get_option` - Input: `option_name` (str), `new_value` (any) — The name of the pandas option and the new value to set. - Output: The current value of the specified option (after setting it). 3. `reset_specific_option` - Input: `option_name` (str) — The name of the pandas option to reset. - Output: None. 4. `use_temp_option` - Input: `option_name` (str), `temp_value` (any) — The name of the pandas option and the temporary value to apply. - Output: A string indicating if the temporary setting was applied successfully. 5. `set_floating_format` - Input: `format_string` (str) — A string representing the floating-point number format. - Output: A confirmation message. You may refer to the pandas documentation to understand the specifics of each function and their usage.","solution":"import pandas as pd def describe_option_example(option_name: str) -> str: Returns a description of the specified pandas option. try: description = pd.get_option(option_name) return f\\"The current value of the option \'{option_name}\' is {description}.\\" except KeyError: return f\\"The option \'{option_name}\' does not exist in pandas options.\\" def set_and_get_option(option_name: str, new_value: any) -> any: Sets a pandas option to a new value and retrieves the current value of that option. pd.set_option(option_name, new_value) return pd.get_option(option_name) def reset_specific_option(option_name: str) -> None: Resets the specified pandas option to its default value. pd.reset_option(option_name) def use_temp_option(option_name: str, temp_value: any) -> str: Temporarily sets a pandas option within a context manager and returns a success message. prev_value = pd.get_option(option_name) with pd.option_context(option_name, temp_value): if pd.get_option(option_name) == temp_value: return f\\"The temporary option \'{option_name}\' was successfully set to {temp_value}.\\" return f\\"Failed to set the temporary option \'{option_name}\' to {temp_value}.\\" def set_floating_format(format_string: str) -> str: Sets the floating-point format to the specified format string and returns a confirmation message. try: pd.options.display.float_format = format_string return f\\"Floating-point format set to {format_string}.\\" except ValueError as e: return f\\"Failed to set floating-point format: {e}\\""},{"question":"**Objective**: Write a Python function that retrieves and prints specific site-package information, and demonstrates an understanding of the `site` module\'s functionalities as described in Python 3.10 documentation. **Task**: You are required to implement a function named `site_package_info` that does the following: 1. **Prints all the global site-packages directories.** 2. **Prints the path of the user-specific site-packages directory if the user site-packages are enabled. If disabled, print a relevant message stating it is disabled.** 3. **Adds a given directory to the system\'s site-packages directories and confirms the addition by printing the updated list of global site-packages.** **Function Signature**: ```python def site_package_info(new_directory: str) -> None: pass ``` **Input**: - `new_directory`: A string representing the new directory to be added to the site-packages directories. **Output**: - The function should print the following: 1. A list of all global site-packages directories. 2. The path of the user-specific site-packages directory or a relevant message if it is disabled. 3. The updated list of all global site-packages directories after adding the given new directory. **Constraints**: - Make sure to handle scenarios where user site-packages directories are disabled gracefully. - Ensure that the function performs efficiently without unnecessary recomputations. **Example**: ```python def site_package_info(new_directory: str) -> None: import site # Print all global site-packages directories global_sites = site.getsitepackages() print(\\"Global site-packages directories:\\", global_sites) # Check if user site-packages directory is enabled and print the path if site.ENABLE_USER_SITE: user_site = site.getusersitepackages() print(\\"User-specific site-packages directory:\\", user_site) else: print(\\"User-specific site-packages directory is disabled.\\") # Add new directory to site-packages and print updated list site.addsitedir(new_directory) updated_sites = site.getsitepackages() print(\\"Updated global site-packages directories:\\", updated_sites) # Example usage site_package_info(\\"/path/to/new/site-package\\") ``` *Note*: The example usage shows how the function should be implemented and what kind of outputs are expected. Make sure your function aligns with this illustration but its implementation should be tested with appropriate directories according to your system setup.","solution":"import site def site_package_info(new_directory: str) -> None: Prints global and user-specific site-packages directories information and adds a new directory to the system\'s site-packages directories. :param new_directory: A string representing the new directory to be added to the site-packages directories # Print all global site-packages directories global_sites = site.getsitepackages() print(\\"Global site-packages directories:\\", global_sites) # Check if user-specific site-packages directory is enabled and print the path if site.ENABLE_USER_SITE: user_site = site.getusersitepackages() print(\\"User-specific site-packages directory:\\", user_site) else: print(\\"User-specific site-packages directory is disabled.\\") # Add new directory to site-packages and print updated list site.addsitedir(new_directory) updated_global_sites = site.getsitepackages() print(\\"Updated global site-packages directories:\\", updated_global_sites)"},{"question":"<|Analysis Begin|> The given documentation is about `asyncio.queues` from the Python standard library. - **Classes Provided:** - `asyncio.Queue`: A first-in, first-out (FIFO) queue. - Methods: `empty()`, `full()`, `get()`, `get_nowait()`, `join()`, `put()`, `put_nowait()`, `qsize()`, `task_done()`. - `asyncio.PriorityQueue`: A priority queue that retrieves entries in priority order (lowest first). - `asyncio.LifoQueue`: A last-in, first-out (LIFO) queue. - **Exceptions:** - `asyncio.QueueEmpty`: Raised by `get_nowait()` when the queue is empty. - `asyncio.QueueFull`: Raised by `put_nowait()` when the queue is full. - **Key Features:** - The `Queue` class allows asynchronous operations to manage the addition and removal of items. - The `join()`, `task_done()`, and the pair `put()`/`put_nowait()` and `get()`/`get_nowait()` provide mechanisms for task coordination. - The functionalities can be utilized to distribute workload among multiple worker tasks. - **Usage Example:** - The provided example demonstrates how to create an `asyncio.Queue` and use it to manage asynchronous worker tasks that perform operations on items from the queue. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Implement an asynchronous system to simulate and manage a simplified ticket booking system for a movie theater using `asyncio.Queue`. Problem Statement A movie theater needs a system to manage ticket bookings asynchronously. The theater has the following requirements: 1. **Asynchronous Booking Queue:** - Use the `asyncio.Queue` class to manage ticket booking requests. - The booking queue should have a maximum size capacity limit (`maxsize`) of 50 requests. 2. **Handler Function:** - Implement an asynchronous function `handler(name: str, queue: asyncio.Queue)`: - This function should continuously process ticket booking requests from the queue. - Each request takes a random time between 0.1 and 0.5 seconds to process (use `asyncio.sleep`). - After processing the request, call `queue.task_done()`. - Print a message indicating which handler processed which request. 3. **Booking Simulator Function:** - Implement an asynchronous function `booking_simulator(total_tickets: int, number_of_handlers: int)`: - This function should create and manage the ticket booking system. - Create a `asyncio.Queue` instance with a `maxsize` of `50`. - Add `total_tickets` booking requests to the queue. - Create `number_of_handlers` asynchronous handlers to process the requests. - Ensure the system waits for all requests to be processed using `queue.join()`. - Cancel all handler tasks after all requests have been processed. Input - `total_tickets` (int): The total number of ticket booking requests to be handled. - `number_of_handlers` (int): The number of handler functions to process ticket bookings concurrently. Output - Print statements from handler functions indicating which handler processed which request and a summary message after all requests have been processed. Constraints - You can assume that `1 <= total_tickets <= 100`. - You can assume that `1 <= number_of_handlers <= 10`. Example Usage ```python import asyncio import random async def handler(name: str, queue: asyncio.Queue): while True: ticket_number = await queue.get() await asyncio.sleep(random.uniform(0.1, 0.5)) queue.task_done() print(f\'{name} processed ticket #{ticket_number}\') async def booking_simulator(total_tickets: int, number_of_handlers: int): queue = asyncio.Queue(maxsize=50) for ticket_no in range(1, total_tickets + 1): await queue.put(ticket_no) tasks = [] for i in range(number_of_handlers): task = asyncio.create_task(handler(f\'handler-{i+1}\', queue)) tasks.append(task) await queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) print(\'All booking requests have been processed.\') # To run the booking simulator: asyncio.run(booking_simulator(100, 5)) ``` Use the example provided above as a reference to test and implement your solution effectively.","solution":"import asyncio import random async def handler(name: str, queue: asyncio.Queue): while True: try: ticket_number = await queue.get() await asyncio.sleep(random.uniform(0.1, 0.5)) queue.task_done() print(f\'{name} processed ticket #{ticket_number}\') except asyncio.CancelledError: break async def booking_simulator(total_tickets: int, number_of_handlers: int): queue = asyncio.Queue(maxsize=50) for ticket_no in range(1, total_tickets + 1): await queue.put(ticket_no) tasks = [] for i in range(number_of_handlers): task = asyncio.create_task(handler(f\'handler-{i+1}\', queue)) tasks.append(task) await queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) print(\'All booking requests have been processed.\')"},{"question":"# Email Automation Task with `imaplib` In this task, you will create a Python script that connects to an IMAP4 email server, searches for unread emails from a specific sender, fetches their subject lines, marks them as read, and logs out gracefully. Requirements: 1. **Function**: Implement a function `fetch_unread_emails_from_sender` that performs the following tasks: - Connects to the given IMAP server using provided credentials. - Searches for unread emails from the specified sender. - Fetches the subject lines of these emails. - Marks these emails as read. - Disconnects from the server gracefully. Specifications: - **Input**: - `server` (str): The address of the IMAP server. - `username` (str): The username for logging into the email account. - `password` (str): The password for logging into the email account. - `sender_email` (str): The email address of the sender to filter unread emails. - **Output**: - Returns a list of subject lines of the unread emails from the specified sender. - **Error Handling**: - Handle exceptions properly, especially for connection errors, authentication failures, and fetching issues. - Ensure that the connection is closed properly even if an error occurs. Sample Function Signature: ```python from typing import List import imaplib def fetch_unread_emails_from_sender(server: str, username: str, password: str, sender_email: str) -> List[str]: pass ``` Example: ```python server = \\"imap.example.com\\" username = \\"example_user\\" password = \\"example_pass\\" sender_email = \\"specific_sender@example.com\\" subject_lines = fetch_unread_emails_from_sender(server, username, password, sender_email) print(subject_lines) ``` This example should connect to `imap.example.com` with given credentials, search for unread emails from `specific_sender@example.com`, fetch their subject lines, mark them as read, and print the subject lines. Constraints: - The IMAP server must support the IMAP4 or IMAP4rev1 protocol. - The script should handle the login and fetch operations securely. - Use proper IMAP commands for searching, fetching, and marking emails as read. Notes: - Refer to the `imaplib` documentation for method details and usages. - Ensure to run your script in a secure environment to protect credentials.","solution":"from typing import List import imaplib import email def fetch_unread_emails_from_sender(server: str, username: str, password: str, sender_email: str) -> List[str]: try: # Connect to the server mail = imaplib.IMAP4_SSL(server) # Login to the account mail.login(username, password) # Select the inbox mail.select(\'inbox\') # Search for unread emails from the specified sender status, response = mail.search(None, \'(UNSEEN FROM \\"{0}\\")\'.format(sender_email)) if status != \'OK\': raise Exception(\\"Error searching inbox\\") email_ids = response[0].split() subject_lines = [] for e_id in email_ids: status, msg_data = mail.fetch(e_id, \'(RFC822)\') if status != \'OK\': continue for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) subject_lines.append(msg[\'subject\']) # Mark email as read mail.store(e_id, \'+FLAGS\', \'Seen\') # Logout mail.logout() return subject_lines except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"**Objective**: Create visualizations using Seaborn\'s `Plot` and `Bar` functionalities to demonstrate understanding of defining and transforming variables in plots. Task: Given a dataset `tips` containing the following columns: - `total_bill`: Total bill amount for a meal. - `tip`: Tip amount given. - `sex`: Gender of the person paying the bill. - `smoker`: Whether the person is a smoker or not (`Yes`/`No`). - `day`: Day of the week (`Thur`, `Fri`, `Sat`, `Sun`). - `time`: Time of the meal (`Lunch`, `Dinner`). - `size`: Size of the party. You are required to: 1. Load the dataset using `seaborn.load_dataset(\\"tips\\")`. 2. Create and display a bar plot that shows counts of the number of tips received per day (`day`). 3. Modify the plot to add grouping based on the `sex` column and display them side-by-side using `Dodge`. 4. Create another plot to show the count of different party sizes (`size`) on the `y` axis. 5. Transform the `day` variable to show counts of total_bill being more than 20. Input: A predefined dataset named `tips` is provided through `seaborn.load_dataset(\\"tips\\")`. Output: Multiple bar plots with the specifications described in the task. Constraints: - Use Seaborn\'s objects interface (`so.Plot`, `so.Bar`, and other relevant methods). - Follow the order of operations specified in the task. Example: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # 1. Create and display a bar plot showing counts of the number of tips received per day. plot1 = so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()) plot1.show() # 2. Modify the plot to add grouping based on the `sex` column and display them side-by-side using `Dodge`. plot2 = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot2.show() # 3. Create another plot to show the count of different party sizes on the y-axis. plot3 = so.Plot(tips, y=\\"size\\").add(so.Bar(), so.Count()) plot3.show() # 4. Transform the `day` variable to show counts of total_bill being more than 20. tips[\'high_total_bill\'] = tips[\'total_bill\'] > 20 plot4 = so.Plot(tips, x=\\"day\\", color=\\"high_total_bill\\").add(so.Bar(), so.Count(), so.Dodge()) plot4.show() ``` Make sure to include all of your code and ensure it runs without errors. Verify that the plots display as expected, meeting the specified requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # 1. Create and display a bar plot showing counts of the number of tips received per day. plot1 = so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()) plot1.show() # 2. Modify the plot to add grouping based on the `sex` column and display them side-by-side using `Dodge`. plot2 = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot2.show() # 3. Create another plot to show the count of different party sizes on the y-axis. plot3 = so.Plot(tips, y=\\"size\\").add(so.Bar(), so.Count()) plot3.show() # 4. Transform the `day` variable to show counts of total_bill being more than 20. tips[\'high_total_bill\'] = tips[\'total_bill\'] > 20 plot4 = so.Plot(tips, x=\\"day\\", color=\\"high_total_bill\\").add(so.Bar(), so.Count(), so.Dodge()) plot4.show()"},{"question":"Objective This question aims to assess your understanding and ability to use the Seaborn `objects` interface for data visualization, focusing on creating complex, customized plots. Question You are provided with a dataset named `planets` that contains information about discovered planets. The dataset has the following columns: `method`, `number`, `orbital_period`, `mass`, `distance`, and `year`. Your task is to: 1. Load the `planets` dataset using Seaborn\'s `load_dataset` function. 2. Create a faceted scatter plot that shows the relationship between the `distance` and the `mass` of the planets for each `method` of discovery. 3. Customize the plot by applying the following styles: - Set the axes face color to white and the axes edge color to slategray. - Set the lines\' width in any line plot elements to 4. - Use the \\"ticks\\" style for the axes. 4. Update the default Seaborn plot configuration to use a white grid style and a \\"talk\\" context for rendering. # Expected Function Signature ```python def visualize_planets_data(): pass ``` # Implementation Requirements - Do not change the function signature. - Your code should be runnable, and it should produce the required plots when executed. - Make sure all customizations on the plots are applied as specified. - Your solution should be efficient in terms of both performance and readability. Your implementation of the `visualize_planets_data` function should include the following steps: 1. Load the dataset. 2. Initialize the plot using `so.Plot`. 3. Add faceting by the `method` discovery method, with a sensible wrapping to ensure clear visualization. 4. Include scatter dots representing each planet. 5. Apply and combine the required theme customizations. 6. Set the default theme for future plots. Input There is no input to the function. The dataset is loaded internally using Seaborn\'s `load_dataset(\'planets\')`. Output The function should display the customized faceted plot directly without returning any value. Example When the `visualize_planets_data` function is executed correctly, it should generate a faceted scatter plot visualizing the `distance` and `mass` of planets for each discovery `method`, styled according to the provided customization requirements. Note Please ensure that all customizations are applied correctly and that the plot is fully readable and well-structured.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_planets_data(): # Load the dataset planets = sns.load_dataset(\'planets\') # Update the default Seaborn plot configuration sns.set_theme(style=\\"whitegrid\\", context=\\"talk\\") # Create the plot and apply the required customizations g = sns.FacetGrid(planets, col=\\"method\\", col_wrap=4, height=4) g.map_dataframe(sns.scatterplot, x=\\"distance\\", y=\\"mass\\") # Set the styles for the axes and lines for ax in g.axes.flat: ax.set_facecolor(\'white\') ax.spines[\'bottom\'].set_color(\'slategray\') ax.spines[\'top\'].set_color(\'slategray\') ax.spines[\'left\'].set_color(\'slategray\') ax.spines[\'right\'].set_color(\'slategray\') for line in ax.lines: line.set_linewidth(4) g.set_titles(\\"{col_name}\\") g.fig.tight_layout(w_pad=1, h_pad=2) plt.show()"},{"question":"**Objective:** Demonstrate your understanding of the `pickletools` module. **Description:** You are given multiple pickle files containing serialized Python objects. Your task is to write a Python program that performs the following: 1. Reads and disassembles a given list of pickle files, annotating each line with a description of the opcode and indentation to represent nested structures. 2. Optimizes each of these pickle files by removing unused \\"PUT\\" opcodes to reduce size and improve efficiency. 3. Outputs both the disassembled and optimized versions of the pickles to specified output files. **Function Specification:** Implement the function `process_pickles`: ```python def process_pickles(pickle_files: List[str], dis_output_files: List[str], optimized_output_files: List[str], indentlevel: int = 4, annotate: int = 1) -> None: Process a list of pickle files. Args: - pickle_files (List[str]): List of paths to the pickle files to be processed. - dis_output_files (List[str]): List of paths to the output files for disassembly results. - optimized_output_files (List[str]): List of paths to the output files for optimized pickles. - indentlevel (int, optional): The number of spaces to indent at each MARK level in disassembly. Defaults to 4. - annotate (int, optional): Whether to annotate the disassembly with opcode descriptions. Defaults to 1. Returns: - None pass ``` # Input: - `pickle_files`: A list of strings where each string is the path to a pickle file. - `dis_output_files`: A list of strings where each string is the path to a file where the disassembly of the corresponding pickle file will be written. - `optimized_output_files`: A list of strings where each string is the path to a file where the optimized pickle file will be written. - `indentlevel`: An optional integer specifying the number of spaces to indent at each MARK level in the disassembly. Defaults to 4. - `annotate`: An optional integer (0 or 1) specifying whether to annotate disassembly with opcode descriptions. Defaults to 1. # Output: - No return value. The function should write to the specified output files. # Constraints: - `pickle_files`, `dis_output_files`, and `optimized_output_files` will have the same length. - Ensure that appropriate error handling is in place to manage file read/write errors. # Example Usage: ```python process_pickles( pickle_files=[\'pickles/file1.pkl\', \'pickles/file2.pkl\'], dis_output_files=[\'output/dis_file1.txt\', \'output/dis_file2.txt\'], optimized_output_files=[\'output/opt_file1.pkl\', \'output/opt_file2.pkl\'] ) # This should read \'pickles/file1.pkl\' and \'pickles/file2.pkl\', disassemble them with annotation and indentation, # and write the results to \'output/dis_file1.txt\' and \'output/dis_file2.txt\' respectively. # It should also optimize the pickle files and write the optimized versions to # \'output/opt_file1.pkl\' and \'output/opt_file2.pkl\'. ``` # Notes: - Use the `pickletools.dis` function to perform the disassembly. - Use the `pickletools.optimize` function to optimize the pickles. - Ensure that the output files contain meaningful and readable contents as specified. This coding challenge assesses the understanding of file handling, working with pickles, using the `pickletools` module for disassembly and optimization, and writing results to specified output files.","solution":"import pickle import pickletools from typing import List def process_pickles(pickle_files: List[str], dis_output_files: List[str], optimized_output_files: List[str], indentlevel: int = 4, annotate: int = 1) -> None: for pickle_file, dis_output_file, optimized_output_file in zip(pickle_files, dis_output_files, optimized_output_files): try: # Read the pickle file with open(pickle_file, \'rb\') as pf: data = pf.read() # Disassemble the pickle file with open(dis_output_file, \'w\') as dof: pickletools.dis(data, out=dof, annotate=bool(annotate), indentlevel=indentlevel) # Optimize the pickle file optimized_data = pickletools.optimize(data) # Write the optimized pickle to the output file with open(optimized_output_file, \'wb\') as oof: oof.write(optimized_data) except Exception as e: print(f\\"An error occurred while processing {pickle_file}: {e}\\")"},{"question":"**Title**: Create a Terminal-Based To-Do List Application **Objective**: Write a Python program utilizing the `curses` module to create a terminal-based To-Do list application. The application should support adding new items, marking items as completed, deleting items, and navigating through the list. **Requirements**: 1. **Initialize the Screen**: Set up the terminal screen using `curses.initscr()` and ensure proper cleanup using `curses.wrapper()`. 2. **Main Window**: Create a main window where the list items will be displayed. Allow users to navigate through the items using arrow keys. 3. **Add Items**: Support adding new items to the To-Do list. When \'a\' is pressed, prompt the user to input a new task. 4. **Complete Items**: Allow users to mark items as completed. Highlight the completed items with a different attribute (e.g., using `A_REVERSE`). 5. **Delete Items**: Allow users to delete items from the list using the \'d\' key. 6. **Quit the Application**: Provide a way to gracefully exit the application (e.g., pressing \'q\'). **Instructions**: - The program should handle window resizing and maintain proper display of the list. - Implement input validation to ensure robustness (e.g., handle empty input for new tasks). - Use the `Textbox` class from the `curses.textpad` module for input prompts. - Ensure the interface is intuitive and responsive. # Expected Input and Output: **Input**: - Arrow keys to navigate through list items. - \'a\' key to add a new item (prompts for input and adds to the list). - \'d\' key to delete the currently selected item. - \'Enter\' key to mark the currently selected item as completed. - \'q\' key to quit the application. **Output**: - The terminal should display the list items, with the currently selected item highlighted. - Completed items should be visually distinct from uncompleted items. - Prompts for adding new items should appear at the bottom or top of the terminal window. # Example Scenario: 1. Run the program: ``` python todo_list.py ``` 2. The terminal displays: ``` To-Do List: 1. [ ] Buy groceries 2. [ ] Finish homework 3. [ ] Call mom ``` 3. Navigate using arrow keys, press \'Enter\' on \\"Finish homework\\" to mark it as completed: ``` To-Do List: 1. [ ] Buy groceries 2. [X] Finish homework 3. [ ] Call mom ``` 4. Press \'a\', input \\"Pay bills\\", and press \'Enter\': ``` To-Do List: 1. [ ] Buy groceries 2. [X] Finish homework 3. [ ] Call mom 4. [ ] Pay bills ``` 5. Press \'d\' on \\"Call mom\\" to delete it, press \'q\' to quit. This question is designed to challenge your understanding of the `curses` module, particularly in managing windows, handling user input, and rendering dynamic content in a terminal environment.","solution":"import curses from curses import wrapper from curses.textpad import Textbox def main(stdscr): curses.curs_set(0) stdscr.keypad(True) curses.start_color() curses.init_pair(1, curses.COLOR_GREEN, curses.COLOR_BLACK) curses.init_pair(2, curses.COLOR_WHITE, curses.COLOR_BLACK) curses.init_pair(3, curses.COLOR_BLACK, curses.COLOR_WHITE) current_row = 0 todo_list = [] while True: stdscr.clear() h, w = stdscr.getmaxyx() for idx, task in enumerate(todo_list): x = w//2 - len(task) // 2 y = h//2 - len(todo_list)//2 + idx if idx == current_row: stdscr.attron(curses.color_pair(3)) stdscr.addstr(y, x, task) stdscr.attroff(curses.color_pair(3)) else: if task.startswith(\\"[X]\\"): stdscr.attron(curses.color_pair(2)) stdscr.addstr(y, x, task) if task.startswith(\\"[X]\\"): stdscr.attroff(curses.color_pair(2)) stdscr.refresh() key = stdscr.getch() if key == curses.KEY_UP and current_row > 0: current_row -= 1 elif key == curses.KEY_DOWN and current_row < len(todo_list) - 1: current_row += 1 elif key == ord(\'a\'): stdscr.addstr(h-1, 0, \\"Enter new task: \\") curses.echo() new_task = stdscr.getstr(h-1, len(\\"Enter new task: \\")).decode(\'utf-8\') curses.noecho() if new_task: todo_list.append(\\"[ ] \\" + new_task) elif key == ord(\'d\') and todo_list: del todo_list[current_row] if current_row > 0: current_row -= 1 elif key == ord(\'n\') and todo_list: if todo_list[current_row].startswith(\\"[ ]\\"): todo_list[current_row] = todo_list[current_row].replace(\\"[ ]\\", \\"[X]\\", 1) else: todo_list[current_row] = todo_list[current_row].replace(\\"[X]\\", \\"[ ]\\", 1) elif key == ord(\'q\'): break if __name__ == \'__main__\': wrapper(main)"},{"question":"# Complex Tensor Operations with PyTorch **Objective:** Create a function that performs a series of operations on a complex tensor using PyTorch. The function should demonstrate your understanding of basic and advanced complex tensor operations, including creation, manipulation, and computation of properties. **Function Signature:** ```python import torch def complex_tensor_operations(real_part: torch.Tensor, imag_part: torch.Tensor) -> dict: pass ``` **Parameters:** - `real_part`: A PyTorch tensor of shape `(n, m)` representing the real part of the complex numbers. - `imag_part`: A PyTorch tensor of shape `(n, m)` representing the imaginary part of the complex numbers. **Returns:** A dictionary containing the following keys and values: - `\'original_complex_tensor\'`: The complex tensor created from the provided real and imaginary parts. - `\'real_part\'`: The real part of the complex tensor. - `\'imag_part\'`: The imaginary part of the complex tensor. - `\'magnitude\'`: The magnitude (absolute value) of the complex tensor. - `\'phase\'`: The phase (angle) of the complex tensor. - `\'matmul_result\'`: The result of performing a matrix multiplication of the complex tensor with its conjugate transpose. **Instructions:** 1. **Create a Complex Tensor**: - Combine the `real_part` and `imag_part` tensors to create a complex tensor. 2. **Access Real and Imaginary Parts**: - Extract and store the real and imaginary parts of the complex tensor. 3. **Compute Magnitude and Phase**: - Compute the magnitude (absolute value) and phase (angle) of the complex tensor. 4. **Perform Matrix Multiplication**: - Compute the conjugate transpose of the complex tensor and perform matrix multiplication with the original complex tensor. 5. **Return the Results**: - Store the results in a dictionary as specified in the return format and return this dictionary. **Example:** ```python real_part = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) imag_part = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32) result = complex_tensor_operations(real_part, imag_part) # Example output: # { # \'original_complex_tensor\': tensor([[1.+5.j, 2.+6.j], [3.+7.j, 4.+8.j]]), # \'real_part\': tensor([[1., 2.], [3., 4.]]), # \'imag_part\': tensor([[5., 6.], [7., 8.]]), # \'magnitude\': tensor([[5.0990, 6.3246], [7.6158, 8.9443]]), # \'phase\': tensor([[1.3734, 1.2626], [1.1659, 1.1071]]), # \'matmul_result\': tensor([[85.0000+0.j, 100.0000+0.j], [100.0000+0.j, 117.0000+0.j]]) # } ``` Use appropriate PyTorch functions to implement the operations and ensure the function handles edge cases and constraints as required.","solution":"import torch def complex_tensor_operations(real_part: torch.Tensor, imag_part: torch.Tensor) -> dict: Perform a series of operations on a complex tensor using the given real and imaginary parts. Parameters: - real_part (torch.Tensor): Real part of shape (n, m). - imag_part (torch.Tensor): Imaginary part of shape (n, m). Returns: - dict: A dictionary containing information about the complex tensor operations. # Create a complex tensor complex_tensor = torch.complex(real_part, imag_part) # Extract real and imaginary parts real = torch.real(complex_tensor) imaginary = torch.imag(complex_tensor) # Compute magnitude and phase magnitude = torch.abs(complex_tensor) phase = torch.angle(complex_tensor) # Compute conjugate transpose complex_conjugate_transpose = torch.conj(complex_tensor).T # Perform matrix multiplication matmul_result = torch.matmul(complex_tensor, complex_conjugate_transpose) # Compile result dictionary result = { \'original_complex_tensor\': complex_tensor, \'real_part\': real, \'imag_part\': imaginary, \'magnitude\': magnitude, \'phase\': phase, \'matmul_result\': matmul_result } return result"},{"question":"Objective: Demonstrate your understanding of loading, preprocessing, and modeling with scikit-learn using real-world datasets. Problem Statement: You are provided with the `fetch_california_housing` dataset loader function from `sklearn.datasets`. This dataset contains information about housing in California from the 1990 census. The goal is to predict the median house value for California districts given various features such as population, median income, and housing attributes. Your task is to create a Python function `train_model` that performs the following steps: 1. Load the California Housing dataset using `fetch_california_housing`. 2. Preprocess the dataset by handling any missing values, if present (hint: the dataset may not have missing values, but write your function to handle them gracefully if it does). 3. Split the dataset into training and testing sets with 75% of the data for training and 25% for testing. 4. Train a Linear Regression model on the training set using `sklearn.linear_model.LinearRegression`. 5. Evaluate the model on the testing set by calculating the Mean Squared Error (MSE) using `sklearn.metrics.mean_squared_error`. Your function should return the trained model and the MSE score on the test set. Function Signature: ```python def train_model() -> Tuple[sklearn.linear_model._base.LinearRegression, float]: pass ``` Input: - No input parameters. Output: - The function should return a tuple containing: - The trained Linear Regression model. - The Mean Squared Error (MSE) score on the test data. Constraints: - Use the `fetch_california_housing` function to load the dataset. - Utilize `train_test_split` from `sklearn.model_selection` to split the data. - Use `LinearRegression` from `sklearn.linear_model` for training the model. - Use `mean_squared_error` from `sklearn.metrics` for evaluating the model performance. Here is an example of how your function\'s return value will be validated: ```python model, mse = train_model() print(f\\"Trained Model: {model}\\") print(f\\"Mean Squared Error: {mse}\\") ``` Performance Requirements: - The split, train, and evaluation processes should be optimized to handle the size of the dataset efficiently. - Ensure your function runs without error and within a reasonable time frame on typical datasets of this type. Good luck!","solution":"from typing import Tuple from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error import numpy as np def train_model() -> Tuple[LinearRegression, float]: # Load the dataset data = fetch_california_housing() X, y = data.data, data.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) # Train the Linear Regression model model = LinearRegression() model.fit(X_train, y_train) # Make predictions on the testing set y_pred = model.predict(X_test) # Calculate the Mean Squared Error mse = mean_squared_error(y_test, y_pred) return model, mse"},{"question":"Objective Demonstrate your understanding of PyTorch\'s utility functions for evaluating tensor similarities and errors. You will compare the performance of two given models using these utilities. Problem Statement You are provided with two pre-trained PyTorch models and a dataset. Your task is to evaluate and compare the performance of these models using the following metrics: - Signal-to-Quantization-Noise Ratio (SQNR) - Normalized L2 Error - Cosine Similarity. First, you will need to compute the output tensors of both models for a given dataset. Then, you will use the utility functions from `torch.ao.ns.fx.utils` to compute the metrics. Function Signature ```python import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def evaluate_models(model1: torch.nn.Module, model2: torch.nn.Module, dataloader: torch.utils.data.DataLoader) -> dict: Evaluates the performance of two models using SQNR, normalized L2 error, and cosine similarity. Args: - model1 (torch.nn.Module): The first pre-trained PyTorch model. - model2 (torch.nn.Module): The second pre-trained PyTorch model. - dataloader (torch.utils.data.DataLoader): A PyTorch dataloader containing the dataset. Returns: - dict: A dictionary with keys \'SQNR\', \'Normalized_L2_Error\', and \'Cosine_Similarity\' each containing a tuple with two values (metric_model1, metric_model2). Example: { \'SQNR\': (sqnr_model1, sqnr_model2), \'Normalized_L2_Error\': (l2_error_model1, l2_error_model2), \'Cosine_Similarity\': (cosine_similarity_model1, cosine_similarity_model2) } pass ``` Inputs - `model1`: A PyTorch `nn.Module` object representing the first pre-trained model. - `model2`: A PyTorch `nn.Module` object representing the second pre-trained model. - `dataloader`: A PyTorch `DataLoader` object containing the dataset for evaluation. Outputs - A dictionary with the following keys and values: - `\'SQNR\'`: A tuple of two floats representing the SQNR of `model1` and `model2`. - `\'Normalized_L2_Error\'`: A tuple of two floats representing the normalized L2 error of `model1` and `model2`. - `\'Cosine_Similarity\'`: A tuple of two floats representing the cosine similarity of `model1` and `model2`. Implementation Notes 1. Iterate over the `dataloader` to get the input batches. 2. Make sure to run the models in evaluation mode. 3. Store the outputs (tensors) for both models. 4. Compute the metrics using the utility functions (`compute_sqnr`, `compute_normalized_l2_error`, and `compute_cosine_similarity`) for both models. 5. Return the results in the specified format. Example Usage ```python # Assuming model1 and model2 are pre-trained models and dataloader is already defined results = evaluate_models(model1, model2, dataloader) print(results) ``` Note: The `evaluate_models` function should handle the computation and aggregation of the metrics across multiple batches if the dataset is large.","solution":"import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def evaluate_models(model1: torch.nn.Module, model2: torch.nn.Module, dataloader: torch.utils.data.DataLoader) -> dict: Evaluates the performance of two models using SQNR, normalized L2 error, and cosine similarity. Args: - model1 (torch.nn.Module): The first pre-trained PyTorch model. - model2 (torch.nn.Module): The second pre-trained PyTorch model. - dataloader (torch.utils.data.DataLoader): A PyTorch dataloader containing the dataset. Returns: - dict: A dictionary with keys \'SQNR\', \'Normalized_L2_Error\', and \'Cosine_Similarity\' each containing a tuple with two values (metric_model1, metric_model2). Example: { \'SQNR\': (sqnr_model1, sqnr_model2), \'Normalized_L2_Error\': (l2_error_model1, l2_error_model2), \'Cosine_Similarity\': (cosine_similarity_model1, cosine_similarity_model2) } model1.eval() model2.eval() all_outputs_model1 = [] all_outputs_model2 = [] with torch.no_grad(): for inputs, _ in dataloader: outputs_model1 = model1(inputs) outputs_model2 = model2(inputs) all_outputs_model1.append(outputs_model1) all_outputs_model2.append(outputs_model2) outputs_model1 = torch.cat(all_outputs_model1, dim=0) outputs_model2 = torch.cat(all_outputs_model2, dim=0) sqnr_model1 = compute_sqnr(outputs_model1, outputs_model1) sqnr_model2 = compute_sqnr(outputs_model2, outputs_model2) l2_error_model1 = compute_normalized_l2_error(outputs_model1, outputs_model1) l2_error_model2 = compute_normalized_l2_error(outputs_model2, outputs_model2) cosine_similarity_model1 = compute_cosine_similarity(outputs_model1, outputs_model1) cosine_similarity_model2 = compute_cosine_similarity(outputs_model2, outputs_model2) results = { \'SQNR\': (sqnr_model1, sqnr_model2), \'Normalized_L2_Error\': (l2_error_model1, l2_error_model2), \'Cosine_Similarity\': (cosine_similarity_model1, cosine_similarity_model2), } return results"},{"question":"# Conditional Model Implementation with PyTorch `torch.cond` Objective: Create a PyTorch model that uses the `torch.cond` operator to change its behavior based on the sum of the input tensor’s elements dynamically. This assesses your understanding of PyTorch’s conditional operations and tensor manipulation. Problem Description: You need to implement a class `DynamicSumCondModel` derived from `torch.nn.Module`. The model will compute the output tensor based on the sum of elements in the input tensor. Specifically: - If the sum of the input tensor’s elements is greater than 10, apply a function that computes the element-wise cosine of the input tensor and then multiplies the result by 2. - If the sum of the input tensor’s elements is less than or equal to 10, apply a function that computes the element-wise sine of the input tensor and then adds 1. Your implementation should also include: - The `forward` method for the custom model implementing the conditional logic. - A script to create an instance of your model and demonstrate its behavior using sample inputs. - Exporting the model to illustrate deployment using `torch.export`. Requirements and Constraints: - Input: A PyTorch tensor of arbitrary shape. - Output: A transformed tensor based on the defined conditional operations. - You must use `torch.cond` for the implementation of control flow within the model. - Ensure that the resulting model is exportable using `torch.export`. Example Usage: ```python import torch import torch.nn as nn class DynamicSumCondModel(nn.Module): def __init__(self): super(DynamicSumCondModel, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor): return 2 * torch.cos(x) def false_fn(x: torch.Tensor): return torch.sin(x) + 1 return torch.cond(x.sum() > 10, true_fn, false_fn, (x,)) # Demonstration model = DynamicSumCondModel() # Test with an input tensor tensor1 = torch.randn(5, 5) # random tensor tensor2 = torch.randn(10, 10) # another random tensor output1 = model(tensor1) output2 = model(tensor2) print(\\"Output1:\\", output1) print(\\"Output2:\\", output2) # Export the model dummy_input = torch.randn(5, 5) dynamic_shapes = {\\"x\\": {0: torch.export.Dim(\\"dim\\", min=2)}} exported_model = torch.export.export(model, (dummy_input,), {}, dynamic_shapes=dynamic_shapes) print(exported_model) ``` Use the model instance with various sample inputs to demonstrate its behavior dynamically. Implement your own `DynamicSumCondModel` in the script and provide sample inputs to validate the functionality. Export the model and highlight its structure based on the given constraints.","solution":"import torch import torch.nn as nn class DynamicSumCondModel(nn.Module): def __init__(self): super(DynamicSumCondModel, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor): return 2 * torch.cos(x) def false_fn(x: torch.Tensor): return torch.sin(x) + 1 sum_x = x.sum() if sum_x > 10: return true_fn(x) else: return false_fn(x)"},{"question":"# Advanced Plotting with Seaborn **Objective:** Demonstrate your understanding of `seaborn.objects` by creating a visual analysis of the `penguins` dataset. **Context:** The `penguins` dataset is already loaded and includes the following columns: - `species`: Species of the penguin (Adelie, Chinstrap, Gentoo) - `island`: Island on which the penguin was observed - `bill_length_mm`: Length of the bill in mm - `bill_depth_mm`: Depth of the bill in mm - `flipper_length_mm`: Length of the flipper in mm - `body_mass_g`: Body mass in grams - `sex`: Sex of the penguin (Male, Female) Your task is to create a composite plot to visualize the relationship between the flipper length and body mass, segmented by species and sex. # Requirements: 1. **Scatter Plot**: Plot a scatter plot of `flipper_length_mm` vs. `body_mass_g` using different colors for each species. 2. **Jittered Overlay**: Add jitter to the scatter plot points to avoid overlap. 3. **Encapsulate Distribution**: Overlay a range that encapsulates the interquartile range (IQR) for each species. 4. **Performance**: Ensure your code is efficient and executes within a reasonable time for plotting. **Expected Input:** None. The dataset `penguins` is pre-loaded and can be directly used in your analysis. Assume the necessary seaborn imports are already done. **Expected Output:** A single composite plot meeting the above specifications. **Constraints:** - Use only seaborn\'s `objects` interface (i.e., `seaborn.objects.Plot` and associated methods). - Your code should be clear and well-commented to illustrate your thought process. ```python # Example structure import seaborn.objects as so # Generate the composite plot ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"species\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75])) # Add any other required customization here ).show() ``` Focus on ensuring that your visualizations maintain clarity and effectively communicate the underlying trends within the dataset.","solution":"import seaborn.objects as so import seaborn as sns # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") def create_composite_plot(): Creates a composite plot visualizing the relationship between flipper length and body mass, segmented by species and sex in the penguins dataset. # Generate the composite plot ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"species\\") .facet(col=\\"sex\\") .add(so.Dots(), so.Jitter(0.05)) .add(so.Range(), so.Perc([25, 75])) ).show()"},{"question":"# Tuning the Decision Threshold for a Classifier **Objective**: Implement a function to tune the decision threshold of a given binary classifier to maximize recall while keeping precision above a certain level. **Function Signature**: ```python def tune_decision_threshold(classifier, X_train, y_train, X_test, y_test, min_precision): Tunes the decision threshold of the given classifier to maximize recall while ensuring precision is above the specified level. Parameters: - classifier (sklearn.base.BaseEstimator): An instance of a scikit-learn binary classifier. - X_train (numpy.ndarray): Training feature matrix. - y_train (numpy.ndarray): Training labels. - X_test (numpy.ndarray): Testing feature matrix. - y_test (numpy.ndarray): Testing labels. - min_precision (float): Minimum required precision level. Returns: - best_threshold (float): The decision threshold that maximizes recall while maintaining at least the specified precision. - best_recall (float): The recall score corresponding to the best threshold. - precision_at_best_recall (float): The precision score corresponding to the best threshold. - f1_at_best_recall (float): The F1 score corresponding to the best threshold. pass ``` # Instructions 1. Train the given classifier on the training data. 2. Use the `predict_proba` method to obtain the probability estimates for the positive class on the testing data. 3. Calculate precision and recall for thresholds ranging from 0 to 1 with a step size of 0.01. 4. Identify the threshold that provides the highest recall while maintaining precision above or equal to `min_precision`. 5. Return the threshold, the corresponding recall, precision, and F1 score. # Example ```python from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split # Generate synthetic data X, y = make_classification(n_samples=1000, n_classes=2, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and train classifier classifier = LogisticRegression() classifier.fit(X_train, y_train) # Minimum precision requirement min_precision = 0.8 # Tune the threshold and evaluate best_threshold, best_recall, precision_at_best_recall, f1_at_best_recall = tune_decision_threshold(classifier, X_train, y_train, X_test, y_test, min_precision) print(\\"Best Threshold:\\", best_threshold) print(\\"Best Recall:\\", best_recall) print(\\"Precision at Best Recall:\\", precision_at_best_recall) print(\\"F1 Score at Best Recall:\\", f1_at_best_recall) ``` **Constraints**: - The classifier should be a binary classifier supporting the `predict_proba` method. - The function should handle potential edge cases such as precision or recall being impossible to satisfy the given constraints. **Performance Requirements**: - The function should efficiently calculate precision, recall, and F1 score for each possible threshold. - Consider using vectorized operations and minimizing the number of loops to optimize performance. # Hint You can use sklearn\'s `precision_recall_curve` from `sklearn.metrics` to help calculate precision and recall for different thresholds.","solution":"import numpy as np from sklearn.metrics import precision_recall_curve, f1_score def tune_decision_threshold(classifier, X_train, y_train, X_test, y_test, min_precision): Tunes the decision threshold of the given classifier to maximize recall while ensuring precision is above the specified level. Parameters: - classifier (sklearn.base.BaseEstimator): An instance of a scikit-learn binary classifier. - X_train (numpy.ndarray): Training feature matrix. - y_train (numpy.ndarray): Training labels. - X_test (numpy.ndarray): Testing feature matrix. - y_test (numpy.ndarray): Testing labels. - min_precision (float): Minimum required precision level. Returns: - best_threshold (float): The decision threshold that maximizes recall while maintaining at least the specified precision. - best_recall (float): The recall score corresponding to the best threshold. - precision_at_best_recall (float): The precision score corresponding to the best threshold. - f1_at_best_recall (float): The F1 score corresponding to the best threshold. # Train the classifier on the training data classifier.fit(X_train, y_train) # Get the probability estimates for the positive class on the testing data y_proba = classifier.predict_proba(X_test)[:, 1] # Calculate precision and recall for thresholds ranging from 0 to 1 precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba) best_threshold = 0 best_recall = 0 precision_at_best_recall = 0 f1_at_best_recall = 0 for precision, recall, threshold in zip(precisions, recalls, thresholds): if precision >= min_precision: f1 = f1_score(y_test, y_proba >= threshold) if recall > best_recall: best_recall = recall precision_at_best_recall = precision best_threshold = threshold f1_at_best_recall = f1 return best_threshold, best_recall, precision_at_best_recall, f1_at_best_recall"},{"question":"# PyTorch MPS Device Utilization Objective Demonstrate your understanding of PyTorch\'s MPS backend for high-performance training on GPUs in MacOS devices by implementing a function that: 1. Checks for the availability of the `mps` backend. 2. Initializes specified Tensors on the `mps` device. 3. A model is defined and moved to the `mps` device. 4. Performs a specified operation on the Tensors and runs them through the model. Function Signature ```python import torch import torch.nn as nn def train_on_mps(): pass ``` Requirements - Check if the `mps` backend is available. - If `mps` is not available, print an appropriate message and return from the function. - If `mps` is available: - Create a Tensor `x` with shape `(10, 10)` filled with ones on the `mps` device. - Define a simple neural network model with one fully connected layer. - Move the model to the `mps` device. - Perform a matrix multiplication of `x` with itself element-wise. - Pass the result through the model. - Print the model\'s predictions. Example Output If `mps` is available: ``` tensor([[...]], device=\'mps:0\') ``` If `mps` is not available: ``` MPS not available because the current PyTorch install was not built with MPS enabled. ``` or ``` MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine. ``` Constraints - You should handle the absence of the `mps` device with appropriate error messages. - Maintain the same behavior and functionality if running in an environment without any GPU support. This question tests students\' understanding of the `mps` backend and their competence in GPU computing using PyTorch on supported MacOS devices.","solution":"import torch import torch.nn as nn def train_on_mps(): # Check if MPS backend is available if not torch.backends.mps.is_available(): if torch.backends.mps.is_built(): print(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") else: print(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") return # Create a Tensor on the MPS device device = torch.device(\\"mps\\") x = torch.ones((10, 10), device=device) # Define a simple neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 10) def forward(self, x): return self.fc1(x) model = SimpleModel().to(device) # Perform element-wise multiplication of x with itself y = x * x # Pass the result through the model output = model(y) # Print the model\'s predictions print(output) # For testing purposes, ensure the function doesn\'t immediately run when imported if __name__ == \\"__main__\\": train_on_mps()"},{"question":"# Pandas Coding Assessment Objective Your task is to implement a function that processes a large collection of Parquet files containing time series data. The goal is to calculate and return the value counts of a specified column across all files, managing memory efficiently. Problem Statement Given a directory containing multiple Parquet files, each representing time series data, implement a function `calculate_value_counts(directory_path, column_name)` that calculates the value counts of the specified column across all files without loading all the data into memory at once. Requirements 1. **Input:** - `directory_path` (str): The path to the directory containing Parquet files. - `column_name` (str): The name of the column for which to calculate value counts. 2. **Output:** - A `pandas.Series` object containing the value counts of the specified column across all files. The series should have values as indices and their counts as the values. 3. **Constraints:** - Each Parquet file fits into memory individually but the entire dataset does not. - The function should not load all files into memory simultaneously. 4. **Performance Requirements:** - The function should be optimized for memory efficiency using chunking techniques. - It should handle a large number of files efficiently. Example Usage Suppose the directory `data/timeseries/` contains the following Parquet files: ``` data/timeseries/ ├── ts-00.parquet ├── ts-01.parquet ... ``` Each file contains a DataFrame with at least the following columns: `timestamp`, `name`, `id`, `x`, `y`. ```python import pandas as pd import pathlib def calculate_value_counts(directory_path, column_name): counts = pd.Series(dtype=int) files = pathlib.Path(directory_path).glob(\\"*.parquet\\") for path in files: df = pd.read_parquet(path) counts = counts.add(df[column_name].value_counts(), fill_value=0) return counts.astype(int) # Example call: # result = calculate_value_counts(\\"data/timeseries/\\", \\"name\\") # print(result) ``` **Note**: Ensure that your implementation handles reading files and aggregating results efficiently by utilizing the provided documentation tips. **Hint**: You can use functions like `pd.read_parquet` to read individual Parquet files and `Series.value_counts` to calculate value counts for a column.","solution":"import pandas as pd import pathlib def calculate_value_counts(directory_path, column_name): Calculate value counts for a specified column across multiple Parquet files. Parameters: directory_path (str): The path to the directory containing Parquet files. column_name (str): The name of the column for which to calculate value counts. Returns: pandas.Series: A Series object containing the value counts of the specified column across all files. counts = pd.Series(dtype=int) files = pathlib.Path(directory_path).glob(\\"*.parquet\\") for path in files: df = pd.read_parquet(path) value_counts = df[column_name].value_counts() counts = counts.add(value_counts, fill_value=0) return counts.astype(int)"},{"question":"# Coding Assessment: Advanced Exception Handling in Python Objective: Design a function that performs division of two numbers with robust exception handling and signal processing using Python\'s C API functionalities. Problem Statement: You are required to implement a Python C extension module that provides a function `safe_divide` which performs division of two floating-point numbers while handling various types of exceptions and signals effectively. Function Signature: ```python def safe_divide(a: float, b: float) -> float ``` Requirements: 1. Perform the division of two numbers `a` and `b`. 2. Handle the following exceptions: - `ZeroDivisionError`: Raised when `b` is `0`. - `OverflowError`: Raised when the division result causes overflow. - `TypeError`: Raised when `a` or `b` is not a float. 3. Issue a custom warning if `a` or `b` are unusually large or small (define your thresholds appropriately). 4. Check and handle any interrupts or signals during the execution. 5. Ensure that any error is properly cleared or printed before the function exits. Constraints: - The function should be implemented as a Python C extension and should leverage the functionalities described in the provided documentation. - Use appropriate error and signal handling mechanisms to ensure that the function behaves correctly under various error conditions and interruptions. Input: - `a` (float): The numerator - `b` (float): The denominator Output: - Return the result of `a / b` as a float if no exceptions occur. Example: ```python >>> safe_divide(25.0, 5.0) 5.0 >>> safe_divide(25.0, 0) ZeroDivisionError: Division by zero is not allowed. >>> safe_divide(\'25\', 5.0) TypeError: Both arguments must be floats. ``` Notes: - Ensure that your implementation adheres to best practices for memory management and reference counting as per the Python C API guidelines. - Provide appropriate docstrings and comments in your C code for clarity.","solution":"import warnings def safe_divide(a: float, b: float) -> float: Function to perform division of two floats a and b with robust exception handling. Parameters: a : float : Numerator b : float : Denominator Returns: float : Result of the division a / b Raises: ZeroDivisionError : If b is 0. OverflowError : If the result of the division causes overflow. TypeError : If a or b are not floats. try: # Verify that input arguments are of float type if not isinstance(a, float) or not isinstance(b, float): raise TypeError(\\"Both arguments must be floats.\\") # Issue a custom warning if a or b are unusually large or small if abs(a) > 1e12 or abs(b) > 1e12: warnings.warn(\\"One or both of the input values are unusually large.\\") if abs(a) < 1e-12 and a != 0 or abs(b) < 1e-12 and b != 0: warnings.warn(\\"One or both of the input values are unusually small.\\") # Perform the division result = a / b # Check for overflow if abs(result) > 1e308: raise OverflowError(\\"The result of the division is too large.\\") return result except ZeroDivisionError: print(\\"ZeroDivisionError: Division by zero is not allowed.\\") raise except TypeError as e: print(f\\"TypeError: {e}\\") raise except OverflowError as e: print(f\\"OverflowError: {e}\\") raise except Exception as e: print(f\\"An unexpected error occurred: {e}\\") raise"},{"question":"You are provided with the `pwd` module, which allows access to the Unix user account and password database. Your task is to implement a function to collect and process user information based on certain criteria. Task Implement a function `get_users_with_shell(shell: str) -> list` that fetches and returns a list of dictionaries containing user information whose login shells match the specified **shell**. Each dictionary should include the following keys: - `name`: user\'s login name - `uid`: user\'s numerical user ID - `home_dir`: user\'s home directory Function Signature ```python def get_users_with_shell(shell: str) -> list: ``` Input - `shell` (str): The name of the shell (e.g., \'/bin/bash\', \'/bin/sh\'). Output - list: A list of dictionaries, where each dictionary contains the: - `name` (str): the login name. - `uid` (int): the numerical user ID. - `home_dir` (str): the home directory. Example ```python users = get_users_with_shell(\'/bin/bash\') ``` If there are users `john` and `doe` with `/bin/bash` as their shell, and their corresponding user IDs are `1001` and `1002`, and their home directories are `/home/john` and `/home/doe`, respectively, the function should return: ```python [ {\'name\': \'john\', \'uid\': 1001, \'home_dir\': \'/home/john\'}, {\'name\': \'doe\', \'uid\': 1002, \'home_dir\': \'/home/doe\'} ] ``` Constraints - Assume the `shell` input is always a valid string. - If no users have the specified shell, return an empty list. Notes - You must use the `pwd` module to fetch the user information. - Handle any potential exceptions that may arise due to invalid user IDs or names. # Solution Template Below is a template to help you start: ```python import pwd def get_users_with_shell(shell: str) -> list: # Your implementation here pass ``` Good luck, and write clean and efficient code!","solution":"import pwd def get_users_with_shell(shell: str) -> list: Returns a list of dictionaries containing user information whose login shells match the specified shell. Args: shell (str): The name of the shell (e.g., \'/bin/bash\', \'/bin/sh\'). Returns: list: A list of dictionaries with user information. users_with_shell = [] for user in pwd.getpwall(): if user.pw_shell == shell: user_info = { \'name\': user.pw_name, \'uid\': user.pw_uid, \'home_dir\': user.pw_dir } users_with_shell.append(user_info) return users_with_shell"},{"question":"# Advanced Python ByteArray Manipulation You are required to implement a function that works with the `bytearray` objects as specified in the documentation. The goal is to create a new bytearray from given inputs, manipulate it by concatenating additional data, and then resize the result based on certain criteria. Function Signature ```python def manage_bytearrays(obj1, obj2, resize_length): This function performs the following operations: 1. Creates a bytearray from `obj1`. 2. Creates a bytearray from `obj2`. 3. Concatenates these two bytearrays. 4. Resizes the resulting bytearray to `resize_length`. Parameters: obj1: Any object that implements the buffer protocol obj2: Any object that implements the buffer protocol resize_length: The new length to resize the resultant bytearray to. Returns: A new bytearray object that is the result of the above operations. Constraints: - `obj1` and `obj2` should implement the buffer protocol. - `resize_length` should be a non-negative integer. Raises: - ValueError if the resizing cannot be performed. # Your code here ``` Input Format: - `obj1`: Any object that implements the buffer protocol (like `bytes`, `bytearray`, etc.) - `obj2`: Any object that implements the buffer protocol (like `bytes`, `bytearray`, etc.) - `resize_length`: an integer denoting the new size for the resultant bytearray. Output Format: - A new `bytearray` object that has been created from `obj1`, concatenated with `obj2`, and resized to `resize_length`. Constraints: - Both `obj1` and `obj2` must support the buffer protocol. - `resize_length` should be a non-negative integer. Example: ```python #Example 1 obj1 = bytearray(b\'hello\') obj2 = bytearray(b\'world\') resize_length = 15 print(manage_bytearrays(obj1, obj2, resize_length)) # Expected output: bytearray(b\'helloworldx00x00x00x00x00\') #Example 2 obj1 = b\'foo\' obj2 = b\'bar\' resize_length = 5 print(manage_bytearrays(obj1, obj2, resize_length)) # Expected output: bytearray(b\'fooba\') ``` **Note:** - The function should handle creating bytearrays, concatenation, and resizing properly. - The function should ensure correct handling of memory and buffer-related operations as implied by the constraints.","solution":"def manage_bytearrays(obj1, obj2, resize_length): This function performs the following operations: 1. Creates a bytearray from `obj1`. 2. Creates a bytearray from `obj2`. 3. Concatenates these two bytearrays. 4. Resizes the resulting bytearray to `resize_length`. Parameters: obj1: Any object that implements the buffer protocol obj2: Any object that implements the buffer protocol resize_length: The new length to resize the resultant bytearray to. Returns: A new bytearray object that is the result of the above operations. Constraints: - `obj1` and `obj2` should implement the buffer protocol. - `resize_length` should be a non-negative integer. Raises: - ValueError if the resizing cannot be performed. # Create bytearrays from obj1 and obj2 ba1 = bytearray(obj1) ba2 = bytearray(obj2) # Concatenate the two bytearrays combined = ba1 + ba2 # Resize the bytearray combined = combined[:resize_length] + b\'x00\' * (resize_length - len(combined)) return combined"},{"question":"Objective: Demonstrate understanding of PyTorch\'s random number generation and neural network weight initialization. Problem Statement: You are required to implement a function that initializes the weights of a given neural network using a specific random seed. The function should ensure reproducibility by setting the random seed before initializing weights. You will use the `torch.nn` module to define a simple neural network consisting of two linear layers. Function Signature: ```python def initialize_weights_with_seed(seed: int) -> torch.nn.Module: Initializes the weights of a two-layer neural network using the specified random seed. Args: - seed (int): The random seed for reproducibility. Returns: - model (torch.nn.Module): The initialized neural network model. ``` Requirements: 1. Use the provided `seed` to set the random seed for PyTorch\'s random number generator. 2. Define a simple neural network model with the following architecture: - First linear layer: input size 10, output size 5. - Second linear layer: input size 5, output size 2. 3. Initialize the weights of both layers using a normal distribution with mean `0` and standard deviation `0.01`. 4. Ensure the bias terms of both layers are initialized to zeros. 5. The function should return the initialized model. Example Usage: ```python model = initialize_weights_with_seed(42) ``` Constraints: - You are only allowed to use PyTorch functionalities to achieve the task. Additional Information: - The `torch.manual_seed` function can be used to set the random seed. - Weight initialization can be done by directly accessing the `weight` and `bias` attributes of `torch.nn.Linear` layers. Notes: - Ensure that your solution ensures reproducibility by correctly setting the random seed and initializing weights as specified.","solution":"import torch import torch.nn as nn def initialize_weights_with_seed(seed: int) -> torch.nn.Module: Initializes the weights of a two-layer neural network using the specified random seed. Args: - seed (int): The random seed for reproducibility. Returns: - model (torch.nn.Module): The initialized neural network model. # Set the random seed for reproducibility torch.manual_seed(seed) # Define a simple two-layer neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(10, 5) self.layer2 = nn.Linear(5, 2) self._initialize_weights() def _initialize_weights(self): # Initialize layer1 weights with normal distribution (mean=0, std=0.01) and bias to zeros nn.init.normal_(self.layer1.weight, mean=0, std=0.01) nn.init.constant_(self.layer1.bias, 0) # Initialize layer2 weights with normal distribution (mean=0, std=0.01) and bias to zeros nn.init.normal_(self.layer2.weight, mean=0, std=0.01) nn.init.constant_(self.layer2.bias, 0) def forward(self, x): x = self.layer1(x) x = self.layer2(x) return x # Instantiate and return the model model = SimpleNN() return model"},{"question":"**Coding Assessment Question** You are given a dataset containing information about penguins from three different islands. Your task is to create a series of visualizations to explore the dataset and provide meaningful insights. Specifically, you must demonstrate your ability to create complex plots and customize the legend placements. # Input Format You do not need to take any input from the user. Instead, use the seaborn library to load the `penguins` dataset directly. # Constraints - Make sure to handle any missing values appropriately. - Use the seaborn library for all visualizations and matplotlib where necessary for additional customizations. # Tasks 1. **Data Loading and Cleaning** - Load the `penguins` dataset using seaborn. - Handle any missing values by removing rows with missing values. 2. **Univariate Analysis** - Create a histogram of the `bill_length_mm` attribute, colored by `species`. 3. **Bivariate Analysis** - Create a scatter plot showing `bill_length_mm` vs `flipper_length_mm`, colored by `species`. 4. **Faceted Plot** - Create faceted histograms of the `bill_length_mm` attribute for each `island`. Ensure the legends are positioned in the \'upper left\' and inside the plot. 5. **Legend Customization** - For the histogram created in step 2, move the legend to the \'center right\' of the plot. - For the scatter plot created in step 3, move the legend to the upper left outside the plot with precise placement using `bbox_to_anchor`. # Output Format Your code should produce and display the plots with the specified customizations. Ensure each plot has appropriate titles, labels, and legends. # Example There is no concrete input or output to display since this is a visualization task. However, upon running your code, we should see: - A histogram of `bill_length_mm` colored by `species` with the legend moved to the \'center right\'. - A scatter plot of `bill_length_mm` vs `flipper_length_mm` colored by `species` with the legend at the upper left outside the plot. - Faceted histograms of `bill_length_mm` for each island with correctly positioned legends inside the plots. # Code Skeleton ```python import seaborn as sns import matplotlib.pyplot as plt # Task 1: Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Remove rows with missing values penguins = penguins.dropna() # Task 2: Univariate Analysis ax = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax, \\"center right\\") # Task 3: Bivariate Analysis ax = sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"flipper_length_mm\\", hue=\\"species\\") sns.move_legend(ax, \\"upper left\\", bbox_to_anchor=(1, 1)) # Task 4: Faceted Plot g = sns.displot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3) sns.move_legend(g, \\"upper left\\", bbox_to_anchor=(.55, .45), frameon=False) # Show the plots plt.show() ``` Make sure your implemented solution follows the structure and requirements outlined above.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Task 1: Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Remove rows with missing values penguins = penguins.dropna() # Task 2: Univariate Analysis plt.figure(figsize=(10, 6)) ax = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", multiple=\\"stack\\") ax.legend(loc=\'center right\') plt.title(\'Histogram of Bill Length by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Count\') plt.show() # Task 3: Bivariate Analysis plt.figure(figsize=(10, 6)) ax = sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"flipper_length_mm\\", hue=\\"species\\") ax.legend(loc=\'upper left\', bbox_to_anchor=(1, 1)) plt.title(\'Bill Length vs Flipper Length by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Flipper Length (mm)\') plt.show() # Task 4: Faceted Plot g = sns.displot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=3) for ax in g.axes.flat: ax.legend(loc=\'upper left\', frameon=False) g.set_axis_labels(\'Bill Length (mm)\', \'Count\') g.set_titles(\'Island: {col_name}\') plt.show()"},{"question":"# Async Task Manager **Objective:** Create a Python script using the asyncio event loop to manage and execute tasks. The script will have to schedule a series of tasks, handle exceptions, and ensure proper cleanup of resources. **Requirements:** 1. **Task Scheduling:** - Create a function `task_handler(name, delay)` that simulates an asynchronous task. It should print a message indicating the start and end of the task, including the task name and delay. ```python async def task_handler(name: str, delay: int): print(f\\"Task {name} started, will run for {delay} seconds.\\") await asyncio.sleep(delay) print(f\\"Task {name} completed.\\") ``` 2. **Main Function:** - Implement an async main function that: - Creates a list of tasks using `asyncio.create_task()`. - Schedules the tasks to run concurrently. - Waits for all tasks to complete using `asyncio.gather()`. ```python async def main(): tasks = [ asyncio.create_task(task_handler(\'Task1\', 2)), asyncio.create_task(task_handler(\'Task2\', 4)), asyncio.create_task(task_handler(\'Task3\', 1)) ] await asyncio.gather(*tasks) ``` 3. **Exception Handling:** - Modify the `task_handler` to raise an exception for one of the tasks. - Update the main function to handle exceptions using a try-except block. ```python async def task_handler(name: str, delay: int): print(f\\"Task {name} started, will run for {delay} seconds.\\") await asyncio.sleep(delay) if name == \'Task2\': raise Exception(f\\"Error in {name}\\") print(f\\"Task {name} completed.\\") async def main(): tasks = [ asyncio.create_task(task_handler(\'Task1\', 2)), asyncio.create_task(task_handler(\'Task2\', 4)), asyncio.create_task(task_handler(\'Task3\', 1)) ] try: await asyncio.gather(*tasks) except Exception as e: print(f\\"Exception occurred: {e}\\") ``` 4. **Clean-up:** - Ensure that all resources are cleaned up properly after task completion. Use the necessary shutdown methods for the event loop. ```python if __name__ == \'__main__\': loop = asyncio.get_event_loop() try: loop.run_until_complete(main()) finally: loop.run_until_complete(loop.shutdown_asyncgens()) loop.close() ``` **Input and Output:** - **Input:** No direct input; tasks and delays are hardcoded. - **Output:** The script output will display task start and end messages and handle any exceptions raised. **Constraints:** 1. Use only features and methods mentioned in the provided documentation. 2. Ensure that the event loop and all tasks are properly managed and closed. **Performance Requirements:** - The tasks should run concurrently and efficiently utilize the asyncio event loop. - Resource cleanup should be properly handled to ensure no memory leaks or unclosed resources. **Bonus:** - Implement logging instead of print statements to track the task progress and errors. - Add command-line arguments to specify task names and delays.","solution":"import asyncio async def task_handler(name: str, delay: int): print(f\\"Task {name} started, will run for {delay} seconds.\\") await asyncio.sleep(delay) if name == \'Task2\': raise Exception(f\\"Error in {name}\\") print(f\\"Task {name} completed.\\") async def main(): tasks = [ asyncio.create_task(task_handler(\'Task1\', 2)), asyncio.create_task(task_handler(\'Task2\', 4)), asyncio.create_task(task_handler(\'Task3\', 1)) ] try: await asyncio.gather(*tasks) except Exception as e: print(f\\"Exception occurred: {e}\\") if __name__ == \'__main__\': loop = asyncio.get_event_loop() try: loop.run_until_complete(main()) finally: loop.run_until_complete(loop.shutdown_asyncgens()) loop.close()"},{"question":"Analyzing Regression Residuals with Seaborn In this task, you are required to use the seaborn library to analyze the residuals of a linear regression model. Your goal is to visualize the residuals and identify potential violations of linear regression assumptions. Instructions: 1. **Load the Dataset:** - Load the `mpg` dataset from seaborn. 2. **Basic Residual Plot:** - Create a basic residual plot to visualize the residuals of a linear regression model where `x` is \\"weight\\" and `y` is \\"displacement\\". 3. **Highlight Structure:** - Create another residual plot where `x` is \\"horsepower\\" and `y` is \\"mpg\\" to identify any structure in the residuals that might indicate a violation of linear regression assumptions. 4. **Handle Higher-Order Trends:** - Adjust the previous residual plot by removing higher-order trends using a second-order polynomial. 5. **Add LOWESS Curve:** - Further adjust the residual plot by adding a LOWESS curve to emphasize any underlying structure in the residuals. 6. **Analyze and Present:** - Comment on any patterns or structures you observe in the plots. Specifically, discuss whether the adjustments (higher-order trends and LOWESS curve) helped in stabilizing the residuals. Requirements: - Use seaborn\'s `residplot` function for all plots. - Customize the plots with appropriate titles, labels, and color settings to make them informative and visually appealing. - Write a brief analysis (2-3 sentences) for each plot discussing your observations and conclusions. Example Code Snippet for Reference: ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset mpg = sns.load_dataset(\\"mpg\\") # Basic residual plot sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual Plot: Weight vs Displacement\\") plt.show() # Residual plot to highlight structure sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Residual Plot: Horsepower vs MPG\\") plt.show() # Removing higher-order trends sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Residual Plot with Higher-Order Polynomial: Horsepower vs MPG\\") plt.show() # Adding a LOWESS curve sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"red\\")) plt.title(\\"Residual Plot with LOWESS: Horsepower vs MPG\\") plt.show() ``` Make sure that your solution is well-commented and follows good coding practices. Submission: - A Python script or Jupyter notebook (.py or .ipynb) containing your plots and analyses. - Each plot should be clearly labeled with titles, x-axis, and y-axis labels. - Observational comments should be included either within the code as comments or in markdown cells (if using Jupyter notebook).","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset mpg = sns.load_dataset(\\"mpg\\") def plot_residuals(): # Basic residual plot plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual Plot: Weight vs Displacement\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Residuals\\") plt.show() # Observations: # This plot shows the residuals of a linear regression between weight and displacement. # We can observe if there\'s any apparent pattern or randomness in the residuals. # Residual plot to highlight structure plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Residual Plot: Horsepower vs MPG\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Observations: # This plot looks for any structure in the residuals that might indicate a misspecification # of the linear model. If there\'s a clear pattern, it could suggest that the relationship # is not purely linear. # Removing higher-order trends plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Residual Plot with Higher-Order Polynomial: Horsepower vs MPG\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Observations: # By fitting a second-order polynomial, we try to handle any higher-order trend. # This plot helps to check if the polynomial fit reduces the structure in residuals. # Adding a LOWESS curve plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"red\\")) plt.title(\\"Residual Plot with LOWESS: Horsepower vs MPG\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Observations: # Adding a LOWESS curve helps in emphasizing any underlying structure in the residuals. # If the LOWESS curve fluctuates widely, it may indicate non-linearity or other issues # in the model fit."},{"question":"**Question: Implement a Custom Filename Matcher** You are given two lists: one containing filenames and another containing different patterns. Your task is to implement a function that categorizes each filename based on whether it matches any of the given patterns using Unix shell-style wildcards. # Function ```python def categorize_filenames(filenames, patterns): pass ``` # Input - `filenames`: List of strings representing filenames (e.g., `[\\"file1.txt\\", \\"file2.py\\", \\"image.png\\"]`). - `patterns`: List of strings representing shell-style wildcard patterns (e.g., `[\\"*.txt\\", \\"*.py\\"]`). # Output - A dictionary where the keys are the patterns and the values are lists of filenames that match the respective patterns, preserving the order of filenames. # Constraints - You cannot use the `fnmatch.filter` method directly. - You must handle both case-sensitive and case-insensitive matching scenarios. - Assume filenames and patterns contain only valid Unix filename characters. - The number of filenames (`len(filenames)`) and patterns (`len(patterns)`) will not exceed 1000. # Example ```python filenames = [\\"file1.txt\\", \\"file2.py\\", \\"image.png\\", \\"file3.TXT\\"] patterns = [\\"*.txt\\", \\"*.py\\", \\"*.TXT\\"] assert categorize_filenames(filenames, patterns) == { \\"*.txt\\": [\\"file1.txt\\"], \\"*.py\\": [\\"file2.py\\"], \\"*.TXT\\": [\\"file3.TXT\\"] } ``` # Notes - Use the `fnmatch.fnmatch` and `fnmatch.fnmatchcase` functions where applicable. - Ensure your solution efficiently categorizes the filenames without checking each pattern against every filename redundantly. Write your implementation of the function `categorize_filenames`.","solution":"import fnmatch def categorize_filenames(filenames, patterns): Categorizes each filename based on whether it matches any of the given patterns using Unix shell-style wildcards. Args: - filenames (list of str): List of filenames. - patterns (list of str): List of shell-style wildcard patterns. Returns: - dict: Dictionary where keys are patterns and values are lists of filenames matching the patterns. result = {pattern: [] for pattern in patterns} for filename in filenames: for pattern in patterns: if fnmatch.fnmatchcase(filename, pattern): result[pattern].append(filename) return result"},{"question":"Advanced Seaborn Visualization You are provided with the `diamonds` dataset from the Seaborn library, which contains various attributes of diamonds such as price, carat, clarity, cut, color, and others. # Task Using the `seaborn.boxenplot` function, create a comprehensive visualization that meets the following criteria: 1. **Group the diamonds by their `cut` and visualize the distribution of their `price` using boxen plots.** 2. **Add another layer of categorization based on whether the diamond is large or small using the `carat` attribute (consider diamonds with `carat` > 1 as large and those with `carat` <= 1 as small). Use different colors to represent these groups.** 3. **Customize the boxen plots such that:** - The boxes are not filled (`fill=False`). - The width method used should be linear (`width_method=\'linear\'`). - The line width of the box edges is set to 0.5 (`linewidth=.5`), and the line color is set to a shade of gray (`linecolor=\'.7\'`). # Expected Input and Output Formats - **Input:** None (The `diamonds` dataset is loaded directly within the function). - **Output:** A Matplotlib Axes object showing the customized boxen plots. # Constraints - Use only `seaborn` and `pandas` libraries for data loading and visualization. - Ensure the plot is neatly labeled with appropriate titles and axis labels. # Performance Requirements The plot should be generated quickly and efficiently, handling the size of the diamonds dataset (~54,000 records) without performance issues. # Implementation ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_diamonds(): # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Categorize the diamonds as large or small large_diamond = diamonds[\\"carat\\"] > 1 diamonds[\\"size\\"] = large_diamond.map({True: \\"Large\\", False: \\"Small\\"}) # Create the boxen plot plt.figure(figsize=(12, 8)) ax = sns.boxenplot( data=diamonds, x=\\"price\\", y=\\"cut\\", hue=\\"size\\", fill=False, width_method=\\"linear\\", linewidth=0.5, linecolor=\'.7\' ) # Set plot title and labels ax.set_title(\\"Price Distribution of Diamonds by Cut and Size\\") ax.set_xlabel(\\"Price\\") ax.set_ylabel(\\"Cut\\") plt.legend(title=\'Size\') plt.tight_layout() # Return the plot return ax # Function call for testing visualize_diamonds() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_diamonds(): # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Categorize the diamonds as large or small diamonds[\\"size\\"] = diamonds[\\"carat\\"].apply(lambda carat: \\"Large\\" if carat > 1 else \\"Small\\") # Create the boxen plot plt.figure(figsize=(12, 8)) ax = sns.boxenplot( data=diamonds, x=\\"price\\", y=\\"cut\\", hue=\\"size\\", dodge=True, palette=\\"pastel\\", linewidth=0.5 ) # Set plot title and labels ax.set_title(\\"Price Distribution of Diamonds by Cut and Size\\") ax.set_xlabel(\\"Price\\") ax.set_ylabel(\\"Cut\\") # Customize legend plt.legend(title=\\"Size\\") plt.tight_layout() # Return the plot return ax"},{"question":"**Email Parsing Assessment** In this assessment, you are required to implement a function that processes email messages and extracts specific information from them. You will utilize the `email.parser` module to parse emails and retrieve the necessary details. # Problem Statement Write a function `extract_email_details(emails: List[bytes]) -> List[Dict[str, Any]]` that accepts a list of emails as bytes-like objects. The function should parse each email, extract specific information, and return a list of dictionaries containing the extracted details. Each dictionary should have the following keys: - `subject` - `from` - `to` - `date` - `is_multipart` - `body` # Input - `emails`: A list of bytes-like objects containing email messages. # Output - A list of dictionaries with the extracted details from each email. # Constraints - Each email in the input list is a well-formatted email message. - The `body` field should contain the plain text content of the email. For multipart messages, it should contain the text content of the first subpart. - If any header is missing in an email, the corresponding dictionary value should be an empty string. - The expected format for the `date` field is \'%Y-%m-%d %H:%M:%S\' (e.g., \'2023-10-01 14:30:00\'). If the date is missing or invalid, the field should be an empty string. # Example ```python from typing import List, Dict, Any import email from email.policy import default def extract_email_details(emails: List[bytes]) -> List[Dict[str, Any]]: details_list = [] for email_bytes in emails: msg = email.message_from_bytes(email_bytes, policy=default) details = { \'subject\': msg[\'subject\'] or \'\', \'from\': msg[\'from\'] or \'\', \'to\': msg[\'to\'] or \'\', \'date\': \'\', \'is_multipart\': msg.is_multipart(), \'body\': \'\' } if msg[\'date\']: try: date = email.utils.parsedate_to_datetime(msg[\'date\']) details[\'date\'] = date.strftime(\'%Y-%m-%d %H:%M:%S\') except Exception: details[\'date\'] = \'\' if msg.is_multipart(): for part in msg.iter_parts(): if part.get_content_type() == \'text/plain\': details[\'body\'] = part.get_payload(decode=True).decode() break else: details[\'body\'] = msg.get_payload(decode=True).decode() details_list.append(details) return details_list ``` Write the implementation for the `extract_email_details` function. Use the provided example as a reference for the expected behavior.","solution":"from typing import List, Dict, Any import email from email.policy import default def extract_email_details(emails: List[bytes]) -> List[Dict[str, Any]]: details_list = [] for email_bytes in emails: msg = email.message_from_bytes(email_bytes, policy=default) details = { \'subject\': msg[\'subject\'] or \'\', \'from\': msg[\'from\'] or \'\', \'to\': msg[\'to\'] or \'\', \'date\': \'\', \'is_multipart\': msg.is_multipart(), \'body\': \'\' } if msg[\'date\']: try: date = email.utils.parsedate_to_datetime(msg[\'date\']) details[\'date\'] = date.strftime(\'%Y-%m-%d %H:%M:%S\') except Exception: details[\'date\'] = \'\' if msg.is_multipart(): for part in msg.iter_parts(): if part.get_content_type() == \'text/plain\': details[\'body\'] = part.get_payload(decode=True).decode() break else: details[\'body\'] = msg.get_payload(decode=True).decode() details_list.append(details) return details_list"},{"question":"**Objective**: Assess understanding of pandas integration with PyArrow, focusing on creating and manipulating PyArrow-backed data structures and leveraging PyArrow\'s performance benefits. **Problem Description**: You are tasked to demonstrate proficiency in utilizing PyArrow with pandas. Specifically, you need to construct a PyArrow-backed pandas DataFrame, perform various operations on it, and utilize PyArrow for efficient I/O operations. **Instructions**: 1. **DataFrame Construction**: - Create a pandas DataFrame named `df` with the following data and respective PyArrow-backed data types: ``` | a (int32[pyarrow]) | b (float64[pyarrow]) | c (string[pyarrow]) | |-----------------------|------------------------|---------------------| | 10 | 1.5 | apple | | 20 | 2.5 | banana | | 30 | 3.5 | cherry | ``` 2. **Operations**: - Compute the mean of column `a`. - Add 3.5 to each value in column `b`. - Check which values in column `c` contain the letter `\'a\'`. 3. **I/O Reading**: - Convert the DataFrame `df` to a CSV string using PyArrow as the engine. - Read the CSV string back into a DataFrame `df_read`, ensuring it is PyArrow-backed. **Expected Input and Output Formats**: - The function should not take any inputs but should output: - The DataFrame `df` after performing the specified operations. - The DataFrame `df_read` after reading back from the CSV string. **Constraints**: - Ensure the minimum supported PyArrow version is installed. **Performance Requirements**: - Utilize PyArrow to enhance performance where applicable. **Function Signature**: ```python import pandas as pd def pandas_pyarrow_operations(): import pyarrow as pa import io # Step 1: Create the DataFrame with PyArrow-backed data types # Your code here # Step 2: Perform operations # Your code here # Step 3: Convert DataFrame to CSV using PyArrow and read it back # Your code here return df, df_read ``` **Notes**: - Use proper PyArrow data types as specified. - Ensure the operations leverage PyArrow for performance benefits. - Validate the PyArrow-backed data structure after reading from the CSV.","solution":"import pandas as pd import pyarrow as pa import io def pandas_pyarrow_operations(): # Step 1: Create the DataFrame with PyArrow-backed data types data = { \\"a\\": pd.Series([10, 20, 30], dtype=\\"int32[pyarrow]\\"), \\"b\\": pd.Series([1.5, 2.5, 3.5], dtype=\\"float64[pyarrow]\\"), \\"c\\": pd.Series([\\"apple\\", \\"banana\\", \\"cherry\\"], dtype=\\"string[pyarrow]\\") } df = pd.DataFrame(data) # Step 2: Perform operations mean_a = df[\'a\'].mean() df[\'b\'] = df[\'b\'] + 3.5 contains_a = df[\'c\'].str.contains(\'a\') # Step 3: Convert DataFrame to CSV using PyArrow and read it back csv_buffer = io.StringIO() df.to_csv(csv_buffer, index=False) csv_string = csv_buffer.getvalue() df_read = pd.read_csv(io.StringIO(csv_string), dtype={ \'a\': \\"int32[pyarrow]\\", \'b\': \\"float64[pyarrow]\\", \'c\': \\"string[pyarrow]\\" }) return df, df_read"},{"question":"**Coding Assessment Question:** # Objective: Assess your understanding of seaborn\'s capabilities for visualizing statistical relationships by creating complex, customized scatter and line plots. You will demonstrate your ability to use different seaborn functionalities such as semantic mappings, custom color palettes, and faceting. # Problem: You are provided with two datasets: `tips` and `fmri`. The `tips` dataset contains information about restaurant tips, and the `fmri` dataset contains information about brain activity. 1. **Scatter Plot: Visualizing Tips Data** - Create a scatter plot showing the relationship between `total_bill` and `tip`. - Use color (`hue`) to differentiate between smokers and non-smokers. - Use marker style (`style`) to differentiate between different times of the day (`time`). - Use point size (`size`) to represent the size of the party (`size`). - Customize the color palette to a sequential palette of your choice. 2. **Line Plot: Visualizing fmri Data** - Create a line plot showing the average `signal` over `timepoint`. - Color the lines based on the `event` and differentiate the `region` with line style (`style`). - Add confidence intervals showing the spread of the data using standard deviation. - Facet the plot by `event` on the columns and by `region` on the rows to see detailed individual traces. # Expected Input and Output: - **Input:** - `tips` and `fmri` datasets loaded in the provided environment. - **Output:** - Two seaborn plots as specified. # Constraints: - Use the seaborn `relplot` function for both scatter and line plots. - Ensure plots are clear and their legends are comprehensible. - Customize the appearance to make the plots visually appealing. # Example Code: ```python import seaborn as sns import matplotlib.pyplot as plt # Load datasets tips = sns.load_dataset(\\"tips\\") fmri = sns.load_dataset(\\"fmri\\") # Scatter Plot - Tips data sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", style=\\"time\\", size=\\"size\\", palette=\\"ch:r=-.5,l=.75\\" ) # Line Plot - fmri data sns.relplot( data=fmri, kind=\\"line\\", x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"region\\", errorbar=\\"sd\\", col=\\"event\\", row=\\"region\\" ) plt.show() ``` **Note:** This example is provided for illustration. Your task is to implement a similar solution by customizing the plots as per the given requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_scatter_plot_tips(): tips = sns.load_dataset(\\"tips\\") sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", style=\\"time\\", size=\\"size\\", palette=\\"ch:r=-.5,l=.75\\" ) plt.title(\\"Scatter Plot of Tips\\") plt.show() def create_line_plot_fmri(): fmri = sns.load_dataset(\\"fmri\\") sns.relplot( data=fmri, kind=\\"line\\", x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"region\\", errorbar=\\"sd\\", col=\\"event\\", row=\\"region\\" ) plt.title(\\"Line Plot of fmri data\\") plt.show()"},{"question":"**Coding Assessment Question** # Problem Statement: You are tasked with creating a Python function that takes a directory path as input, compresses all `.txt` files in that directory using the `gzip` format, and saves the compressed files in a specified output directory. The compression should maintain the directory structure, i.e., subdirectories should also be processed recursively. # Function Signature: ```python def compress_txt_files(input_dir: str, output_dir: str, compresslevel: int = 9) -> None: pass ``` # Input: - `input_dir` (str): Path to the input directory containing `.txt` files. - `output_dir` (str): Path to the output directory where compressed files should be saved. - `compresslevel` (int): Compression level to be used by `gzip`. Defaults to 9. # Output: - The function does not return any value. It writes compressed files to the `output_dir`. # Example: Given the following directory structure in `input_dir`: ``` input_dir/ ├── file1.txt ├── file2.txt └── sub_dir ├── file3.txt └── file4.txt ``` After calling `compress_txt_files(\'input_dir\', \'output_dir\')`, the directory structure in `output_dir` should be: ``` output_dir/ ├── file1.txt.gz ├── file2.txt.gz └── sub_dir ├── file3.txt.gz └── file4.txt.gz ``` # Constraints: - The function should process only `.txt` files. - The function should correctly handle nested directories. - You can assume that the input directory exists and is readable. - You can assume that the output directory exists and is writable. # Usage of the `gzip` Module: - Utilize the `gzip.open` method for compressing files. - Ensure that the compression level can be adjusted via the `compresslevel` parameter. # Task: Implement the function `compress_txt_files` following the specifications mentioned.","solution":"import os import gzip import shutil def compress_txt_files(input_dir: str, output_dir: str, compresslevel: int = 9) -> None: Compresses all .txt files in the input directory and subdirectories and saves them in the output directory with the same relative paths. Args: input_dir (str): Path to the input directory containing .txt files. output_dir (str): Path to the output directory where compressed files should be saved. compresslevel (int): Compression level to be used by gzip. Defaults to 9. for root, _, files in os.walk(input_dir): # Create corresponding directory structure in output_dir relative_path = os.path.relpath(root, input_dir) dest_dir = os.path.join(output_dir, relative_path) os.makedirs(dest_dir, exist_ok=True) for file in files: if file.endswith(\'.txt\'): input_file_path = os.path.join(root, file) output_file_path = os.path.join(dest_dir, file + \'.gz\') with open(input_file_path, \'rb\') as f_in: with gzip.open(output_file_path, \'wb\', compresslevel=compresslevel) as f_out: shutil.copyfileobj(f_in, f_out)"},{"question":"# Question: Implementing a ResourceLock Class with Context Manager Behavior You are required to implement a class `ResourceLock` that manages access to a shared resource. It should support context manager behavior to ensure that locks are acquired and released appropriately. Requirements: 1. **Constructor**: - `ResourceLock(resource)`: Accepts a string identifier for the resource. 2. **Context Manager Methods**: - Implement `__enter__` and `__exit__` methods to acquire and release locks. 3. **Special Methods**: - Implement `__repr__` to provide a meaningful representation of the object. - Implement `__call__` method that allows checking the state of the lock by calling the object with no arguments, returning a boolean value indicating if the resource is currently locked (`True` if locked, `False` otherwise). 4. **Lock Management**: - Maintain an internal state indicating whether the resource is currently locked. - Ensure that the resource status changes correctly when entering and exiting the context. Implementation: ```python class ResourceLock: def __init__(self, resource: str): Initialize the resource lock pass def __enter__(self): Acquire the lock on the resource, raise RuntimeError if the resource is already locked when entering. pass def __exit__(self, exc_type, exc_value, traceback): Release the lock on the resource. pass def __repr__(self): Return a meaningful string representation of the instance pass def __call__(self): Check the state of the lock: True if locked, False otherwise pass # Example usage resource = \\"Resource_A\\" lock = ResourceLock(resource) # Should print the representation of the lock object print(lock) # Locking should work as a context manager try: with lock: print(lock()) # Should output: True print(lock()) # Should output: False except RuntimeError as e: print(e) ``` Constraints: - Raise `RuntimeError` if attempting to acquire an already locked resource. - Ensure the lock state is correctly maintained across multiple enter/exit cycles. Expected Output: 1. Correctly print the string representation of the `ResourceLock` object. 2. Demonstrate proper lock management and context manager behavior. 3. Raise appropriate exceptions when attempting to acquire a locked resource. Write your implementation of the `ResourceLock` class below: ```python # Your implementation here ```","solution":"class ResourceLock: def __init__(self, resource: str): Initialize the resource lock with a name and a lock state self.resource = resource self._is_locked = False def __enter__(self): Acquire the lock on the resource, raise RuntimeError if the resource is already locked when entering. if self._is_locked: raise RuntimeError(f\\"The resource \'{self.resource}\' is already locked.\\") self._is_locked = True return self def __exit__(self, exc_type, exc_value, traceback): Release the lock on the resource. self._is_locked = False def __repr__(self): Return a meaningful string representation of the instance return f\\"ResourceLock(resource={self.resource}, is_locked={self._is_locked})\\" def __call__(self): Check the state of the lock: True if locked, False otherwise return self._is_locked"},{"question":"# Python Coding Assessment: Log File Analyzer Problem Statement You are required to write a Python program that analyzes a log file. The log file contains multiple lines, each representing a log entry with the following format: ``` <timestamp> <log_level> <message> ``` - `<timestamp>` is a string representing the date and time of the log entry in the format `YYYY-MM-DD HH:MM:SS`. - `<log_level>` is one of the following values: `INFO`, `WARNING`, `ERROR`. - `<message>` is an arbitrary string containing the log message. Your task is to implement the following functions: 1. **`read_log_file(file_path: str) -> list[dict]`**: - This function reads the log file from the given `file_path` and returns a list of dictionaries. Each dictionary should represent a log entry with keys: `timestamp`, `log_level`, and `message`. 2. **`filter_logs_by_level(logs: list[dict], level: str) -> list[dict]`**: - This function filters the list of log dictionaries to only include entries with the given log level (`level`). 3. **`sort_logs_by_timestamp(logs: list[dict]) -> list[dict]`**: - This function sorts the list of log dictionaries by their timestamp in ascending order. 4. **`summarize_log_levels(logs: list[dict]) -> dict`**: - This function returns a dictionary summarizing the count of each log level in the given list of logs. The keys should be the log levels, and the values should be the counts of each log level. Input and Output Formats 1. **Function: `read_log_file(file_path: str)`** - **Input**: - `file_path` (str): The path to the log file. - **Output**: - A list of dictionaries, where each dictionary represents a log entry with the keys: `timestamp` (str), `log_level` (str), and `message` (str). 2. **Function: `filter_logs_by_level(logs: list[dict], level: str)`** - **Input**: - `logs` (list[dict]): The list of log dictionaries. - `level` (str): The log level to filter by (`INFO`, `WARNING`, `ERROR`). - **Output**: - A list of log dictionaries filtered by the given log level. 3. **Function: `sort_logs_by_timestamp(logs: list[dict])`** - **Input**: - `logs` (list[dict]): The list of log dictionaries. - **Output**: - A list of log dictionaries sorted by the timestamp in ascending order. 4. **Function: `summarize_log_levels(logs: list[dict])`** - **Input**: - `logs` (list[dict]): The list of log dictionaries. - **Output**: - A dictionary summarizing the count of each log level (`INFO`, `WARNING`, `ERROR`). Constraints - You can assume that the log file is well-formed and all timestamps are in the correct format. - The functions should handle large log files efficiently. Example Suppose the content of the log file `log.txt` is as follows: ``` 2023-04-21 10:00:00 INFO User logged in 2023-04-21 10:05:12 WARNING Disk space low 2023-04-21 10:15:30 ERROR Exception occurred 2023-04-21 10:20:45 INFO User logged out ``` Calling the functions with this log file should produce the following results: ```python file_path = \'log.txt\' logs = read_log_file(file_path) # logs: # [ # {\'timestamp\': \'2023-04-21 10:00:00\', \'log_level\': \'INFO\', \'message\': \'User logged in\'}, # {\'timestamp\': \'2023-04-21 10:05:12\', \'log_level\': \'WARNING\', \'message\': \'Disk space low\'}, # {\'timestamp\': \'2023-04-21 10:15:30\', \'log_level\': \'ERROR\', \'message\': \'Exception occurred\'}, # {\'timestamp\': \'2023-04-21 10:20:45\', \'log_level\': \'INFO\', \'message\': \'User logged out\'} # ] filtered_logs = filter_logs_by_level(logs, \'INFO\') # filtered_logs: # [ # {\'timestamp\': \'2023-04-21 10:00:00\', \'log_level\': \'INFO\', \'message\': \'User logged in\'}, # {\'timestamp\': \'2023-04-21 10:20:45\', \'log_level\': \'INFO\', \'message\': \'User logged out\'} # ] sorted_logs = sort_logs_by_timestamp(logs) # sorted_logs: # [ # {\'timestamp\': \'2023-04-21 10:00:00\', \'log_level\': \'INFO\', \'message\': \'User logged in\'}, # {\'timestamp\': \'2023-04-21 10:05:12\', \'log_level\': \'WARNING\', \'message\': \'Disk space low\'}, # {\'timestamp\': \'2023-04-21 10:15:30\', \'log_level\': \'ERROR\', \'message\': \'Exception occurred\'}, # {\'timestamp\': \'2023-04-21 10:20:45\', \'log_level\': \'INFO\', \'message\': \'User logged out\'} # ] summary = summarize_log_levels(logs) # summary: # { # \'INFO\': 2, # \'WARNING\': 1, # \'ERROR\': 1 # } ``` Notes - Make sure to include appropriate error handling where necessary. - Optimize your implementations for performance, particularly for large log files.","solution":"import datetime def read_log_file(file_path: str) -> list[dict]: logs = [] with open(file_path, \'r\') as file: for line in file: parts = line.strip().split(\' \', 2) timestamp, log_level, message = parts[0] + \' \' + parts[1], parts[2].split(\' \')[0], parts[2].split(\' \', 1)[1] logs.append({ \'timestamp\': timestamp, \'log_level\': log_level, \'message\': message }) return logs def filter_logs_by_level(logs: list[dict], level: str) -> list[dict]: return [log for log in logs if log[\'log_level\'] == level] def sort_logs_by_timestamp(logs: list[dict]) -> list[dict]: return sorted(logs, key=lambda x: datetime.datetime.strptime(x[\'timestamp\'], \'%Y-%m-%d %H:%M:%S\')) def summarize_log_levels(logs: list[dict]) -> dict: summary = {\'INFO\': 0, \'WARNING\': 0, \'ERROR\': 0} for log in logs: summary[log[\'log_level\']] += 1 return summary"},{"question":"# Question: Unsupervised Dimensionality Reduction and Classification with scikit-learn In this task, you are required to implement a function that performs unsupervised dimensionality reduction on a given dataset and then classifies the reduced data using a supervised learning algorithm. You will work with the following unsupervised dimensionality reduction techniques provided by scikit-learn: 1. Principal Component Analysis (PCA) 2. Random Projections 3. Feature Agglomeration You are also required to scale the features before applying the dimensionality reduction techniques. After reducing the dimensions, you will use a supervised learning algorithm (e.g., a Support Vector Classifier) to classify the data. Function Signature ```python from sklearn.base import BaseEstimator def dimensionality_reduction_classification(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, method: str, n_components: int, classifier: BaseEstimator) -> np.ndarray: \'\'\' Perform dimensionality reduction and classification on the dataset. Parameters: X_train (np.ndarray): Training feature set. y_train (np.ndarray): Training labels. X_test (np.ndarray): Test feature set. method (str): Dimensionality reduction method (\'pca\', \'random_projection\', \'feature_agglomeration\'). n_components (int): Number of components to keep after reducing the dimensionality. classifier (BaseEstimator): A sklearn classifier instance to be used after dimensionality reduction. Returns: np.ndarray: Predicted labels for X_test. \'\'\' pass ``` Requirements - The function should first scale the features using `StandardScaler`. - Based on the value of the `method` parameter, it should apply the respective dimensionality reduction technique: - **PCA**: Use `sklearn.decomposition.PCA` - **Random Projections**: Use `sklearn.random_projection.SparseRandomProjection` or `GaussianRandomProjection` - **Feature Agglomeration**: Use `sklearn.cluster.FeatureAgglomeration` - Apply the given classifier on the reduced dataset and return the predicted labels for `X_test`. Example Usage ```python from sklearn.decomposition import PCA from sklearn.random_projection import SparseRandomProjection from sklearn.cluster import FeatureAgglomeration from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC import numpy as np from sklearn.datasets import load_iris # Load dataset data = load_iris() X, y = data.data, data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define classifier clf = SVC() # Example 1: Using PCA predictions = dimensionality_reduction_classification(X_train, y_train, X_test, method=\'pca\', n_components=2, classifier=clf) # Example 2: Using Random Projections predictions = dimensionality_reduction_classification(X_train, y_train, X_test, method=\'random_projection\', n_components=2, classifier=clf) # Example 3: Using Feature Agglomeration predictions = dimensionality_reduction_classification(X_train, y_train, X_test, method=\'feature_agglomeration\', n_components=2, classifier=clf) ``` Constraints - Ensure that the dimensionality reduction retains at least `n_components` dimensions. - The classifier should be a valid scikit-learn classifier instance. - The function should handle potential issues such as incompatible dimensionality reduction method strings and invalid component numbers gracefully, raising appropriate exceptions where necessary.","solution":"import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.random_projection import SparseRandomProjection, GaussianRandomProjection from sklearn.cluster import FeatureAgglomeration from sklearn.base import BaseEstimator def dimensionality_reduction_classification(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, method: str, n_components: int, classifier: BaseEstimator) -> np.ndarray: Perform dimensionality reduction and classification on the dataset. Parameters: X_train (np.ndarray): Training feature set. y_train (np.ndarray): Training labels. X_test (np.ndarray): Test feature set. method (str): Dimensionality reduction method (\'pca\', \'random_projection\', \'feature_agglomeration\'). n_components (int): Number of components to keep after reducing the dimensionality. classifier (BaseEstimator): A sklearn classifier instance to be used after dimensionality reduction. Returns: np.ndarray: Predicted labels for X_test. if method not in [\'pca\', \'random_projection\', \'feature_agglomeration\']: raise ValueError(f\\"Method \'{method}\' is not supported. Choose from \'pca\', \'random_projection\', \'feature_agglomeration\'.\\") # Scaling the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Dimensionality reduction if method == \'pca\': dim_reducer = PCA(n_components=n_components) elif method == \'random_projection\': dim_reducer = SparseRandomProjection(n_components=n_components) if n_components <= X_train.shape[1] else GaussianRandomProjection(n_components=n_components) elif method == \'feature_agglomeration\': dim_reducer = FeatureAgglomeration(n_clusters=n_components) X_train_reduced = dim_reducer.fit_transform(X_train_scaled) X_test_reduced = dim_reducer.transform(X_test_scaled) # Train the classifier classifier.fit(X_train_reduced, y_train) # Predict the results predictions = classifier.predict(X_test_reduced) return predictions"},{"question":"# Advanced Python Exception Handling You are tasked with implementing a small module that simulates a file processing system with several potential error conditions. This will test your ability to handle a variety of exceptions properly and ensure your code is robust and informative in the face of errors. Requirements: 1. **Custom Exceptions**: Create three custom exceptions: - `FileFormatError`: Raised when a file has an incorrect format. - `ProcessingError`: Raised for general processing errors. - `CriticalError`: Raised when a critical error occurs, subclassed from `ProcessingError`. 2. **Function Implementation**: - Write a function `process_file(file_name: str) -> None` that: 1. Simulates reading from a file and processes its content. 2. Raises `FileFormatError` if the file format is not as expected. 3. Raises a `ProcessingError` for general issues, but if the issue is critical, raise `CriticalError` and chain it using `raise ... from ...`. 3. **Exception Handling**: - Implement top-level exception handling in another function `safe_process(file_name: str) -> None` that: 1. Calls `process_file(file_name)`. 2. Catches and handles all possible exceptions. 3. For `FileFormatError`, print out an appropriate error message. 4. For `ProcessingError` and its subclasses, print the error message and the cause if any. 5. Catches other exceptions generically and logs a message indicating an unexpected issue. Input: - `file_name` as a string representing the file to process. For simplicity, you do not need to interact with the file system; you can simulate file content within the function itself. Expected Output: - Proper error messages printed on exceptions, e.g., \\"Error: Incorrect file format.\\", \\"Error: Processing issue occurred. Cause: XYZ\\". Constraints: - Ensure proper use of exception handling and chaining. - Assume file content inside the function (for simulation purposes). Example: ```python class FileFormatError(Exception): pass class ProcessingError(Exception): pass class CriticalError(ProcessingError): pass def process_file(file_name: str) -> None: # Example simulation of the function logic if file_name == \\"bad_format.txt\\": raise FileFormatError(\\"Incorrect file format.\\") elif file_name == \\"critical_issue.txt\\": try: # Simulate some processing that fails critically raise ValueError(\\"Critical internal issue.\\") except ValueError as e: raise CriticalError(\\"A critical processing error occurred.\\") from e elif file_name == \\"general_issue.txt\\": raise ProcessingError(\\"General processing error.\\") else: print(f\\"Processing {file_name} successfully completed.\\") def safe_process(file_name: str) -> None: try: process_file(file_name) except FileFormatError as e: print(f\\"Error: {e}\\") except ProcessingError as e: print(f\\"Error: {e}. Cause: {e.__cause__}\\") except Exception as e: print(f\\"Unexpected error: {e}\\") # Testing the functions safe_process(\\"bad_format.txt\\") safe_process(\\"critical_issue.txt\\") safe_process(\\"general_issue.txt\\") safe_process(\\"good_file.txt\\") ``` Implement all the functions and exceptions as specified and ensure your testing cases are thorough.","solution":"class FileFormatError(Exception): Raised when a file has an incorrect format. pass class ProcessingError(Exception): Raised for general processing errors. pass class CriticalError(ProcessingError): Raised when a critical error occurs. pass def process_file(file_name: str) -> None: Simulates reading from a file and processes its content. Raises FileFormatError, ProcessingError, or CriticalError as needed. if file_name == \\"bad_format.txt\\": raise FileFormatError(\\"Incorrect file format.\\") elif file_name == \\"critical_issue.txt\\": try: # Simulate some processing that fails critically raise ValueError(\\"Critical internal issue.\\") except ValueError as e: raise CriticalError(\\"A critical processing error occurred.\\") from e elif file_name == \\"general_issue.txt\\": raise ProcessingError(\\"General processing error.\\") else: print(f\\"Processing {file_name} successfully completed.\\") def safe_process(file_name: str) -> None: Handles exceptions raised by process_file and prints appropriate error messages. try: process_file(file_name) except FileFormatError as e: print(f\\"Error: {e}\\") except ProcessingError as e: print(f\\"Error: {e}. Cause: {e.__cause__}\\") except Exception as e: print(f\\"Unexpected error: {e}\\") # Example usage # safe_process(\\"bad_format.txt\\") # safe_process(\\"critical_issue.txt\\") # safe_process(\\"general_issue.txt\\") # safe_process(\\"good_file.txt\\")"},{"question":"# Question: You are required to create a custom callable Python class `CustomCallable` that adheres to both `tp_call` and `vectorcall` protocols as described in the `python310` package documentation. This class should be able to handle both positional and keyword arguments correctly. To demonstrate your understanding, implement the `CustomCallable` class such that: 1. It implements `tp_call` to support positional arguments passed as a tuple and keyword arguments passed as a dictionary. 2. It also implements `vectorcall` for efficient internal calls using a vectorcall function. You need to: - Provide proper implementation of both protocols. - Ensure that the callable functionalities demonstrate different types of calls (no arguments, one argument, multiple positional arguments, and keyword arguments). - Use relevant safety checks as detailed in recursion control for each protocol. Requirements: 1. Implement `tp_call` protocol to support calling instances of the class with positional and keyword arguments. 2. Implement `vectorcall` protocol ensuring efficient call handling using vectorcallfunc. 3. Demonstrate the callable functionalities via a series of test cases at the end, including: - No arguments call - Single positional argument - Multiple positional arguments - Keyword arguments # Expected input and output format: - Input: - CustomCallable object upon instantiation should accept any initial set of configuration but callable invocation should demonstrate different types of argument passing. - Output: - Correct handling and output of given arguments when the callable object is invoked. # Constraints: 1. Follow the CPython conventions for both protocols. 2. Ensure that your class handles recursion and potential errors gracefully. 3. Demonstrate performance efficiency where vectorcall should offer better performance when applicable. Start with the main implementation, then provide explanatory comments or docstrings to clarify the handling of protocols and calling mechanisms for the class. This assessment is challenging as it tests both fundamental understanding and advanced implementation skills involving callable protocols in Python. # Example: ```python class CustomCallable: def __init__(self, config=None): # Initialization with optional configuration # Implementation of tp_call protocol def tp_call(self, args, kwargs): # Handle callable with positional and keyword arguments # Implementation of vectorcall protocol def vectorcall(self, args, nargsf, kwnames): # Handle callable efficiently using vectorcallfunc # Example Test Cases cc = CustomCallable(config={\'example\': \'value\'}) # Test tp_call with no arguments result = cc() # Expected result based on defined handling # Test vectorcall with single positional argument result = cc(\\"single_arg\\") # Expected result # Test tp_call with multiple positional arguments result = cc(1, 2, 3) # Expected result # Test vectorcall with keyword arguments result = cc(arg1=1, arg2=2) # Expected result # More comprehensive tests as needed ``` Ensure your class and test cases comprehensively demonstrate the working of both protocols.","solution":"import functools class CustomCallable: def __init__(self, config=None): self.config = config def __call__(self, *args, **kwargs): # This method handles both tp_call and vectorcall protocols in Python self.tp_call(args, kwargs) def tp_call(self, args, kwargs): Handles callable with tuple of positional arguments and dictionary of keyword arguments. print(\\"tp_call invoked\\") print(f\\"Positional args: {args}\\") print(f\\"Keyword args: {kwargs}\\") def vectorcall(self, *args, **kwargs): Handles callable efficiently using vectorcall function which is a way to call functions more efficiently by avoiding the creation of argument tuple and dictionary. print(\\"vectorcall invoked\\") print(f\\"Arguments: {args}\\") if kwargs: print(f\\"Keyword args: {kwargs}\\") else: print(\\"No keyword arguments\\") # Example of usage: cc = CustomCallable(config={\'example\': \'value\'}) # Test tp_call with no arguments cc() # Test vectorcall with single positional argument cc.vectorcall(\\"single_arg\\") # Test tp_call with multiple positional arguments cc(1, 2, 3) # Test vectorcall with keyword arguments cc.vectorcall(arg1=1, arg2=2)"},{"question":"# Covariance Estimation and Comparison In this assessment, you will implement a covariance estimation pipeline using scikit-learn, applying various covariance estimation techniques and comparing their performance. You are provided with a synthetic dataset for this task. Dataset You will generate synthetic data using the following steps: 1. Create a 100x5 matrix of random samples from a normal distribution. 2. Introduce a few outliers to the dataset. Tasks 1. **Generate Synthetic Data**: - Generate a 100x5 matrix of random samples from a multivariate normal distribution with a mean of 0 and a standard deviation of 1. - Introduce 5 outliers to the dataset by adding a large number (e.g., 10) to randomly chosen entries. 2. **Covariance Estimation**: - Compute the covariance matrix using the _Empirical Covariance_ method. - Compute the covariance matrix using the _Ledoit-Wolf Shrinkage_ method. - Compute the covariance matrix using the _Oracle Approximating Shrinkage (OAS)_ method. - Compute the covariance matrix using the _Minimum Covariance Determinant (MCD)_ method to handle outliers. 3. **Comparison**: - Compute the Mahalanobis distance of each point in the dataset with respect to each covariance estimator. - Plot these distances to visualize and compare how outliers affect each estimator. You may use any plotting library like matplotlib. Implementation ```python import numpy as np import matplotlib.pyplot as plt from sklearn.covariance import (EmpiricalCovariance, LedoitWolf, OAS, MinCovDet, empirical_covariance, ledoit_wolf, oas) def generate_data(): # Generate random data np.random.seed(42) data = np.random.randn(100, 5) # Introduce outliers outliers = np.random.choice(np.arange(100*5), 5, replace=False) data = data.flatten() data[outliers] += 10 data = data.reshape(100, 5) return data def plot_mahalanobis_distances(data, distances, title): Plots the Mahalanobis distances for comparison plt.figure() for label, dist in distances.items(): plt.plot(dist, label=label) plt.title(title) plt.xlabel(\\"Sample Index\\") plt.ylabel(\\"Mahalanobis Distance\\") plt.legend() plt.show() def main(): # Generate the data data = generate_data() # Create covariance estimation models emp_cov = EmpiricalCovariance().fit(data) lw_cov = LedoitWolf().fit(data) oas_cov = OAS().fit(data) mcd_cov = MinCovDet().fit(data) # Compute Mahalanobis distances emp_dist = emp_cov.mahalanobis(data) lw_dist = lw_cov.mahalanobis(data) oas_dist = oas_cov.mahalanobis(data) mcd_dist = mcd_cov.mahalanobis(data) # Collect distances in a dictionary for plotting distances = { \\"Empirical Covariance\\": emp_dist, \\"Ledoit-Wolf\\": lw_dist, \\"OAS\\": oas_dist, \\"MCD\\": mcd_dist } # Plot the Mahalanobis distances plot_mahalanobis_distances(data, distances, \\"Mahalanobis Distances Comparison\\") if __name__ == \\"__main__\\": main() ``` Requirements: - **Input**: The provided code does not require any input as the data is generated within the function. - **Output**: Visualize the Mahalanobis distances and analyze the plot to understand the effect of outliers on different covariance estimators. Constraints: - Use only scikit-learn for covariance estimation. - Use matplotlib for plotting. - Make sure to properly handle any potential issues with random state for reproducibility.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.covariance import EmpiricalCovariance, LedoitWolf, OAS, MinCovDet def generate_data(): # Generate random data np.random.seed(42) data = np.random.randn(100, 5) # Introduce outliers outliers = np.random.choice(np.arange(100*5), 5, replace=False) data = data.flatten() data[outliers] += 10 data = data.reshape(100, 5) return data def plot_mahalanobis_distances(data, distances, title): Plots the Mahalanobis distances for comparison plt.figure() for label, dist in distances.items(): plt.plot(dist, label=label) plt.title(title) plt.xlabel(\\"Sample Index\\") plt.ylabel(\\"Mahalanobis Distance\\") plt.legend() plt.show() def main(): # Generate the data data = generate_data() # Create covariance estimation models emp_cov = EmpiricalCovariance().fit(data) lw_cov = LedoitWolf().fit(data) oas_cov = OAS().fit(data) mcd_cov = MinCovDet().fit(data) # Compute Mahalanobis distances emp_dist = emp_cov.mahalanobis(data) lw_dist = lw_cov.mahalanobis(data) oas_dist = oas_cov.mahalanobis(data) mcd_dist = mcd_cov.mahalanobis(data) # Collect distances in a dictionary for plotting distances = { \\"Empirical Covariance\\": emp_dist, \\"Ledoit-Wolf\\": lw_dist, \\"OAS\\": oas_dist, \\"MCD\\": mcd_dist } # Plot the Mahalanobis distances plot_mahalanobis_distances(data, distances, \\"Mahalanobis Distances Comparison\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Implement a classification task using `sklearn.neighbors` and compare the performance of different nearest neighbor algorithms. **Problem Statement**: You are provided with a dataset that contains labeled points in a 2D space. Your task is to implement a nearest neighbor classifier using two different algorithms: `KDTree` and `BallTree`. You need to: 1. Load and preprocess the dataset. 2. Implement the nearest neighbor classification. 3. Evaluate the performance of these algorithms. 4. Compare their performance, elaborating on time complexity and accuracy. **Specifications**: 1. **Input**: - A CSV file named `data.csv` with three columns: `x1`, `x2`, and `label`. 2. **Output**: - Print the accuracy of the classification for each algorithm. - Print the time taken to fit and predict the data using each algorithm. **Constraints**: - Use a fixed `k` value of 3 for k-nearest neighbors. - Assume that the dataset is sufficiently large to show meaningful differences in performance. **Implementation Details**: 1. **Data Loading**: - Load the dataset using pandas. - Split the dataset into a training set (70%) and a test set (30%). 2. **Algorithm Implementation**: - **KDTree**: - Use `sklearn.neighbors.KDTree` for nearest neighbors classification. - **BallTree**: - Use `sklearn.neighbors.BallTree` for nearest neighbors classification. 3. **Evaluation**: - Measure the accuracy of the classifier on the test set. - Measure the time taken to fit the model and the time taken to predict using both KDTree and BallTree. 4. **Comparison**: - Compare the accuracy and time complexity of fitting and predicting using KDTree and BallTree. - Discuss any observed differences in performance and suggest possible reasons based on the characteristics of the dataset and algorithms. **Example Code Structure**: ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.neighbors import KDTree, BallTree from sklearn.metrics import accuracy_score import time # Step 1: Load and preprocess the dataset data = pd.read_csv(\'data.csv\') X = data[[\'x1\', \'x2\']].values y = data[\'label\'].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 2: Implement KDTree classification start_time = time.time() kdtree = KDTree(X_train, leaf_size=30, metric=\'euclidean\') kd_neighbors = kdtree.query(X_test, k=3, return_distance=False) kd_predictions = [round(y_train[i].mean()) for i in kd_neighbors] kd_accuracy = accuracy_score(y_test, kd_predictions) kd_time = time.time() - start_time # Step 3: Implement BallTree classification start_time = time.time() balltree = BallTree(X_train, leaf_size=30, metric=\'euclidean\') ball_neighbors = balltree.query(X_test, k=3, return_distance=False) ball_predictions = [round(y_train[i].mean()) for i in ball_neighbors] ball_accuracy = accuracy_score(y_test, ball_predictions) ball_time = time.time() - start_time # Step 4: Compare performance print(f\\"KDTree Accuracy: {kd_accuracy}, Time: {kd_time}s\\") print(f\\"BallTree Accuracy: {ball_accuracy}, Time: {ball_time}s\\") ``` **Deliverables**: - Python script implementing the above tasks. - A detailed report comparing the performance of KDTree and BallTree, providing insights into their time complexity and accuracy.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.neighbors import KDTree, BallTree from sklearn.metrics import accuracy_score import time def load_and_preprocess_data(filepath): data = pd.read_csv(filepath) X = data[[\'x1\', \'x2\']].values y = data[\'label\'].values return train_test_split(X, y, test_size=0.3, random_state=42) def knn_kdtree(X_train, y_train, X_test, k=3): kdtree = KDTree(X_train, leaf_size=30, metric=\'euclidean\') kd_neighbors = kdtree.query(X_test, k=k, return_distance=False) kd_predictions = [y_train[neighbors].mean() > 0.5 for neighbors in kd_neighbors] return kd_predictions def knn_balltree(X_train, y_train, X_test, k=3): balltree = BallTree(X_train, leaf_size=30, metric=\'euclidean\') ball_neighbors = balltree.query(X_test, k=k, return_distance=False) ball_predictions = [y_train[neighbors].mean() > 0.5 for neighbors in ball_neighbors] return ball_predictions def evaluate_knn(filepath): X_train, X_test, y_train, y_test = load_and_preprocess_data(filepath) # KDTree Classification start_time = time.time() kd_predictions = knn_kdtree(X_train, y_train, X_test) kd_accuracy = accuracy_score(y_test, kd_predictions) kd_time = time.time() - start_time # BallTree Classification start_time = time.time() ball_predictions = knn_balltree(X_train, y_train, X_test) ball_accuracy = accuracy_score(y_test, ball_predictions) ball_time = time.time() - start_time return { \\"kd_accuracy\\": kd_accuracy, \\"kd_time\\": kd_time, \\"ball_accuracy\\": ball_accuracy, \\"ball_time\\": ball_time }"},{"question":"You are tasked with creating a Python C extension module using the Python C API that provides a function to manipulate and return information about a list according to the user\'s specifications. Function Specification: Create a function `manipulate_list(Py_ssize_t n, PyObject *operations)` that: 1. Creates a new list of size `n`. 2. Initializes the list with zeros. 3. Accepts a list of operations (Python list) where each operation is specified as a dictionary. The operations should be performed in the order they appear in the list. 4. Handles the following operations: * \\"append\\": Adds an element to the end of the list. * \\"insert\\": Inserts an element at a specified position. * \\"set_item\\": Sets the value at a specified index. * \\"get_item\\": Retrieves the value at a specified index. * \\"reverse\\": Reverses the list. * \\"sort\\": Sorts the list. * \\"size\\": Returns the size of the list. 5. Returns a dictionary containing the results of the \\"get_item\\" and \\"size\\" operations. Input and Output Format: - `n`: a non-negative integer representing the initial size of the list. - `operations`: a list of dictionaries, where each dictionary has the structure defined above. - The function returns a dictionary containing the results of the \\"get_item(s)\\" and \\"size\\" operations. Example Usage: Suppose `manipulate_list` is implemented and exposed via the C extension module `listmanip`. Below is an example of how it would be invoked from Python: ```python import listmanip operations = [ {\\"operation\\": \\"set_item\\", \\"index\\": 1, \\"value\\": 10}, {\\"operation\\": \\"append\\", \\"value\\": 5}, {\\"operation\\": \\"insert\\", \\"index\\": 0, \\"value\\": 3}, {\\"operation\\": \\"get_item\\", \\"index\\": 2}, {\\"operation\\": \\"reverse\\"}, {\\"operation\\": \\"sort\\"}, {\\"operation\\": \\"size\\"} ] result = listmanip.manipulate_list(3, operations) print(result) # Should output something like {\'get_item\': [5], \'size\': 4} ``` Constraints: 1. The provided size `n` must be a non-negative integer. 2. The indices for \\"set_item\\", \\"insert\\", and \\"get_item\\" operations must be within the bounds of the list. 3. If an operation is invalid or fails, the function should raise an appropriate Python exception. Performance Requirements: The function should be efficient in terms of both time and space complexity, leveraging the capabilities of the Python C API for optimal performance.","solution":"def manipulate_list(n, operations): Creates and manipulates a list according to the specified operations. Parameters: n (int): The initial size of the list, populated with zeros. operations (list): List of dictionaries specifying operations. Returns: dict: Results of \'get_item\' and \'size\' operations. if not isinstance(n, int) or n < 0: raise ValueError(\\"The size \'n\' must be a non-negative integer.\\") lst = [0] * n result = {\'get_item\': [], \'size\': None} for operation in operations: op = operation[\'operation\'] if op == \'append\': lst.append(operation[\'value\']) elif op == \'insert\': index = operation[\'index\'] if not (0 <= index <= len(lst)): raise IndexError(\\"Index out of bounds for \'insert\' operation.\\") lst.insert(index, operation[\'value\']) elif op == \'set_item\': index = operation[\'index\'] if not (0 <= index < len(lst)): raise IndexError(\\"Index out of bounds for \'set_item\' operation.\\") lst[index] = operation[\'value\'] elif op == \'get_item\': index = operation[\'index\'] if not (0 <= index < len(lst)): raise IndexError(\\"Index out of bounds for \'get_item\' operation.\\") result[\'get_item\'].append(lst[index]) elif op == \'reverse\': lst.reverse() elif op == \'sort\': lst.sort() elif op == \'size\': result[\'size\'] = len(lst) else: raise ValueError(f\\"Invalid operation: {op}\\") return result"},{"question":"Objective The task is to create a custom extension for `pandas` Series that allows easy manipulation and analysis of telephone numbers. Telephone numbers will be stored as a custom data type `PhoneNumber` within a `pandas.Series`, which will support standard operations such as validation, formatting, and extracting components (e.g., country code, area code). Specifications 1. **Custom Data Type (`PhoneNumberDtype`)**: - Implement an extension dtype `PhoneNumberDtype` derived from `pandas.api.extensions.ExtensionDtype`. - The `PhoneNumberDtype` should represent a telephone number string, e.g., \\"+1-234-567-8901\\". 2. **Custom Extension Array (`PhoneNumberArray`)**: - Implement an extension array `PhoneNumberArray` derived from `pandas.api.extensions.ExtensionArray`. - This array should store and manage `PhoneNumber` objects. 3. **Accessor (`PhoneNumberAccessor`)**: - Implement a custom accessor for `pandas.Series` to expose methods specific to `PhoneNumberArray`. - Methods to include: - `validate`: Check if all phone numbers in the Series are valid. - `format`: Return phone numbers formatted uniformly. - `extract_country_code`: Return the country code of each phone number. 4. **Implementation Requirements**: - **Input Format**: A `pandas.Series` containing phone number strings. - **Output Format**: - `validate`: Boolean indicating whether all phone numbers are valid. - `format`: A `pandas.Series` with uniformly formatted phone numbers. - `extract_country_code`: A `pandas.Series` with country codes extracted from phone numbers. - **Constraints**: Assume that phone numbers follow the format `\\"+<country_code>-<area_code>-<local_number>\\"`. ```python import pandas as pd from pandas.api.extensions import ExtensionDtype, ExtensionArray, register_series_accessor class PhoneNumberDtype(ExtensionDtype): # Define the class, ensure it extends ExtensionDtype ... class PhoneNumberArray(ExtensionArray): # Define the class, ensure it extends ExtensionArray ... @register_series_accessor(\\"phone\\") class PhoneNumberAccessor: def __init__(self, pandas_obj): self._obj = pandas_obj def validate(self): Validate the series contains only valid phone numbers. ... def format(self): Format the phone numbers uniformly. ... def extract_country_code(self): Extract and return the country code for each phone number. ... # Example Usage: if __name__ == \\"__main__\\": # Example data data = pd.Series([\\"+1-234-567-8901\\", \\"+44-123-456-7890\\"]) # Convert series to PhoneNumber dtype (implementation required) phone_series = data.astype(\'phone_number\') # Validation of phone numbers is_valid = phone_series.phone.validate() print(f\\"Phone numbers are valid: {is_valid}\\") # Formatting phone numbers formatted_series = phone_series.phone.format() print(formatted_series) # Extracting country codes country_codes = phone_series.phone.extract_country_code() print(country_codes) ``` **Note**: This exercise requires you to fully implement `PhoneNumberDtype`, `PhoneNumberArray`, and the relevant methods in `PhoneNumberAccessor` to handle phone number data in a `pandas` Series efficiently. Hints - Refer to the `pandas` extension documentation for detailed guidance on creating custom data types and arrays. - Use regular expressions for phone number validation and component extraction. - Ensure thorough testing of each method using various phone number formats.","solution":"import re import pandas as pd from pandas.api.extensions import ExtensionDtype, ExtensionArray, register_extension_dtype, register_series_accessor import numpy as np # Custom Data Type for PhoneNumber class PhoneNumberDtype(ExtensionDtype): name = \'phone_number\' type = str kind = \'O\' na_value = np.nan @classmethod def construct_array_type(cls): return PhoneNumberArray register_extension_dtype(PhoneNumberDtype) # Custom Extension Array for PhoneNumber class PhoneNumberArray(ExtensionArray): def __init__(self, phone_numbers): self.data = pd.Series(phone_numbers, dtype=\'object\') def __getitem__(self, item): return self.data.iloc[item] def __len__(self): return len(self.data) def __setitem__(self, key, value): self.data.iloc[key] = value @property def dtype(self): return PhoneNumberDtype() def isna(self): return self.data.isna() def take(self, indices, allow_fill=False, fill_value=None): new_data = self.data.take(indices) return PhoneNumberArray(new_data) def copy(self): return PhoneNumberArray(self.data.copy()) @register_series_accessor(\\"phone\\") class PhoneNumberAccessor: def __init__(self, pandas_obj): self._obj = pandas_obj def validate(self): Validate the series contains only valid phone numbers. valid_phone_number = re.compile(r\'^+d{1,3}-d{1,3}-d{3}-d{4}\') return self._obj.apply(lambda x: bool(valid_phone_number.match(x))).all() def format(self): Format the phone numbers uniformly. formatted_phone_number = re.compile(r\'^+(d{1,3})-(d{1,3})-(d{3})-(d{4})\') return self._obj.apply(lambda x: formatted_phone_number.sub(r\'+1-2-3-4\', x)) def extract_country_code(self): Extract and return the country code for each phone number. country_code_extraction = re.compile(r\'^+(d{1,3})-\') return self._obj.apply(lambda x: country_code_extraction.match(x).group(1) if country_code_extraction.match(x) else np.nan) # Example Usage if __name__ == \\"__main__\\": # Example data data = pd.Series([\\"+1-234-567-8901\\", \\"+44-123-456-7890\\"]) # Convert series to PhoneNumber dtype phone_series = pd.Series(PhoneNumberArray(data.to_list())) # Validation of phone numbers is_valid = phone_series.phone.validate() print(f\\"Phone numbers are valid: {is_valid}\\") # Formatting phone numbers formatted_series = phone_series.phone.format() print(formatted_series) # Extracting country codes country_codes = phone_series.phone.extract_country_code() print(country_codes)"},{"question":"Mailcap Utility Function You are tasked with implementing a utility function for handling Mailcap files. This function will help retrieve commands based on MIME types and specific actions. **Function:** ```python def get_mailcap_command(MIMEtype: str, action: str = \'view\', filename: str = \'/dev/null\', param_list: list = []) -> tuple: Finds and returns the mailcap command line and corresponding entry for a given MIME type and action. Args: MIMEtype (str): The MIME type to look up. action (str): The action type (\'view\', \'compose\', \'edit\', etc.). Default is \'view\'. filename (str): The filename to be substituted in the command. Default is \'/dev/null\'. param_list (list): A list of named parameters in the form \'name=value\'. Default is an empty list. Returns: tuple: A 2-tuple where the first element is the command line to be executed, and the second element is the mailcap entry. Returns (None, None) if no match is found or if invalid characters are present. # Your implementation here ``` Requirements: 1. **File Handling:** Use the `mailcap.getcaps()` function to retrieve all the mailcap entries on the system. 2. **Command Matching:** Use the `mailcap.findmatch()` function to find the appropriate command line for the given MIME type and action. 3. **Parameter Substitution:** Handle parameter substitution in the command line using the `param_list` argument. 4. **Security Checks:** Ensure that no disallowed characters are injected into the command line from the filename or param_list. 5. **Fallback:** Provide a fallback mechanism where the function returns `(None, None)` if no suitable match is found or if security checks fail. **Examples:** ```python # Example 1: Basic usage cmd, entry = get_mailcap_command(\'video/mpeg\', filename=\'video1.mpg\') print(cmd) # Output might be something like: \'xmpeg video1.mpg\' print(entry) # Output will be the corresponding mailcap entry dictionary # Example 2: With parameters cmd, entry = get_mailcap_command(\'application/pdf\', action=\'edit\', filename=\'document.pdf\', param_list=[\'user=jdoe\', \'mode=light\']) print(cmd) # Output might reflect substitutions such as: \'pdfeditor document.pdf user=jdoe mode=light\' print(entry) # Output will be the corresponding mailcap entry dictionary # Example 3: No match found cmd, entry = get_mailcap_command(\'audio/unknown\', filename=\'audiofile.aud\') print(cmd) # Output will be: None print(entry) # Output will be: None ``` Constraints: - The MIME type, action, filename, and parameters should only contain alphanumeric characters and the following symbols: \\"@+=:,./-_\\". If any other characters are present, the function should return `(None, None)`. - Assume the `mailcap` module functions as described and is available in the environment. You are required to implement the `get_mailcap_command` function that fulfills the above criteria.","solution":"import mailcap import re def get_mailcap_command(MIMEtype: str, action: str = \'view\', filename: str = \'/dev/null\', param_list: list = []) -> tuple: Finds and returns the mailcap command line and corresponding entry for a given MIME type and action. Args: MIMEtype (str): The MIME type to look up. action (str): The action type (\'view\', \'compose\', \'edit\', etc.). Default is \'view\'. filename (str): The filename to be substituted in the command. Default is \'/dev/null\'. param_list (list): A list of named parameters in the form \'name=value\'. Default is an empty list. Returns: tuple: A 2-tuple where the first element is the command line to be executed, and the second element is the mailcap entry. Returns (None, None) if no match is found or if invalid characters are present. # Allowed characters in MIMEtype, action, filename, and parameters allowed_chars = re.compile(r\'^[w@+=:,./_-]+\') # Security check for allowed characters if not (allowed_chars.match(MIMEtype) and allowed_chars.match(action) and allowed_chars.match(filename)): return (None, None) for param in param_list: if not allowed_chars.match(param): return (None, None) caps = mailcap.getcaps() filtered_params = {k: v for param in param_list for k, v in [param.split(\'=\')]} command, entry = mailcap.findmatch(caps, MIMEtype, action, filename=filename, plist=filtered_params) return (command, entry)"},{"question":"Objective To assess your understanding of the Python `gettext` module for internationalization and localization by implementing a class that can dynamically switch languages based on user input and translate provided strings. Problem Statement You are tasked with creating a Python class `LanguageTranslator` that utilizes the `gettext` module to support dynamic language switching and string translation. Your class should be able to: 1. Load translations for multiple languages from `.mo` files located in the specified directory. 2. Switch the active language based on user input. 3. Translate messages using the active language. Implementation Details 1. **Initialization**: - The class should be initialized with a domain name and the path to the directory containing the translation `.mo` files. - It should preload the translation catalogs for multiple languages. 2. **Methods**: - `switch_language(self, language_code)`: Switches the active language to the one specified by `language_code`. - `translate(self, message_id)`: Translates the given message ID using the currently active language. 3. **Example**: ```python # Your translation files are stored in /path/to/translations and the domain is \'myapp\' translator = LanguageTranslator(\'myapp\', \'/path/to/translations\') translator.switch_language(\'es\') # Switch to Spanish print(translator.translate(\'hello_world\')) # Should print \'Hola Mundo\' if this translation exists in the Spanish .mo file translator.switch_language(\'fr\') # Switch to French print(translator.translate(\'hello_world\')) # Should print \'Bonjour le monde\' if this translation exists in the French .mo file ``` Input - The `language_code` for the `switch_language` method is a string representing the language code (e.g., \'en\', \'es\', \'fr\'). - The `message_id` for the `translate` method is a string representing the message ID that needs to be translated. Output - The `translate` method should return the translated string based on the active language. Constraints - Assume at least English (\'en\') is always available in the translations. - If a translation for a message ID does not exist in the active language, return the message ID itself. Requirements - Do not use global namespace changes; keep everything within the class scope. - This is a simplified use case, so complex plurals or context-based translations are not required. Performance - Ensure that loading and switching languages is efficient. Code Template Here is a starter template for your class: ```python import gettext import os class LanguageTranslator: def __init__(self, domain, localedir): self.domain = domain self.localedir = localedir self.translations = {} self.current_language = \'en\' self._load_translations() def _load_translations(self): languages = [name for name in os.listdir(self.localedir) if os.path.isdir(os.path.join(self.localedir, name))] for language in languages: translation = gettext.translation(self.domain, localedir=self.localedir, languages=[language], fallback=True) self.translations[language] = translation def switch_language(self, language_code): if language_code in self.translations: self.current_language = language_code else: raise ValueError(f\\"Language {language_code} not found\\") def translate(self, message_id): return self.translations[self.current_language].gettext(message_id) ``` Complete the class to meet the requirements of the problem statement.","solution":"import gettext import os class LanguageTranslator: def __init__(self, domain, localedir): self.domain = domain self.localedir = localedir self.translations = {} self.current_language = \'en\' self._load_translations() def _load_translations(self): Loads translation catalogs for multiple languages from the specified directory. languages = [name for name in os.listdir(self.localedir) if os.path.isdir(os.path.join(self.localedir, name))] for language in languages: translation = gettext.translation(self.domain, localedir=self.localedir, languages=[language], fallback=True) self.translations[language] = translation def switch_language(self, language_code): Switches the active language to the one specified by language_code. if language_code in self.translations: self.current_language = language_code else: raise ValueError(f\\"Language {language_code} not found\\") def translate(self, message_id): Translates the given message ID using the currently active language. return self.translations[self.current_language].gettext(message_id)"},{"question":"Objective Assess the understanding of the `pickle` module\'s serialization and de-serialization features, including custom serialization for complex objects. Problem Statement You are tasked with implementing a custom serialization and de-serialization mechanism for a class that represents a mutable collection of items. This class will have its custom methods for pickling and unpickling to handle both basic and advanced serialization scenarios, including handling of persistent state and custom reducers. Class Definition Define a class `CustomCollection` that: 1. Stores a list of items. 2. Tracks the version of the collection. 3. Implements a custom pickling mechanism using the `pickle` module. Requirements: 1. **Class Attributes**: - `items`: A list of items in the collection. - `version`: An integer representing the version of the collection. 2. **Methods**: - `__init__(self, items: list, version: int = 1)`: Initialize the collection with the given list of items and a version number. - `add_item(self, item)`: Add an item to the collection. - `__getstate__(self)`: Define how the object\'s state is pickled. - `__setstate__(self, state)`: Define how the object\'s state is restored during unpickling. - `__reduce__(self)`: Define a custom reducer for the object. 3. **Pickle Functions**: - `pickle_data(obj, file_path)`: Pickles the `CustomCollection` object to the specified file. - `unpickle_data(file_path)`: Unpickles the `CustomCollection` object from the specified file. Constraints: - The `items` list should be pickled and unpickled correctly. - The `version` attribute should also be included in the pickling process. Input and Output: - **Input**: - Operations to add items to the collection and update its state. - A file path to save the pickled data. - **Output**: - Serialized and deserialized objects maintain their state correctly. Example Usage: ```python collection = CustomCollection(items=[\'item1\', \'item2\']) collection.add_item(\'item3\') pickle_data(collection, \'collection.pkl\') # Later in the program or in another program restored_collection = unpickle_data(\'collection.pkl\') assert restored_collection.items == [\'item1\', \'item2\', \'item3\'] assert restored_collection.version == 1 ``` # Implementation Write the implementation for the following tasks: 1. Define the `CustomCollection` class with the required methods. 2. Implement the `pickle_data` and `unpickle_data` functions. ```python import pickle class CustomCollection: def __init__(self, items: list, version: int = 1): self.items = items self.version = version def add_item(self, item): self.items.append(item) def __getstate__(self): # Return the state of the object that should be pickled state = self.__dict__.copy() return state def __setstate__(self, state): # Restore the state from the unpickled data self.__dict__.update(state) def __reduce__(self): # Provide custom reduction for the object return (self.__class__, (self.items, self.version)) def pickle_data(obj, file_path): with open(file_path, \'wb\') as file: pickle.dump(obj, file) def unpickle_data(file_path): with open(file_path, \'rb\') as file: return pickle.load(file) # Example usage if __name__ == \'__main__\': # Creating and pickling the collection collection = CustomCollection(items=[\'item1\', \'item2\']) collection.add_item(\'item3\') pickle_data(collection, \'collection.pkl\') # Unpickling the collection restored_collection = unpickle_data(\'collection.pkl\') assert restored_collection.items == [\'item1\', \'item2\', \'item3\'] assert restored_collection.version == 1 print(\\"Serialization and Deserialization successful!\\") ``` # Evaluation Criteria 1. Correctness: The code should correctly serialize and deserialize the `CustomCollection` object, preserving its state. 2. Consistency: The restored object\'s state should match exactly with the initial state. 3. Handling Errors: Proper handling of potential errors during pickling and unpickling. 4. Code Structure: The implementation should be clean, maintainable, and modular.","solution":"import pickle class CustomCollection: def __init__(self, items: list, version: int = 1): self.items = items self.version = version def add_item(self, item): self.items.append(item) def __getstate__(self): # Return the state of the object that should be pickled state = self.__dict__.copy() return state def __setstate__(self, state): # Restore the state from the unpickled data self.__dict__.update(state) def __reduce__(self): # Provide custom reduction for the object return (self.__class__, (self.items, self.version)) def pickle_data(obj, file_path): with open(file_path, \'wb\') as file: pickle.dump(obj, file) def unpickle_data(file_path): with open(file_path, \'rb\') as file: return pickle.load(file)"},{"question":"You are required to design a utility class `BZ2FileProcessor` that leverages the `bz2` module to handle both compression and decompression of text data. This class should support the following operations: 1. **Compression**: - Compress a given text file and save it as a `.bz2` file. 2. **Decompression**: - Decompress a given `.bz2` file and output the content as a text file. 3. **Incremental Compression**: - Simulate reading a large text file in chunks, compressing each chunk incrementally, and saving the result as a `.bz2` file. 4. **Incremental Decompression**: - Read a `.bz2` file incrementally, decompressing each chunk and reconstructing the original file. Your task is to implement the `BZ2FileProcessor` class with the following methods: Methods: 1. `compress_file(input_file: str, output_file: str, compresslevel: int = 9) -> None`: - **Description**: Compress the content of `input_file` and save it to `output_file.bz2`. - **Args**: - `input_file` (str): Path to the input text file. - `output_file` (str): Path where the compressed file will be saved. - `compresslevel` (int, optional): Compression level (1 to 9). Default is 9. 2. `decompress_file(input_file: str, output_file: str) -> None`: - **Description**: Decompress the content of `input_file.bz2` and save it to `output_file`. - **Args**: - `input_file` (str): Path to the input compressed file. - `output_file` (str): Path where the decompressed file will be saved. 3. `incremental_compress(input_file: str, output_file: str, compresslevel: int = 9, chunk_size: int = 1024) -> None`: - **Description**: Compress the content of `input_file` in chunks and save incrementally to `output_file.bz2`. - **Args**: - `input_file` (str): Path to the input text file. - `output_file` (str): Path where the compressed file will be saved. - `compresslevel` (int, optional): Compression level (1 to 9). Default is 9. - `chunk_size` (int, optional): Size of the chunks to read from the file. Default is 1024 bytes. 4. `incremental_decompress(input_file: str, output_file: str, chunk_size: int = 1024) -> None`: - **Description**: Decompress the content of `input_file.bz2` in chunks and reconstruct the original file to `output_file`. - **Args**: - `input_file` (str): Path to the input compressed file. - `output_file` (str): Path where the decompressed file will be saved. - `chunk_size` (int, optional): Size of the chunks to read and decompress. Default is 1024 bytes. Example Usage ```python # compress a file processor = BZ2FileProcessor() processor.compress_file(\\"example.txt\\", \\"example_compressed.bz2\\") # decompress a file processor.decompress_file(\\"example_compressed.bz2\\", \\"example_decompressed.txt\\") # incremental compress a file processor.incremental_compress(\\"large_example.txt\\", \\"large_example_compressed.bz2\\") # incremental decompress a file processor.incremental_decompress(\\"large_example_compressed.bz2\\", \\"large_example_decompressed.txt\\") ``` Constraints - Assume the input and output file paths are valid. - All file operations should handle exceptions gracefully and provide meaningful error messages. Implement the `BZ2FileProcessor` class in Python.","solution":"import bz2 import os class BZ2FileProcessor: @staticmethod def compress_file(input_file: str, output_file: str, compresslevel: int = 9) -> None: try: with open(input_file, \'rb\') as f_in, bz2.BZ2File(output_file, \'wb\', compresslevel=compresslevel) as f_out: f_out.write(f_in.read()) except Exception as e: print(f\\"An error occurred during compression: {e}\\") @staticmethod def decompress_file(input_file: str, output_file: str) -> None: try: with bz2.BZ2File(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: f_out.write(f_in.read()) except Exception as e: print(f\\"An error occurred during decompression: {e}\\") @staticmethod def incremental_compress(input_file: str, output_file: str, compresslevel: int = 9, chunk_size: int = 1024) -> None: try: with open(input_file, \'rb\') as f_in, bz2.BZ2File(output_file, \'wb\', compresslevel=compresslevel) as f_out: while True: data = f_in.read(chunk_size) if not data: break f_out.write(data) except Exception as e: print(f\\"An error occurred during incremental compression: {e}\\") @staticmethod def incremental_decompress(input_file: str, output_file: str, chunk_size: int = 1024) -> None: try: with bz2.BZ2File(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: while True: data = f_in.read(chunk_size) if not data: break f_out.write(data) except Exception as e: print(f\\"An error occurred during incremental decompression: {e}\\")"},{"question":"Objective: Design a machine-learning pipeline using `scikit-learn` to classify the iris flowers dataset. This task will assess your understanding of data loading, preprocessing, model training, and evaluation using scikit-learn. Required Function: You will need to implement the function `classify_iris_flowers`. ```python def classify_iris_flowers(): Loads the iris dataset, performs preprocessing, trains a logistic regression classifier, and evaluates the classifier using accuracy score and confusion matrix. Returns: accuracy (float): The accuracy score of the classifier on the test set. conf_matrix (ndarray): The confusion matrix of the classifier\'s predictions. ``` # Details: 1. **Loading the Dataset**: - Use the `load_iris` function from `sklearn.datasets` to load the iris dataset. 2. **Data Preparation**: - Split the dataset into training and testing sets using an 80-20 split. You can use `train_test_split` from `sklearn.model_selection`. 3. **Model Training**: - Train a `LogisticRegression` model using the training data. Use `LogisticRegression` from `sklearn.linear_model`. 4. **Model Evaluation**: - Evaluate the model using the test data. - Calculate and return the accuracy score using `accuracy_score` from `sklearn.metrics`. - Compute and return the confusion matrix using `confusion_matrix` from `sklearn.metrics`. # Constraints: - Use a random state of 42 for reproducibility in train-test split. - No additional libraries are allowed other than `numpy` and `scikit-learn`. # Expected Input and Output: - There are no function inputs. - The output should be a tuple containing: - `accuracy`: A float representing the accuracy of the classifier. - `conf_matrix`: A numpy ndarray representing the confusion matrix. # Example: ```python accuracy, conf_matrix = classify_iris_flowers() print(f\\"Accuracy: {accuracy}\\") print(f\\"Confusion Matrix:n{conf_matrix}\\") ``` Notes: - Ensure you test your function to confirm it performs as expected. - Handle any potential exceptions that might occur, such as issues with data splitting or model training. This question tests your ability to effectively utilize the functionalities of scikit-learn for a typical machine learning task from start to finish.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, confusion_matrix def classify_iris_flowers(): Loads the iris dataset, performs preprocessing, trains a logistic regression classifier, and evaluates the classifier using accuracy score and confusion matrix. Returns: accuracy (float): The accuracy score of the classifier on the test set. conf_matrix (np.ndarray): The confusion matrix of the classifier\'s predictions. # Load the iris dataset iris = load_iris() X = iris.data y = iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the Logistic Regression model model = LogisticRegression(max_iter=200) model.fit(X_train, y_train) # Predict on test set y_pred = model.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) # Compute confusion matrix conf_matrix = confusion_matrix(y_test, y_pred) return accuracy, conf_matrix"},{"question":"**Title:** Enhanced String Conversion In Python **Objective:** Write a Python function that mimics the C functions `PyOS_string_to_double` and `PyOS_double_to_string` using native Python capabilities. This will assess your understanding of Python string manipulation, error handling, and floating-point arithmetic. **Task:** Implement two functions: 1. `string_to_double(s: str) -> float`: - Converts a string `s` to a floating-point number. - If the conversion fails (e.g., the string does not represent a valid floating-point number), it should raise a `ValueError`. - If the string represents a value too large to store in a float (e.g., \\"1e500\\"), it should raise an `OverflowError`. 2. `double_to_string(val: float, format_code: str, precision: int, flags: int) -> str`: - Converts a floating-point number `val` to a string based on the given `format_code` and `precision`. - The `format_code` can be one of \'e\', \'E\', \'f\', \'F\', \'g\', \'G\', or \'r\'. - The `precision` specifies the number of digits after the decimal point. - The `flags` can include: - `1` for always preceding the string with a sign. - `2` for ensuring the string looks like a float (adds \\".0\\" if needed). - `4` for \\"alternate\\" formatting rules (similar to the \'#\' specifier in `format`). **Function Signatures:** ```python def string_to_double(s: str) -> float: # your code here pass def double_to_string(val: float, format_code: str, precision: int, flags: int) -> str: # your code here pass ``` **Input:** - For `string_to_double(s: str) -> float`: - `s`: A string representation of a floating-point number (e.g., \\"123.45\\"). - For `double_to_string(val: float, format_code: str, precision: int, flags: int) -> str`: - `val`: A floating-point number (e.g., 123.45). - `format_code`: A single character specifying the format (\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', or \'r\'). - `precision`: An integer specifying the precision. - `flags`: An integer representing flags combined using bitwise OR. **Output:** - For `string_to_double(s: str) -> float`: - Returns the floating-point number represented by the string. - Raises `ValueError` if the string is not a valid representation of a floating-point number. - Raises `OverflowError` if the number is too large. - For `double_to_string(val: float, format_code: str, precision: int, flags: int) -> str`: - Returns the string representation of the floating-point number based on the format code, precision, and flags. **Constraints:** - Implement the functions without using external libraries beyond Python\'s standard library. - Ensure the functions properly handle edge cases and errors. **Example:** ```python # Example usage of string_to_double print(string_to_double(\\"123.45\\")) # Should print: 123.45 print(string_to_double(\\"1e-10\\")) # Should print: 1e-10 try: print(string_to_double(\\"invalid\\")) except ValueError: print(\\"Caught a ValueError\\") try: print(string_to_double(\\"1e500\\")) except OverflowError: print(\\"Caught an OverflowError\\") # Example usage of double_to_string print(double_to_string(123.45, \'f\', 2, 0)) # Should print: \\"123.45\\" print(double_to_string(-123.45, \'e\', 1, 1)) # Should print: \\"-1.2e+02\\" print(double_to_string(12345.67, \'g\', 6, 2)) # Should print: \\"12346.0\\" ``` **Notes:** - Pay attention to the correct string formatting based on the C-style specifications. - Properly handle and raise exceptions as described.","solution":"def string_to_double(s: str) -> float: Converts a string s to a floating-point number. Raises ValueError if the conversion fails, and OverflowError if the number is too large. try: result = float(s) # Check for overflow by comparing to a very large number if result == float(\'inf\') or result == -float(\'inf\'): raise OverflowError(\'The value is too large to be represented as a float.\') return result except ValueError as ve: raise ValueError(f\\"Cannot convert \'{s}\' to float\\") from ve def double_to_string(val: float, format_code: str, precision: int, flags: int) -> str: Converts a floating-point number val to a string based on the given format_code and precision. if format_code not in \'eEfFgGr\': raise ValueError(\\"Invalid format code\\") # Create the format specification string format_spec = f\\".{precision}{format_code}\\" # Apply flags if flags & 1: format_spec = \\"+\\" + format_spec if flags & 2: if format_code in \'fF\': val = float(f\\"{val:.{precision}f}\\") elif format_code in \'gG\' and \'.\' not in f\\"{val:.{precision}g}\\": format_spec = format_spec.rstrip(\'g\').rstrip(\'G\') + \'g\' + \'#\' formatted_str = format(val, format_spec) if flags & 4 and format_code in \'fF\': if \'.\' not in formatted_str: formatted_str += \'.0\' return formatted_str"},{"question":"Implement a function `check_and_sync_device(device_index: int) -> str` that performs the following tasks: 1. Checks if the specified device index is within the range of available devices. 2. If the device index is valid, it sets the device to this index. 3. Synchronizes the operations on the selected device. 4. Returns a success message with the current device index and stream. **Function Signature:** ```python def check_and_sync_device(device_index: int) -> str: ``` **Input:** - `device_index` (int): The index of the device to check and synchronize. **Output:** - Returns a string message indicating success and providing details about the current device index and stream. **Constraints:** - The device index must be non-negative. - If no accelerator is available, return an error message. **Example:** ```python # Assume the current device count is 2 # Valid device index within the range print(check_and_sync_device(1)) # Output: \\"Device set to index 1, current stream: Stream 0\\" # Invalid device index outside the range print(check_and_sync_device(3)) # Output: \\"Error: Invalid device index\\" # Checking availability when no accelerator is available print(check_and_sync_device(0)) # Output: \\"Error: No accelerator available\\" ``` **Performance Requirements:** - The function should efficiently handle device queries and synchronization. - The function should validate inputs before performing operations.","solution":"def check_and_sync_device(device_index: int) -> str: Checks if the specified device index is within the range of available devices. If the device index is valid, it sets the device to this index. Synchronizes the operations on the selected device. Returns a success message with the current device index and stream. import torch if not torch.cuda.is_available(): return \\"Error: No accelerator available\\" device_count = torch.cuda.device_count() if device_index < 0 or device_index >= device_count: return \\"Error: Invalid device index\\" torch.cuda.set_device(device_index) stream = torch.cuda.current_stream(device_index) return f\\"Device set to index {device_index}, current stream: Stream {stream.id}\\""},{"question":"Objective You need to demonstrate your understanding of the `urllib.robotparser` package in Python. You are required to write a function that performs multiple operations using the `RobotFileParser` class and answers specific questions about the rules defined in the `robots.txt` file of a given website. Function Specifications Implement the function `analyze_robots_txt(url, user_agent)` that: 1. Takes two arguments: - `url` (str): The URL of the website whose `robots.txt` file needs to be analyzed. - `user_agent` (str): The user agent string to query against the rules in the `robots.txt` file. 2. Creates an instance of `RobotFileParser`. 3. Sets the URL of the `robots.txt` file using `set_url(url)`. 4. Reads and parses the `robots.txt` file using the `read()` method. 5. Returns a dictionary with the following information: - `can_fetch_root` (bool): Whether the given `user_agent` is allowed to fetch the root URL (`/`). - `can_fetch_sample` (bool): Whether the given `user_agent` is allowed to fetch a sample URL (`/sample-path`). - `crawl_delay` (int or None): The crawl delay value for the given `user_agent`. - `request_rate` (tuple or None): The request-rate value as a tuple `(requests, seconds)`. - `sitemaps` (list or None): The list of sitemap URLs. Example ```python def analyze_robots_txt(url, user_agent): pass # Implement your function here # Example usage result = analyze_robots_txt(\\"http://www.example.com\\", \\"MyUserAgent\\") print(result) ``` Constraints - You may assume the `robots.txt` file is well-formed and accessible. - Handle the case where specific rules are not defined for the given `user_agent`. Notes - Make sure to consider the default behavior if a rule is not specified for the given `user_agent`. - Make use of the `RobotFileParser` class methods effectively. **You must not perform any I/O operations such as reading from or writing to files. The function should solely focus on the parsing and decision-making based on the `robots.txt` content.** This is an open-ended question with multiple correct solutions. Focus on correctness, readability, and efficiency in your implementation.","solution":"from urllib.robotparser import RobotFileParser def analyze_robots_txt(url, user_agent): Analyzes the robots.txt file of a given website. Args: - url (str): The URL of the website whose robots.txt file needs to be analyzed. - user_agent (str): The user agent string to query against the rules in the robots.txt file. Returns: - dict: Dictionary with the following keys: - \'can_fetch_root\' (bool) - \'can_fetch_sample\' (bool) - \'crawl_delay\' (int or None) - \'request_rate\' (tuple or None) - \'sitemaps\' (list) robot_parser = RobotFileParser() robots_txt_url = url if url.endswith(\\"/robots.txt\\") else url + \\"/robots.txt\\" robot_parser.set_url(robots_txt_url) robot_parser.read() can_fetch_root = robot_parser.can_fetch(user_agent, \'/\') can_fetch_sample = robot_parser.can_fetch(user_agent, \'/sample-path\') crawl_delay = robot_parser.crawl_delay(user_agent) request_rate = robot_parser.request_rate(user_agent) sitemaps = robot_parser.site_maps() return { \'can_fetch_root\': can_fetch_root, \'can_fetch_sample\': can_fetch_sample, \'crawl_delay\': crawl_delay, \'request_rate\': request_rate, \'sitemaps\': sitemaps if sitemaps else [] }"},{"question":"# Question: Implementing a Simple POP3 Email Client You are required to implement a basic email client using Python\'s `poplib` module. The client should connect to a POP3 server, authenticate using a provided username and password, retrieve message summaries, and delete specific messages based on user input. Function Specification Function: `pop3_email_client(host, port, username, password, delete_list)` - `host` (str): The hostname of the POP3 server. - `port` (int): The port number to connect to. Use the standard port 110 for non-SSL connections. - `username` (str): The username for authenticating with the POP3 server. - `password` (str): The password for authenticating with the POP3 server. - `delete_list` (list of int): A list of message numbers that should be deleted after retrieval. The function should perform the following steps: 1. Establish a connection to the POP3 server. 2. Authenticate using the provided username and password. 3. Retrieve a list of all message summaries (message number and size) and print them. 4. Delete the messages specified in the `delete_list` from the server. 5. Ensure proper cleanup by signing off from the server using the `quit` command to commit deletions and unlock the mailbox. Constraints - The function should handle possible exceptions appropriately, such as connection errors, authentication failures, and invalid message numbers. - Ensure the function prints meaningful error messages for debugging purposes. - The function should not print sensitive information like passwords. Example ```python def pop3_email_client(host, port, username, password, delete_list): import poplib try: # Establish connection to the POP3 server server = poplib.POP3(host, port) # Authenticate server.user(username) server.pass_(password) # Retrieve message list messages = server.list()[1] print(\\"Message Summary:\\") for msg in messages: print(msg) # Delete specified messages for msg_num in delete_list: server.dele(msg_num) # Signoff server.quit() except poplib.error_proto as e: print(f\\"POP3 error: {e}\\") except Exception as e: print(f\\"General error: {e}\\") # Example usage host = \\"pop.example.com\\" port = 110 username = \\"example_user\\" password = \\"example_pass\\" delete_list = [1, 2] pop3_email_client(host, port, username, password, delete_list) ``` Notes - This example assumes a non-SSL connection for simplicity. For SSL connections, you would typically use the `POP3_SSL` class instead. - You may test the function using a test POP3 server or mock environment to ensure it behaves as expected. Good luck!","solution":"import poplib def pop3_email_client(host, port, username, password, delete_list): Connect to a POP3 server, authenticate, retrieve message summaries, and delete specified messages. try: # Establish connection to the POP3 server server = poplib.POP3(host, port) # Authenticate server.user(username) server.pass_(password) # Retrieve message list messages = server.list()[1] summaries = [] for msg in messages: summaries.append(msg) print(\\"Message Summary:\\") for summary in summaries: print(summary) # Delete specified messages for msg_num in delete_list: server.dele(msg_num) # Signoff server.quit() except poplib.error_proto as e: print(f\\"POP3 error: {e}\\") except Exception as e: print(f\\"General error: {e}\\")"},{"question":"# Regular Expression Operations with Python\'s `re` Module Problem Statement You are building a text-processing utility that needs to perform various operations using regular expressions. The operations include: 1. Extracting all email addresses from a text. 2. Reformatting dates from `MM/DD/YYYY` format to `YYYY-MM-DD` format. 3. Validating phone numbers to ensure they match the pattern `(XXX) XXX-XXXX`. 4. Replacing all occurrences of a specified word with another word, but only if the word is not part of a larger word (i.e., it should be a whole word replacement). Implement the following functions: 1. `extract_emails(text: str) -> list[str]`: - Extract all email addresses from the given text and return them as a list of strings. 2. `reformat_dates(text: str) -> str`: - Find all dates in the `MM/DD/YYYY` format in the given text and reformat them to `YYYY-MM-DD`. 3. `validate_phone_number(phone_number: str) -> bool`: - Validate if the given phone number matches the pattern `(XXX) XXX-XXXX`. Return `True` if it matches, `False` otherwise. 4. `replace_word(text: str, old_word: str, new_word: str) -> str`: - Replace all occurrences of `old_word` with `new_word` in the text. The replacement should only occur if the `old_word` is a whole word and not part of another word. Function Signatures ```python import re from typing import List def extract_emails(text: str) -> List[str]: pass def reformat_dates(text: str) -> str: pass def validate_phone_number(phone_number: str) -> bool: pass def replace_word(text: str, old_word: str, new_word: str) -> str: pass ``` Input and Output Formats 1. `extract_emails(text: str) -> list`: - **Input:** A string containing text. - **Output:** A list of strings, each representing an extracted email address. ```python print(extract_emails(\\"Please contact us at support@example.com or sales@example.org.\\")) # Output: [\'support@example.com\', \'sales@example.org\'] ``` 2. `reformat_dates(text: str) -> str`: - **Input:** A string containing text with dates. - **Output:** A string with dates reformatted to `YYYY-MM-DD`. ```python print(reformat_dates(\\"Today\'s date is 12/31/2022.\\")) # Output: \\"Today\'s date is 2022-12-31.\\" ``` 3. `validate_phone_number(phone_number: str) -> bool`: - **Input:** A string representing a phone number. - **Output:** A boolean indicating if the phone number is valid. ```python print(validate_phone_number(\\"(123) 456-7890\\")) # Output: True ``` 4. `replace_word(text: str, old_word: str, new_word: str) -> str`: - **Input:** A string containing text, the old word to be replaced, and the new word. - **Output:** A string with the specified word replaced. ```python print(replace_word(\\"Hello world, welcome to the programming world.\\", \\"world\\", \\"universe\\")) # Output: \\"Hello universe, welcome to the programming universe.\\" ``` Constraints - Valid email addresses follow the pattern `local_part@domain_part`. - Dates are strictly in `MM/DD/YYYY` format. - Phone numbers are strictly in the `(XXX) XXX-XXXX` format. - The replacement word should not be a part of another word. Good luck, and happy coding!","solution":"import re from typing import List def extract_emails(text: str) -> List[str]: pattern = r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\' return re.findall(pattern, text) def reformat_dates(text: str) -> str: pattern = r\'b(d{2})/(d{2})/(d{4})b\' replace_pattern = r\'3-1-2\' return re.sub(pattern, replace_pattern, text) def validate_phone_number(phone_number: str) -> bool: pattern = r\'^(d{3}) d{3}-d{4}\' return bool(re.match(pattern, phone_number)) def replace_word(text: str, old_word: str, new_word: str) -> str: pattern = r\'b\' + re.escape(old_word) + r\'b\' return re.sub(pattern, new_word, text)"},{"question":"# Pandas DataFrame Deep Dive Objective Demonstrate your understanding of DataFrame memory usage, truth evaluation, and UDF mutation in pandas by completing the following tasks. Problem Statement You are provided with a dataset consisting of various data types. You need to perform several operations on this dataset as described below: 1. **Memory Usage Evaluation**: Write a function `evaluate_memory_usage(df: pd.DataFrame) -> dict` that takes a DataFrame and: - Returns a dictionary where the keys are the column names and the values are the memory usage of each column. 2. **Boolean Operations and Truth Evaluation**: Create a function `evaluate_series_boolean(series: pd.Series) -> bool` that takes a Series and: - Returns `True` if any value in the Series is `True`, otherwise `False`. 3. **Mutating DataFrames in UDFs**: Design a function `safe_apply_function(df: pd.DataFrame, func: callable) -> pd.DataFrame` that: - Safely applies a user defined function `func` to each column of a DataFrame without causing unintended mutations in the original dataframe. - Ensure the function returns the altered DataFrame while leaving the original DataFrame unchanged. Example ```python import pandas as pd import numpy as np # Example DataFrame dtypes = [\\"int64\\", \\"float64\\", \\"datetime64[ns]\\", \\"timedelta64[ns]\\", \\"object\\", \\"bool\\", \\"complex128\\"] n = 100 data = {t: np.random.randint(1, 100, n).astype(t) if t != \'bool\' else np.random.choice([True, False], n) for t in dtypes} df = pd.DataFrame(data) # Memory Usage Evaluation def evaluate_memory_usage(df): memory_usage = df.memory_usage(deep=True) return memory_usage.to_dict() # Boolean Operations and Truth Evaluation def evaluate_series_boolean(series): return series.any() # Mutating DataFrames in UDFs def safe_apply_function(df, func): def apply_safe(s): s_copy = s.copy() return func(s_copy) return df.apply(apply_safe) # Example function to mutate the dataframe def simple_mutation(series): series.iloc[0] = 999 return series # Usage print(evaluate_memory_usage(df)) print(evaluate_series_boolean(df[\'bool\'])) new_df = safe_apply_function(df, simple_mutation) print(new_df.head()) print(df.head()) # original df remains unchanged ``` Constraints 1. Assume the DataFrame will have at least one of each required dtype. 2. The User Defined Functions (UDF) are not expected to be complex for this assessment. Test your functions thoroughly to ensure they behave as expected.","solution":"import pandas as pd import numpy as np def evaluate_memory_usage(df: pd.DataFrame) -> dict: Returns a dictionary where the keys are the column names and the values are the memory usage of each column. memory_usage = df.memory_usage(deep=True) return memory_usage.to_dict() def evaluate_series_boolean(series: pd.Series) -> bool: Returns True if any value in the Series is True, otherwise False. return series.any() def safe_apply_function(df: pd.DataFrame, func: callable) -> pd.DataFrame: Safely applies the user defined function `func` to each column of the DataFrame, returning a new DataFrame with the modifications, leaving the original DataFrame unchanged. def apply_safe(col): col_copy = col.copy() return func(col_copy) return df.apply(apply_safe)"},{"question":"Objective Your task is to create a Python script that uses the `venv` module to create a virtual environment with custom initialization behavior. Specifically, we want to extend the `EnvBuilder` class to not only set up the environment but also to install specific Python packages from a given URL. Requirements 1. **CustomEnvBuilder Class**: Create a class `CustomEnvBuilder` that extends `venv.EnvBuilder`. 2. **Custom Initialization**: - Integrate the capability to install specified packages during the virtual environment setup. - Implement a method `install_package(context, package_name, url)` to handle the downloading and installation of packages from the given URL into the virtual environment. 3. **Command-Line Interface**: Develop a command-line interface to accept: - Directory for virtual environment creation. - Optional package installations, specified in the format `package_name:url`. Input and Output - **Input Format**: - A directory path where the virtual environment will be created. - Zero or more package installations specified in the format `package_name:url`. Example command to run your script: ```sh python create_env.py /path/to/newenv requests:https://bootstrap.pypa.io/get-pip.py ``` - **Output**: - Create a virtual environment at the specified directory. - Install the stated packages from their respective URLs. - Log the installation process, showing success or failure for each package installation. Constraints - Ensure the script handles exceptions and provides meaningful error messages. - The script should be compatible with Python 3.6+. Example Usage and Expected Behavior Suppose you want to create a virtual environment in `/myenv` and install `pip`: ```sh python create_env.py /myenv pip:https://bootstrap.pypa.io/get-pip.py ``` This should: - Create a virtual environment in `/myenv`. - Install `pip` from the provided URL. - Output the progress of the environment creation and package installation. Performance Requirements - The script should efficiently handle the creation of virtual environments and the installation of packages without significant delay or memory overhead. Implementation Skeleton ```python import os import venv from subprocess import Popen, PIPE import sys from threading import Thread from urllib.parse import urlparse from urllib.request import urlretrieve class CustomEnvBuilder(venv.EnvBuilder): def __init__(self, packages=None, *args, **kwargs): self.packages = packages if packages else [] super().__init__(*args, **kwargs) def post_setup(self, context): for package_name, url in self.packages: self.install_package(context, package_name, url) def install_package(self, context, package_name, url): # Method to download and install a package from the specified URL _, _, path, _, _, _ = urlparse(url) fn = os.path.split(path)[-1] binpath = context.bin_path distpath = os.path.join(binpath, fn) urlretrieve(url, distpath) args = [context.env_exe, fn] p = Popen(args, stdout=PIPE, stderr=PIPE, cwd=binpath) t1 = Thread(target=self.reader, args=(p.stdout, \'stdout\')) t1.start() t2 = Thread(target=self.reader, args=(p.stderr, \'stderr\')) t2.start() p.wait() t1.join() t2.join() def reader(self, stream, context): while True: s = stream.readline() if not s: break print(s.decode(\'utf-8\')) stream.close() if __name__ == \'__main__\': import argparse parser = argparse.ArgumentParser(description=\'Create a virtual environment and install specified packages.\') parser.add_argument(\'env_dir\', type=str, help=\'Directory where the virtual environment will be created.\') parser.add_argument(\'packages\', type=str, nargs=\'*\', help=\'Packages to install in the format package_name:url\') args = parser.parse_args() packages = [tuple(pkg.split(\':\')) for pkg in args.packages] builder = CustomEnvBuilder(packages=packages, with_pip=True) builder.create(args.env_dir) ``` Use this template and modify it to fulfill the requirements and constraints.","solution":"import os import sys import venv from subprocess import Popen, PIPE from urllib.parse import urlparse from urllib.request import urlretrieve from threading import Thread class CustomEnvBuilder(venv.EnvBuilder): def __init__(self, packages=None, *args, **kwargs): self.packages = packages if packages else [] super().__init__(*args, **kwargs) def post_setup(self, context): for package_name, url in self.packages: self.install_package(context, package_name, url) def install_package(self, context, package_name, url): try: _, _, path, _, _, _ = urlparse(url) fn = os.path.split(path)[-1] binpath = context.bin_path distpath = os.path.join(binpath, fn) urlretrieve(url, distpath) args = [context.env_exe, distpath] p = Popen(args, stdout=PIPE, stderr=PIPE, cwd=binpath) t1 = Thread(target=self.reader, args=(p.stdout, \'stdout\')) t1.start() t2 = Thread(target=self.reader, args=(p.stderr, \'stderr\')) t2.start() p.wait() t1.join() t2.join() print(f\\"Successfully installed {package_name} from {url}\\") except Exception as e: print(f\\"Failed to install {package_name} from {url}: {e}\\") def reader(self, stream, context): while True: s = stream.readline() if not s: break print(s.decode(\'utf-8\')) stream.close() if __name__ == \'__main__\': import argparse parser = argparse.ArgumentParser(description=\'Create a virtual environment and install specified packages.\') parser.add_argument(\'env_dir\', type=str, help=\'Directory where the virtual environment will be created.\') parser.add_argument(\'packages\', type=str, nargs=\'*\', help=\'Packages to install in the format package_name:url\') args = parser.parse_args() packages = [tuple(pkg.split(\':\')) for pkg in args.packages] builder = CustomEnvBuilder(packages=packages, with_pip=True) builder.create(args.env_dir)"},{"question":"**Objective:** You are given a dataset with high-dimensional data. Your task is to implement a solution that reduces the dimensionality of the data, clusters the samples into distinct groups, and identifies any potential outliers within the data. **Requirements:** 1. Implement a function `process_unsupervised_learning(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]` where: - `data`: A 2D NumPy array of shape `(n_samples, n_features)` representing the high-dimensional data. 2. The function should perform the following steps: - **Dimensionality Reduction**: Reduce the data to 2 dimensions using PCA (Principal Component Analysis). - **Clustering**: Cluster the reduced data into 3 distinct groups using K-means clustering. - **Outlier Detection**: Use the Isolation Forest algorithm to detect outliers in the original high-dimensional data. 3. The function should return: - `reduced_data`: A 2D NumPy array of shape `(n_samples, 2)` representing the data reduced to 2 dimensions. - `cluster_labels`: A 1D NumPy array of shape `(n_samples,)` containing the cluster labels assigned to each sample. - `outlier_labels`: A 1D NumPy array of shape `(n_samples,)` where -1 represents an outlier, and 1 represents an inlier. **Constraints:** - You must use scikit-learn for PCA, K-means, and Isolation Forest implementations. - Ensure your solution is efficient and can handle large datasets. **Example:** ```python import numpy as np data = np.random.rand(100, 10) # Sample high-dimensional data reduced_data, cluster_labels, outlier_labels = process_unsupervised_learning(data) print(reduced_data.shape) # Output: (100, 2) print(cluster_labels.shape) # Output: (100,) print(outlier_labels.shape) # Output: (100,) ``` This example demonstrates the application of dimensionality reduction, clustering, and outlier detection on a random dataset. **Notes:** - You may assume that the input data does not contain any missing values. - Provide appropriate comments and documentation in your code for clarity.","solution":"import numpy as np from sklearn.decomposition import PCA from sklearn.cluster import KMeans from sklearn.ensemble import IsolationForest def process_unsupervised_learning(data: np.ndarray) -> tuple: Reduces the dimensionality of the high-dimensional data using PCA, clusters the reduced data using K-means, and detects outliers using Isolation Forest. Parameters: data (np.ndarray): A 2D NumPy array of shape (n_samples, n_features) representing the high-dimensional data. Returns: tuple: A tuple containing: - reduced_data (np.ndarray): Data reduced to 2 dimensions. - cluster_labels (np.ndarray): Cluster labels assigned to each sample. - outlier_labels (np.ndarray): Labels indicating whether each sample is an outlier (-1 for outliers, 1 for inliers). # Dimensionality Reduction using PCA pca = PCA(n_components=2) reduced_data = pca.fit_transform(data) # Clustering using K-means kmeans = KMeans(n_clusters=3, random_state=42) cluster_labels = kmeans.fit_predict(reduced_data) # Outlier Detection using Isolation Forest isolation_forest = IsolationForest(random_state=42) outlier_labels = isolation_forest.fit_predict(data) return reduced_data, cluster_labels, outlier_labels"},{"question":"**Objective:** Design a Python program that processes a set of text files containing details about books (title, author, published year) and outputs a summary report. This task will assess your ability to handle file I/O, data manipulation, and the use of modules in Python. **Task:** You need to implement the following functions: 1. **`read_books_from_files(file_list: List[str]) -> List[Dict[str, Union[str, int]]]:`** - **Input:** - `file_list`: A list of file paths, each file containing details of books. Each file has the following format: ``` Title1,Author1,Year1 Title2,Author2,Year2 ... ``` - **Output:** - A list of dictionaries, where each dictionary contains information about a book with keys: \\"title\\", \\"author\\", and \\"year\\". - **Constraints:** - The function should handle different file formats and possible errors in file reading. 2. **`generate_summary_report(books: List[Dict[str, Union[str, int]]]) -> Dict[str, Union[str, int, List[str]]]:`** - **Input:** - `books`: A list of dictionaries, each containing information about a book. - **Output:** - A summary report in dictionary format with the following keys: - `\\"total_books\\"`: total number of books. - `\\"oldest_book\\"`: title of the oldest book. - `\\"unique_authors\\"`: list of unique authors. - `\\"most_recent_book\\"`: title of the most recently published book. - **Constraints:** - The function should handle tie cases appropriately (e.g., multiple books published in the same oldest or most recent year). **Example:** ```python file_list = [\\"books1.txt\\", \\"books2.txt\\"] # contents of books1.txt: # The Great Gatsby,F. Scott Fitzgerald,1925 # To Kill a Mockingbird,Harper Lee,1960 # contents of books2.txt: # 1984,George Orwell,1949 # Catch-22,Joseph Heller,1961 books = read_books_from_files(file_list) # Expected output: # [ # {\\"title\\": \\"The Great Gatsby\\", \\"author\\": \\"F. Scott Fitzgerald\\", \\"year\\": 1925}, # {\\"title\\": \\"To Kill a Mockingbird\\", \\"author\\": \\"Harper Lee\\", \\"year\\": 1960}, # {\\"title\\": \\"1984\\", \\"author\\": \\"George Orwell\\", \\"year\\": 1949}, # {\\"title\\": \\"Catch-22\\", \\"author\\": \\"Joseph Heller\\", \\"year\\": 1961} # ] summary = generate_summary_report(books) # Expected output: # { # \\"total_books\\": 4, # \\"oldest_book\\": \\"The Great Gatsby\\", # \\"unique_authors\\": [\\"F. Scott Fitzgerald\\", \\"Harper Lee\\", \\"George Orwell\\", \\"Joseph Heller\\"], # \\"most_recent_book\\": \\"Catch-22\\" # } ``` **Performance Requirements:** - The solution should be efficient in terms of both time and space complexity. - Assume there can be up to 10,000 books across all files. **Additional Notes:** - You may assume that the input files are well-formed but must handle cases where files may not be found or accessible. - Use Python\'s standard library to handle file I/O and data manipulation. # Submission Requirements: - Your code should be well-documented with comments explaining the logic. - You should include test cases to verify the correctness of your functions.","solution":"from typing import List, Dict, Union import os def read_books_from_files(file_list: List[str]) -> List[Dict[str, Union[str, int]]]: Reads book information from a list of files and returns a list of dictionaries. Args: - file_list (List[str]): A list of file paths. Returns: - books (List[Dict[str, Union[str, int]]]): A list of book dictionaries. books = [] for file in file_list: if not os.path.isfile(file): print(f\\"File {file} does not exist or cannot be accessed.\\") continue with open(file, \'r\') as f: for line in f: parts = line.strip().split(\',\') if len(parts) == 3: try: book = { \\"title\\": parts[0], \\"author\\": parts[1], \\"year\\": int(parts[2]) } books.append(book) except ValueError: print(f\\"Invalid year format in line: {line}\\") return books def generate_summary_report(books: List[Dict[str, Union[str, int]]]) -> Dict[str, Union[str, int, List[str]]]: Generates a summary report from a list of book dictionaries. Args: - books (List[Dict[str, Union[str, int]]]): A list of book dictionaries. Returns: - summary (Dict[str, Union[str, int, List[str]]]): A summary report dictionary. if not books: return { \\"total_books\\": 0, \\"oldest_book\\": \\"\\", \\"unique_authors\\": [], \\"most_recent_book\\": \\"\\" } oldest_book = min(books, key=lambda x: x[\\"year\\"])[\\"title\\"] most_recent_book = max(books, key=lambda x: x[\\"year\\"])[\\"title\\"] unique_authors = list({book[\\"author\\"] for book in books}) return { \\"total_books\\": len(books), \\"oldest_book\\": oldest_book, \\"unique_authors\\": unique_authors, \\"most_recent_book\\": most_recent_book }"},{"question":"Objective You are tasked with demonstrating your understanding of using Seaborn\'s `plotting_context` function to customize the aesthetics of plots. Write a function using Seaborn that generates and saves plots with different context settings. Task 1. Write a function `generate_custom_plots(data, filename_prefix)` that takes in: - `data`: A Pandas DataFrame containing at least two columns: \\"category\\" (categorical data) and \\"value\\" (numerical data). - `filename_prefix`: A string that will serve as the prefix for the filenames of the saved plots. 2. The function should: - Generate and save four plots using four different context settings: \\"paper\\", \\"notebook\\", \\"talk\\", and \\"poster\\". - Each plot should be a simple line plot with \\"category\\" on the x-axis and \\"value\\" on the y-axis. - Save each plot to a file named `<filename_prefix>_<context>.png` (e.g., `plot_paper.png`, `plot_notebook.png`, etc.). 3. Ensure that the changes in context settings are reflected in the saved plots. Constraints - You must use a context manager (`with sns.plotting_context(...)`) to temporarily set the context for each plot. - You can assume that the input DataFrame `data` will always have at least two columns: \\"category\\" and \\"value\\". Example ```python import pandas as pd # Sample data data = pd.DataFrame({ \'category\': [\\"A\\", \\"B\\", \\"C\\", \\"D\\"], \'value\': [10, 15, 5, 20] }) # Function call generate_custom_plots(data, \'plot\') # This should generate and save the following files: # - plot_paper.png # - plot_notebook.png # - plot_talk.png # - plot_poster.png ``` Expected function signature ```python def generate_custom_plots(data, filename_prefix): # your code here ``` Important Notes - Make sure to handle any imports required within the function or at the start of your script. - Focus on the correct application of Seaborn\'s `plotting_context` to demonstrate the changes in plot aesthetics.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_custom_plots(data, filename_prefix): Generate and save line plots with different context settings using Seaborn. Parameters: - data: pandas DataFrame containing at least \\"category\\" and \\"value\\" columns. - filename_prefix: String prefix for the filenames of the saved plots. contexts = [\'paper\', \'notebook\', \'talk\', \'poster\'] for context in contexts: with sns.plotting_context(context): plt.figure() sns.lineplot(data=data, x=\'category\', y=\'value\') filename = f\\"{filename_prefix}_{context}.png\\" plt.savefig(filename) plt.close()"},{"question":"You are given a dataset containing information about different species of penguins. The dataset includes the flipper length (in mm), bill length (in mm), bill depth (in mm), and species of each penguin. Your task is to write a function that creates and customizes a joint plot using the `seaborn` library to visualize the relationship between the flipper length and the bill depth of penguins. The function should include the following features: 1. **Scatterplot with Conditional Coloring**: Color the data points by the species of the penguins. 2. **Density Curves**: Add density curves in the marginal axes. 3. **Linear Regression Line**: Overlay a linear regression line on the joint plot. 4. **Customization of Appearance**: Customize the joint plot to use different colors for different species. Additionally, set the marker type of the scatter plot to a circle (\\"o\\") and its size to 50. Set the KDE plot color for the density curves to \\"black\\" and the opacity (alpha) to 0.7. 5. **Hexagonal Bin Plot**: Instead of scatterplot, create a hexagonal bin plot to show the density of the data points in the joint plot. 6. **Additional Customizations**: Pass further customization options for the marginal plots, such as setting the number of bins in the histogram to 30 and disabling the fill for the histograms. 7. **Figure Size**: The plot should have a height of 8 inches and a width ratio of 3 for the marginal plots. # Expected Function Signature: ```python def customize_jointplot(data: pd.DataFrame): pass ``` # Input - `data`: A Pandas DataFrame containing at least the columns `\\"species\\"`, `\\"flipper_length_mm\\"`, and `\\"bill_depth_mm\\"`. # Output - The function should display the customized joint plot using Seaborn. # Example ```python import seaborn as sns import pandas as pd # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Call the function customize_jointplot(penguins) ``` # Constraints and Limitations 1. You can assume that the input DataFrame is clean and does not contain any missing values. 2. Ensure the code is efficient and leverages Seaborn\'s functionality to the fullest for generating the plot. # Notes - Make sure all necessary imports are included. - The resulting plot should clearly show the relationship between the two variables and the distribution conditioned on the species.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def customize_jointplot(data: pd.DataFrame): Creates and customizes a joint plot to visualize the relationship between flipper length and bill depth of penguins. # Create the joint plot with hexagonal density bin plot g = sns.jointplot( x=\\"flipper_length_mm\\", y=\\"bill_depth_mm\\", data=data, kind=\\"hex\\", height=8, marginal_kws=dict(bins=30, fill=False) ) # Linear regression line sns.regplot( x=\\"flipper_length_mm\\", y=\\"bill_depth_mm\\", data=data, ax=g.ax_joint, scatter=False, color=\'black\' ) # Customize marginal plots for ax in [g.ax_marg_x, g.ax_marg_y]: ax.set_facecolor(\'#f0f0f0\') # Set the figure size and aspect ratio g.fig.set_figwidth(16) g.fig.set_figheight(8) plt.show()"},{"question":"# Custom Plot Design with Seaborn and Matplotlib Objective Create a comprehensive plot using seaborn and matplotlib that analyzes the `tips` dataset. This dataset contains information about the total bill, tip amount, sex, smoker status, day of the week, time of day, and size of the dining party. Dataset Use seaborn’s `tips` dataset. Requirements 1. **Load the `tips` dataset** from seaborn. 2. **Create a `seaborn.objects.Plot` instance**: - Use \\"total_bill\\" on the x-axis. - Use \\"tip\\" on the y-axis. - Color the data points based on the \\"time\\" column. 3. **Add appropriate layers**: - Add dot points for each data entry (represent each dining instance as a dot). - Create facets based on the \\"smoker\\" column (separate plots for smokers and non-smokers). 4. **Customize the plot** with matplotlib (e.g., add titles, modify appearance): - Add a title to each facet indicating \\"Smoker\\" or \\"Non-Smoker\\". - Add a custom annotation box to the \\"Non-Smoker\\" facet with any important note (e.g., \\"Non-smokers tend to have higher tips\\"). Input and Output Formats This is a code-only exercise. There is no direct input or output format. Your task is to provide a script that meets the above requirements and displays the plot when executed. Constraints - You must use seaborn\'s `Plot` class and matplotlib for customization. - Ensure the visualizations are clear and labeled appropriately. Code ```python import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt import matplotlib as mpl # 1. Load the tips dataset tips = sns.load_dataset(\\"tips\\") # 2. Create a seaborn.objects.Plot instance plot = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"time\\").add(so.Dots()) # 3. Create facets based on the smoker column faceted_plot = plot.facet(col=\\"smoker\\") # 4. Customize the plot with matplotlib fig = plt.figure(figsize=(10, 5), dpi=100) smokers, non_smokers = fig.subfigures(1, 2) # Plot for Smokers smokers_ax = smokers.subplots() plot.on(smokers_ax).plot() smokers.suptitle(\\"Smokers\\") # Plot for Non-Smokers non_smokers_ax = non_smokers.subplots() plot.on(non_smokers_ax).plot() non_smokers.suptitle(\\"Non-Smokers\\") # Custom annotation for Non-Smokers plot rect = mpl.patches.Rectangle((0.5, 0.5), width=.4, height=.2, color=\\"C3\\", alpha=.3, transform=non_smokers_ax.transAxes, clip_on=False) non_smokers_ax.add_artist(rect) non_smokers_ax.text(x=0.7, y=0.6, s=\\"Non-smokers tend to have higher tips\\", ha=\\"center\\", va=\\"center\\", transform=non_smokers_ax.transAxes) plt.show() ```","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt import matplotlib as mpl def create_comprehensive_plot(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a seaborn.objects.Plot instance plot = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"time\\").add(so.Dots()) # Create facets based on the smoker column faceted_plot = plot.facet(col=\\"smoker\\") # Customize the plot with matplotlib fig = plt.figure(figsize=(10, 5), dpi=100) smokers, non_smokers = fig.subfigures(1, 2) # Plot for Smokers smokers_ax = smokers.subplots() plot.on(smokers_ax).plot() smokers.suptitle(\\"Smokers\\") # Plot for Non-Smokers non_smokers_ax = non_smokers.subplots() plot.on(non_smokers_ax).plot() non_smokers.suptitle(\\"Non-Smokers\\") # Custom annotation for Non-Smokers plot rect = mpl.patches.Rectangle((0.5, 0.5), width=.4, height=.2, color=\\"C3\\", alpha=.3, transform=non_smokers_ax.transAxes, clip_on=False) non_smokers_ax.add_artist(rect) non_smokers_ax.text(x=0.7, y=0.6, s=\\"Non-smokers tend to have higher tips\\", ha=\\"center\\", va=\\"center\\", transform=non_smokers_ax.transAxes) plt.show()"},{"question":"# Asynchronous Programming with Coroutines In this assessment, you are required to demonstrate your understanding of Python\'s async and await functionality by creating an asynchronous function and handling coroutines. Problem Statement You are tasked to implement a simplified version of an asynchronous web scraping framework. You will write an asynchronous function to simulate fetching data from multiple web pages concurrently and aggregate the results. Task 1. Write an asynchronous function `fetch_page_content(url: str) -> str` that simulates fetching the content of a web page. Since we are simulating this, it should return a string formatted as \\"Content of {url}\\" after waiting for 1 second (you can use `await asyncio.sleep(1)` to simulate the wait). 2. Write another asynchronous function `fetch_all_contents(urls: List[str]) -> List[str]` that accepts a list of URLs and uses `fetch_page_content` to fetch their content concurrently. This function should return a list of strings where each string is the content of the respective URL. # Requirements - You should use `async def` to declare your asynchronous functions. - Use `await` where necessary to wait for the coroutine results. - Utilize `asyncio.gather` to run multiple coroutines concurrently. - You should handle the list of URLs and the content fetching concurrently to ensure efficient execution. # Example ```python import asyncio from typing import List # Define the asynchronous function to fetch page content async def fetch_page_content(url: str) -> str: await asyncio.sleep(1) return f\\"Content of {url}\\" # Define the asynchronous function to fetch all page contents async def fetch_all_contents(urls: List[str]) -> List[str]: tasks = [fetch_page_content(url) for url in urls] contents = await asyncio.gather(*tasks) return contents # Example usage async def main(): urls = [\\"http://example.com/page1\\", \\"http://example.com/page2\\", \\"http://example.com/page3\\"] retrieved_contents = await fetch_all_contents(urls) print(retrieved_contents) # Run the example asyncio.run(main()) ``` # Constraints - Assume that the input list of URLs will contain between 1 and 100 URLs. - Each URL in the input list is a valid URL string. - Aim to keep the functions as efficient and clear as possible. # Performance Considerations - Ensure your implementation fetches the contents of all URLs concurrently rather than sequentially to leverage the benefits of asynchronous programming.","solution":"import asyncio from typing import List async def fetch_page_content(url: str) -> str: Simulates fetching the content of a web page. Args: url (str): The URL of the web page. Returns: str: Simulated content of the web page. await asyncio.sleep(1) return f\\"Content of {url}\\" async def fetch_all_contents(urls: List[str]) -> List[str]: Fetches the content of multiple web pages concurrently. Args: urls (List[str]): List of URLs to fetch content from. Returns: List[str]: List of contents of the respective URLs. tasks = [fetch_page_content(url) for url in urls] contents = await asyncio.gather(*tasks) return contents"},{"question":"You are given a list of dictionaries where each dictionary represents a student with the following structure: ```python [ {\\"name\\": \\"Alice\\", \\"age\\": 23, \\"grades\\": [85, 90, 92]}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"grades\\": [79, 81, 78]}, {\\"name\\": \\"Charlie\\", \\"age\\": 23, \\"grades\\": [91, 95, 94]}, ... ] ``` Write a function `process_students` that processes the list of students and returns the following information: 1. The name of the student with the highest average grade. 2. A new sorted list of students\' names by their ages, and in case of ties, by their names. 3. The total number of unique ages among students. 4. The names of students that have all grades above a certain threshold. Your function should take a list of dictionaries along with a grade threshold (an integer) as input and return a tuple `(name, sorted_names, unique_ages_count, above_threshold_names)` where: - `name` is a string representing the name of the student with the highest average grade. - `sorted_names` is a list of strings representing the sorted names of students. - `unique_ages_count` is an integer representing the count of unique ages. - `above_threshold_names` is a list of names of students who have all grades above the given threshold. # Input - A list of dictionaries (as described above) and an integer as the grade threshold. # Output - A tuple containing: - A string (name of the student with the highest average grade). - A list of strings (sorted list of names). - An integer (count of unique ages). - A list of strings (names of students with all grades above the threshold). # Constraints - The list will have at least one student. - Each student will have at least one grade in the grades list. - Student names are unique. # Example ```python students = [ {\\"name\\": \\"Alice\\", \\"age\\": 23, \\"grades\\": [85, 90, 92]}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"grades\\": [79, 81, 78]}, {\\"name\\": \\"Charlie\\", \\"age\\": 23, \\"grades\\": [91, 95, 94]} ] threshold = 80 result = process_students(students, threshold) print(result) # Expected Output: # (\'Charlie\', [\'Bob\', \'Alice\', \'Charlie\'], 2, [\'Alice\', \'Charlie\']) ``` # Notes - You must use built-in functions to achieve the desired results wherever applicable. - Pay attention to performance considerations for larger datasets. # Implementation ```python def process_students(students, threshold): # Step 1: Find the student with the highest average grade def average_grade(student): return sum(student[\'grades\']) / len(student[\'grades\']) top_student = max(students, key=average_grade)[\'name\'] # Step 2: Sort the students by age and name sorted_students = sorted(students, key=lambda s: (s[\'age\'], s[\'name\'])) sorted_names = [student[\'name\'] for student in sorted_students] # Step 3: Count unique ages unique_ages = len(set(student[\'age\'] for student in students)) # Step 4: Find students with all grades above the threshold above_threshold_names = [student[\'name\'] for student in students if all(grade > threshold for grade in student[\'grades\'])] return (top_student, sorted_names, unique_ages, above_threshold_names) ```","solution":"def process_students(students, threshold): # Step 1: Find the student with the highest average grade def average_grade(student): return sum(student[\'grades\']) / len(student[\'grades\']) top_student = max(students, key=average_grade)[\'name\'] # Step 2: Sort the students by age and name sorted_students = sorted(students, key=lambda s: (s[\'age\'], s[\'name\'])) sorted_names = [student[\'name\'] for student in sorted_students] # Step 3: Count unique ages unique_ages = len(set(student[\'age\'] for student in students)) # Step 4: Find students with all grades above the threshold above_threshold_names = [student[\'name\'] for student in students if all(grade > threshold for grade in student[\'grades\'])] return (top_student, sorted_names, unique_ages, above_threshold_names)"},{"question":"Advanced Seaborn Plot Customization Objective: To assess your understanding of advanced customization techniques in Seaborn, you will be required to create plots with specific aesthetic properties. Task: 1. **Data Preparation:** - Generate a dataset `data` consisting of 30 samples and 5 features. Each feature should follow a normal distribution but with different means. 2. **Plot Customization Part 1:** - Create a single figure with four subplots (2x2 grid), each using a different Seaborn style: `darkgrid`, `whitegrid`, `dark`, and `ticks`. - In each subplot, draw a boxplot of the generated dataset `data`. 3. **Plot Customization Part 2:** - In the subplot with the `ticks` style, remove the top and right spines using `sns.despine`. - For the subplot with the `whitegrid` style, set the axes facecolor to be a light grey. 4. **Context Scaling:** - Separate the aesthetics question by examining the dataset\'s visualization suitable for different contexts. Create another plot of the dataset using a violin plot while testing the following contexts: `paper`, `notebook`, `talk`, and `poster`. - For each context, increase the font scale and line width. 5. **Temporary Styling:** - Use a `with` statement to create temporary subplot style changes within the main figure. Implementation Details: 1. **Data Generation:** - Use `numpy` to generate the dataset. 2. **Plot Customization:** - Utilize `seaborn` and `matplotlib` for plotting. - Set each style appropriately using `sns.set_style`. 3. **Context Scaling Settings:** - Use `sns.set_context` and appropriately set the context and properties. 4. **Temporary Styling:** - Use context managers (`with` statement) to apply temporary styling. Expected Output: - A figure with four boxplots, each in a different Seaborn style with specified customizations. - A sequence of violin plots showing the dataset visualized under different contexts with appropriate scaling. Code Skeleton: ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Data generation data = np.random.normal(size=(30, 5)) + np.arange(5) # Create figure with subplots for different styles f, axes = plt.subplots(2, 2, figsize=(10, 10)) # Define styles styles = [\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"ticks\\"] # Apply styles and create boxplots for ax, style in zip(axes.flatten(), styles): sns.set_style(style) sns.boxplot(data=data, ax=ax) if style == \\"ticks\\": sns.despine(ax=ax) if style == \\"whitegrid\\": ax.set_facecolor(\\".9\\") # Create plots for different contexts contexts = [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"] plt.figure(figsize=(12, 8)) for i, context in enumerate(contexts): plt.subplot(2, 2, i+1) sns.set_context(context, font_scale=1.5, rc={\\"lines.linewidth\\": 2.5}) sns.violinplot(data=data) plt.title(f\\"Context: {context}\\") plt.tight_layout() plt.show() # Temporary Styling using \'with\' statement plt.figure(figsize=(12, 8)) with sns.axes_style(\\"whitegrid\\"): sns.boxplot(data=data) plt.show() ``` Submission: Submit your code as a Python file or Jupyter notebook. Ensure it contains all necessary imports and runs as expected without errors.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def generate_data(samples, features): Generate a dataset with normal distribution with different means for the features. return np.random.normal(size=(samples, features)) + np.arange(features) def plot_customization_part1(data): Create a single figure with four subplots (2x2 grid) with different Seaborn styles. f, axes = plt.subplots(2, 2, figsize=(10, 10)) # Define styles styles = [\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"ticks\\"] # Apply styles and create boxplots for ax, style in zip(axes.flatten(), styles): sns.set_style(style) sns.boxplot(data=data, ax=ax) if style == \\"ticks\\": sns.despine(ax=ax) if style == \\"whitegrid\\": ax.set_facecolor(\\".9\\") plt.show() def plot_customization_part2_and_contexts(data): Create another plot of the dataset using violin plots with different contexts. contexts = [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"] plt.figure(figsize=(12, 8)) for i, context in enumerate(contexts): plt.subplot(2, 2, i + 1) sns.set_context(context, font_scale=1.5, rc={\\"lines.linewidth\\": 2.5}) sns.violinplot(data=data) plt.title(f\\"Context: {context}\\") plt.tight_layout() plt.show() def plot_with_temp_styling(data): Create a plot using temporary styling within a context manager. plt.figure(figsize=(12, 8)) with sns.axes_style(\\"whitegrid\\"): sns.boxplot(data=data) plt.show() # Example function to execute all parts if __name__ == \\"__main__\\": data = generate_data(30, 5) plot_customization_part1(data) plot_customization_part2_and_contexts(data) plot_with_temp_styling(data)"},{"question":"You are given a dataset containing information on the tips received by waitstaff at a restaurant (`tips`). The dataset includes the following columns: - `total_bill`: The total bill amount (in dollars). - `tip`: The tip given (in dollars). - `sex`: The sex of the person paying the bill (male or female). - `smoker`: Whether the person is a smoker (yes or no). - `day`: The day of the week (Thur, Fri, Sat, Sun). - `time`: The time of day (Lunch, Dinner). - `size`: The size of the group. Your task is to use `seaborn` to create a complex visualization grid based on this dataset. Requirements: 1. Create a `FacetGrid` object: - Initialize the grid with `tips` dataset. - Arrange the grid with days as columns and time as rows. 2. Plot a scatterplot for each facet showing the relationship between `total_bill` and `tip`: - Ensure plots are colored by the `sex` variable. 3. Customize the grid appearance: - Set the height of each plot to 4 units and aspect ratio to 0.75. - Add horizontal reference lines at the median `tip` value across all data. - Add a legend to the grid. 4. Additional customization: - Annotate each facet with the number of observations (`N = <number>`). - Set axis labels for `total_bill` and `tip`. - Use descriptive titles for each row. 5. Perform further customization on the underlying matplotlib objects: - Change the face color of the plots in the \\"Dinner\\" row to \\".90\\" (light gray). Here is the expected input and output format: **Input:** ```python import seaborn as sns # Load dataset tips = sns.load_dataset(\\"tips\\") ``` **Output:** A fully formed grid of scatterplots meeting the requirements specified above. Note: Submit your code as the answer to this question.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load dataset tips = sns.load_dataset(\\"tips\\") # Initialize the grid with days as columns and time as rows. g = sns.FacetGrid(tips, col=\\"day\\", row=\\"time\\", hue=\\"sex\\", height=4, aspect=0.75, margin_titles=True) # Plot scatterplots g.map(plt.scatter, \\"total_bill\\", \\"tip\\", alpha=0.7) # Calculate overall median tip median_tip = tips[\'tip\'].median() # Add horizontal reference lines at the median tip value for ax in g.axes.flat: ax.axhline(median_tip, ls=\\"--\\", color=\\"red\\", alpha=0.5) # Add a legend to the grid g.add_legend() # Customize appearance for ax in g.axes.flat: # Annotate number of observations ax.text(0.5, 0.9, f\'N = {len(tips)}\', horizontalalignment=\'center\', verticalalignment=\'center\', transform=ax.transAxes, bbox={\'boxstyle\': \'round,pad=0.3\', \'facecolor\': \'wheat\', \'alpha\': 0.5, \'edgecolor\': \'none\'}) # Set axis labels ax.set_xlabel(\'Total Bill\') ax.set_ylabel(\'Tip\') # Set descriptive titles for each row g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") # Change the face color of the plots in the \\"Dinner\\" row to light gray for ax, title in zip(g.axes.flat, g.row_names): if \\"Dinner\\" in title: ax.set_facecolor(\'.90\') plt.show()"},{"question":"Objective: Your task is to create a function that reads data from a binary file, processes it, and stores the result into an array. You will demonstrate your understanding of the `array` module by following the guidelines below. Problem Statement: You are given a binary file containing a sequence of signed 32-bit integers (type code `\'i\'`). The file could be large, so your solution should be efficient in terms of both time and space complexity. Write a function `process_binary_file(file_path: str) -> array.array` that performs the following steps: 1. Reads all integers from the given binary file. 2. Creates an array of type `\'i\'` (signed 32-bit integers). 3. Appends the integers to the array. 4. Scans the array to remove all duplicates while preserving the first occurrence of each integer. 5. Returns the processed array. Requirements: - The input path (`file_path`) is the path to the binary file. - The function should handle any file size and should not load the entire file into memory at once. - The processed array should preserve the order of first occurrences of each integer. - Use the `array` module for creating and manipulating the array. Constraints: - You may assume the binary file exists and contains properly formatted 32-bit signed integers. - You should handle any possible integers within the 32-bit signed range. - Efficiently handle large files without running into memory issues. Example: Imagine your function is provided with a binary file path containing the following integers in binary format: `1, 2, 3, 2, 4, 1, 5` The resulting array after processing should be: `array(\'i\', [1, 2, 3, 4, 5])` Note: - You can use the methods of the `array` class to achieve reading from and writing to files, and manipulating the array. - Do not use any additional libraries (e.g., `numpy`) to solve this problem. ```python import array def process_binary_file(file_path: str) -> array.array: # Your implementation here pass ```","solution":"import array def process_binary_file(file_path: str) -> array.array: # Initialize the array to hold 32-bit signed integers int_array = array.array(\'i\') # Read the binary file and populate the array with open(file_path, \'rb\') as f: while chunk := f.read(4): int_array.append(int.from_bytes(chunk, byteorder=\'little\', signed=True)) # Process the array to remove duplicates while preserving order seen = set() result_array = array.array(\'i\') for num in int_array: if num not in seen: seen.add(num) result_array.append(num) return result_array"},{"question":"# Objective The goal of this assignment is to test your understanding of XML parsing and manipulation using the `xml.etree.ElementTree` module in Python. # Problem Statement You are provided with a string of XML data representing a collection of books in a library. Each book has the following structure in the XML: ```xml <books> <book> <title>Book Title 1</title> <author>Author Name 1</author> <year>Year 1</year> <price>Price 1</price> </book> <book> <title>Book Title 2</title> <author>Author Name 2</author> <year>Year 2</year> <price>Price 2</price> </book> ... </books> ``` Write a function `parse_and_modify_books(xml_data: str, increment: float) -> str` that performs the following tasks: 1. Parses the XML data. 2. For each book in the XML, increases the price by the specified `increment`. 3. Returns the modified XML data as a string. # Input Format - `xml_data (str)`: A string containing XML data representing a collection of books. - `increment (float)`: A floating-point number to increment each book\'s price. # Output Format - The function should return a string containing the modified XML data. # Constraints - You may assume the structure of the XML is well-formed and conforms to the described format. - The price in each book element will be a valid floating point number. # Example ```python xml_data = <books> <book> <title>Book Title 1</title> <author>Author Name 1</author> <year>2001</year> <price>29.99</price> </book> <book> <title>Book Title 2</title> <author>Author Name 2</author> <year>2003</year> <price>39.99</price> </book> </books> increment = 10.0 result = parse_and_modify_books(xml_data, increment) print(result) ``` Expected output: ```xml <books> <book> <title>Book Title 1</title> <author>Author Name 1</author> <year>2001</year> <price>39.99</price> </book> <book> <title>Book Title 2</title> <author>Author Name 2</author> <year>2003</year> <price>49.99</price> </book> </books> ``` # Notes - You should use the `xml.etree.ElementTree` module for parsing and modifying the XML data. - Ensure that the modified XML retains the original structure and only updates the prices. Good luck and happy coding!","solution":"import xml.etree.ElementTree as ET def parse_and_modify_books(xml_data: str, increment: float) -> str: Parses the XML data, increments the price of each book, and returns the modified XML as a string. Parameters: - xml_data (str): A string containing the initial XML data. - increment (float): The value to increment each book\'s price by. Returns: - str: The modified XML data as a string. # Parse the XML data root = ET.fromstring(xml_data) # Iterate over each book and increase the price for book in root.findall(\'book\'): price_elem = book.find(\'price\') if price_elem is not None: current_price = float(price_elem.text) new_price = current_price + increment price_elem.text = f\\"{new_price:.2f}\\" # Convert the modified XML tree back to a string return ET.tostring(root, encoding=\'unicode\')"},{"question":"# Coding Challenge: Advanced Tuple Manipulation **Objective:** Implement a function that performs multiple operations on a tuple to demonstrate your understanding of tuple manipulations, including creating, slicing, and aggregating tuples. **Detailed Requirements:** Write a function `tuple_operations` that accepts a list of integers and performs the following steps: 1. **Create a Tuple:** Convert the list of integers into a tuple. 2. **Divide the Tuple:** Split the tuple into two equal halves. If the original tuple has an odd length, the first half should have one more element than the second half. 3. **Sum and Difference Tuples:** Create two new tuples: - The first tuple should contain the sums of the corresponding elements from each half. - The second tuple should contain the absolute differences of the corresponding elements from each half. 4. **Combine Tuples:** Concatenate the sum tuple and the difference tuple into a single resultant tuple. **Function Signature:** ```python def tuple_operations(numbers: list[int]) -> tuple[int, ...]: ``` **Input:** - `numbers`: A list of integers (0 ≤ length of list ≤ 1000, -10^6 ≤ integers in list ≤ 10^6) **Output:** - A single tuple containing the elements of the sum tuple followed by the elements of the difference tuple, all derived as specified. **Example:** ```python numbers = [1, 2, 3, 4, 5] # After converting to a tuple: (1, 2, 3, 4, 5) # First half: (1, 2, 3) # Second half: (4, 5) # Sum tuple: (5, 7) because 1+4=5, 2+5=7 # Difference tuple: (3, 3) because |1-4|=3, |2-5|=3 # Resultant tuple: (5, 7, 3, 3) result = tuple_operations([1, 2, 3, 4, 5]) print(result) # Output should be (5, 7, 3, 3) ``` **Constraints:** - The implementation should handle edge cases such as empty list inputs and varying lengths of the input list. - The function should run efficiently within the provided constraints. Ensure to validate your solution with multiple test cases and document any assumptions made.","solution":"def tuple_operations(numbers: list[int]) -> tuple[int, ...]: # Step 1: Create a Tuple from the list of numbers numbers_tuple = tuple(numbers) length = len(numbers_tuple) if length == 0: return () # Step 2: Divide the Tuple into two halves # If the length is odd, the first half will have one more element than the second half mid_index = (length + 1) // 2 first_half = numbers_tuple[:mid_index] second_half = numbers_tuple[mid_index:] # Step 3: Create Sum and Difference tuples min_length = min(len(first_half), len(second_half)) sums = tuple(first_half[i] + second_half[i] for i in range(min_length)) differences = tuple(abs(first_half[i] - second_half[i]) for i in range(min_length)) # Step 4: Combine and return the resulting tuple result = sums + differences return result"},{"question":"**Objective:** Demonstrate understanding of advanced usage of the `sys` module for profiling and debugging a Python program. --- Problem Statement You are tasked with implementing a Python function that uses the `sys` module to profile the execution of a script and collect debugging information when an unhandled exception occurs. Specifically, you will: 1. Set up a profiling function that records function call counts. 2. Implement an exception hook that logs information about unhandled exceptions including the stack trace. 3. Ensure that your code uses appropriate `sys` functions and handles edge cases. Your task is to implement the following functions: 1. **initialize_profiler(profile_data)** - **Input:** `profile_data` (dictionary) - A dictionary to store profiling data (function call counts). - **Output:** None - **Description:** Set up a system-wide profiler that increments the call count for each function that is called. 2. **exception_handler(exc_type, exc_value, exc_traceback)** - **Input:** - `exc_type` (type) - The type of the exception. - `exc_value` (BaseException) - The exception instance raised. - `exc_traceback` (traceback) - A traceback object encapsulating the call stack at the point of the exception. - **Output:** None - **Description:** Log the details of the exception, including the type, value, and full traceback. 3. **setup_system_hooks(profile_data)** - **Input:** `profile_data` (dictionary) - A dictionary to store profiling data (function call counts). - **Output:** None - **Description:** Set the profiler and exception handler using the `sys` module functions. 4. **run_script(script_path)** - **Input:** `script_path` (str) - The path to a Python script to be executed. - **Output:** None - **Description:** Execute the script located at the given path within the configured profiling and exception handling environment. ```python import sys import traceback def initialize_profiler(profile_data): def profiler(frame, event, arg): if event == \'call\': function_name = frame.f_code.co_name if function_name in profile_data: profile_data[function_name] += 1 else: profile_data[function_name] = 1 return profiler sys.setprofile(profiler) def exception_handler(exc_type, exc_value, exc_traceback): log_entry = { \\"type\\": exc_type.__name__, \\"value\\": str(exc_value), \\"traceback\\": \'\'.join(traceback.format_exception(exc_type, exc_value, exc_traceback)) } with open(\\"error_log.txt\\", \\"a\\") as log_file: log_file.write(str(log_entry) + \'n\') def setup_system_hooks(profile_data): # Set up profiler initialize_profiler(profile_data) # Set up custom excepthook sys.excepthook = exception_handler def run_script(script_path): # Read and execute the script in a context where hooks are set with open(script_path) as script_file: source_code = script_file.read() exec(source_code) # Example usage: if __name__ == \\"__main__\\": profile_data = {} setup_system_hooks(profile_data) run_script(\'path/to/your/script.py\') print(profile_data) ``` **Notes:** - You should be mindful of thread safety when working with the profiling data. - Ensure your `exception_handler` robustly logs all necessary information without crashing. - Test your code thoroughly with scripts that include function calls and potential exceptions to verify the profiling and exception handling.","solution":"import sys import traceback def initialize_profiler(profile_data): Initialize the system profiler to count function calls. def profiler(frame, event, arg): if event == \'call\': function_name = frame.f_code.co_name if function_name in profile_data: profile_data[function_name] += 1 else: profile_data[function_name] = 1 return profiler sys.setprofile(profiler) def exception_handler(exc_type, exc_value, exc_traceback): Log unhandled exceptions detailing type, value, and traceback. log_entry = { \\"type\\": exc_type.__name__, \\"value\\": str(exc_value), \\"traceback\\": \'\'.join(traceback.format_exception(exc_type, exc_value, exc_traceback)) } with open(\\"error_log.txt\\", \\"a\\") as log_file: log_file.write(str(log_entry) + \'n\') def setup_system_hooks(profile_data): Setup the profiler and exception handler. initialize_profiler(profile_data) sys.excepthook = exception_handler def run_script(script_path): Execute a Python script at the given path. with open(script_path) as script_file: source_code = script_file.read() exec(compile(source_code, script_path, \'exec\'))"},{"question":"**Coding Assessment Question: Managing Object Lifecycles with Weak References** **Objective**: Create a Python class that effectively utilizes weak references to manage object lifecycles and prevents memory leaks. **Background**: Weak references allow an object to be referenced without preventing it from being garbage collected. They are particularly useful when dealing with caches or circular references. **Task**: 1. Implement a class `Observable` that maintains a list of observers. 2. Observers should be weak references to prevent them from preventing garbage collection. 3. Implement methods to add and remove observers. 4. Implement a method to notify all observers when an event occurs. 5. Ensure that the class can handle the removal of observers that have been garbage collected. **Details**: - Define an `Observer` class with a method `update(self, message)` which prints the message received. - Define the `Observable` class with the following methods: - `add_observer(self, observer)`: Adds an observer. - `remove_observer(self, observer)`: Removes an observer. - `notify_observers(self, message)`: Notifies all non-garbage collected observers with a message. - Use weak references to manage the list of observers. **Constraints**: - The solution should avoid holding strong references to observers, ensuring they can be garbage collected. - Properly handle the case where an observer is garbage collected and no longer needs to be notified. **Example**: ```python import weakref class Observer: def update(self, message): print(f\'Observer received: {message}\') class Observable: def __init__(self): self._observers = [] def add_observer(self, observer): # Implementation def remove_observer(self, observer): # Implementation def notify_observers(self, message): # Implementation # Example usage observable = Observable() observer1 = Observer() observer2 = Observer() observable.add_observer(observer1) observable.add_observer(observer2) observable.notify_observers(\'Hello Observers\') # Both observers should print the message del observer1 # Manually deleting to simulate garbage collection observable.notify_observers(\'Observer1 should not receive this\') # Expected Output: # Observer received: Hello Observers # Observer received: Hello Observers # Observer received: Observer1 should not receive this ``` **Performance Requirements**: - The `notify_observers` method should run in O(n) where n is the number of observers. - Ensure that memory is not leaked by properly removing garbage collected observers. **Submission**: Provide the implementation of the `Observable` class based on the above specifications.","solution":"import weakref class Observer: def update(self, message): print(f\'Observer received: {message}\') class Observable: def __init__(self): self._observers = [] def _remove_dead_references(self): self._observers = [ref for ref in self._observers if ref() is not None] def add_observer(self, observer): self._remove_dead_references() self._observers.append(weakref.ref(observer)) def remove_observer(self, observer): self._remove_dead_references() self._observers = [ref for ref in self._observers if ref() is not observer] def notify_observers(self, message): self._remove_dead_references() for ref in self._observers: observer = ref() if observer is not None: observer.update(message)"},{"question":"You are provided with a Python source code file named `sample_code.py`. Your task is to analyze the symbol tables for the functions and classes defined in this source code. You need to implement the following functions: 1. **analyze_code_structure(source_code: str, filename: str) -> dict**: - **Input**: - `source_code`: A string containing the Python source code. - `filename`: The name of the file containing the code, provided for context. - **Output**: - A dictionary with the structure: ```python { \'functions\': [ { \'name\': function_name, \'parameters\': [parameter_names], \'locals\': [local_names], \'globals\': [global_names], \'nonlocals\': [nonlocal_names], \'frees\': [free_variable_names] }, ... ], \'classes\': [ { \'name\': class_name, \'methods\': [method_names] }, ... ] } ``` 2. **symbol_properties(source_code: str, filename: str, symbol_name: str) -> dict**: - **Input**: - `source_code`: A string containing the Python source code. - `filename`: The name of the file containing the code. - `symbol_name`: The name of the symbol whose properties need to be analyzed. - **Output**: - A dictionary with properties of the symbol: ```python { \'name\': symbol_name, \'is_referenced\': bool, \'is_imported\': bool, \'is_parameter\': bool, \'is_global\': bool, \'is_nonlocal\': bool, \'is_declared_global\': bool, \'is_local\': bool, \'is_annotated\': bool, \'is_free\': bool, \'is_assigned\': bool, \'is_namespace\': bool } ``` # Constraints - You can assume that the code provided will be valid Python code. - The `symbol_name` for the `symbol_properties` function will always be present in the source code provided. # Example For the `sample_code.py` with the following content: ```python def foo(x, y): z = x + y return z class Bar: def method(self): pass ``` - `analyze_code_structure` should return: ```python { \'functions\': [ { \'name\': \'foo\', \'parameters\': [\'x\', \'y\'], \'locals\': [\'z\'], \'globals\': [], \'nonlocals\': [], \'frees\': [] } ], \'classes\': [ { \'name\': \'Bar\', \'methods\': [\'method\'] } ] } ``` - `symbol_properties` with `symbol_name=\'z\'` should return: ```python { \'name\': \'z\', \'is_referenced\': True, \'is_imported\': False, \'is_parameter\': False, \'is_global\': False, \'is_nonlocal\': False, \'is_declared_global\': False, \'is_local\': True, \'is_annotated\': False, \'is_free\': False, \'is_assigned\': True, \'is_namespace\': False } ``` Implement the functions `analyze_code_structure` and `symbol_properties` using the `symtable` module.","solution":"import symtable def analyze_code_structure(source_code: str, filename: str) -> dict: symtable_obj = symtable.symtable(source_code, filename, \'exec\') def parse_function(func): table = func.get_symbols() return { \'name\': func.get_name(), \'parameters\': [name for name in func.get_parameters()], \'locals\': [sym.get_name() for sym in table if sym.is_local() and not sym.is_parameter()], \'globals\': [sym.get_name() for sym in table if sym.is_global()], \'nonlocals\': [sym.get_name() for sym in table if sym.is_nonlocal()], \'frees\': [sym.get_name() for sym in table if sym.is_free()] } def parse_class(cls): return { \'name\': cls.get_name(), \'methods\': [func.get_name() for func in cls.get_children() if func.get_type() == \'function\'] } structure = { \'functions\': [], \'classes\': [] } for child in symtable_obj.get_children(): if child.get_type() == \'function\': structure[\'functions\'].append(parse_function(child)) elif child.get_type() == \'class\': structure[\'classes\'].append(parse_class(child)) return structure def symbol_properties(source_code: str, filename: str, symbol_name: str) -> dict: symtable_obj = symtable.symtable(source_code, filename, \'exec\') def find_symbol(table, name): for sym in table.get_symbols(): if sym.get_name() == name: return sym for child in table.get_children(): res = find_symbol(child, name) if res: return res return None symbol = find_symbol(symtable_obj, symbol_name) if symbol is None: return {} return { \'name\': symbol_name, \'is_referenced\': symbol.is_referenced(), \'is_imported\': symbol.is_imported(), \'is_parameter\': symbol.is_parameter(), \'is_global\': symbol.is_global(), \'is_nonlocal\': symbol.is_nonlocal(), \'is_declared_global\': symbol.is_declared_global(), \'is_local\': symbol.is_local(), \'is_annotated\': symbol.is_annotated(), \'is_free\': symbol.is_free(), \'is_assigned\': symbol.is_assigned(), \'is_namespace\': symbol.is_namespace() }"},{"question":"Coding Assessment Question # Objective Implement a Python program to efficiently calculate the sum of squares of a list of numbers using asynchronous execution with `ProcessPoolExecutor` from the \\"concurrent.futures\\" module. The solution should demonstrate the student\'s ability to use the `ProcessPoolExecutor`, handle `Future` objects, and manage asynchronous execution flow. # Problem Statement You are given a list of integers, and you need to calculate the sum of squares of all these integers using the `ProcessPoolExecutor`. The calculation of the square of each integer should be done in parallel using separate processes. # Function Signature ```python def sum_of_squares(numbers: List[int]) -> int: pass ``` # Input - `numbers`: A list of integers (e.g., `[1, 2, 3, 4, 5]`). # Output - Returns an integer which is the sum of squares of all numbers in the list (e.g., `55` for input `[1, 2, 3, 4, 5]`). # Constraints 1. The list `numbers` can have up to 10,000 integers. 2. Each integer in the list is between -10,000 and 10,000. 3. The calculation should efficiently handle large input sizes and utilize parallel processing to achieve this. # Example ```python # Example input numbers = [1, 2, 3, 4, 5] # Example output result = sum_of_squares(numbers) print(result) # Output: 55 ``` # Notes - Utilize `ProcessPoolExecutor` for parallel processing. - Use `map` or `submit` methods to delegate tasks and gather results. - Ensure the function manages exceptions and handles the `Future` results properly. - Include error handling for any potential exceptions during execution (e.g., timeouts, process failures). # Hints - You may find the `map` method of `ProcessPoolExecutor` useful for mapping a function over a list of inputs. - Ensure proper shutdown of the executor to free resources. - Consider edge cases like empty input list which should return `0`. # Additional Task After writing the function, provide a brief explanation on how `ProcessPoolExecutor` helps in optimizing the performance for this specific task and how it compares to `ThreadPoolExecutor`.","solution":"from concurrent.futures import ProcessPoolExecutor from typing import List def square(n: int) -> int: return n * n def sum_of_squares(numbers: List[int]) -> int: if not numbers: return 0 with ProcessPoolExecutor() as executor: results = executor.map(square, numbers) return sum(results)"},{"question":"**Programming Task: File Audit and Cleanup Tool** Write a Python function `file_audit_and_cleanup` that automates the process of auditing and cleaning up files within a specified directory. The function should: 1. Accept two parameters: - `directory_path`: a string representing the path to the directory to be audited. - `file_size_limit`: an integer representing the maximum file size (in bytes) allowed in the directory. 2. Perform the following tasks: - List all files and directories within `directory_path`. - For each file, check its size. - If the file size exceeds `file_size_limit`, delete the file. - For each subdirectory, list its contents and report: - The total number of files. - The total size of all files. - Print a summary report with the following: - Total size and count of files in `directory_path`. - Total size and count of files in each subdirectory. 3. Ensure the tool handles exceptions gracefully: - If a directory or file cannot be accessed due to permissions, log the issue without halting execution. Here\'s the function signature to get you started: ```python import os def file_audit_and_cleanup(directory_path: str, file_size_limit: int) -> None: # Ensure the path is valid and accessible if not os.path.exists(directory_path): raise FileNotFoundError(f\\"The directory \'{directory_path}\' does not exist.\\") if not os.path.isdir(directory_path): raise NotADirectoryError(f\\"The path \'{directory_path}\' is not a directory.\\") # Helper function to get the size and count of files in a directory def get_dir_info(path): total_size = 0 file_count = 0 for entry in os.scandir(path): try: if entry.is_file(): total_size += os.path.getsize(entry.path) file_count += 1 elif entry.is_dir(): dir_size, dir_count = get_dir_info(entry.path) total_size += dir_size file_count += dir_count except Exception as e: print(f\\"Could not access {entry.path}: {e}\\") return total_size, file_count # Main audit and cleanup process for entry in os.scandir(directory_path): try: if entry.is_file(): file_size = os.path.getsize(entry.path) if file_size > file_size_limit: os.remove(entry.path) print(f\\"Deleted file \'{entry.name}\' of size {file_size} bytes\\") elif entry.is_dir(): dir_size, dir_count = get_dir_info(entry.path) print(f\\"Directory \'{entry.name}\' contains {dir_count} files, total size {dir_size} bytes\\") except Exception as e: print(f\\"Could not process {entry.path}: {e}\\") # Generate the summary report for the main directory total_size, total_files = get_dir_info(directory_path) print(f\\"nSummary Report for {directory_path}:\\") print(f\\"Total size of all files: {total_size} bytes\\") print(f\\"Total count of all files: {total_files}\\") ``` # Input - `directory_path` (string): Path to the directory to be audited. - `file_size_limit` (int): Maximum size (in bytes) a file can have to remain in the directory. # Output - Print statements indicating the status of the cleanup operations and a summary report. # Example Usage ```python file_audit_and_cleanup(\\"/path/to/directory\\", 1000000) ``` # Constraints - The function should handle directories and files with special permissions gracefully. - The function should not terminate on encountering an error with a specific file or directory; it should log the error and continue. - Ensure efficient traversal and handling of potentially large directories.","solution":"import os def file_audit_and_cleanup(directory_path: str, file_size_limit: int) -> None: Audits and cleans up a specified directory by deleting files that exceed the size limit and providing a summary report. :param directory_path: Path to the directory to be audited. :param file_size_limit: Maximum size (in bytes) a file can have to remain in the directory. # Ensure the path is valid and accessible if not os.path.exists(directory_path): raise FileNotFoundError(f\\"The directory \'{directory_path}\' does not exist.\\") if not os.path.isdir(directory_path): raise NotADirectoryError(f\\"The path \'{directory_path}\' is not a directory.\\") # Helper function to get the size and count of files in a directory def get_dir_info(path): total_size = 0 file_count = 0 for entry in os.scandir(path): try: if entry.is_file(): total_size += os.path.getsize(entry.path) file_count += 1 elif entry.is_dir(): dir_size, dir_count = get_dir_info(entry.path) total_size += dir_size file_count += dir_count except Exception as e: print(f\\"Could not access {entry.path}: {e}\\") return total_size, file_count # Main audit and cleanup process for entry in os.scandir(directory_path): try: if entry.is_file(): file_size = os.path.getsize(entry.path) if file_size > file_size_limit: os.remove(entry.path) print(f\\"Deleted file \'{entry.name}\' of size {file_size} bytes\\") elif entry.is_dir(): dir_size, dir_count = get_dir_info(entry.path) print(f\\"Directory \'{entry.name}\' contains {dir_count} files, total size {dir_size} bytes\\") except Exception as e: print(f\\"Could not process {entry.path}: {e}\\") # Generate the summary report for the main directory total_size, total_files = get_dir_info(directory_path) print(f\\"nSummary Report for {directory_path}:\\") print(f\\"Total size of all files: {total_size} bytes\\") print(f\\"Total count of all files: {total_files}\\")"},{"question":"<|Analysis Begin|> The provided documentation for PyTorch\'s `torch.nn.functional` module covers various functions categorized into several sections: 1. Convolution functions 2. Pooling functions 3. Attention mechanisms 4. Non-linear activation functions 5. Linear functions 6. Dropout functions 7. Sparse functions 8. Distance functions 9. Loss functions 10. Vision functions 11. DataParallel functions Each category includes multiple functions that are valuable for constructing and manipulating neural networks. The descriptions hint at a wide range of functionalities available, such as convolutions, pooling, activation functions, linear operations, dropout, embeddings, loss calculations, vision-related operations, and parallel processing. Given the breadth of functions, crafting a coding assessment question around building a basic neural network model involving convolutions, pooling, non-linear activations, and loss calculation would be appropriate. This would require students to demonstrate their understanding of the fundamentals, including data preparation, model definition, forward propagation, and loss computation. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Title:** Implementing a Convolutional Neural Network (CNN) with PyTorch\'s Functional API **Description:** You are tasked with implementing a convolutional neural network (CNN) using PyTorch\'s `torch.nn.functional` module. The CNN will be used to classify images from a simplified dataset. The network should consist of the following layers: 1. A 2D Convolutional layer with 32 filters, a kernel size of 3x3, stride of 1, and padding of 1. 2. A 2D Max Pooling layer with a kernel size of 2x2 and a stride of 2. 3. A ReLU activation function. 4. Another 2D Convolutional layer with 64 filters, a kernel size of 3x3, stride of 1, and padding of 1. 5. Another 2D Max Pooling layer with a kernel size of 2x2 and a stride of 2. 6. Another ReLU activation function. 7. A fully connected (linear) layer that outputs 10 classes. You will also need to implement the forward pass of the network. **Input:** - A 4D tensor `x` of shape `(batch_size, in_channels, height, width)` representing a batch of images. - The variable `num_classes` indicating the number of classes (use 10 for this question). **Output:** - A tensor of shape `(batch_size, num_classes)` representing the class scores for each image in the batch. **Constraints:** - Do not use high-level modules such as `torch.nn.Conv2d` or `torch.nn.MaxPool2d`. Only use functions from `torch.nn.functional`. - You may use standard PyTorch tensor operations and methods. **Performance requirements:** - The implemented network should be computationally efficient and should avoid unnecessary operations. # Function Signature ```python import torch import torch.nn.functional as F class SimpleCNN(torch.nn.Module): def __init__(self, num_classes: int): super(SimpleCNN, self).__init__() # Define any necessary parameters or layers here def forward(self, x: torch.Tensor) -> torch.Tensor: Forward pass of the CNN. Parameters: x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width) Returns: torch.Tensor: Output tensor of shape (batch_size, num_classes) # Implement the forward pass here return x # Example Usage # model = SimpleCNN(num_classes=10) # input_tensor = torch.randn(8, 3, 32, 32) # example batch of 8 RGB images of size 32x32 # output = model.forward(input_tensor) # print(output.shape) # Expected: (8, 10) ``` **Evaluation Criteria:** - Correctness: The forward pass should accurately follow the specified architecture. - Efficiency: Avoid using unnecessary operations or inefficient practices. - Understanding of PyTorch functionalities from `torch.nn.functional`.","solution":"import torch import torch.nn.functional as F class SimpleCNN(torch.nn.Module): def __init__(self, num_classes: int): super(SimpleCNN, self).__init__() # Initialize parameters needed for convolution and fully connected layers self.num_classes = num_classes self.conv1_weight = torch.nn.Parameter(torch.randn(32, 3, 3, 3)) # 32 filters, 3 input channels, 3x3 kernels self.conv1_bias = torch.nn.Parameter(torch.randn(32)) self.conv2_weight = torch.nn.Parameter(torch.randn(64, 32, 3, 3)) # 64 filters, 32 input channels, 3x3 kernels self.conv2_bias = torch.nn.Parameter(torch.randn(64)) self.fc_weight = torch.nn.Parameter(torch.randn(num_classes, 4096)) # Fully connected layer weights self.fc_bias = torch.nn.Parameter(torch.randn(num_classes)) def forward(self, x: torch.Tensor) -> torch.Tensor: Forward pass of the CNN. Parameters: x (torch.Tensor): Input tensor of shape (batch_size, in_channels, height, width) Returns: torch.Tensor: Output tensor of shape (batch_size, num_classes) # Perform first convolution with padding=1 x = F.conv2d(x, self.conv1_weight, self.conv1_bias, stride=1, padding=1) # Apply ReLU activation x = F.relu(x) # Perform max pooling x = F.max_pool2d(x, kernel_size=2, stride=2) # Perform second convolution with padding=1 x = F.conv2d(x, self.conv2_weight, self.conv2_bias, stride=1, padding=1) # Apply ReLU activation x = F.relu(x) # Perform max pooling x = F.max_pool2d(x, kernel_size=2, stride=2) # Flatten the tensor for the fully connected layer x = torch.flatten(x, start_dim=1) # Apply the fully connected layer x = F.linear(x, self.fc_weight, self.fc_bias) return x # Example usage if __name__ == \\"__main__\\": model = SimpleCNN(num_classes=10) input_tensor = torch.randn(8, 3, 32, 32) # Example batch of 8 RGB images of size 32x32 output = model.forward(input_tensor) print(output.shape) # Expected: (8, 10)"},{"question":"**Memory Management in Python using C API** # Problem Statement: You are tasked to implement a simplified memory manager in Python using the C API functions described in the provided documentation. The memory manager should dynamically allocate, reallocate, and free memory for a given number of elements, and ensure there are no memory leaks or mismanagement of memory. # Requirements: 1. Implement a Python module (`mymemmanager`) that provides the following functionalities: - `allocate_memory(size: int): int` - Allocates a block of memory for `size` elements and returns a pointer to the allocated memory. - `reallocate_memory(ptr: int, new_size: int): int` - Reallocates the previously allocated memory block pointed to by `ptr` to hold `new_size` elements and returns a new pointer to the reallocated memory. - `free_memory(ptr: int): None` - Frees the memory block pointed to by `ptr`. 2. Use the appropriate functions for raw memory allocation from the `PyMem` or `PyObject` families. # Constraints: - You should ensure that the functions from the same memory allocation domain are consistently used. - Your implementation should handle memory allocation failures gracefully by raising a `MemoryError` in Python when any allocation or reallocation fails. - Avoid mixing different allocators for a single memory block. - Ensure thread safety if applicable. # Example Usage: ```python import mymemmanager # Allocate memory for 10 elements ptr = mymemmanager.allocate_memory(10) # Reallocate memory to hold 20 elements ptr = mymemmanager.reallocate_memory(ptr, 20) # Free the allocated memory mymemmanager.free_memory(ptr) ``` # Notes: 1. Use `ctypes` or `cffi` to interface with the C API functions and manage raw memory directly within your Python module. 2. You are provided with the following C library functions from the `PyMem` family to accomplish this task: - `PyMem_RawMalloc` - `PyMem_RawCalloc` - `PyMem_RawRealloc` - `PyMem_RawFree` 3. If required, refer to the provided documentation for specific function signatures and usage details. Make sure your solution is robust and handles edge cases such as zero-size allocation, null pointers, and reallocation failures correctly.","solution":"import ctypes import sys # C functons from the PyMem family for raw memory management libpython = ctypes.PyDLL(None) PyMem_RawMalloc = libpython.PyMem_RawMalloc PyMem_RawMalloc.restype = ctypes.c_void_p PyMem_RawMalloc.argtypes = [ctypes.c_size_t] PyMem_RawRealloc = libpython.PyMem_RawRealloc PyMem_RawRealloc.restype = ctypes.c_void_p PyMem_RawRealloc.argtypes = [ctypes.c_void_p, ctypes.c_size_t] PyMem_RawFree = libpython.PyMem_RawFree PyMem_RawFree.restype = None PyMem_RawFree.argtypes = [ctypes.c_void_p] def allocate_memory(size: int) -> int: Allocates a block of memory of `size` elements and returns a pointer to the allocated memory. pointer = PyMem_RawMalloc(size) if not pointer: raise MemoryError(\\"Memory allocation failed\\") return pointer def reallocate_memory(ptr: int, new_size: int) -> int: Reallocates the previously allocated memory block pointed to by `ptr` to hold `new_size` elements and returns a new pointer to the reallocated memory. pointer = PyMem_RawRealloc(ptr, new_size) if not pointer: raise MemoryError(\\"Memory reallocation failed\\") return pointer def free_memory(ptr: int) -> None: Frees the memory block pointed to by `ptr`. PyMem_RawFree(ptr)"},{"question":"Data Visualization with pandas **Objective:** Write a Python function using pandas and its plotting module to visualize given datasets. Your function should demonstrate your understanding of some core plotting functions available in `pandas.plotting`. **Function Specification:** ```python def visualize_data(df): Visualizes the given DataFrame using various pandas plotting functions. Parameters: df (pandas.DataFrame): The input DataFrame containing the data to be visualized. Returns: None pass ``` **Requirements:** 1. **Scatter Matrix Plot:** - Create a scatter matrix plot for the numerical columns in the DataFrame. - Save the plot as `scatter_matrix.png`. 2. **Autocorrelation Plot:** - Create an autocorrelation plot for a column named `target` in the DataFrame. - Save the plot as `autocorrelation.png`. 3. **Parallel Coordinates Plot:** - Create a parallel coordinates plot for the DataFrame. Use a column named `class` to color by. - Save the plot as `parallel_coordinates.png`. 4. **Andrew\'s Curves Plot:** - Create Andrew\'s curves plot for the DataFrame. Use a column named `class` to color by. - Save the plot as `andrews_curves.png`. **Input Constraints:** - The DataFrame `df` will always contain the columns `target` and `class` alongside other numerical columns. - You can assume that there are no missing values in the DataFrame. **Output Requirements:** - Your function should not return anything but should save the plots as specified. **Evaluation Criteria:** - Correct implementation and usage of pandas plotting functions. - Proper handling of the DataFrame to create the required plots. - The correctness and clarity of the saved plots. Consider the following DataFrame example to test your function: ```python import pandas as pd data = { \'feature1\': [1.2, 2.3, 3.1, 4.8, 5.6], \'feature2\': [4.1, 3.6, 4.2, 5.9, 6.3], \'target\': [10, 20, 10, 30, 20], \'class\': [\'A\', \'B\', \'A\', \'B\', \'A\'] } df = pd.DataFrame(data) visualize_data(df) ``` Use this sample data to ensure your function works correctly.","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix, autocorrelation_plot, parallel_coordinates, andrews_curves def visualize_data(df): Visualizes the given DataFrame using various pandas plotting functions. Parameters: df (pandas.DataFrame): The input DataFrame containing the data to be visualized. Returns: None # Scatter Matrix Plot scatter_matrix(df, figsize=(10, 10)) plt.savefig(\'scatter_matrix.png\') plt.clf() # Autocorrelation Plot autocorrelation_plot(df[\'target\']) plt.savefig(\'autocorrelation.png\') plt.clf() # Parallel Coordinates Plot parallel_coordinates(df, class_column=\'class\') plt.savefig(\'parallel_coordinates.png\') plt.clf() # Andrew\'s Curves Plot andrews_curves(df, class_column=\'class\') plt.savefig(\'andrews_curves.png\') plt.clf()"},{"question":"Coding Assessment Question # Objective Implement a simplified version of a chat server using Python\'s `socket` module. This server should be able to handle multiple clients concurrently using non-blocking sockets and must support both IPv4 and IPv6 connections. The server should echo messages back to all connected clients. # Instructions 1. **Server Implementation (`chat_server.py`):** - Create a non-blocking TCP server that listens on a specified port and handles both IPv4 and IPv6 clients. - The server should be able to manage multiple client connections concurrently. - When a client sends a message, the server echoes this message to all connected clients, including the sender. - The server should properly handle socket timeouts and any potential exceptions. 2. **Client Implementation (`chat_client.py`):** - Create a TCP client that connects to the server using either IPv4 or IPv6. - The client should continuously read messages from the server and display them to the user. - The client should allow the user to send messages to the server. # Constraints - Use the `socket` module functionalities to handle sockets. Do not use any external libraries. - The server should handle at least 100 concurrent client connections. - Ensure that the server and client properly clean up resources (i.e., close sockets) on exit. - The server should log all connections and disconnections. # Expected Input and Output - **Server:** - Input: Port number to listen on. - Output: Logs of connections, disconnections, and messages. - **Client:** - Input: Server address (hostname or IP), Port number. - Output: User input messages sent to the server, messages from the server displayed to the user. # Example - Run the server: ```bash python chat_server.py 12345 ``` - Outputs: ``` Server started on port 12345... New connection from (\'::1\', 12346, 0, 0) Message from (\'::1\', 12346, 0, 0): Hello, everyone! New connection from (\'192.168.1.5\', 56789) Message from (\'192.168.1.5\', 56789): Hi there! ``` - Run the client: ```bash python chat_client.py localhost 12345 ``` - User inputs: ``` Hello, everyone! ``` - Outputs: ``` Message from server: Hello, everyone! Message from server: Hi there! ``` # Hints - Use the `select` module to handle multiple socket connections efficiently. - Ensure you handle potential exceptions like `OSError`, `ConnectionResetError`, etc. - Logging can be done using Python\'s `logging` module. # Submission Submit the following files: 1. `chat_server.py` 2. `chat_client.py` # Performance Requirements Ensure the server can efficiently handle multiple clients, adapt to connection changes dynamically, and maintain stable performance under load.","solution":"import socket import select import logging HOST = \'\' # Empty string denotes that the server can bind to any IP address. PORT = 12345 logging.basicConfig(level=logging.DEBUG, format=\'%(asctime)s - %(levelname)s - %(message)s\') def start_server(host, port): server_socket = socket.socket(socket.AF_INET6, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.setblocking(False) server_socket.bind((host, port)) server_socket.listen(5) logging.info(f\\"Server started on port {port}...\\") inputs = [server_socket] clients = {} try: while inputs: readable, _, exceptional = select.select(inputs, [], inputs) for s in readable: if s is server_socket: connection, client_address = s.accept() logging.info(f\\"New connection from {client_address}\\") connection.setblocking(False) inputs.append(connection) clients[connection] = client_address else: data = s.recv(1024) if data: message = f\\"Message from {clients[s]}: {data.decode()}\\" logging.info(message) for client in clients: client.send(data) else: logging.info(f\\"Closing connection {clients[s]}\\") if s in inputs: inputs.remove(s) s.close() del clients[s] for s in exceptional: logging.debug(f\\"Handling exceptional condition for {clients[s]}\\") if s in inputs: inputs.remove(s) s.close() del clients[s] except Exception as e: logging.error(f\\"Exception in server: {e}\\") finally: for s in inputs: s.close() if __name__ == \\"__main__\\": start_server(HOST, PORT)"},{"question":"**Question: Working with Meta Tensors** In this task, you will demonstrate your understanding of handling meta tensors in PyTorch by performing operations involving meta devices. You will need to construct and manipulate neural network models using these meta tensors. Specifically, your task involves the following steps: 1. **Load a meta tensor**: Create a random tensor and save it. Then load this tensor into memory using the `meta` device. 2. **Create a neural network**: Define a simple neural network with linear layers using meta tensors. 3. **Move and initialize**: Transfer the neural network from the meta device to a CPU device and manually initialize its parameters. # Step by Step Instructions: 1. Create a random tensor and save this tensor to disk using `torch.save`. 2. Load the saved tensor using `torch.load` with `map_location=\'meta\'`. 3. Define a simple neural network model using `torch.nn.Module` with the following architecture: - Input layer of size 10 to hidden layer of size 20. - Hidden layer of size 20 to output layer of size 1. 4. Construct the neural network within a meta device context using `torch.device(\'meta\')`. 5. Implement a method to convert this model to CPU and manually reinitialize its parameters using random values. # Constraints - Use PyTorch functions and modules as specified. - Ensure that the actual tensor data is not loaded until the final step of reinitializing on the CPU. # Expected Input and Output - Input: None (The network architecture and operations are predefined). - Output: Print statements showcasing the progression through meta tensor loading, meta model creation, and final CPU model with initialized parameters. # Example Code Outline ```python import torch import torch.nn as nn def main(): # Step 1: Create and save a random tensor rand_tensor = torch.randn(2) torch.save(rand_tensor, \'foo.pt\') # Step 2: Load the tensor on \'meta\' device meta_tensor = torch.load(\'foo.pt\', map_location=\'meta\') print(meta_tensor) # Step 3: Define a simple neural network model using meta tensors class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(10, 20) self.layer2 = nn.Linear(20, 1) def forward(self, x): x = self.layer1(x) x = self.layer2(x) return x with torch.device(\'meta\'): meta_model = SimpleNN() print(meta_model) # Step 4: Move to CPU and manually initialize parameters cpu_model = meta_model.to_empty(device=\\"cpu\\") for param in cpu_model.parameters(): param.data = torch.randn_like(param) print(cpu_model) if __name__ == \'__main__\': main() ``` **Extension (Optional):** - Implement a method to count the number of parameters in the model without moving data onto a CPU or GPU.","solution":"import torch import torch.nn as nn def main(): # Step 1: Create and save a random tensor rand_tensor = torch.randn(2) torch.save(rand_tensor, \'foo.pt\') # Step 2: Load the tensor on \'meta\' device meta_tensor = torch.load(\'foo.pt\', map_location=\'meta\') print(f\\"Meta tensor: {meta_tensor}\\") # Step 3: Define a simple neural network model using meta tensors class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(10, 20) self.layer2 = nn.Linear(20, 1) def forward(self, x): x = self.layer1(x) x = self.layer2(x) return x with torch.device(\'meta\'): meta_model = SimpleNN() print(\\"Meta model:\\", meta_model) # Step 4: Move to CPU and manually initialize parameters cpu_model = meta_model.to_empty(device=\\"cpu\\") for param in cpu_model.parameters(): param.data = torch.randn_like(param) print(\\"CPU model with initialized parameters:\\", cpu_model) return cpu_model if __name__ == \'__main__\': main()"},{"question":"**Objective: Implement a function using pandas that demonstrates advanced DataFrame manipulations, boolean operations, handling missing values, and memory usage reporting.** **Question:** Implement a function called `analyze_dataframe` that takes a dictionary of data, performs several operations on the data, and provides insights about the DataFrame. # Function Signature ```python def analyze_dataframe(data: dict) -> dict: pass ``` # Input: - `data`: A dictionary where keys are column names and values are lists of integers or floats representing column data. This dictionary will be converted to a pandas DataFrame. # Output: - A dictionary containing the following keys and their respective descriptions: - `\'initial_memory_usage\'`: Memory usage of the DataFrame after creation. - `\'memory_usage_after_dropping_nan\'`: Memory usage after dropping any rows containing NaN values. - `\'memory_usage_after_filling_nan\'`: Memory usage after filling NaN values with zeros. - `\'filtered_dataframe\'`: The DataFrame after filtering out rows where any column has a value greater than 100. - `\'mutated_dataframe\'`: The DataFrame after applying a UDF that adds 10 to each column value. # Constraints: 1. The DataFrame can contain missing values (NaNs) after initial creation. 2. Use the `memory_usage` parameter correctly to get a detailed and accurate memory usage report. 3. Use appropriate methods to filter and mutate the DataFrame. 4. Ensure proper handling of type promotions when introducing NaNs. # Example: ```python def analyze_dataframe(data): import pandas as pd import numpy as np # Step 1: Create DataFrame from data df = pd.DataFrame(data) # Step 2: Calculate initial memory usage initial_memory_usage = df.memory_usage(deep=True).sum() # Step 3: Drop rows with NaNs and calculate memory usage df_dropped_nan = df.dropna() memory_usage_after_dropping_nan = df_dropped_nan.memory_usage(deep=True).sum() # Step 4: Fill NaNs with zeros and calculate memory usage df_filled_nan = df.fillna(0) memory_usage_after_filling_nan = df_filled_nan.memory_usage(deep=True).sum() # Step 5: Filter out rows where any column value > 100 df_filtered = df[(df <= 100).all(axis=1)] # Step 6: Apply UDF to mutate DataFrame by adding 10 to each value def add_ten(x): return x + 10 df_mutated = df.applymap(add_ten) # Return results return { \'initial_memory_usage\': initial_memory_usage, \'memory_usage_after_dropping_nan\': memory_usage_after_dropping_nan, \'memory_usage_after_filling_nan\': memory_usage_after_filling_nan, \'filtered_dataframe\': df_filtered, \'mutated_dataframe\': df_mutated } # Test example data = { \'A\': [1, 2, 3, 4, np.nan], \'B\': [5, 6, np.nan, 8, 9], \'C\': [10, 110, 30, 40, 50] } result = analyze_dataframe(data) print(result) ``` **Note for Students:** - You should ensure proper type handling when dealing with NaN values. - The memory reports should be detailed and accurate. - Your function should be efficient and handle edge cases, such as empty DataFrames or DataFrames with all NaN values.","solution":"import pandas as pd import numpy as np def analyze_dataframe(data: dict) -> dict: # Step 1: Create DataFrame from data df = pd.DataFrame(data) # Step 2: Calculate initial memory usage initial_memory_usage = df.memory_usage(deep=True).sum() # Step 3: Drop rows with NaNs and calculate memory usage df_dropped_nan = df.dropna() memory_usage_after_dropping_nan = df_dropped_nan.memory_usage(deep=True).sum() # Step 4: Fill NaNs with zeros and calculate memory usage df_filled_nan = df.fillna(0) memory_usage_after_filling_nan = df_filled_nan.memory_usage(deep=True).sum() # Step 5: Filter out rows where any column value > 100 df_filtered = df[(df <= 100).all(axis=1)] # Step 6: Apply UDF to mutate DataFrame by adding 10 to each value def add_ten(x): return x + 10 df_mutated = df.applymap(add_ten) # Return results return { \'initial_memory_usage\': initial_memory_usage, \'memory_usage_after_dropping_nan\': memory_usage_after_dropping_nan, \'memory_usage_after_filling_nan\': memory_usage_after_filling_nan, \'filtered_dataframe\': df_filtered, \'mutated_dataframe\': df_mutated }"},{"question":"**Kernel Approximation and SVM Classification** In this exercise, you will demonstrate your understanding of kernel approximation methods from `scikit-learn` by implementing and evaluating the Nystroem method for kernel approximation. You are to use this approximation to preprocess a dataset before training a Support Vector Machine (SVM) classifier on it. # Task 1. Load the provided dataset. 2. Implement kernel approximation using the `Nystroem` method. 3. Train a linear SVM classifier on the transformed data. 4. Evaluate and compare the performance of the classifier trained on the approximated features to a classifier trained on the exact RBF kernel. # Requirements 1. **Input**: - A dataset consisting of feature matrix `X` and target vector `y`. You can use the Iris dataset from `sklearn.datasets`. 2. **Output**: - The performance metrics (accuracy) of the linear SVM classifier on the kernel-approximated data. - The performance comparison with the exact RBF kernel SVM. 3. **Constraints**: - Use a random subset of the data, with `n_components` significantly smaller than the total number of samples. - Report results using a consistent metric such as accuracy. # Implementation Steps 1. Load the Iris dataset: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.kernel_approximation import Nystroem from sklearn.metrics import accuracy_score data = load_iris() X, y = data.data, data.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) ``` 2. Apply Nystroem kernel approximation: ```python n_components = 50 # Choose n_components < number of samples nystroem = Nystroem(n_components=n_components, random_state=42) X_train_nystroem = nystroem.fit_transform(X_train) X_test_nystroem = nystroem.transform(X_test) ``` 3. Train a linear SVM on the transformed data: ```python from sklearn.linear_model import SGDClassifier linear_svm = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) linear_svm.fit(X_train_nystroem, y_train) y_pred_nystroem = linear_svm.predict(X_test_nystroem) accuracy_nystroem = accuracy_score(y_test, y_pred_nystroem) print(f\\"Accuracy with Nystroem kernel approximation: {accuracy_nystroem:.2f}\\") ``` 4. Train an exact RBF kernel SVM for comparison: ```python rbf_svm = SVC(kernel=\'rbf\', gamma=\'scale\', random_state=42) rbf_svm.fit(X_train, y_train) y_pred_rbf = rbf_svm.predict(X_test) accuracy_rbf = accuracy_score(y_test, y_pred_rbf) print(f\\"Accuracy with exact RBF kernel: {accuracy_rbf:.2f}\\") ``` # Expected Outputs - Final accuracy score of the linear SVM with Nystroem approximation. - Final accuracy score of the SVM classifier with the exact RBF kernel. - A comparison and discussion of the results focusing on the performances and computational advantages of using kernel approximation.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.kernel_approximation import Nystroem from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def kernel_approximation_and_svm_classification(): # Load dataset data = load_iris() X, y = data.data, data.target # Split dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Apply Nystroem kernel approximation n_components = 50 nystroem = Nystroem(n_components=n_components, random_state=42) X_train_nystroem = nystroem.fit_transform(X_train) X_test_nystroem = nystroem.transform(X_test) # Train a linear SVM on the approximated data linear_svm = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) linear_svm.fit(X_train_nystroem, y_train) y_pred_nystroem = linear_svm.predict(X_test_nystroem) accuracy_nystroem = accuracy_score(y_test, y_pred_nystroem) # Train an exact RBF kernel SVM rbf_svm = SVC(kernel=\'rbf\', gamma=\'scale\', random_state=42) rbf_svm.fit(X_train, y_train) y_pred_rbf = rbf_svm.predict(X_test) accuracy_rbf = accuracy_score(y_test, y_pred_rbf) return accuracy_nystroem, accuracy_rbf"},{"question":"Objective Implement a custom file reading function that utilizes the built-in `open()` function to read a file, but processes the file content according to specific requirements. Problem Statement You are required to create a module that reads the content of a file, converts all the text to lower case, and counts the occurrences of each word in the file. Your implementation should include a custom `open()` function that internally uses Python\'s built-in `open()` function. Function Specifications 1. **Function Name**: `custom_open` 2. **Input**: - `file_path` (string): The path to the file to be opened. - `mode` (string): The mode in which the file should be opened. For this task, it should always be `\'r\'` (read mode). 3. **Output**: Return an instance of a custom `WordCounter` class. 3. **Class Name**: `WordCounter` 4. **Methods**: - `__init__(self, file)`: Initializes the instance with a file object. - `read_and_count(self) -> dict`: Reads the content of the file, converts all text to lower case, and returns a dictionary where the keys are words and values are the counts of those words in the text. # Example Usage ```python import builtins def custom_open(file_path, mode=\'r\'): Custom open function that wraps the built-in open function. f = builtins.open(file_path, mode) if mode == \'r\': return WordCounter(f) else: raise ValueError(\'This custom open function only supports read mode.\') class WordCounter: Wrapper around a file that counts the occurrences of each word in the file. def __init__(self, file): self._file = file def read_and_count(self) -> dict: content = self._file.read() content = content.lower() words = content.split() word_count = {} for word in words: if word in word_count: word_count[word] += 1 else: word_count[word] = 1 return word_count # Example usage: # Assuming \'example.txt\' contains the text: \\"Hello world! Hello everyone.\\" word_counter = custom_open(\'example.txt\') print(word_counter.read_and_count()) # Output should be: {\'hello\': 2, \'world!\': 1, \'everyone.\': 1} ``` Constraints - The file will contain only text data. - The function should handle large files efficiently. - Handle cases where the file may not exist (i.e., raise appropriate exceptions). Hints - Use the `builtins.open` function inside your `custom_open` function. - Implement the functionality to convert text to lower case and split into words. - Count the occurrences of each word.","solution":"import builtins def custom_open(file_path, mode=\'r\'): Custom open function that wraps the built-in open function. f = builtins.open(file_path, mode) if mode == \'r\': return WordCounter(f) else: raise ValueError(\'This custom open function only supports read mode.\') class WordCounter: Wrapper around a file that counts the occurrences of each word in the file. def __init__(self, file): self._file = file def read_and_count(self) -> dict: Reads the content of the file, converts all text to lower case, and returns a dictionary where the keys are words and values are the counts of those words in the text. content = self._file.read() content = content.lower() words = content.split() word_count = {} for word in words: if word in word_count: word_count[word] += 1 else: word_count[word] = 1 return word_count"},{"question":"Objective The objective of this assessment is to evaluate your ability to create complex visualizations using the seaborn library. You will demonstrate your understanding of various seaborn functionalities including scatter plots, line plots, and the use of facet grids to visualize complex relationships in a dataset. Problem Statement Given the famous `tips` dataset included in seaborn, create the following visualizations and customize them as specified: 1. **Scatter Plot with Hue and Style Semantics** - Create a scatter plot displaying the relationship between `total_bill` and `tip`. - Use the `smoker` column to color the points (hue semantic). - Change the style of the points based on the `time` column. - Customize the palette to `coolwarm`. 2. **Line Plot Showing Trend Over Time** - Create a line plot using the `fmri` dataset. - Plot the average `signal` as a function of `timepoint`. - Add a `hue` semantic to differentiate the lines based on `event`. - Set the style of the lines according to the `region` column. - Disable error bars. 3. **Faceted Scatter Plot for Comprehensive Comparison** - Use the `tips` dataset to create a faceted scatter plot. - Plot the relationship between `total_bill` and `tip`, coloring the points according to `sex`. - Facet the plot columns by `time` and rows by `day`. - Ensure each facet has a height of 5 and an aspect ratio of 1. Guidelines - Use appropriate titles and labels for axes. - Ensure that each of the plots is clearly annotated and interpretable. - Provide a brief explanation (in comments) of any parameters and customizations you use. Constraints - Your code should be well-documented and follow best practices for code quality. - Ensure all plots are generated without errors. Input and Output - **Input:** No direct input. You will use seaborn\'s `tips` and `fmri` datasets. - **Output:** Three visualizations as specified above. Performance Requirements - Your code should execute efficiently without unnecessary computations. - The plots should be rendered clearly within a Jupyter Notebook or a script with inline visualization enabled. Example ```python import seaborn as sns import matplotlib.pyplot as plt # Example 1: Simple scatter plot with custom palette tips = sns.load_dataset(\'tips\') sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'smoker\', style=\'time\', palette=\'coolwarm\') plt.title(\'Scatter plot of Total Bill vs Tip\') plt.show() # Example 2: Simple line plot with multiple semantics fmri = sns.load_dataset(\'fmri\') sns.lineplot(data=fmri, x=\'timepoint\', y=\'signal\', hue=\'event\', style=\'region\') plt.title(\'Line plot of Signal over Timepoint\') plt.show() # Example 3: Faceted scatter plot sns.relplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'sex\', col=\'time\', row=\'day\', height=5, aspect=1) plt.title(\'Faceted Scatter plot of Total Bill vs Tip\') plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_scatter_plot(): # Load the \'tips\' dataset tips = sns.load_dataset(\'tips\') # Create scatter plot with specified customizations scatter_plot = sns.scatterplot( data=tips, x=\'total_bill\', y=\'tip\', hue=\'smoker\', style=\'time\', palette=\'coolwarm\' ) scatter_plot.set_title(\'Scatter plot of Total Bill vs Tip\') scatter_plot.set_xlabel(\'Total Bill\') scatter_plot.set_ylabel(\'Tip\') plt.legend(title=\'Smoker / Time\') plt.show() def create_line_plot(): # Load the \'fmri\' dataset fmri = sns.load_dataset(\'fmri\') # Create line plot with specified customizations line_plot = sns.lineplot( data=fmri, x=\'timepoint\', y=\'signal\', hue=\'event\', style=\'region\', ci=None ) line_plot.set_title(\'Line plot of Signal over Timepoint\') line_plot.set_xlabel(\'Timepoint\') line_plot.set_ylabel(\'Signal\') plt.legend(title=\'Event / Region\') plt.show() def create_faceted_scatter_plot(): # Load the \'tips\' dataset tips = sns.load_dataset(\'tips\') # Create faceted scatter plot with specified customizations faceted_scatter_plot = sns.relplot( data=tips, x=\'total_bill\', y=\'tip\', hue=\'sex\', col=\'time\', row=\'day\', height=5, aspect=1, ) faceted_scatter_plot.set_titles(\'Time: {col_name} | Day: {row_name}\') faceted_scatter_plot.set_xlabels(\'Total Bill\') faceted_scatter_plot.set_ylabels(\'Tip\') plt.show()"},{"question":"<|Analysis Begin|> The `nntplib` module in Python offers an interface to the Network News Transfer Protocol (NNTP), which is used to read, post, and manage articles on news servers. Although the module is deprecated since Python 3.11, it still contains a variety of functionalities that are essential for interacting with NNTP servers following standards like RFC 977, RFC 2980, and RFC 3977. The primary class within this module is `NNTP`, and its SSL counterpart `NNTP_SSL`, which provide methods to connect to NNTP servers and perform actions like listing newsgroups, retrieving articles, posting articles, and managing server communications. Key concepts necessary for understanding and utilizing this module include: 1. **Connection Management**: Establishing a connection with the NNTP server using the NNTP or NNTP_SSL class. 2. **Authentication**: Using AUTHINFO commands for user login if required. 3. **Newsgroup Operations**: Commands like `group`, `list`, `newgroups`, `newnews`, `over`, and more to retrieve and manage articles and newsgroups. 4. **Article Operations**: Commands like `article`, `head`, `body`, `post`, and `ihave` to perform specific article operations. 5. **Error Handling**: Understanding the exceptions like `NNTPReplyError`, `NNTPTemporaryError`, `NNTPPermanentError`, and more to handle various error scenarios appropriately. Given these functionalities, we can design a coding assessment question that targets the students\' abilities to manage connections, retrieve specific articles, handle errors, and iterate and process data. <|Analysis End|> <|Question Begin|> You are required to implement a function in Python that connects to an NNTP server, retrieves the titles and authors of the last N articles from a specified newsgroup, and manages any NNTP related exceptions that might occur during the process. # Function Signature ```python def fetch_recent_articles(server: str, newsgroup: str, count: int, user: str = None, password: str = None) -> list: Connects to an NNTP server, retrieves the titles and authors of the last `count` articles from `newsgroup`. Parameters: server (str): The address of the NNTP server. newsgroup (str): The name of the newsgroup. count (int): The number of recent articles to retrieve. user (str, optional): The username for authentication. Defaults to None. password (str, optional): The password for authentication. Defaults to None. Returns: list: A list of tuples where each tuple contains the article number, title, and author of the articles. Example: [(1086, \\"Title 1\\", \\"Author 1\\"), (1087, \\"Title 2\\", \\"Author 2\\")] Raises: NNTPPermanentError: If an error with a response code in the range 500--599 occurs. NNTPTemporaryError: If an error with a response code in the range 400--499 occurs. ``` # Constraints and Requirements 1. Use the `nntplib` module. 2. Handle exceptions like `NNTPPermanentError` and `NNTPTemporaryError` using appropriate try-except blocks. 3. Decode any headers that might contain non-ASCII characters. 4. Assume articles might not have all headers, and handle such cases gracefully. # Example Usage ```python # Example usage of the function server = \'news.gmane.io\' newsgroup = \'gmane.comp.python.committers\' count = 5 try: recent_articles = fetch_recent_articles(server, newsgroup, count) for article in recent_articles: print(f\\"Article Number: {article[0]}, Title: {article[1]}, Author: {article[2]}\\") except Exception as e: print(f\\"An error occurred: {str(e)}\\") ``` You will need to handle: - Connecting to the server. - Selecting the appropriate newsgroup. - Fetching the relevant article metadata. - Decoding the necessary headers to extract readable titles and authors. - Handling any exceptions, particularly those specific to the nntplib module. # Note Make sure to safely close the connection to the server once all operations are completed, either using the `with` statement or explicitly calling the `quit()` method.","solution":"import nntplib from email.parser import HeaderParser from email.header import decode_header def fetch_recent_articles(server, newsgroup, count, user=None, password=None): Connects to an NNTP server, retrieves the titles and authors of the last `count` articles from `newsgroup`. Parameters: server (str): The address of the NNTP server. newsgroup (str): The name of the newsgroup. count (int): The number of recent articles to retrieve. user (str, optional): The username for authentication. Defaults to None. password (str, optional): The password for authentication. Defaults to None. Returns: list: A list of tuples where each tuple contains the article number, title, and author of the articles. Example: [(1086, \\"Title 1\\", \\"Author 1\\"), (1087, \\"Title 2\\", \\"Author 2\\")] Raises: NNTPPermanentError: If an error with a response code in the range 500--599 occurs. NNTPTemporaryError: If an error with a response code in the range 400--499 occurs. try: with nntplib.NNTP(server) as nntp: if user and password: resp, auth_resp = nntp.login(user, password) resp, count_, first, last, name = nntp.group(newsgroup) start = max(int(last) - count + 1, int(first)) resp, overviews = nntp.over((start, last)) header_parser = HeaderParser() articles = [] for article in overviews: art_num = article[\'number\'] header_lines = \'n\'.join(article[\'lines\']) headers = header_parser.parsestr(header_lines) def decode_header_field(header): decoded_parts = decode_header(header) return \' \'.join([part.decode(enc) if isinstance(part, bytes) else part for part, enc in decoded_parts]) title = headers[\'subject\'] if title: title = decode_header_field(title) author = headers[\'from\'] if author: author = decode_header_field(author) articles.append((art_num, title, author)) return articles except nntplib.NNTPPermanentError as e: raise e except nntplib.NNTPTemporaryError as e: raise e except Exception as e: raise RuntimeError(f\\"An unexpected error occurred: {str(e)}\\")"},{"question":"You are tasked with creating a fault management system for a Python application that leverages the functionality provided by the `faulthandler` module. This fault management system should be able to handle immediate faults, schedule traceback dumps after a timeout, and manage user-defined signals effectively. # Requirements: 1. **Immediate Fault Handling**: - Enable fault handling for the application. - Ensure that tracebacks are logged into a specified log file. 2. **Scheduled Traceback Dump**: - Schedule a traceback dump to occur 5 seconds after starting the application. - Ensure that the scheduled dump happens continuously every 5 seconds. 3. **User-Defined Signal Handling**: - Register a custom signal handler for a user-defined signal (e.g., `signal.SIGUSR1` on Unix systems). - Ensure that the custom signal handler logs tracebacks into the specified log file. # Function Definitions: You need to implement the following functions: 1. `enable_fault_handling(log_file_path: str) -> None`: - **Input**: - `log_file_path` (str): Path to the log file where tracebacks should be written. - **Output**: - None - **Functionality**: Enables fault handling and directs tracebacks to the specified log file. 2. `schedule_traceback_dump(timeout: int, log_file_path: str) -> None`: - **Input**: - `timeout` (int): Timeout in seconds after which tracebacks should be dumped. - `log_file_path` (str): Path to the log file where tracebacks should be written. - **Output**: - None - **Functionality**: Schedules a traceback dump to occur every `timeout` seconds and writes tracebacks to the specified log file. 3. `enable_signal_handling(signum: int, log_file_path: str) -> None`: - **Input**: - `signum` (int): Signal number to register the custom signal handler for. - `log_file_path` (str): Path to the log file where tracebacks should be written. - **Output**: - None - **Functionality**: Registers a custom signal handler that logs tracebacks when the specified signal is received. 4. `main() -> None`: - **Input**: - None - **Output**: - None - **Functionality**: Main function that calls the above functions to set up the fault management system. # Constraints: - Assume the log file path provided will always be valid. - Handle any potential exceptions that might occur during fault handling or signal registration. - Ensure tracebacks from all threads are logged. # Example: ```python import faulthandler import signal import time # Implement the function enable_fault_handling(log_file_path) here # Implement the function schedule_traceback_dump(timeout, log_file_path) here # Implement the function enable_signal_handling(signum, log_file_path) here def main(): log_file_path = \\"fault_handler.log\\" enable_fault_handling(log_file_path) schedule_traceback_dump(5, log_file_path) enable_signal_handling(signal.SIGUSR1, log_file_path) # The application will run indefinitely for demonstration purposes while True: time.sleep(1) if __name__ == \\"__main__\\": main() ``` Ensure your implementation meets the requirements and handles possible exceptions appropriately. Test it thoroughly in a safe, controlled environment to avoid disrupting your local system.","solution":"import faulthandler import signal import time import threading def enable_fault_handling(log_file_path: str) -> None: Enables fault handling and directs tracebacks to the specified log file. Args: log_file_path (str): Path to the log file where tracebacks should be written. Returns: None try: with open(log_file_path, \'w\') as log_file: faulthandler.enable(log_file) except (FileNotFoundError, IOError) as e: print(f\\"Failed to enable fault handling: {e}\\") def schedule_traceback_dump(timeout: int, log_file_path: str) -> None: Schedules a traceback dump to occur every `timeout` seconds and writes tracebacks to the specified log file. Args: timeout (int): Timeout in seconds after which tracebacks should be dumped. log_file_path (str): Path to the log file where tracebacks should be written. Returns: None def dump_traceback(): while True: time.sleep(timeout) try: with open(log_file_path, \'a\') as log_file: faulthandler.dump_traceback(file=log_file, all_threads=True) except (FileNotFoundError, IOError) as e: print(f\\"Failed to dump traceback: {e}\\") thread = threading.Thread(target=dump_traceback) thread.daemon = True thread.start() def enable_signal_handling(signum: int, log_file_path: str) -> None: Registers a custom signal handler that logs tracebacks when the specified signal is received. Args: signum (int): Signal number to register the custom signal handler for. log_file_path (str): Path to the log file where tracebacks should be written. Returns: None def signal_handler(signum, frame): try: with open(log_file_path, \'a\') as log_file: faulthandler.dump_traceback(file=log_file, all_threads=True) except (FileNotFoundError, IOError) as e: print(f\\"Failed to handle signal {signum}: {e}\\") signal.signal(signum, signal_handler) def main() -> None: log_file_path = \\"fault_handler.log\\" enable_fault_handling(log_file_path) schedule_traceback_dump(5, log_file_path) enable_signal_handling(signal.SIGUSR1, log_file_path) # The application will run indefinitely for demonstration purposes while True: time.sleep(1) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Seaborn Plotting with `so.Dodge` and Other Transformations Objective: Create a complex bar plot using seaborn that demonstrates the student\'s understanding of several transformations, specifically handling overlaps with `so.Dodge`. Task: Write a Python function `create_complex_plot(data)` that takes a pandas DataFrame `data`, and generates a seaborn plot with the following requirements: 1. **Dataset:** Use any built-in dataset from seaborn suitable for plotting. 2. **Bar Plot:** Create a bar plot where: - The x-axis represents a categorical variable. - Bars are colored by another categorical variable. - Use `so.Dodge` to handle overlapping data points, with the following specifications: - `empty=\\"drop\\"` to center the bars. - Add a gap of 0.1 between the bars. 3. **Additional Transformations:** - Implement an additional transformation like `so.Jitter()` to slightly move the positions of the data points. 4. **Distinct Groups:** Ensure that if the dataset includes multiple semantic variables, they are properly dodged and displayed. Function Signature: ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd def create_complex_plot(data: pd.DataFrame) -> None: pass ``` Constraints: - The function should not return any value. It should simply display the plot. - Ensure proper error handling to manage datasets that may not fit all requirements. Example Usage: ```python data = load_dataset(\\"tips\\") create_complex_plot(data) ``` Expected Output: A seaborn plot that meets the above specifications and visually represents the data in a clear and informative way.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd import matplotlib.pyplot as plt def create_complex_plot(data: pd.DataFrame) -> None: Creates a complex bar plot using seaborn showing the use of so.Dodge and additional transformations. Parameters: data (pd.DataFrame): DataFrame containing the data to be used for the plot. # Check if the necessary columns exist in the data if \'day\' not in data.columns or \'sex\' not in data.columns or \'total_bill\' not in data.columns: raise ValueError(\\"The input data must contain \'day\', \'sex\', and \'total_bill\' columns.\\") # Creating the plot p = (so.Plot(data, x=\'day\', y=\'total_bill\', color=\'sex\') .add(so.Bar(), so.Dodge(empty=\'drop\', gap=0.1)) .add(so.Dot(), so.Jitter(0.05))) # Display the plot p.show() # Example usage: if __name__ == \\"__main__\\": data = load_dataset(\\"tips\\") create_complex_plot(data)"},{"question":"**Objective:** Demonstrate understanding of parallelism in scikit-learn and optimize a model\'s performance using appropriate configurations. **Problem Statement:** You are given a dataset containing features of houses and their prices. Your task is to implement a training pipeline using scikit-learn\'s `RandomForestRegressor` model. The objective is to optimize the training process using parallelism efficiently. **Requirements:** 1. Implement a `RandomForestRegressor` model with the following constraints: - Use `n_estimators` = 100. - Use cross-validation with 5 folds. 2. Optimize the training process by configuring parallelism using the `joblib` backend and environment variables. 3. Ensure that the number of threads does not exceed the available CPU cores to avoid oversubscription. 4. Measure the time taken to fit the model and evaluate its performance. **Input:** - A DataFrame `df` with feature columns and a target column `price`. **Output:** - Print the time taken to train the model. - Print the cross-validation score of the model. **Constraints:** - Assume the system has 8 CPU cores. - Use appropriate values for `n_jobs`, `OMP_NUM_THREADS`, and other relevant environment variables. **Example Usage:** ```python import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import cross_val_score import time import os # Example DataFrame data = { \'feature1\': [1, 2, 3, 4, 5], \'feature2\': [5, 4, 3, 2, 1], \'price\': [250000, 300000, 400000, 500000, 450000] } df = pd.DataFrame(data) # Solution here # Output: # Time taken to train the model: X.XX seconds # Cross-validation score: [0.89, 0.87, 0.90, 0.88, 0.86] ``` **Hint:** Use `from joblib import parallel_backend` and `threadpoolctl` to manage the parallel backends and thread pools. Ensure the configuration is set before fitting the model. --- You can assume the basic familiarity with scikit-learn\'s estimator fitting and evaluation mechanisms. The focus should be on demonstrating the understanding and application of parallel configurations to optimize performance.","solution":"import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import cross_val_score import time import os from joblib import parallel_backend def train_model(df): Trains a RandomForestRegressor on the provided DataFrame and optimizes training using parallelism. Parameters: df (pd.DataFrame): DataFrame containing the features and target variable \'price\'. Returns: None # Set environment variables for optimization os.environ[\\"OMP_NUM_THREADS\\"] = \\"1\\" # Avoid oversubscription os.environ[\\"MKL_NUM_THREADS\\"] = \\"1\\" os.environ[\\"OPENBLAS_NUM_THREADS\\"] = \\"1\\" os.environ[\\"NUMEXPR_NUM_THREADS\\"] = \\"1\\" # Variables n_estimators = 100 n_jobs = 8 # Assuming the system has 8 CPU cores # Define features and target X = df.drop(columns=[\'price\']) y = df[\'price\'] # Define the model model = RandomForestRegressor(n_estimators=n_estimators, n_jobs=n_jobs) # Measure the time taken to cross-validate the model start_time = time.time() with parallel_backend(\'threading\'): scores = cross_val_score(model, X, y, cv=5) end_time = time.time() # Print the time taken and cross-validation scores print(f\\"Time taken to train the model: {end_time - start_time:.2f} seconds\\") print(f\\"Cross-validation scores: {scores}\\") # Sample DataFrame data = { \'feature1\': [1, 2, 3, 4, 5], \'feature2\': [5, 4, 3, 2, 1], \'price\': [250000, 300000, 400000, 500000, 450000] } df = pd.DataFrame(data) # Example usage train_model(df)"},{"question":"Objective Implement a Python function that utilizes set and frozenset operations efficiently to solve a given problem. You are to demonstrate your comprehension of fundamental and advanced concepts of \\"set\\" and \\"frozenset\\" in Python. Problem Statement Design a function `unique_elements_union(sets: List[List[int]]) -> Set[int]` that takes a list of lists as input, where each sublist represents a set of integers. The function should return a set containing all unique elements from the union of these sublists. Additionally, if any sublist has duplicate elements, it should be converted into a frozenset before taking the union. Input Format - `sets`: A list of lists, where each sublist contains integers. For example, `[[1, 2, 2, 3], [3, 4, 5], [5, 6]]`. Output Format - A set containing all unique elements from the union of the input lists. For example, `{1, 2, 3, 4, 5, 6}`. Constraints 1. The input list `sets` contains at most (10^4) sublists. 2. Each sublist contains at most (10^3) integers. 3. Each integer in the sublists is in the range ([-10^6, 10^6]). Requirements - The function should handle large input sizes efficiently. - Proper error handling should be implemented to manage non-iterable inputs or other edge cases. Function Signature ```python from typing import List, Set def unique_elements_union(sets: List[List[int]]) -> Set[int]: pass ``` Example ```python # Test case 1 input_sets = [[1, 2, 2, 3], [3, 4, 5], [5, 6]] output = unique_elements_union(input_sets) print(output) # Output: {1, 2, 3, 4, 5, 6} # Test case 2 input_sets = [[10, 20, 30, 20], [30, 40, 50], [50, 60, 70]] output = unique_elements_union(input_sets) print(output) # Output: {10, 20, 30, 40, 50, 60, 70} ``` Notes - Remember to convert sublists with duplicate elements to frozenset for proper handling. - The function should be robust enough to handle potential error scenarios as described in the input format.","solution":"from typing import List, Set def unique_elements_union(sets: List[List[int]]) -> Set[int]: Returns a set containing all unique elements from the union of the input lists. Each sublist is converted to a frozenset before taking the union if it has duplicate elements. union_set = set() for sublist in sets: frozenset_sublist = frozenset(sublist) # Convert sublist to a frozenset union_set.update(frozenset_sublist) # Update the union set with elements from the frozenset return union_set"},{"question":"**Objective:** Implement a custom descriptor in Python that enforces both value constraints (like validators) and transformation (like converting strings to uppercase). **Problem Statement:** You need to create a custom descriptor `TransformAndValidate` that ensures: 1. The attribute value is always a string. 2. The string is transformed to upper case on assignment. 3. The string has a length within a specified minimum and maximum length. You will then use this descriptor in a `Person` class to control the `name` attribute. **Requirements:** 1. Implement the `TransformAndValidate` descriptor. 2. Ensure the `TransformAndValidate` descriptor has the following methods: - `__set_name__`: To set the private and public names of the attribute. - `__get__`: To return the attribute value. - `__set__`: To set the attribute value after transforming and validating it. **Input and Output Formats:** - **The Descriptor Class:** ```python class TransformAndValidate: def __init__(self, minsize=None, maxsize=None): # Initialize the descriptor with minsize and maxsize constraints def __set_name__(self, owner, name): # Set the private and public attribute names def __get__(self, instance, owner): # Retrieve the attribute value def __set__(self, instance, value): # Set the transformed and validated attribute value ``` - **The Person Class:** ```python class Person: name = TransformAndValidate(minsize=3, maxsize=10) def __init__(self, name): self.name = name ``` **Constraints:** - `minsize` and `maxsize` are non-negative integers. - The string must be between `minsize` and `maxsize` inclusive. - If the constraints are violated, raise a `ValueError`. **Example Usage:** ```python # Valid case p = Person(\\"tom\\") print(p.name) # Output: \\"TOM\\" # Error case try: p.name = \\"to\\" except ValueError as e: print(e) # Output: \\"Expected \'to\' to be at least 3 characters.\\" try: p.name = \\"a very long name\\" except ValueError as e: print(e) # Output: \\"Expected \'a very long name\' to be no longer than 10 characters.\\" ``` **Additional Notes:** - Ensure the `TransformAndValidate` descriptor logs access using the `logging` module. - Store the actual string in a private attribute (`_name` for `name`). ```python # Your implementation here import logging logging.basicConfig(level=logging.INFO) class TransformAndValidate: def __init__(self, minsize=None, maxsize=None): self.minsize = minsize self.maxsize = maxsize def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, instance, owner): value = getattr(instance, self.private_name, None) logging.info(\'Accessing %r giving %r\', self.public_name, value) return value def __set__(self, instance, value): if not isinstance(value, str): raise ValueError(f\'Expected {value!r} to be a str\') if self.minsize is not None and len(value) < self.minsize: raise ValueError(f\'Expected {value!r} to be at least {self.minsize} characters.\') if self.maxsize is not None and len(value) > self.maxsize: raise ValueError(f\'Expected {value!r} to be no longer than {self.maxsize} characters.\') value = value.upper() logging.info(\'Updating %r to %r\', self.public_name, value) setattr(instance, self.private_name, value) class Person: name = TransformAndValidate(minsize=3, maxsize=10) def __init__(self, name): self.name = name ``` Use this code to implement and validate your solution.","solution":"import logging logging.basicConfig(level=logging.INFO) class TransformAndValidate: def __init__(self, minsize=None, maxsize=None): self.minsize = minsize self.maxsize = maxsize def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, instance, owner): value = getattr(instance, self.private_name, None) logging.info(\'Accessing %r giving %r\', self.public_name, value) return value def __set__(self, instance, value): if not isinstance(value, str): raise ValueError(f\'Expected {value!r} to be a str\') if self.minsize is not None and len(value) < self.minsize: raise ValueError(f\'Expected {value!r} to be at least {self.minsize} characters.\') if self.maxsize is not None and len(value) > self.maxsize: raise ValueError(f\'Expected {value!r} to be no longer than {self.maxsize} characters.\') value = value.upper() logging.info(\'Updating %r to %r\', self.public_name, value) setattr(instance, self.private_name, value) class Person: name = TransformAndValidate(minsize=3, maxsize=10) def __init__(self, name): self.name = name"},{"question":"# Question: Minimal Reproducible Example for a Model Warning Background You are working on a machine learning project and encounter a warning message when training a model with scikit-learn\'s `GradientBoostingRegressor`. Your task is to generate a minimal, reproducible example that still triggers the warning. The warning encountered is: ``` UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names ``` Task 1. Create synthetic data that when used to fit a `GradientBoostingRegressor` model, produces the above warning. 2. Your solution must be minimal and self-contained. 3. Use any of the synthetic data generation methods shown in the documentation, like numpy arrays or scikit-learn\'s sample generators. Input You do not need to handle any user input for this task. Output Your script should: 1. Print the warning message mentioned above. 2. Ensure the script is formatted well, with proper imports and easy to copy-paste. Constraints 1. You must reproduce the exact warning message provided above. 2. The code should be concise and avoid unnecessary steps. Example Here is an example of a code snippet to guide you: ```python import pandas as pd from sklearn.ensemble import GradientBoostingRegressor df = pd.DataFrame( { \\"feature_name\\": [-12.32, 1.43, 30.01, 22.17], \\"target\\": [72, 55, 32, 43], } ) X = df[[\\"feature_name\\"]] y = df[\\"target\\"] gbdt = GradientBoostingRegressor() gbdt.fit(X, y) # no warning gbdt = GradientBoostingRegressor(n_iter_no_change=5) gbdt.fit(X, y) # raises warning ``` **Note:** This example is an illustration; your job is to ensure that your code triggers the specific warning and meets the guidelines mentioned. Evaluation Criteria Your solution will be evaluated based on: 1. Conciseness and readability of the code. 2. Correct reproduction of the warning message. 3. Appropriate use of synthetic data generation tools where necessary. Good luck!","solution":"import numpy as np import pandas as pd from sklearn.ensemble import GradientBoostingRegressor # Create synthetic data X = np.array([[1.0], [2.0], [3.0], [4.0], [5.0]]) y = np.array([1.1, 1.9, 3.1, 4.1, 5.0]) # Convert X to DataFrame to include feature names X_df = pd.DataFrame(X, columns=[\\"feature\\"]) # Fit the model model = GradientBoostingRegressor() model.fit(X, y) # Attempt to resurface the warning by fitting the model with DataFrame having feature names y_pred = model.predict(X_df)"},{"question":"# AsyncIO Subprocess Management Task **Objective:** Create an asynchronous Python function `run_subprocesses(commands: List[str]) -> List[str]` that takes a list of shell commands, runs them concurrently as subprocesses using asyncio, and returns their stdout results. **Function Signature:** ```python async def run_subprocesses(commands: List[str]) -> List[str]: ``` **Input:** - `commands`: A list of strings, where each string is a shell command to be executed (e.g., `[\\"ls\\", \\"echo \'Hello, World!\'\\"]`). **Output:** - Returns a list of strings. Each string contains the stdout output of the corresponding command in the input list. If a command produces no stdout or fails with an error, return an empty string for that command. **Constraints:** - The function should run all subprocesses concurrently. - Use `asyncio.create_subprocess_shell()` for running shell commands. - Stdout for each command should be captured using a pipe. - Handle errors by capturing stderr and returning an empty string for commands that fail. - Ensure proper cleanup and termination of subprocesses to avoid orphan processes. **Example Usage:** ```python import asyncio from typing import List async def run_subprocesses(commands: List[str]) -> List[str]: results = [] process_list = [] async def run(cmd): try: proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() if proc.returncode == 0: return stdout.decode() else: return \\"\\" except Exception as e: return \\"\\" # Create tasks for each command tasks = [asyncio.create_task(run(cmd)) for cmd in commands] # Gather all the results concurrently results = await asyncio.gather(*tasks) return results # Example commands to run commands = [\\"ls\\", \\"echo \'Hello, World!\'\\"] output = asyncio.run(run_subprocesses(commands)) # Expected output: (the results may vary based on current directory files) # [\\"list of files in the current directory\\", \\"Hello, World!\\"] print(output) ``` **Note:** - Make sure to handle the scenario where a command might fail and return appropriate results. - Ensure that the function is efficient and handles concurrency effectively using asyncio.","solution":"import asyncio from typing import List async def run_subprocesses(commands: List[str]) -> List[str]: results = [] async def run(cmd: str) -> str: try: proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() if proc.returncode == 0: return stdout.decode() else: return \\"\\" except Exception: return \\"\\" # Create tasks for each command tasks = [asyncio.create_task(run(cmd)) for cmd in commands] # Gather all the results concurrently results = await asyncio.gather(*tasks) return results"},{"question":"# URL Manipulation Task Objective: The goal of this task is to evaluate your ability to work with the `urllib.parse` module to parse, manipulate, and reconstruct URLs and their components effectively. Problem Statement: You are tasked with writing a Python function `sanitize_and_normalize_url(url, query_removals, query_additions)` that: 1. **Parses** the given `url` into its components. 2. **Removes** specified query parameters listed in `query_removals`. 3. **Adds or updates** specified query parameters based on the dictionary `query_additions`. 4. **SANITIZES** the resulting URL by ensuring special characters are properly quoted. 5. **Combines** the modified components back into a valid URL string. Function Signature: ```python def sanitize_and_normalize_url(url: str, query_removals: list, query_additions: dict) -> str: ``` Parameters: - `url` (str): The full URL to be sanitized and normalized. - `query_removals` (list of str): A list of query parameter keys to be removed. - `query_additions` (dict): A dictionary of query parameter keys and values to be added or updated. Returns: - A sanitized and normalized URL string. Example: ```python url = \\"http://example.com/path/to/resource?name=test&age=25#section\\" query_removals = [\\"age\\"] query_additions = {\\"gender\\": \\"m\\", \\"age\\": \\"30\\"} result = sanitize_and_normalize_url(url, query_removals, query_additions) print(result) # Output: \'http://example.com/path/to/resource?name=test&gender=m&age=30#section\' ``` Constraints: - The function should validate the input URL and ensure it is properly sanitized. - Query removals should handle non-existent keys gracefully. - Query additions can replace existing parameters if they are already present and should quote special characters where required. Notes: - Use the provided `urllib.parse` module and its functions to handle parsing, quoting, and constructing the URL. - Ensure the resulting URL conforms to RFC standards and is encoded properly. Testing: - Add your own test cases covering various edge cases and normal scenarios. - Consider cases with no query parameters, multiple removals, multiple additions, and special characters in URLs.","solution":"import urllib.parse def sanitize_and_normalize_url(url, query_removals, query_additions): Parses the given URL, removes specified query parameters, adds/updates specified query parameters, sanitizes the final URL and returns it. # Parse the URL parsed_url = urllib.parse.urlparse(url) # Parse the query parameters into a dictionary query_params = urllib.parse.parse_qs(parsed_url.query) # Remove specified query parameters for param in query_removals: query_params.pop(param, None) # Add/update specified query parameters for param, value in query_additions.items(): query_params[param] = [value] # Construct the query string query_string = urllib.parse.urlencode(query_params, doseq=True) # Construct the final URL sanitized_url = urllib.parse.urlunparse(parsed_url._replace(query=query_string)) return sanitized_url"},{"question":"# Objective The goal of this coding assessment is to evaluate your understanding of PyTorch\'s meta device and your ability to manipulate tensors using meta tensors. # Task You are provided with a neural network model on your local disk, saved as `\'model.pt\'`. Your task is to perform the following operations: 1. Load the model onto the meta device. 2. Analyze the architecture of the loaded model (list its parameters and their shapes). 3. Transfer the model back to CPU, leaving all parameters uninitialized. 4. Reinitialize the parameters and serialize the initialized model back to disk. # Your solution should implement the following function: ```python import torch import torch.nn as nn def process_meta_model(model_path: str, output_path: str) -> None: Process a PyTorch model using the meta device. Args: model_path (str): Path to the input model file. output_path (str): Path where the reinitialized model will be saved. Returns: None # Write your code here ``` # Detailed Steps: 1. **Load the model onto the meta device**: - Use `torch.load` with `map_location=\'meta\'` to load the model. 2. **Analyze the architecture**: - List all parameters of the model and their shapes. Print this information to standard output. 3. **Transfer the model back to CPU (uninitialized)**: - Use the `to_empty` method to transfer the model back to the CPU, leaving parameters uninitialized. 4. **Reinitialize the parameters**: - For simplicity, reinitialize the parameters to all ones using `torch.nn.init.ones_`. 5. **Serialize the reinitialized model**: - Save the reinitialized model to the specified `output_path` using `torch.save`. Here is an example of how the function can be used: ```python process_meta_model(\'model.pt\', \'reinitialized_model.pt\') ``` # Constraints - Assume the model file `\'model.pt\'` is available and contains a valid PyTorch model. - Ensure that all operations are performed without putting unnecessary load on memory by leveraging the meta device. # Example Output When analyzing the model architecture, your function should print statements like: ``` Parameter: weight, Shape: torch.Size([10, 20]) Parameter: bias, Shape: torch.Size([10]) ... ``` This output helps verify that model parameters have been correctly identified during the analysis phase.","solution":"import torch import torch.nn as nn import torch.optim as optim def process_meta_model(model_path: str, output_path: str) -> None: Process a PyTorch model using the meta device. Args: model_path (str): Path to the input model file. output_path (str): Path where the reinitialized model will be saved. Returns: None # Load the model onto the meta device model = torch.load(model_path, map_location=\'meta\') # Analyze the architecture and print parameters and their shapes for name, param in model.named_parameters(): print(f\\"Parameter: {name}, Shape: {param.shape}\\") # Transfer the model back to CPU (uninitialized) model_cpu = model.to_empty(device=\'cpu\') # Reinitialize the parameters to ones for param in model_cpu.parameters(): nn.init.ones_(param) # Serialize the reinitialized model torch.save(model_cpu, output_path)"},{"question":"**Objective**: Implement a Python function that simulates some aspects of the buffer protocol, focusing on managing the underlying buffer and providing access to it in a controlled manner. **Problem Statement**: You need to create a class `CustomBuffer` that simulates a simple version of the buffer protocol. The class should manage a buffer and allow: 1. Access to the buffer in a read-only manner. 2. Access to the buffer with write permissions (if allowed). 3. Release of the buffer when it is no longer needed. The class should also keep track of the reference count and ensure that releasing the buffer reduces the reference correctly. **Class definition**: ```python class CustomBuffer: def __init__(self, buffer, readonly=False): Initialize the buffer object. :param buffer: The underlying buffer (a bytearray or bytes object). :param readonly: A boolean indicating if the buffer is read-only. pass def acquire_buffer(self): Increment the reference count by one and return the buffer. :return: The internal buffer. pass def release_buffer(self): Decrement the reference count by one. If the reference count is zero, set the buffer to None. pass def is_readonly(self): Check if the buffer is read-only. :return: Boolean indicating if the buffer is read-only. pass def current_references(self): Get the current reference count of the buffer. :return: Integer reference count. pass ``` **Functionality Requirements**: - The `__init__` method should store the buffer and initialize ref count. - The `acquire_buffer` method should increment the reference count and return the buffer. - The `release_buffer` method should decrement the reference count and set the buffer to `None` when the reference count reaches zero. - The `is_readonly` method should return whether the buffer is read-only. - The `current_references` method should return the current reference count. # Example Usage ```python data = bytearray(b\\"Hello, World!\\") cb = CustomBuffer(data) print(cb.is_readonly()) # Output: False buf = cb.acquire_buffer() print(buf) # Output: bytearray(b\'Hello, World!\') print(cb.current_references()) # Output: 1 cb.release_buffer() print(cb.current_references()) # Output: 0 print(cb.acquire_buffer()) # Output: None ``` **Constraints**: - Assume the buffer is initially provided as a `bytes` or `bytearray`. - The reference count should not go below zero. **Performance**: - The solution should manage reference counting efficiently with time complexity O(1) for `acquire_buffer` and `release_buffer`. Implement the `CustomBuffer` class according to the given specifications.","solution":"class CustomBuffer: def __init__(self, buffer, readonly=False): Initialize the buffer object. :param buffer: The underlying buffer (a bytearray or bytes object). :param readonly: A boolean indicating if the buffer is read-only. if not isinstance(buffer, (bytes, bytearray)): raise TypeError(\\"buffer must be a bytes or bytearray object\\") self._buffer = buffer self._readonly = readonly self._ref_count = 0 def acquire_buffer(self): Increment the reference count by one and return the buffer. :return: The internal buffer. self._ref_count += 1 return self._buffer def release_buffer(self): Decrement the reference count by one. If the reference count is zero, set the buffer to None. if self._ref_count > 0: self._ref_count -= 1 if self._ref_count == 0: self._buffer = None def is_readonly(self): Check if the buffer is read-only. :return: Boolean indicating if the buffer is read-only. return self._readonly def current_references(self): Get the current reference count of the buffer. :return: Integer reference count. return self._ref_count"},{"question":"# Unix User Account Information Retrieval You are required to implement a function that interacts with the Unix password database using the `pwd` module. The function should retrieve information about all users and return it in a structured format. Function Specification **Function Name:** `get_user_info()` **Input:** None **Output:** A dictionary where the keys are user names (`pw_name`) and the values are dictionaries containing the following key-value pairs: - `\\"uid\\"`: The numerical user ID (`pw_uid`). - `\\"gid\\"`: The numerical group ID (`pw_gid`). - `\\"home\\"`: The user\'s home directory (`pw_dir`). - `\\"shell\\"`: The user\'s command interpreter (`pw_shell`). Example ```python import pwd def get_user_info(): # Your solution code here user_info = get_user_info() print(user_info) ``` Example expected output: ```python { \'root\': {\'uid\': 0, \'gid\': 0, \'home\': \'/root\', \'shell\': \'/bin/bash\'}, \'user\': {\'uid\': 1000, \'gid\': 1000, \'home\': \'/home/user\', \'shell\': \'/bin/bash\'}, # more users... } ``` Constraints - The `pwd` module should be used to interact with the Unix password database. - The function should handle potential exceptions where user information might not be retrievable. - The function should be designed to be efficient and clear. Notes - The system running this function should be a Unix-based system, as the `pwd` module is specific to Unix. Implement the `get_user_info` function, ensuring it meets the above specifications.","solution":"import pwd def get_user_info(): Retrieves information about all users from the Unix password database. Returns: dict: A dictionary with usernames as keys and a dictionary of user info as values. user_info = {} try: for user in pwd.getpwall(): user_info[user.pw_name] = { \\"uid\\": user.pw_uid, \\"gid\\": user.pw_gid, \\"home\\": user.pw_dir, \\"shell\\": user.pw_shell } except Exception as e: print(f\\"An error occurred: {e}\\") return user_info"},{"question":"You are tasked with creating a personal expense tracker in Python. The tracker will record daily expenses and provide a summary of your spending. The data will be manipulated using lists and strings, and the program must follow these exact specifications: 1. **Input**: A series of daily expenses, represented as strings. Each string includes the day of the month and the expense amount separated by a space. For example: ``` \\"1 100.50\\" \\"2 55.75\\" \\"3 40.00\\" ... \\"10 75.00\\" ``` 2. **Function Requirements**: - Implement a function `process_expenses(expenses: List[str]) -> Tuple[List[float], float, float, float]` where `expenses` is a list of strings. - The function should perform the following tasks: - Parse the list of expenses. - Extract and convert the expense amounts to floats. - Store the parsed expense amounts in a list. - Calculate the total expenditure. - Determine the average daily expense. - Find the day with the highest expense. 3. **Output**: - A list of daily expenses in the order they were input. - The total expenditure for the given period. - The average daily expenditure. - The day of the month with the highest expense. # Example ```python def process_expenses(expenses: List[str]) -> Tuple[List[float], float, float, float]: # Your code goes here # Example input expenses = [ \\"1 100.50\\", \\"2 55.75\\", \\"3 40.00\\", \\"4 75.00\\" ] # Function call daily_expenses, total, average, max_day = process_expenses(expenses) # Expected output: # daily_expenses = [100.50, 55.75, 40.00, 75.00] # total = 271.25 # average = 67.81 # max_day = 1 ``` # Constraints: - You can assume the month has at most 31 days. - Expense values will always be valid floating-point numbers. - The input list will have at least one expense. Implement the function to meet the requirements and ensure your solution handles edge cases appropriately.","solution":"from typing import List, Tuple def process_expenses(expenses: List[str]) -> Tuple[List[float], float, float, int]: # Parse expenses and extract the amounts daily_expenses = [] for entry in expenses: day, amount = entry.split() daily_expenses.append(float(amount)) # Calculate total expenditure total_expenditure = sum(daily_expenses) # Calculate average daily expense average_expenditure = total_expenditure / len(daily_expenses) # Find the day with the highest expense max_day = expenses[daily_expenses.index(max(daily_expenses))].split()[0] max_day = int(max_day) return daily_expenses, total_expenditure, average_expenditure, max_day"},{"question":"**Problem Statement:** You are working on an application that needs to parse a raw email message, inspect its structure, modify some parts of the message, and then serialize it back to a byte stream. Specifically, you need to perform the following tasks: 1. Parse the raw email message and create an EmailMessage object. 2. Extract and print the subject of the email. 3. Extract and print the sender\'s email address. 4. Extract and print all recipient email addresses. 5. If the email has any attachments, print the names of the attached files. 6. Add a new plain text part to the email with a specified content (e.g., \\"This is an appended message.\\"). 7. Serialize the modified email back into a byte stream. Your implementation should follow these constraints: - You should use the `Parser` class from the `email.parser` module to parse the raw email message. - You should use the `EmailMessage` class from the `email.message` module to represent and manipulate the email. - Ensure RFC compliance in your implementation. **Function Signature:** ```python def process_email(raw_email: bytes, new_message_content: str) -> bytes: pass ``` **Input:** - `raw_email` (bytes): A raw byte stream representing the email message. - `new_message_content` (str): The content to be added as a new plain text part of the email. **Output:** - returns a modified email message (bytes): The serialized byte stream of the modified email message. **Example:** ```python raw_email = b From: sender@example.com To: recipient@example.com Subject: Test email MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\\"===============7330845974216740156==\\" --===============7330845974216740156== Content-Type: text/plain This is the body of the email. --===============7330845974216740156== Content-Type: text/plain; name=\\"attachment.txt\\" Content-Disposition: attachment; filename=\\"attachment.txt\\" This is the content of the attachment. --===============7330845974216740156==-- new_message_content = \\"This is an appended message.\\" result = process_email(raw_email, new_message_content) print(result.decode()) # For illustrative purposes, decoded byte stream to string ``` The output will be the modified email with the appended message content serialized back into a byte stream. **Note:** - You can assume that the raw email message is well-formed and adheres to RFC standards. - Handle the email and its parts appropriately, ensuring correct MIME type handling.","solution":"from email import policy from email.parser import BytesParser from email.message import EmailMessage def process_email(raw_email: bytes, new_message_content: str) -> bytes: # Parse the raw email message msg = BytesParser(policy=policy.default).parsebytes(raw_email) # Task 2: Extract and print the subject of the email print(\\"Subject:\\", msg[\'Subject\']) # Task 3: Extract and print the sender\'s email address print(\\"From:\\", msg[\'From\']) # Task 4: Extract and print all recipient email addresses print(\\"To:\\", msg[\'To\']) # Task 5: If the email has any attachments, print the names of the attached files for attachment in msg.iter_attachments(): print(\\"Attachment filename:\\", attachment.get_filename()) # Task 6: Add a new plain text part to the email with the specified content new_part = EmailMessage() new_part.set_content(new_message_content) if msg.is_multipart(): msg.add_attachment(new_message_content, subtype=\'plain\') else: original_content = msg.get_payload() msg.set_content(original_content) msg.add_attachment(new_message_content, subtype=\'plain\') # Task 7: Serialize the modified email back into a byte stream return msg.as_bytes()"},{"question":"Objective The goal of this exercise is to assess your ability to use PyTorch\'s `torch.cond` function to implement data-dependent control flow. You will create a custom PyTorch module that uses `torch.cond` to perform different operations depending on the characteristics of the input tensor. Problem Statement You are required to implement a PyTorch module called `CustomCondModule`. This module will apply a different function to the input tensor based on if the sum of the tensor elements is greater than 10 or not. 1. If the sum of the elements in the input tensor is greater than 10, apply a function that squares each element in the tensor. 2. If the sum is 10 or less, apply a function that takes the natural logarithm of each element in the tensor (ensure to handle non-positive values appropriately by considering their absolute values). Your module should follow these specifications: Specifications 1. **Input**: A PyTorch tensor `x` of any shape. 2. **Output**: The tensor resulting from applying either the squaring function or the logarithmic function based on the sum of `x`. # Constraints - You can assume the input tensor always contains floating-point numbers. - Handle the natural logarithm operation with non-positive values by using their absolute values: `torch.log(torch.abs(x))`. # Performance Requirements - The solution should effectively leverage `torch.cond` for branching. - Ensure the solution does not mutate the input tensor. Function Signature ```python import torch class CustomCondModule(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: # Define the true and false functions here def true_fn(x: torch.Tensor) -> torch.Tensor: return x ** 2 def false_fn(x: torch.Tensor) -> torch.Tensor: return torch.log(torch.abs(x)) # Implement the torch.cond logic return torch.cond(x.sum() > 10, true_fn, false_fn, (x,)) ``` # Example ```python module = CustomCondModule() input_tensor = torch.tensor([1.0, 2.0, 3.0]) # sum is 6.0 output_tensor = module(input_tensor) print(output_tensor) # Expected: tensor([0.6931, 1.0986, 1.3863]) input_tensor = torch.tensor([4.0, 5.0, 6.0]) # sum is 15.0 output_tensor = module(input_tensor) print(output_tensor) # Expected: tensor([16.0, 25.0, 36.0]) ``` Use this question to test the student\'s ability to implement dynamic control flows using `torch.cond` in PyTorch.","solution":"import torch import torch.nn.functional as F class CustomCondModule(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: # Define the true and false functions here def true_fn(x: torch.Tensor) -> torch.Tensor: return x ** 2 def false_fn(x: torch.Tensor) -> torch.Tensor: return torch.log(torch.abs(x)) # Check condition and apply corresponding function if x.sum() > 10: return true_fn(x) else: return false_fn(x)"},{"question":"**Coding Assessment Question** # Title: Implementing a Custom Data Type with Special Methods and Exception Handling Objective: Design and implement a custom data type `TimeDuration` that represents a duration of time in hours and minutes. Implement special methods to support addition, subtraction, and comparison between `TimeDuration` instances. Also, handle possible errors using appropriate exception handling. Problem Statement: 1. **Define a class `TimeDuration`.** 2. **Attributes**: - `hours` (int): represents the number of hours. - `minutes` (int): represents the number of minutes. 3. **Implement the following methods**: - `__init__(self, hours: int, minutes: int)`: Initialize the `TimeDuration` instance. Ensure that `minutes` are always less than 60 by converting excess minutes to hours. - `__str__(self) -> str`: Return the string representation in `\\"{hours}h {minutes}m\\"` format. - `__add__(self, other: \'TimeDuration\') -> \'TimeDuration\'`: Support the addition of two `TimeDuration` objects. - `__sub__(self, other: \'TimeDuration\') -> \'TimeDuration\'`: Support the subtraction of two `TimeDuration` objects. Ensure the result is non-negative (raise an exception if the result would be negative). - `__eq__(self, other: \'TimeDuration\') -> bool`: Compare two `TimeDuration` objects for equality. - `__lt__(self, other: \'TimeDuration\') -> bool`: Less than comparison between two `TimeDuration` objects. 4. **Exception Handling**: - Raise a `ValueError` if the subtraction of two `TimeDuration` objects results in a negative duration. Input and Output: - **Input**: Two `TimeDuration` objects. - **Output**: Result of addition, subtraction, or comparison. Constraints: - Ensure that `hours` and `minutes` are always integers. - Minutes should be in the range of 0-59 after initialization. - Subtraction should handle and raise exceptions for invalid operations. Example: ```python # Example Usage td1 = TimeDuration(2, 45) td2 = TimeDuration(1, 30) print(td1) # Output: \\"2h 45m\\" print(td2) # Output: \\"1h 30m\\" td3 = td1 + td2 print(td3) # Output: \\"4h 15m\\" try: td4 = td2 - td1 except ValueError as e: print(e) # Output: \\"Cannot subtract to get negative time duration.\\" print(td1 == td2) # Output: False print(td1 < td2) # Output: False print(td2 < td1) # Output: True ``` Note: - Properly test your implementation with edge cases, such as adding durations that result in hours and minutes wrapping around, and subtracting to exactly zero minutes.","solution":"class TimeDuration: def __init__(self, hours: int, minutes: int): self.hours = hours + minutes // 60 self.minutes = minutes % 60 def __str__(self): return f\\"{self.hours}h {self.minutes}m\\" def __add__(self, other): total_minutes = (self.hours + other.hours) * 60 + self.minutes + other.minutes return TimeDuration(0, total_minutes) def __sub__(self, other): total_minutes_self = self.hours * 60 + self.minutes total_minutes_other = other.hours * 60 + other.minutes if total_minutes_self < total_minutes_other: raise ValueError(\\"Cannot subtract to get negative time duration.\\") result_minutes = total_minutes_self - total_minutes_other return TimeDuration(0, result_minutes) def __eq__(self, other): return self.hours == other.hours and self.minutes == other.minutes def __lt__(self, other): return (self.hours, self.minutes) < (other.hours, other.minutes)"},{"question":"Objective: Create a class to handle compression and decompression of data using the `zlib` module. Your implementation must include methods for compressing and decompressing data, along with verifying the integrity of data using checksums. Instructions: 1. **Class Definition**: - Define a class `Compressor`. 2. **Methods**: - **`compress_data` Method**: - Input: `self, data: bytes, level: int = -1` - Output: `bytes` (compressed data) - Functionality: Compress `data` using the given compression `level` which ranges from 0 to 9. Level -1 is the default and uses the default compromise between speed and compression. - **`decompress_data` Method**: - Input: `self, compressed_data: bytes` - Output: `bytes` (decompressed data) - Functionality: Decompress `compressed_data`. - **`verify_integrity` Method**: - Input: `self, original_data: bytes, compressed_data: bytes` - Output: `bool` - Functionality: Verify that the decompressed data matches the original data using checksums (both Adler-32 and CRC32 checksums should be calculated for verification). 5. **Constraints**: - The class should handle zlib related errors gracefully, raising appropriate exceptions with error messages. 6. **Performance**: - The methods `compress_data` and `decompress_data` should handle data sizes efficiently, ensuring that large data streams do not cause memory issues. Example: ```python # Example Usage data = b\'This is a test string for compression\' compressor = Compressor() # Compress the data compressed_data = compressor.compress_data(data, level=6) # Decompress the data back decompressed_data = compressor.decompress_data(compressed_data) # Verify integrity is_valid = compressor.verify_integrity(data, compressed_data) print(is_valid) # Output: True ``` Note: Ensure your implementation includes adequate documentation and error handling. Submission: Submit the python file containing the `Compressor` class.","solution":"import zlib class Compressor: def compress_data(self, data: bytes, level: int = -1) -> bytes: Compresses the given data using the specified compression level. :param data: Data to be compressed :param level: Compression level (0-9, with -1 as default for balanced compression) :return: Compressed data try: return zlib.compress(data, level) except zlib.error as e: raise Exception(f\\"Compression error: {e}\\") def decompress_data(self, compressed_data: bytes) -> bytes: Decompresses the given compressed data. :param compressed_data: Data to be decompressed :return: Decompressed data try: return zlib.decompress(compressed_data) except zlib.error as e: raise Exception(f\\"Decompression error: {e}\\") def verify_integrity(self, original_data: bytes, compressed_data: bytes) -> bool: Verifies the integrity of compressed data by comparing checksums. :param original_data: Original uncompressed data :param compressed_data: Data that has been compressed :return: True if decompressed data matches the original data, False otherwise. try: decompressed_data = self.decompress_data(compressed_data) if zlib.adler32(original_data) == zlib.adler32(decompressed_data) and zlib.crc32(original_data) == zlib.crc32(decompressed_data): return True else: return False except Exception as e: raise Exception(f\\"Integrity verification error: {e}\\")"},{"question":"**Coding Assessment Question: Visualizing Airline Passenger Data with Seaborn** **Objective:** Create a Python function using the seaborn library that visualizes airline passenger data in various formats. This task will assess your understanding of seaborn\'s lineplot function and your ability to manipulate datasets and customize plots. **Task:** Write a function `visualize_airline_data` that performs the following: 1. **Load the `flights` dataset** from seaborn. 2. **Generate and display** a line plot for each of the following specifications: - **Plot 1:** Show the total number of passengers over the years. - **Plot 2:** Show the monthly trend over the years, with separate lines for each month. - **Plot 3:** Provide a year-wise comparison between the months using `hue` and `style`. - **Plot 4:** Show the mean number of passengers per year with 95% confidence intervals (CI). 3. **Save each plot** as a PNG file with appropriate names (`plot1.png`, `plot2.png`, etc.). **Constraints:** - The function should not take any parameters. - Plots should be clearly labeled with titles, axis labels, and legends where appropriate. - Ensure to use different color palettes for each plot to distinguish the trends clearly. **Expected Output:** - Four PNG files saved in the current working directory, named `plot1.png`, `plot2.png`, `plot3.png`, and `plot4.png`. **Sample Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_airline_data(): # Load dataset flights = sns.load_dataset(\\"flights\\") # Plot 1: Total number of passengers over the years plt.figure() sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\") plt.title(\'Total Number of Passengers Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.savefig(\'plot1.png\') # Plot 2: Monthly trend over the years plt.figure() sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\") plt.title(\'Monthly Trend Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.savefig(\'plot2.png\') # Plot 3: Year-wise comparison between months using hue and style plt.figure() sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", style=\\"month\\") plt.title(\'Year-wise Comparison Between Months\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.savefig(\'plot3.png\') # Plot 4: Mean number of passengers per year with 95% CI plt.figure() sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", estimator=\'mean\', ci=95) plt.title(\'Mean Number of Passengers Per Year with 95% CI\') plt.xlabel(\'Year\') plt.ylabel(\'Mean Number of Passengers\') plt.savefig(\'plot4.png\') # Call the function visualize_airline_data() ``` **Note:** Ensure that your solution is efficient and leverages seaborn\'s functionality to visualize and save the desired plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_airline_data(): # Load dataset flights = sns.load_dataset(\\"flights\\") # Plot 1: Total number of passengers over the years plt.figure() sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\") plt.title(\'Total Number of Passengers Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.savefig(\'plot1.png\') plt.close() # Plot 2: Monthly trend over the years plt.figure() sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", palette=\'tab10\') plt.title(\'Monthly Trend Over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.legend(title=\'Month\') plt.savefig(\'plot2.png\') plt.close() # Plot 3: Year-wise comparison between months using hue and style plt.figure() sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", style=\\"month\\", palette=\'tab20\') plt.title(\'Year-wise Comparison Between Months\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.legend(title=\'Month\') plt.savefig(\'plot3.png\') plt.close() # Plot 4: Mean number of passengers per year with 95% CI plt.figure() sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", estimator=\'mean\', ci=95, palette=\'tab10\') plt.title(\'Mean Number of Passengers Per Year with 95% CI\') plt.xlabel(\'Year\') plt.ylabel(\'Mean Number of Passengers\') plt.savefig(\'plot4.png\') plt.close()"},{"question":"# Question: Implement Custom Kernel Function and Compare with Predefined Kernels Objective: Write a Python function to implement a custom kernel function and compare its performance on a sample dataset with predefined kernels provided by scikit-learn. Demonstrate your understanding of how to create and use custom kernels, and how to evaluate them using scikit-learn\'s SVC (Support Vector Classifier). Requirements: 1. Implement a custom kernel function that combines properties of polynomial and RBF kernels. 2. Use the custom kernel function in an SVM classifier. 3. Compare the classification performance of the custom kernel with predefined linear, polynomial, and RBF kernels. 4. Use accuracy as the performance metric for comparison. Details: 1. **Custom Kernel Function**: - The custom kernel function should be a combination of polynomial and RBF kernels. - Formula: ( k_{custom}(x, y) = (gamma_1 x^T y + c_0)^d times exp(-gamma_2 | x-y |^2) ) - Parameters: (gamma_1 = 0.1), (gamma_2 = 0.1), (c_0 = 1), (d = 3) 2. **Input Data**: ```python import numpy as np from sklearn.datasets import make_classification X, y = make_classification(n_samples=100, n_features=20, random_state=42) ``` 3. **Function Definition**: ```python def custom_kernel(X, Y, gamma1=0.1, gamma2=0.1, c0=1, d=3): # Implement the custom kernel function here pass ``` 4. **Model Training and Evaluation**: - Use `sklearn.svm.SVC` with `kernel=\'precomputed\'` for the custom kernel. - Split the data into training and testing sets using `train_test_split` from `sklearn.model_selection`. - Train and evaluate classifiers using linear, polynomial, RBF, and the custom kernel. - Report the accuracy for each kernel type. 5. **Output Format**: - The function should print the accuracy for each kernel type. - Example output: ```plaintext Linear kernel accuracy: 0.85 Polynomial kernel accuracy: 0.88 RBF kernel accuracy: 0.90 Custom kernel accuracy: 0.92 ``` Constraints: - Ensure reproducibility by setting a random seed where necessary. - Do not use any additional libraries outside of `numpy` and `sklearn`. Function Template: ```python import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score from sklearn.metrics.pairwise import polynomial_kernel, rbf_kernel def custom_kernel(X, Y, gamma1=0.1, gamma2=0.1, c0=1, d=3): # Polynomial part poly = polynomial_kernel(X, Y, degree=d, gamma=gamma1, coef0=c0) # RBF part rbf = rbf_kernel(X, Y, gamma=gamma2) # Combine return poly * rbf # Prepare data X, y = make_classification(n_samples=100, n_features=20, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define and evaluate models kernels = { \'linear\': \'linear\', \'polynomial\': \'poly\', \'rbf\': \'rbf\', \'custom\': custom_kernel(X_train, X_train), } # Train and evaluate the classifiers for kernel_name, kernel in kernels.items(): if kernel_name == \'custom\': clf = SVC(kernel=\'precomputed\') clf.fit(kernel, y_train) K_test = custom_kernel(X_test, X_train) y_pred = clf.predict(K_test) else: clf = SVC(kernel=kernel) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"{kernel_name.capitalize()} kernel accuracy: {accuracy:.2f}\\") ```","solution":"import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score from sklearn.metrics.pairwise import polynomial_kernel, rbf_kernel def custom_kernel(X, Y, gamma1=0.1, gamma2=0.1, c0=1, d=3): Custom kernel combining polynomial and RBF kernels. Parameters: X, Y : array-like of shape (n_samples_X, n_features), (n_samples_Y, n_features) gamma1 : float, default=0.1 gamma2 : float, default=0.1 c0 : float, default=1 d : int, default=3 Returns: K : array-like of shape (n_samples_X, n_samples_Y) poly = polynomial_kernel(X, Y, degree=d, gamma=gamma1, coef0=c0) rbf = rbf_kernel(X, Y, gamma=gamma2) return poly * rbf # Prepare data X, y = make_classification(n_samples=100, n_features=20, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define and evaluate models def evaluate_kernels(X_train, X_test, y_train, y_test): kernels = { \'linear\': \'linear\', \'polynomial\': \'poly\', \'rbf\': \'rbf\', \'custom\': \'precomputed\', } accuracies = {} for kernel_name, kernel in kernels.items(): if kernel_name == \'custom\': K_train = custom_kernel(X_train, X_train) clf = SVC(kernel=\'precomputed\') clf.fit(K_train, y_train) K_test = custom_kernel(X_test, X_train) y_pred = clf.predict(K_test) else: clf = SVC(kernel=kernel) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) accuracies[kernel_name] = accuracy print(f\\"{kernel_name.capitalize()} kernel accuracy: {accuracy:.2f}\\") return accuracies evaluate_kernels(X_train, X_test, y_train, y_test)"},{"question":"# Question You have been provided with a CSV file named `sales_data.csv` containing the following columns: - `date`: The date of the sale (format: YYYY-MM-DD) - `product_id`: A unique identifier for the product - `store_id`: A unique identifier for the store - `units_sold`: The number of units sold - `unit_price`: The price per unit sold - `revenue`: The total revenue generated from the sale Using **pandas**, complete the following tasks: 1. **Data Loading and Basic Info**: - Load the data into a DataFrame. - Display basic statistics of the DataFrame, including the shape, data types, and missing values count for each column. 2. **Data Cleaning**: - Handle any missing values in the `units_sold` and `unit_price` columns by filling them with the median of their respective columns. - Ensure the `revenue` column is correctly calculated as `units_sold * unit_price`. If not, correct it. 3. **Data Aggregation and Analysis**: - Calculate the total revenue generated per month for each store. - Determine the top 5 products with the highest total revenue. - Generate a summary table showing the following statistics for each store: - Total units sold - Average revenue per sale - Total number of sales 4. **Data Visualization**: - Plot the total monthly revenue for each store on a line plot. - Create a bar plot showing the revenue of the top 5 products. Input: - Path to the CSV file: `sales_data.csv` Output: - Text: Basic information and statistics of the loaded DataFrame, including missing values count. - DataFrames: Cleaned DataFrame, monthly revenue per store DataFrame, summary statistics per store. - Plots: Line plot of monthly revenue per store, bar plot of revenue for top 5 products. Constraints: - Ensure your code is efficient and handles large datasets gracefully. - Properly handle date parsing for time-series analysis. --- ```python import pandas as pd import matplotlib.pyplot as plt def sales_analysis(csv_path): # 1. Load data df = pd.read_csv(csv_path, parse_dates=[\'date\']) # Basic information print(\\"DataFrame Info:\\") print(df.info()) print(\\"nMissing Values Count:\\") print(df.isnull().sum()) # 2. Data Cleaning df[\'units_sold\'].fillna(df[\'units_sold\'].median(), inplace=True) df[\'unit_price\'].fillna(df[\'unit_price\'].median(), inplace=True) df[\'revenue\'] = df[\'units_sold\'] * df[\'unit_price\'] # 3. Data Aggregation and Analysis # (a) Monthly revenue per store df[\'month\'] = df[\'date\'].dt.to_period(\'M\') monthly_revenue = df.groupby([\'store_id\', \'month\'])[\'revenue\'].sum().unstack() # (b) Top 5 products by highest total revenue top_products = df.groupby(\'product_id\')[\'revenue\'].sum().nlargest(5) # (c) Summary statistics per store summary_stats = df.groupby(\'store_id\').agg( total_units_sold=(\'units_sold\', \'sum\'), avg_revenue_per_sale=(\'revenue\', \'mean\'), total_sales=(\'revenue\', \'count\') ) # 4. Data Visualization # (a) Total monthly revenue per store monthly_revenue.T.plot(kind=\'line\', figsize=(10, 6)) plt.xlabel(\'Month\') plt.ylabel(\'Total Revenue\') plt.title(\'Total Monthly Revenue per Store\') plt.legend(title=\'Store ID\') plt.show() # (b) Revenue of top 5 products top_products.plot(kind=\'bar\', figsize=(10, 6)) plt.xlabel(\'Product ID\') plt.ylabel(\'Total Revenue\') plt.title(\'Top 5 Products by Revenue\') plt.show() return monthly_revenue, summary_stats # Example usage: # sales_analysis(\'sales_data.csv\') ```","solution":"import pandas as pd import matplotlib.pyplot as plt def sales_analysis(csv_path): # 1. Load data df = pd.read_csv(csv_path, parse_dates=[\'date\']) # Basic information print(\\"DataFrame Info:\\") print(df.info()) print(\\"nMissing Values Count:\\") print(df.isnull().sum()) # 2. Data Cleaning df[\'units_sold\'].fillna(df[\'units_sold\'].median(), inplace=True) df[\'unit_price\'].fillna(df[\'unit_price\'].median(), inplace=True) df[\'revenue\'] = df[\'units_sold\'] * df[\'unit_price\'] # 3. Data Aggregation and Analysis # (a) Monthly revenue per store df[\'month\'] = df[\'date\'].dt.to_period(\'M\') monthly_revenue = df.groupby([\'store_id\', \'month\'])[\'revenue\'].sum().unstack() # (b) Top 5 products by highest total revenue top_products = df.groupby(\'product_id\')[\'revenue\'].sum().nlargest(5) # (c) Summary statistics per store summary_stats = df.groupby(\'store_id\').agg( total_units_sold=(\'units_sold\', \'sum\'), avg_revenue_per_sale=(\'revenue\', \'mean\'), total_sales=(\'revenue\', \'count\') ) # 4. Data Visualization # (a) Total monthly revenue per store monthly_revenue.T.plot(kind=\'line\', figsize=(10, 6)) plt.xlabel(\'Month\') plt.ylabel(\'Total Revenue\') plt.title(\'Total Monthly Revenue per Store\') plt.legend(title=\'Store ID\') plt.show() # (b) Revenue of top 5 products top_products.plot(kind=\'bar\', figsize=(10, 6)) plt.xlabel(\'Product ID\') plt.ylabel(\'Total Revenue\') plt.title(\'Top 5 Products by Revenue\') plt.show() return monthly_revenue, summary_stats"},{"question":"# Advanced List Operations Assessment Problem Statement You are tasked with developing a function that performs a series of operations on lists. The function should support multiple actions, allowing for the dynamic manipulation of lists through specified commands. Your implementation will process a list of commands that dictate the operations to perform on the main list. Function Signature ```python def list_operations(commands: list, initial_list: list) -> list: This function processes a series of commands to manipulate an initial list. Parameters: - commands (list): A list of tuples where each tuple represents a command and its parameters. - initial_list (list): The initial list to be manipulated. Returns: - list: The final list after all operations have been performed. ``` Commands The `commands` list contains tuples, where the first element is a string representing the operation, and the subsequent elements are the parameters for that operation. Below are the supported commands and their descriptions: 1. **append(x)**: Adds item `x` to the end of the list. - Tuple format: `(\\"append\\", x)` 2. **insert(i, x)**: Inserts item `x` at the specified position `i`. - Tuple format: `(\\"insert\\", i, x)` 3. **remove(x)**: Removes the first occurrence of item `x` from the list. - Tuple format: `(\\"remove\\", x)` 4. **pop([i])**: Removes and returns the item at position `i`. If `i` is not provided, removes and returns the last item. - Tuple format: `(\\"pop\\", i)` or `(\\"pop\\",)` 5. **clear()**: Removes all items from the list. - Tuple format: `(\\"clear\\",)` 6. **index(x[, start, end])**: Returns the index of the first occurrence of `x` in the list. The search can be confined to the slice `start:end`. - Tuple format: `(\\"index\\", x, start, end)` or `(\\"index\\", x, start)` or `(\\"index\\", x)` 7. **count(x)**: Returns the number of times `x` appears in the list. - Tuple format: `(\\"count\\", x)` 8. **sort(*, key=None, reverse=False)**: Sorts the list in ascending order by default. The `reverse` parameter can be set to sort in descending order. - Tuple format: `(\\"sort\\", key, reverse)` 9. **reverse()**: Reverses the elements of the list in place. - Tuple format: `(\\"reverse\\",)` 10. **copy()**: Returns a shallow copy of the list. - Tuple format: `(\\"copy\\",)` Example ```python commands = [ (\\"append\\", 4), (\\"append\\", 3), (\\"insert\\", 1, 2), (\\"pop\\", 2), (\\"sort\\", None, False), (\\"reverse\\",), (\\"count\\", 3), (\\"remove\\", 2), (\\"clear\\",), (\\"append\\", 5), ] initial_list = [1] result = list_operations(commands, initial_list) print(result) # Output: [5] ``` Constraints - All elements will be of valid types for the operations specified. - The list will contain at most 10^4 elements. - Parameter values for index-based operations (`insert`, `pop`, `index`) will always be within the list boundaries. Solution Requirements - Implement the specified function. - Ensure the function handles each command correctly and efficiently. Good luck, and happy coding!","solution":"def list_operations(commands: list, initial_list: list) -> list: This function processes a series of commands to manipulate an initial list. Parameters: - commands (list): A list of tuples where each tuple represents a command and its parameters. - initial_list (list): The initial list to be manipulated. Returns: - list: The final list after all operations have been performed. result = initial_list[:] for command in commands: operation = command[0] params = command[1:] if operation == \\"append\\": result.append(params[0]) elif operation == \\"insert\\": result.insert(params[0], params[1]) elif operation == \\"remove\\": result.remove(params[0]) elif operation == \\"pop\\": if params: result.pop(params[0]) else: result.pop() elif operation == \\"clear\\": result.clear() elif operation == \\"index\\": if len(params) == 3: result.index(params[0], params[1], params[2]) elif len(params) == 2: result.index(params[0], params[1]) else: result.index(params[0]) elif operation == \\"count\\": result.count(params[0]) elif operation == \\"sort\\": result.sort(key=params[0], reverse=params[1]) elif operation == \\"reverse\\": result.reverse() elif operation == \\"copy\\": result.copy() return result"},{"question":"**Question:** You have been hired by a company to develop a simplified web scraper using `asyncio`. This web scraper needs to fetch data from multiple URLs concurrently and process each webpage to extract the title. You are required to implement this web scraper efficiently, ensuring that all asynchronous tasks are handled correctly, even in the presence of blocking I/O operations. # Function Requirements 1. **fetch_title(url)**: - **Input**: A single string `url` representing the webpage URL. - **Output**: A single string which is the title of the webpage. 2. **fetch_all_titles(urls)**: - **Input**: A list of strings `urls`, where each string is a webpage URL. - **Output**: A list of strings, where each string is the title of the corresponding webpage from the input list. 3. **main(urls)**: - **Input**: A list of strings `urls`, where each string is a URL. - **Output**: None (prints out each URL with its extracted title). # Constraints - Use `asyncio` to handle concurrent tasks. - Assume there can be network delays and processing time when fetching and processing each webpage. - Ensure that blocking operations are handled using `executor` to avoid blocking the event loop. - Incorporate asyncio\'s debug mode to assist in identifying potential issues. - Handle any potential exceptions that occur during the fetching or processing of webpage titles. # Example For a list of URLs like: ```python urls = [ \'https://example.com\', \'https://example.org\', \'https://example.net\' ] ``` The output should be: ``` URL: https://example.com, Title: Example Domain URL: https://example.org, Title: Example Organization URL: https://example.net, Title: Example Network ``` # Implementation Use the following stub as a starting point for your implementation: ```python import asyncio import requests from concurrent.futures import ThreadPoolExecutor from bs4 import BeautifulSoup import logging logging.basicConfig(level=logging.DEBUG) # Enable asyncio debug mode asyncio.get_event_loop().set_debug(True) async def fetch_title(url): # Function to fetch the title from a single URL pass async def fetch_all_titles(urls): # Function to fetch titles from all URLs concurrently pass async def main(urls): # Main function to coordinate the fetching of titles and printing results pass if __name__ == \\"__main__\\": urls = [ \'https://example.com\', \'https://example.org\', \'https://example.net\' ] asyncio.run(main(urls)) ``` Ensure your code handles exceptions properly and includes debug logging for better troubleshooting. Avoid blocking the event loop with longer executions, and leverage asyncio features to manage concurrency efficiently.","solution":"import asyncio import requests from concurrent.futures import ThreadPoolExecutor from bs4 import BeautifulSoup import logging logging.basicConfig(level=logging.DEBUG) # Enable asyncio debug mode asyncio.get_event_loop().set_debug(True) def fetch_title_sync(url): Synchronous function to fetch the title from a single URL. try: response = requests.get(url) response.raise_for_status() # Raises HTTPError for bad responses soup = BeautifulSoup(response.text, \'html.parser\') title = soup.title.string if soup.title else \'No Title Found\' return title except Exception as e: logging.error(f\\"Error fetching title from {url}: {e}\\") return \'Error\' async def fetch_title(url): Asynchronous wrapper to fetch the title from a single URL. loop = asyncio.get_event_loop() with ThreadPoolExecutor() as executor: title = await loop.run_in_executor(executor, fetch_title_sync, url) return title async def fetch_all_titles(urls): Fetch titles from all URLs concurrently. tasks = [fetch_title(url) for url in urls] titles = await asyncio.gather(*tasks) return titles async def main(urls): Main function to coordinate the fetching of titles and printing results. titles = await fetch_all_titles(urls) for url, title in zip(urls, titles): print(f\\"URL: {url}, Title: {title}\\") if __name__ == \\"__main__\\": urls = [ \'https://example.com\', \'https://example.org\', \'https://example.net\' ] asyncio.run(main(urls))"},{"question":"# Pandas Reshaping and Pivot Table Application **Objective:** Assess the student\'s ability to manipulate and reshape data using pandas\' advanced functionalities such as `pivot`, `pivot_table`, `melt`, and `stack`. --- **Problem Statement:** You are given a dataset containing information on different products sold in various stores on specific dates. Your task is to perform various data manipulations to reshape and analyze the dataset. **Dataset Description:** The dataset is structured as follows: ``` | date | store | product | sales | profit | |------------|-------|---------|-------|--------| | 2023-01-01 | A | apple | 20 | 10 | | 2023-01-01 | A | orange | 15 | 8 | | 2023-01-01 | B | banana | 10 | 5 | | 2023-01-02 | A | apple | 18 | 9 | | 2023-01-02 | A | banana | 25 | 12 | | 2023-01-02 | B | orange | 22 | 11 | ... (and so on) ``` **Instructions:** 1. Load the dataset into a pandas DataFrame. 2. Reshape the DataFrame such that each store\'s sales are pivoted, making the dates the index and the products the columns. There should be columns representing the sales and the profit. 3. Create a pivot table that aggregates the total sales and total profit for each product across all dates and stores, with appropriate margins to show the totals. 4. Unpivot the reshaped DataFrame (from step 2) back to the original long format. 5. Use the `stack` and `unstack` methods to display the reshaped DataFrame with a multi-level index on both index and columns. 6. Generate a cross-tabulation of the number of sales transactions (frequencies) for each store and product combination for a particular date. 7. Convert the product category into dummy variables. Write functions for each step as described below: **Function Specifications:** 1. **load_data** ```python def load_data(file_path: str) -> pd.DataFrame: Load data from the given file path into a pandas DataFrame. Args: - file_path (str): The path to the data file. Returns: - pd.DataFrame: The loaded data. pass ``` 2. **reshape_data** ```python def reshape_data(df: pd.DataFrame) -> pd.DataFrame: Reshape the DataFrame pivoting store\'s sales. Args: - df (pd.DataFrame): The original data. Returns: - pd.DataFrame: The reshaped DataFrame. pass ``` 3. **create_pivot_table** ```python def create_pivot_table(df: pd.DataFrame) -> pd.DataFrame: Create a pivot table aggregating total sales and profit for each product. Args: - df (pd.DataFrame): The reshaped data. Returns: - pd.DataFrame: The pivot table with margins. pass ``` 4. **unpivot_data** ```python def unpivot_data(df: pd.DataFrame) -> pd.DataFrame: Unpivot the reshaped DataFrame back to the original long format. Args: - df (pd.DataFrame): The reshaped data. Returns: - pd.DataFrame: The unpivoted DataFrame. pass ``` 5. **stack_unstack_data** ```python def stack_unstack_data(df: pd.DataFrame) -> pd.DataFrame: Apply stack and unstack methods to the DataFrame to create a multi-level index. Args: - df (pd.DataFrame): The reshaped data. Returns: - pd.DataFrame: The stacked and unstacked DataFrame. pass ``` 6. **cross_tabulation** ```python def cross_tabulation(df: pd.DataFrame, date: str) -> pd.DataFrame: Generate a cross-tabulation of sales transactions for each store and product for a specific date. Args: - df (pd.DataFrame): The original data. - date (str): The specific date. Returns: - pd.DataFrame: The cross-tabulation DataFrame. pass ``` 7. **convert_to_dummies** ```python def convert_to_dummies(df: pd.DataFrame) -> pd.DataFrame: Convert the product category into dummy variables. Args: - df (pd.DataFrame): The original data. Returns: - pd.DataFrame: The DataFrame with dummy variables for products. pass ``` **Constraints:** - You should use the pandas library for all data manipulations. - Ensure that the reshaped DataFrame retains all relevant information. - Performance is not a primary concern, but aim to write efficient code. --- **Example Usage:** ```python df = load_data(\\"sales_data.csv\\") reshaped_df = reshape_data(df) pivot_table = create_pivot_table(reshaped_df) unpivoted_df = unpivot_data(reshaped_df) stacked_unstacked_df = stack_unstack_data(reshaped_df) cross_tab_df = cross_tabulation(df, \\"2023-01-01\\") dummies_df = convert_to_dummies(df) ``` Final submission should include all the above functions and demonstrate their usage with a sample dataset.","solution":"import pandas as pd def load_data(file_path: str) -> pd.DataFrame: Load data from the given file path into a pandas DataFrame. Args: - file_path (str): The path to the data file. Returns: - pd.DataFrame: The loaded data. return pd.read_csv(file_path) def reshape_data(df: pd.DataFrame) -> pd.DataFrame: Reshape the DataFrame pivoting store\'s sales. Args: - df (pd.DataFrame): The original data. Returns: - pd.DataFrame: The reshaped DataFrame. reshaped_df = df.pivot_table(index=\'date\', columns=\'product\', values=[\'sales\', \'profit\'], aggfunc=\'sum\') reshaped_df.columns = [\'_\'.join(col).strip() for col in reshaped_df.columns.values] return reshaped_df def create_pivot_table(df: pd.DataFrame) -> pd.DataFrame: Create a pivot table aggregating total sales and profit for each product. Args: - df (pd.DataFrame): The reshaped data. Returns: - pd.DataFrame: The pivot table with margins. pivot_table = pd.pivot_table(df, values=[\'sales\', \'profit\'], index=[\'product\'], aggfunc={\'sales\': \'sum\', \'profit\': \'sum\'}, margins=True) return pivot_table def unpivot_data(df: pd.DataFrame) -> pd.DataFrame: Unpivot the reshaped DataFrame back to the original long format. Args: - df (pd.DataFrame): The reshaped data. Returns: - pd.DataFrame: The unpivoted DataFrame. unpivoted_df = df.reset_index().melt(id_vars=[\'date\'], var_name=\'product\', value_name=\'values\') return unpivoted_df def stack_unstack_data(df: pd.DataFrame) -> pd.DataFrame: Apply stack and unstack methods to the DataFrame to create a multi-level index. Args: - df (pd.DataFrame): The reshaped data. Returns: - pd.DataFrame: The stacked and unstacked DataFrame. stacked_df = df.stack() unstacked_df = stacked_df.unstack() return unstacked_df def cross_tabulation(df: pd.DataFrame, date: str) -> pd.DataFrame: Generate a cross-tabulation of sales transactions for each store and product for a specific date. Args: - df (pd.DataFrame): The original data. - date (str): The specific date. Returns: - pd.DataFrame: The cross-tabulation DataFrame. specific_date_df = df[df[\'date\'] == date] cross_tab_df = pd.crosstab(specific_date_df[\'store\'], specific_date_df[\'product\']) return cross_tab_df def convert_to_dummies(df: pd.DataFrame) -> pd.DataFrame: Convert the product category into dummy variables. Args: - df (pd.DataFrame): The original data. Returns: - pd.DataFrame: The DataFrame with dummy variables for products. dummies_df = pd.get_dummies(df, columns=[\'product\']) return dummies_df"},{"question":"# Question: Advanced Tuple Operations You are required to implement a utility class `AdvancedTuple` that provides various operations on tuples. This class should encapsulate tuple manipulations with methods to create, access, resize, and slice tuples. **Class Definition:** ```python class AdvancedTuple: def __init__(self, elements: tuple): Initialize the AdvancedTuple with the given elements. :param elements: A tuple containing elements to initialize with. pass def get_size(self): Return the size of the tuple. :return: Integer size of the tuple. pass def get_item(self, pos: int): Get the item at the specified position. :param pos: Position of the item to retrieve. :return: The element at the specified position. :raises IndexError: If the position is out of bounds. pass def resize(self, new_size: int): Resize the tuple to the new size. :param new_size: The new size of the tuple. :return: None :raises ValueError: If the new size is negative. pass def get_slice(self, low: int, high: int): Get a slice of the tuple between the specified indices. :param low: Lower bound index for the slice. :param high: Upper bound index for the slice. :return: A new tuple containing the elements of the slice. :raises IndexError: If either index is out of bounds. pass ``` **Input and Output Formats:** - `__init__(elements)`: Initializes with a tuple of elements. - `get_size()`: Returns an integer representing the size of the tuple. - `get_item(pos)`: Returns the element at index `pos`. Raises `IndexError` if the index is out of bounds. - `resize(new_size)`: Resizes the internal tuple to `new_size` elements. Raises `ValueError` if `new_size` is negative. If increasing size, fills new positions with `None`. - `get_slice(low, high)`: Returns a new tuple containing the elements from index `low` to `high` (like `tuple[low:high]`). Raises `IndexError` if indices are out of bounds. **Constraints:** - The initial tuple provided in the constructor will have at most `10^6` elements. - The `resize` method should handle resizing efficiently, even for large tuples, without unnecessary overhead. - Negative indexing is not required in `get_slice`. Use the following example to test your implementation: ```python # Example usage: at = AdvancedTuple((1, 2, 3, 4, 5)) print(at.get_size()) # Output: 5 print(at.get_item(2)) # Output: 3 at.resize(3) print(at.get_size()) # Output: 3 print(at.get_slice(1, 3)) # Output: (2, 3) ``` Implement the `AdvancedTuple` class in Python to meet the requirements outlined above.","solution":"class AdvancedTuple: def __init__(self, elements: tuple): Initialize the AdvancedTuple with the given elements. :param elements: A tuple containing elements to initialize with. self._elements = tuple(elements) def get_size(self): Return the size of the tuple. :return: Integer size of the tuple. return len(self._elements) def get_item(self, pos: int): Get the item at the specified position. :param pos: Position of the item to retrieve. :return: The element at the specified position. :raises IndexError: If the position is out of bounds. if pos < 0 or pos >= len(self._elements): raise IndexError(\\"Position out of bounds\\") return self._elements[pos] def resize(self, new_size: int): Resize the tuple to the new size. :param new_size: The new size of the tuple. :return: None :raises ValueError: If the new size is negative. if new_size < 0: raise ValueError(\\"New size cannot be negative\\") current_size = len(self._elements) if new_size > current_size: self._elements = self._elements + (None,) * (new_size - current_size) else: self._elements = self._elements[:new_size] def get_slice(self, low: int, high: int): Get a slice of the tuple between the specified indices. :param low: Lower bound index for the slice. :param high: Upper bound index for the slice. :return: A new tuple containing the elements of the slice. :raises IndexError: If either index is out of bounds. if low < 0 or high > len(self._elements) or low > high: raise IndexError(\\"Slice indices out of bounds\\") return self._elements[low:high]"},{"question":"**Coding Assessment Question**: # Objective To test understanding of Python\'s `asyncio` Futures and their use in asynchronous programming. # Problem Statement Write a Python function called `fetch_data` which: 1. Simulates fetching data asynchronously by using `asyncio.Future`. 2. Schedules a task that fetches data from a mock external source after a delay (use `asyncio.sleep` to simulate the delay). 3. Implements a callback function that processes the fetched data and prints it, this function should be attached to the Future object using `add_done_callback`. 4. Handles cases where the Future is cancelled or encounters an exception. # Function Signature: ```python async def fetch_data(url: str) -> asyncio.Future: pass ``` # Input: - `url` (str): A mock URL from which data is to be fetched. # Output: - Returns an `asyncio.Future` instance that represents the completion of the data fetch operation. The Future\'s result is the mock data fetched from the URL as a string. # Constraints: 1. The function should use the `asyncio` module for the asynchronous operations. 2. Simulate data fetching by returning \\"Data from {url}\\" after a delay. 3. Handle cases where the Future is cancelled or raises an exception. # Example: ```python import asyncio async def main(): future = await fetch_data(\\"http://example.com\\") try: result = await future print(result) except asyncio.CancelledError: print(\\"Fetching data was cancelled.\\") except Exception as e: print(f\\"An error occurred: {e}\\") asyncio.run(main()) ``` # Explanation: 1. Implement the `fetch_data` function. 2. In `fetch_data`, create a Future instance. 3. Schedule a task that sets the result of the Future after a delay using `asyncio.create_task` and `asyncio.sleep`. 4. Attach a done callback to the Future that processes the result and prints it. 5. Ensure that proper exception handling is implemented when awaiting the Future. **Notes**: - Make sure to properly manage the event loop and task scheduling. - Demonstrate an understanding of Futures, Tasks, and callbacks through this implementation.","solution":"import asyncio from typing import Any async def fetch_data(url: str) -> asyncio.Future: future = asyncio.Future() async def mock_fetch(): try: # Simulate a network delay await asyncio.sleep(1) # Set the result on the future future.set_result(f\\"Data from {url}\\") except asyncio.CancelledError: future.cancel() def callback(fut: asyncio.Future): try: data = fut.result() print(data) except asyncio.CancelledError: print(\\"Fetching data was cancelled.\\") except Exception as e: print(f\\"An error occurred: {e}\\") future.add_done_callback(callback) asyncio.create_task(mock_fetch()) return future"},{"question":"# Pandas Coding Assessment: Timedelta Manipulation and Analysis Objective: You are required to perform a series of operations involving pandas `Timedelta` objects and `TimedeltaIndex`. This exercise will assess your ability to create, manipulate, and analyze timedelta data effectively using pandas. Task: 1. **Data Preparation** - Create a `DataFrame` named `df` with two columns \\"Start\\" and \\"End\\" which represent start and end times of certain events. Use the following data to construct the `DataFrame`: ```plaintext Start End 2023-08-01 08:00:00 2023-08-01 12:30:00 2023-08-02 09:15:00 2023-08-02 11:45:00 2023-08-03 11:00:00 2023-08-03 14:00:00 ``` 2. **Calculate the Duration** - Add a new column named \\"Duration\\" to the `df` which calculates the difference (`End` - `Start`) as `Timedelta` objects. 3. **Determine the Average Duration** - Compute the mean duration of all events and store it in a variable called `mean_duration`. 4. **Conversion to Custom Frequency** - Convert the \\"Duration\\" column to seconds and store the result in a new column called \\"Duration_seconds\\". 5. **Generate and Analyze a Timedelta Series** - Create a `TimedeltaIndex` ranging from 1 hour to 7 hours with an interval of 1 hour. - Use this `TimedeltaIndex` to construct a new `Series` named `time_series` with random integer values between 1 and 100 as the data. 6. **Resampling the Series** - Resample the `time_series` to an even 2-hour frequency using the sum of the values in each resample bin, and store the result in a variable called `resampled_series`. 7. **Fill Missing Values** - Assume that some values in the `resampled_series` are missing. Fill any `NaN` values with a `Timedelta` of 1 hour and update the `resampled_series`. 8. **Calculate the Final Stats** - Compute the total sum and mean of the values in the `resampled_series` (in hours) after filling missing values, and store these in variables named `final_sum` and `final_mean` respectively. Constraints: - Ensure that all operations are performed using pandas functions and capabilities. - Ensure the appropriate use of `Timedelta`, `TimedeltaIndex`, and type conversions. - Resampling and filling should be handled appropriately without introducing arbitrary errors. Expected Output: - The final DataFrame `df` with columns: \\"Start\\", \\"End\\", \\"Duration\\", and \\"Duration_seconds\\". - The `mean_duration` as a `Timedelta` object. - The `time_series` pandas Series with its index as `TimedeltaIndex`. - The `resampled_series` after resampling and filling `NaN` values. - The `final_sum` and `final_mean` values calculated from the `resampled_series`. You are required to write the implementation for the above tasks. Make sure your code is well-documented and follows best practices for data manipulation using pandas. ```python import pandas as pd import numpy as np # 1. Data Preparation data = { \\"Start\\": [\\"2023-08-01 08:00:00\\", \\"2023-08-02 09:15:00\\", \\"2023-08-03 11:00:00\\"], \\"End\\": [\\"2023-08-01 12:30:00\\", \\"2023-08-02 11:45:00\\", \\"2023-08-03 14:00:00\\"] } df = pd.DataFrame(data) df[\\"Start\\"] = pd.to_datetime(df[\\"Start\\"]) df[\\"End\\"] = pd.to_datetime(df[\\"End\\"]) # 2. Calculate the Duration df[\\"Duration\\"] = df[\\"End\\"] - df[\\"Start\\"] # 3. Determine the Average Duration mean_duration = df[\\"Duration\\"].mean() # 4. Conversion to Custom Frequency df[\\"Duration_seconds\\"] = df[\\"Duration\\"].astype(\\"timedelta64[s]\\") # 5. Generate and Analyze a Timedelta Series timedelta_index = pd.timedelta_range(start=\\"1 hour\\", periods=7, freq=\\"1H\\") time_series = pd.Series(np.random.randint(1, 101, size=len(timedelta_index)), index=timedelta_index) # 6. Resampling the Series resampled_series = time_series.resample(\\"2H\\").sum() # 7. Fill Missing Values resampled_series = resampled_series.fillna(pd.Timedelta(hours=1)) # 8. Calculate the Final Stats final_sum = resampled_series.sum() final_mean = resampled_series.mean() ```","solution":"import pandas as pd import numpy as np # 1. Data Preparation data = { \\"Start\\": [\\"2023-08-01 08:00:00\\", \\"2023-08-02 09:15:00\\", \\"2023-08-03 11:00:00\\"], \\"End\\": [\\"2023-08-01 12:30:00\\", \\"2023-08-02 11:45:00\\", \\"2023-08-03 14:00:00\\"] } df = pd.DataFrame(data) df[\\"Start\\"] = pd.to_datetime(df[\\"Start\\"]) df[\\"End\\"] = pd.to_datetime(df[\\"End\\"]) # 2. Calculate the Duration df[\\"Duration\\"] = df[\\"End\\"] - df[\\"Start\\"] # 3. Determine the Average Duration mean_duration = df[\\"Duration\\"].mean() # 4. Conversion to Custom Frequency df[\\"Duration_seconds\\"] = df[\\"Duration\\"].dt.total_seconds() # 5. Generate and Analyze a Timedelta Series timedelta_index = pd.timedelta_range(start=\\"1 hour\\", periods=7, freq=\\"1H\\") time_series = pd.Series(np.random.randint(1, 101, size=len(timedelta_index)), index=timedelta_index) # 6. Resampling the Series resampled_series = time_series.resample(\\"2H\\").sum() # 7. Fill Missing Values resampled_series = resampled_series.fillna(pd.Timedelta(hours=1)) # 8. Calculate the Final Stats final_sum = resampled_series.sum() final_mean = resampled_series.mean()"},{"question":"**Extension Type for Mathematical Points** As part of an advanced Python course, you are required to implement a C extension defining a new type `Point` that represents a point in a 2D space, with `x` and `y` as coordinates. The point should support basic operations and interaction methods. # Objective Implement a Python C extension that defines a `Point` type with the following requirements: 1. **Attributes**: - `x`: A float representing the x-coordinate. - `y`: A float representing the y-coordinate. 2. **Methods**: - `magnitude(self) -> float`: Returns the distance of the point from the origin, calculated as √(x² + y²). - `add(self, other) -> Point`: Returns a new `Point` which is the sum of the current point and another point. 3. **Creation and Initialization**: - The `Point` type should initialize its `x` and `y` values to 0 by default. - Allow initialization of `x` and `y` through the constructor. 4. **Encapsulation**: - Ensure that `x` and `y` are encapsulated and can only be set to float values. If not, raise a `TypeError`. 5. **Garbage Collection**: - Implement support for Python’s garbage collection mechanisms to ensure proper memory management. 6. **Subclassing**: - The `Point` type should be subclassable. # Input and Output Specification - **Constructor**: Takes optional keyword arguments `x` and `y` which must be floats. - **Methods**: - `magnitude()`: No arguments, returns a float. - `add(other)`: Takes another `Point` as an argument, returns a new `Point`. # Constraints - The library should be implemented in a `point.c` file. - Use appropriate error handling and ensure attribute values are always floats. - Follow Python’s naming conventions and maintain compatibility with Python’s garbage collection. # Performance Requirements - Efficient in terms of time and space complexity. Ensure garbage collection and memory handling are optimized. # Building Instructions - Include a `setup.py` file to build the extension module. - The module should be named `pointmodule` and the type `Point` should be accessible via `import pointmodule`. Use the following structure as a guideline for your implementation: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> #include <math.h> #include \\"structmember.h\\" typedef struct { PyObject_HEAD PyObject *x; PyObject *y; } PointObject; static void Point_dealloc(PointObject *self); static PyObject* Point_new(PyTypeObject *type, PyObject *args, PyObject *kwds); static int Point_init(PointObject *self, PyObject *args, PyObject *kwds); static PyObject* Point_magnitude(PointObject *self); static PyObject* Point_add(PointObject *self, PyObject *other); static PyMemberDef Point_members[] = { // Define encapsulated attributes here {NULL} /* Sentinel */ }; static PyMethodDef Point_methods[] = { {\\"magnitude\\", (PyCFunction)Point_magnitude, METH_NOARGS, \\"Calculate magnitude from origin\\"}, {\\"add\\", (PyCFunction)Point_add, METH_O, \\"Add another point\\"}, {NULL} /* Sentinel */ }; static PyTypeObject PointType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"pointmodule.Point\\", .tp_doc = \\"Point objects\\", .tp_basicsize = sizeof(PointObject), .tp_itemsize = 0, .tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_HAVE_GC, .tp_new = Point_new, .tp_init = (initproc)Point_init, .tp_dealloc = (destructor)Point_dealloc, .tp_methods = Point_methods, .tp_members = Point_members, // Add other tp_slots for garbage collection and memory management if necessary }; static PyModuleDef pointmodule = { PyModuleDef_HEAD_INIT, .m_name = \\"pointmodule\\", .m_doc = \\"Example module that creates an extension type Point.\\", .m_size = -1, }; PyMODINIT_FUNC PyInit_pointmodule(void) { PyObject *m; if (PyType_Ready(&PointType) < 0) return NULL; m = PyModule_Create(&pointmodule); if (m == NULL) return NULL; Py_INCREF(&PointType); if (PyModule_AddObject(m, \\"Point\\", (PyObject *)&PointType) < 0) { Py_DECREF(&PointType); Py_DECREF(m); return NULL; } return m; } ``` # Deliverables - `point.c` : The C extension source file. - `setup.py` : The setup script to build the module. Validate your implementation by importing `pointmodule` and performing operations like creating `Point` objects, calculating magnitudes, and adding points.","solution":"# solution.py from math import sqrt class Point: def __init__(self, x=0.0, y=0.0): self._x = float(x) self._y = float(y) @property def x(self): return self._x @x.setter def x(self, value): self._x = float(value) @property def y(self): return self._y @y.setter def y(self, value): self._y = float(value) def magnitude(self): return sqrt(self._x**2 + self._y**2) def add(self, other): if not isinstance(other, Point): raise TypeError(\\"The operand must be an instance of Point\\") return Point(self._x + other.x, self._y + other.y) def __repr__(self): return f\\"Point({self._x}, {self._y})\\" # This file is a placeholder for simulating the behaviour implemented directly in C extension"},{"question":"Advanced Garbage Collection Management Objective: Write a Python program that demonstrates advanced management of the garbage collector using the `gc` module. Your program should enable garbage collection, perform a controlled garbage collection, retrieve and analyze statistics, and handle uncollectable objects. Task: 1. **Enable and Disable GC**: Initially disable the garbage collector using `gc.disable()`. Create a few objects that reference each other forming a reference cycle and store them in a list. 2. **Manual Collection**: Re-enable the garbage collector using `gc.enable()`. Manually trigger a full garbage collection using `gc.collect(generation=2)`. 3. **Retrieve and Display Statistics**: Retrieve garbage collection statistics and display them, including the number of collections per generation and the count of collected and uncollectable objects. 4. **Handle Uncollectable Objects**: - Set the garbage collector to save all unreachable objects using `gc.set_debug(gc.DEBUG_SAVEALL)`. - Perform another garbage collection and retrieve the list of unreachable but uncollectable objects from `gc.garbage`. 5. **Callback Implementation**: Attach a callback function to `gc.callbacks` that logs the start and stop of the garbage collection process, including the number of objects collected and uncollectable. 6. **Analysis Report**: Implement a function that summarizes the findings including: - The number of objects collected. - The number of unreachable but uncollectable objects. - Any insights or patterns observed in the garbage collection statistics. Requirements: - Your program should be self-contained and not rely on any external input. - You should use appropriate functions from the `gc` module to manage and retrieve garbage collection information. - Ensure that your program includes comments explaining each step or function usage for clarity. Example Output: The program should print: 1. Initial creation of objects and disabling the garbage collector. 2. Statistics after manually triggering garbage collection. 3. List of uncollected objects found. 4. Logs from the callback during garbage collection. 5. A summary report of the findings. Implementation: ```python import gc import sys def gc_callback(phase, info): print(f\\"GC Phase: {phase}\\") print(f\\"Info: {info}\\") def create_reference_cycle(): obj1 = {} obj2 = {\'ref\': obj1} obj1[\'ref\'] = obj2 return [obj1, obj2] def analyze_stats(): stats = gc.get_stats() print(\\"nGC Statistics:n\\") for gen, stat in enumerate(stats): print(f\\"Generation {gen}:\\") print(f\\" Collections: {stat[\'collections\']}\\") print(f\\" Collected: {stat[\'collected\']}\\") print(f\\" Uncollectable: {stat[\'uncollectable\']}\\") print(\\"n\\") def main(): # 1. Disable GC and create reference cycles gc.disable() ref_cycles = create_reference_cycle() print(\\"Created reference cycles.\\") # 2. Enable GC and perform manual collection gc.enable() unreachable = gc.collect(generation=2) print(f\\"Unreachable objects after manual collection: {unreachable}\\") # 3. Retrieve and display statistics analyze_stats() # 4. Handle uncollectable objects gc.set_debug(gc.DEBUG_SAVEALL) gc.collect() print(f\\"nUncollectable objects found: {len(gc.garbage)}\\") # Output the list of garbage for obj in gc.garbage: print(obj) # 5. Attach callback for garbage collection process gc.callbacks.append(gc_callback) gc.collect() # 6. Analysis Report print(\\"nAnalysis Report:\\") print(f\\"Objects Collected: {unreachable}\\") print(f\\"Uncollectable Objects: {len(gc.garbage)}\\") if __name__ == \\"__main__\\": main() ``` Ensure to test your program in a suitable Python environment. Your implementation should showcase a deep understanding of managing garbage collection using the `gc` module.","solution":"import gc def gc_callback(phase, info): print(f\\"GC Phase: {phase}\\") print(f\\"Info: {info}\\") def create_reference_cycle(): obj1 = {} obj2 = {\'ref\': obj1} obj1[\'ref\'] = obj2 return [obj1, obj2] def analyze_stats(): stats = gc.get_stats() analysis = \\"nGC Statistics:n\\" for gen, stat in enumerate(stats): analysis += f\\"Generation {gen}:n\\" analysis += f\\" Collections: {stat[\'collections\']}n\\" analysis += f\\" Collected: {stat[\'collected\']}n\\" analysis += f\\" Uncollectable: {stat[\'uncollectable\']}n\\" return analysis def handle_garbage(): gc.collect() uncollectable_objects = gc.garbage garbage_info = f\\"nUncollectable objects found: {len(uncollectable_objects)}n\\" return garbage_info, uncollectable_objects def main(): # 1. Disable GC and create reference cycles gc.disable() ref_cycles = create_reference_cycle() print(\\"Created reference cycles.\\") # 2. Enable GC and perform manual collection gc.enable() unreachable = gc.collect(generation=2) print(f\\"Unreachable objects after manual collection: {unreachable}\\") # 3. Retrieve and display statistics stats = analyze_stats() print(stats) # 4. Handle uncollectable objects gc.set_debug(gc.DEBUG_SAVEALL) garbage_info, uncollectable_objs = handle_garbage() print(garbage_info) # Output the list of garbage for obj in uncollectable_objs: print(obj) # 5. Attach callback for garbage collection process gc.callbacks.append(gc_callback) gc.collect() # 6. Analysis Report print(\\"nAnalysis Report:\\") print(f\\"Objects Collected: {unreachable}\\") print(f\\"Uncollectable Objects: {len(gc.garbage)}\\") if __name__ == \\"__main__\\": main()"},{"question":"# PyTorch Coding Assessment Question Objective Design and implement a custom neural network using PyTorch\'s `torch.nn` module. The network should include the following elements: - Combination of convolutional, pooling, and fully connected (linear) layers. - Use of non-linear activations like ReLU. - Application of normalization techniques. - Implementation of dropout to prevent overfitting. - A forward pass that takes an input tensor and returns the output. Requirements 1. **Inputs and Outputs:** - Input: A batch of images with dimensions (batch_size, channels, height, width). - Output: A tensor of shape (batch_size, num_classes) where `num_classes` is the number of target classes. 2. **Constraints:** - Use at least one convolutional layer, one pooling layer, one fully connected layer, one normalization layer, and one dropout layer. - Activation functions must include ReLU. - The network should handle variable input batch sizes. 3. **Performance:** - Efficient implementation considering both memory and computational speed. - Provide a test case with a sample input tensor and show the output shape. Steps to Follow 1. Define a custom neural network class `CustomNet` that inherits from `nn.Module`. 2. In `__init__` method, set up the layers according to the requirements. 3. Implement the `forward` method to define how the data flows through the network. 4. Write code to create an instance of `CustomNet`. 5. Provide a test case that demonstrates the network\'s functionality with a sample input tensor. Example ```python import torch import torch.nn as nn class CustomNet(nn.Module): def __init__(self, num_classes=10): super(CustomNet, self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1) self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) self.relu = nn.ReLU() self.norm = nn.BatchNorm2d(16) self.drop = nn.Dropout(p=0.5) self.fc = nn.Linear(16*16*16, num_classes) # Assuming input image size of 32x32 def forward(self, x): x = self.conv1(x) x = self.pool(x) x = self.relu(x) x = self.norm(x) x = torch.flatten(x, 1) # flatten all dimensions except batch x = self.drop(x) x = self.fc(x) return x # Create a sample input tensor with batch size 8 and 3 color channels (RGB image of size 32x32) input_tensor = torch.randn(8, 3, 32, 32) model = CustomNet(num_classes=10) output = model(input_tensor) # Print the output shape print(output.shape) # Expected output shape: (8, 10) ``` Ensure your implementation follows these guidelines. Submission Submit your `CustomNet` class implementation along with a sample input and its corresponding output after passing through the network. Explain the design choices you made and their benefits.","solution":"import torch import torch.nn as nn class CustomNet(nn.Module): def __init__(self, num_classes=10): super(CustomNet, self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1) self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) self.relu = nn.ReLU() self.norm = nn.BatchNorm2d(16) self.drop = nn.Dropout(p=0.5) self.fc = nn.Linear(16*16*16, num_classes) # Assuming input image size of 32x32 def forward(self, x): x = self.conv1(x) x = self.pool(x) x = self.relu(x) x = self.norm(x) x = torch.flatten(x, 1) # flatten all dimensions except batch x = self.drop(x) x = self.fc(x) return x # Create a sample input tensor with batch size 8 and 3 color channels (RGB image of size 32x32) input_tensor = torch.randn(8, 3, 32, 32) model = CustomNet(num_classes=10) output = model(input_tensor) # Print the output shape print(output.shape) # Expected output shape: (8, 10)"},{"question":"# PyTorch Coding Assessment: Custom Dataset and DataLoader Implementation Objective: The goal of this assessment is to evaluate your understanding of PyTorch\'s `Dataset` and `DataLoader` classes by implementing a custom dataset and utilizing various DataLoader functionalities. Problem Statement: You are given a dataset consisting of pairs of images and their corresponding labels. Your task is to implement a custom Map-style dataset `CustomImageDataset` and use PyTorch\'s `DataLoader` to efficiently load and preprocess this data. Additionally, you need to demonstrate the use of multi-process data loading and memory pinning. Instructions: 1. **Create a Custom Dataset:** - Implement a class `CustomImageDataset` that inherits from `torch.utils.data.Dataset`. - The dataset should initialize with a list of image file paths, a list of corresponding labels, and any necessary transformations. - Implement the `__len__` method to return the size of the dataset. - Implement the `__getitem__` method to read an image and its label from the file system, apply transformations, and return them. 2. **DataLoader Setup:** - Create an instance of `CustomImageDataset` and initialize it with some dummy data (e.g., dummy image paths and labels). - Create a `DataLoader` to batch and shuffle the data. Set the `batch_size` to 4 and enable multi-process data loading by setting `num_workers` to 2. - Use a default collate function to automatically batch the data. 3. **Memory Pinning:** - Enable memory pinning in the `DataLoader` by setting `pin_memory=True`. - Ensure that the fetched data tensors are pinned. 4. **Testing and Verification:** - Iterate through the batches of data using the `DataLoader` and print the shape of each batch to verify correct batching. - Print whether the tensors in the batch are pinned to memory. Code Skeleton: ```python import torch from torch.utils.data import Dataset, DataLoader from PIL import Image import numpy as np class CustomImageDataset(Dataset): def __init__(self, image_paths, labels, transform=None): self.image_paths = image_paths self.labels = labels self.transform = transform def __len__(self): # Return the total number of samples return len(self.image_paths) def __getitem__(self, idx): # Read the image from the file system image = Image.open(self.image_paths[idx]) label = self.labels[idx] # Apply transformations if any if self.transform: image = self.transform(image) # Convert image and label to tensors image = torch.tensor(np.array(image)) label = torch.tensor(label) return image, label # Dummy image paths and labels image_paths = [\'path/to/image1.jpg\', \'path/to/image2.jpg\', ...] labels = [0, 1, ...] # Create dataset instance dataset = CustomImageDataset(image_paths, labels) # Create DataLoader with multi-process loading and memory pinning dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True) # Iterate through the DataLoader and print batch shapes and memory pinning status for batch_idx, (images, labels) in enumerate(dataloader): print(f\\"Batch {batch_idx + 1}\\") print(f\\"Images shape: {images.shape}\\") print(f\\"Labels shape: {labels.shape}\\") print(f\\"Images pinned: {images.is_pinned()}\\") print(f\\"Labels pinned: {labels.is_pinned()}\\") ``` Expected Output: - The shape of each batch should be `(4, Height, Width)` for images and `(4,)` for labels. - The `is_pinned()` method should return `True` for both images and labels within the batch, indicating that memory pinning is correctly enabled. **Note**: Since this is a simulated example, you might need to replace `\'path/to/image1.jpg\'` and other paths with actual paths to image files on your system. Constraints: - Ensure that your custom dataset can handle any image size and type. - Handle potential issues such as missing or corrupted image files gracefully. - Ensure the custom dataset and data loader work efficiently with large datasets.","solution":"import torch from torch.utils.data import Dataset, DataLoader from PIL import Image import numpy as np import os class CustomImageDataset(Dataset): def __init__(self, image_paths, labels, transform=None): self.image_paths = image_paths self.labels = labels self.transform = transform def __len__(self): # Return the total number of samples return len(self.image_paths) def __getitem__(self, idx): # Read the image from the file system try: image = Image.open(self.image_paths[idx]).convert(\'RGB\') except (FileNotFoundError, IOError): image = Image.new(\'RGB\', (224, 224)) # Create a default image in case of error print(f\\"Warning: Failed to load {self.image_paths[idx]}\\") label = self.labels[idx] # Apply transformations if any if self.transform: image = self.transform(image) # Convert image and label to tensors image = torch.tensor(np.array(image), dtype=torch.float32).permute(2, 0, 1) label = torch.tensor(label, dtype=torch.int64) return image, label # Dummy image paths and labels image_paths = [\'path/to/image1.jpg\', \'path/to/image2.jpg\', \'path/to/image3.jpg\', \'path/to/image4.jpg\'] labels = [0, 1, 0, 1] # Dummy transformation (no transformations) transform = None # Create dataset instance dataset = CustomImageDataset(image_paths, labels, transform=transform) # Create DataLoader with multi-process loading and memory pinning dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2, pin_memory=True) # Iterate through the DataLoader and print batch shapes and memory pinning status for batch_idx, (images, labels) in enumerate(dataloader): print(f\\"Batch {batch_idx + 1}\\") print(f\\"Images shape: {images.shape}\\") print(f\\"Labels shape: {labels.shape}\\") print(f\\"Images pinned: {images.is_pinned()}\\") print(f\\"Labels pinned: {labels.is_pinned()}\\")"},{"question":"# Pandas Data Handling with Duplicate Labels You are provided with a CSV file named `data.csv` that contains raw, messy data. This CSV file potentially has duplicate labels in its index. Your task is to process this data with the following goals: 1. **Load the Data:** - Load the data from the CSV file into a DataFrame. 2. **Detect and Handle Duplicates:** - Check and report if there are any duplicate row indices. - If duplicates are found, print out the duplicated labels and indices. 3. **Remove Duplicates:** - Deduplicate the DataFrame by: - Keeping the first occurrence of each duplicate index row. - Aggregating data by taking the average of all rows with the same index if necessary. 4. **Set Constraints:** - Disallow any further duplicate labels in the DataFrame using the `allows_duplicate_labels` flag. 5. **Output the Processed Data:** - Print the final deduplicated DataFrame. - Save the deduplicated DataFrame to a new CSV file named `clean_data.csv`. # Function Definition Please implement the function `process_data` which takes one argument: - `file_path` (str): The path to the CSV file to be processed. The function should not return anything but should print the necessary information and save the processed DataFrame to `clean_data.csv`. # Constraints and Notes - You should not assume the data in the CSV file has any specific format other than potential duplicate row indices. - Ensure that the steps to handle duplicates are clear and that any further operations on the DataFrame prevent reintroducing duplicate labels. - If no duplicates are found, the original DataFrame should be used directly and saved. # Example Assuming `data.csv` has the following content: ``` index,column1,column2,column3 a,10,20,30 b,15,25,35 a,20,30,40 c,25,35,45 b,30,40,50 ``` Running your function `process_data(\'data.csv\')` should print something like: ``` Duplicate indices detected: [\'a\', \'b\'] Indices positions: [0, 2] for \'a\', [1, 4] for \'b\' ``` And produce a `clean_data.csv` file with content like: ``` index,column1,column2,column3 a,15.0,25.0,35.0 b,22.5,32.5,42.5 c,25.0,35.0,45.0 ``` > **Hint:** Use DataFrame\'s `duplicated`, `groupby`, and `set_flags` functions as described in the provided documentation.","solution":"import pandas as pd def process_data(file_path): # Load the DataFrame df = pd.read_csv(file_path, index_col=0) # Check for duplicates in the index duplicated_indices = df.index[df.index.duplicated()].unique() if len(duplicated_indices) > 0: print(f\\"Duplicate indices detected: {duplicated_indices.tolist()}\\") for label in duplicated_indices: positions = df.index.get_loc(label) print(f\\"Indices positions: {positions} for \'{label}\'\\") # Handle duplicates by averaging rows with the same index df = df.groupby(df.index).mean() # Disallow further duplicate labels df = df.set_flags(allows_duplicate_labels=False) # Print the final DataFrame print(df) # Save the DataFrame to a new CSV file df.to_csv(\'clean_data.csv\') # Example usage: # process_data(\'data.csv\')"},{"question":"Objective: To assess your understanding of file handling, manipulating audio data, and working with the `wave` module in Python. Problem Statement: You are required to implement two functions using the `wave` module in Python: 1. **`read_wave_file(file_path: str) -> tuple`**: - This function should read a WAV file from the specified `file_path`, and return a tuple containing: - The number of audio channels (e.g., 1 for mono, 2 for stereo). - The sample width in bytes. - The frame rate (sampling frequency) in Hz. - The number of audio frames. - A bytes object containing the actual audio frames. 2. **`write_wave_file(audio_data: tuple, output_path: str)`**: - This function should create a new WAV file at the specified `output_path` using the provided `audio_data`. The `audio_data` tuple should be in the format returned by the `read_wave_file` function. Input and Output: - **Input:** - For `read_wave_file`, the input is a string `file_path` which specifies the path to an existing WAV file. - For `write_wave_file`, the input is a tuple `audio_data` and a string `output_path`. The `audio_data` tuple has the following structure: `(num_channels, sample_width, frame_rate, num_frames, frames)`, where: - `num_channels` (int): Number of audio channels. - `sample_width` (int): Sample width in bytes. - `frame_rate` (int): Frame rate (sampling frequency) in Hz. - `num_frames` (int): Number of audio frames. - `frames` (bytes): The actual audio frames data. - **Output:** - `read_wave_file` returns a tuple with the aforementioned structure. - `write_wave_file` writes a WAV file to the specified path; it does not return anything. Constraints: - You may assume that the input WAV files are well-formed. - Focus on correctly handling the file operations and adhering to WAV file format specifications. - Ensure that your solution handles possible exceptions such as file not found or read/write errors gracefully. Example: ```python # Sample usage: audio_data = read_wave_file(\\"example.wav\\") write_wave_file(audio_data, \\"output.wav\\") ``` # Notes: - You need to import the `wave` module. - Please handle the file operations properly using context managers to ensure files are closed after operations.","solution":"import wave def read_wave_file(file_path: str) -> tuple: Reads a WAV file and returns its properties and audio frames. Args: - file_path (str): Path to the WAV file. Returns: - Tuple containing: - Number of audio channels (int). - Sample width in bytes (int). - Frame rate in Hz (int). - Number of audio frames (int). - Audio frames as a bytes object. with wave.open(file_path, \'rb\') as wav_file: num_channels = wav_file.getnchannels() sample_width = wav_file.getsampwidth() frame_rate = wav_file.getframerate() num_frames = wav_file.getnframes() frames = wav_file.readframes(num_frames) return num_channels, sample_width, frame_rate, num_frames, frames def write_wave_file(audio_data: tuple, output_path: str): Writes audio data to a new WAV file at the specified path. Args: - audio_data (tuple): Audio data in the format (num_channels, sample_width, frame_rate, num_frames, frames). - output_path (str): Path to save the new WAV file. num_channels, sample_width, frame_rate, num_frames, frames = audio_data with wave.open(output_path, \'wb\') as wav_file: wav_file.setnchannels(num_channels) wav_file.setsampwidth(sample_width) wav_file.setframerate(frame_rate) wav_file.setnframes(num_frames) wav_file.writeframes(frames)"},{"question":"# PyTorch Gradient Checking **Objective**: Implement a function that leverages PyTorch\'s `gradcheck` and `gradgradcheck` utilities to verify the gradients of both real and complex functions. Your implementation should handle both default and fast modes, appropriately interpreting the outputs to validate gradient correctness. --- Problem Statement You are provided with two types of functions: 1. A real-valued function `f: R^N -> R^M` where inputs are real vectors and outputs are real vectors. 2. A complex-valued function `g: C^N -> R^M` where inputs are complex vectors and outputs are real vectors. Both functions are passed as inputs to your solution along with their test cases. Your task is to: 1. Implement a verification function to check the correctness of their gradients using both `gradcheck` and `gradgradcheck` from PyTorch. 2. Ensure your implementation can handle both default and fast modes. 3. Summarize the results in a clear and detailed manner. --- Function Signature ```python import torch from torch.autograd import gradcheck, gradgradcheck def verify_gradients(f_real, f_complex, inputs_real, inputs_complex, eps=1e-6, atol=1e-5, rtol=1e-3, fast_mode=False): Verify the gradients of given real and complex functions using PyTorch\'s gradcheck and gradgradcheck utilities. Parameters: - f_real (callable): A real-valued function f: R^N -> R^M. - f_complex (callable): A complex-valued function g: C^N -> R^M. - inputs_real (torch.Tensor): Input tensor(s) for the real-valued function. - inputs_complex (torch.Tensor): Input tensor(s) for the complex-valued function. - eps (float): A small epsilon value for numerical differentiation. - atol (float): Absolute tolerance for gradient checking. - rtol (float): Relative tolerance for gradient checking. - fast_mode (bool): Flag to indicate whether to use fast mode for gradient checking. Returns: - dict: A dictionary summarizing the results of gradient checks for both real and complex functions. pass ``` --- Inputs 1. **f_real**: A callable representing the real-valued function. 2. **f_complex**: A callable representing the complex-valued function. 3. **inputs_real**: A tensor or tuple of tensors representing the inputs to `f_real`. 4. **inputs_complex**: A tensor or tuple of tensors representing the inputs to `f_complex`. 5. **eps**: (Optional) Epsilon for numerical differentiation (`default=1e-6`). 6. **atol**: (Optional) Absolute tolerance for gradient checking (`default=1e-5`). 7. **rtol**: (Optional) Relative tolerance for gradient checking (`default=1e-3`). 8. **fast_mode**: (Optional) Boolean indicating whether to use fast mode for gradient checking (`default=False`). --- Outputs A dictionary summarizing the results of gradient checks, with structure: ```python { \\"real_gradcheck\\": bool, \\"real_gradgradcheck\\": bool, \\"complex_gradcheck\\": bool, \\"complex_gradgradcheck\\": bool, \\"errors\\": list } ``` Where: - **real_gradcheck**: Boolean indicating the result of `gradcheck` for the real function. - **real_gradgradcheck**: Boolean indicating the result of `gradgradcheck` for the real function. - **complex_gradcheck**: Boolean indicating the result of `gradcheck` for the complex function. - **complex_gradgradcheck**: Boolean indicating the result of `gradgradcheck` for the complex function. - **errors**: A list of error messages encountered during the checks. --- Constraints 1. Ensure the use of appropriate PyTorch functionalities and handle any potential exceptions. 2. The function should be able to verify gradients in both forward and backward AD modes. 3. Provide clear and concise error messages for any failed checks. --- **Example Input and Output** ```python def f_real(x): return x**2 def f_complex(z): return torch.abs(z)**2 inputs_real = torch.tensor([1.0, -2.0, 3.0], dtype=torch.double, requires_grad=True) inputs_complex = torch.tensor([1.0 + 1j, 0.5 - 0.5j, -1.5 + 1.5j], dtype=torch.complex128, requires_grad=True) result = verify_gradients(f_real, f_complex, inputs_real, inputs_complex, fast_mode=True) print(result) ``` Expected Output: ```python { \\"real_gradcheck\\": True, \\"real_gradgradcheck\\": True, \\"complex_gradcheck\\": True, \\"complex_gradgradcheck\\": True, \\"errors\\": [] } ``` **Note**: The above example demonstrates the typical structure and expected results. You need to handle more comprehensive tests internally in your function to verify the validity of gradients thoroughly.","solution":"import torch from torch.autograd import gradcheck, gradgradcheck def verify_gradients(f_real, f_complex, inputs_real, inputs_complex, eps=1e-6, atol=1e-5, rtol=1e-3, fast_mode=False): Verify the gradients of given real and complex functions using PyTorch\'s gradcheck and gradgradcheck utilities. Parameters: - f_real (callable): A real-valued function f: R^N -> R^M. - f_complex (callable): A complex-valued function g: C^N -> R^M. - inputs_real (torch.Tensor): Input tensor(s) for the real-valued function. - inputs_complex (torch.Tensor): Input tensor(s) for the complex-valued function. - eps (float): A small epsilon value for numerical differentiation. - atol (float): Absolute tolerance for gradient checking. - rtol (float): Relative tolerance for gradient checking. - fast_mode (bool): Flag to indicate whether to use fast mode for gradient checking. Returns: - dict: A dictionary summarizing the results of gradient checks for both real and complex functions. results = { \\"real_gradcheck\\": False, \\"real_gradgradcheck\\": False, \\"complex_gradcheck\\": False, \\"complex_gradgradcheck\\": False, \\"errors\\": [] } # Convert inputs to tuples if they are not already if not isinstance(inputs_real, tuple): inputs_real = (inputs_real,) if not isinstance(inputs_complex, tuple): inputs_complex = (inputs_complex,) try: results[\\"real_gradcheck\\"] = gradcheck(f_real, inputs_real, eps=eps, atol=atol, rtol=rtol, fast_mode=fast_mode) except Exception as e: results[\\"errors\\"].append(f\\"Real function gradcheck failed: {e}\\") try: results[\\"real_gradgradcheck\\"] = gradgradcheck(f_real, inputs_real, eps=eps, atol=atol, rtol=rtol, fast_mode=fast_mode) except Exception as e: results[\\"errors\\"].append(f\\"Real function gradgradcheck failed: {e}\\") try: results[\\"complex_gradcheck\\"] = gradcheck(f_complex, inputs_complex, eps=eps, atol=atol, rtol=rtol, fast_mode=fast_mode) except Exception as e: results[\\"errors\\"].append(f\\"Complex function gradcheck failed: {e}\\") try: results[\\"complex_gradgradcheck\\"] = gradgradcheck(f_complex, inputs_complex, eps=eps, atol=atol, rtol=rtol, fast_mode=fast_mode) except Exception as e: results[\\"errors\\"].append(f\\"Complex function gradgradcheck failed: {e}\\") return results"},{"question":"# PyTorch Coding Assessment Question **Objective**: This task is focused on your ability to manipulate module parameters in PyTorch, particularly during the conversion process. You will be required to implement PyTorch functions and demonstrate an understanding of module parameter management. # Problem Statement You are given a neural network implemented in PyTorch. Your task is to: 1. Implement a function `convert_model` that takes a PyTorch model and converts it such that: - All parameters are swapped or overwritten based on specific flags. - Uses the functions from `torch.__future__` module. 2. Implement and demonstrate the usage of these functions: - `toggle_overwrite_module_parameters` - `toggle_swap_module_parameters` - `apply_conversion_flags_and_convert` # Specifications 1. `toggle_overwrite_module_parameters`: - **Input**: A boolean `value`. - **Output**: None. - **Functionality**: This function sets the `overwrite_module_params_on_conversion` flag using `set_overwrite_module_params_on_conversion`. 2. `toggle_swap_module_parameters`: - **Input**: A boolean `value`. - **Output**: None. - **Functionality**: This function sets the `swap_module_params_on_conversion` flag using `set_swap_module_params_on_conversion`. 3. `apply_conversion_flags_and_convert`: - **Input**: A PyTorch model. - **Output**: A converted PyTorch model. - **Functionality**: This function converts the model\'s parameters based on the current states of `overwrite_module_params_on_conversion` and `swap_module_params_on_conversion`. 4. `convert_model`: - **Input**: A PyTorch model, two boolean flags (`overwrite_flag`, `swap_flag`). - **Output**: A PyTorch model. - **Functionality**: This function should: - Call `toggle_overwrite_module_parameters` with `overwrite_flag`. - Call `toggle_swap_module_parameters` with `swap_flag`. - Return the converted model using `apply_conversion_flags_and_convert`. # Constraints - The implementation should be efficient and use PyTorch in-built functions wherever applicable. - You should appropriately manage model parameters during conversion based on the flags. # Example ```python import torch.nn as nn import torch.__future__ as future # Define a simple PyTorch model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 10) def forward(self, x): return self.fc(x) # Instantiate the model model = SimpleModel() # Implement the required functions based on the above specifications # Example usage toggle_overwrite_module_parameters(True) toggle_swap_module_parameters(False) converted_model = convert_model(model, overwrite_flag=True, swap_flag=False) print(converted_model) ``` **Note**: Please make sure to use the functions provided by `torch.__future__` to implement your solution. The exact details of how parameters are managed during conversion should fit within the logical framework explained above.","solution":"import torch.nn as nn from torch.__future__ import set_overwrite_module_params_on_conversion, set_swap_module_params_on_conversion def toggle_overwrite_module_parameters(value: bool): set_overwrite_module_params_on_conversion(value) def toggle_swap_module_parameters(value: bool): set_swap_module_params_on_conversion(value) def apply_conversion_flags_and_convert(model: nn.Module) -> nn.Module: # Assuming the presence of a fictional function called convert_model in torch.__future__ # As there is no `convert_model` in PyTorch normally, we\'ll simulate the conversion for this task # The function convert_model(model) is assumed to exist within the torch.__future__ package # Here we\'ll mock a conversion process as it doesn\'t exist in actual PyTorch # This is pseudo code as the real implementation depends on PyTorch\'s future library # Note: Normally this should be a real conversion return model # Returning the model directly as there\'s no real function available def convert_model(model: nn.Module, overwrite_flag: bool, swap_flag: bool) -> nn.Module: toggle_overwrite_module_parameters(overwrite_flag) toggle_swap_module_parameters(swap_flag) return apply_conversion_flags_and_convert(model) # If a specific `convert_model` function existed in torch.__future__, it would be applied here. # Since PyTorch as of now doesn\'t include such a function, we are using the namespace correctly, # but this will not reflect actual functionality until such future updates."},{"question":"# Question: Parallel Sum Calculation using `concurrent.futures` **Objective:** Write a function that uses the `concurrent.futures` module to compute the sum of squares of a list of integers in parallel. Function Signature ```python def parallel_sum_of_squares(numbers: List[int], num_workers: int) -> int: pass ``` Input - `numbers`: A list of integers `[n1, n2, n3, ..., nk]` where `1 <= k <= 10^6` and `-10^4 <= ni <= 10^4`. - `num_workers`: An integer specifying the number of worker threads to use, where `1 <= num_workers <= 100`. Output - An integer which is the sum of the squares of the integers in the `numbers` list. Requirements 1. Use the `concurrent.futures.ThreadPoolExecutor` for parallel execution. 2. Divide the list of numbers among the worker threads such that the work is evenly distributed. 3. Handle any exceptions that may occur during the execution of the tasks. 4. Ensure that all worker threads have completed their execution before computing the final result. Example ```python numbers = [1, 2, 3, 4] num_workers = 2 print(parallel_sum_of_squares(numbers, num_workers)) # Output: 30 # Explanation: 1^2 + 2^2 + 3^2 + 4^2 = 1 + 4 + 9 + 16 = 30 ``` Constraints - Ensure that the implementation minimizes the overhead associated with creating and managing threads. - Think about performance implications and strive for an optimal solution. Note You may assume that the input list `numbers` is non-empty and that `num_workers` will not exceed the number of elements in the `numbers` list. Happy coding!","solution":"from typing import List import concurrent.futures def sum_of_squares(numbers: List[int]) -> int: return sum(x ** 2 for x in numbers) def parallel_sum_of_squares(numbers: List[int], num_workers: int) -> int: chunk_size = (len(numbers) + num_workers - 1) // num_workers sublists = [numbers[i:i + chunk_size] for i in range(0, len(numbers), chunk_size)] total_sum = 0 with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor: futures = [executor.submit(sum_of_squares, sublist) for sublist in sublists] for future in concurrent.futures.as_completed(futures): result = future.result() total_sum += result return total_sum"},{"question":"Advanced Violin Plots with Seaborn You are given a dataset and your task is to create various violin plots using the seaborn library. Follow the requirements below to demonstrate your understanding of the `violinplot` functionality in seaborn. Dataset You will use the Titanic dataset that comes preloaded in seaborn. ```python import seaborn as sns df = sns.load_dataset(\\"titanic\\") ``` Task Requirements 1. **Basic Violin Plot** - Create a basic violin plot showing the distribution of passenger ages. 2. **Bivariate Violin Plot** - Create a bivariate violin plot showing the distribution of passenger ages grouped by travel class. 3. **Violin Plot with Hue** - Create a violin plot showing the distribution of passenger ages grouped by travel class and colored by whether they survived or not. 4. **Split Violins** - Create a split violin plot to visualize the age distribution by travel class and survival status. The split violins should show the quartiles inside. 5. **Customized Violin Plot** - Create a violin plot where each violin\'s width is normalized by the number of observations, show the individual observations inside the violins, and restrict the KDE smoothing to the observed data range. 6. **Native Scale and Formatter** - Create a violin plot with the fare on the y-axis and the age, rounded to nearest decade, on the x-axis. Use a custom formatter to label the age categories in decades. Function Signature Your function should be named `create_violin_plots()` and should not take any parameters. All necessary imports should be done within the function. Example of Function Call ```python def create_violin_plots(): import seaborn as sns import matplotlib.pyplot as plt # Load dataset df = sns.load_dataset(\\"titanic\\") # Task 1: Basic Violin Plot sns.violinplot(x=df[\\"age\\"]) plt.show() # Task 2: Bivariate Violin Plot sns.violinplot(data=df, x=\\"age\\", y=\\"class\\") plt.show() # Task 3: Violin Plot with Hue sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"survived\\") plt.show() # Task 4: Split Violins sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"survived\\", split=True, inner=\\"quart\\") plt.show() # Task 5: Customized Violin Plot sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", inner=\\"point\\", density_norm=\\"count\\", cut=0) plt.show() # Task 6: Native Scale and Formatter decades = lambda x: f\\"{int(x)}s\\" sns.violinplot(x=df[\\"age\\"].round(-1), y=df[\\"fare\\"], formatter=decades) plt.show() # Call the function to generate the plots create_violin_plots() ``` Requirements and Constraints - You must use the seaborn library to create the plots. - Each plot should be shown one at a time using `plt.show()`. - Ensure to handle any missing data appropriately. - The function `create_violin_plots()` should not return any value, but it should display all the required plots. Evaluation Criteria - **Correctness**: Your plots should accurately represent the specified distributions. - **Completeness**: All six requirements should be fulfilled within the function. - **Style**: Your code should be well-organized and make appropriate use of seaborn\'s features.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_violin_plots(): # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Task 1: Basic Violin Plot plt.figure() sns.violinplot(x=df[\\"age\\"]).set_title(\\"Distribution of Passenger Ages\\") plt.show() # Task 2: Bivariate Violin Plot plt.figure() sns.violinplot(data=df, x=\\"age\\", y=\\"class\\").set_title(\\"Distribution of Passenger Ages by Class\\") plt.show() # Task 3: Violin Plot with Hue plt.figure() sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"survived\\", palette=\\"muted\\").set_title(\\"Ages Grouped by Class and Survival\\") plt.show() # Task 4: Split Violins plt.figure() sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"survived\\", split=True, inner=\\"quart\\", palette=\\"pastel\\").set_title(\\"Split Violin by Class and Survival\\") plt.show() # Task 5: Customized Violin Plot plt.figure() sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", inner=\\"point\\", scale=\\"count\\", cut=0).set_title(\\"Customized Violin Plot by Class\\") plt.show() # Task 6: Native Scale and Formatter df[\'age_decade\'] = df[\'age\'].dropna().apply(lambda x: f\\"{int(x // 10 * 10)}s\\") plt.figure() sns.violinplot(x=\\"age_decade\\", y=\\"fare\\", data=df).set_title(\\"Fare by Age Decade\\") plt.show()"},{"question":"# **Command-Line Arguments Handler with `argparse`** **Problem Statement:** You are required to implement a Python script that uses the `argparse` module to handle command-line arguments. The script will perform basic arithmetic operations (addition, subtraction, multiplication, and division) and must allow several configuration options via optional arguments. **Functionality Requirements:** 1. Define a Python script that: - Takes two positional arguments, `num1` and `num2`, which are integers. - Takes an optional argument `--operation` (or `-o`) to specify the arithmetic operation. Valid operations are `add`, `sub`, `mul`, and `div`. Default operation is `add`. - Takes an optional verbosity flag `--verbose` (or `-v`). If specified, the script should provide detailed output. 2. Display the result of the arithmetic operation specified by the user or add the two numbers if no operation is specified. 3. If the `--verbose` flag is provided, the script should output the operation details (e.g., \\"Adding 5 and 3 gives 8\\"). Without the flag, just display the result number. **Input Format:** - Two positional integer arguments: `num1` and `num2` - One optional argument: `-o` or `--operation` (a string: `add`, `sub`, `mul`, or `div`) - One optional flag: `-v` or `--verbose` **Output Format:** - Display the result of the arithmetic operation. - If `--verbose` is specified, display a detailed description alongside the result. **Examples:** **Example 1:** ```bash python3 script.py 5 3 -o add -v Adding 5 and 3 gives 8 ``` **Example 2:** ```bash python3 script.py 10 2 -o div 5.0 ``` **Example 3:** ```bash python3 script.py 15 5 20 ``` **Example 4:** ```bash python3 script.py 15 5 -v Adding 15 and 5 gives 20 ``` **Constraints:** - The script should gracefully handle and display a meaningful error when an invalid operation is provided. - Division by zero should be handled and an appropriate error message should be displayed. **Implementation:** Implement the functionality in a script named `script.py`. Here is a starting point for your script: ```python import argparse def main(): parser = argparse.ArgumentParser(description=\\"Perform basic arithmetic operations on two integers.\\") parser.add_argument(\\"num1\\", type=int, help=\\"The first integer\\") parser.add_argument(\\"num2\\", type=int, help=\\"The second integer\\") parser.add_argument(\\"-o\\", \\"--operation\\", choices=[\\"add\\", \\"sub\\", \\"mul\\", \\"div\\"], default=\\"add\\", help=\\"The operation to perform: add, sub, mul, div\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"Increase output verbosity\\") args = parser.parse_args() num1 = args.num1 num2 = args.num2 operation = args.operation verbose = args.verbose if operation == \\"add\\": result = num1 + num2 operation_str = \\"Adding\\" elif operation == \\"sub\\": result = num1 - num2 operation_str = \\"Subtracting\\" elif operation == \\"mul\\": result = num1 * num2 operation_str = \\"Multiplying\\" elif operation == \\"div\\": if num2 == 0: print(\\"Error: Division by zero is not allowed.\\") return result = num1 / num2 operation_str = \\"Dividing\\" if verbose: print(f\\"{operation_str} {num1} and {num2} gives {result}\\") else: print(result) if __name__ == \\"__main__\\": main() ``` Make sure to test your script with various input scenarios to ensure robustness.","solution":"import argparse def perform_operation(num1, num2, operation, verbose): if operation == \\"add\\": result = num1 + num2 operation_str = \\"Adding\\" elif operation == \\"sub\\": result = num1 - num2 operation_str = \\"Subtracting\\" elif operation == \\"mul\\": result = num1 * num2 operation_str = \\"Multiplying\\" elif operation == \\"div\\": if num2 == 0: return \\"Error: Division by zero is not allowed.\\" result = num1 / num2 operation_str = \\"Dividing\\" if verbose: return f\\"{operation_str} {num1} and {num2} gives {result}\\" else: return result def main(): parser = argparse.ArgumentParser(description=\\"Perform basic arithmetic operations on two integers.\\") parser.add_argument(\\"num1\\", type=int, help=\\"The first integer\\") parser.add_argument(\\"num2\\", type=int, help=\\"The second integer\\") parser.add_argument(\\"-o\\", \\"--operation\\", choices=[\\"add\\", \\"sub\\", \\"mul\\", \\"div\\"], default=\\"add\\", help=\\"The operation to perform: add, sub, mul, div\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"Increase output verbosity\\") args = parser.parse_args() result = perform_operation(args.num1, args.num2, args.operation, args.verbose) print(result) if __name__ == \\"__main__\\": main()"},{"question":"Question You are provided with a list of file paths and a reference directory. Your task is to write a Python function that processes these paths using functionalities from the `os.path` module. Specifically, you need to: 1. Normalize all paths and convert them to their absolute versions. 2. Determine the longest common sub-path among all the given paths. 3. For each path, return the base name and the size of the file it points to, only if the file exists. 4. Return this information in a structured manner. # Function Signature ```python def process_paths(paths: List[str], reference_dir: str) -> Dict[str, Any]: pass ``` # Input - `paths`: A list of strings where each string is a file path. - `reference_dir`: A string representing the reference directory from which relative paths should be resolved. # Output The function should return a dictionary with the following structure: - `\\"common_subpath\\"`: The longest common sub-path among all the given paths. - `\\"details\\"`: A list of dictionaries for each path in the input `paths`. Each dictionary should contain: - `\\"original_path\\"`: The original path provided in the `paths` list. - `\\"absolute_path\\"`: The normalized, absolute version of the original path. - `\\"basename\\"`: The base name of the file of the normalized path. - `\\"size\\"`: The size of the file in bytes if it exists, otherwise it should be `None`. # Constraints - You can assume that the paths list is not empty and contains valid paths. - The length of the paths list does not exceed 1000. - Ensure that necessary error handling is done, especially when dealing with file existence and permissions. # Example ```python paths = [ \\"docs/report.txt\\", \\"/home/user/docs/chapter1.docx\\", \\"~/project/docs/notes.md\\" ] reference_dir = \\"/home/user\\" result = process_paths(paths, reference_dir) # Sample Output Format # { # \\"common_subpath\\": \\"/home/user/docs\\", # \\"details\\": [ # { # \\"original_path\\": \\"docs/report.txt\\", # \\"absolute_path\\": \\"/home/user/docs/report.txt\\", # \\"basename\\": \\"report.txt\\", # \\"size\\": 1024 # Example file size # }, # { # \\"original_path\\": \\"/home/user/docs/chapter1.docx\\", # \\"absolute_path\\": \\"/home/user/docs/chapter1.docx\\", # \\"basename\\": \\"chapter1.docx\\", # \\"size\\": 2048 # Example file size # }, # { # \\"original_path\\": \\"~/project/docs/notes.md\\", # \\"absolute_path\\": \\"/home/user/project/docs/notes.md\\", # \\"basename\\": \\"notes.md\\", # \\"size\\": 512 # Example file size # }, # ] # } ``` # Notes - Use the functions provided in the `os.path` module to perform the operations. - Ensure all paths are handled correctly according to the OS path specification. Good luck!","solution":"import os from typing import List, Dict, Any def process_paths(paths: List[str], reference_dir: str) -> Dict[str, Any]: # Normalize and convert paths to absolute paths absolute_paths = [os.path.abspath(os.path.expanduser(os.path.join(reference_dir, path))) for path in paths] # Determine the longest common sub-path common_subpath = os.path.commonpath(absolute_paths) details = [] for original_path, absolute_path in zip(paths, absolute_paths): basename = os.path.basename(absolute_path) if os.path.exists(absolute_path) and os.path.isfile(absolute_path): size = os.path.getsize(absolute_path) else: size = None details.append({ \\"original_path\\": original_path, \\"absolute_path\\": absolute_path, \\"basename\\": basename, \\"size\\": size }) return { \\"common_subpath\\": common_subpath, \\"details\\": details }"},{"question":"# Curses-Based Todo List Application You are required to implement a simple text-based \\"Todo List\\" application using the curses module in Python. The application should allow users to add, view, and delete tasks through a basic user interface controlled by keyboard input. Function Specifications 1. **`main(stdscr)`**: - Initialize the screen using curses functions. - Initialize the todo list (initially empty). - Continuously display the main menu options and process user inputs until the user decides to exit the application. 2. **Menu Options**: - **\'a\'**: Add a new task. - **\'d\'**: Delete a task. - **\'v\'**: View all tasks. - **\'q\'**: Quit the application. 3. **Helper Functions**: - **`add_task(stdscr, tasks)`**: Adds a new task. Prompt the user to enter a task description and append it to the tasks list. - **`delete_task(stdscr, tasks)`**: Deletes a task. Display a list of tasks with indices, prompting the user to select the index of the task to be deleted. - **`view_tasks(stdscr, tasks)`**: Displays all tasks in the list. - **`display_menu(stdscr)`**: Displays the main menu options. Input and Output Formats - **Input**: The user\'s inputs through the keyboard (characters \'a\', \'d\', \'v\', \'q\', and corresponding data for tasks). - **Output**: Changes to the terminal display reflecting the user\'s actions (adding, deleting, viewing tasks, or showing menu). Detailed Requirements 1. **Initialization**: - Use `curses.initscr()` to initialize the screen. - Use `curses.noecho()` to turn off input echoing. - Use `curses.cbreak()` to react to keys instantly. - Enable keypad mode with `stdscr.keypad(True)`. 2. **Task Management**: - When adding a task, use `curs_set(True)` to show the cursor for input. - Tasks should be displayed in a numbered list when viewing and deleting. 3. **Display and Refresh**: - Use `stdscr.clear()` and `stdscr.refresh()` to manage screen updates. - Ensure the display updates appropriately after each action. 4. **Error Handling**: - Handle any unexpected inputs gracefully. - Restore the terminal state properly upon exiting using `curses.endwin()`. Here is the skeleton code to get you started: ```python import curses def add_task(stdscr, tasks): stdscr.clear() stdscr.addstr(\\"Enter task: \\") curses.echo() task = stdscr.getstr().decode(\'utf-8\') curses.noecho() tasks.append(task) stdscr.addstr(f\\"Task \'{task}\' added.nPress any key to return to menu.\\") stdscr.getch() def delete_task(stdscr, tasks): stdscr.clear() if not tasks: stdscr.addstr(\\"No tasks to delete.nPress any key to return to menu.\\") stdscr.getch() return stdscr.addstr(\\"Select task to delete:n\\") for idx, task in enumerate(tasks): stdscr.addstr(f\\"{idx + 1}. {task}n\\") stdscr.addstr(\\"Enter task number: \\") try: task_num = int(stdscr.getstr().decode(\'utf-8\')) if 1 <= task_num <= len(tasks): removed_task = tasks.pop(task_num - 1) stdscr.addstr(f\\"Task \'{removed_task}\' deleted.nPress any key to return to menu.\\") else: stdscr.addstr(\\"Invalid task number.nPress any key to return to menu.\\") except ValueError: stdscr.addstr(\\"Invalid input.nPress any key to return to menu.\\") stdscr.getch() def view_tasks(stdscr, tasks): stdscr.clear() if not tasks: stdscr.addstr(\\"No tasks available.nPress any key to return to menu.\\") else: for idx, task in enumerate(tasks): stdscr.addstr(f\\"{idx + 1}. {task}n\\") stdscr.addstr(\\"nPress any key to return to menu.\\") stdscr.getch() def display_menu(stdscr): stdscr.addstr(\\"TODO List Applicationn\\") stdscr.addstr(\\"a - Add taskn\\") stdscr.addstr(\\"d - Delete taskn\\") stdscr.addstr(\\"v - View tasksn\\") stdscr.addstr(\\"q - Quitn\\") def main(stdscr): curses.noecho() curses.cbreak() stdscr.keypad(True) tasks = [] while True: stdscr.clear() display_menu(stdscr) stdscr.refresh() choice = stdscr.getch() if choice == ord(\'a\'): add_task(stdscr, tasks) elif choice == ord(\'d\'): delete_task(stdscr, tasks) elif choice == ord(\'v\'): view_tasks(stdscr, tasks) elif choice == ord(\'q\'): break curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main) ``` **Constraints**: - Ensure proper cleanup of the terminal state even if an unexpected error occurs. - Handle all edge cases, like deleting from an empty list or invalid task numbers, gracefully. **Performance Requirements**: - The application should remain responsive and manage user input efficiently. Implement the missing parts and ensure the application works correctly by handling user inputs and displaying the corresponding outputs.","solution":"import curses def add_task(stdscr, tasks): stdscr.clear() stdscr.addstr(\\"Enter task: \\") curses.echo() task = stdscr.getstr().decode(\'utf-8\') curses.noecho() tasks.append(task) stdscr.addstr(f\\"Task \'{task}\' added.nPress any key to return to menu.\\") stdscr.getch() def delete_task(stdscr, tasks): stdscr.clear() if not tasks: stdscr.addstr(\\"No tasks to delete.nPress any key to return to menu.\\") stdscr.getch() return stdscr.addstr(\\"Select task to delete:n\\") for idx, task in enumerate(tasks): stdscr.addstr(f\\"{idx + 1}. {task}n\\") stdscr.addstr(\\"Enter task number: \\") try: task_num = int(stdscr.getstr().decode(\'utf-8\')) if 1 <= task_num <= len(tasks): removed_task = tasks.pop(task_num - 1) stdscr.addstr(f\\"Task \'{removed_task}\' deleted.nPress any key to return to menu.\\") else: stdscr.addstr(\\"Invalid task number.nPress any key to return to menu.\\") except ValueError: stdscr.addstr(\\"Invalid input.nPress any key to return to menu.\\") stdscr.getch() def view_tasks(stdscr, tasks): stdscr.clear() if not tasks: stdscr.addstr(\\"No tasks available.nPress any key to return to menu.\\") else: for idx, task in enumerate(tasks): stdscr.addstr(f\\"{idx + 1}. {task}n\\") stdscr.addstr(\\"nPress any key to return to menu.\\") stdscr.getch() def display_menu(stdscr): stdscr.addstr(\\"TODO List Applicationn\\") stdscr.addstr(\\"a - Add taskn\\") stdscr.addstr(\\"d - Delete taskn\\") stdscr.addstr(\\"v - View tasksn\\") stdscr.addstr(\\"q - Quitn\\") def main(stdscr): curses.noecho() curses.cbreak() stdscr.keypad(True) tasks = [] while True: stdscr.clear() display_menu(stdscr) stdscr.refresh() choice = stdscr.getch() if choice == ord(\'a\'): add_task(stdscr, tasks) elif choice == ord(\'d\'): delete_task(stdscr, tasks) elif choice == ord(\'v\'): view_tasks(stdscr, tasks) elif choice == ord(\'q\'): break curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"# Advanced Linear Algebra with PyTorch You are given multiple matrices, and the task is to perform various linear algebra operations using the PyTorch library. Your implementation should include functions to compute matrix inverses, solve linear systems, and perform matrix decompositions. **Task:** 1. Write a function `compute_matrix_inverse(matrix: torch.Tensor) -> torch.Tensor` that computes the inverse of a given square matrix. Use `torch.linalg.inv`. 2. Write a function `solve_linear_system(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor` that solves a linear system of equations given matrix `A` and matrix `B` where `A * X = B`. Use `torch.linalg.solve`. 3. Write a function `compute_svd(matrix: torch.Tensor) -> tuple` that performs Singular Value Decomposition (SVD) on a given matrix. Use `torch.linalg.svd`. **Input:** - For function `compute_matrix_inverse`, a single argument `matrix` which is a square matrix tensor. - For function `solve_linear_system`, two arguments `A` (coefficient matrix) and `B` (constant matrix) which are tensor matrices. - For function `compute_svd`, a single argument `matrix` which is a tensor matrix. **Output:** - For function `compute_matrix_inverse`, return the inverse of the input matrix. - For function `solve_linear_system`, return the solution tensor `X`. - For function `compute_svd`, return a tuple containing the left singular vectors, the singular values, and the right singular vectors of the input matrix. **Constraints:** - The matrices will be two-dimensional tensors. - All matrices provided as inputs will have valid shapes for the respective operations. **Example:** ```python import torch from typing import tuple def compute_matrix_inverse(matrix: torch.Tensor) -> torch.Tensor: return torch.linalg.inv(matrix) def solve_linear_system(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: return torch.linalg.solve(A, B) def compute_svd(matrix: torch.Tensor) -> tuple: U, S, Vh = torch.linalg.svd(matrix) return (U, S, Vh) # Example usage: A = torch.tensor([[3.0, 1.0], [2.0, 4.0]]) B = torch.tensor([[1.0], [2.0]]) # Computing inverse inverse_A = compute_matrix_inverse(A) # Solving linear system X = solve_linear_system(A, B) # SVD decomposition U, S, Vh = compute_svd(A) ``` Write your implementation following the example provided.","solution":"import torch from typing import Tuple def compute_matrix_inverse(matrix: torch.Tensor) -> torch.Tensor: Computes the inverse of a given square matrix using torch.linalg.inv. :param matrix: A square matrix tensor. :return: The inverse of the input matrix. return torch.linalg.inv(matrix) def solve_linear_system(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Solves the linear system of equations given matrix A and matrix B where A * X = B using torch.linalg.solve. :param A: Coefficient matrix tensor. :param B: Constant matrix tensor. :return: The solution tensor X. return torch.linalg.solve(A, B) def compute_svd(matrix: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Performs Singular Value Decomposition (SVD) on a given matrix using torch.linalg.svd. :param matrix: A matrix tensor. :return: A tuple containing the left singular vectors, the singular values, and the right singular vectors of the input matrix. U, S, Vh = torch.linalg.svd(matrix) return U, S, Vh"},{"question":"You are tasked with developing a resource management system for handling both synchronous and asynchronous resources using the `contextlib` module. Your goal is to ensure that resources are properly managed and cleaned up at the end of their usage, even when errors occur during processing. # Part 1: Synchronous Resource Management Implement a context manager using the `contextlib.contextmanager` decorator to manage a generic resource. The resource should be acquired on entering the context, and released on exiting the context. Your implementation should: 1. Acquire the resource (simulated by printing \\"Resource acquired\\"). 2. Yield the resource. 3. Release the resource (simulated by printing \\"Resource released\\"). Write a function `use_synchronous_resource` that uses your context manager to perform some operations (simulated by printing \\"Using resource\\"). # Part 2: Asynchronous Resource Management Implement an asynchronous context manager using the `contextlib.asynccontextmanager` decorator to manage an asynchronous resource. The resource should be acquired asynchronously and released asynchronously. Your implementation should: 1. Asynchronously acquire the resource (simulated by printing \\"Async resource acquired\\" and using `await asyncio.sleep(1)`). 2. Yield the resource. 3. Asynchronously release the resource (simulated by printing \\"Async resource released\\" and using `await asyncio.sleep(1)`). Write an asynchronous function `use_asynchronous_resource` that uses your asynchronous context manager to perform some operations (simulated by printing \\"Using async resource\\" and using `await asyncio.sleep(1)`). # Part 3: Combining Multiple Context Managers Finally, use the `contextlib.ExitStack` and `contextlib.AsyncExitStack` to manage multiple synchronous and asynchronous resources together. 1. Implement a function `manage_multiple_resources` that uses `contextlib.ExitStack` to manage multiple synchronous resources. 2. Implement an asynchronous function `manage_multiple_async_resources` that uses `contextlib.AsyncExitStack` to manage multiple asynchronous resources. # Constraints - The synchronous and asynchronous context managers should handle exceptions correctly and ensure proper cleanup. - Your code should demonstrate the correct use of `contextlib` utilities and decorators. # Example of Expected Output ``` Resource acquired Using resource Resource released Async resource acquired Using async resource Async resource released Managing multiple resources: Resource acquired Resource acquired Using multiple resources Resource released Resource released Managing multiple async resources: Async resource acquired Async resource acquired Using multiple async resources Async resource released Async resource released ``` # Implementation ```python import asyncio import contextlib # Part 1: Synchronous Resource Management @contextlib.contextmanager def sync_resource_manager(): print(\\"Resource acquired\\") try: yield \\"resource\\" finally: print(\\"Resource released\\") def use_synchronous_resource(): with sync_resource_manager() as resource: print(\\"Using resource\\") # Part 2: Asynchronous Resource Management @contextlib.asynccontextmanager async def async_resource_manager(): print(\\"Async resource acquired\\") try: yield \\"async resource\\" finally: print(\\"Async resource released\\") async def use_asynchronous_resource(): async with async_resource_manager() as resource: print(\\"Using async resource\\") await asyncio.sleep(1) # Part 3: Combining Multiple Context Managers def manage_multiple_resources(): with contextlib.ExitStack() as stack: resources = [stack.enter_context(sync_resource_manager()) for _ in range(2)] print(\\"Using multiple resources\\") async def manage_multiple_async_resources(): async with contextlib.AsyncExitStack() as stack: resources = [await stack.enter_async_context(async_resource_manager()) for _ in range(2)] print(\\"Using multiple async resources\\") await asyncio.sleep(1) # Running the examples if __name__ == \\"__main__\\": use_synchronous_resource() asyncio.run(use_asynchronous_resource()) print(\\"nManaging multiple resources:\\") manage_multiple_resources() print(\\"nManaging multiple async resources:\\") asyncio.run(manage_multiple_async_resources()) ```","solution":"import asyncio import contextlib # Part 1: Synchronous Resource Management @contextlib.contextmanager def sync_resource_manager(): print(\\"Resource acquired\\") try: yield \\"resource\\" finally: print(\\"Resource released\\") def use_synchronous_resource(): with sync_resource_manager() as resource: print(\\"Using resource\\") # Part 2: Asynchronous Resource Management @contextlib.asynccontextmanager async def async_resource_manager(): print(\\"Async resource acquired\\") await asyncio.sleep(1) try: yield \\"async resource\\" finally: await asyncio.sleep(1) print(\\"Async resource released\\") async def use_asynchronous_resource(): async with async_resource_manager() as resource: print(\\"Using async resource\\") await asyncio.sleep(1) # Part 3: Combining Multiple Context Managers def manage_multiple_resources(): with contextlib.ExitStack() as stack: resources = [stack.enter_context(sync_resource_manager()) for _ in range(2)] print(\\"Using multiple resources\\") async def manage_multiple_async_resources(): async with contextlib.AsyncExitStack() as stack: resources = [await stack.enter_async_context(async_resource_manager()) for _ in range(2)] print(\\"Using multiple async resources\\") await asyncio.sleep(1) # Example usage if __name__ == \\"__main__\\": use_synchronous_resource() asyncio.run(use_asynchronous_resource()) print(\\"nManaging multiple resources:\\") manage_multiple_resources() print(\\"nManaging multiple async resources:\\") asyncio.run(manage_multiple_async_resources())"},{"question":"# Kernel Density Estimation (KDE) Implementation Objective: To assess your understanding and ability to implement and utilize Kernel Density Estimation (KDE) using scikit-learn, you will write a function and answer a few questions regarding the impact of different parameters on the KDE model. Problem Statement: You are given a set of 2D points generated from a known distribution. Your task is to: 1. Implement a function `compute_kde` that fits a KDE model using scikit-learn and returns the estimated density at given points. 2. Experiment with different kernels and bandwidths, and observe their effects on the estimated densities. 3. Report your findings. Function Signature: ```python from sklearn.neighbors import KernelDensity import numpy as np def compute_kde(X_train, X_test, kernel=\'gaussian\', bandwidth=1.0): Fit a Kernel Density Estimator to the training data and compute the density at the test points. Parameters: - X_train: np.ndarray, shape (n_samples_train, n_features), The training data. - X_test: np.ndarray, shape (n_samples_test, n_features), The test data where density estimates are computed. - kernel: str, optional (default=\'gaussian\'), The kernel to use. One of [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'] - bandwidth: float, optional (default=1.0), The bandwidth (smoothing parameter) Returns: - densities: np.ndarray, shape (n_samples_test,), The log density of the test points. pass ``` Tasks: 1. **Implement the Function**: - Fit the `KernelDensity` model on `X_train` using the specified `kernel` and `bandwidth`. - Compute and return the log density estimates for `X_test`. 2. **Experimentation**: - Use the following kernels: \'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'. - Test with at least three different bandwidth values: 0.2, 1.0, and 2.0. - Plot the resulting density estimates for a grid of points covering the range of `X_train`. - Observe and report the effect of changing the kernel and bandwidth on the density estimates. 3. **Report**: - Write a brief report summarizing: - How different kernels affect the smoothness and shape of the resulting density estimates. - How different bandwidth values influence the model performance. Example Usage: ```python X_train = np.random.randn(100, 2) # 100 points sampled from a Gaussian distribution in 2D X_test = np.random.randn(50, 2) # 50 points sampled from a Gaussian distribution in 2D densities = compute_kde(X_train, X_test, kernel=\'gaussian\', bandwidth=1.0) print(densities) # Output will be the log density estimates for the test points ``` Constraints: - Use only the specified kernels and bandwidth values. - Ensure the function is efficient and runs within a reasonable time frame for large datasets.","solution":"from sklearn.neighbors import KernelDensity import numpy as np def compute_kde(X_train, X_test, kernel=\'gaussian\', bandwidth=1.0): Fit a Kernel Density Estimator to the training data and compute the density at the test points. Parameters: - X_train: np.ndarray, shape (n_samples_train, n_features), The training data. - X_test: np.ndarray, shape (n_samples_test, n_features), The test data where density estimates are computed. - kernel: str, optional (default=\'gaussian\'), The kernel to use. One of [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'] - bandwidth: float, optional (default=1.0), The bandwidth (smoothing parameter) Returns: - densities: np.ndarray, shape (n_samples_test,), The log density of the test points. kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(X_train) log_density = kde.score_samples(X_test) return log_density"},{"question":"**Objective**: Implement a function that mimics the functionality of the `2to3` fixer for the `print` statement and `raw_input` function. Problem Statement You are provided with Python 2 code as a string. Your task is to convert this Python 2 code to Python 3 code by replacing: 1. The `print` statement with the `print()` function. 2. The `raw_input()` function with `input()`. Specifically: - The `print` statement in Python 2 (e.g., `print \\"Hello\\"`) should be converted to the `print` function in Python 3 (e.g., `print(\\"Hello\\")`). - The `raw_input()` function in Python 2 should be converted to `input()` in Python 3. You need to implement the function `convert_to_python3(py2_code: str) -> str` that performs these transformations. Input - `py2_code` (str): A string containing Python 2 code. Output - (str): A string containing the transformed Python 3 code. Constraints - The input code will have a maximum length of 1000 characters. - The input code may contain multiple lines. Example # Input ```python py2_code = \'\'\'print \\"Hello, World!\\" name = raw_input(\\"Enter your name: \\") print \\"Hello, \\" + name\'\'\' ``` # Output ```python \'print(\\"Hello, World!\\")nname = input(\\"Enter your name: \\")nprint(\\"Hello, \\" + name)\' ``` # Function Signature ```python def convert_to_python3(py2_code: str) -> str: pass ``` Notes - You can assume that the `print` statements and the `raw_input` function calls do not have any syntax errors in the provided Python 2 code. - You do not need to handle nested structures where `print` or `raw_input` might appear within other constructs (e.g., inside strings) for simplicity. Good luck!","solution":"import re def convert_to_python3(py2_code: str) -> str: Converts Python 2 code to Python 3 by replacing print statements with print functions and raw_input() with input(). # Replace `raw_input()` with `input()` py3_code = py2_code.replace(\'raw_input(\', \'input(\') # Replace `print` statements with `print()` function calls. # We will use a regex to capture `print` statements and their arguments print_replacements = [] def replace_print_statement(match): # Extract the print content content = match.group(1) # Create the python3 print statement return f\'print({content})\' # Regex to match `print` statements pattern = re.compile(r\'print (.+)\') py3_code = pattern.sub(replace_print_statement, py3_code) return py3_code"},{"question":"You are given a dataset representing the relationship between advertising spend (`X`) and sales (`y`) for a company\'s product. Your goal is to use the `IsotonicRegression` class from the `scikit-learn` library to model this relationship with a non-decreasing function. # Task 1. Load the provided dataset into a pandas DataFrame. 2. Implement a function `fit_isotonic_regression` that fits an isotonic regression model to the data. 3. Use this model to predict the sales for a new set of advertising spend values. 4. Plot the original data points and the isotonic regression line. # Input - A CSV file containing the dataset with two columns: `Advertising Spend` and `Sales`. - A list of new advertising spend values for prediction. # Output - A list of predicted sales values for the new advertising spend inputs. - A plot displaying the original data points and the isotonic regression line. # Function Signature ```python def fit_isotonic_regression(file_path: str, new_advertising_spend: List[float]) -> List[float]: pass ``` # Constraints - The dataset contains at least 100 data points. - Ensure the predictions are non-decreasing. - The `new_advertising_spend` list must contain valid numerical values within the range of the original `Advertising Spend` values. # Example Given the following dataset in a file called \\"data.csv\\": ``` Advertising Spend,Sales 10,30 15,45 20,40 25,50 30,60 ``` and the following new advertising spend values: ```python new_advertising_spend = [12, 22, 28] ``` a call to `fit_isotonic_regression(\\"data.csv\\", new_advertising_spend)` should return: ```python [32.5, 45.0, 55.0] ``` and plot the following graph: - The scatter plot of the original data points. - The isotonic regression line fitting the data. # Notes 1. You may use libraries such as pandas, scikit-learn, and matplotlib for loading data, fitting the model, and plotting, respectively. 2. Ensure your code is efficient and handles edge cases gracefully.","solution":"import pandas as pd from sklearn.isotonic import IsotonicRegression import matplotlib.pyplot as plt import numpy as np from typing import List def fit_isotonic_regression(file_path: str, new_advertising_spend: List[float]) -> List[float]: # Load dataset data = pd.read_csv(file_path) # Extract the columns into X and y X = data[\'Advertising Spend\'].values y = data[\'Sales\'].values # Fit Isotonic Regression model ir = IsotonicRegression(increasing=True) ir.fit(X, y) # Predict for new advertising spends new_advertising_spend = np.array(new_advertising_spend) predicted_sales = ir.transform(new_advertising_spend) # Plot the original data and isotonic regression line plt.scatter(X, y, label=\'Original Data\') grid = np.linspace(X.min(), X.max(), 1000) plt.plot(grid, ir.transform(grid), color=\'red\', label=\'Isotonic Regression Line\') plt.scatter(new_advertising_spend, predicted_sales, color=\'green\', label=\'Predictions\') plt.xlabel(\'Advertising Spend\') plt.ylabel(\'Sales\') plt.title(\'Isotonic Regression of Advertising Spend vs. Sales\') plt.legend() plt.show() return predicted_sales.tolist()"},{"question":"Objective To demonstrate your understanding of file system path handling in Python, you are required to implement a function that safely retrieves the file system representation of a given path. Task Implement a function `get_filesystem_path` that takes a single argument `path` (which can be a string, bytes object, or an os.PathLike object) and returns its filesystem representation following the rules of `PyOS_FSPath` described in the documentation. If the path is not a string, bytes, or `os.PathLike`, raise a `TypeError`. Function Signature ```python def get_filesystem_path(path: Any) -> Union[str, bytes]: pass ``` Input - `path`: A variable that can be of type `str`, `bytes`, or an object implementing the `os.PathLike` interface. Output - Return the filesystem path as a `str` or `bytes` if the conversion is successful. - Raise a `TypeError` if the input path is not of an expected type. Constraints 1. You must use the `os.fspath()` function or similar approach to retrieve the filesystem path. 2. The function should handle different Python versions gracefully where `os.PathLike` is not implemented. Examples ```python >>> get_filesystem_path(\'/path/to/file\') \'/path/to/file\' >>> get_filesystem_path(b\'/path/to/file\') b\'/path/to/file\' >>> import os >>> get_filesystem_path(os.path.join(\'path\', \'to\', \'file\')) \'path/to/file\' >>> class CustomPath: ... def __fspath__(self): ... return \'/custom/path\' >>> get_filesystem_path(CustomPath()) \'/custom/path\' >>> get_filesystem_path(12345) Traceback (most recent call last): ... TypeError: Expected string, bytes or os.PathLike object, not int ``` Notes - Make sure to consider edge cases and test your solution with different types of inputs. - Clearly comment your code to explain the logic behind each step.","solution":"import os from typing import Any, Union def get_filesystem_path(path: Any) -> Union[str, bytes]: Returns the filesystem representation of the provided path. Parameters: path (Any): The path to convert, can be of type str, bytes, or an object implementing os.PathLike Returns: Union[str, bytes]: Filesystem path as a string or bytes Raises: TypeError: If the provided path is not of an expected type try: return os.fspath(path) except TypeError: raise TypeError(f\\"Expected string, bytes or os.PathLike object, not {type(path).__name__}\\")"},{"question":"You are tasked with creating a function `backup_directory_with_metadata` that creates a backup of a specified directory. The backup should be a complete replica of the source directory, including file metadata such as permissions, access times, and ownership. This function should handle symbolic links appropriately according to the provided configuration. Function Signature ```python def backup_directory_with_metadata(src: str, dst: str, follow_symlinks: bool = True, preserve_ownership: bool = True) -> str: pass ``` Parameters - `src (str)`: The path to the source directory that needs to be backed up. - `dst (str)`: The path to the destination directory where the backup will be stored. This should not be an existing directory. - `follow_symlinks (bool)`: A flag indicating whether to follow symbolic links (default is `True`). If `False`, symbolic links should be copied themselves. - `preserve_ownership (bool)`: A flag indicating whether to preserve file ownership (default is `True`). If `True` and the platform supports it, the ownership of files should be preserved in the backup. Returns - `str`: The path to the newly created backup directory. Constraints - The function should raise a `ValueError` if the specified `dst` directory already exists. - The function should raise a `FileNotFoundError` if the `src` directory does not exist. - The function should handle platform-specific differences in metadata preservation and raise an appropriate error if the platform does not support certain features. Example Usage ```python src_directory = \\"/path/to/source\\" dst_directory = \\"/path/to/destination\\" backup_directory_with_metadata(src_directory, dst_directory, follow_symlinks=False, preserve_ownership=True) # This should create a backup of src_directory at dst_directory, preserving ownership and copying symbolic links as links. ``` # Notes - You must use the `shutil` module\'s functions to achieve this. - Consider edge cases such as non-existent source paths, existing destination paths, and handling of symbolic links according to the `follow_symlinks` parameter. - Ensure that the function performs efficiently and handles large directories and files appropriately. - Document your code and add comments where necessary to explain your logic.","solution":"import os import shutil import errno def backup_directory_with_metadata(src: str, dst: str, follow_symlinks: bool = True, preserve_ownership: bool = True) -> str: Creates a backup of a specified directory with metadata. Parameters: - src (str): The path to the source directory that needs to be backed up. - dst (str): The path to the destination directory where the backup will be stored. - follow_symlinks (bool): Flag indicating whether to follow symbolic links (default is True). - preserve_ownership (bool): Flag indicating whether to preserve file ownership (default is True). Returns: - str: The path to the newly created backup directory. if not os.path.exists(src): raise FileNotFoundError(f\\"Source directory \'{src}\' does not exist.\\") if os.path.exists(dst): raise ValueError(f\\"Destination directory \'{dst}\' already exists.\\") try: shutil.copytree(src, dst, symlinks=not follow_symlinks) if preserve_ownership: for dirpath, dirnames, filenames in os.walk(dst): for name in dirnames + filenames: path = os.path.join(dirpath, name) src_path = os.path.join(src, os.path.relpath(path, start=dst)) if os.path.exists(src_path): shutil.copystat(src_path, path, follow_symlinks=follow_symlinks) return dst except Exception as e: shutil.rmtree(dst, ignore_errors=True) raise e"},{"question":"# Question: Extracting and Displaying Module Information You are provided with the `pyclbr` module documentation. Using the functionalities provided by this module, your task is to implement a function that reads a specified Python module and displays detailed information about the classes and functions defined within it. Function Signature ```python def display_module_info(module_name: str, path: Optional[List[str]] = None) -> None: ``` Input - `module_name`: A string with the name of the module to read. It may be the name of a module within a package. - `path`: An optional list of directory paths. If given, these paths are prepended to `sys.path` to locate the module source code. Output The function should print the following information: - The name of each top-level class and function in the module, along with the line number where it is defined. - For each class, list its methods and the line number where each method is defined. - For each function, print if it is an asynchronous function. - For any nested classes and functions, accurately represent their nesting and include similar details. Constraints 1. You must use the `pyclbr.readmodule_ex` function to read the module information. 2. The function should print the detailed information in a human-readable format, with proper indentation for nested elements. Example Given a Python module `example_module.py` with the following content: ```python class MyClass: def method1(self): pass async def method2(self): pass def my_function(): pass ``` Calling `display_module_info(\'example_module\')` should produce: ``` Top-level Classes and Functions: Class: MyClass, defined at line 1 Method: method1, defined at line 2 Method: method2, defined at line 4 (asynchronous) Function: my_function, defined at line 7 ``` Note that the example output should correctly represent nested classes and functions, if any. Hints - Explore the attributes of the `Function` and `Class` objects returned by `readmodule_ex` to gather the necessary details. - Use recursion if needed to handle nested definitions.","solution":"from typing import List, Optional import pyclbr import sys def display_module_info(module_name: str, path: Optional[List[str]] = None) -> None: if path: original_sys_path = sys.path[:] sys.path = path + sys.path try: module_info = pyclbr.readmodule_ex(module_name) finally: if path: sys.path = original_sys_path def display_class_info(cls, indent=0): indentation = \\" \\" * indent print(f\\"{indentation}Class: {cls.name}, defined at line {cls.lineno}\\") for method_name, line_number in sorted(cls.methods.items()): is_async = \\" (asynchronous)\\" if cls.async_methods and method_name in cls.async_methods else \\"\\" print(f\\"{indentation} Method: {method_name}, defined at line {line_number}{is_async}\\") for nested_class_name in cls.subclasses: display_class_info(module_info[nested_class_name], indent + 1) print(\\"Top-level Classes and Functions:\\") for item in sorted(module_info.values(), key=lambda x: x.lineno): if isinstance(item, pyclbr.Function): is_async = \\" (asynchronous)\\" if item.async_ else \\"\\" print(f\\"Function: {item.name}, defined at line {item.lineno}{is_async}\\") elif isinstance(item, pyclbr.Class): display_class_info(item)"},{"question":"# Asyncio Chat Server In this task, you are required to implement an asynchronous chat server using `asyncio` that can handle multiple clients simultaneously. The server should accept incoming connections, handle message broadcasting to all connected clients, and manage client disconnections gracefully. Requirements 1. **Server Setup**: - Create and run an asyncio event loop. - Create a TCP server that listens for incoming connections on a specified IP address and port (e.g., `127.0.0.1` and port `8888`). 2. **Client Connection Handling**: - Accept incoming client connections and add them to a list of active connections. - Each client should be handled in a separate task. 3. **Message Broadcasting**: - When a client sends a message, broadcast it to all other connected clients. - Ensure that the server can handle messages sent from multiple clients concurrently. 4. **Client Disconnection Handling**: - Detect when a client disconnects and remove them from the list of active connections. - Ensure that the server continues to operate correctly after a client disconnection. Constraints - Implement the solution using asyncio\'s low-level APIs, such as those for creating servers, managing tasks, and handling sockets. - Ensure to handle exceptions and edge cases, such as network errors or invalid data. - The server should support at least 10 concurrent client connections. Input and Output Format - **Input**: There is no specific input format since the server will be receiving connections and messages asynchronously. - **Output**: No specific output format; the server will broadcast client messages to other connected clients. Example For illustrative purposes: 1. Start the server on `127.0.0.1:8888`. 2. A client connects and sends a message \\"Hello\\". 3. The server broadcasts \\"Client 1: Hello\\" to all other clients. 4. Another client connects and receives subsequent messages broadcast by the server. Implementation Notes You will need to make use of methods like `loop.create_server()`, `loop.create_task()`, `transport.write()`, and appropriate protocols for handling connections and data reception. Here is a starting template to guide you: ```python import asyncio class ChatServerProtocol(asyncio.Protocol): def __init__(self, clients): self.clients = clients def connection_made(self, transport): self.transport = transport self.clients.append(self) address = transport.get_extra_info(\'peername\') print(f\'Connection from {address}\') def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') for client in self.clients: if client is not self: client.transport.write(data) def connection_lost(self, exc): self.clients.remove(self) print(\'Connection closed\') async def main(): clients = [] loop = asyncio.get_running_loop() server = await loop.create_server( lambda: ChatServerProtocol(clients), \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main()) ``` Complete the implementation to make sure it handles the requirements and edge cases effectively.","solution":"import asyncio class ChatServerProtocol(asyncio.Protocol): def __init__(self, clients): self.clients = clients def connection_made(self, transport): self.transport = transport self.clients.append(self) address = transport.get_extra_info(\'peername\') print(f\'Connection from {address}\') def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') for client in self.clients: if client is not self: client.transport.write(data) def connection_lost(self, exc): self.clients.remove(self) print(\'Connection closed\') async def main(): clients = [] loop = asyncio.get_running_loop() server = await loop.create_server( lambda: ChatServerProtocol(clients), \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main())"},{"question":"**Question: Analyzing and Visualizing Data with Seaborn** You are provided with a dataset containing information about various products and their prices over different time periods. Your task is to create a plot using the seaborn library that visualizes the trend of product prices over time. Additionally, you will need to utilize seaborn\'s styling capabilities to style the plot appropriately. **Dataset Format:** The dataset will be a list of dictionaries, where each dictionary represents a product\'s data: ```python data = [ {\\"product\\": \\"Product A\\", \\"month\\": \\"January\\", \\"price\\": 100}, {\\"product\\": \\"Product B\\", \\"month\\": \\"January\\", \\"price\\": 150}, ... ] ``` **Requirements:** 1. **Function Definition:** Define a function `plot_price_trends(data)` that accepts the dataset as input. 2. **Data Preparation:** Convert the list of dictionaries into a Pandas DataFrame for easier manipulation. 3. **Plotting:** - Create a line plot using seaborn that shows the price trends for each product over time. - Use different colors for each product. 4. **Styling:** - Retrieve and print the default seaborn style parameters. - Use the \\"darkgrid\\" style for the plot. - Add titles and labels to make the plot more informative. 5. **Temporary Style Change:** - Use the \\"whitegrid\\" style temporarily to create a secondary plot (e.g., a bar plot of prices in the latest month). **Constraints:** - You must use seaborn for the plotting. - The dataset will have at least 10 entries and up to 1000 entries. **Expected Function Signature:** ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_price_trends(data): # Your code here pass ``` **Example Usage:** ```python data = [ {\\"product\\": \\"Product A\\", \\"month\\": \\"January\\", \\"price\\": 100}, {\\"product\\": \\"Product B\\", \\"month\\": \\"January\\", \\"price\\": 150}, {\\"product\\": \\"Product A\\", \\"month\\": \\"February\\", \\"price\\": 110}, {\\"product\\": \\"Product B\\", \\"month\\": \\"February\\", \\"price\\": 160}, {\\"product\\": \\"Product A\\", \\"month\\": \\"March\\", \\"price\\": 105}, {\\"product\\": \\"Product B\\", \\"month\\": \\"March\\", \\"price\\": 170}, ] plot_price_trends(data) ``` **Expected Output:** - A line plot showing the price trends of the products over time. - A secondary plot temporarily using \\"whitegrid\\" styling, showing prices in the latest month (e.g., bar plot).","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_price_trends(data): Function to create a plot using seaborn that visualizes the trend of product prices over time. Parameters: data (list): List of dictionaries containing product data. Each dictionary should have \'product\', \'month\', and \'price\' keys. Returns: None # Convert the list of dictionaries into a Pandas DataFrame df = pd.DataFrame(data) # Print the default seaborn style parameters print(\\"Default seaborn style parameters:\\", sns.axes_style()) # Set the seaborn style to darkgrid sns.set(style=\\"darkgrid\\") # Create a line plot for price trends over time plt.figure(figsize=(10, 6)) sns.lineplot(x=\'month\', y=\'price\', hue=\'product\', data=df, marker=\'o\') # Add titles and labels plt.title(\\"Product Price Trends Over Time\\") plt.xlabel(\\"Month\\") plt.ylabel(\\"Price\\") plt.legend(title=\'Product\', loc=\'upper left\') # Show the plot plt.show() # Temporarily set the style to whitegrid and create a bar plot with sns.axes_style(\\"whitegrid\\"): # Get the latest month latest_month = df[\'month\'].unique()[-1] latest_data = df[df[\'month\'] == latest_month] # Create a bar plot for the latest month\'s data plt.figure(figsize=(10, 6)) sns.barplot(x=\'product\', y=\'price\', data=latest_data) # Add titles and labels plt.title(f\\"Product Prices in {latest_month}\\") plt.xlabel(\\"Product\\") plt.ylabel(\\"Price\\") # Show the plot plt.show()"},{"question":"You are tasked with deploying a Flask application called `flaskr` to a production server. The application has already been developed and tested locally. Now, you need to prepare it for a production environment. Your task is to complete the following steps: 1. **Build the Flask Application**: Create a distribution file for the `flaskr` application. Provide the code or command to generate this file. 2. **Set Up the Environment**: Explain how to set up a virtual environment and install the created distribution file on a new machine. 3. **Initialize the Database**: Write the command needed to initialize the database on the new machine. 4. **Configure the Secret Key**: Generate a new secret key for the production environment and explain how to configure it. 5. **Run with a Production Server**: Provide the command to run the `flaskr` application using Waitress. # Instructions: 1. Write the steps, commands, and scripts needed to deploy the `flaskr` application. 2. Ensure that your instructions are clear and executable in a real-world scenario. # Expected Outputs: 1. **Build Command**: The command or script to create the distribution file. 2. **Environment Setup**: Instructions and commands to set up a virtual environment and install the distribution file. 3. **Database Initialization**: The command to initialize the database. 4. **Secret Key Configuration**: The command to generate a new secret key and the script snippet to configure it in the application. 5. **Production Server Command**: The command to run the application using Waitress. # Constraints: 1. Assume `flaskr` is the name of the Flask application. 2. The instructions should be suitable for both Windows and Linux environments where applicable.","solution":"# The following steps outline how to deploy a Flask application named \'flaskr\' to a production server. # Step 1: Build the Flask Application # Run this command in the root directory of the \'flaskr\' application to create a distribution file. python setup.py sdist # Step 2: Set Up the Environment # On the new machine, follow these steps to set up a virtual environment and install the created distribution file. # Create a virtual environment python -m venv venv # Activate the virtual environment # On Windows: venvScriptsactivate # On Linux/MacOS: source venv/bin/activate # Install the created distribution file (replace \'flaskr-x.y.tar.gz\' with the correct file name) pip install dist/flaskr-x.y.tar.gz # Step 3: Initialize the Database # Run this command to initialize the database. flask init-db # Step 4: Configure the Secret Key # Generate a new secret key for the production environment. # Use Python\'s secrets module to generate a secure secret key. >>> python -c \'import secrets; print(secrets.token_hex(16))\' \'4f2e172b9f1e96120c75f416634d94d2\' # Configure the secret key in the application\'s configuration. # Edit the config.py file of the \'flaskr\' application to set the secret key. # Replace \'your-secret-key\' with the generated secret key. # config.py import os class Config: SECRET_KEY = os.environ.get(\'SECRET_KEY\') or \'4f2e172b9f1e96120c75f416634d94d2\' # Step 5: Run the application with a Production Server (Waitress) # Use the following command to run the \'flaskr\' application using Waitress. # Install Waitress pip install waitress # Run the application with Waitress (replace \'flaskr\' with the actual module name if different) waitress-serve --call \'flaskr:create_app\'"},{"question":"<|Analysis Begin|> The provided documentation is for the \\"wsgiref\\" module, which is a reference implementation of the WSGI (Web Server Gateway Interface) specification. The \\"wsgiref\\" module includes several submodules, each providing different utilities for working with WSGI in Python. These submodules include `wsgiref.util`, `wsgiref.headers`, `wsgiref.simple_server`, `wsgiref.validate`, and `wsgiref.handlers`. Among other things, these provide utility functions for environment manipulation, response header handling, a simple WSGI server implementation, a validation tool for WSGI applications, and base classes for implementing WSGI servers and gateways. Clearly, based on the documentation, WSGI and the `wsgiref` module are essential for creating web applications and servers that adhere to the WSGI standard. We can create a coding question that involves building a small WSGI application using the utilities and tools provided by the `wsgiref` module, specifically focusing on creating, handling, and responding to HTTP requests. <|Analysis End|> <|Question Begin|> # WSGI Application Challenge Create a WSGI application using the `wsgiref` module that serves a different response depending on the URL path specified in the request. Specifically, you need to implement an application object that: 1. Responds with a JSON object containing a \'message\' when the URL path is `/hello`. 2. Responds with a plain text message listing the HTTP request headers when the URL path is `/headers`. 3. Responds with a 404 Not Found error for any other URL path. You will use the `wsgiref.simple_server` to create and run this WSGI application. Make sure to use utilities provided within the `wsgiref` module where appropriate. Instructions: 1. Define the WSGI application object `my_app(environ, start_response)` that handles the specified request paths. 2. Use the `make_server` function from `wsgiref.simple_server` to serve the application on port 8000. 3. Implement the logic for handling the `/hello` path, so that it returns a JSON response with a \'message\' key. 4. Implement the logic for handling the `/headers` path, so that it returns a plain text response listing all HTTP request headers. 5. Return a 404 Not Found error for any other paths. Expected Input/Output - Input: HTTP request to the server. - Output: Appropriate HTTP response based on the URL path. Example code structure (to be filled in by the student): ```python from wsgiref.simple_server import make_server from wsgiref.util import setup_testing_defaults import json def my_app(environ, start_response): # Set up a default WSGI environment setup_testing_defaults(environ) # Determine the path path = environ.get(\'PATH_INFO\', \'\') # Add application logic here if path == \'/hello\': response_text = json.dumps({\'message\': \'Hello, World!\'}) start_response(\'200 OK\', [(\'Content-Type\', \'application/json\')]) return [response_text.encode(\'utf-8\')] elif path == \'/headers\': response_text = \'n\'.join([f\'{k}: {v}\' for k, v in environ.items()]) start_response(\'200 OK\', [(\'Content-Type\', \'text/plain\')]) return [response_text.encode(\'utf-8\')] else: response_text = \'404 Not Found\' start_response(\'404 Not Found\', [(\'Content-Type\', \'text/plain\')]) return [response_text.encode(\'utf-8\')] if __name__ == \'__main__\': with make_server(\'\', 8000, my_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` Complete the application object `my_app` to satisfy the requirements.","solution":"from wsgiref.simple_server import make_server from wsgiref.util import setup_testing_defaults import json def my_app(environ, start_response): # Set up a default WSGI environment setup_testing_defaults(environ) # Determine the path path = environ.get(\'PATH_INFO\', \'\') # Add application logic here if path == \'/hello\': response_text = json.dumps({\'message\': \'Hello, World!\'}) start_response(\'200 OK\', [(\'Content-Type\', \'application/json\')]) return [response_text.encode(\'utf-8\')] elif path == \'/headers\': response_text = \'n\'.join([f\'{k}: {v}\' for k, v in environ.items()]) start_response(\'200 OK\', [(\'Content-Type\', \'text/plain\')]) return [response_text.encode(\'utf-8\')] else: response_text = \'404 Not Found\' start_response(\'404 Not Found\', [(\'Content-Type\', \'text/plain\')]) return [response_text.encode(\'utf-8\')] if __name__ == \'__main__\': with make_server(\'\', 8000, my_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"Objective Demonstrate your understanding of the `collections.abc` module by creating a custom collection class that behaves like a sequence but with special constraints on the values it holds. Requirements 1. Implement a custom class `IntRangeSequence` that behaves like an immutable sequence and only holds integer values within a specified range. 2. Your class should inherit from `collections.abc.Sequence` and implement the required abstract methods. 3. The class should raise a `ValueError` if any non-integer value or out-of-range value is added to the sequence. Specifications - **Class Name:** `IntRangeSequence` - **Constructor:** ```python def __init__(self, start: int, end: int, *values: int): ``` - `start` and `end` define the valid integer range `[start, end]` (inclusive on both sides). - `values` is a variable-length positional argument representing the items of the sequence. - The constructor should enforce that all `values` are integers within the specified range. - **Methods to Implement:** - `__getitem__(self, index) -> int` - `__len__(self) -> int` - Implement any other necessary methods to support sequence operations. Constraints - Values must be integers within the range specified by `start` and `end`. Example Usage ```python # Creating an instance seq = IntRangeSequence(1, 10, 2, 3, 5, 7) # Expected outputs print(len(seq)) # Output: 4 print(seq[0]) # Output: 2 print(seq[2]) # Output: 5 print(5 in seq) # Output: True print(list(seq)) # Output: [2, 3, 5, 7] # Should raise ValueError seq_invalid = IntRangeSequence(1, 10, 2, 15) # Raises ValueError because 15 is out of range ``` Write the implementation of the class `IntRangeSequence` according to the above specifications.","solution":"from collections.abc import Sequence class IntRangeSequence(Sequence): def __init__(self, start: int, end: int, *values: int): self.start = start self.end = end self.values = [] for value in values: if not isinstance(value, int): raise ValueError(f\\"Value {value} is not an integer.\\") if not (start <= value <= end): raise ValueError(f\\"Value {value} is out of the specified range [{start}, {end}].\\") self.values.append(value) def __getitem__(self, index): return self.values[index] def __len__(self): return len(self.values) def __contains__(self, item): return item in self.values def __iter__(self): return iter(self.values) def __repr__(self): return f\\"IntRangeSequence({self.start}, {self.end}, {\', \'.join(map(str, self.values))})\\""},{"question":"# XML Data Manipulation with `xml.etree.ElementTree` **Objective:** You are tasked with writing a Python function to process XML data using the `xml.etree.ElementTree` module. The function should locate specific elements, modify their attributes and text, and output the modified XML. **Function Signature:** ```python def process_xml(xml_string: str, tag_to_find: str, new_text: str, new_attribute: dict) -> str: Process XML data to find, modify specified elements, and return the modified XML string. :param xml_string: A string representation of the XML to be processed. :param tag_to_find: The tag name of elements to find and modify. :param new_text: The new text to be set for the found elements. :param new_attribute: A dictionary of new attributes to set for the found elements. :return: A string representation of the modified XML. ``` **Input/Output:** - **Input:** - `xml_string` (str): A string representation of the XML content. - `tag_to_find` (str): The name of the tag whose elements need to be found and modified. - `new_text` (str): The new text content to be set in each found element. - `new_attribute` (dict): A dictionary containing new attribute-value pairs to be set in each found element. - **Output:** - A string representation of the modified XML content. **Constraints:** - The XML is guaranteed to be well-formed. - The `tag_to_find` will always be present in the XML string. - The `new_attribute` dictionary will contain a non-zero number of attribute pairs. **Example:** ```python # Given this XML string: xml_input = \'\'\' <root> <item id=\\"1\\">First</item> <item id=\\"2\\">Second</item> <item id=\\"3\\">Third</item> </root> \'\'\' # Process the XML to change the text of <item> elements and add a new attribute \'class\' modified_xml = process_xml(xml_input, \'item\', \'Modified Text\', {\'class\': \'updated\'}) # Expected output: \'\'\' <root> <item id=\\"1\\" class=\\"updated\\">Modified Text</item> <item id=\\"2\\" class=\\"updated\\">Modified Text</item> <item id=\\"3\\" class=\\"updated\\">Modified Text</item> </root> \'\'\' ``` **Hints:** - Use the `xml.etree.ElementTree` module for parsing and manipulating the XML. - The `findall` method may be useful for locating all elements with a specified tag. - `set` method can be used to update or add new attributes to an element. - Remember to convert the modified ElementTree back to a string format using `ElementTree.tostring` for the output.","solution":"import xml.etree.ElementTree as ET def process_xml(xml_string: str, tag_to_find: str, new_text: str, new_attribute: dict) -> str: Process XML data to find, modify specified elements, and return the modified XML string. :param xml_string: A string representation of the XML to be processed. :param tag_to_find: The tag name of elements to find and modify. :param new_text: The new text to be set for the found elements. :param new_attribute: A dictionary of new attributes to set for the found elements. :return: A string representation of the modified XML. root = ET.fromstring(xml_string) for elem in root.findall(tag_to_find): elem.text = new_text for key, value in new_attribute.items(): elem.set(key, value) return ET.tostring(root, encoding=\'unicode\', method=\'xml\')"},{"question":"**Complex Data Management with Data Classes** You need to efficiently manage a collection of information about books in a library. Each book has various attributes, such as title, author, ISBN, number of copies available, and keywords. Implement a class `LibraryBook` using `dataclasses` module with the following requirements: 1. **Attributes:** - `title` (str): The title of the book. - `author` (str): The author of the book. - `isbn` (str): The ISBN number of the book. - `available_copies` (int): The number of copies currently available in the library. - `keywords` (list of str): Keywords associated with the book. 2. **Methods:** - `__post_init__`: Ensure that `available_copies` is non-negative and `isbn` is a valid format. - `is_available`: Returns a boolean indicating if the book is available for borrowing (i.e., `available_copies` > 0). - `borrow_book`: Decrements the `available_copies` by 1 if the book is available; otherwise, raise an exception `NotAvailableError`. - `return_book`: Increments the `available_copies` by 1. 3. **Custom Exception:** - `NotAvailableError`: Raised when attempting to borrow a book that has no available copies. 4. **Additional Requirements:** - Provide a function `add_book_to_library(library, book)` to add a new book to the library. - Provide a function `search_books_by_keyword(library, keyword)` that returns a list of books with the given keyword. **Function Specifications:** ```python from dataclasses import dataclass, field from typing import List class NotAvailableError(Exception): pass @dataclass class LibraryBook: title: str author: str isbn: str available_copies: int keywords: List[str] = field(default_factory=list) def __post_init__(self): # Ensure available_copies is non-negative # Ensure ISBN is of correct format (e.g., 13 digits or 10 digits) pass def is_available(self) -> bool: # Return True if available copies > 0 pass def borrow_book(self): # Decrement available_copies by 1 if the book is available # Otherwise, raise NotAvailableError pass def return_book(self): # Increment available_copies by 1 pass def add_book_to_library(library: List[LibraryBook], book: LibraryBook): # Add book to library list pass def search_books_by_keyword(library: List[LibraryBook], keyword: str) -> List[LibraryBook]: # Return list of books containing the keyword in their keywords list pass ``` **Input and Output Example:** ```python # Creating books book1 = LibraryBook(title=\\"Python Basics\\", author=\\"John Doe\\", isbn=\\"1234567890123\\", available_copies=10, keywords=[\\"python\\", \\"programming\\"]) book2 = LibraryBook(title=\\"Advanced Python\\", author=\\"Jane Smith\\", isbn=\\"9876543210123\\", available_copies=5, keywords=[\\"python\\", \\"advanced\\"]) # Adding books to library library = [] add_book_to_library(library, book1) add_book_to_library(library, book2) # Searching books by keyword books = search_books_by_keyword(library, \\"python\\") print([book.title for book in books]) # Output: [\'Python Basics\', \'Advanced Python\'] ``` Your implementation should accurately follow the requirements, ensuring that the `LibraryBook` class and associated functions work correctly and efficiently.","solution":"from dataclasses import dataclass, field from typing import List import re class NotAvailableError(Exception): pass @dataclass class LibraryBook: title: str author: str isbn: str available_copies: int keywords: List[str] = field(default_factory=list) def __post_init__(self): if self.available_copies < 0: raise ValueError(\\"Number of available copies cannot be negative\\") if not re.match(r\'^d{10}(d{3})?\', self.isbn): raise ValueError(\\"ISBN must be a 10 or 13 digit number\\") def is_available(self) -> bool: return self.available_copies > 0 def borrow_book(self): if self.available_copies <= 0: raise NotAvailableError(\\"No copies available to borrow\\") self.available_copies -= 1 def return_book(self): self.available_copies += 1 def add_book_to_library(library: List[LibraryBook], book: LibraryBook): library.append(book) def search_books_by_keyword(library: List[LibraryBook], keyword: str) -> List[LibraryBook]: return [book for book in library if keyword in book.keywords]"},{"question":"You are tasked with creating a secure messaging system between two parties using the HMAC (Hash-based Message Authentication Code) mechanism provided by Python\'s `hmac` module. The goal is to ensure that messages sent between the parties are authenticated, i.e., it can be verified that the message was sent by a party possessing the shared secret key and that its contents were not tampered with. Write a function `secure_message_exchange` that takes in a secret key and a series of messages, and outputs a list of tuples each containing a message and its corresponding HMAC digest in hexadecimal format. Additionally, implement a function `verify_message` that takes the same secret key, a message, and its HMAC digest in hexadecimal format and returns `True` if the digest is valid for the given message and key, otherwise returns `False`. Function Signatures ```python def secure_message_exchange(key: bytes, messages: list[str], digestmod: str) -> list[tuple[str, str]]: pass def verify_message(key: bytes, message: str, hex_digest: str, digestmod: str) -> bool: pass ``` Input 1. `key`: A bytes object that represents the shared secret key. 2. `messages`: A list of strings where each string is a message that needs to be authenticated. 3. `digestmod`: A string denoting the hash algorithm to use (e.g., \'sha256\'). For the `verify_message` function: 1. `key`: A bytes object that represents the shared secret key. 2. `message`: A string that represents the message to be verified. 3. `hex_digest`: A string containing the hexadecimal format of the HMAC digest. 4. `digestmod`: A string denoting the hash algorithm to use (e.g., \'sha256\'). Output - `secure_message_exchange` returns a list of tuples, each tuple containing the original message and its corresponding HMAC digest in hexadecimal format `(message, hex_digest)`. - `verify_message` returns a boolean value (`True` or `False`). Example ```python key = b\'secret_key\' messages = [\\"Hello, World!\\", \\"Python is awesome.\\", \\"Secure this message.\\"] digestmod = \'sha256\' # Generate authenticated messages auth_messages = secure_message_exchange(key, messages, digestmod) print(auth_messages) # Expected Output: [(\'Hello, World!\', \'...\', ...), ...] # HMAC digests will vary # Verify the first message from the output message, hex_digest = auth_messages[0] is_valid = verify_message(key, message, hex_digest, digestmod) print(is_valid) # Expected Output: True ``` Constraints 1. The `key` should not exceed 64 bytes. 2. The `messages` list should contain at most 1000 messages, each message not exceeding 1024 characters. 3. The hash algorithm specified in `digestmod` should be supported by the `hashlib` module. # Note - Use the `compare_digest` function for secure comparison of digests in the `verify_message` function. - Consider performance implications when concatenating multiple messages for the `update` function in `secure_message_exchange`.","solution":"import hmac import hashlib def secure_message_exchange(key: bytes, messages: list[str], digestmod: str) -> list[tuple[str, str]]: Computes HMAC digests for a series of messages using the specified key and hash algorithm. Args: key (bytes): Shared secret key. messages (list of str): List of messages to authenticate. digestmod (str): Hash algorithm to use (e.g., \'sha256\'). Returns: list of tuples: Each tuple contains the message and its HMAC digest in hexadecimal format. results = [] for message in messages: hmac_obj = hmac.new(key, message.encode(), digestmod) hex_digest = hmac_obj.hexdigest() results.append((message, hex_digest)) return results def verify_message(key: bytes, message: str, hex_digest: str, digestmod: str) -> bool: Verifies the HMAC digest of a message using the specified key and hash algorithm. Args: key (bytes): Shared secret key. message (str): The message to verify. hex_digest (str): HMAC digest in hexadecimal format. digestmod (str): Hash algorithm to use (e.g., \'sha256\'). Returns: bool: True if the digest is valid for the given message and key, otherwise False. hmac_obj = hmac.new(key, message.encode(), digestmod) expected_digest = hmac_obj.hexdigest() return hmac.compare_digest(expected_digest, hex_digest)"},{"question":"# Question You are given a dataset and instructions to visualize specific relationships between its features using Seaborn. You need to create a complex plot that incorporates multiple types of annotations, color mappings, and alignments. Task 1. Load the `glue` dataset directly using Seaborn\'s `load_dataset` function. 2. Transform the dataset: - Pivot the dataset so that \'Model\' and \'Encoder\' become row indices, \'Task\' becomes column names, and \'Score\' is the values. - Add a new column, \'Average\', that contains the average score for each model, rounded to one decimal place. - Sort the dataset by the \'Average\' column in descending order. 3. Create a scatter plot of the `SST-2` scores vs. `MRPC` scores, where: - Each point represents a model. - Points are colored by the `Encoder` type. - Model names are displayed as text annotations above each point. 4. Create a bar plot displaying the `Average` scores for each model, where: - Bars are horizontally aligned. - Each bar is annotated with its average score. - Use additional Matplotlib parameters to bold the text annotations. Both plots should be clearly labeled and aesthetically pleasing. Ensure `Encoder` types have unique colors and implement appropriate text alignment for clarity. Input - No input is required from the user. Output - Two visualizations as described above. Constraints - Use Seaborn and Matplotlib libraries. - Ensure that all plots are properly labeled and legible. - Handle the dataset transformation within the provided instructions. Below is an example outline to guide your implementation: ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load and transform the dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Create first plot (Scatter plot with text annotations) scatter_plot = ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\")) ) plt.show() # Create second plot (Bar plot with annotations) bar_plot = ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) plt.show() ``` **Note:** Ensure to import all necessary libraries and follow guidelines on text alignment and aesthetic quality.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_and_transform_dataset(): # Load the \'glue\' dataset glue = sns.load_dataset(\\"glue\\") # Pivot the dataset pivoted_glue = glue.pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") # Add a new column \'Average\' with the average score for each model, rounded to one decimal place pivoted_glue[\\"Average\\"] = pivoted_glue.mean(axis=1).round(1) # Sort the dataset by \'Average\' in descending order sorted_glue = pivoted_glue.sort_values(\\"Average\\", ascending=False) return sorted_glue def create_scatter_plot(glue_data): # Create a scatter plot of the \'SST-2\' scores vs \'MRPC\' scores plt.figure(figsize=(12, 8)) scatter_plot = sns.scatterplot(data=glue_data, x=\'SST-2\', y=\'MRPC\', hue=\'Encoder\', palette=\\"deep\\") for line in range(0, glue_data.shape[0]): plt.text(glue_data[\'SST-2\'][line], glue_data[\'MRPC\'][line], glue_data.index[line][0], horizontalalignment=\'center\', size=\'medium\', color=\'black\', weight=\'semibold\') plt.title(\'SST-2 vs MRPC Scores by Model\') plt.xlabel(\'SST-2 Score\') plt.ylabel(\'MRPC Score\') plt.legend(title=\'Encoder\') plt.show() def create_bar_plot(glue_data): # Create a bar plot displaying the \'Average\' scores for each model plt.figure(figsize=(12, 8)) bar_plot = sns.barplot(x=glue_data[\'Average\'], y=glue_data.index.get_level_values(\'Model\'), palette=\\"deep\\") for index, value in enumerate(glue_data[\'Average\']): bar_plot.text(value, index, f\'{value}\', color=\'black\', ha=\\"left\\", va=\\"center\\", weight=\'bold\') plt.title(\'Average Scores by Model\') plt.xlabel(\'Average Score\') plt.ylabel(\'Model\') plt.show() # Load and transform the dataset glue_data = load_and_transform_dataset() # Create the scatter plot create_scatter_plot(glue_data) # Create the bar plot create_bar_plot(glue_data)"},{"question":"# Advanced Python Programming Task Problem Statement You are tasked with developing a multi-threaded Python application that simulates a simple logging system. Your objective is to implement a class `Logger` and demonstrate its usage in a multi-threaded context. The `Logger` should be capable of writing log messages to a file with thread-safe operation and support different logging levels. Requirements 1. **Logger Class**: - Implement a `Logger` class that writes log messages to a file. - The `Logger` class should have methods: `debug`, `info`, `warning`, `error`, and `critical`, which correspond to different logging levels. - Each log message should include a timestamp, the logging level, and the message. - Ensure thread-safety so that multiple threads can write to the log file concurrently without data corruption. 2. **Multi-threading**: - Create a multi-threaded application where multiple threads generate log messages using the `Logger` class. - Use the `threading` module to manage threads. - Demonstrate the logging functionality by spawning multiple threads, each writing several log messages. Input and Output 1. **Logger Class**: - Constructor: `Logger(logfile: str)` - Methods: ```python def debug(self, msg: str) -> None: ... def info(self, msg: str) -> None: ... def warning(self, msg: str) -> None: ... def error(self, msg: str) -> None: ... def critical(self, msg: str) -> None: ... ``` 2. **Multi-threaded Application**: - No explicit input; your implementation will demonstrate functionality by generating log messages. 3. **Output**: - A log file with timestamps, logging levels, and messages from multiple threads. Constraints and Limitations - Your `Logger` implementation must ensure that file writing operations are thread-safe. - Use the `logging` module to assist with logging functionality; however, you are required to implement the class methods and thread-safety manually. - Handle any potential exceptions gracefully. Example ```python import threading from datetime import datetime class Logger: # your implementation here # Logger instance shared by multiple threads logger = Logger(\'app.log\') def log_messages(thread_id: int): for i in range(10): logger.info(f\\"Thread-{thread_id} - Message {i}\\") # Create and start threads threads = [] for i in range(5): thread = threading.Thread(target=log_messages, args=(i,)) threads.append(thread) thread.start() # Wait for all threads to finish for thread in threads: thread.join() print(\\"Logging complete. Check the \'app.log\' file for output.\\") ``` In the example above: - `Logger` is instantiated to log to `app.log`. - Five threads are created, each thread logging ten `info` messages. The expected output format in the log file (`app.log`) is: ``` 2023-10-20 12:00:00,000 - INFO - Thread-0 - Message 0 2023-10-20 12:00:00,001 - INFO - Thread-0 - Message 1 ... 2023-10-20 12:00:00,050 - INFO - Thread-4 - Message 9 ``` Your task is to provide the full implementation of the `Logger` class and demonstrate its usage in a multi-threaded context as shown in the example.","solution":"import threading import logging from datetime import datetime class Logger: def __init__(self, logfile: str): self.lock = threading.Lock() self.logger = logging.getLogger(\'ThreadSafeLogger\') self.logger.setLevel(logging.DEBUG) handler = logging.FileHandler(logfile) formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') handler.setFormatter(formatter) self.logger.addHandler(handler) def _log(self, level, msg: str): with self.lock: if level == \'DEBUG\': self.logger.debug(msg) elif level == \'INFO\': self.logger.info(msg) elif level == \'WARNING\': self.logger.warning(msg) elif level == \'ERROR\': self.logger.error(msg) elif level == \'CRITICAL\': self.logger.critical(msg) def debug(self, msg: str): self._log(\'DEBUG\', msg) def info(self, msg: str): self._log(\'INFO\', msg) def warning(self, msg: str): self._log(\'WARNING\', msg) def error(self, msg: str): self._log(\'ERROR\', msg) def critical(self, msg: str): self._log(\'CRITICAL\', msg) # Logger instance shared by multiple threads logger = Logger(\'app.log\') def log_messages(thread_id: int): for i in range(10): logger.info(f\\"Thread-{thread_id} - Message {i}\\") # Create and start threads threads = [] for i in range(5): thread = threading.Thread(target=log_messages, args=(i,)) threads.append(thread) thread.start() # Wait for all threads to finish for thread in threads: thread.join() print(\\"Logging complete. Check the \'app.log\' file for output.\\")"},{"question":"**Question: Evaluating Classification Model Performance using Custom Scoring Functions** **Objective:** Implement a custom scoring function using scikit-learn\'s `make_scorer` and evaluate a classifier\'s performance on a given dataset. The custom scoring function should combine multiple existing metrics to provide a more comprehensive evaluation. **Description:** 1. Load the Iris dataset from scikit-learn. 2. Split the dataset into training and test sets. 3. Implement a custom scoring function that combines Precision, Recall, and F1-score into a single evaluation metric. The custom scoring function should return the average of Precision, Recall, and F1-score. 4. Train a `RandomForestClassifier` on the training set. 5. Evaluate the classifier using the custom scoring function. **Task Requirements:** 1. Implement a function `custom_scoring(y_true, y_pred)` that calculates Precision, Recall, and F1-score using `sklearn.metrics.precision_score`, `sklearn.metrics.recall_score`, and `sklearn.metrics.f1_score`. Return the average of these three scores. 2. Use `make_scorer` from `sklearn.metrics` to create a scorer object from `custom_scoring`. 3. Train a `RandomForestClassifier` on the training set. 4. Evaluate the trained classifier using cross-validation with the custom scorer. **Input:** - The custom scoring function `custom_scoring(y_true, y_pred)` should take two inputs: - `y_true`: Ground truth (true labels). - `y_pred`: Predicted labels. **Output:** - A float representing the average of Precision, Recall, and F1-score for the classifier on the test set. **Constraints:** - Use `train_test_split` with `test_size=0.3` and `random_state=42` for splitting the dataset. - Use the RandomForestClassifier with default parameters. **Example Usage:** ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, cross_val_score from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score # Load the Iris dataset data = load_iris() X, y = data.data, data.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define the custom scoring function def custom_scoring(y_true, y_pred): precision = precision_score(y_true, y_pred, average=\'micro\') recall = recall_score(y_true, y_pred, average=\'micro\') f1 = f1_score(y_true, y_pred, average=\'micro\') return (precision + recall + f1) / 3 # Create the scorer using make_scorer scorer = make_scorer(custom_scoring) # Train the RandomForestClassifier clf = RandomForestClassifier() clf.fit(X_train, y_train) # Evaluate the classifier using cross-validation with the custom scorer scores = cross_val_score(clf, X_test, y_test, cv=5, scoring=scorer) print(\'Average custom score:\', scores.mean()) ``` Good luck!","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, cross_val_score from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score # Load the Iris dataset data = load_iris() X, y = data.data, data.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define the custom scoring function def custom_scoring(y_true, y_pred): precision = precision_score(y_true, y_pred, average=\'micro\') recall = recall_score(y_true, y_pred, average=\'micro\') f1 = f1_score(y_true, y_pred, average=\'micro\') return (precision + recall + f1) / 3 # Create the scorer using make_scorer scorer = make_scorer(custom_scoring) # Train the RandomForestClassifier clf = RandomForestClassifier() clf.fit(X_train, y_train) # Evaluate the classifier using cross-validation with the custom scorer scores = cross_val_score(clf, X_test, y_test, cv=5, scoring=scorer) average_custom_score = scores.mean() print(\'Average custom score:\', average_custom_score) # Function to be used in the unit tests def evaluate_model(): return average_custom_score"},{"question":"# Custom Rotating Log Handler You are required to implement a custom logging handler in Python using the `logging.handlers` module. This custom handler will rotate log files based on both size and time. Specifically, you need to create a handler that rotates the log file when either: - The log file reaches a certain size, or - A certain time interval passes. Your `CustomRotatingHandler` class should inherit from the `BaseRotatingHandler` and combine the functionalities of both `RotatingFileHandler` and `TimedRotatingFileHandler`. # Requirements: 1. The handler should be initialized with parameters for the maximum file size (`maxBytes`), rotation interval (`interval`), and the type of interval (`when`). Additionally, it should take parameters for `filename`, `backupCount`, `encoding`, `delay`, and `errors`. 2. The handler should rotate the log file either when the file size exceeds `maxBytes` or when the interval defined by `when` and `interval` passes. 3. When rotating, if `backupCount` is non-zero, the system should save old log files by appending extensions \'.1\', \'.2\', etc., to the filename. 4. The handler should support file encoding and delay file opening until the first log entry is emitted. # Class Specification: ```python import logging from logging.handlers import BaseRotatingHandler import os import time from datetime import datetime, timedelta class CustomRotatingHandler(BaseRotatingHandler): def __init__(self, filename, mode=\'a\', maxBytes=0, backupCount=0, encoding=None, delay=False, when=\'h\', interval=1, errors=None): # Your implementation here pass def shouldRollover(self, record): # Your implementation here pass def doRollover(self): # Your implementation here pass def emit(self, record): # Your implementation here pass ``` # Constraints: - `filename` should be a string representing the log file name. - `mode` should be the file open mode and defaults to \'a\' (append). - `maxBytes` should be a non-negative integer. If zero, rollover never occurs due to file size. - `backupCount` should be a non-negative integer. If zero, no backup files will be created. - `when` should be a string representing the type of interval. Possible values are \'S\' (seconds), \'M\' (minutes), \'H\' (hours), \'D\' (days), \'W0\'-\'W6\' (day of the week), \'midnight\'. - `interval` should be a positive integer representing the number of `when` units. - The handler should handle various encoding errors as specified by the `errors` parameter. # Example Usage: ```python import logging logger = logging.getLogger(\'my_logger\') logger.setLevel(logging.DEBUG) handler = CustomRotatingHandler( filename=\'app.log\', maxBytes=1024*1024, # 1 MB backupCount=5, when=\'D\', interval=1, encoding=\'utf-8\' ) formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') handler.setFormatter(formatter) logger.addHandler(handler) # Log some messages for i in range(10000): logger.debug(f\'Message {i}\') ``` Implement the `CustomRotatingHandler` class as described.","solution":"import logging from logging.handlers import BaseRotatingHandler import os import time from datetime import datetime, timedelta class CustomRotatingHandler(BaseRotatingHandler): def __init__(self, filename, mode=\'a\', maxBytes=0, backupCount=0, encoding=None, delay=False, when=\'h\', interval=1, errors=None): self.filename = filename self.mode = mode self.maxBytes = maxBytes self.backupCount = backupCount self.encoding = encoding self.delay = delay self.when = when.upper() self.interval = interval self.errors = errors # Determine the rollover time interval if self.when == \'S\': self.interval_timedelta = timedelta(seconds=self.interval) elif self.when == \'M\': self.interval_timedelta = timedelta(minutes=self.interval) elif self.when == \'H\': self.interval_timedelta = timedelta(hours=self.interval) elif self.when == \'D\': self.interval_timedelta = timedelta(days=self.interval) elif self.when.startswith(\'W\'): self.interval_timedelta = timedelta(weeks=1) self.day_of_week = int(self.when[1]) elif self.when == \'MIDNIGHT\': self.interval_timedelta = timedelta(days=1) else: raise ValueError(\\"Invalid rollover interval specified\\") self.rollover_at = self.compute_initial_rollover() super().__init__(filename, mode, encoding, delay) def compute_initial_rollover(self): current_time = time.time() if self.when == \'MIDNIGHT\': t = datetime.now() self.rollover_time = t.replace(hour=0, minute=0, second=0, microsecond=0) + self.interval_timedelta elif self.when.startswith(\'W\'): t = datetime.now() self.rollover_time = t + self.interval_timedelta weekday = self.rollover_time.weekday() while weekday != self.day_of_week: self.rollover_time += timedelta(days=1) weekday = self.rollover_time.weekday() else: self.rollover_time = datetime.now() + self.interval_timedelta return self.rollover_time.timestamp() def shouldRollover(self, record): if self.stream is None: self.stream = self._open() if self.maxBytes > 0: self.stream.seek(0, 2) # go to the end if self.stream.tell() >= self.maxBytes: return 1 if time.time() >= self.rollover_at: return 1 return 0 def doRollover(self): self.stream.close() if self.backupCount > 0: for i in range(self.backupCount - 1, 0, -1): sfn = f\\"{self.filename}.{i}\\" dfn = f\\"{self.filename}.{i + 1}\\" if os.path.exists(sfn): if os.path.exists(dfn): os.remove(dfn) os.rename(sfn, dfn) dfn = self.filename + \\".1\\" if os.path.exists(dfn): os.remove(dfn) os.rename(self.baseFilename, dfn) self.stream = self._open() current_time = time.time() self.rollover_at = self.compute_initial_rollover() def emit(self, record): try: if self.shouldRollover(record): self.doRollover() logging.FileHandler.emit(self, record) except Exception: self.handleError(record)"},{"question":"Problem Statement You are given a list of tuples where each tuple contains two elements: a string (representing a person\'s name) and an integer (representing the number of hours they\'ve worked in a week). Your task is to perform several data manipulations and provide aggregated data in the form of a dictionary. The steps are as follows: 1. **Input List**: ```python work_hours = [(\\"Alice\\", 34), (\\"Bob\\", 40), (\\"Alice\\", 29), (\\"Bob\\", 36), (\\"Charlie\\", 41)] ``` 2. **Task 1**: Merge the work hours for each person. 3. **Task 2**: For each person, determine if they are eligible for overtime. In this scenario, eligibility is defined as having worked more than 35 hours in a week on average. 4. **Task 3**: Create a dictionary where each key is a person\'s name, and the value is a tuple containing the total hours worked and their overtime eligibility status. Expected Output A dictionary where: - Each key is a person\'s name. - Values are tuples of the form `(total_hours_worked, is_overtime_eligible)`. Constraints - The list can have repetitive entries for individuals. - Names are unique strings and can include alphabets only. - Hours are non-negative integers. Function Signature ```python def aggregate_work_hours(work_hours: list) -> dict: pass ``` Example ```python work_hours = [(\\"Alice\\", 34), (\\"Bob\\", 40), (\\"Alice\\", 29), (\\"Bob\\", 36), (\\"Charlie\\", 41)] result = aggregate_work_hours(work_hours) print(result) # Output: {\'Alice\': (63, False), \'Bob\': (76, True), \'Charlie\': (41, True)} ``` Implementation Guide 1. **Initialization**: - Create a dictionary to accumulate total hours for each person. 2. **Aggregation**: - Iterate over the list of tuples and sum the hours for each person. 3. **Average Calculation and Eligibility**: - Calculate the average weekly hours for each person and determine if they meet the overtime criteria. 4. **Dictionary Construction**: - Populate the resulting dictionary as required.","solution":"def aggregate_work_hours(work_hours): Aggregates work hours per person and determines their overtime eligibility. Args: work_hours (list): A list of tuples where each tuple contains a string (person\'s name) and an integer (hours worked). Returns: dict: A dictionary where each key is a person\'s name and the value is a tuple containing total hours worked and overtime eligibility status (True/False). from collections import defaultdict total_hours = defaultdict(int) # Aggregate hours for each person for name, hours in work_hours: total_hours[name] += hours # Create result dictionary result = {} for name, hours in total_hours.items(): is_overtime_eligible = (hours / len([x for x in work_hours if x[0] == name])) > 35 result[name] = (hours, is_overtime_eligible) return result"},{"question":"Advanced Array Manipulation **Objective:** Assess your understanding of the `array` module in Python for creating and manipulating arrays with various numeric types, as well as your ability to use array methods to perform complex tasks. **Problem Statement:** Write a Python function `manipulate_arrays` which takes two arguments: 1. `typecode`: A string representing the type code of the array (e.g., \'i\' for signed int, \'f\' for float). 2. `operations`: A list of tuples, where each tuple represents an operation to be performed on the array. The operation can be one of the following: - `(\\"append\\", x)`: Append the element `x` to the array. - `(\\"extend\\", iterable)`: Extend the array with elements from the `iterable`. - `(\\"insert\\", index, x)`: Insert the element `x` at position `index`. - `(\\"pop\\", [index])`: Remove and return the element at `index`. If `index` is not provided, pop the last element. - `(\\"remove\\", x)`: Remove the first occurrence of `x` from the array. - `(\\"reverse\\", )`: Reverse the array. - `(\\"tobytes\\", )`: Convert the array to bytes and return the byte representation. - `(\\"tolist\\", )`: Convert the array to a list and return the list. The function should return the final state of the array after all operations have been applied. If the operation is `tobytes` or `tolist`, only return the array in that specific representation without further modifications. **Constraints:** - The type code provided must be one of the valid type codes described in the documentation. - The element types in the operations must be compatible with the type code of the array. - Any invalid operation type should raise a `ValueError`. **Function Signature:** ```python def manipulate_arrays(typecode: str, operations: list) -> [array.array, bytes, list]: pass ``` **Examples:** ```python # Example 1 result = manipulate_arrays(\'i\', [(\\"append\\", 1), (\\"append\\", 2), (\\"extend\\", [3, 4]), (\\"pop\\", ), (\\"reverse\\", ), (\\"tolist\\", )]) # Expected output: [3, 2, 1] # Example 2 result = manipulate_arrays(\'f\', [(\\"append\\", 1.1), (\\"append\\", 2.2), (\\"tobytes\\", )]) # Expected output: b\'x9ax99x99x99x99x99xf1?x9ax99x99x99x99x99x01@\' ``` Explanation: 1. The `manipulate_arrays` function initializes the array with the given `typecode`. 2. Iterates over the `operations`, applying each operation in sequence. 3. For \\"tobytes\\" and \\"tolist\\" operations, the function converts the array and stops further modifications. 4. The function handles each operation appropriately, performing specified actions, and finally returns the modified array\'s final state or its byte/list representation as required. **Notes:** - Ensure the array type and operations conform to the limits and constraints of array type code functionality. - You may use `try/except` blocks to capture and handle invalid operations or types.","solution":"import array def manipulate_arrays(typecode: str, operations: list): arr = array.array(typecode) for operation in operations: op = operation[0] if op == \\"append\\": arr.append(operation[1]) elif op == \\"extend\\": arr.extend(operation[1]) elif op == \\"insert\\": arr.insert(operation[1], operation[2]) elif op == \\"pop\\": if len(operation) == 1: arr.pop() else: arr.pop(operation[1]) elif op == \\"remove\\": arr.remove(operation[1]) elif op == \\"reverse\\": arr.reverse() elif op == \\"tobytes\\": return arr.tobytes() elif op == \\"tolist\\": return arr.tolist() else: raise ValueError(f\\"Invalid operation: {op}\\") return arr"},{"question":"You are tasked with creating a command-line interface (CLI) tool for managing tasks using Python\'s `argparse` module. Requirements 1. **Command Structure:** Your CLI tool should support the following structure: ``` taskcli.py <command> [options] [arguments] ``` 2. **Commands and Options:** Implement the following subcommands with the specified options: - **add:** Add a new task. - `--title/-t`: The title of the task (required). - `--description/-d`: A brief description of the task (optional). - `--priority/-p`: Priority level of the task (default: 3). - **list:** List all tasks. - `--priority/-p`: Filter tasks by priority level (optional). - `--completed/-c`: Show only completed tasks (optional). - **complete:** Mark a task as completed. - `--task-id`: ID of the task to be marked as completed (required). 3. **Storage:** - Mock the task storage using an in-memory list of dictionaries in the format: ```python tasks = [ {\\"id\\": 1, \\"title\\": \\"Task 1\\", \\"description\\": \\"Description 1\\", \\"priority\\": 1, \\"completed\\": False}, {\\"id\\": 2, \\"title\\": \\"Task 2\\", \\"description\\": \\"Description 2\\", \\"priority\\": 2, \\"completed\\": False} ] ``` 4. **Function Definitions:** - Your script should define at least the following functions: - `add_task(args)`: Add a task based on provided arguments. - `list_tasks(args)`: List tasks with optional filtering. - `complete_task(args)`: Mark a task as completed based on its ID. 5. **Handling Commands:** - Ensure that the correct function is called based on the given subcommand. - Handle errors gracefully (e.g., missing required arguments) and display appropriate error messages. Constraints - Use the `argparse` module for parsing the command-line arguments. - Ensure the script is self-contained and doesn\'t rely on external storage or packages. Example Usage ```sh # Add a new task python taskcli.py add --title \\"New Task\\" --description \\"Some task description\\" --priority 2 # List all tasks python taskcli.py list # List tasks filtered by priority python taskcli.py list --priority 2 # Mark a task as completed python taskcli.py complete --task-id 1 ``` ```python import argparse tasks = [ {\\"id\\": 1, \\"title\\": \\"Task 1\\", \\"description\\": \\"Description 1\\", \\"priority\\": 1, \\"completed\\": False}, {\\"id\\": 2, \\"title\\": \\"Task 2\\", \\"description\\": \\"Description 2\\", \\"priority\\": 2, \\"completed\\": False} ] def add_task(args): # Implement adding a task pass def list_tasks(args): # Implement listing tasks pass def complete_task(args): # Implement marking a task as completed pass def main(): parser = argparse.ArgumentParser(description=\\"Task management CLI tool\\") subparsers = parser.add_subparsers(dest=\\"command\\", required=True) # Add \\"add\\" command parser_add = subparsers.add_parser(\'add\', help=\'Add a new task\') parser_add.add_argument(\'-t\', \'--title\', required=True, help=\'Title of the task\') parser_add.add_argument(\'-d\', \'--description\', help=\'Description of the task\') parser_add.add_argument(\'-p\', \'--priority\', type=int, default=3, help=\'Priority of the task\') parser_add.set_defaults(func=add_task) # Add \\"list\\" command parser_list = subparsers.add_parser(\'list\', help=\'List all tasks\') parser_list.add_argument(\'-p\', \'--priority\', type=int, help=\'Filter by priority\') parser_list.add_argument(\'-c\', \'--completed\', action=\'store_true\', help=\'Show only completed tasks\') parser_list.set_defaults(func=list_tasks) # Add \\"complete\\" command parser_complete = subparsers.add_parser(\'complete\', help=\'Mark a task as completed\') parser_complete.add_argument(\'--task-id\', type=int, required=True, help=\'ID of the task to complete\') parser_complete.set_defaults(func=complete_task) # Parse and call the appropriate function based on the command args = parser.parse_args() args.func(args) if __name__ == \'__main__\': main() ```","solution":"import argparse # Mocking the task storage tasks = [ {\\"id\\": 1, \\"title\\": \\"Task 1\\", \\"description\\": \\"Description 1\\", \\"priority\\": 1, \\"completed\\": False}, {\\"id\\": 2, \\"title\\": \\"Task 2\\", \\"description\\": \\"Description 2\\", \\"priority\\": 2, \\"completed\\": False} ] def add_task(args): Add a new task with the given title, description, and priority. new_task = { \\"id\\": len(tasks) + 1, \\"title\\": args.title, \\"description\\": args.description if args.description else \\"\\", \\"priority\\": args.priority, \\"completed\\": False } tasks.append(new_task) print(f\\"Added task: {new_task}\\") def list_tasks(args): List all tasks, optionally filtering by priority and completed status. filtered_tasks = tasks if args.priority is not None: filtered_tasks = [task for task in filtered_tasks if task[\\"priority\\"] == args.priority] if args.completed: filtered_tasks = [task for task in filtered_tasks if task[\\"completed\\"]] if not filtered_tasks: print(\\"No tasks found.\\") else: for task in filtered_tasks: print(task) def complete_task(args): Mark a task as completed given its ID. for task in tasks: if task[\\"id\\"] == args.task_id: task[\\"completed\\"] = True print(f\\"Task {args.task_id} marked as complete.\\") return print(f\\"Task with id {args.task_id} not found.\\") def main(): parser = argparse.ArgumentParser(description=\\"Task management CLI tool\\") subparsers = parser.add_subparsers(dest=\\"command\\", required=True) # Add \\"add\\" command parser_add = subparsers.add_parser(\'add\', help=\'Add a new task\') parser_add.add_argument(\'-t\', \'--title\', required=True, help=\'Title of the task\') parser_add.add_argument(\'-d\', \'--description\', help=\'Description of the task\') parser_add.add_argument(\'-p\', \'--priority\', type=int, default=3, help=\'Priority of the task\') parser_add.set_defaults(func=add_task) # Add \\"list\\" command parser_list = subparsers.add_parser(\'list\', help=\'List all tasks\') parser_list.add_argument(\'-p\', \'--priority\', type=int, help=\'Filter by priority\') parser_list.add_argument(\'-c\', \'--completed\', action=\'store_true\', help=\'Show only completed tasks\') parser_list.set_defaults(func=list_tasks) # Add \\"complete\\" command parser_complete = subparsers.add_parser(\'complete\', help=\'Mark a task as completed\') parser_complete.add_argument(\'--task-id\', type=int, required=True, help=\'ID of the task to complete\') parser_complete.set_defaults(func=complete_task) # Parse and call the appropriate function based on the command args = parser.parse_args() args.func(args) if __name__ == \'__main__\': main()"},{"question":"# Asynchronous File Processing and Network Communication **Objective:** Implement an asynchronous system that reads data from multiple files concurrently, processes the data, and sends the processed data to a server over a network. **Requirements:** 1. **Reading Files:** - Read data from multiple files concurrently. - Each file contains a list of integers, one integer per line. 2. **Processing Data:** - For each integer read from the files, increment its value by `1`. 3. **Network Communication:** - Create an asynchronous TCP connection to a server. - Send the processed integers to the server. - Ensure that integers sent to the server arrive in the same order they are read from the file. 4. **Synchronization:** - Ensure that no two file reading processes interfere with one another. - Use asyncio synchronization primitives like `Lock` to manage concurrency. # Function Signature ```python import asyncio async def process_and_send(file_paths: list, server_host: str, server_port: int): pass ``` # Input: - `file_paths` (list of str): List containing the file paths to read from. - `server_host` (str): The hostname of the server to connect to. - `server_port` (int): The port number of the server to connect to. # Output: - This function does not return any value. It should send all processed integers to the server. # Constraints: - Each file contains up to `1000` integers. - You are not allowed to use any synchronous I/O operations. - Ensure you handle potential networking errors and file reading errors gracefully. # Example: Assume the following file contents for illustration: - `file1.txt`: ``` 1 2 3 ``` - `file2.txt`: ``` 4 5 6 ``` - Server listens on `localhost` at port `8888`. Example usage of the function: ```python await process_and_send([\'file1.txt\', \'file2.txt\'], \'localhost\', 8888) ``` Expected behavior: - The function connects to the server and sends these integers in this order: ``` 2, 3, 4, 5, 6, 7 ``` # Note: - You may use the provided high-level asyncio API functions such as `run`, `create_task`, `open_connection`, and synchronization primitives. - Include comprehensive exception handling to cover interruptions in reading, processing, or network communication. - Ensure that the server receives integers in the same order as read from the files. # Evaluation Criteria: - Correctness of the implementation (reading, processing, sending). - Proper usage of asyncio high-level APIs and synchronization primitives. - Ability to manage concurrent tasks and synchronization. - Handling of edge cases and potential errors.","solution":"import asyncio from asyncio import Lock async def read_file(file_path, file_lock, queue): async with file_lock: try: async with aiofiles.open(file_path, mode=\'r\') as f: async for line in f: num = int(line.strip()) await queue.put(num + 1) except Exception as e: print(f\\"Error reading file {file_path}: {e}\\") async def send_to_server(host, port, queue): reader, writer = await asyncio.open_connection(host, port) while True: num = await queue.get() if num is None: break writer.write(f\\"{num}n\\".encode()) await writer.drain() writer.close() await writer.wait_closed() async def process_and_send(file_paths, server_host, server_port): queue = asyncio.Queue() file_lock = Lock() file_tasks = [asyncio.create_task(read_file(file_path, file_lock, queue)) for file_path in file_paths] sender_task = asyncio.create_task(send_to_server(server_host, server_port, queue)) await asyncio.gather(*file_tasks) await queue.put(None) await sender_task"},{"question":"# Asynchronous Web Scraper with asyncio Objective Design a Python function using the `asyncio` library to fetch data concurrently from multiple URLs, process the content, and store the results. Problem Statement You need to implement a function `async_web_scraper(urls: List[str]) -> Dict[str, int]` that takes a list of URLs and performs the following operations concurrently using asyncio: 1. Fetch the content of each URL. 2. Compute the number of words in the fetched content. 3. Store the result in a dictionary where the key is the URL, and the value is the word count. Function Signature ```python from typing import List, Dict async def async_web_scraper(urls: List[str]) -> Dict[str, int]: pass ``` Input - `urls` (List[str]): A list of URLs to be fetched. Each URL is a string. Output - Returns a dictionary with URLs as keys and their respective word counts as values. Constraints - The function should handle network errors gracefully and should not stop processing other URLs if one fails. - Maximum number of concurrent requests should be 5. - Assume that the URLs always return a valid HTTP response with HTML content. Example ```python urls = [ \\"http://example.com/page1\\", \\"http://example.com/page2\\", \\"http://example.com/page3\\" ] result = await async_web_scraper(urls) # Example Output: # { # \\"http://example.com/page1\\": 345, # \\"http://example.com/page2\\": 567, # \\"http://example.com/page3\\": 123 # } ``` Performance Requirements - The solution should make efficient use of asyncio to ensure that fetching and processing the URLs is done concurrently. - The function must limit the number of concurrent HTTP requests to 5. Additional Information - You may use `aiohttp` library for making HTTP requests asynchronously. - Below is a sample implementation to get you started with making asynchronous HTTP requests using aiohttp: ```python import aiohttp import asyncio async def fetch(url): async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() ``` Evaluation Criteria - Correct usage of asyncio constructs such as tasks and coroutines. - Efficient handling of concurrent tasks. - Proper error handling. - Adherence to the performance requirements and constraints. Good luck and happy coding!","solution":"from typing import List, Dict import aiohttp import asyncio from collections import defaultdict async def fetch_content(url: str, session: aiohttp.ClientSession) -> str: try: async with session.get(url) as response: return await response.text() except Exception as e: print(f\'Error fetching {url}: {e}\') return \'\' async def process_url(url: str, session: aiohttp.ClientSession) -> Dict[str, int]: content = await fetch_content(url, session) word_count = len(content.split()) if content else 0 return {url: word_count} async def async_web_scraper(urls: List[str]) -> Dict[str, int]: word_count_dict = {} async with aiohttp.ClientSession() as session: tasks = [process_url(url, session) for url in urls] for future in asyncio.as_completed(tasks): result = await future word_count_dict.update(result) return word_count_dict"},{"question":"# Seaborn Advanced Plotting Assessment Background You are provided with a dataset containing information about the tips received by waitstaff in a restaurant. The dataset includes several fields such as the day of the week, the size of the party, the total bill, and the amount of the tip. Objective You need to design and create a visualization using the `seaborn.objects` module to analyze and present the data in a meaningful way. Your task is to: 1. Load the \\"tips\\" dataset using seaborn. 2. Create a bar plot that shows the distribution of tips received across different days of the week. 3. Further, enhance this bar plot: * Display separate bars for males and females. * Use different colors for the bars based on gender. * Stack the bars based on the `smoker` status (i.e., separate within each gender category, show the split between smokers and non-smokers). Requirements 1. The plot should be created using the `seaborn.objects` interface. 2. The x-axis should represent the days of the week. 3. The y-axis should represent the count of tips. 4. Different colors should represent different genders. 5. Bars should be stacked based on the `smoker` status. 6. Provide proper labeling for both axes and a title for the plot. **Input** No input is required as you will be using the \\"tips\\" dataset provided by seaborn. **Output** Generate a bar plot following the specified requirements and display it. Constraints - Ensure your code is well-documented. - You should handle any potential edge cases, e.g., missing data in the dataset. - The plot must be easy to understand and visually convey the required information clearly. Example Solution ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, x=\\"day\\", color=\\"sex\\", group=\\"smoker\\") .add(so.Bar(), so.Count(), so.Stack()) ) plot.label(x=\\"Day of the Week\\", y=\\"Count of Tips\\", title=\\"Distribution of Tips Received across Different Days\\") plot.show() ``` Note You are not restricted to the provided code structure and can use alternative approaches to meet the objective as long as you stick to the requirements listed above.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_tips_barplot(): Creates a barplot showing the distribution of tips received across different days of the week. The plot includes separate bars for males and females and is stacked based on the smoker status. # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, x=\\"day\\", color=\\"sex\\", group=\\"smoker\\") .add(so.Bar(), so.Count(), so.Stack()) ) plot.label(x=\\"Day of the Week\\", y=\\"Count of Tips\\", title=\\"Distribution of Tips Received across Different Days\\") plot.show() # Call the function to create and display the plot create_tips_barplot()"},{"question":"# Question: You are working on a text processing application where you need to extract certain patterned information from large chunks of text data. Given your expertise in regular expressions, you are tasked to implement a function that will help extract specific data. **Task:** Implement a function called `extract_info` that takes a single parameter: `text` (a string containing multiple lines of text). The `extract_info` function should perform the following operations: 1. Extract all email addresses present in the text. An email address is defined as a sequence of characters that match the pattern `[_a-zA-Z0-9]+@[a-zA-Z]+.[a-zA-Z]{2,}`. 2. Extract all dates present in the text in the format `DD-MM-YYYY` or `DD/MM/YYYY`. 3. Extract all words that end with \'ly\'. 4. Return a dictionary with the keys `\'emails\'`, `\'dates\'`, and `\'adverbs\'`, containing lists of the respective extracted items. **Input:** - `text`: A string containing multiple lines with various content (emails, dates, words, etc). **Output:** - A dictionary with three keys: - `\'emails\'`: A list of extracted email addresses. - `\'dates\'`: A list of extracted dates in the format `DD-MM-YYYY` or `DD/MM/YYYY`. - `\'adverbs\'`: A list of extracted words that end with \'ly\'. **Constraints:** - You must use regular expressions to perform the data extraction. - The order of the items in the lists should be based on their occurrences in the input text. **Example:** ```python def extract_info(text): # Your implementation here text = Here are some sample data: First, an email: john.doe@example.com, jane_doe123@domain.co.uk Here are some dates: 01-01-2020, 15/12/1999 And some adverbs quite quickly and silently, largely ignored. result = extract_info(text) print(result) # Expected output: # { # \'emails\': [\'john.doe@example.com\', \'jane_doe123@domain.co.uk\'], # \'dates\': [\'01-01-2020\', \'15/12/1999\'], # \'adverbs\': [\'quickly\', \'silently\', \'largely\'] # } ``` **Notes:** - Ensure your function efficiently handles typical cases as well as edge cases. - Handle overlapping cases where the same substring might match multiple patterns cautiously.","solution":"import re def extract_info(text): Extracts emails, dates, and adverbs ending with \'ly\' from the given text. Args: text (str): The input text containing multiple lines. Returns: dict: A dictionary with keys \'emails\', \'dates\', and \'adverbs\' containing lists of the respective extracted items. email_pattern = re.compile(r\'[_a-zA-Z0-9]+@[a-zA-Z]+.[a-zA-Z]{2,}\') date_pattern = re.compile(r\'bd{2}-d{2}-d{4}b|bd{2}/d{2}/d{4}b\') adverb_pattern = re.compile(r\'bw+lyb\') emails = email_pattern.findall(text) dates = date_pattern.findall(text) adverbs = adverb_pattern.findall(text) return {\'emails\': emails, \'dates\': dates, \'adverbs\': adverbs}"},{"question":"Coding Assessment Question **Objective**: Demonstrate your understanding of CPU stream management and synchronization using PyTorch. # Question Implement a function `parallel_matrix_addition` that performs matrix addition using two separate streams on CPU devices in PyTorch. The goal is to split the matrix addition into two parts and manage these parts using distinct streams to ensure that the addition operations run concurrently. # Function Signature ```python def parallel_matrix_addition(matrix1: torch.Tensor, matrix2: torch.Tensor) -> torch.Tensor: pass ``` # Input - `matrix1`: A PyTorch Tensor of shape `(N, M)`. - `matrix2`: A PyTorch Tensor of shape `(N, M)`. # Output - A PyTorch Tensor of shape `(N, M)` representing the element-wise sum of `matrix1` and `matrix2`. # Constraints - You must use CPU streams to parallelize the matrix addition. - Ensure the operations are synchronized correctly so that the final output tensor is valid. # Example ```python import torch matrix1 = torch.tensor([[1, 2, 3], [4, 5, 6]]) matrix2 = torch.tensor([[6, 5, 4], [3, 2, 1]]) result = parallel_matrix_addition(matrix1, matrix2) print(result) # Output should be tensor([[7, 7, 7], [7, 7, 7]]) ``` # Notes - Consider edge cases such as empty matrices or mismatched dimensions within the constraints of valid inputs. - Use `torch.cpu.Stream` and `torch.cpu.synchronize` to manage the streams and synchronization respectively. - Make sure that `matrix1` and `matrix2` have the same shape, otherwise raise a `ValueError`. # Hints - You may need to split the workload into two parts and compute each part on a separate stream. - Use synchronization to ensure that the computation on both streams is complete before retrieving the result. Good luck, and make sure to test your implementation thoroughly!","solution":"import torch def parallel_matrix_addition(matrix1: torch.Tensor, matrix2: torch.Tensor) -> torch.Tensor: if matrix1.size() != matrix2.size(): raise ValueError(\\"Matrices must have the same dimensions.\\") result = torch.zeros_like(matrix1) num_rows = matrix1.size(0) mid = num_rows // 2 stream1 = torch.cuda.Stream() stream2 = torch.cuda.Stream() with torch.cuda.stream(stream1): result[:mid, :] = matrix1[:mid, :] + matrix2[:mid, :] with torch.cuda.stream(stream2): result[mid:, :] = matrix1[mid:, :] + matrix2[mid:, :] torch.cuda.synchronize() return result"},{"question":"Task: You are tasked with processing a large list of text files to count the occurrences of a specific keyword in each file, and then aggregate the results. To achieve efficient processing and leverage multi-core CPUs, you will use the `concurrent.futures` module with `ThreadPoolExecutor`. Function Signature: ```python def keyword_occurrence_counter(file_list: List[str], keyword: str) -> int: ``` Input: 1. `file_list`: A list of file paths (strings) to be processed. 2. `keyword`: A keyword (string) to be counted within the files. Output: - Returns the total count of keyword occurrences across all files. Constraints: - Assume each file is a text file with UTF-8 encoding. - Handle scenarios where files might be large, up to 1 GB in size. - Ensure that the function utilizes threading for parallel processing. - For simplicity, assume that the keyword search is case-sensitive. Implementation Details: - Implement a helper function that counts keyword occurrences in a single file: ```python def count_keyword_in_file(file_path: str, keyword: str) -> int: # Implement this helper function to return the count of keyword occurrences in the file. ``` - Use `concurrent.futures.ThreadPoolExecutor` to manage parallel execution of the helper function across multiple files. - Collect and aggregate the results appropriately. Example: ```python # Example file list file_list = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] keyword = \\"python\\" # Assuming the occurrences of \\"python\\" in the files are as follows: # file1.txt: 4 occurrences # file2.txt: 7 occurrences # file3.txt: 5 occurrences # The function should return: # 16 result = keyword_occurrence_counter(file_list, keyword) print(result) # Output: 16 ``` Ensure that your solution is efficient, correctly handles parallel processing, and appropriately aggregates the keyword counts.","solution":"from typing import List import concurrent.futures def count_keyword_in_file(file_path: str, keyword: str) -> int: Counts the number of occurrences of the keyword in the file specified by file_path. Parameters: file_path (str): The path to the file where the keyword will be counted. keyword (str): The keyword to count in the file. Returns: int: The number of occurrences of the keyword in the file. count = 0 with open(file_path, \'r\', encoding=\'utf-8\') as file: for line in file: count += line.count(keyword) return count def keyword_occurrence_counter(file_list: List[str], keyword: str) -> int: Counts the total occurrences of the specified keyword across multiple files using multi-threading. Parameters: file_list (List[str]): A list of file paths to be processed. keyword (str): The keyword to be counted within the files. Returns: int: The total count of keyword occurrences across all files. total_count = 0 with concurrent.futures.ThreadPoolExecutor() as executor: futures = [executor.submit(count_keyword_in_file, file_path, keyword) for file_path in file_list] for future in concurrent.futures.as_completed(futures): total_count += future.result() return total_count"},{"question":"Automated FTP Backup Objective: You are required to write a Python function that automates the process of downloading files from an FTP server and storing them locally. The function should be able to switch between using regular FTP and FTP over TLS, handle errors appropriately, and ensure that all connections are properly closed. Function Signature: ```python def ftp_backup(server: str, user: str, passwd: str, directory: str, use_tls: bool, local_dir: str) -> None: pass ``` Parameters: - `server` (str): The FTP server address (e.g., \'ftp.example.com\'). - `user` (str): The username for logging into the FTP server. - `passwd` (str): The password for logging into the FTP server. - `directory` (str): The directory on the FTP server from which to download files. - `use_tls` (bool): If `True`, connect to the server using FTP over TLS; otherwise, use regular FTP. - `local_dir` (str): The local directory where the downloaded files should be stored. Requirements: 1. The function should connect to the specified FTP server and login using provided credentials. 2. It should navigate to the specified directory on the FTP server. 3. The function should list all files in the directory and download them to the specified local directory. 4. If `use_tls` is `True`, the connection should be secured using TLS. 5. Proper error handling should be implemented for connection issues, authentication failures, and file transfer errors. 6. Ensure that all connections are properly closed after the operation is complete. Constraints: - Assume the local directory exists and is writable. - The number of files in the FTP directory is moderate (not more than 100 files). Example: ```python ftp_backup(\'ftp.example.com\', \'anonymous\', \'anonymous@\', \'/pub/files\', False, \'./downloads\') ``` In this example, the function connects to \'ftp.example.com\' using regular FTP, logs in as an anonymous user, navigates to the \'/pub/files\' directory, downloads all files in this directory, and saves them to the local \'./downloads\' directory. Notes: - You may use the standard `ftplib` module in Python for implementing the FTP operations. - Consider using loops to handle multiple files and ensure proper exception handling to make the function robust against errors. Good luck!","solution":"import ftplib import os def ftp_backup(server: str, user: str, passwd: str, directory: str, use_tls: bool, local_dir: str) -> None: try: if use_tls: ftp = ftplib.FTP_TLS(server) ftp.login(user=user, passwd=passwd) ftp.prot_p() # Secure the data connection else: ftp = ftplib.FTP(server) ftp.login(user=user, passwd=passwd) ftp.cwd(directory) os.makedirs(local_dir, exist_ok=True) for filename in ftp.nlst(): local_filepath = os.path.join(local_dir, filename) with open(local_filepath, \'wb\') as local_file: ftp.retrbinary(f\'RETR {filename}\', local_file.write) except ftplib.all_errors as e: print(f\\"FTP error: {e}\\") finally: if \'ftp\' in locals(): ftp.quit() # The function is now implemented with error handling and support for TLS."},{"question":"You are provided with a Python project, and you need to create a source distribution of the project using the `sdist` command. Your task is to write a function `create_source_distribution` that performs the following steps: 1. Initializes a Python package with the necessary directory structure and files. 2. Sets up the `setup.py` file to describe the package metadata. 3. Creates a `MANIFEST.in` file to include specific files in the distribution. 4. Generates the source distribution in both `.tar.gz` and `.zip` formats. Additionally, make sure to: - Include all Python files in the `src` directory. - Include all `.md` and `.rst` files in the root directory. - Exclude any file in directories named `build`. - Use the default formats for both Unix (`.tar.gz`) and Windows (`.zip`). **Function Signature:** ```python def create_source_distribution(): pass ``` **Input:** - No direct input. The function should work on the current directory structure. **Output:** - The function should create the source distribution files in the current directory. **Example Directory Structure to Initialize:** The following directory structure should be created by your function before initializing the package: ``` my_package/ src/ __init__.py module1.py module2.py README.md setup.py MANIFEST.in build/ tempfile.txt ``` **Example Usage:** ```python create_source_distribution() # Should create my_package-<version>.tar.gz and my_package-<version>.zip in the current directory ``` **Hints:** 1. You can use the `os` and `shutil` modules to create directories and files. 2. Use the `setuptools` library for setting up the package. 3. The `MANIFEST.in` file should contain: ``` include *.md *.rst recursive-include src *.py prune build ``` 4. After setting up the directory structure and files, use the command: ```python subprocess.run([\\"python\\", \\"setup.py\\", \\"sdist\\", \\"--formats=gztar,zip\\"]) ``` **Constraints:** - The function must create the necessary directory structure if it doesn\'t exist. - Ensure that extra files in the `build` directory are excluded from the distribution. **Performance Requirements:** - The function should complete the source distribution creation process in a reasonable time for typical project sizes.","solution":"import os import subprocess import shutil def create_source_distribution(): package_name = \'my_package\' src_dir = os.path.join(package_name, \'src\') build_dir = os.path.join(package_name, \'build\') # Ensure the directory structure is correct os.makedirs(src_dir, exist_ok=True) os.makedirs(build_dir, exist_ok=True) # Create necessary files if they don\'t exist with open(os.path.join(src_dir, \'__init__.py\'), \'w\') as f: f.write(\\"# Init file for the packagen\\") with open(os.path.join(src_dir, \'module1.py\'), \'w\') as f: f.write(\\"# Module 1n\\") with open(os.path.join(src_dir, \'module2.py\'), \'w\') as f: f.write(\\"# Module 2n\\") with open(os.path.join(package_name, \'README.md\'), \'w\') as f: f.write(\\"# READMEn\\") with open(os.path.join(package_name, \'setup.py\'), \'w\') as f: f.write( \\"from setuptools import setup, find_packagesn\\" \\"setup(n\\" \\" name=\'my_package\',n\\" \\" version=\'0.1\',n\\" \\" packages=find_packages(where=\'src\'),n\\" \\" package_dir={\'\': \'src\'},n\\" \\")n\\" ) with open(os.path.join(package_name, \'MANIFEST.in\'), \'w\') as f: f.write( \\"include *.md *.rstn\\" \\"recursive-include src *.pyn\\" \\"prune buildn\\" ) # Current working directory must be the package directory original_cwd = os.getcwd() os.chdir(package_name) try: # Create the source distribution subprocess.run([\\"python\\", \\"setup.py\\", \\"sdist\\", \\"--formats=gztar,zip\\"], check=True) finally: # Change back to the original directory os.chdir(original_cwd)"},{"question":"Exception Handling with asyncio You are required to implement a set of asynchronous functions to handle file operations using the `asyncio` library. During these operations, you need to properly manage and handle exceptions related to `asyncio` as documented. Specifically, you will handle the `IncompleteReadError` and `LimitOverrunError` exceptions. # Task Implement an asynchronous function `read_file` and a helper function `_handle_exceptions`: 1. **read_file(file_path: str, buffer_size: int) -> bytes:** - This function should asynchronously read from a file at the specified `file_path` with a given `buffer_size`. - If an `IncompleteReadError` occurs, it should handle the exception, read the remaining bytes, and return the complete content. - If a `LimitOverrunError` occurs, it should handle the exception by skipping the `consumed` bytes and continue reading. - For any other exception, it should log the error and re-raise it. 2. **_handle_exceptions(func: Callable) -> Callable:** - This is a decorator function that can be used to wrap the `read_file` function to manage `IncompleteReadError` and `LimitOverrunError`. # Inputs - `file_path` (str): Path to the file to be read. - `buffer_size` (int): The number of bytes to read in each chunk. # Outputs - Returns a `bytes` object containing the content of the file read. # Constraints - Do not use synchronous file read operations. - Handle only `IncompleteReadError` and `LimitOverrunError` defined in the provided documentation. - Implement proper logging for unidentified exceptions. # Example ```python import asyncio # Example usage async def main(): file_content = await read_file(\\"sample.txt\\", 1024) print(file_content) asyncio.run(main()) ``` # Notes - Ensure the file paths used in testing exist. - Use appropriate logging instead of `print` statements for errors.","solution":"import asyncio import logging from asyncio import IncompleteReadError, LimitOverrunError # Setting up the logger logging.basicConfig(level=logging.ERROR) logger = logging.getLogger(__name__) def _handle_exceptions(func): Decorator to handle exceptions during async file read operations. async def wrapper(*args, **kwargs): try: return await func(*args, **kwargs) except IncompleteReadError as e: logger.error(f\\"IncompleteReadError: {e}\\") # Handle IncompleteReadError - read remaining bytes return e.partial + await e.reader.read() except LimitOverrunError as e: logger.error(f\\"LimitOverrunError: {e}\\") # Handle LimitOverrunError - skip consumed bytes and continue await e.reader.readexactly(e.consumed) return await func(*args, **kwargs) except Exception as e: logger.error(f\\"Unhandled exception: {e}\\") raise return wrapper @_handle_exceptions async def read_file(file_path: str, buffer_size: int) -> bytes: Asynchronously reads a file at the specified path with the given buffer size. Handles IncompleteReadError and LimitOverrunError exceptions. content = bytearray() try: async with asyncio.open(file_path, \'rb\') as file: while True: chunk = await file.read(buffer_size) if not chunk: break content.extend(chunk) except Exception as e: logger.error(f\\"Error reading file: {e}\\") raise return bytes(content)"},{"question":"<|Analysis Begin|> The given documentation focuses on the usage of seaborn with the new `seaborn.objects` module, highlighting various plot creation methods, integration with matplotlib, and customization techniques using this new interface. It includes examples on: 1. Creating plots with `seaborn.objects.Plot`. 2. Passing `matplotlib.axes.Axes` to modify the plot. 3. Creating a figure using `matplotlib.pyplot.figure` and `matplotlib.figure.Figure`. 4. Deep customization using matplotlib patches and text. 5. Utilizing `matplotlib.figure.Figure.subfigures` for composite subplot arrangements. The documentation provides a basis for creating exercises that involve: - Creating and customizing plots with `seaborn.objects.Plot`. - Manipulating matplotlib figures and axes. - Combining multiple plots using subfigures and facets. - Building plots with enhanced customization and access to the underlying matplotlib objects. Given these points, we can design a question that requires the student to demonstrate their understanding of creating and customizing plots in seaborn using the new interface, as well as their ability to manipulate matplotlib objects for further customization and layout configuration. <|Analysis End|> <|Question Begin|> # Problem Statement: You have been given a dataset of diamond prices and their attributes. Your task is to create a detailed visualization using Seaborn\'s new `seaborn.objects` module, integrated with matplotlib\'s functionality. The visualization should provide insights into the relationship between the carat and price of diamonds, including additional customization and embedding a histogram breakdown of diamond prices by their cut. # Input: - A dataset `diamonds` which can be loaded using `seaborn.load_dataset(\\"diamonds\\")`. # Requirements: 1. Create a scatter plot showing the relationship between the `carat` and `price` of diamonds. 2. Integrate this plot into a matplotlib figure created using `plt.figure()`. 3. Customize the plot by adding a rectangular annotation and some text outside the plotting area. 4. Create a companion histogram plot showing the distribution of `price`, facetted by the `cut` of diamonds. The x-axis should be on a logarithmic scale. 5. Arrange these two plots within a single figure using `matplotlib.figure.Figure.subfigures`, placing the scatter plot on the left and the histogram on the right. # Output: - Your function should generate and display the described composite figure. # Constraints: - You must use the Seaborn\'s new `seaborn.objects` module for creating the plots. - Ensure that the layout is clear, and the annotations are neatly placed. - Use matplotlib for additional customizations and arranging the subfigures. # Example Function Signature: ```python def visualize_diamond_data(): import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create Plot scatter_plot = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) histogram_plot = (so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False)) # Create the matplotlib figure with subfigures fig = mpl.figure.Figure(figsize=(14, 6), dpi=100, layout=\\"constrained\\") sf1, sf2 = fig.subfigures(1, 2) # Add the scatter plot to the first subfigure scatter_plot.on(sf1).plot() # Customize the scatter plot ax = sf1.axes[0] rect = mpl.patches.Rectangle( xy=(0, 1), width=.4, height=.1, color=\\"C1\\", alpha=.2, transform=ax.transAxes, clip_on=False, ) ax.add_artist(rect) ax.text( x=rect.get_width() / 2, y=1 + rect.get_height() / 2, s=\\"Diamonds: very sparkly!\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax.transAxes, ) # Add the histogram plot to the second subfigure histogram_plot.on(sf2) # Show the combined figure plt.show() ``` # Notes: - Ensure to import necessary libraries within your function. - The visualizations should be clear and neatly arranged. - Pay attention to the customization and placement of text and annotations.","solution":"def visualize_diamond_data(): import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create Plot scatter_plot = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) histogram_plot = (so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False)) # Create the matplotlib figure with subfigures fig = mpl.figure.Figure(figsize=(14, 6), dpi=100, layout=\\"constrained\\") sf1, sf2 = fig.subfigures(1, 2) # Add the scatter plot to the first subfigure scatter_plot.on(sf1).plot() # Customize the scatter plot ax = sf1.axes[0] rect = mpl.patches.Rectangle( xy=(0, 1), width=.4, height=.1, color=\\"C1\\", alpha=.2, transform=ax.transAxes, clip_on=False, ) ax.add_artist(rect) ax.text( x=rect.get_width() / 2, y=1 + rect.get_height() / 2, s=\\"Diamonds: very sparkly!\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax.transAxes, ) # Add the histogram plot to the second subfigure histogram_plot.on(sf2).plot() # Show the combined figure plt.show()"},{"question":"You are responsible for managing a student\'s grade book which keeps track of students\' scores. The scores need to be maintained in sorted order for easier computation and lookup. Using the `bisect` module, implement functions to insert scores maintaining the order and to efficiently find students\' scores falling in certain ranges. Tasks to Implement 1. **insert_score(scores: List[Tuple[str, int]], name: str, score: int) -> None**: This function takes a list of tuples `scores`, where each tuple contains a student\'s name and score, in sorted order by score. Insert a new student\'s score into the list while maintaining the order. - `scores`: List of tuples [(str, int)], where each tuple represents a student\'s name and score, sorted by score. - `name`: Name of the student. - `score`: Score of the student. - This function should not return anything. The `scores` list should be modified in place. 2. **find_students_in_range(scores: List[Tuple[str, int]], low: int, high: int) -> List[str]**: This function returns a list of names of students whose scores fall within the range `[low, high]` (inclusive), maintaining the original sorted order of scores. - `scores`: List of tuples [(str, int)], where each tuple represents a student\'s name and score, sorted by score. - `low`: Lower bound of the score range. - `high`: Upper bound of the score range. - Returns: List of student names who fall within the given range, maintaining the original sorted order by score. # Constraints - The scores list has at most 10^5 elements. - The scores are between 0 and 100. # Example ```python from bisect import insort_left, bisect_left, bisect_right # Sample usage scores = [(\\"Alice\\", 70), (\\"Bob\\", 82), (\\"Charlie\\", 90)] insert_score(scores, \\"David\\", 85) print(scores) # Output: [(\'Alice\', 70), (\'Bob\', 82), (\'David\', 85), (\'Charlie\', 90)] students_in_range = find_students_in_range(scores, 80, 90) print(students_in_range) # Output: [\'Bob\', \'David\', \'Charlie\'] students_in_range = find_students_in_range(scores, 60, 75) print(students_in_range) # Output: [\'Alice\'] ```","solution":"from bisect import insort_left, bisect_left, bisect_right from typing import List, Tuple def insert_score(scores: List[Tuple[str, int]], name: str, score: int) -> None: Insert a new student\'s score into the list while maintaining the order. insort_left(scores, (name, score), key=lambda x: x[1]) def find_students_in_range(scores: List[Tuple[str, int]], low: int, high: int) -> List[str]: Return a list of names of students whose scores fall within the range [low, high] (inclusive), maintaining the original sorted order of scores. left_index = bisect_left(scores, low, key=lambda x: x[1]) right_index = bisect_right(scores, high, key=lambda x: x[1]) return [scores[i][0] for i in range(left_index, right_index)]"},{"question":"# Question: Advanced Itertools Combinations **Objective:** Implement a function that generates a sequence of unique triple products of elements from four different iterables, but filtered in a specific order to ensure sequences that are not strictly increasing are omitted. # Function Signature: ```python def generate_filtered_triple_products(iter1: list, iter2: list, iter3: list, iter4: list) -> list: pass ``` # Input: 1. `iter1`, `iter2`, `iter3`, `iter4`: Lists of integers. All lists are non-empty, and their elements are distinct. # Output: A list of tuples, where each tuple is a unique combination of one element from each input list, creating a triple product filtered to hold elements in a strictly increasing order. # Constraints: - Each list will contain between 1 and 1000 elements. - The solution should be optimized for performance to handle the upper limits efficiently. # Example: ```python iter1 = [1, 2] iter2 = [3, 4] iter3 = [5, 6] iter4 = [7, 8] generate_filtered_triple_products(iter1, iter2, iter3, iter4) ``` **Expected Output:** ```python [(1, 3, 5), (1, 3, 6), (1, 4, 5), (1, 4, 6), (2, 3, 5), (2, 3, 6), (2, 4, 5), (2, 4, 6)] ``` # Explanation: The function should generate all possible combinations of three elements (one from each of the first three iterables) and filter out the ones that do not hold a strictly increasing order. # Tip: Consider using functions from the `itertools` module, such as `product()` and `combinations()`, along with `filter()`. # Further Task: After obtaining the filtered combinations of three, add an additional layer where each triplet is paired with every element of `iter4`, maintaining a strictly increasing order for the entire sequence, and ensuring uniqueness. **Note:** Focus on clear code structure, efficiency, and correct utilization of `itertools`.","solution":"from itertools import product def generate_filtered_triple_products(iter1, iter2, iter3, iter4): This function generates a sequence of unique triple products of elements from four different iterables, but filtered in a specific order to ensure sequences that are not strictly increasing are omitted. result = [] for x in iter1: for y in iter2: for z in iter3: if x < y < z: for w in iter4: if z < w: result.append((x, y, z, w)) return result"},{"question":"# Question: Unicode String Manipulations **Objective**: Implement functions that utilize Python\'s Unicode APIs to perform specific tasks. These tasks will test your understanding of Unicode string handling, encoding/decoding processes, and character property checks. Task 1: Unicode Character Properties Implement a function `unicode_properties(unicode_string)` that takes a Unicode string and returns a dictionary containing the following properties: - `is_whitespace`: A list of boolean values indicating whether each character in the string is a whitespace character. - `is_upper`: A list of boolean values indicating whether each character is an uppercase character. - `is_lower`: A list of boolean values indicating whether each character is a lowercase character. - `is_decimal`: A list of boolean values indicating whether each character is a decimal digit. ```python def unicode_properties(unicode_string: str) -> dict: # Your implementation here pass ``` Task 2: Encoding and Decoding Implement functions `encode_to_utf8` and `decode_from_utf8` to encode a given Unicode string to UTF-8 bytes and decode UTF-8 bytes back to a Unicode string, respectively. ```python def encode_to_utf8(unicode_string: str) -> bytes: # Your implementation here pass def decode_from_utf8(utf8_bytes: bytes) -> str: # Your implementation here pass ``` Task 3: String Translation Implement a function `translate_string(unicode_string, char_map)` that translates each character in `unicode_string` using the provided character map `char_map`. The character map is a dictionary mapping Unicode ordinals to Unicode String. ```python def translate_string(unicode_string: str, char_map: dict) -> str: # Your implementation here pass ``` # Constraints 1. For `unicode_properties`, you must use the respective properties functions provided in the documentation. 2. For `encode_to_utf8` and `decode_from_utf8`, you should use the functions or macros that deal with UTF-8 encoding and decoding from the documentation. 3. For `translate_string`, make sure to use the `PyUnicode_Translate` (or its wrapper in Python) provided in the documentation. # Examples ```python # Example usage for Task 1: props = unicode_properties(\\"Hello World 123\\") print(props) # Outputs: { # \'is_whitespace\': [False, False, False, False, False, True, False, False, False, False, False, True, False, False, False], # \'is_upper\': [True, False, False, False, False, False, True, False, False, False, False, False, False, False, False], # \'is_lower\': [False, True, True, True, True, False, False, True, True, True, True, False, False, False, False], # \'is_decimal\': [False, False, False, False, False, False, False, False, False, False, False, False, True, True, True] # } # Example usage for Task 2: encoded = encode_to_utf8(\\"Hello\\") print(encoded) # Outputs: b\'Hello\' decoded = decode_from_utf8(b\'Hello\') print(decoded) # Outputs: \'Hello\' # Example usage for Task 3: char_map = {ord(\'a\'): \'1\', ord(\'e\'): \'2\', ord(\'i\'): \'3\', ord(\'o\'): \'4\', ord(\'u\'): \'5\'} translated = translate_string(\\"Hello World\\", char_map) print(translated) # Outputs: \'H2ll4 W4rld\' ``` **Note**: Ensure that your implementations utilize the proper functions and macros from the Unicode API documentation provided.","solution":"import unicodedata def unicode_properties(unicode_string: str) -> dict: return { \'is_whitespace\': [char.isspace() for char in unicode_string], \'is_upper\': [char.isupper() for char in unicode_string], \'is_lower\': [char.islower() for char in unicode_string], \'is_decimal\': [char.isdecimal() for char in unicode_string] } def encode_to_utf8(unicode_string: str) -> bytes: return unicode_string.encode(\'utf-8\') def decode_from_utf8(utf8_bytes: bytes) -> str: return utf8_bytes.decode(\'utf-8\') def translate_string(unicode_string: str, char_map: dict) -> str: return unicode_string.translate(char_map)"},{"question":"**Question: Implementing Audio Playback Controller** **Objective:** Design a class `AudioController` that demonstrates the management and playback of audio using `ossaudiodev` functionalities. This will test your understanding of file-like operations and audio device handling in Python using the `ossaudiodev` module. **Class to Implement:** ```python class AudioController: def __init__(self, device=None): \'\'\' Initializes the AudioController with the given OSS audio device. If no device is provided, it defaults to the value of AUDIODEV environment variable, or \'/dev/dsp\' if the environment variable is not set. Parameters: device (str): Optional audio device filename to use. \'\'\' pass def configure_device(self, format, channels, rate): \'\'\' Configures the audio device with the specified parameters. Parameters: format (str): The audio format (choose from the formats listed in the documentation). channels (int): Number of audio channels (1 for mono, 2 for stereo). rate (int): Sample rate in samples per second (common values are 8000, 11025, 22050, 44100, 96000). Returns: tuple: The actual parameters set by the device. \'\'\' pass def play_audio(self, data, blocking=True): \'\'\' Plays the given audio data on the configured audio device. Parameters: data (bytes): The audio data to play. blocking (bool): If True, the function should block until all data is played. If False, play in non-blocking mode. Returns: int: The number of bytes written to the device when in blocking mode. None: When in non-blocking mode. \'\'\' pass def close_device(self): \'\'\' Closes the audio device. \'\'\' pass ``` **Instructions:** 1. **Initialization:** - Implement the `__init__` method to open the specified OSS audio device. If not specified, use the device from `AUDIODEV` environment variable or default to `/dev/dsp`. - Use `ossaudiodev.open()` to open the audio device. 2. **Configure Device:** - Implement the `configure_device` method to set audio device parameters using `setparameters()`. Ensure flexibility by handling parameters similar to `setfmt()`, `channels()`, and `speed()`. - Return the actual parameters set by the device. 3. **Playing Audio:** - Implement the `play_audio` method to write audio data to the device. Use `write()` for blocking mode and `writeall()` for non-blocking mode. - Handle either mode appropriately and ensure all data is written or adequately handled in non-blocking mode. 4. **Close Device:** - Implement the `close_device` method to close the audio device using `close()` method and ensure the device cannot be reused after closing. **Constraints:** - Ensure that the specified `format`, `channels`, and `rate` are supported by the OSS audio device. - Handle any exceptions that may arise, particularly `OSError` and `OSSAudioError`. - Do not modify the state of a closed device. **Input/Output:** - Initialization takes an optional device filename. - `configure_device` takes `format`, `channels`, and `rate` and returns a tuple of the actual parameters set. - `play_audio` takes `data` (bytes) and `blocking` (boolean), returns the number of bytes written if blocking, or None if non-blocking. - `close_device` takes no parameters and does not return anything. **Example Usage:** ```python controller = AudioController() params = controller.configure_device(\'AFMT_S16_LE\', 2, 44100) print(f\\"Configured parameters: {params}\\") data = b\\"...binary audio data...\\" controller.play_audio(data, blocking=True) controller.close_device() ``` Design and implement the `AudioController` class following these specifications.","solution":"import ossaudiodev import os class AudioController: def __init__(self, device=None): Initializes the AudioController with the given OSS audio device. If no device is provided, it defaults to the value of AUDIODEV environment variable, or \'/dev/dsp\' if the environment variable is not set. if device is None: device = os.environ.get(\'AUDIODEV\', \'/dev/dsp\') self.audio_dev = ossaudiodev.open(\'w\') self.device = device self.is_open = True def configure_device(self, format, channels, rate): Configures the audio device with the specified parameters. Parameters: format (str): The audio format (choose from the formats listed in the documentation). channels (int): Number of audio channels (1 for mono, 2 for stereo). rate (int): Sample rate in samples per second (common values are 8000, 11025, 22050, 44100, 96000). Returns: tuple: The actual parameters set by the device. if not self.is_open: raise RuntimeError(\\"Audio device is closed.\\") format_constant = getattr(ossaudiodev, format) self.audio_dev.setfmt(format_constant) actual_channels = self.audio_dev.channels(channels) actual_rate = self.audio_dev.speed(rate) return (format, actual_channels, actual_rate) def play_audio(self, data, blocking=True): Plays the given audio data on the configured audio device. Parameters: data (bytes): The audio data to play. blocking (bool): If True, the function should block until all data is played. If False, play in non-blocking mode. Returns: int: The number of bytes written to the device when in blocking mode. None: When in non-blocking mode. if not self.is_open: raise RuntimeError(\\"Audio device is closed.\\") if blocking: bytes_written = self.audio_dev.write(data) return bytes_written else: self.audio_dev.writeall(data) return None def close_device(self): Closes the audio device. if not self.is_open: raise RuntimeError(\\"Audio device is already closed.\\") self.audio_dev.close() self.is_open = False"},{"question":"**Seaborn Coding Assessment Question** # Objective: Assess your ability to create complex visualizations using seaborn\'s `so` (seaborn objects) interface. # Problem Statement: You are given the penguins dataset. Create a visualization showing the relationship between body mass and species, faceted by island. Include the following elements in your plot: 1. Use a dot plot to show individual observations. 2. Compute and display the mean body mass for each species using error bars representing one standard deviation. 3. Customize the styles for better readability: - Different colors for each species. - Facet the plot by `island`. # Dataset: You can load the dataset using the following code: ```python import seaborn.objects as so from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") ``` # Expected Input: - No input required. The data and expected plots should be derived programmatically. # Expected Output: - A seaborn plot as described above, correctly displaying data and computed statistics. # Constraints: - Use seaborn\'s `so` (seaborn objects) interface. - Ensure the plot is readable with appropriate colors and styles. - Handle missing values gracefully if they exist. # Performance Requirements: - The code should run efficiently, suitable for datasets similar in size to `penguins`. # Example Solution Outline: While the exact implementation may vary, your solution should follow these steps: 1. Load the penguins dataset. 2. Create a seaborn `so.Plot` object. 3. Add dot plots for each observation. 4. Add mean error bars. 5. Customize the plot with colors and faceting. ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"species\\") .facet(\\"island\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Render the plot plot.show() ``` This outline is provided for guidance, and students are expected to fill in the details to meet the specified problem statement.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"species\\") .facet(\\"island\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Render the plot plot.show()"},{"question":"# Question You are tasked with implementing distributed training for a simple feedforward neural network using PyTorch\'s `torch.nn.parallel.DistributedDataParallel` (DDP). You will need to set up the DDP for a 3-layer feedforward network and ensure proper synchronization across multiple processes during training. Requirements 1. **Model Architecture**: Implement a 3-layer feedforward neural network with the following layers: - Input layer: 100 neurons (input size) -> 128 neurons - Hidden layer: 128 neurons -> 64 neurons - Output layer: 64 neurons -> 10 neurons (output size) 2. **Training Loop**: - Use Mean Squared Error (MSELoss) as the loss function. - Use SGD as the optimizer with a learning rate of 0.01. 3. **Distributed Setup**: - Use DDP to distribute the training across 4 processes. - Ensure gradients are synchronized appropriately and model parameters are updated correctly across all processes. 4. **Implementation**: - Write a function `train(rank, world_size)` to handle the process-specific initialization and training. - Set up environment variables for master address and port. - Use `mp.spawn` to start the training processes. 5. **Performance**: - Ensure the implementation efficiently handles synchronization with minimal overhead. Constraints - Use only PyTorch for implementation (`torch`, `torch.nn`, `torch.optim`, `torch.distributed`, `torch.multiprocessing`). - Do not use any other libraries for distributed training. Input and Output - **Input**: None - **Output**: Print the final loss from each process. Example Code Structure You may start with the following code structure: ```python import torch import torch.distributed as dist import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim import os from torch.nn.parallel import DistributedDataParallel as DDP class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(100, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x def train(rank, world_size): # Initialize process group dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) torch.manual_seed(0) # Model creation and DDP wrapper model = SimpleNN().to(rank) ddp_model = DDP(model, device_ids=[rank]) # Loss and optimizer loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) # Dummy data inputs = torch.randn(20, 100).to(rank) labels = torch.randn(20, 10).to(rank) # Training loop for epoch in range(10): optimizer.zero_grad() outputs = ddp_model(inputs) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() print(f\\"Rank {rank}, Loss: {loss.item()}\\") dist.destroy_process_group() def main(): world_size = 4 os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main() ``` Revise the given example to match the requirements and ensure that it is capable of running distributed training across multiple processes.","solution":"import torch import torch.distributed as dist import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim import os from torch.nn.parallel import DistributedDataParallel as DDP class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(100, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x def train(rank, world_size): # Initialize process group dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) torch.manual_seed(0) # Model creation and DDP wrapper model = SimpleNN().to(rank) ddp_model = DDP(model, device_ids=[rank]) # Loss and optimizer loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) # Dummy data inputs = torch.randn(20, 100).to(rank) labels = torch.randn(20, 10).to(rank) # Training loop for epoch in range(10): optimizer.zero_grad() outputs = ddp_model(inputs) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() print(f\\"Rank {rank}, Loss: {loss.item()}\\") dist.destroy_process_group() def main(): world_size = 4 os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with implementing a function that processes a list of strings and performs several operations using Python\'s built-in functions. The function should: 1. Filter out strings that are empty or contain only whitespace. 2. Convert all non-empty strings to their ASCII representation. 3. Iterate over each string and compute its hash value. 4. Calculate the maximum and minimum hash values from the computed hash values. 5. Create a dictionary where keys are the original strings and values are their corresponding hash values. The function should be named `process_strings` and meet the following specifications: Function Signature ```python def process_strings(strings: list) -> dict: pass ``` Parameters - `strings` (list): A list of strings to be processed. Returns - `dict`: A dictionary with original strings as keys and their corresponding hash values as values. Constraints - The input list will contain at most 1000 strings. - Each string will have a length of at most 100 characters. Example ```python input_strings = [\\"hello\\", \\" world \\", \\"\\", \\" \\", \\"Python3!\\"] output = process_strings(input_strings) print(output) ``` Expected Output: ```python { \\"hello\\": 127937526171146338, \\" world \\": 2473307671223768913, \\"Python3!\\": -7926777097729298693 } ``` # Requirements - Use the `filter()` function to remove empty strings. - Use the `ascii()` function to convert strings to their ASCII representation. - Compute hash values using the `hash()` function. - Use `max()` and `min()` functions to find the maximum and minimum hash values (just to ensure you understand their usage, but not necessarily to return them). - Return a dictionary with original strings as keys and their hash values as values.","solution":"def process_strings(strings: list) -> dict: Processes a list of strings by filtering out empty or whitespace-only strings, converting them to their ASCII representation, computing their hash values, and creating a dictionary with the original strings as keys and their hash values as values. # Filter out empty or whitespace strings filtered_strings = filter(lambda s: s.strip() != \\"\\", strings) # Create dictionary with original strings and their hash values result = {s: hash(ascii(s)) for s in filtered_strings} # Optionally compute max and min hash values to show usage but won\'t include them in final dict hash_values = result.values() if hash_values: # check if there are any hash values max_hash = max(hash_values) min_hash = min(hash_values) return result"},{"question":"Advanced Data Analysis with pandas GroupBy You are provided with a dataset containing information about employees in a company, stored in a `pandas` DataFrame. The dataset has the following columns: - `employee_id`: An integer identifier for each employee. - `department`: The name of the department the employee belongs to. - `age`: The age of the employee. - `salary`: The salary of the employee. - `date_of_joining`: The date when the employee joined the company (as a `datetime` object). Your task is to perform several analyses on this dataset using `pandas` and the `GroupBy` object. # Input You are given a DataFrame `df` with the aforementioned columns. # Tasks 1. **Calculate Department Statistics:** * Group the data by `department` and calculate the total number of employees, the average salary, and the maximum age in each department. * Return this information as a DataFrame sorted by the department name. 2. **Top Salaries in Each Department:** * For each department, find the top 3 highest salaries. * Return a DataFrame containing `employee_id`, `department`, and `salary` of these employees, sorted by department and salary in descending order. 3. **Salary Increase Analysis:** * Assume each employee receives an annual salary increase of 5%. * Compute and return a DataFrame showing each employee\'s `employee_id`, `department`, current `salary`, and a `projected_salary` after 3 years of continuous employment. 4. **Visualize Age Distribution:** * Plot a histogram of the ages of employees in each department. * Ensure each department\'s histogram is uniquely identifiable in the plot (use different colors or labels). # Sample Code ```python import pandas as pd # Example DataFrame (You can assume that this DataFrame is provided to you) data = { \'employee_id\': [1, 2, 3, 4, 5], \'department\': [\'HR\', \'Engineering\', \'HR\', \'Engineering\', \'Marketing\'], \'age\': [25, 32, 40, 28, 35], \'salary\': [50000, 60000, 70000, 80000, 90000], \'date_of_joining\': pd.to_datetime([\'2020-01-01\', \'2019-01-01\', \'2018-01-01\', \'2017-01-01\', \'2016-01-01\']) } df = pd.DataFrame(data) # Task 1: Calculate Department Statistics def calculate_department_statistics(df): grouped = df.groupby(\'department\').agg({ \'employee_id\': \'count\', \'salary\': \'mean\', \'age\': \'max\' }).rename(columns={\'employee_id\': \'total_employees\', \'salary\': \'average_salary\', \'age\': \'max_age\'}).reset_index() return grouped.sort_values(by=\'department\') # Task 2: Top Salaries in Each Department def top_salaries_in_each_department(df): grouped = df.groupby(\'department\').apply(lambda x: x.nlargest(3, \'salary\')).reset_index(drop=True) return grouped[[\'employee_id\', \'department\', \'salary\']] # Task 3: Salary Increase Analysis def salary_increase_analysis(df): df[\'projected_salary\'] = df[\'salary\'] * (1.05 ** 3) return df[[\'employee_id\', \'department\', \'salary\', \'projected_salary\']] # Task 4: Visualize Age Distribution import matplotlib.pyplot as plt def plot_age_distribution(df): departments = df[\'department\'].unique() for department in departments: subset = df[df[\'department\'] == department] plt.hist(subset[\'age\'], alpha=0.5, label=department) plt.xlabel(\'Age\') plt.ylabel(\'Number of Employees\') plt.title(\'Age Distribution by Department\') plt.legend(title=\'Departments\') plt.show() # Example usage: department_stats = calculate_department_statistics(df) top_salaries = top_salaries_in_each_department(df) salary_projection = salary_increase_analysis(df) plot_age_distribution(df) # Note: In a testing environment, you would ensure that the DataFrame is provided and the plots are appropriately displayed. ``` # Constraints - You should not modify the input DataFrame directly; return new DataFrames as required. - Follow best practices for pandas operations, especially when handling large datasets. # Evaluation Your solution will be evaluated based on: - Correctness of the results. - Efficient use of pandas `GroupBy` methods. - Clarity and readability of the code. - Proper handling of edge cases (e.g., departments with fewer than 3 employees).","solution":"import pandas as pd import matplotlib.pyplot as plt # Task 1: Calculate Department Statistics def calculate_department_statistics(df): grouped = df.groupby(\'department\').agg({ \'employee_id\': \'count\', \'salary\': \'mean\', \'age\': \'max\' }).rename(columns={\'employee_id\': \'total_employees\', \'salary\': \'average_salary\', \'age\': \'max_age\'}).reset_index() return grouped.sort_values(by=\'department\') # Task 2: Top Salaries in Each Department def top_salaries_in_each_department(df): grouped = df.groupby(\'department\').apply(lambda x: x.nlargest(3, \'salary\')).reset_index(drop=True) return grouped[[\'employee_id\', \'department\', \'salary\']] # Task 3: Salary Increase Analysis def salary_increase_analysis(df): df[\'projected_salary\'] = df[\'salary\'] * (1.05 ** 3) return df[[\'employee_id\', \'department\', \'salary\', \'projected_salary\']] # Task 4: Visualize Age Distribution def plot_age_distribution(df): departments = df[\'department\'].unique() for department in departments: subset = df[df[\'department\'] == department] plt.hist(subset[\'age\'], alpha=0.5, label=department) plt.xlabel(\'Age\') plt.ylabel(\'Number of Employees\') plt.title(\'Age Distribution by Department\') plt.legend(title=\'Departments\') plt.show()"},{"question":"Objective: Write a Python function that analyzes a given string and verifies if it meets the following conditions: 1. All characters are valid Unicode characters with assigned names. 2. All characters are either letters or numbers. 3. The string is in Normal Form C (NFC). Function Signature: ```python def analyze_unicode_string(s: str) -> Tuple[bool, str]: pass ``` Parameters: - `s`: A string consisting of Unicode characters. Returns: - A tuple `(is_valid, status)` where: - `is_valid` (bool): `True` if the string meets all the defined conditions, otherwise `False`. - `status` (str): A description of the status. Possible values are: - `\\"Valid Unicode String\\"`: if the string meets all conditions. - `\\"Invalid Unicode Character\\"`: if there is at least one character with no assigned Unicode name. - `\\"Contains Non-letter/Non-number Character\\"`: if there is at least one character that is not a letter or a number. - `\\"Not Normal Form C\\"`: if the string is not in Normal Form C. Constraints: - The function should handle exceptions appropriately, especially when looking up character names or properties that may not exist. - The performance of the function should be efficient even for strings with a length of up to 10,000 characters. Example: ```python # Example 1: s = \\"Test123\\" print(analyze_unicode_string(s)) # Output: (True, \\"Valid Unicode String\\") # Example 2: s = \\"Test@123\\" print(analyze_unicode_string(s)) # Output: (False, \\"Contains Non-letter/Non-number Character\\") # Example 3: s = \\"u0043u0327\\" print(analyze_unicode_string(s)) # Output: (False, \\"Not Normal Form C\\") # Example 4: s = \\"u00C7\\" print(analyze_unicode_string(s)) # Output: (True, \\"Valid Unicode String\\") ``` Notes: - Utilize the `unicodedata` module’s functions such as `lookup(), name(), category(), normalize(), is_normalized()` to implement the solution. - Ensure that you handle any potential exceptions like `ValueError` or `KeyError` for invalid character lookups.","solution":"import unicodedata from typing import Tuple def analyze_unicode_string(s: str) -> Tuple[bool, str]: Analyzes the provided string to check if it meets the given conditions: 1. All characters are valid Unicode characters with assigned names. 2. All characters are either letters or numbers. 3. The string is in Normal Form C (NFC). Parameters: s (str): Input string consisting of Unicode characters. Returns: Tuple[bool, str]: A tuple containing a boolean indicating validity and a status message. # Check if the string is in Normal Form C if not unicodedata.is_normalized(\'NFC\', s): return (False, \\"Not Normal Form C\\") for char in s: try: # Check if the character has a valid Unicode name unicodedata.name(char) except ValueError: return (False, \\"Invalid Unicode Character\\") # Check if the character is either a letter or a number if not (char.isalpha() or char.isdigit()): return (False, \\"Contains Non-letter/Non-number Character\\") return (True, \\"Valid Unicode String\\")"},{"question":"# Advanced PyTorch Assessment Fleet-wide Operator Profiling and Custom Hook Integration **Objective**: Your task is to create a custom fleet-wide profiling mechanism using PyTorch\'s `torch.autograd.profiler` and implement a mechanism to attach custom metadata to TorchScript models during the saving process. Part 1: Fleet-wide Profiling 1. **Function Implementation**: - Implement a function `init_profiling` that initializes global callbacks to profile PyTorch operations. - The callback should log the name of each operation and the number of inputs it receives. 2. **Constraints**: - Sample one operation in every fifty. This means the sampling probability should be set to 0.02. - Ensure the profiling mechanism is enabled in the calling thread before running any model. **Function Signature**: ```python def init_profiling(): pass ``` Part 2: Metadata Attachment 1. **Function Implementation**: - Implement a function `save_model_with_metadata` to save a PyTorch model with additional metadata attached. - Use `torch.jit.save` and pass `_extra_files` to include custom metadata in the saved model. 2. **Constraints**: - Metadata should include information about the user saving the model and the model description. - The metadata must be saved in a JSON format. **Function Signature**: ```python def save_model_with_metadata(model: torch.jit.ScriptModule, file_path: str, metadata: dict): pass ``` The `metadata` dictionary may look like: ```python metadata = { \\"producer\\": \\"user_name\\", \\"description\\": \\"This is a sample model for testing\\" } ``` Example Usage: ```python import torch # Sample dummy model class MyModel(torch.nn.Module): def forward(self, x): return x * 2 # Convert to TorchScript dummy_model = torch.jit.script(MyModel()) # Initialize profiling init_profiling() # Metadata information metadata_info = { \\"producer\\": \\"test_user\\", \\"description\\": \\"Simple model to test metadata attachment\\" } # Save model with metadata save_model_with_metadata(dummy_model, \\"model.pt\\", metadata_info) # Load and verify metadata (for validation purpose) loaded_model = torch.jit.load(\\"model.pt\\", _extra_files={\\"metadata.json\\": \\"\\"}) print(loaded_model.metadata()) ``` **Note**: - You should provide complete implementations for both functions `init_profiling` and `save_model_with_metadata`. - Thoroughly test your provided solution to ensure it meets the outlined requirements. **Assessment Criteria**: - Correctness of the implementation. - Compliance with the specified constraints. - Proper usage of PyTorch\'s advanced extension points and hooks as described.","solution":"import torch import torch.autograd.profiler as profiler import json def init_profiling(): Initializes global callbacks to profile PyTorch operations. Logs the name of each operation and the number of inputs it receives. Samples one operation in every fifty (2% sampling rate). torch.autograd.profiler.profile(enabled=True, use_cuda=False) handler = torch.autograd.profiler.profile() def _callback(op_name, inputs, _): if torch.randint(0, 50, (1,)).item() == 0: print(f\\"Operation: {op_name}, Number of inputs: {len(inputs)}\\") handler.fn = _callback return handler def save_model_with_metadata(model: torch.jit.ScriptModule, file_path: str, metadata: dict): Saves a TorchScript model with additional metadata. Parameters ---------- model : torch.jit.ScriptModule The TorchScript model to be saved. file_path : str The path to save the model. metadata : dict Dictionary containing metadata such as producer and description. assert isinstance(metadata, dict), \\"Metadata must be a dictionary\\" _extra_files = {\\"metadata.json\\": json.dumps(metadata)} torch.jit.save(model, file_path, _extra_files=_extra_files)"},{"question":"You are developing a Python application that processes data in parallel using multiple threads. Your task is to implement a function that starts several worker threads to perform computations and aggregates the results. You need to manage the threads using the `_thread` module for synchronization and ensure safe data access. Implement the function `compute_parallel(data, num_threads)` that performs the following steps: 1. **Data Partitioning**: - The input `data` is a list of integers. - Divide the list `data` into `num_threads` sublists as evenly as possible. 2. **Worker Function**: - Define a worker function that takes a sublist, computes the sum of its elements, appends the result to a global list `results` which should be protected using a lock to ensure thread safety. 3. **Thread Management**: - Start exactly `num_threads` threads using `_thread.start_new_thread()`, each executing the worker function on its respective sublist. - Ensure each started thread completes before the main thread proceeds. 4. **Aggregation**: - Once all threads have finished, aggregate the partial results from `results` into a single sum and return it. 5. **Signal Handling**: - Implement a mechanism to interrupt the main thread if a computation takes too long (e.g., more than 10 seconds), raise a `RuntimeError`. Function Signature: ```python def compute_parallel(data: List[int], num_threads: int) -> int: pass ``` Input: - `data`: List of integers, e.g., [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] - `num_threads`: Number of threads to use, a positive integer. Output: - An integer, which is the sum of all elements in `data`. Example: ```python data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] num_threads = 3 result = compute_parallel(data, num_threads) print(result) # Output should be 55 ``` Constraints: - Ensure the computation completes within a reasonable time frame (10 seconds). - Properly handle any exceptional scenarios where a thread fails or times out. Implementing this function will demonstrate your understanding of threading, synchronization using locks, and signal handling with the `_thread` module in Python.","solution":"import _thread import threading # The _thread module does not have a Lock class. We need threading.Lock for thread safety. from typing import List import time def compute_parallel(data: List[int], num_threads: int) -> int: # Ensuring the number of threads does not exceed the length of the data num_threads = min(num_threads, len(data)) # Dividing data into sublists for each thread. chunk_size = len(data) // num_threads chunks = [data[i * chunk_size:(i + 1) * chunk_size] for i in range(num_threads)] # To handle the leftover elements after chunking. for i in range(len(data) % num_threads): chunks[i].append(data[num_threads * chunk_size + i]) results = [] lock = threading.Lock() threads = [] def worker(sublist): result = sum(sublist) with lock: results.append(result) def start_worker_thread(sublist): nonlocal threads thread = threading.Thread(target=worker, args=(sublist,)) threads.append(thread) thread.start() # Starting threads for chunk in chunks: start_worker_thread(chunk) # Ensuring main thread waits for all worker threads to complete for thread in threads: thread.join() return sum(results)"},{"question":"Question: Manipulate Email Messages using `email.message.Message` class You are asked to implement a series of tasks involving the `email.message.Message` class from the `email` package in `python310`. You need to create, manipulate, and analyze email messages as specified below. 1. **Function: `create_simple_email`** - **Description:** Create a simple email message using the `email.message.Message` class. - **Inputs:** - `from_address` (str): The sender\'s email address. - `to_address` (str): The recipient\'s email address. - `subject` (str): The subject of the email. - `body` (str): The body of the email. - **Output:** Return a `Message` object representing the email. ```python def create_simple_email(from_address: str, to_address: str, subject: str, body: str) -> Message: pass ``` 2. **Function: `attach_file_to_email`** - **Description:** Attach a file to an existing email message. - **Inputs:** - `message` (`Message`): The email message object to which the file will be attached. - `file_content` (bytes): The content of the file. - `filename` (str): The name of the file to be attached. - **Output:** Return the modified `Message` object with the attached file. ```python def attach_file_to_email(message: Message, file_content: bytes, filename: str) -> Message: pass ``` 3. **Function: `extract_attachments`** - **Description:** Extract filenames of all attachments from a given email message. - **Inputs:** - `message` (`Message`): The email message object from which attachments will be extracted. - **Output:** Return a list of filenames of all attached files. ```python def extract_attachments(message: Message) -> list: pass ``` 4. **Function: `summarize_email_headers`** - **Description:** Summarize the headers of an email message, listing all headers and their values. - **Inputs:** - `message` (`Message`): The email message object whose headers will be summarized. - **Output:** Return a dictionary where keys are header names and values are header values. ```python def summarize_email_headers(message: Message) -> dict: pass ``` Constraints - Use the `email.message.Message` class only. - The email messages are simple and do not require advanced content types or encoding. Example Usage ```python msg = create_simple_email(\\"sender@example.com\\", \\"recipient@example.com\\", \\"Greetings\\", \\"Hello, this is a test email.\\") msg = attach_file_to_email(msg, b\\"File content\\", \\"test.txt\\") print(extract_attachments(msg)) # Output: [\'test.txt\'] print(summarize_email_headers(msg)) # Output: {\'From\': \'sender@example.com\', \'To\': \'recipient@example.com\', \'Subject\': \'Greetings\', ...} ``` Good luck!","solution":"from email.message import EmailMessage def create_simple_email(from_address: str, to_address: str, subject: str, body: str) -> EmailMessage: Creates a simple email message. msg = EmailMessage() msg.set_content(body) msg[\'From\'] = from_address msg[\'To\'] = to_address msg[\'Subject\'] = subject return msg def attach_file_to_email(message: EmailMessage, file_content: bytes, filename: str) -> EmailMessage: Attaches a file to an email message. message.add_attachment(file_content, maintype=\'application\', subtype=\'octet-stream\', filename=filename) return message def extract_attachments(message: EmailMessage) -> list: Extracts filenames of all attachments from a given email message. filenames = [] for part in message.iter_attachments(): filenames.append(part.get_filename()) return filenames def summarize_email_headers(message: EmailMessage) -> dict: Summarizes the headers of an email message. return dict(message.items())"},{"question":"# PyTorch CUDA Advanced Operations Assessment Objective: You are tasked with demonstrating your understanding of CUDA functionalities in PyTorch by writing code that utilizes various CUDA-related operations including device management, tensor operations on CUDA, and memory management. Question: 1. **Device Management and Tensor Operations**: a. Write a function `cuda_tensor_operations` that: - Accepts a list of integers. - Converts this list into a PyTorch tensor and transfers it to the CUDA device. - Performs element-wise square operation on the tensor. - Returns the resultant tensor back to the CPU. ```python import torch def cuda_tensor_operations(input_list): Accepts a list of integers, converts it to a CUDA tensor, performs element-wise square, and returns the resultant tensor to CPU. Args: input_list (list): List of integers Returns: torch.Tensor: Tensor containing the squared values, on CPU # Your code here ``` 2. **Memory Management**: b. Write a second function `cuda_memory_stats` that: - Returns the current memory statistics of the default CUDA device, including memory allocated, cached, and utilized. ```python def cuda_memory_stats(): Retrieves and returns the current memory statistics of the default CUDA device. Returns: dict: A dictionary containing memory statistics: - \'memory_allocated\': currently allocated memory in bytes - \'memory_cached\': currently cached memory in bytes - \'memory_utilized\': total memory utilized by current process in bytes # Your code here ``` 3. **Random Number Generation on CUDA**: c. Write a function `random_cuda_tensor` that: - Receives an integer `size` and a seed value for the random number generator. - Seeds the random number generator on the CUDA device. - Generates a tensor of specified `size` with random values on the CUDA device. - Returns the tensor back to the CPU. ```python def random_cuda_tensor(size, seed): Generates a tensor of random values on CUDA with given size and seed, and returns it on CPU. Args: size (int): Size of the tensor to be generated seed (int): Seed for the random number generator Returns: torch.Tensor: Tensor of random values on CPU # Your code here ``` Constraints: - Your solution should ensure that any allocated GPU memory is deallocated after operations. - Use appropriate PyTorch CUDA functions and ensure efficient memory usage. - Consider edge cases such as empty input lists or invalid sizes. # Expected Input/Output: 1. For `cuda_tensor_operations([1, 2, 3])`: - Output: `tensor([1, 4, 9])` 2. For `cuda_memory_stats()`: - Output: A dictionary containing memory statistics like `memory_allocated`, `memory_cached`, and `memory_utilized`. 3. For `random_cuda_tensor(5, 42)`: - Output: A tensor of size 5 on CPU with random values, e.g., `tensor([0.3285, 0.2118, 0.1217, 0.1457, 0.8784])`. (Note: Actual values will vary but are reproducible with the same seed.) Good luck!","solution":"import torch def cuda_tensor_operations(input_list): Accepts a list of integers, converts it to a CUDA tensor, performs element-wise square, and returns the resultant tensor to CPU. Args: input_list (list): List of integers Returns: torch.Tensor: Tensor containing the squared values, on CPU # Convert the list to a tensor and transfer it to CUDA tensor = torch.tensor(input_list).cuda() # Perform element-wise square operation on the tensor squared_tensor = tensor ** 2 # Move the resultant tensor back to CPU and return return squared_tensor.cpu() def cuda_memory_stats(): Retrieves and returns the current memory statistics of the default CUDA device. Returns: dict: A dictionary containing memory statistics: - \'memory_allocated\': currently allocated memory in bytes - \'memory_cached\': currently cached memory in bytes - \'memory_utilized\': total memory utilized by current process in bytes # Retrieve memory statistics from the default CUDA device memory_stats = { \'memory_allocated\': torch.cuda.memory_allocated(), \'memory_cached\': torch.cuda.memory_reserved(), \'memory_utilized\': torch.cuda.max_memory_reserved() } return memory_stats def random_cuda_tensor(size, seed): Generates a tensor of random values on CUDA with given size and seed, and returns it on CPU. Args: size (int): Size of the tensor to be generated seed (int): Seed for the random number generator Returns: torch.Tensor: Tensor of random values on CPU # Seed the random number generator on the CUDA device torch.manual_seed(seed) torch.cuda.manual_seed(seed) # Generate a random tensor of specified size on the CUDA device random_tensor = torch.randn(size).cuda() # Move the tensor back to CPU and return it return random_tensor.cpu()"},{"question":"# Custom Object Implementation and Memory Management Objective: Create a Python extension type using a class that simulates a simple \\"Counter\\" object. This object must support basic operations such as initialization, incrementing, retrieving current value, and resetting. Additionally, implement support for garbage collection to handle cyclic references. Requirements: 1. **Initialization**: - The Counter should be initialized with an optional starting value (default to 0). 2. **Operations**: - `increment(amount)`: Increment the counter by the specified amount. - `value()`: Return the current value of the counter. - `reset()`: Reset the counter to its initial value. 3. **Attribute Access and Memory Management**: - Provide appropriate methods to manage attributes and ensure they are accessible. 4. **Garbage Collection Support**: - Implement mechanisms to properly handle cyclic references and participate in Python\'s garbage collection. Constraints: - You are to define your custom type using `PyTypeObject` and associated structures. - Utilize appropriate slots and methods to implement the described functionalities. - Ensure your implementation is efficient and follows Python\'s memory management best practices. Implementation Hints: - Use provided typedefs and structures like `PyObject`, `PyTypeObject`, etc. - Ensure proper use of reference counting and garbage collection hooks to manage memory and cyclic references. - You are encouraged to refer to Python C API documentation where necessary to understand how to build and manage custom object types. Example Usage: ```python # Assume the module is named `counter` and contains the `Counter` type from counter import Counter # Initialize the counter c = Counter(10) # Increment the counter by 5 c.increment(5) # Retrieve the current value (expected: 15) print(c.value()) # Reset the counter c.reset() # Retrieve the current value after reset (expected: 10) print(c.value()) ``` This exercise will test your understanding of Python\'s object type implementation, attribute management, and garbage collection support.","solution":"class Counter: def __init__(self, starting_value=0): Initializes the Counter with an optional starting value (default to 0). self.starting_value = starting_value self.current_value = starting_value def increment(self, amount): Increment the counter by the specified amount. self.current_value += amount def value(self): Return the current value of the counter. return self.current_value def reset(self): Reset the counter to its initial value. self.current_value = self.starting_value"},{"question":"# Question: You are provided with a Python codebase for a simple library system. The system includes a `Library` class that manages a collection of books. Each book is a dictionary containing details like `title`, `author`, and `year_published`. The `Library` class supports adding new books, finding books by title, and removing books by title. Your task is to write a set of unit tests for the `Library` class using the `unittest` framework. Ensure that you cover various scenarios, including successful operations and error conditions. Implement setup and teardown methods to prepare and clean up test data. Use appropriate assertion methods to validate the functionality. # The `Library` Class: ```python class Library: def __init__(self): self.books = [] def add_book(self, book): Adds a new book to the library. self.books.append(book) def find_book_by_title(self, title): Finds a book by its title. for book in self.books: if book[\'title\'] == title: return book return None def remove_book_by_title(self, title): Removes a book by its title. for book in self.books: if book[\'title\'] == title: self.books.remove(book) return True return False ``` # Requirements: 1. Create a test class named `TestLibrary` that inherits from `unittest.TestCase`. 2. Implement the following test methods in the `TestLibrary` class: - `test_add_book`: Test adding a new book to the library. - `test_find_book_by_title`: Test finding a book by its title. - `test_find_nonexistent_book`: Test finding a book that does not exist. - `test_remove_book_by_title`: Test removing a book by its title. - `test_remove_nonexistent_book`: Test removing a book that does not exist. 3. Use the `setUp` method to initialize a `Library` instance and add some sample books before each test case. 4. Use the `tearDown` method to clean up the `Library` instance after each test case. 5. Use appropriate assertion methods to validate the results of the operations. # Example: ```python import unittest class TestLibrary(unittest.TestCase): def setUp(self): self.library = Library() self.sample_books = [ {\'title\': \'1984\', \'author\': \'George Orwell\', \'year_published\': 1949}, {\'title\': \'To Kill a Mockingbird\', \'author\': \'Harper Lee\', \'year_published\': 1960} ] for book in self.sample_books: self.library.add_book(book) def tearDown(self): del self.library def test_add_book(self): new_book = {\'title\': \'The Great Gatsby\', \'author\': \'F. Scott Fitzgerald\', \'year_published\': 1925} self.library.add_book(new_book) self.assertIn(new_book, self.library.books) def test_find_book_by_title(self): book = self.library.find_book_by_title(\'1984\') self.assertIsNotNone(book) self.assertEqual(book[\'title\'], \'1984\') def test_find_nonexistent_book(self): book = self.library.find_book_by_title(\'Nonexistent Book\') self.assertIsNone(book) def test_remove_book_by_title(self): removed = self.library.remove_book_by_title(\'1984\') self.assertTrue(removed) self.assertIsNone(self.library.find_book_by_title(\'1984\')) def test_remove_nonexistent_book(self): removed = self.library.remove_book_by_title(\'Nonexistent Book\') self.assertFalse(removed) if __name__ == \'__main__\': unittest.main() ``` Ensure your implementation is correct by running the tests. The tests should pass and provide adequate coverage for the `Library` class functionalities.","solution":"class Library: def __init__(self): self.books = [] def add_book(self, book): Adds a new book to the library. self.books.append(book) def find_book_by_title(self, title): Finds a book by its title. for book in self.books: if book[\'title\'] == title: return book return None def remove_book_by_title(self, title): Removes a book by its title. for book in self.books: if book[\'title\'] == title: self.books.remove(book) return True return False"},{"question":"Background The `uu` module in Python allows for the encoding and decoding of arbitrary binary data using uuencode format, typically for transferring over ASCII-only connections. This is a deprecated method as of Python 3.11, but understanding it can offer insights into data encoding at a lower level. This question involves working with the `uu` module to encode and decode files. Problem Statement You are given the task of creating a command-line utility that can compress and decompress `.txt` files using the uuencode format. The utility should be able to: 1. **Encode** a given text file to a `.uu` file. 2. **Decode** a given `.uu` file back to its original text file. You are to implement two functions: 1. **`encode_uu(input_filepath: str, output_filepath: str) -> None`** - **Input**: - `input_filepath` (str): The path of the text file to be encoded. - `output_filepath` (str): The path where the encoded uuencoded file should be saved. - **Output**: - None - **Functionality**: - This function should read the contents of the specified `input_filepath` file, use the `uu.encode` method to encode it to uuencode format, and write the result to `output_filepath`. 2. **`decode_uu(input_filepath: str, output_filepath: str) -> None`** - **Input**: - `input_filepath` (str): The path of the `.uu` file to be decoded. - `output_filepath` (str): The path where the decoded text file should be saved. - **Output**: - None - **Functionality**: - This function should read the uuencoded file from `input_filepath`, decode it using `uu.decode`, and write the original content to `output_filepath`. Constraints - The input files should be small text files (not exceeding 1MB). - The functions should handle any exceptions raised during file operations and print appropriate error messages. - The program should ensure that the file permissions are preserved during the encoding and decoding processes. Example Let\'s say we have a text file named `example.txt` with content: ``` Hello World! This is a test file. ``` **Encoding Process**: ```python input_filepath = \'example.txt\' output_filepath = \'example_encoded.uu\' encode_uu(input_filepath, output_filepath) ``` This creates a file `example_encoded.uu` in uuencode format. **Decoding Process**: ```python input_filepath = \'example_encoded.uu\' output_filepath = \'example_decoded.txt\' decode_uu(input_filepath, output_filepath) ``` This should recreate the `example.txt` content in `example_decoded.txt`. Note: Make use of the `uu` module functions `uu.encode` and `uu.decode` for this task. You may also need to handle file operations and exceptions appropriately.","solution":"import uu import os def encode_uu(input_filepath: str, output_filepath: str) -> None: Encodes a text file to the uuencode format. Args: - input_filepath (str): Path to the input text file to be encoded. - output_filepath (str): Path to save the uuencoded file. Returns: - None try: # Open the input file for reading and the output file for writing in binary mode. with open(input_filepath, \'rb\') as infile, open(output_filepath, \'wb\') as outfile: uu.encode(infile, outfile, name=os.path.basename(input_filepath)) except Exception as e: print(f\\"Error encoding file {input_filepath} to {output_filepath}: {e}\\") def decode_uu(input_filepath: str, output_filepath: str) -> None: Decodes a uuencoded file back to its original text format. Args: - input_filepath (str): Path to the uuencoded file to be decoded. - output_filepath (str): Path to save the decoded text file. Returns: - None try: # Open the uuencoded file for reading and the output file for writing in binary mode. with open(input_filepath, \'rb\') as infile, open(output_filepath, \'wb\') as outfile: uu.decode(infile, outfile) except Exception as e: print(f\\"Error decoding file {input_filepath} to {output_filepath}: {e}\\")"},{"question":"You are tasked with creating a serialization and deserialization mechanism for a PyTorch model that ensures efficient storage management. You need to: 1. Define a simple custom neural network module. 2. Save its state dictionary to a file. 3. Ensure that tensor views are preserved when loading the state back. 4. Implement a function to clone tensors before saving if they share storage objects with larger tensors. 5. Provide methods to save and load the model state, ensuring compatibility between the saved and loaded states. # Instructions 1. **Define the Neural Network Module**: - Create a custom PyTorch module with at least two linear layers. 2. **Save with Cloning if Necessary**: - Implement a function `save_model_with_cloning(model, file_path)` that saves the state dictionary of the model. - If any tensors in the model\'s state dictionary share storage with larger tensors, clone them before saving to reduce file size. 3. **Load and Preserve Views**: - Implement a function `load_model_and_preserve_views(model, file_path)` that loads the state dictionary from the file and restores it to the model. - Ensure that any view relationships in the tensors are preserved. # Specifications - **Input**: - `save_model_with_cloning(model, file_path)`: - `model`: The PyTorch module to save. - `file_path`: The file path to save the model state dictionary. - `load_model_and_preserve_views(model, file_path)`: - `model`: The PyTorch module to load the state dictionary into. - `file_path`: The file path from which to load the model state dictionary. - **Output**: - `save_model_with_cloning()` should save the state dictionary to the specified file. - `load_model_and_preserve_views()` should load the state dictionary from the specified file and update the given model. - **Constraints**: - The solution must handle scenarios where tensors in the state dictionary share storage with larger tensors. - The loaded state should preserve any view relationships present in the tensors. - **Performance**: - The solution should be efficient in terms of both time and storage usage. # Example ```python # Define the custom module class MyModule(torch.nn.Module): def __init__(self): super(MyModule, self).__init__() self.layer1 = torch.nn.Linear(10, 5) self.layer2 = torch.nn.Linear(5, 2) def forward(self, x): return self.layer2(torch.relu(self.layer1(x))) # Example usage model = MyModule() # Save the model state dictionary with cloning if necessary save_model_with_cloning(model, \'model_state.pth\') # Load the model state dictionary and preserve views loaded_model = MyModule() load_model_and_preserve_views(loaded_model, \'model_state.pth\') ``` Ensure your solution correctly implements the save and load functions with handling for shared storage and view preservation. # Hints - Use `torch.save` and `torch.load` for saving and loading the state dictionaries. - Use `tensor.clone()` to create independent copies of tensors with shared storage. - Check for storage sharing using `tensor.storage()`.","solution":"import torch class MyModule(torch.nn.Module): def __init__(self): super(MyModule, self).__init__() self.layer1 = torch.nn.Linear(10, 5) self.layer2 = torch.nn.Linear(5, 2) def forward(self, x): return self.layer2(torch.relu(self.layer1(x))) def save_model_with_cloning(model, file_path): state_dict = model.state_dict() cloned_state_dict = {} for key, tensor in state_dict.items(): if tensor.storage_offset() != 0 or tensor.is_shared(): tensor = tensor.clone() cloned_state_dict[key] = tensor torch.save(cloned_state_dict, file_path) def load_model_and_preserve_views(model, file_path): state_dict = torch.load(file_path) model.load_state_dict(state_dict)"},{"question":"# XML Document Processing Using `xml.etree.ElementTree` You are provided with an XML document containing information about books in a library. Write a Python function that parses this XML document and processes it according to the following requirements: 1. **Read the XML Document**: Parse the provided XML data. 2. **Filter Books by Genre and Year**: Extract all books that belong to a given genre and published in a specific year. 3. **Update Book Ratings**: Increase the rating of all filtered books by 0.5, but ensure the rating does not exceed 5.0. 4. **Add a New Book**: Add a new book to the XML document with attributes and sub-elements provided as arguments to the function. 5. **Remove Books by Author**: Remove all books written by a specified author. 6. **Save the Updated XML**: Write the updated XML document to a new file. Here is the XML structure you will start with: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> <genre>Fiction</genre> <year>2021</year> <rating>4.5</rating> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <genre>Science</genre> <year>2020</year> <rating>3.0</rating> </book> <!-- More books --> </library> ``` **Function Signature**: ```python def process_library_xml(xml_data: str, genre: str, year: int, new_book: dict, author_to_remove: str, output_file: str) -> None: pass ``` **Input**: - `xml_data` (str): A string representation of the XML document. - `genre` (str): The genre to filter books. - `year` (int): The publication year to filter books. - `new_book` (dict): A dictionary containing the new book\'s details with keys: `title`, `author`, `genre`, `year`, and `rating`. - `author_to_remove` (str): The author whose books should be removed. - `output_file` (str): The filename for the output updated XML document. **Output**: - The function should not return anything. It writes the processed XML to the specified `output_file`. **Constraints**: - Ensure the rating does not exceed 5.0 after incrementing. - The XML structure should remain well-formed after modifications. - Useful `xml.etree.ElementTree` methods: `parse()`, `fromstring()`, `findall()`, `append()`, `remove()`, `tostring()`, `write()`, etc. **Example Usage**: ```python xml_input = \'\'\'<library> <book> <title>Book Title 1</title> <author>Author 1</author> <genre>Fiction</genre> <year>2021</year> <rating>4.5</rating> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <genre>Science</genre> <year>2020</year> <rating>3.0</rating> </book> </library>\'\'\' new_book_details = { \'title\': \'New Book Title\', \'author\': \'New Author\', \'genre\': \'Science\', \'year\': 2023, \'rating\': 4.2 } process_library_xml(xml_input, \'Science\', 2020, new_book_details, \'Author 2\', \'updated_library.xml\') ``` In this example, the function: 1. Filters and updates the rating of the book with the genre \\"Science\\" and year \\"2020\\". 2. Adds a new book with the specified details. 3. Removes all books by \\"Author 2\\". 4. Writes the resulting XML to `updated_library.xml`.","solution":"import xml.etree.ElementTree as ET def process_library_xml(xml_data: str, genre: str, year: int, new_book: dict, author_to_remove: str, output_file: str) -> None: # Parse the provided XML data root = ET.fromstring(xml_data) # Filter and update ratings of books for book in root.findall(\'book\'): book_genre = book.find(\'genre\').text book_year = int(book.find(\'year\').text) if book_genre == genre and book_year == year: rating_elem = book.find(\'rating\') rating = float(rating_elem.text) new_rating = min(rating + 0.5, 5.0) rating_elem.text = str(new_rating) # Remove books by specified author for book in root.findall(\'book\'): book_author = book.find(\'author\').text if book_author == author_to_remove: root.remove(book) # Add new book to the XML new_book_elem = ET.Element(\'book\') for key, value in new_book.items(): sub_elem = ET.SubElement(new_book_elem, key) sub_elem.text = str(value) root.append(new_book_elem) # Write the updated XML to the output file tree = ET.ElementTree(root) tree.write(output_file, encoding=\'utf-8\', xml_declaration=True)"},{"question":"# Advanced Coding Assessment Objective: To test the understanding of students on the `inspect` module by requiring them to implement functions that utilize various introspective capabilities provided by the `inspect` module. Question: Implement a class `FunctionInspector` with the following methods: 1. `get_function_metadata(func)`: This method takes a function `func` as its parameter and returns a dictionary with the following keys: - `name`: Name of the function. - `docstring`: Documentation string of the function. - `source_code`: Source code of the function. - `args`: List of argument names of the function. - `varargs`: The name of the `*args` parameter or `None`. - `kwargs`: The name of the `**kwargs` parameter or `None`. - `defaults`: A tuple of the default values for the function’s arguments or `None`. - `annotations`: A dictionary of parameter names to annotations. 2. `get_class_methods(cls)`: This method takes a class `cls` as its parameter and returns a dictionary where keys are the method names, and values are the respective method objects of the given class. 3. `get_signature_details(func)`: This method takes a function `func` as its parameter and returns a dictionary with the following keys: - `parameters`: An ordered mapping of the function parameters\' names to their corresponding `Parameter` objects. - `return_annotation`: The return annotation for the function or `Signature.empty` if not annotated. Constraints: - You must use the functionalities provided by the `inspect` module. - Raise appropriate errors if the input is not valid (e.g., the input is not a function for `get_function_metadata`, not a class for `get_class_methods`, and so on). - Ensure the returned metadata is accurate and well-formatted. Example: ```python import inspect class FunctionInspector: @staticmethod def get_function_metadata(func): # Your code here @staticmethod def get_class_methods(cls): # Your code here @staticmethod def get_signature_details(func): # Your code here # Example Usage: def example_function(a: int, b: str = \\"default\\") -> bool: This is an example function return isinstance(b, str) class ExampleClass: def method1(self): pass def method2(self, param): pass print(FunctionInspector.get_function_metadata(example_function)) # Output: # { # \'name\': \'example_function\', # \'docstring\': \'This is an example function\', # \'source_code\': \\"def example_function(a: int, b: str = \'default\') -> bool:n \\"\\"\\"This is an example function\\"\\"\\"n return isinstance(b, str)n\\", # \'args\': [\'a\', \'b\'], # \'varargs\': None, # \'kwargs\': None, # \'defaults\': (\'default\',), # \'annotations\': {\'a\': int, \'b\': str, \'return\': bool} # } print(FunctionInspector.get_class_methods(ExampleClass)) # Output: # { # \'method1\': <function ExampleClass.method1 at ...>, # \'method2\': <function ExampleClass.method2 at ...> # } print(FunctionInspector.get_signature_details(example_function)) # Output: # { # \'parameters\': {\'a\': <Parameter \\"a: int\\">, \'b\': <Parameter \\"b: str = \'default\'\\">}, # \'return_annotation\': <class \'bool\'> # } ``` Hints: - Use `inspect.getfullargspec` and `inspect.signature` to gather function details. - Use `inspect.getmembers` with appropriate predicates to filter out class methods. - For fetching source code, use `inspect.getsource`.","solution":"import inspect class FunctionInspector: @staticmethod def get_function_metadata(func): if not inspect.isfunction(func): raise ValueError(\\"Provided input is not a function\\") func_metadata = { \\"name\\": func.__name__, \\"docstring\\": inspect.getdoc(func), \\"source_code\\": inspect.getsource(func), \\"args\\": [], \\"varargs\\": None, \\"kwargs\\": None, \\"defaults\\": None, \\"annotations\\": func.__annotations__ } full_args_spec = inspect.getfullargspec(func) func_metadata[\\"args\\"] = full_args_spec.args func_metadata[\\"varargs\\"] = full_args_spec.varargs func_metadata[\\"kwargs\\"] = full_args_spec.varkw func_metadata[\\"defaults\\"] = full_args_spec.defaults return func_metadata @staticmethod def get_class_methods(cls): if not inspect.isclass(cls): raise ValueError(\\"Provided input is not a class\\") methods = {name: member for name, member in inspect.getmembers(cls, predicate=inspect.isfunction)} return methods @staticmethod def get_signature_details(func): if not inspect.isfunction(func): raise ValueError(\\"Provided input is not a function\\") signature = inspect.signature(func) return { \\"parameters\\": signature.parameters, \\"return_annotation\\": signature.return_annotation }"},{"question":"# PyTorch Coding Assessment Question Objective Demonstrate your understanding of PyTorch\'s neural network parameter initialization by implementing a custom neural network initialization function. Problem Statement You are tasked with writing a function `custom_weight_init` that initializes the weights of a given neural network model based on the initialization techniques provided by the `torch.nn.init` module. Function Signature ```python import torch import torch.nn as nn import torch.nn.init as init def custom_weight_init(model: nn.Module, method: str) -> None: Initialize the weights of the given model using the specified method. Parameters: model (nn.Module): The neural network model whose weights need to be initialized. method (str): The method of initialization to be used. It can be one of the following: \'uniform\', \'normal\', \'constant\', \'ones\', \'zeros\', \'eye\', \'dirac\', \'xavier_uniform\', \'xavier_normal\', \'kaiming_uniform\', \'kaiming_normal\', \'trunc_normal\', \'orthogonal\', \'sparse\' Returns: None: The function updates the model weights in-place. ``` Input - `model`: A PyTorch neural network model (an instance of `nn.Module`). - `method`: A string specifying the initialization method. This string must be one of the following: - `\'uniform\'` - `\'normal\'` - `\'constant\'` - `\'ones\'` - `\'zeros\'` - `\'eye\'` - `\'dirac\'` - `\'xavier_uniform\'` - `\'xavier_normal\'` - `\'kaiming_uniform\'` - `\'kaiming_normal\'` - `\'trunc_normal\'` - `\'orthogonal\'` - `\'sparse\'` Output - The function should not return anything. It updates the weights of the model in-place using the specified initialization method. Constraints - You can assume the `method` parameter will always be a valid string from the specified list. - The function should handle different types of layers in the model, including `nn.Linear` and `nn.Conv2d`. Example ```python # Define a simple neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) def forward(self, x): x = self.conv1(x) x = torch.flatten(x, 1) x = self.fc1(x) return x # Create an instance of the model model = SimpleModel() # Initialize the weights of the model using the \'xavier_uniform\' method custom_weight_init(model, \'xavier_uniform\') ``` Notes - The function should iterate over all layers of the given model and apply the specified initialization method to each layer\'s weights. - Ensure that the function does not alter gradient tracking while performing the initialization. Solution Here you might provide a potential solution for educational purposes, but let students attempt to solve it independently first. ```python import torch import torch.nn as nn import torch.nn.init as init def custom_weight_init(model: nn.Module, method: str) -> None: init_methods = { \'uniform\': init.uniform_, \'normal\': init.normal_, \'constant\': init.constant_, \'ones\': init.ones_, \'zeros\': init.zeros_, \'eye\': init.eye_, \'dirac\': init.dirac_, \'xavier_uniform\': init.xavier_uniform_, \'xavier_normal\': init.xavier_normal_, \'kaiming_uniform\': init.kaiming_uniform_, \'kaiming_normal\': init.kaiming_normal_, \'trunc_normal\': init.trunc_normal_, \'orthogonal\': init.orthogonal_, \'sparse\': init.sparse_, } def init_weights(m): if isinstance(m, (nn.Linear, nn.Conv2d)): if method in init_methods: init_methods[method](m.weight) model.apply(init_weights) ```","solution":"import torch import torch.nn as nn import torch.nn.init as init def custom_weight_init(model: nn.Module, method: str) -> None: init_methods = { \'uniform\': init.uniform_, \'normal\': init.normal_, \'constant\': init.constant_, \'ones\': init.ones_, \'zeros\': init.zeros_, \'eye\': init.eye_, \'dirac\': init.dirac_, \'xavier_uniform\': init.xavier_uniform_, \'xavier_normal\': init.xavier_normal_, \'kaiming_uniform\': init.kaiming_uniform_, \'kaiming_normal\': init.kaiming_normal_, \'trunc_normal\': init.trunc_normal_, \'orthogonal\': init.orthogonal_, \'sparse\': init.sparse_, } def init_weights(m): if isinstance(m, (nn.Linear, nn.Conv2d)): if method in init_methods: init_methods[method](m.weight) with torch.no_grad(): # Ensure gradients are not tracked during initialization model.apply(init_weights)"},{"question":"# XML Parsing with `xml.parsers.expat` You are required to implement a function `parse_xml(xml_string: str) -> dict` that parses a given XML string and returns a nested dictionary representation of its contents. The dictionary should map element names to their respective content, including attributes and nested elements. Function Signature ```python def parse_xml(xml_string: str) -> dict: pass ``` Input - `xml_string`: A string containing the XML data. Output - A dictionary representing the XML structure. Example For the following XML string input: ```xml <root> <parent id=\\"top\\"> <child1 name=\\"paul\\">Text goes here</child1> <child2 name=\\"fred\\">More text</child2> </parent> </root> ``` The function should return: ```python { \\"root\\": { \\"parent\\": { \\"@id\\": \\"top\\", \\"child1\\": { \\"@name\\": \\"paul\\", \\"#text\\": \\"Text goes here\\" }, \\"child2\\": { \\"@name\\": \\"fred\\", \\"#text\\": \\"More text\\" } } } } ``` Constraints - The XML will always be well-formed. - Element names, attributes, and text content should be properly parsed. - Handle potential nested elements. Performance - The function should be efficient and handle XML strings up to a reasonable length (e.g., up to 1MB). Implementation Notes - Use the `xml.parsers.expat` module. - Setup handlers for `StartElementHandler`, `EndElementHandler`, and `CharacterDataHandler`. - Manage the nested structure using an appropriate data structure like a stack. Error Handling - Raise a `ValueError` if any parsing errors occur, and provide a descriptive error message using the `ErrorString(error_number)`.","solution":"import xml.parsers.expat def parse_xml(xml_string: str) -> dict: Parses a given XML string and returns a nested dictionary representation of its contents. parser = xml.parsers.expat.ParserCreate() result = {} stack = [] current = result stack.append(current) def start_element(name, attrs): nonlocal current new_element = {} for key, value in attrs.items(): new_element[\\"@{}\\".format(key)] = value if name in current: if not isinstance(current[name], list): current[name] = [current[name]] current[name].append(new_element) current = current[name][-1] else: current[name] = new_element current = new_element stack.append(current) def end_element(name): stack.pop() nonlocal current current = stack[-1] def char_data(data): nonlocal current data = data.strip() if data: if \\"#text\\" in current: current[\\"#text\\"] += data else: current[\\"#text\\"] = data parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_string, True) except xml.parsers.expat.ExpatError as e: raise ValueError(\\"XML parsing error: {}\\".format(xml.parsers.expat.ErrorString(e.code))) return result"},{"question":"Objective Demonstrate your comprehension of PyTorch\'s nested tensors by implementing a function that performs specific operations on a batch of variable-length sequences. Task You are given batches of variable-length sequences, and your task is to implement a function `process_nested_tensor` that: 1. Creates a nested tensor from the given sequences. 2. Computes a specified operation on each sub-tensor. 3. Returns the processed nested tensor in both nested and padded formats. Function Definition ```python import torch def process_nested_tensor(sequences, operation, padding_value=0, dtype=torch.float32): Creates a nested tensor from the sequences, applies the specified operation to each sub-tensor, and returns the processed nested tensor in both nested and padded formats. Parameters: sequences (list of lists): A list of lists where each inner list represents a sequence of variable length. operation (callable): A function to apply to each sub-tensor in the nested tensor. padding_value (float, optional): Value to use for padding the sequences. Default is 0. dtype (torch.dtype, optional): Data type of the nested tensor. Default is torch.float32. Returns: tuple: A tuple containing the processed nested tensor and the padded tensor. pass ``` Input - `sequences`: A list of lists where each inner list represents a sequence of variable length (e.g., a batch of sentences with different lengths). - `operation`: A callable function (e.g., `torch.sin`, custom lambda function) that applies an operation to each component tensor in the nested tensor. - `padding_value`: (Optional) A value to use for padding the sequences when converting back to a dense tensor. Default is 0. - `dtype`: (Optional) The data type of the nested tensor. Default is `torch.float32`. Output - A tuple containing: - The processed nested tensor (of type `torch.Tensor`). - The padded tensor (of type `torch.Tensor`) with the specified padding value. Constraints - You are not allowed to use padding to handle the variable-length sequences initially; instead, you must use nested tensors. - Your implementation should efficiently handle the conversion between nested and padded formats. Example ```python import torch def example_operation(tensor): # Example operation: add 1 to each element return tensor + 1 sequences = [ [1, 2, 3], [4, 5], [6, 7, 8, 9] ] nested_tensor, padded_tensor = process_nested_tensor(sequences, example_operation, padding_value=0) print(nested_tensor) # Output: Nested Tensor containing the sequences with each element incremented by 1 print(padded_tensor) # Output: Padded Tensor with shape (3, 4) and padded with zeros ``` Implement the function `process_nested_tensor` to solve the above task.","solution":"import torch import torch.nn.functional as F def process_nested_tensor(sequences, operation, padding_value=0, dtype=torch.float32): Creates a nested tensor from the sequences, applies the specified operation to each sub-tensor, and returns the processed nested tensor in both nested and padded formats. Parameters: sequences (list of lists): A list of lists where each inner list represents a sequence of variable length. operation (callable): A function to apply to each sub-tensor in the nested tensor. padding_value (float, optional): Value to use for padding the sequences. Default is 0. dtype (torch.dtype, optional): Data type of the nested tensor. Default is torch.float32. Returns: tuple: A tuple containing the processed nested tensor and the padded tensor. # Create nested tensor nested_tensor = [torch.tensor(seq, dtype=dtype) for seq in sequences] # Apply operation to each sub-tensor processed_nested_tensor = [operation(tensor) for tensor in nested_tensor] # Find the maximum length for padding max_length = max(map(len, sequences)) # Pad the tensors and stack them to form a padded tensor padded_tensor_list = [F.pad(tensor, (0, max_length - tensor.size(0)), value=padding_value) for tensor in processed_nested_tensor] padded_tensor = torch.stack(padded_tensor_list) return processed_nested_tensor, padded_tensor"},{"question":"Problem Statement You are tasked with implementing an enhanced interactive console that logs commands executed by the user along with their results. The objective is to create a subclass of `code.InteractiveConsole` that captures and stores every command entered and its output in a log. This will help in reviewing the commands and their results after the interactive session ends. Required Functionality 1. **Implement a subclass of `code.InteractiveConsole` called `LoggedInteractiveConsole`:** - **Constructor**: Initialize the base class and an empty list to store logs. - **Override the `runcode()` method** to capture the command and its result or exception and append this information to a log list. - **Override the `interact()` method** to optionally print the log after the session ends. 2. **Capture stdout and stderr** during command execution to store the outputs and any errors separately in your logs. 3. **Implement a method `get_log()`** that returns the list of logs captured during the session. Expected Input and Output Formats 1. **Constructor**: - Input: `locals` (dictionary, optional), `filename` (string, optional, defaults to `\'<console>\'`) - Output: Instance of `LoggedInteractiveConsole` 2. **runcode(code)**: - Input: `code` (code object to be executed) - Operation: Executes the code, logs the command and its output or exception. - Output: None 3. **get_log()**: - Input: None - Output: List of tuples, where each tuple contains the command string, its output, and any error message (if applicable). Constraints and Limitations - Ensure that the commands and their results are stored in the order they are executed. - The log should handle multiline commands properly. - The log should capture any type of exception raised during the execution of a command. - Performance should be adequate for normal interactive use; large-scale command inputs are not the main focus. Example ```python from code import InteractiveConsole import io import sys class LoggedInteractiveConsole(InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals, filename) self.log = [] def runcode(self, code): # capturing stdout and stderr old_stdout = sys.stdout old_stderr = sys.stderr sys.stdout = stdout_capture = io.StringIO() sys.stderr = stderr_capture = io.StringIO() try: exec(code, self.locals) # Execute the code output = stdout_capture.getvalue() error = stderr_capture.getvalue() self.log.append((code, output, error)) except Exception as e: error = stderr_capture.getvalue() + str(e) self.log.append((code, \'\', error)) finally: sys.stdout = old_stdout sys.stderr = old_stderr def interact(self, banner=None, exitmsg=None): super().interact(banner, exitmsg) # print logs after the session ends print(\\"nSession Log:\\") for command, output, error in self.log: print(\\"Command:\\", command) print(\\"Output:\\", output) print(\\"Error:\\", error if error else \\"No errors\\") def get_log(self): return self.log ``` Task 1. Write the `LoggedInteractiveConsole` class as described above. 2. Demonstrate its functionality with a test script that: - Instantiates the console. - Executes several commands (including some with errors). - Ends the interactive session and prints the log.","solution":"import code import io import sys class LoggedInteractiveConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals, filename) self.log = [] def runcode(self, code): old_stdout = sys.stdout old_stderr = sys.stderr sys.stdout = stdout_capture = io.StringIO() sys.stderr = stderr_capture = io.StringIO() try: exec(code, self.locals) except Exception as e: stderr_capture.write(str(e)) output = stdout_capture.getvalue() error = stderr_capture.getvalue() if error: self.log.append((code, output, error)) else: self.log.append((code, output, \\"No errors\\")) sys.stdout = old_stdout sys.stderr = old_stderr def get_log(self): return self.log def interact(self, banner=None, exitmsg=None): super().interact(banner, exitmsg) print(\\"nSession Log:\\") for command, output, error in self.log: print(f\\"Command: {command}nOutput: {output}nError: {error}n\\")"},{"question":"# Question: You are provided with a dataset representing the number of products sold by different employees over various months. Use Seaborn to visualize this data effectively. Your task involves creating a well-designed plot that conveys meaningful insights. **Dataset:** ```python data = { \\"Employee\\": [\\"John\\", \\"John\\", \\"John\\", \\"Alice\\", \\"Alice\\", \\"Alice\\", \\"Mark\\", \\"Mark\\", \\"Mark\\", \\"Eve\\", \\"Eve\\", \\"Eve\\"], \\"Month\\": [\\"January\\", \\"February\\", \\"March\\", \\"January\\", \\"February\\", \\"March\\", \\"January\\", \\"February\\", \\"March\\", \\"January\\", \\"February\\", \\"March\\"], \\"Sales\\": [30, 35, 40, 20, 25, 30, 50, 45, 40, 60, 70, 65] } ``` **Requirements:** 1. Load the data into a Pandas DataFrame. 2. Use Seaborn to create a bar plot that shows the monthly sales for each employee. 3. Customize the plot by setting a Seaborn theme of your choice. 4. Enhance the plot by modifying its appearance: change colors, adjust grid lines, remove top and right spines, label axes and title appropriately. 5. Finally, save the plot as an image file named `employee_sales.png`. **Function Signature:** ```python def visualize_employee_sales(data: dict) -> None: # Implementation here ``` **Example:** Your plot should look similar to the example below but with the customizations you applied: ![example_plot](https://via.placeholder.com/400x300?text=Example+Plot) **Notes:** - Use `sns.set_theme()` to configure the overall theme. - Utilize `barplot` for visualizing the data. - Apply `rc` parameter settings for additional customizations. - Ensure the final plot is saved using `plt.savefig()`. **Input:** - `data`: A dictionary containing the sales data as provided above. **Output:** - There should be no return value. The function is expected to save the plot as `employee_sales.png`.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_employee_sales(data: dict) -> None: # Load data into a DataFrame df = pd.DataFrame(data) # Set Seaborn theme sns.set_theme(style=\\"whitegrid\\") # Create a bar plot plt.figure(figsize=(10, 6)) bar_plot = sns.barplot(x=\\"Month\\", y=\\"Sales\\", hue=\\"Employee\\", data=df, palette=\\"pastel\\") # Customize the plot appearance bar_plot.set_title(\\"Monthly Sales by Employee\\") bar_plot.set_xlabel(\\"Month\\") bar_plot.set_ylabel(\\"Sales\\") sns.despine(top=True, right=True) # Save the plot as an image file plt.savefig(\\"employee_sales.png\\") # Show the plot plt.show()"},{"question":"# Terminal Control Assessment You are required to demonstrate your understanding of Unix-based terminal control using the `tty` module in Python. Problem Statement Write a function `configure_terminal(fd: int, mode: str) -> None` that configures a file descriptor to a specified mode using the `tty` module. The function should accept two parameters: 1. `fd` (int): The file descriptor of the terminal. 2. `mode` (str): The mode to set for the terminal, which can either be `\\"raw\\"` or `\\"cbreak\\"`. The function should: - Use the appropriate function from the `tty` module to change the terminal mode based on the `mode` parameter. - Raise a `ValueError` if the `mode` parameter is not `\\"raw\\"` or `\\"cbreak\\"`. Function Signature ```python def configure_terminal(fd: int, mode: str) -> None: pass ``` Constraints - The function should handle the `mode` parameter case-insensitively. - You may assume that the provided file descriptor `fd` is valid and the system is Unix-based. Example Usage ```python import os import termios import tty # Example file descriptor for the terminal (stdin is commonly used) fd = os.open(\'/dev/tty\', os.O_RDWR) # Set terminal to raw mode configure_terminal(fd, \'raw\') # Set terminal to cbreak mode configure_terminal(fd, \'cbreak\') # Invalid mode should raise a ValueError try: configure_terminal(fd, \'invalid\') except ValueError as ve: print(ve) # Expected output: \\"Invalid mode specified\\" ``` Note Make sure to handle the terminal modes correctly by using `tty.setraw` and `tty.setcbreak` functions, and remember to convert the mode parameter to the correct case format if necessary.","solution":"import tty def configure_terminal(fd: int, mode: str) -> None: Configures a file descriptor to a specified mode using the tty module. Parameters: fd (int): The file descriptor of the terminal. mode (str): The mode to set for the terminal, either \'raw\' or \'cbreak\'. Raises: ValueError: If the mode is not \'raw\' or \'cbreak\'. mode = mode.lower() if mode == \'raw\': tty.setraw(fd) elif mode == \'cbreak\': tty.setcbreak(fd) else: raise ValueError(\\"Invalid mode specified\\")"},{"question":"You are tasked with analyzing a dataset and visualizing its distributions using Seaborn\'s ECDF plot functionality. Your goal is to demonstrate your ability to understand and effectively use the different parameters of `sns.ecdfplot` to extract meaningful insights from the dataset. Follow the instructions below to complete the assignment. Dataset You will be working with the `penguins` dataset that can be loaded directly from Seaborn. ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` Instructions 1. **Basic ECDF Plot** Create a basic ECDF plot of the `flipper_length_mm` variable. The plot should show the cumulative distribution of `flipper_length_mm`. 2. **ECDF Plot with Hue Mapping** Create an ECDF plot of the `bill_length_mm` variable, using the `species` variable for hue mapping. This plot should allow you to compare the distributions of `bill_length_mm` across different penguin species. 3. **Complementary ECDF Plot** Create an ECDF plot of the `bill_depth_mm` variable, but plot the empirical complementary CDF (1 - CDF). Use the `island` variable for hue mapping. 4. **Count-based ECDF Plot** Create an ECDF plot of the `body_mass_g` variable using the absolute count as the y-axis statistic. Use the `sex` variable for hue mapping. Expected Output - Four Seaborn plots as described in the instructions. - Each plot must: - Appropriately use the `sns.ecdfplot` function with the necessary parameters. - Include relevant titles and axis labels for clarity. - Be displayed inline. Example Code Here is a generic template to get you started on loading the dataset and creating your Seaborn plots. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Basic ECDF Plot plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\'ECDF of Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'ECDF\') plt.show() # 2. ECDF Plot with Hue Mapping plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\'ECDF of Bill Length by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'ECDF\') plt.legend(title=\'Species\') plt.show() # 3. Complementary ECDF Plot plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"bill_depth_mm\\", hue=\\"island\\", complementary=True) plt.title(\'Complementary ECDF of Bill Depth by Island\') plt.xlabel(\'Bill Depth (mm)\') plt.ylabel(\'1 - ECDF\') plt.legend(title=\'Island\') plt.show() # 4. Count-based ECDF Plot plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"body_mass_g\\", hue=\\"sex\\", stat=\\"count\\") plt.title(\'Count-based ECDF of Body Mass by Sex\') plt.xlabel(\'Body Mass (g)\') plt.ylabel(\'Count\') plt.legend(title=\'Sex\') plt.show() ``` Submission Submit your code as a single Python file or Jupyter notebook that contains all four plots along with the necessary commands to generate and display them.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguins_ecdf(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Basic ECDF Plot plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\'ECDF of Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'ECDF\') plt.show() # 2. ECDF Plot with Hue Mapping plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\'ECDF of Bill Length by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'ECDF\') plt.legend(title=\'Species\') plt.show() # 3. Complementary ECDF Plot plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"bill_depth_mm\\", hue=\\"island\\", complementary=True) plt.title(\'Complementary ECDF of Bill Depth by Island\') plt.xlabel(\'Bill Depth (mm)\') plt.ylabel(\'1 - ECDF\') plt.legend(title=\'Island\') plt.show() # 4. Count-based ECDF Plot plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"body_mass_g\\", hue=\\"sex\\", stat=\\"count\\") plt.title(\'Count-based ECDF of Body Mass by Sex\') plt.xlabel(\'Body Mass (g)\') plt.ylabel(\'Count\') plt.legend(title=\'Sex\') plt.show()"},{"question":"**Coding Assessment Question:** You are required to implement a Python function `download_and_process_data(urls: List[str], timeout: int, max_workers: int) -> Dict[str, Any]` using the `concurrent.futures` module. This function should fetch data from multiple URLs using parallel tasks and process it. # Function Specification: 1. **Input:** - `urls` (List[str]): A list of URLs to fetch data from. - `timeout` (int): The maximum number of seconds to wait for each URL. - `max_workers` (int): The maximum number of threads to use for the concurrent execution. 2. **Output:** - A dictionary where: - Keys are URLs (str). - Values are: - The content (bytes) if the task is successful. - The exception message (str) if an exception occurs during the task. # Constraints and Requirements: 1. **Constraints:** - Use `ThreadPoolExecutor` for executing the tasks. - Handle potential exceptions like timeouts or invalid URLs gracefully. - Ensure to shut down the executor properly after the tasks are completed. 2. **Performance Requirements:** - The function should be optimized to fetch and process the maximum number of URLs concurrently as specified by `max_workers`. # Example: ```python urls = [ \'http://www.foxnews.com/\', \'http://www.cnn.com/\', \'http://europe.wsj.com/\', \'http://www.bbc.co.uk/\', \'http://nonexistant-subdomain.python.org/\' ] timeout = 10 max_workers = 3 result = download_and_process_data(urls, timeout, max_workers) for url, content in result.items(): if isinstance(content, bytes): print(f\'{url} - Fetched {len(content)} bytes successfully.\') else: print(f\'{url} - Failed with exception: {content}\') ``` Your task is to complete the function `download_and_process_data` following the specification given above.","solution":"import concurrent.futures import requests from typing import List, Dict, Any def fetch_url(url: str, timeout: int) -> Any: try: response = requests.get(url, timeout=timeout) response.raise_for_status() return response.content except Exception as e: return str(e) def download_and_process_data(urls: List[str], timeout: int, max_workers: int) -> Dict[str, Any]: results = {} with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_url = {executor.submit(fetch_url, url, timeout): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: result = future.result() results[url] = result except Exception as e: results[url] = str(e) return results"},{"question":"# Question: Creating a Threaded TCP Echo Server Implement a threaded TCP echo server using the `socketserver` module. The server should handle multiple clients concurrently. Requirements: 1. **Request Handler**: - Create a class `EchoRequestHandler` that subclasses `socketserver.BaseRequestHandler`. - Override the `handle()` method to read data from the client, print the client\'s address and data received, and send the same data back to the client. 2. **Server**: - Create a class `ThreadedTCPServer` that applies `socketserver.ThreadingMixIn` and `socketserver.TCPServer`. - The server should run in a separate thread so that it can accept multiple client connections simultaneously. 3. **Main Program**: - Instantiate the server using `ThreadedTCPServer` class, binding the server to \'localhost\' and port 9999. - Use the `with` statement to automatically close the server when the program exits. - Run the server using `server.serve_forever()` within a separate thread to keep it running. 4. **Client Testing**: - Write a simple client script to connect to the server, send a message, and print the server’s response. Example Output: When running the server: ``` Server loop running in thread: Thread-1 127.0.0.1 wrote: b\'hello world\' 127.0.0.1 wrote: b\'python is great\' ``` When testing with a client: ``` Sent: hello world Received: hello world Sent: python is great Received: python is great ``` Here is a template to get you started: ```python import socketserver import threading class EchoRequestHandler(socketserver.BaseRequestHandler): def handle(self): data = self.request.recv(1024) # Receiving the data from client print(f\\"{self.client_address[0]} wrote: {data}\\") self.request.sendall(data) # Send the same data back to client class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \'localhost\', 9999 server = ThreadedTCPServer((HOST, PORT), EchoRequestHandler) with server: ip, port = server.server_address server_thread = threading.Thread(target=server.serve_forever) server_thread.daemon = True server_thread.start() print(f\\"Server loop running in thread: {server_thread.name}\\") # Keep the main thread alive to let the server run try: while True: pass except KeyboardInterrupt: print(\\"Server is shutting down...\\") server.shutdown() ``` Additionally, here is a client script for testing: ```python import socket def client(ip, port, message): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: sock.connect((ip, port)) sock.sendall(bytes(message, \'utf-8\')) response = str(sock.recv(1024), \'utf-8\') print(\\"Sent: \\", message) print(\\"Received: \\", response) if __name__ == \\"__main__\\": client(\\"localhost\\", 9999, \\"hello world\\") client(\\"localhost\\", 9999, \\"python is great\\") ``` # Constraints: - Ensure proper handling of exceptions in both server and client code. - The server should run indefinitely until interrupted (e.g., by a KeyboardInterrupt). # Evaluation Criteria: - Correct implementation of the `handle()` method to echo received messages. - Proper usage of `ThreadingMixIn` to handle requests concurrently. - Server and client code should run without errors. - The server should correctly handle multiple clients sending messages concurrently.","solution":"import socketserver import threading class EchoRequestHandler(socketserver.BaseRequestHandler): def handle(self): data = self.request.recv(1024) # Receiving the data from client print(f\\"{self.client_address[0]} wrote: {data}\\") self.request.sendall(data) # Send the same data back to client class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \'localhost\', 9999 server = ThreadedTCPServer((HOST, PORT), EchoRequestHandler) with server: ip, port = server.server_address server_thread = threading.Thread(target=server.serve_forever) server_thread.daemon = True server_thread.start() print(f\\"Server loop running in thread: {server_thread.name}\\") # Keep the main thread alive to let the server run try: while True: pass except KeyboardInterrupt: print(\\"Server is shutting down...\\") server.shutdown()"}]'),I={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:z,isLoading:!1}},computed:{filteredPoems(){const o=this.searchQuery.trim().toLowerCase();return o?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(o)||e.solution&&e.solution.toLowerCase().includes(o)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(o=>setTimeout(o,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},F={class:"card-container"},R={key:0,class:"empty-state"},q=["disabled"],L={key:0},O={key:1};function N(o,e,l,h,i,n){const m=g("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),_(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),s(b,null,v(n.displayedPoems,(r,f)=>(a(),w(m,{key:f,poem:r},null,8,["poem"]))),128)),n.displayedPoems.length===0?(a(),s("div",R,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),n.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>n.loadMore&&n.loadMore(...r))},[i.isLoading?(a(),s("span",O,"Loading...")):(a(),s("span",L,"See more"))],8,q)):d("",!0)])}const M=p(I,[["render",N],["__scopeId","data-v-ede07ddf"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/67.md","filePath":"deepseek/67.md"}'),j={name:"deepseek/67.md"},B=Object.assign(j,{setup(o){return(e,l)=>(a(),s("div",null,[x(M)]))}});export{Y as __pageData,B as default};
